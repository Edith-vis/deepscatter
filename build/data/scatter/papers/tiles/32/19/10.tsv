id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
fa6ca81f2304d0632ba2a572bcd1c0b3d5dc5a36	convergence of gradient method for training ridge polynomial neural network	convergence;gradient algorithm;monotonicity;ridge polynomial neural network	The ridge polynomial neural network is one of the most popular higher-order neural networks, which has the powerful capability of approximating reasonable functions while avoiding the combinatorial increase in the number of weights required. In this paper, we study the convergence of gradient method with batch updating rule for ridge polynomial neural network, and a monotonicity theorem and two convergence theorems (including a weak convergence and a strong convergence) are proved. The experimental results demonstrate that the proposed theorems are valid.	algorithm;approximation;artificial neural network;gradient method;group method of data handling;polynomial	Xin Yu;Fei Deng	2012	Neural Computing and Applications	10.1007/s00521-012-0915-4	mathematical optimization;combinatorics;discrete mathematics;modes of convergence;compact convergence;machine learning;mathematics;convergence tests;weak convergence;normal convergence;statistics	ML	22.675207638912337	-33.64085479832351	158476
7a00c7afc6af6ace9e0ac78766fee4e05d045231	relaxations for inference in restricted boltzmann machines		We propose a randomized relax-and-round inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted Boltzmann machines. We also use our underlying sampler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods. 1. Background and setup A binary pairwise Markov random field (MRF) over n variables x ∈ {0, 1} models a probability distribution pÃ(x) ∝ exp(xÃx). The non-diagonal entries of the matrix Ã ∈ Rn×n encode pairwise potentials between variables while its diagonal entries encode unary potentials. The exponentiated linear term xÃx is the negative energy or simply the score of the MRF. A restricted Boltzmann machine (RBM) is a particular MRF whose variables are split into two classes, visible and hidden, and in which intra-class pairwise potentials are disallowed. Notation We write Symn for the set of symmetric n× n real matrices, and S to denote the unit sphere {x ∈ R : ‖x‖2 = 1}. All vectors are columns unless stated otherwise. 1.1. Integer quadratic programming Finding the maximum a posteriori (MAP) value of a discrete pairwise MRF can be cast as an integer quadratic program (IQP) given by max x∈{−1,1}n xAx (1) International Conference on Learning Representations, Banff, Canada, 2014. ∗Authors contributed equally. Note that we have the domain constraint x ∈ {−1, 1} rather than {0, 1}. We relate the two in Section 2.3.	approximation algorithm;banff world media festival;column (database);encode;gibbs sampling;linear programming relaxation;local optimum;local search (optimization);markov chain;markov random field;partition function (mathematics);quadratic programming;randomized algorithm;randomized rounding;restricted boltzmann machine;sampling (signal processing);search algorithm;term (logic);the matrix;unary operation	Sida I. Wang;Roy Frostig;Percy Liang;Christopher D. Manning	2013	CoRR		boltzmann machine;mathematical optimization;machine learning;mathematics;restricted boltzmann machine;statistics	ML	23.60493360869503	-31.28110428092288	159127
2baba19653aaa6768f3df22f97a80d537ba329f8	discriminative unsupervised learning of structured predictors	structure learning;unsupervised learning;hidden markov model;baum welch;conditional model;semidenite programming	We present a new unsupervised algorithm for training structured predictors that is discriminative, convex, and avoids the use of EM. The idea is to formulate an unsupervised version of structured learning methods, such as maximum margin Markov networks, that can be trained via semidefinite programming. The result is a discriminative training criterion for structured predictors (like hidden Markov models) that remains unsupervised and does not create local minima. To reduce training cost, we reformulate the training procedure to mitigate the dependence on semidefinite programming, and finally propose a heuristic procedure that avoids semidefinite programming entirely. Experimental results show that the convex discriminative procedure can produce better conditional models than conventional Baum-Welch (EM) training.	artificial neural network;baum–welch algorithm;convex function;graphical user interface;heuristic;hidden markov model;markov chain;markov random field;maxima and minima;semi-supervised learning;semiconductor industry;semidefinite programming;structured prediction;teaching method;time complexity;topological graph theory;unsupervised learning;welch's method	Linli Xu;Dana F. Wilkinson;Finnegan Southey;Dale Schuurmans	2006		10.1145/1143844.1143977	unsupervised learning;speech recognition;computer science;baum–welch algorithm;machine learning;pattern recognition;hidden markov model;discriminative model	ML	22.595370591211204	-34.23776771164839	159503
3ebe2fff4d618e07d1302920ed4bf56e564be15e	learning privately with labeled and unlabeled examples		A private learner is an algorithm that given a sample of labeled individual examples outputs a generalizing hypothesis while preserving the privacy of each individual. In 2008, Kasiviswanathan et al. (FOCS 2008) gave a generic construction of private learners, in which the sample complexity is (generally) higher than what is needed for non-private learners. This gap in the sample complexity was then further studied in several followup papers, showing that (at least in some cases) this gap is unavoidable. Moreover, those papers considered ways to overcome the gap, by relaxing either the privacy or the learning guarantees of the learner. We suggest an alternative approach, inspired by the (non-private) models of semi-supervised learning and active-learning, where the focus is on the sample complexity of labeled examples whereas unlabeled examples are of a significantly lower cost. We consider private semi-supervised learners that operate on a random sample, where only a (hopefully small) portion of this sample is labeled. The learners have no control over which of the sample elements are labeled. Our main result is that the labeled sample complexity of private learners is characterized by the VC dimension. We present two generic constructions of private semi-supervised learners. The first construction is of learners where the labeled sample complexity is proportional to the VC dimension of the concept class, however, the unlabeled sample complexity of the algorithm is as big as the representation length of domain elements. Our second construction presents a new technique for decreasing the labeled sample complexity of a given private learner, while roughly maintaining its unlabeled sample complexity. In addition, we show that in some settings the labeled sample complexity does not depend on the privacy parameters of the learner.	algorithm;concept class;privacy;sample complexity;semi-supervised learning;semiconductor industry;supervised learning;symposium on foundations of computer science;vc dimension	Amos Beimel;Kobbi Nissim;Uri Stemmer	2015		10.1137/1.9781611973730.32	theoretical computer science;machine learning;data mining;mathematics	Theory	20.44028049670817	-33.0473040175335	159572
1b86f5c6b3d637a30d253208abc45d486a6728d8	privlogit: efficient privacy-preserving logistic regression by tailoring numerical optimizers		Safeguarding privacy in machine learning is highly desirable, especially in collaborative studies across many organizations. Privacy-preserving distributed machine learning (based on cryptography) is popular to solve the problem. However, existing cryptographic protocols still incur excess computational overhead. Here, we make a novel observation that this is partially due to naive adoption of mainstream numerical optimization (e.g., Newton method) and failing to tailor for secure computing. This work presents a contrasting perspective: customizing numerical optimization specifically for secure settings. We propose a seemingly less-favorable optimization method that can in fact significantly accelerate privacy-preserving logistic regression. Leveraging this new method, we propose two new secure protocols for conducting logistic regression in a privacy-preserving and distributed manner. Extensive theoretical and empirical evaluations prove the competitive performance of our two secure proposals while without compromising accuracy or privacy: with speedup up to 2.3x and 8.1x, respectively, over state-of-the-art; and even faster as data scales up. Such drastic speedup is on top of and in addition to performance improvements from existing (and future) state-of-the-art cryptography. Our work provides a new way towards efficient and practical privacy-preserving logistic regression for large-scale studies which are common for modern science.	cryptographic protocol;cryptography;failure;logistic regression;machine learning;mathematical optimization;newton;newton's method;numerical analysis;offset binary;overhead (computing);privacy;speedup	Wei Xie;Yang Wang;Steven M. Boker;Donald E. Brown	2016	CoRR		computer science;theoretical computer science;machine learning;data mining;distributed computing;statistics	Security	24.477114363813516	-34.19633613619275	159660
75b1e3687f53f4921cb9b77d37358d8c2a7dcddb	black-box optimization with a politician		We propose a new framework for black-box convex optimization which is well-suited for situations where gradient computations are expensive. We derive a new method for this framework which leverages several concepts from convex optimization, from standard first-order methods (e.g. gradient descent or quasi-Newton methods) to analytical centers (i.e. minimizers of selfconcordant barriers). We demonstrate empirically that our new technique compares favorably with state of the art algorithms (such as BFGS).	algorithm;black box;computation;convex optimization;first-order predicate;gradient descent;mathematical optimization;newton;quasi-newton method	Sébastien Bubeck;Yin Tat Lee	2016			mathematical optimization;proximal gradient methods for learning;theoretical computer science;mathematics;proximal gradient methods;mathematical economics	ML	24.19553119582844	-33.54127260609946	160211
7deeb7b082d3def4b0eb15d879ea576b0a2d9b97	feature selection via mathematical programming	nonlinear programming;linear constraint;feature space;objective function;mathematical programming;artificial intelligence;feature selection	The problem of discriminating between two nite point sets in n-dimensional feature space by a separating plane that utilizes as few of the features as possible, is formulated as a mathematical program with a parametric objective function and linear constraints. The step function that appears in the objective function can be approximated by a sigmoid or by a concave exponential on the nonnegative real line, or it can be treated exactly by considering the equivalent linear program with equilibrium constraints (LPEC). Computational tests of these three approaches on publicly available real-world databases have been carried out and compared with an adaptation of the optimal brain damage (OBD) method for reducing neural network complexity. One feature selection algorithm via concave minimization (FSV) reduced cross-validation error on a cancer prognosis database by 35.4% while reducing problem features from 32 to 4. Feature selection is an important problem in machine learning 18, 15, 16, 17, 33]. In its basic form the problem consists of eliminating as many of the features in a given problem as possible, while still carrying out a preassigned task with acceptable accuracy. Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. In the present work, our task is to discriminate between two given sets in an n-dimensional feature space by using as few of the given features as possible. We shall formulate this problem as a mathematical program with a parametric objective function that will attempt to achieve this task by generating a separating plane in a feature space of as small a dimension as possible while minimizing the average distance of misclassiied points to the plane. One of the computational experiments that we carried out on our feature selection procedure showed its eeectiveness, not only in minimizing the number of features selected, but also in quickly recognizing and removing spurious random features that were introduced. Thus, on the Wisconsin Prognosis Breast Cancer WPBC database 36] with a feature space of 32 dimensions and 6 random features added, one of our algorithms FSV (11) immediately removed the 6 random features as well as 28 of the original features resulting in a separating plane in a 4-dimensional reduced feature space. By using tenfold cross-validation 35], separation error in the 4-dimensional space was reduced 35.4% from the corresponding error in the original problem space. (See Section 3 for details.) We …	approximation algorithm;artificial neural network;brain implant;computation;concave function;cross-validation (statistics);database;experiment;feature selection;feature vector;linear programming;long-term predicted excitation coding;loss function;machine learning;mathematical optimization;optimization problem;problem domain;selection algorithm;sigmoid function;time complexity	Paul S. Bradley;Olvi L. Mangasarian;William Nick Street	1998	INFORMS Journal on Computing	10.1287/ijoc.10.2.209	mathematical optimization;feature vector;linear-fractional programming;nonlinear programming;computer science;machine learning;pattern recognition;mathematics;feature selection	ML	21.965325900260193	-37.777352621387955	160708
5ccb1dbbaacff07f5b2b90dcc11b230724a99e3e	stochastic separation theorems	extreme point;fisher’s discriminant;linear separability;machine learning;measure concentration;random set	The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M1-ϑ, where 1>ϑ>0 is a given small constant. Exact values of a,b>0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant ϑ. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.	algorithm;artificial intelligence;iterative method;machine learning;moderate response;numerical analysis;numerical integration	Alexander N. Gorban;Ivan Tyukin	2017	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2017.07.014	combinatorics;discrete mathematics;machine learning;mathematics;statistics	AI	21.12279859362971	-31.734420843302438	161179
6695a1f2442002f7f987d7303b5b55c092f093f3	weight initialization without local minima in deep nonlinear neural networks		In this paper, we propose a new weight initialization method called even initialization for wide and deep nonlinear neural networks with the ReLU activation function. We prove that no poor local minimum exists in the initial loss landscape in the wide and deep nonlinear neural network initialized by the even initialization method that we propose. Specifically, in the initial loss landscape of such a wide and deep ReLU neural network model, the following four statements hold true: 1) the loss function is non-convex and non-concave; 2) every local minimum is a global minimum; 3) every critical point that is not a global minimum is a saddle point; and 4) bad saddle points exist. We also show that the weight values initialized by the even initialization method are contained in those initialized by both of the (often used) standard initialization and He initialization methods.	activation function;concave function;convex set;critical point (network science);essence;loss function;maxima and minima;network model;neural networks;nonlinear system;rectifier (neural networks)	Tohru Nitta	2018	CoRR		mathematical optimization;mathematics;critical point (thermodynamics);artificial neural network;initialization;nonlinear system;maxima and minima;saddle point;activation function	ML	21.679093932531053	-32.26128359776109	161225
33d169f920ec9cc514830fda64f084ad1aff55e4	generalization in clustering with unobserved features	supervised learning;collaborative filtering	We argue that when objects are characterized by many attributes, clustering them on the basis of a relatively small randomsubset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. The scheme is demonstrated for collaborative filtering of users with movies rating as attributes.	cluster analysis;collaborative filtering;supervised learning	Eyal Krupka;Naftali Tishby	2005			correlation clustering;computer science;collaborative filtering;machine learning;pattern recognition;data mining;mathematics;cluster analysis;supervised learning	ML	19.557200073875375	-35.22545632969413	161844
24fa76a8319dd6b52c2ffcd261291634ef2eaecc	discriminative structure and parameter learning for markov logic networks	statistical relational learning;graphical model;relational learning;first order logic	Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods.	benchmark (computing);first-order logic;first-order predicate;graphical model;markov chain;markov logic network;statistical classification;statistical relational learning	Tuyen N. Huynh;Raymond J. Mooney	2008		10.1145/1390156.1390209	natural language processing;statistical relational learning;computer science;machine learning;pattern recognition;first-order logic;mathematics;graphical model	ML	17.40475994391938	-35.39558667294148	162708
1f8392e8bac0287c24ac790b46fc2fc68271ed53	feature construction using theory-guided sampling and randomised search	search space;inductive logic programming;word sense disambiguation;language processing;feature construction;prediction accuracy;feature selection;prediction model;local search;exhaustive search	It has repeatedly been found that very good predictive models can result from using Boolean features constructed by an an Inductive Logic Programming (ILP) system with access to relevant relational information. The process of feature construction by an ILP system, sometimes called “propositionalization”, has been largely done either as a pre-processing step (in which a large set of possibly useful features are constructed first, and then a predictive model is constructed) or by tightly coupling feature construction and model construction (in which a predictive model is constructed with each new feature, and only those that result in a significant improvement in performance are retained). These represent two extremes, similar in spirit to filter and wrapperbased approaches to feature selection. An interesting, third perspective on the problem arises by taking search-based view of feature construction. In this, we conceptually view the task as searching through subsets of all possible features that can be constructed by the ILP system. Clearly an exhaustive search of such a space will usually be intractable. We resort instead to a randomised local search which repeatedly constructs randomly (but non-uniformly) a subset of features and then performs a greedy local search starting from this subset. The number of possible features usually prohibits an enumeration of all local moves. Consequently, the next move in the search-space is guided by the errors made by the model constructed using the current set of features. This can be seen as sampling non-uniformly from the set of all possible local moves, with a view of selecting only those capable of improving performance. The result is a procedure in which a feature subset is initially generated in the pre-processing style, but further alterations are guided actively by actual model predictions. We test this procedure on language processing task of word-sense disambiguation. Good models have previously been obtained for this task using an SVM in conjunction with ILP features constructed in the pre-processing style. Our results show an improvement on these previous results: predictive accuracies are usually higher, and substantially fewer features are needed.	brute-force search;feature selection;feature vector;filter (signal processing);greedy algorithm;inductive logic programming;inductive reasoning;local search (optimization);predictive modelling;preprocessor;randomness;sampling (signal processing);support vector machine;word sense;word-sense disambiguation	Sachindra Joshi;Ganesh Ramakrishnan;Ashwin Srinivasan	2008		10.1007/978-3-540-85928-4_14	computer science;artificial intelligence;local search;machine learning;pattern recognition;brute-force search;data mining;mathematics;predictive modelling;feature selection;feature	AI	17.367884824862397	-36.590134869032944	163573
5ffa728a0f06ba9c2f0d6a5fd9b6a374b02bcc27	a note on reinforcement learning with wasserstein distance regularisation, with applications to multipolicy learning		In this note we describe an application of Wasserstein distance to Reinforcement Learning. The Wasserstein distance in question is between the distribution of mappings of trajectories of a policy into some metric space, and some other fixed distribution (which may, for example, come from another policy). Different policies induce different distributions, so given an underlying metric, the Wasserstein distance quantifies how different policies are. This can be used to learn multiple polices which are different in terms of such Wasserstein distances by using a Wasserstein regulariser. Changing the sign of the regularisation parameter, one can learn a policy for which its trajectory mapping distribution is attracted to a given fixed distribution.	reinforcement learning	Mohammed Abdullah;Aldo Pacchiano;Moez Draief	2018	CoRR		mathematical optimization;machine learning;metric space;artificial intelligence;mathematics;reinforcement learning;trajectory	ML	22.110797325529226	-31.151457643616194	163618
3e2742cbc683e2422e504f248ccf63f1a7983c69	a note on learning algorithms for quadratic assignment with graph neural networks		Many inverse problems are formulated as optimization problems over certain appropriate input distributions. Recently, there has been a growing interest in understanding the computational hardness of these optimization problems, not only in the worst case, but in an average-complexity sense under this same input distribution. In this note, we are interested in studying another aspect of hardness, related to the ability to learn how to solve a problem by simply observing a collection of previously solved instances. These are used to supervise the training of an appropriate predictive model that parametrizes a broad class of algorithms, with the hope that the resulting “algorithm” will provide good accuracy-complexity tradeoffs in the average sense. We illustrate this setup on the Quadratic Assignment Problem, a fundamental problem in Network Science. We observe that data-driven models based on Graph Neural Networks offer intriguingly good performance, even in regimes where standard relaxation based techniques appear to suffer.	algorithm;best, worst and average case;computation;linear programming relaxation;mathematical optimization;network science;neural networks;neural network software;optimization problem;predictive modelling;quadratic assignment problem	Alex Nowak;Soledad Villar;Afonso S. Bandeira;Joan Bruna	2017	CoRR		mathematical optimization;artificial neural network;artificial intelligence;network science;mathematics;machine learning;quadratic equation;quadratic assignment problem;algorithm;inverse problem;graph;optimization problem	ML	22.256435516470926	-32.34787113288945	163661
b47129f074c80515d1dde08ad7654562c73294bc	mixing complexity and its applications to neural networks.		We suggest analyzing neural networks through the prism of space constraints. We observe that most training algorithms applied in practice use bounded memory, which enables us to use a new notion introduced in the study of spacetime tradeoffs that we call mixing complexity. This notion was devised in order to measure the (in)ability to learn using a bounded-memory algorithm. In this paper we describe how we use mixing complexity to obtain new results on what can and cannot be learned using neural networks.	algorithm;data acquisition;maximal set;mixing (mathematics);neural networks;set cover problem;vc dimension	Michal Moshkovitz;Naftali Tishby	2017	CoRR		theoretical computer science;machine learning;artificial intelligence;artificial neural network;bounded function;computer science	ML	19.860194120218136	-31.804264170794504	163871
f31f2a8c156c4c03ced797d658d818ef43533244	classification with gaussians and convex loss	varying gaussian kernel;regularization error;gaussian kernel;sample error;fourier analysis technique;excess misclassification error;convex loss;general convex loss function;sobolev smoothness condition;tikhonov regularization scheme;general loss function;binary classification;reproducing kernel hilbert space	This paper considers binary classification algorithms gene rated from Tikhonov regularization schemes associated with general convex loss functions and v arying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misc lassification error. Allowing varying Gaussian kernels in the algorithms improves learning rates m asured by regularization error and sample error. Special structures of Gaussian kernels enabl us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bo unded regularizing functions achieving polynomial decays of the regularization error under a Sobol ev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kern els. The convexity of the general loss function plays a very important role in our analysis.	algorithm;approximation;binary classification;call of duty: black ops;fourier analysis;hilbert space;loss function;minimal instruction set computer;offset binary;polynomial	Dao-Hong Xiang;Ding-Xuan Zhou	2009	Journal of Machine Learning Research	10.1145/1577069.1755832	regularization perspectives on support vector machines;mathematical optimization;mathematical analysis;discrete mathematics;mathematics;statistics	ML	21.54479027749066	-33.12553890684928	164069
0b2a34d499ece34dee0b58fc3323ab30659ddf00	multiplicative updates for large margin classifiers	quadratic programming;quadratic program;programmation quadratique;methode noyau;intelligence artificielle;classification;machine learning;metodo nucleo;artificial intelligence;kernel method;programacion cuadratica;inteligencia artificial;clasificacion;large margin classifier	Various problems in nonnegative quadratic programming arise in the training of large margin classifiers. We derive multiplicative updates for these problems that converge monotonically to the desired solutions for hard and soft margin classifiers. The updates differ strikingly in form from other multiplicative updates used in machine learning. In this paper, we provide complete proofs of convergence for these updates and extend previous work to incorporate sum and box constraints in addition to nonnegativity. Comments Postprint version. Published in Lecture Notes in Computer Science, Volume 2777, Learning Theory and Kernel Machines, 2003, pages 188-202. Publisher URL: http://dx.doi.org/10.1007/b12006 This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/cis_papers/167 Multiplicative Updates for Large Margin Classifiers Fei Sha, Lawrence K. Saul, and Daniel D. Lee 1 Department of Computer and Information Science 2 Department of Electrical and Systems Engineering University of Pennsylvania 200 South 33rd Street, Philadelphia, PA 19104 {feisha,lsaul,ddlee}@seas.upenn.edu Abstract. Various problems in nonnegative quadratic programming arise in the training of large margin classifiers. We derive multiplicative updates for these problems that converge monotonically to the desired solutions for hard and soft margin classifiers. The updates differ strikingly in form from other multiplicative updates used in machine learning. In this paper, we provide complete proofs of convergence for these updates and extend previous work to incorporate sum and box constraints in addition to nonnegativity. Various problems in nonnegative quadratic programming arise in the training of large margin classifiers. We derive multiplicative updates for these problems that converge monotonically to the desired solutions for hard and soft margin classifiers. The updates differ strikingly in form from other multiplicative updates used in machine learning. In this paper, we provide complete proofs of convergence for these updates and extend previous work to incorporate sum and box constraints in addition to nonnegativity.	converge;information and computer science;information science;kernel method;large margin nearest neighbor;lecture notes in computer science;machine learning;margin classifier;quadratic programming;systems engineering	Fei Sha;Lawrence K. Saul;Daniel D. Lee	2003		10.1007/978-3-540-45167-9_15	kernel method;mathematical optimization;margin;biological classification;computer science;artificial intelligence;machine learning;mathematics;quadratic programming;algorithm;statistics	ML	21.763978007179684	-33.6301173105475	164834
56f0909bab96006c381488c6782676b0ced3de23	an evaluation of an algorithm for inductive learning of bayesian belief networks using simulated data sets	bayesian belief network;inductive learning	Bayesian learning of belief networks (BLN) is a method for automatically constructing belief networks (ENs) from data using search and Bayesian scoring techniques. K2 is a particular instantiation of the method that implements a greedy search strategy. To evaluate the accuracy of K2, we randomly generated a number of BNs and for each of those we simulated data sets. K2 was then used to induce the generating BNs from the simulated data. We examine the performance of the program, and the factors that influence it We also present a simple BN model, developed from our results, which predicts the accuracy of K2, when given various characteristics of the data set .	bayesian network;greedy algorithm;inductive reasoning;procedural generation;universal instantiation	Constantin F. Aliferis;Gregory F. Cooper	1994		10.1016/B978-1-55860-332-5.50006-7	computer science;machine learning;pattern recognition;bayesian network;data mining	ML	17.411695812663414	-35.71658377732692	164913
3752de99c786b84d77b7cffe02147a630bf19043	revising the structure of bayesian network classifiers in the presence of missing data		Abstract Traditionally, algorithms that learn the structure of Bayesian Networks either start from an empty graph and add edges to it bit by bit or add/ remove/reverse edges in a randomly initialized graph. In both cases, the search space is constituted of all the nodes and edges connecting them. Searching within such a vast scope is a hard task, which gets worse in the presence of a dataset with missing values. However, it may be the case that an initial structure already exists and to make it reflect the set of examples it would be required to modify only a subset of the graph. Thus, instead of searching through the entire space of possible connections between the nodes, the problem could be reduced to selecting a subset of the edges and revising them. In this work, we present a novel algorithm for refining the structure of Bayesian networks from incomplete data, named BaBReN (Bayes Ball for Revising Networks). BaBReN has as ultimate goal to improve the inference value of the class variable. Thus, the algorithm tries to solve classification issues by proposing local modifications to the edges connecting the nodes that influence the erroneous classification. The Bayes Ball algorithm – based on the d-separation criteria – is responsible for selecting those relevant nodes. By focusing only on the influential nodes, BaBReN is executed independently of the number of variables in the domain. BaBReN is compared to a constraint-based algorithm (GS), a hybrid one (MMHC) and a score-based one (SEM with GHC), presenting better or competitive results regarding time and classification score.	bayesian network;missing data	Roosevelt Sardinha;Aline Paes;Gerson Zaverucha	2018	Inf. Sci.	10.1016/j.ins.2018.02.011	mathematics;machine learning;artificial intelligence;class variable;missing data;bayesian network;bayes' theorem;inference;null graph;graph	AI	17.373383300973593	-36.60691696947008	165779
3f7f6c2b7f2264b316d767623834379878212fc8	neumann optimizer: a practical optimization algorithm for deep neural networks		Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30%. Our work is practical and easily usable by others – only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.	algorithm;approximation;computation;computational resource;deep learning;hessian;imagenet;mathematical optimization;stochastic optimization	Shankar Krishnan;Ying Xiao;Rif A. Saurous	2017	CoRR		computer science;artificial neural network;machine learning;mathematical optimization;stochastic optimization;deep learning;algorithm;computation;hyperparameter;trade-off;hessian matrix;artificial intelligence	ML	22.964414723726886	-31.92778541328394	165960
b8ecac0c9e613d0e7fe01257a90009b4a0c32ddc	a dual active-set algorithm for regularized monotonic regression	probability theory and statistics;berakningsmatematik;computational mathematics;sannolikhetsteori och statistik	Monotonic (isotonic) regression is a powerful tool used for solving a wide range of important applied problems. One of its features, which poses a limitation on its use in some areas, is that it produces a piecewise constant fitted response. For smoothing the fitted response, we introduce a regularization term in the monotonic regression, formulated as a least distance problem with monotonicity constraints. The resulting smoothed monotonic regression is a convex quadratic optimization problem. We focus on the case, where the set of observations is completely (linearly) ordered. Our smoothed pool-adjacent-violators algorithm is designed for solving the regularized problem. It belongs to the class of dual active-set algorithms. We prove that it converges to the optimal solution in a finite number of iterations that does not exceed the problem size. One of its advantages is that the active set is progressively enlarging by including one or, typically, more constraints per iteration. This resulted in solving large-scale test problems in a few iterations, whereas the size of that problems was prohibitively too large for the conventional quadratic optimization solvers. Although the complexity of our algorithm grows quadratically with the problem size, we found its running time to grow almost linearly in our computational experiments.	active set method;algorithm;analysis of algorithms;approximation algorithm;computation;computational complexity theory;experiment;isotonic regression;iteration;mathematical optimization;maxima and minima;non-monotonic logic;optimization problem;quadratic programming;scalability;shingled magnetic recording;smoothed analysis;smoothing;time complexity;tree (data structure)	Oleg Burdakov;Oleg Sysoev	2017	J. Optimization Theory and Applications	10.1007/s10957-017-1060-0	mathematical optimization;combinatorics;discrete mathematics;numerical analysis;mathematics	AI	24.238892701065588	-33.59097318030421	165987
692b9157732f55bb79cec5f3918ee1381bbe8890	efficient benchmarking of algorithm configuration procedures via model-based surrogates		The optimization of algorithm (hyper-)parameters is crucial for achieving peak performance across a wide range of domains, ranging from deep neural networks to solvers for hard combinatorial problems. The resulting algorithm configuration (AC) problem has attracted much attention from the machine learning community. However, the proper evaluation of new AC procedures is hindered by two key hurdles. First, AC benchmarks are hard to set up. Second and even more significantly, they are computationally expensive: a single run of an AC procedure involves many costly runs of the target algorithm whose performance is to be optimized in a given AC benchmark scenario. One common workaround is to optimize cheap-to-evaluate artificial benchmark functions (e.g., Branin) instead of actual algorithms; however, these have different properties than realistic AC problems. Here, we propose an alternative benchmarking approach that is similarly cheap to evaluate but much closer to the original AC problem: replacing expensive benchmarks by surrogate benchmarks constructed from AC benchmarks. These surrogate benchmarks approximate the response surface corresponding to true target algorithm performance using a regression model, and the original and surrogate benchmark share the same (hyper-)parameter space. In our experiments, we construct and evaluate surrogate benchmarks for hyperparameter optimization as well as for AC problems that involve performance optimization of solvers for hard combinatorial problems, drawing training data from the runs of existing AC procedures. We show that our surrogate benchmarks capture overall important characteristics of the AC scenarios, such as highand low-performing regions, from which they were derived, while being much easier to use and orders of magnitude cheaper to evaluate.	analysis of algorithms;approximation algorithm;artificial neural network;benchmark (computing);deep learning;experiment;machine learning;mathematical optimization;response surface methodology;surrogates;workaround	Katharina Eggensperger;Marius Thomas Lindauer;Holger H. Hoos;Frank Hutter;Kevin Leyton-Brown	2017	CoRR		mathematical optimization;simulation;computer science;machine learning	AI	23.37122298722848	-31.44799240157085	166388
0238b59932ca4f4433a09e9acb92867df2a54039	an incremental model selection algorithm based on cross-validation for finding the architecture of a hidden markov model on hand gesture data sets	forward backward;state space search;model selection;optimisation;tree searching computational complexity hidden markov models learning artificial intelligence optimisation;complexity theory;multiplication operator;hidden markov model;hidden markov models machine learning computer architecture machine learning algorithms data engineering graphical models bayesian methods application software state space methods computational complexity;data mining;incremental model selection algorithm;computer architecture;accuracy;computational modeling;hidden markov models;computational complexity;state space;cross validation hidden markov model model selection;single multiple operators;learning problems;cross validation;learning artificial intelligence;tree searching;breadth first search;computational complexity incremental model selection algorithm cross validation hidden markov model hand gesture data sets multiparameter learning problem state space search forward backward search single multiple operators depth first breadth first search;forward backward search;depth first breadth first search;multiparameter learning problem;exhaustive search;data models;hand gesture data sets	In a multi-parameter learning problem, besides choosing the architecture of the learner, there is the problem of finding the optimal parameters to get maximum performance. When the number of parameters to be tuned increases, it becomes infeasible to try all the parameter sets, hence we need an automatic mechanism to find the optimum parameter setting using computationally feasible algorithms. In this paper, we define the problem of optimizing the architecture of a Hidden Markov Model (HMM) as a state space search and propose the MSUMO (Model Selection Using Multiple Operators) framework that incrementally modifies the structure and checks for improvement using cross-validation. There are five variants that use forward/backward search, single/multiple operators, and depth-first/breadth-first search. On four hand gesture data sets, we compare the performance of MSUMO with the optimal parameter set found by exhaustive search in terms of expected error and computational complexity.	backward induction;breadth-first search;brute-force search;computational complexity theory;cross-validation (statistics);depth-first search;hidden markov model;markov chain;model selection;selection algorithm;state space search	Aydin Ulas;Olcay Taner Yildiz	2009	2009 International Conference on Machine Learning and Applications	10.1109/ICMLA.2009.91	data modeling;multiplication operator;mathematical optimization;breadth-first search;computer science;state space;artificial intelligence;machine learning;pattern recognition;brute-force search;accuracy and precision;computational complexity theory;computational model;hidden markov model;cross-validation;model selection;statistics	ML	17.483855525723023	-35.65187168318762	166663
f8d1baae0c9907897aab4d7848c9de0756a2fef4	gaussian process bandits with adaptive discretization		In this paper, the problem of maximizing a black-box function f : X → R is studied in the Bayesian framework with a Gaussian Process (GP) prior. In particular, a new algorithm for this problem is proposed, and high probability bounds on its simple and cumulative regret are established. The query point selection rule in most existing methods involves an exhaustive search over an increasingly fine sequence of uniform discretizations of X . The proposed algorithm, in contrast, adaptively refines X which leads to a lower computational complexity, particularly when X is a subset of a high dimensional Euclidean space. In addition to the computational gains, sufficient conditions are identified under which the regret bounds of the new algorithm improve upon the known results. Finally an extension of the algorithm to the case of contextual bandits is proposed, and high probability bounds on the contextual regret are presented.		Shubhanshu Shekhar;Tara Javidi	2017	CoRR		euclidean space;statistics;mathematical optimization;bayesian optimization;mathematics;brute-force search;regret;discretization;gaussian process;computational complexity theory;bayesian probability	ML	24.067956815069426	-32.009593120225645	166824
a1e84c25202fb1414312652b58e0b368c505c693	progressive deep neural networks acceleration via soft filter pruning		This paper proposed a Progressive Soft Filter Pruning method (PSFP) to prune the filters of deep Neural Networks which can thus be accelerated in the inference. Specifically, the proposed PSFP method prunes the network progressively and enables the pruned filters to be updated when training the model after pruning. PSFP has three advantages over previous works: 1) Larger model capacity. Updating previously pruned filters provides our approach with larger optimization space than fixing the filters to zero. Therefore, the network trained by our method has a larger model capacity to learn from the training data. 2) Less dependence on the pre-trained model. Large capacity enables our method to train from scratch and prune the model simultaneously. In contrast, previous filter pruning methods should be conducted on the basis of the pre-trained model to guarantee their performance. Empirically, PSFP from scratch outperforms the previous filter pruning methods. 3) Pruning the neural network progressively makes the selection of low-norm filters much more stable, which has a potential to get a better performance. Moreover, our approach has been demonstrated effective for many advanced CNN architectures. Notably, on ILSCRC-2012, our method reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. On ResNet-50, our progressive pruning method have 1.08% top-1 accuracy improvement over the pruning method without progressive pruning.	algorithm;alpha–beta pruning;artificial neural network;deep learning;flops;mathematical optimization;neural networks;particle filter	Yang He;Xuanyi Dong;Guoliang Kang;Yanwei Fu;Yi Yang	2018	CoRR		pruning;acceleration;artificial neural network;pattern recognition;machine learning;training set;inference;computer science;artificial intelligence	AI	17.32419995757679	-32.52111412503526	166909
d22ff6701ae694a5c97c54bc3903a15e240905bb	improving training time of hessian-free optimization for deep neural networks using preconditioning and sampling		Hessian-free training has become a popular parallel second order optimization technique for Deep Neural Network training. This study aims at speeding up Hessian-free training, both by means of decreasing the amount of data used for training, as well as through reduction of the number of Krylov subspace solver iterations used for implicit estimation of the Hessian. In this paper, we develop an L-BFGS based preconditioning scheme that avoids the need to access the Hessian explicitly. Since L-BFGS cannot be regarded as a fixed-point iteration, we further propose the employment of flexible Krylov subspace solvers that retain the desired theoretical convergence guarantees of their conventional counterparts. Second, we propose a new sampling algorithm, which geometrically increases the amount of data utilized for gradient and Krylov subspace iteration calculations. On a 50-hr English Broadcast News task, we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a 300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no loss in WER. These results suggest that even further speed-up is expected, as problems scale and complexity grows.	artificial neural network;deep learning;hessian;mathematical optimization;preconditioner;sampling (signal processing);truncated newton method	Tara N. Sainath;Lior Horesh;Brian Kingsbury;Aleksandr Y. Aravkin;Bhuvana Ramabhadran	2013	CoRR		mathematical optimization;computer science;theoretical computer science;machine learning	ML	23.834056051495892	-33.25662281838577	167188
0b5f20953bffccfb0ed2bb26a92c42e83ab95d78	a new neural implementation of exploratory projection pursuit	analisis componente principal;hebbian learning;fiabilidad;reliability;projection pursuit;funcion densidad probabilidad;probability density function;projection method;tipo dato;data type;fonction densite probabilite;methode projection;principal component analysis;fiabilite;regle apprentissage;metodo proyeccion;analyse composante principale;learning artificial intelligence;reseau neuronal;type donnee;red neuronal;neural network;apprentissage intelligence artificielle	We investigate an extension of the learning rules in a Principal Component Analysis network which has been derived to be optimal for a specific probability density function(pdf). We note that this probability density function is one of a family of pdfs and investigate the learning rules formed in order to be optimal for several members of this family. We show that, whereas previous authors [5] have viewed the single member of the family as an extension of PCA, it is more appropriate to view the whole family of learning rules as methods of performing Exploratory Projection Pursuit(EPP). We explore the performance of our method first in response to an artificial data type, then to a real data set.	exploratory testing	Colin Fyfe;Emilio Corchado	2002		10.1007/3-540-45675-9_77	projection pursuit;probability density function;hebbian theory;data type;computer science;artificial intelligence;machine learning;reliability;mathematics;projection method;artificial neural network;statistics;principal component analysis	NLP	19.886434528678667	-34.19795638128827	168592
9c2d20e686227519a47ecde2d67eb7b47f687f42	enresnet: resnet ensemble via the feynman-kac formalism		We propose a simple yet powerful ResNet ensemble algorithm which consists of two components: First, we modify the base ResNet by adding variance specified Gaussian noise to the output of each original residual mapping. Second, we average over the production of multiple parallel and jointly trained modified ResNets to get the final prediction. Heuristically, these two simple steps give an approximation to the well-known Feynman-Kac formula for representing the solution of a transport equation with viscosity, or a convection-diffusion equation. This simple ensemble algorithm improves neural nets’ generalizability and robustness towards adversarial attack. In particular, for the CIFAR10 benchmark, with the projected gradient descent adversarial training, we show that even an ensemble of two ResNet20 leads to a 5% higher accuracy towards the strongest iterative fast gradient sign attack than the state-of-the-art adversarial defense algorithm.	algorithm;approximation;artificial neural network;benchmark (computing);formal grammar;gradient descent;heuristic;iterative method;multicanonical ensemble;whole earth 'lectronic link	Bao Wang;Binjie Yuan;Zuoqiang Shi;Stanley Osher	2018	CoRR			ML	22.273482388778152	-32.26856355962807	168806
1206745a51e46cd3ca229bd804fb6beddc0a27af	multi-label active learning with conditional bernoulli mixtures		Multi-label learning is an important machine learning task. In multi-label classification tasks, the label space is larger than the traditional single-label classification, and annotations of multi-label instances are typically more time-consuming or expensive to obtain. Thus, it is necessary to take advantage of active learning to solve this problem. In this paper, we present three active learning methods with the conditional Bernoulli mixture (CBM) model for multi-label classification. The first two methods utilize the least confidence and approximated entropy as the selection criteria to pick the most informative instances, respectively. Particularly, an efficient approximated calculation via dynamic programming is developed to compute the approximated entropy. The third method is based on the cluster information from the CBM, which implicitly takes the advantage of the label correlations. Finally, we demonstrate the effectiveness of the proposed methods through experiments on both synthetic and real-world datasets.	bernoulli polynomials;multi-label classification	Junyu Chen;Shiliang Sun;Jing Zhao	2018		10.1007/978-3-319-97304-3_73	artificial intelligence;pattern recognition;computer science;machine learning;active learning;multi-label classification;bernoulli's principle;dynamic programming	Vision	18.87224567899486	-37.67394889031458	169623
89a0ad5ef9725e54ef0b3edf360090c2aebea91d	the one class support vector machine solution path	outlier ranking;solution path;kernel;one class classification;minimum volume set estimation;support vector machines;nonparametric estimation;support vector machines gaussian processes pattern classification;piecewise linear techniques;gaussian processes;level set;density level set estimation support vector machines one class classification solution path;density level set estimation;upper bound;density estimation;support vector machines level set kernel clustering algorithms costs bandwidth static var compensators machine learning upper bound piecewise linear techniques;entire solution;machine learning;bandwidth selection;clustering;gaussian kernel;pattern classification;clustering algorithms;bandwidth;static var compensators;support vector machine;density estimation one class support vector machine gaussian kernel clustering outlier ranking minimum volume set estimation;density level set;one class support vector machine	This paper applies the algorithm of Hastie et al., (2004) to the problem of learning the entire solution path of the one class support vector machine (OC-SVM) as its free parameter ν varies from 0 to 1. The OC-SVM with Gaussian kernel is a nonparametric estimator of a level set of the density governing the observed sample, with the parameter ν implicitly defining the corresponding level. Thus, the path algorithm produces estimates of all level sets and can therefore be applied to a variety of problems requiring estimation of multiple level sets including clustering, outlier ranking, minimum volume set estimation, and density estimation. The algorithm's cost is comparable to the cost of computing the OC-SVM for a single point on the path. We introduce a heuristic for enforced nestedness of the sets in the path, and present a method for kernel bandwidth selection based in minimum integrated volume, a kind of AUC criterion. These methods are illustrated on three datasets.	algorithm;cluster analysis;heuristic;nestedness;support vector machine	Gyemin Lee;Clayton Scott	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366287	support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;cluster analysis	Robotics	24.26847767837688	-37.586515884047785	169961
64611a7a1c5e64e39622d10584cf44c37e437482	online dictionary learning algorithm with periodic updates and its application to image denoising	online learning;dictionary learning;image denoising;sparse representation	We introduce a coe cient update procedure into existing batch and online dictionary learning algorithms. We rst propose an algorithm which is a coe cient updated version of the Method of Optimal Directions (MOD) dictionary learning algorithm (DLA). The MOD algorithm with coe cient updates presents a computationally expensive dictionary learning iteration with high convergence rate. Secondly, we present a periodically coe cient updated version of the online Recursive Least Squares (RLS)-DLA, where the data is used sequentially to gradually improve the learned dictionary. The developed algorithm provides a periodical update improvement over the RLS-DLA, and we call it as the Periodically Updated RLS Estimate (PURE) algorithm for dictionary learning. The performance of the proposed DLAs in synthetic dictionary learning and image denoising settings demonstrates that the coe cient update procedure improves the dictionary learning ability.	algorithm;analysis of algorithms;cobham's thesis;computational complexity theory;data dictionary;dictionary;discrete cosine transform;drive letter assignment;iteration;iterative method;machine learning;mod database;modulo operation;naruto shippuden: clash of ninja revolution 3;noise reduction;rate of convergence;recursion (computer science);recursive least squares filter;simulation;sparse approximation;sparse matrix;synthetic intelligence;tip (unix utility)	Ender M. Eksioglu	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.11.036	speech recognition;k-svd;computer science;machine learning;pattern recognition;sparse approximation;statistics	ML	24.27888955245144	-35.17139697495032	170079
b2978ca5aaa0b41bd7d62c45ed564c86a8ce30c2	stochastic variational inference with gradient linearization		Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.		Tobias Plötz;Anne S. Wannenwetsch;Stefan Roth	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00169	kullback–leibler divergence;machine learning;local optimum;mathematical optimization;artificial intelligence;inference;random field;stochastic process;computer science;expression (mathematics);linearization;convergence (routing)	Vision	24.287957364436252	-32.75216669594794	170253
9263afe22cacaea3bf33a1ac2a93bd9e5905991f	incremental learning-to-learn with statistical guarantees		In learning-to-learn the goal is to infer a learning algorithm that works well on a class of tasks sampled from an unknown metadistribution. In contrast to previous work on batch learning-to-learn, we consider a scenario where tasks are presented sequentially and the algorithm needs to adapt incrementally to improve its performance on future tasks. Key to this setting is for the algorithm to rapidly incorporate new observations into the model as they arrive, without keeping them in memory. We focus on the case where the underlying algorithm is Ridge Regression parametrised by a symmetric positive semidefinite matrix. We propose to learn this matrix by applying a stochastic strategy to minimize the empirical error incurred by Ridge Regression on future tasks sampled from the meta-distribution. We study the statistical properties of the proposed algorithm and prove non-asymptotic bounds on its excess transfer risk, that is, the generalization performance on new tasks from the same meta-distribution. We compare our online learning-to-learn approach with a state-ofthe-art batch method, both theoretically and empirically.	algorithm;computation;data (computing);experiment;feature learning;file spanning;machine learning;metaheuristic;modeling perspective;offset binary;online and offline;singular value decomposition;sparse language;sparse matrix;word lists by frequency	Giulia Denevi;Carlo Ciliberto;Dimitris Stamos;Massimiliano Pontil	2018			positive-definite matrix;artificial intelligence;computer science;incremental learning;mathematical optimization;machine learning;matrix (mathematics);parameterized complexity	ML	24.592889788792274	-32.507348785635365	170413
a7ee15075ff6d9b93accd287ec20e1f0d7cfa722	ensemble pruning based on objection maximization with a general distributed framework		Ensemble pruning, selecting a subset of individual learners from an original ensemble, alleviates the deficiencies of ensemble learning on the cost of time and space. Accuracy and diversity serve as two crucial factors while they usually conflict with each other. To balance both of them, we formalize the ensemble pruning problem as an objection maximization problem based on information entropy. Then we propose an ensemble pruning method including a centralized version and a distributed version, in which the latter is to speed up the former’s execution. At last, we extract a general distributed framework for ensemble pruning, which can be widely suitable for most of existing ensemble pruning methods and achieve less time consuming without much accuracy decline. Experimental results validate the efficiency of our framework and methods, particularly with regard to a remarkable improvement of the execution speed, accompanied by gratifying accuracy performance.	alpha–beta pruning;centralized computing;ensemble learning;entropy (information theory);entropy maximization;expectation–maximization algorithm	Yijun Bian;Yijun Wang;Yaqiang Yao;Bingbing Jiang	2018	CoRR		spacetime;machine learning;entropy (information theory);pruning;speedup;artificial intelligence;mathematics;ensemble learning;maximization	AI	17.77591997430806	-37.300950069418825	170725
07318220d7fe8de7a3e4ef9efc0dc9429c4a43d2	sparse kernel svms via cutting-plane training	cutting-plane training;added flexibility;training set;good quality;conventional methods scale quadratically;sparse kernel svms;empirical evaluation;arbitrary basis vector;kernel svms tractable;training svms;improved sparsity;large training set;cutting plane;support vector machine;kernel methods;sparse data;support vector;support vector machines;basis pursuit	We explore an algorithm for training SVMs with Kernels that can represent the learned rule using arbitrary basis vectors, not just the support vectors (SVs) from the training set. This results in two benefits. First, the added flexibility makes it possible to find sparser solutions of good quality, substantially speeding-up prediction. Second, the improved sparsity can also make training of Kernel SVMs more efficient, especially for high-dimensional and sparse data (e.g. text classification). This has the potential to make training of Kernel SVMs tractable for large training sets, where conventional methods scale quadratically due to the linear growth of the number of SVs. In addition to a theoretical analysis of the algorithm, we also present an empirical evaluation.	algorithm;basis (linear algebra);cobham's thesis;cutting-plane method;document classification;kernel (operating system);linear function;linux;mathematical optimization;sparse language;sparse matrix;test set	Thorsten Joachims;Chun-Nam John Yu	2009	Machine Learning	10.1007/s10994-009-5126-6	support vector machine;content analysis;computer science;artificial intelligence;machine learning;mathematics;algorithm	ML	20.823030197173065	-37.89990196514618	170889
fb6925f4dff73b0e7f261cc695f1571a7977449e	scalable semisupervised gmm for big data quality prediction in multimode processes		In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.		Le Yao;Zhiqiang Ge	2019	IEEE Transactions on Industrial Electronics	10.1109/TIE.2018.2856200		ML	19.28951712926366	-37.657093977644436	170987
e49cc4a42d09f8e0f6326ea4d078ae8b1194f985	faster gaussian summation: theory and experiment	series expansion;machine learning;bandwidth selection;kernel density estimate;gaussian kernel;error bound;data structure	We provide faster algorithms for the problem of Gaussian summation, which occurs in many machine learning methods. We develop two new extensions an O(D) Taylor expansion for the Gaussian kernel with rigorous error bounds and a new error control scheme integrating any arbitrary approximation method within the best discretealgorithmic framework using adaptive hierarchical data structures. We rigorously evaluate these techniques empirically in the context of optimal bandwidth selection in kernel density estimation, revealing the strengths and weaknesses of current state-of-the-art approaches for the first time. Our results demonstrate that the new error control scheme yields improved performance, whereas the series expansion approach is only effective in low dimensions (five or less). 1 Fast Gaussian Summation Kernel summations occur ubiquitously in both old and new machine learning algorithms, including kernel density estimation, kernel regression, radial basis function networks, spectral clustering, and kernel PCA (Gray & Moore, 2001; de Freitas et al., 2006). This paper will focus on the most common form	algorithm;approximation;cluster analysis;data structure;error detection and correction;gaussian blur;hierarchical database model;kernel density estimation;kernel principal component analysis;machine learning;radial (radio);radial basis function;series expansion;spectral clustering	Dongryeol Lee;Alexander G. Gray	2006	CoRR		kernel density estimation;kernel method;mathematical optimization;kernel embedding of distributions;radial basis function kernel;data structure;series expansion;computer science;machine learning;mathematics;gaussian function;variable kernel density estimation;statistics	ML	24.170539887670273	-36.159607032030465	171137
41324c30235bb1388ecdae5c276166d839a5f0e3	new incremental learning algorithm for semi-supervised support vector machine		Semi-supervised learning is especially important in data mining applications because it can make use of plentiful unlabeled data to train the high-quality learning models. Semi-Supervised Support Vector Machine (S3VM) is a powerful semi-supervised learning model. However, the high computational cost and non-convexity severely impede the S3VM method in large-scale applications. Although several learning algorithms were proposed for S3VM, scaling up S3VM is still an open problem. To address this challenging problem, in this paper, we propose a new incremental learning algorithm to scale up S3VM (IL-S3VM) based on the path following technique in the framework of Difference of Convex (DC) programming. The traditional DC programming based algorithms need multiple outer loops and are not suitable for incremental learning, and traditional path following algorithms are limited to convex problems. Our new IL-S3VM algorithm based on the path-following technique can directly update the solution of S3VM to converge to a local minimum within one outer loop so that the efficient incremental learning can be achieved. More importantly, we provide the finite convergence analysis for our new algorithm. To the best of our knowledge, our new IL-S3VM algorithm is the first efficient path following algorithm for a non-convex problem (i.e., S3VM) with local minimum convergence guarantee. Experimental results on a variety of benchmark datasets not only confirm the finite convergence of IL-S3VM, but also show a huge reduction of computational time compared with existing batch and incremental learning algorithms, while retaining the similar generalization performance.	algorithm;algorithmic efficiency;algorithmic learning theory;benchmark (computing);computation;converge;convex optimization;data mining;image scaling;machine learning;maxima and minima;semi-supervised learning;semiconductor industry;supervised learning;support vector machine;time complexity	Bin Gu;Xiao-Tong Yuan;Songcan Chen;Heng Huang	2018		10.1145/3219819.3220092	support vector machine;incremental learning;artificial intelligence;computer science;machine learning;open problem;scale-up;scaling;algorithm;convergence (routing)	ML	20.74776346224441	-37.891464125094195	171311
e5d15693ec4572634d6ac02a8568d787687d67d3	an accelerated learning algorithm of gaussian mixture processes	gaussian mixture processes;gaussian mixture;learning algorithm;gaussian processes;gaussian processes expectation maximisation algorithm;square root update method gaussian mixture processes parametric learning;iterative method accelerated parametric learning algorithm gaussian mixture process square root update method log likelihood function gaussian mixture em algorithm;square root update method;parametric learning;em algorithm;likelihood function;expectation maximisation algorithm	This paper presents an accelerated algorithm of parametric learning, in Gaussian mixture processes, which employs Square-root Update method and erases the constraints of the log-likelihood function by utilizing auxiliary parameters embedding the constraints. The algorithm enables us to improve poor convergence, avoids us unstable implementation and removes unnecessary iterations in Gaussian mixture EM algorithm. Our algorithm also allows inexact searches for finding the parameters to maximize the log-likelihood function during the computation, and enables us to implement much efficiently.	computation;control theory;expectation–maximization algorithm;iteration	Isamu Shioya;Takao Miura	2012	2012 Second International Conference on Digital Information and Communication Technology and it's Applications (DICTAP)	10.1109/DICTAP.2012.6215412	gaussian random field;mathematical optimization;expectation–maximization algorithm;machine learning;mixture model;gaussian process;mathematics;likelihood function;gaussian filter;gaussian function;statistics	Robotics	24.098184305457874	-34.84827082644442	171330
2159ead5352ef3d4add48d113ddc90ded19813bc	the minimum information principle for discriminative learning	exponential model;iterative algorithm;machine learning;discrimination learning;maximum entropy model;minimum mutual information;information theoretic;maximum entropy	Exponential models of distributions are widely used in machine learning for classification and modelling. It is well known that they can be interpreted as maximum entropy models under empirical expectation constraints. In this work, we argue that for classification tasks, mutual information is a more suitable information theoretic measure to be optimized. We show how the principle of minimum mutual information generalizes that of maximum entropy, and provides a comprehensive framework for building discriminative classifiers. A game theoretic interpretation of our approach is then given, and several generalization bounds provided. We present iterative algorithms for solving the minimum information problem and its convex dual, and demonstrate their performance on various classification tasks. The results show that minimum information classifiers outperform the corresponding maximum entropy models.	algorithm;convex optimization;database;discriminative model;entropy (information theory);game theory;iterative method;machine learning;mutual information;principle of maximum entropy	Amir Globerson;Naftali Tishby	2004			variation of information;mathematical optimization;maximum-entropy markov model;joint entropy;information diagram;binary entropy function;transfer entropy;maximum entropy probability distribution;computer science;principle of maximum entropy;machine learning;pattern recognition;mathematics;mutual information;maximum entropy spectral estimation;interaction information;conditional entropy;statistics	ML	21.554202885800034	-35.733083360642986	171376
bed6bd867091fbf2f8dd7a01289d7679a41ef63d	fast amortized inference and learning in log-linear models with randomly perturbed nearest neighbor search		Inference in log-linear models scales linearly in the size of output space in the worst-case. This is often a bottleneck in natural language processing and computer vision tasks when the output space is feasibly enumerable but very large. We propose a method to perform inference in log-linear models with sublinear amortized cost. Our idea hinges on using Gumbel random variable perturbations and a pre-computed Maximum Inner Product Search data structure to access the most-likely elements in sublinear amortized time. Our method yields provable runtime and accuracy guarantees. Further, we present empirical experiments on ImageNet and Word Embeddings showing significant speedups for sampling, inference, and learning in log-linear models.	amortized analysis;best, worst and average case;black box;computer vision;experiment;feature vector;imagenet;linear model;log-linear model;natural language processing;nearest neighbor search;overhead (computing);partition function (mathematics);precomputation;provable security;randomness;sampling (signal processing);search data structure;sparse matrix;word embedding	Stephen Mussmann;Daniel Levy;Stefano Ermon	2017	CoRR		artificial intelligence;mathematical optimization;machine learning;mathematics;best bin first;search data structure;inference;random variable;gumbel distribution;sublinear function;amortized analysis;nearest neighbor search;pattern recognition	ML	23.158816189057173	-35.85309645218388	171740
d5f3edaa5d2a60a799969679021b3c914aa792de	bias vs. variance decomposition for regression and classification	empirical study;learning algorithm;supervised learning;machine learning;mean square error;statistics;variance decomposition	  In this chapter, the important concepts of bias and variance are introduced. After an intuitive introduction to the bias/variance  tradeoff, we discuss the bias/variance decompositions of the mean square error (in the context of regression problems) and  of the mean misclassification error (in the context of classification problems). Then, we carry out a small empirical study  providing some insight about how the parameters of a learning algorithm influence bias and variance.    		Pierre Geurts	2005		10.1007/978-0-387-09823-4_37	empirical risk minimization;computer science;machine learning;pattern recognition;ensemble learning;supervised learning;variance decomposition of forecast errors;one-way analysis of variance;statistics;generalization error	ML	19.956828187839363	-35.08410277030631	172012
e5248939d889a45faa74d11c1a4627ecdd032401	minimum class variance support vector ordinal regression		The support vector ordinal regression (SVOR) method is derived from support vector machine and developed to tackle the ordinal regression problems. However, it ignores the distribution characteristics of the data. In this paper, we propose a novel method to handle the ordinal regression problems.Thismethod is referred to asminimumclass variance support vector ordinal regression (MCVSVOR). In contrast with SVOR, MCVSVOR explicitly takes into account the distribution of the categories and achieves better generalization performance. Moreover, the problem of MCVSVOR can be transformed into one of SVOR. Thus, the existing software of SVOR can be used to solve the problem ofMCVSVOR. In the paper, we first discuss the linear case of MCVSVOR and then develop the nonlinear MCVSVOR through using the kernelization trick. The comprehensive experiment results show that the proposed method is effective and can achieve better generalization performance in contrast with SVOR.	kernelization;mathematical optimization;nonlinear system;optimization problem;ordinal data;ordinal regression;support vector machine	Xiaoming Wang;Jinrong Hu;Zengxi Huang	2017	Int. J. Machine Learning & Cybernetics	10.1007/s13042-016-0582-3	ordinal regression;econometrics;machine learning;mathematics;ordinal optimization;ordinal data;statistics	AI	21.54141645028386	-36.20805370353251	172065
0f0606417ab30b0aa5414cfa57bc469b97c1b6be	learning in the limit with adversarial disturbances	generalization error;decision maker	We study distribution-dependent, data-dependent, learning in the limit with adversarial disturbance. We consider an optimization-based approach to learning binary classifiers from data under worst-case assumptions on the disturbance. The learning process is modeled as a decision-maker who seeks to minimize generalization error, given access only to possibly maliciously corrupted data. Two models for the nature of the disturbance are considered: disturbance in the labels of a certain fraction of the data, and disturbance that also affects the position of the data points. We provide distributiondependent bounds on the amount of error as a function of the noise level for the two models, and describe the optimal strategy of the decision-maker, as well as the worst-case disturbance.	best, worst and average case;binary classification;data dependency;data point;generalization error;language identification in the limit;mathematical optimization;noise (electronics)	Constantine Caramanis;Shie Mannor	2008			machine learning;artificial intelligence;data point;adversarial system;generalization error;binary number;mathematics;pattern recognition	ML	19.125496306137684	-33.710119791294225	172938
3ff06bc6ad54b05ff121b3f2f8148971c23ea55c	deep learning games		We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods. For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game. We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem. Based on these connections we investigate alternative learning methods, and find that regret matching can achieve competitive training performance while producing sparser models than current deep learning strategies.	algorithm;artificial neural network;deep learning;directed acyclic graph;heuristic (computer science);nash equilibrium;recurrent neural network;regret (decision theory);sparse matrix;supervised learning;turing completeness	Dale Schuurmans;Martin Zinkevich	2016			semi-supervised learning;mathematical optimization;artificial intelligence;machine learning;mathematics;normal-form game	ML	21.049879003204808	-32.82767355550362	173511
3e4df90f73495086299e3a0736837174a429bde7	interpretation of prediction models using the input gradient		"""State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a """" black box """". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing."""	algorithm;artificial neural network;black box;convolutional neural network;exemplification;gradient;layer (electronics);linear approximation;machine learning;natural language processing;predictive modelling	Yotam Hechtlinger	2016	CoRR		computer science;artificial intelligence;machine learning;data mining;statistics	ML	21.765374859479056	-31.628419151619553	174167
7e259f2582c32755b968cb725c5c04a00e481417	practical network blocks design with q-learning		Convolutional neural network provides an end-to-end solution to train many computer vision tasks and has gained great successes. However, the design of network architectures usually relies heavily on expert knowledge and is hand-crafted. In this paper, we provide a solution to automatically and efficiently design high performance network architectures. To reduce the search space of network design, we focus on constructing network blocks, which can be stacked to generate the whole network. Blocks are generated through an agent, which is trained with Q-learning to maximize the expected accuracy of the searching blocks on the learning task. Distributed asynchronous framework and early stop strategy are used to accelerate the training process. Our experimental results demonstrate that the network architectures designed by our approach perform competitively compared with handcrafted state-of-the-art networks. We trained the Q-learning on CIFAR-100, and evaluated on CIFAR10 and ImageNet, the designed block structure achieved 3.60% error on CIFAR10 and competitive result on ImageNet. The Q-learning process can be efficiently trained only on 32 GPUs in 3 days.	artificial neural network;computer vision;convolution;convolutional neural network;end-to-end principle;experiment;flops;flow network;graphics processing unit;imagenet;network architecture;network planning and design;q-learning;quantum neural network	Zhao Zhong;Junjie Yan;Cheng-Lin Liu	2017	CoRR		network architecture;convolutional neural network;q-learning;machine learning;artificial intelligence;asynchronous communication;network planning and design;computer science	ML	17.39524575342765	-32.54697192984376	175357
bd969257f4e36f80664d0de9ae026da9772c06b6	introduction to the special issue on colt 2006		This special issue of Machine Learning is dedicated to the Nineteenth Annual Conference on Learning Theory (COLT 2006) held at Pittsburgh, PA, USA, June 22–25, 2006. The papers in this issue were selected from those presented at that conference, and the authors invited to submit completed versions of their work. Once received, these papers underwent the usual refereeing process of Machine Learning. The field of Learning Theory provides a mathematical foundation for the study of machine learning. The analysis proceeds in a formal model so as to provide measures for the performance of a learning algorithm or for the inherent hardness of a given learning problem. The variety of applications for algorithms that learn is reflected in the variety of formal learning models. For instance, we can distinguish between a passive model of “learning from examples” and active models of learning where the algorithm has more control over the information that is gathered. As for learning from examples, a further decision is whether or not we impose statistical assumptions on the sequence of examples. Furthermore, we find different success criteria in different models (like “approximate learning” versus “exact learning”). The papers in this special issue offer a broad view on the current research in the field including studies on several learning models (such as Teaching, Statistical Query Learning,	approximation algorithm;colt;formal language;machine learning	Avrim Blum;Gábor Lugosi;Hans Ulrich Simon	2007	Machine Learning	10.1007/s10994-007-5027-5	mathematics;artificial intelligence;machine learning;formal learning;learning theory;statistical assumption	ML	20.154679025050342	-35.542605940262554	175527
50257e37195a5308610dbbecf25cdbbe1eef8827	kernel-based representation policy iteration with applications to optimal path tracking of wheeled mobile robots	approximation policy iteration;kernel methods;path tracking;reinforcement learning;wheeled mobile robots	How to improve the generalization and approximation abil- ity in reinforcement learning (RL) is still an open issue in recent years. Aiming at this problem, this paper presents a novel kernel-based rep- resentation policy iteration (KRPI) method for reinforcement learning in optimal path tracking of mobile robots. In the proposed method, the kernel trick is employed to map the original state space into a high- dimensional feature space and the Laplacian operator in the feature space is obtained by minimizing an objective function of optimal embedding. In the experiments, the KRPI-based PD controller was applied to the optimal path tracking problem of a wheeled mobile robot. It is demon- strated that the proposed method can obtain better near-optimal control policies than previous approaches.	iteration;kernel (operating system);markov decision process;robot	Zhenhua Huang;Xin Xu;Lei Ye;Lei Zuo	2013		10.1007/978-3-642-42057-3_91	mathematical optimization;simulation;machine learning;mathematics	Robotics	22.670862011191918	-36.93513216335811	175892
301bc0a33a7a2fcaf5401e14e741178bc561d836	fast stochastic methods for nonsmooth nonconvex optimization		We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonconvex part is smooth and the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we show provably faster convergence than batch proximal gradient descent. Finally, we prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, that subsumes several recent works. This paper builds upon our recent series of papers on fast stochastic methods for smooth nonconvex optimization [22, 23], with a novel analysis for nonconvex and nonsmooth functions.	algorithm;converge;convex function;gradient descent;gradient method;mathematical optimization;optimizing compiler;proximal gradient methods for learning;rate of convergence;stationary process	Sashank J. Reddi;Suvrit Sra;Barnabás Póczos;Alexander J. Smola	2016	CoRR		mathematical optimization;mathematical analysis;mathematics;mathematical economics	ML	23.826123425838922	-33.299663954890605	175912
93d227499aabd76f478af29063862ef4dbaa89a7	fsmrank: feature selection algorithm for learning to rank	learning to rank accelerated gradient algorithm feature selection generalization bound;learning to rank algorithm fsmrank feature selection algorithm joint convex optimization formulation ranking errors flexible framework accelerated gradient algorithm fast convergence rate generalization bound optimization problem rademacher complexity public letor benchmark dataset;convex programming;information retrieval;algorithm theory;computational complexity;learning artificial intelligence algorithm theory computational complexity convex programming information retrieval;optimization training prediction algorithms vectors feature extraction machine learning algorithms joints;learning artificial intelligence	In recent years, there has been growing interest in learning to rank. The introduction of feature selection into different learning problems has been proven effective. These facts motivate us to investigate the problem of feature selection for learning to rank. We propose a joint convex optimization formulation which minimizes ranking errors while simultaneously conducting feature selection. This optimization formulation provides a flexible framework in which we can easily incorporate various importance measures and similarity measures of the features. To solve this optimization problem, we use the Nesterov's approach to derive an accelerated gradient algorithm with a fast convergence rate O(1/T2). We further develop a generalization bound for the proposed optimization problem using the Rademacher complexities. Extensive experimental evaluations are conducted on the public LETOR benchmark datasets. The results demonstrate that the proposed method shows: 1) significant ranking performance gain compared to several feature selection baselines for ranking, and 2) very competitive performance compared to several state-of-the-art learning-to-rank algorithms.	behavior;benchmark (computing);convergence (action);convex optimization;evaluation;feature selection;generalization (psychology);genetic selection;gradient;learning to rank;mathematical optimization;optimization problem;rademacher complexity;rate of convergence;selection algorithm	Hanjiang Lai;Yan Pan;Yong Tang	2013	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2247628	mathematical optimization;convex optimization;computer science;machine learning;pattern recognition;mathematics;computational complexity theory;ranking svm;learning to rank;population-based incremental learning;generalization error	ML	22.553893372866874	-37.80500057496302	176365
7e23aeaabb3b58d2a445ae40da2cd9e8f8560753	restricted boltzmann machines with gaussian visible units guided by pairwise constraints		Restricted Boltzmann machines (RBMs) and their variants are usually trained by contrastive divergence (CD) learning, but the training procedure is an unsupervised learning approach, without any guidances of the background knowledge. To enhance the expression ability of traditional RBMs, in this paper, we propose pairwise constraints (PCs) RBM with Gaussian visible units (pcGRBM) model, in which the learning procedure is guided by PCs and the process of encoding is conducted under these guidances. The PCs are encoded in hidden layer features of pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In order to deal with real-valued data, the binary visible units are replaced by linear units with Gaussian noise in the pcGRBM model. In the learning process of pcGRBM, the PCs are iterated transitions between visible and hidden units during CD learning procedure. Then, the proposed model is inferred by approximative gradient descent method and the corresponding learning algorithm is designed. In order to compare the availability of pcGRBM and traditional RBMs with Gaussian visible units, the features of the pcGRBM and RBMs hidden layer are used as input ``data'' for K-means, spectral clustering (SP) and affinity propagation (AP) algorithms, respectively. We also use tenfold cross-validation strategy to train and test pcGRBM model to obtain more meaningful results with PCs which are derived from incremental sampling procedures. A thorough experimental evaluation is performed with 12 image datasets of Microsoft Research Asia Multimedia. The experimental results show that the clustering performance of K-means, SP, and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering tasks shows better performance than some semi-supervised clustering algorithms.	affinity propagation;algorithm;approximation algorithm;cluster analysis;cross reactions;cross-validation (statistics);dimensions;flock;futures studies;gradient descent;increment;inference;iteration;k-means clustering;microsoft research;multilayer perceptron;normal statistical distribution;restricted boltzmann machine;sampling (signal processing);semi-supervised learning;semiconductor industry;software propagation;spectral clustering;unsupervised learning;statistical cluster	Jielei Chu;Hongjun Wang;Hua Meng;Peng Jin;Tianrui Li	2018	IEEE transactions on cybernetics	10.1109/TCYB.2018.2863601	speech recognition;computer science;machine learning;pattern recognition;statistics	AI	23.476715502614514	-31.08183783099569	176395
3bedd45617f8d616d107552103bfc57ed69f6b62	an accelerated gradient method for trace norm minimization	gradient method;matrix completion;machine learning;loss function;multi task learning;semideflnite programming	We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smooth nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O(1/√k), where k is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as O(1/k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O(1/k2) for smooth problems. Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.	algorithm;analysis of algorithms;black box;computer multitasking;first-order reduction;gradient method;iteration;loss function;machine learning;multi-task learning;rate of convergence;semidefinite programming;the matrix	Shuiwang Ji;Jieping Ye	2009		10.1145/1553374.1553434	multi-task learning;mathematical optimization;computer science;gradient method;machine learning;mathematics;algorithm;loss function	ML	24.173096859110952	-33.662026338402114	176669
416917b07f70a9b69be556bc0ca3ac20fe127353	second order online collaborative filtering	drntu engineering computer science and engineering;conference paper	Collaborative Filtering (CF) is one of the most successful learning techniques in building real-world recommender systems. Traditional CF algorithms are often based on batch machine learning methods which suffer from several critical drawbacks, e.g., extremely expensive model retraining cost whenever new samples arrive, unable to capture the latest change of user preferences over time, and high cost and slow reaction to new users or products extension. Such limitations make batch learning based CF methods unsuitable for real-world online applications where data often arrives sequentially and user preferences may change dynamically and rapidly. To address these limitations, we investigate online collaborative filtering techniques for building live recommender systems where the CF model can evolve on-the-fly over time. Unlike the regular first order CF algorithms (e.g., online gradient descent for CF) that converge slowly, in this paper, we present a new framework of second order online collaborative filtering, i.e., Confidence Weighted Online Collaborative Filtering (CWOCF), which applies the second order online optimization methodology to tackle the online collaborative filtering task. We conduct extensive experiments on several large-scale datasets, in which the encouraging results demonstrate that the proposed algorithms obtain significantly lower errors (both RMSE and MAE) than the state-of-the-art first order CF algorithms when receiving the same amount of training data in the online learning process.	algorithm;collaborative filtering;converge;experiment;gradient descent;machine learning;mathematical optimization;online optimization;recommender system;sparse matrix;user (computing);world online	Jing Lu;Steven C. H. Hoi;Jialei Wang	2013			simulation;computer science;machine learning;data mining;multimedia	Web+IR	23.732140393844393	-34.849562265445435	176764
255f055d8c678b53a8a0c9c73f5f6480b125bdb5	constrained maximum mutual information dimensionality reduction for language identification		In this paper we propose Constrained Maximum Mutual Information dimensionality reduction (CMMI), an informationtheoretic based dimensionality reduction technique. CMMI tries to maximize the mutual information between the class labels and the projected (lower dimensional) features, optimized via gradient ascent. Supervised and semi-supervised CMMI are introduced and compared with a state of the art dimensionality reduction technique (Minimum/Maximum Rényi’s Mutual Information using the Stochastic Information Gradient; MRMISIG) for a language identification (LID) task using CallFriend corpus, with favorable results. CMMI also deals with higher dimensional data more gracefully than MRMI-SIG, permitting application to datasets for which MRMI-SIG is computationally prohibitive.	capability maturity model integration;dimensionality reduction;gradient descent;language identification;mutual information;semi-supervised learning;semiconductor industry;times ascent	Shuai Huang;Glen A. Coppersmith;Damianos Karakos	2012			pattern recognition;language identification;dimensionality reduction;mutual information;computer science;machine learning;artificial intelligence	AI	22.217389047614585	-36.872468881351914	178234
5221e620f79d14573c36dafa96169d897c9d1ed9	serboost: semi-supervised boosting with expectation regularization	cost function;prior knowledge;semi supervised learning;large scale;classification accuracy	The application of semi-supervised learning algorithms to large scale vision problems suffers from the bad scaling behavior of most methods. Based on the Expectation Regularization principle, in this paper we propose a novel semi-supervised boosting method, called SERBoost that can be applied to large scale vision problems and its complexity is dominated by the base learners. The algorithm provides a margin regularizer for the boosting cost function and shows a principled way of utilizing prior knowledge. We demonstrate the performance of SERBoost on the Pascal VOC2006 set and compare it to other supervised and semisupervised methods, where SERBoost shows improvements both in terms of classification accuracy and computational speed.	binary classification;expectation–maximization algorithm;image scaling;loss function;machine learning;matrix regularization;semi-supervised learning;semiconductor industry;supervised learning	Amir Saffari;Helmut Grabner;Horst Bischof	2008		10.1007/978-3-540-88690-7_44	semi-supervised learning;computer science;machine learning;pattern recognition;data mining;mathematics;gradient boosting;loss function	Vision	21.792755146856365	-35.471189363468156	179060
d82eed387fef5b675f03972d72a774da55a3bf55	kernel-based actor-critic approach with applications	reinforcement learning;kernel methods;sliding windows;least squares;actor critic algorithm	Recently, actor-critic methods have drawn significant interests in the area of reinforcement learning, and several algorithms have been studied along the line of the actor-critic strategy. In this paper, we consider a new type of actor-critic algorithms employing the kernel methods, which have recently shown to be very effective tools in the various fields of machine learning, and have performed investigations on combining the actor-critic strategy together with kernel methods. More specifically, this paper studies actor-critic algorithms utilizing the kernel-based least-squares estimation and policy gradient, and in its critic’s part, the study uses a sliding-window-based kernel least-squares method, which leads to a fast and efficient value-function-estimation in a nonparametric setting. The applicability of the considered algorithms is illustrated via a robot locomotion problem and a tunnel ventilation control problem.	algorithm;gradient;kernel (operating system);kernel method;least squares;machine learning;reinforcement learning;robot locomotion	Baeksuk Chu;Keun-Woo Jung;Jooyoung Park	2011	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2011.11.4.267	kernel method;mathematical optimization;kernel embedding of distributions;radial basis function kernel;computer science;artificial intelligence;machine learning;graph kernel;tree kernel;polynomial kernel	ML	22.229461371724454	-34.93773016846273	179074
909e0b74499e43b8eaecfac99bbadc2b60fc8e6f	sparse passive-aggressive learning for bounded online kernel methods		One critical deficiency of traditional online kernel learning methods is their unbounded and growing number of support vectors in the online learning process, making them inefficient and non-scalable for large-scale applications. Recent studies on scalable online kernel learning have attempted to overcome this shortcoming, e.g., by imposing a constant budget on the number of support vectors. Although they attempt to bound the number of support vectors at each online learning iteration, most of them fail to bound the number of support vectors for the final output hypothesis, which is often obtained by averaging the series of hypotheses over all the iterations. In this article, we propose a novel framework for bounded online kernel methods, named “Sparse Passive-Aggressive (SPA)” learning, which is able to yield a final output kernel-based hypothesis with a bounded number of support vectors. Unlike the common budget maintenance strategy used by many existing budget online kernel learning approaches, the idea of our approach is to attain the bounded number of support vectors using an efficient stochastic sampling strategy that samples an incoming training example as a new support vector with a probability proportional to its loss suffered. We theoretically prove that SPA achieves an optimal mistake bound in expectation, and we empirically show that it outperforms various budget online kernel learning algorithms. Finally, in addition to general online kernel learning tasks, we also apply SPA to derive bounded online multiple-kernel learning algorithms, which can significantly improve the scalability of traditional Online Multiple-Kernel Classification (OMKC) algorithms while achieving satisfactory learning accuracy as compared with the existing unbounded OMKC algorithms.	algorithm;angular defect;iteration;kernel (operating system);kernel method;machine learning;sampling (signal processing);scalability;sparse;sparse matrix	Jing Lu;Doyen Sahoo;Peilin Zhao;Steven C. H. Hoi	2018	ACM TIST	10.1145/3156684	machine learning;kernel (linear algebra);support vector machine;artificial intelligence;scalability;kernel method;sampling (statistics);bounded function;computer science	ML	22.044284968127254	-35.0484738709443	179185
7bbf5e9bff8d56cb01c74427e5bc7a2fdeedf394	efficient learning algorithm using compact data representation in neural networks		Convolutional neural networks have dramatically improved the prediction accuracy in a wide range of applications, such as vision recognition and natural language processing. However the recent neural networks often require several hundred megabytes of memory for the network parameters, which in turn consume a large amount of energy during computation. In order to achieve better energy efficiency, this work investigates the effects of compact data representation on memory saving for network parameters in artificial neural networks while maintaining comparable accuracy in both training and inference phases. We have studied the dependence of prediction accuracy on the total number of bits for fixed point data representation, using a proper range for synaptic weights. We have also proposed a dictionary based architecture that utilizes a limited number of floating-point entries for all the synaptic weights, with proper initialization and scaling factors to minimize the approximation error. Our experiments using a 5-layer convolutional neural network on Cifar-10 dataset have shown that 8 bits are enough for bit width reduction and dictionary based architecture to achieve 96.0% and 96.5% relative accuracy respectively, compared to the conventional 32-bit floating point.		Masaya Kibune;Michael G. Lee	2017		10.1007/978-3-319-70096-0_33	convolutional neural network;machine learning;competitive learning;artificial neural network;artificial intelligence;types of artificial neural networks;pattern recognition;architecture;deep learning;wake-sleep algorithm;computer science;external data representation	ML	18.080575199263155	-32.007131271826154	179628
dabbbc29956b5e99d591e8f220875ee610e4185a	network linear discriminant analysis		Linear discriminant analysis (LDA) is one of the most popularly used classification methods. With the rapid advance of information technology, network data are becoming increasingly available. A novel method called network linear discriminant analysis (NLDA) is proposed to deal with the classification problem for network data. The NLDA model takes both network information and predictive variables into consideration. Theoretically, the misclassification rate is studied and an upper bound is derived under mild conditions. Furthermore, it is observed that real networks are often sparse in structure. As a result, asymptotic performance of NLDA is also obtained under certain sparsity assumptions. In order to evaluate the finite sample performance of the newly proposed methodology, a number of simulation studies are conducted. Lastly, a real data analysis about Sina Weibo is also presented for illustration purpose.	linear discriminant analysis	Wei Cai;Guoyu Guan;Rui Pan;Xuening Zhu;Hansheng Wang	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2017.07.007	statistics;optimal discriminant analysis;information technology;machine learning;predictive variables;linear discriminant analysis;upper and lower bounds;pattern recognition;computer science;artificial intelligence	ML	19.851209923840305	-35.16164441252325	179888
484b81d1889765bc65c5f6ca02bfac3eee5ed7de	fast computing von neumann entropy for large-scale graphs via quadratic approximations		The von Neumann graph entropy (VNGE) can be used as a measure of graph complexity, which can be the measure of information divergence and distance between graphs. Since computing VNGE is required to find all eigenvalues, it is computationally demanding for a large-scale graph. We propose novel quadratic approximations for computing the von Neumann graph entropy. Modified Taylor and Radial projection approximations are proposed. Our methods reduce the cubic complexity of VNGE to linear complexity. Computational simulations on random graph models and various real network datasets demonstrate the superior performance.	approximation;computation;computational complexity theory;cubic function;radial (radio);radial basis function;random graph;simulation	Hayoung Choi;Jinglian He;Hang Hu;Yuanming Shi	2018	CoRR			Vision	24.204660411772547	-36.283011581059554	180471
66108bda682a800f2cd05a2338374b509759992a	on the sample complexity of learning from a sequence of experiments		We analyze the sample complexity of a new problem: learning from a sequence of experiments. In this problem, the learner should choose a hypothesis that performs well with respect to an infinite sequence of experiments, and their related data distributions. In practice, the learner can only perform m experiments with a total of N samples drawn from those data distributions. By using a Rademacher complexity approach, we show that the gap between the training and generation error is O( √ m N ). We also provide some examples for linear prediction, twolayer neural networks and kernel methods.	artificial neural network;experiment;kernel method;rademacher complexity;sample complexity	Longyun Guo;Jean Honorio;John Morgan	2018	CoRR		sequence;linear prediction;algorithm;artificial neural network;rademacher complexity;kernel method;sample complexity;computer science	ML	20.432727789746476	-33.37599392432976	180783
e55580b1b30956ec0032518128efc865cfc85ad2	learning latent variable structured prediction models with gaussian perturbations		The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs [26, 1, 5, 25]. The large-margin formulation including latent variables [30, 21] not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work [11] has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution, with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally, as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally, we provide a formulation using random samples and relaxations that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy, which enables a polynomial time evaluation of the objective function. We illustrate the method with synthetic experiments and a computer vision application.	computer vision;discounted maximum loss;distortion;experiment;latent variable;loss function;polynomial;structured prediction;synthetic intelligence;time complexity	Kevin Bello;Jean Honorio	2018			mathematical optimization;gaussian;distortion;mathematics;upper and lower bounds;structured prediction;perturbation (astronomy);latent variable	ML	23.49419008069976	-31.57282089023738	181264
1363cc4630aab752c1d7dfea51579d310e2c0935	learning diffusion using hyperparameters		In this paper we advocate for a hyperparametric approach to learn diffusion in the independent cascade (IC) model. The sample complexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large. We study a natural restriction of the hypothesis class using additional information available in order to dramatically reduce the sample complexity of the learning process. In particular we assume that diffusion probabilities can be described as a function of a global hyperparameter and features of the individuals in the network. One of the main challenges with this approach is that training a model reduces to optimizing a non-convex objective. Despite this obstacle, we can shrink the best-known sample complexity bound for learning IC by a factor of |E|/d where |E| is the number of edges in the graph and d is the dimension of the hyperparameter. We show that under mild assumptions about the distribution generating the samples one can provably train a model with low generalization error. Finally, we use large-scale diffusion data from Facebook to show that a hyperparametric model using approximately 20 features per node achieves remarkably high accuracy.	generalization error;sample complexity	Dimitris Kalimeris;Yaron Singer;Karthik Subbian;Udi Weinsberg	2018			machine learning;artificial intelligence;hyperparameter;pattern recognition;computer science	ML	20.072243728521634	-32.963319853258035	181493
b418e3bcaba7c29bfe223e16323c44b13cc3eaee	learning from multiple sources of inaccurate data	program graph;learning algorithms;theoretical model;dato que falta;learning model;68;algorithme apprentissage;68t05;data model;68q;donnee manquante;modele theorique;machine learning;inaccurate data;graphe programme;multiple sources;inductive inference;missing data;68qxx;grafo programa;modelo teorico;68t	Most theoretical models of inductive inference make the idealized assumption that the data available to a learner is from a single and accurate source. The subject of inaccuracies in data emanating from a single source has been addressed by several authors. The present paper argues in favor of a more realistic learning model in which data emanates from multiple sources, some or all of which may be inaccurate. Three kinds of inaccuracies are considered: spurious data (modeled as noisy texts), missing data (modeled as incomplete texts), and a mixture of spurious and missing data (modeled as imperfect texts).#R##N#Motivated by the above argument, the present paper introduces and theoretically analyzes a number of inference criteria in which a learning machine is fed data from multiple sources, some of which may be infected with inaccuracies. The learning situation modeled is the identification in the limit of programs from graphs of computable functions. The main parameters of the investigation are: the kind of inaccuracy, the total number of data sources, the number of faulty data sources which produce data within an acceptable bound, and the bound on the number of errors allowed in the final hypothesis learned by the machine.#R##N#Sufficient conditions are determined under which, for the same kind of inaccuracy, for the same bound on the number of errors in the final hypothesis, and for the same bound on the number of inaccuracies, learning from multiple texts, some of which may be inaccurate, is equivalent to learning from a single inaccurate text. #R##N#The general problem of determining when learning from multiple inaccurate texts is a restriction over learning from a single inaccurate text turns out to be combinatorially very complex. Significant partial results are provided for this problem. Several results are also provided about conditions under which the detrimental effects of multiple texts can be overcome by either allowing more errors in the final hypothesis or by reducing the number of inaccuracies in the texts.#R##N#It is also shown that the usual hierarchies resulting from allowing extra errors in the final program (results in increased learning power) and allowing extra inaccuracies in the texts (results in decreased learning power) hold.#R##N#Finally, it is demonstrated that in the context of learning from multiple inaccurate texts, spurious data is better than missing data, which in turn is better than a mixture of spurious and missing data.		Ganesh Baliga;Sanjay Jain;Arun Sharma	1997	SIAM J. Comput.	10.1137/S0097539792239461	mathematical optimization;missing data;data model;computer science;inductive reasoning;machine learning;data mining;mathematics;algorithm;statistics	Theory	18.76788027158732	-33.72741463557287	181629
1ff2239cdebdf97ccfac2d2759355ed9c6f52888	an information theoretic perspective on multiple classifier systems	classification error;mutual information;multiple classifier system;information theoretic;information theory	This paper examines the benefits that information theory can bring to the study of multiple classifier systems. We discuss relationships between the mutual information and the classification error of a predictor. We proceed to discuss how this concerns ensemble systems, by showing a natural expansion of the ensemble mutual information into “accuracy” and “diversity” components. This natural derivation of a diversity term is an alternative to previous attempts to artificially define a term. The main finding is that diversity in fact exists at multiple orders of correlation, and pairwise diversity can capture only the low order components.	information theory;kerrison predictor;mutual information	Gavin Brown	2009		10.1007/978-3-642-02326-2_35	variation of information;multivariate mutual information;machine learning;pattern recognition;data mining;mathematics;conditional mutual information;interaction information;pointwise mutual information	ML	19.752219807898175	-34.97031827435758	182428
5dcc4f616d4791dbd8199c40661a7564218b0b64	agnostic learning versus prior knowledge in the design of kernel machines	kernel machine design;model selection;learning artificial intelligence convex programming;convex programming;automated model selection;prior knowledge;optimal model parameter;machine learning kernel integrated circuit modeling design optimization automatic control neural networks polynomials learning systems training data support vector machines;convex optimisation;kernel machine;learning methods;automated model selection agnostic learning kernel machine design optimal model parameter convex optimisation;agnostic learning;learning artificial intelligence;leave one out cross validation;optimization model	"""The optimal model parameters of a kernel machine are typically given by the solution of a convex optimisation problem with a single global optimum. Obtaining the best possible performance is therefore largely a matter of the design of a good kernel for the problem at hand, exploiting any underlying structure and optimisation of the regularisation and kernel parameters, i.e. model selection. Fortunately, analytic bounds on, or approximations to, the leave-one-out cross-validation error are often available, providing an efficient and generally reliable means to guide model selection. However, the degree to which the incorporation of prior knowledge improves performance over that which can be obtained using """"standard"""" kernels with automated model selection (i.e. agnostic learning), is an open question. In this paper, we compare approaches using example solutions for all of the benchmark tasks on both tracks of the IJCNN-2007 Agnostic Learning versus Prior Knowledge Challenge."""	approximation;benchmark (computing);convex optimization;cross-validation (statistics);error detection and correction;global optimization;kernel (operating system);kernel method;mathematical optimization;model selection	Gavin C. Cawley;Nicola L. C. Talbot	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371219	kernel method;mathematical optimization;convex optimization;kernel embedding of distributions;computer science;machine learning;pattern recognition;tree kernel;cross-validation;model selection;statistics	ML	21.908724158079362	-35.1303165831995	182698
b1854426e09cd2d153048bfc8130d552d12ee5d3	an accelerated communication-efficient primal-dual optimization framework for structured machine learning		Distributed optimization algorithms are essential for training machine learning models on very large-scale datasets. However, they often suffer from communication bottlenecks. Confronting this issue, a communication-efficient primal-dual coordinate ascent framework (CoCoA) and its improved variant CoCoA+ have been proposed, achieving a convergence rate of O(1/t) for solving empirical risk minimization problems with Lipschitz continuous losses. In this paper, an accelerated variant of CoCoA+ is proposed and shown to possess a convergence rate of O(1/t2) in terms of reducing suboptimality. The analysis of this rate is also notable in that the convergence rate bounds involve constants that, except in extreme cases, are significantly reduced compared to those previously provided for CoCoA+. The results of numerical experiments are provided to show that acceleration can lead to significant performance gains.	algorithm;bottleneck (software);coordinate descent;empirical risk minimization;experiment;lagrange multiplier;machine learning;mathematical optimization;numerical analysis;program optimization;rate of convergence;times ascent	Chenxin Ma;Martin Jaggi;Frank E. Curtis;Nathan Srebro;Martin Takác	2017	CoRR		mathematics;mathematical optimization;discrete mathematics;acceleration;rate of convergence;lipschitz continuity;machine learning;empirical risk minimization;artificial intelligence	ML	24.236079087339785	-33.94210908931767	182789
00699b76c6938a02d73031be029da7f7a81243a7	least squares support vector machine on gaussian wavelet kernel function set	transformation ondelette;metodo cuadrado menor;processus gauss;fonction orthogonale;methode moindre carre;analisis estadistico;least squares method;kernel function;wavelet decomposition;support vector;classification a vaste marge;analisis regresion;statistical analysis;funcion nucleo;analyse statistique;fonction noyau;analyse regression;orthogonal function;regression analysis;transformacion ondita;gaussian process;support vector machine;maquina ejemplo soporte;vector support machine;reseau neuronal;funcion ortogonal;proceso gauss;red neuronal;wavelet transformation;neural network;least squares support vector machine	The kernel function of support vector machine (SVM) is an important factor for the learning result of SVM. Based on the wavelet decomposition and conditions of the support vector kernel function, Gaussian wavelet kernel function set for SVM is proposed. Each one of these kernel functions is a kind of orthonormal function, and it can simulate almost any curve in quadratic continuous integral space, thus it enhances the generalization ability of the SVM. According to the wavelet kernel function and the regularization theory, Least squares support vector machine on Gaussian wavelet kernel function set (LS-GWSVM) is proposed to greatly simplify the solving process of GWSVM. The LS-GWSVM is then applied to the regression analysis and classifying. Experiment results show that the regression’s precision is improved by LS-GWSVM, compared with LS-SVM whose kernel function is Gaussian function.	least squares support vector machine;wavelet	Fangfang Wu;Yinliang Zhao	2006		10.1007/11759966_137	kernel;principal component regression;regularization perspectives on support vector machines;kernel density estimation;support vector machine;least squares support vector machine;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;kernel adaptive filter;mean-shift;kernel principal component analysis;computer science;machine learning;pattern recognition;graph kernel;mathematics;relevance vector machine;gaussian function;variable kernel density estimation;polynomial kernel;artificial neural network;statistics;kernel smoother	ML	19.9969832968599	-34.52630185088362	183752
5a369221c58004b3f17219b459e30a77b4f83435	zeroth-order stochastic variance reduction for nonconvex optimization		As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order O(1/b), where b is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance between the convergence rate and the function query complexity.	algorithm;black box;coordinate descent;decision tree model;deep learning;emoticon;first-order predicate;gradient;iteration;mathematical optimization;optimization problem;rate of convergence;stochastic optimization;variance reduction	Sijia Liu;Bhavya Kailkhura;Pin-Yu Chen;Pai-Shun Ting;Shiyu Chang;Lisa Amini	2018			mathematical optimization;stochastic optimization;estimator;rate of convergence;artificial intelligence;machine learning;artificial neural network;analysis of variance;variance reduction;zeroth law of thermodynamics;mathematics	ML	24.188930319710842	-33.448452083383245	184358
1ff56a5694915436adf42557a48a74b0eeb4e704	a framework for kernel-based multi-category classification	classification algorithm;proof of concept;support vector classification	A geometric framework for understanding multi-category classification is introduced, through which many existing ‘all-together’ algorithms can be understood. The structure enables parsimonious optimisation, through a direct extension of the binary methodology. The focus is on Support Vector Classification, with parallels drawn to related methods. The ability of the framework to compare algorithms is illustrated by a brief discussion of Fisher consistency. Its utility in improving understanding of multi-category analysis is demonstrated through a derivation of improved generalisation bounds. It is also described how this architecture provides insights regarding how to further improve on the speed of existing multi-category classification algorithms. An initial example of how this might be achieved is developed in the formulation of a straightforward multi-category Sequential Minimal Optimisation algorithm. Proof-of-concept experimental results have shown that this, combined with the mapping of pairwise results, is comparable with benchmark optimisation speeds.	algorithm;benchmark (computing);consistency model;kernel (operating system);mathematical optimization;occam's razor;parallels desktop for mac;support vector machine	Simon I. Hill;Arnaud Doucet	2007	J. Artif. Intell. Res.	10.1613/jair.2251	computer science;machine learning;pattern recognition;data mining;mathematics;proof of concept	ML	20.429717408439405	-36.62424012708607	184463
ce3d30280dabcb31ef0bbbe3f81640bf8fd5b113	consistency of randomized and finite sized decision tree ensembles	discretization;ensembles;randomization;decision trees;consistency	Regression via classification (RvC) is a method in which a regression problem is converted into a classification problem. A discretization process is used to covert continuous target value to classes. The discretized data can be used with classifiers as a classification problem. In this paper, we use a discretization method, Extreme Randomized Discretization, in which bin boundaries are created randomly to create ensembles. We present an ensemble method for RvC problems. We show theoretically for a set of problems that if the number of bins is three, the proposed ensembles for RvC perform better than RvC with the equal-width discretization method. We use these results to show that infinite-sized ensembles, consisting of finite-sized decision trees, created by a pure randomized method (split points are created randomly), are not consistent. We also theoretically show, using a set of regression problems, that the performance of these ensembles is dependent on the size of member decision trees.	decision tree;discretization;neural ensemble;randomized algorithm;randomness	Amir Ahmad;Sami M. Halawani;Ibrahim A. Albidewi	2011	Pattern Analysis and Applications	10.1007/s10044-011-0260-8	randomization;machine learning;decision tree;pattern recognition;discretization;mathematics;consistency;statistics	ML	18.45478208584689	-34.348651745413335	184730
f3ad959d0de095247b9841536c146859e6b27404	advances in learning with kernels: theory and practice in a world of growing constraints		Kernel methods consistently outperformed previous generations of learning techniques. They provide a flexible and expressive learning framework that has been successfully applied to a wide range of real world problems but, recently, novel algorithms, such as Deep Neural Networks and Ensemble Methods, have increased their competitiveness against them. Due to the current data growth in size, heterogeneity and structure, the new generation of algorithms are expected to solve increasingly challenging problems. This must be done under growing constraints such as computational resources, memory budget and energy consumption. For these reasons, new ideas have to come up in the field of kernel learning, such as deeper kernels and novel algorithms, to fill the gap that now exists with the most recent learning paradigms. The purpose of this special session is to highlight recent advances in learning with kernels. In particular, this session welcomes contributions toward the solution of the weaknesses (e.g. scalability, computational efficiency and too shallow kernels) and the improvement of the strengths (e.g. the ability of dealing with structural data) of the state of the art kernel methods. We also encourage the submission of new theoretical results in the Statistical Learning Theory framework and innovative solutions to real world problems.	algorithm;computation;computational resource;ensemble learning;kernel (operating system);kernel method;neural networks;scalability;statistical learning theory	Luca Oneto;Nicolò Navarin;Michele Donini;Fabio Aiolli;Davide Anguita	2016			machine learning;artificial intelligence;information system;computer science	ML	18.745514005152145	-37.972986810890525	185853
7af5a5eb1e472ab23c6d4c3dd9b37640533ec89c	analysis of representations for domain adaptation	perforation;discrimination learning;generalization bounds	Discriminative learning methods for classification perfor m well when training and test data are drawn from the same distribution. In many situa tions, though, we have labeled training data for a sourcedomain, and we wish to learn a classifier which performs well on atargetdomain with a different distribution. Under what conditions can we adapt a classifier trained on the source dom ain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition t heoretically with a generalization bound for domain adaption. Our theory illus trates the tradeoffs inherent in designing a representation for domain adaptation nd gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the d ifference between the source and target domains, while at the same time maximizing the margin of the training set.	algorithm;approximation;domain adaptation;effective domain;experiment;heuristic;np-hardness;statistical classification;structured text;test data;test set	Shai Ben-David;John Blitzer;Koby Crammer;Fernando Pereira	2006			computer science;artificial intelligence;machine learning;pattern recognition;mathematics;discrimination learning	ML	19.985270026498807	-35.28853421549696	185981
18350b25e96c281b39d613acd65d0a42a64eac1f	threshold network learning in the presence of equivalences	networked learning	This paper applies the theory of Probably Approximately Correct (PAC) learning to multiple output feedforward threshold networks in which the weights conform to certain equivalences. It is shown that the sample size for reliable learning can be bounded above by a formula similar to that required for single output networks with no equivalences. The best previously obtained bounds are improved for all cases.	feedforward neural network;probably approximately correct learning	John Shawe-Taylor	1991			discrete mathematics;computer science;machine learning;mathematics;algorithm	ML	20.086671378175986	-31.51345462361976	186152
85a12f0a7cfae9c83060622566fe03e7221fd1d3	learning discriminative metrics via generative models and kernel learning	learning algorithm;generic model;statistical machine learning;artificial intelligent;science learning;discrimination learning;discriminative training	Metrics specifying distances between data points can be lea rned in a discriminative manner or from generative models. In this paper, we show how to unify genera tiv nd discriminative learning of metrics via a kernel learning framework. Specifically, we learn local metrics optimized from parametric generative models. These are then used as base kernels to con s ru t a global kernel that minimizes a discriminative training criterion. We consider both linea r and nonlinear combinations of local metric kernels. Our empirical results show that these combination s significantly improve performance on classification tasks. The proposed learning algorithm is also ve ry efficient, achieving order of magnitude speedup in training time compared to previous discriminati ve baseline methods.	algorithm;baseline (configuration management);data point;discriminative model;genera;generative model;kernel (operating system);naruto shippuden: clash of ninja revolution 3;nonlinear system;speedup	Yuan Shi;Yung-Kyun Noh;Fei Sha;Daniel D. Lee	2011	CoRR		semi-supervised learning;instance-based learning;speech recognition;computer science;online machine learning;machine learning;pattern recognition;stability;generative model;active learning;discriminative model;discrimination learning	ML	22.045731725454594	-35.394518778963466	186788
f00255d6d40c0d072d47e52656bc15dcc5ec1840	conditional classification trees using instrumental variables	classification tree;recursive partitioning;supervised learning;discriminant analysis;empirical evidence;classification rules;instrumental variable	The framework of this paper is supervised learning using classification trees. Two types of variables play a role in the definition of the classification rule, namely a response variable and a set of predictors. The tree classifier is built up by a recursive partitioning of the prediction space such to provide internally homogeneous groups of objects with respect to the response classes. In the following, we consider the role played by an instrumental variable to stratify either the variables or the objects. This yields to introduce a tree-based methodology for conditional classification. Two special cases will be discussed to grow multiple discriminant trees and partial predictability trees. These approaches use discriminant analysis and predictability measures respectively. Empirical evidence of their usefulness will be shown in real case studies.	decision tree learning;linear discriminant analysis;recursion;supervised learning	Valerio A. Tutore;Roberta Siciliano;Massimo Aria	2007		10.1007/978-3-540-74825-0_15	empirical evidence;instrumental variable;decision tree learning;computer science;machine learning;pattern recognition;data mining;optimal discriminant analysis;mathematics;supervised learning;one-class classification;statistics;recursive partitioning	ML	19.39255818654831	-35.49604613570417	186919
69c7364608ba9bfe21664ca9dc0d11c93649829c	efficient principal subspace projection of streaming data through fast similarity matching		Big data problems frequently require processing datasets in a streaming fashion, either because all data are available at once but collectively are larger than available memory or because the data intrinsically arrive one data point at a time and must be processed online. Here, we introduce a computationally efficient version of similarity matching [1], a framework for online dimensionality reduction that incrementally estimates the top K-dimensional principal subspace of streamed data while keeping in memory only the last sample and the current iterate. To assess the performance of our approach, we construct and make public a test suite containing both a synthetic data generator and the infrastructure to test online dimensionality reduction algorithms on real datasets, as well as performant implementations of our algorithm and competing algorithms with similar aims. Among the algorithms considered we find our approach to be competitive, performing among the best on both synthetic and real data.	algorithm;algorithmic efficiency;big data;converge;data point;dimensionality reduction;iteration;matlab;numerical analysis;python;rate of convergence;stream (computing);streaming media;synthetic data;synthetic intelligence;test suite;time complexity	Andrea Giovannucci;Victor Minden;Cengiz Pehlevan;Dmitri B. Chklovskii	2018	CoRR		implementation;data mining;online algorithm;artificial intelligence;machine learning;principal component analysis;big data;dimensionality reduction;subspace topology;computer science;test suite;synthetic data	ML	24.486525641858	-36.16988435984075	187047
382aef158e7ea9bec0a4a3c875e99697fe235941	piecewise affine regression via recursive multiple least squares and multicategory discrimination	computacion informatica;recursive multiple least squares;grupo de excelencia;qa75 electronic computers computer science;pwa regression;system identification;clustering;ciencias basicas y experimentales;multicategory discrimination	In nonlinear regression choosing an adequate model structure is often a challenging problem. While simple models (such as linear functions) may not be able to capture the underlying relationship among the variables, over-parametrized models described by a large set of nonlinear basis functions tend to overfit the training data, leading to poor generalization on unseen data. Piecewise-affine (PWA) models can describe nonlinear and possible discontinuous relationships while maintaining simple local affine regressor-to-output mappings, with extreme flexibility when the polyhedral partitioning of the regressor space is learned from data rather than fixed a priori. In this paper, we propose a novel and numerically very efficient two-stage approach for PWA regression based on a combined use of (i) recursive multimodel least-squares techniques for clustering and fitting linear functions to data, and (ii) linear multicategory discrimination, either offline (batch) via a Newton-like algorithm for computing a solution of unconstrained optimization problems with objective functions having a piecewise smooth gradient, or online (recursive) via averaged stochastic gradient descent. © 2016 Elsevier Ltd. All rights reserved.	algorithm;basis function;cluster analysis;computation;least squares;linear function;mathematical optimization;newton;nonlinear system;numerical analysis;online and offline;overfitting;picasa web albums;polyhedron;polynomial;recursion;stochastic gradient descent	Valentina Breschi;Dario Piga;Alberto Bemporad	2016	Automatica	10.1016/j.automatica.2016.07.016	mathematical optimization;system identification;multivariate adaptive regression splines;machine learning;control theory;mathematics;cluster analysis;statistics	AI	23.027188377330948	-34.42538914815143	187144
438e3da53ea45248d34aa423a2fdbb27eb7433dd	distributed multitask reinforcement learning with quadratic convergence		Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. The main reason behind this drawback is the reliance on centeralised solutions. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. These methods, however, suffer from two drawbacks. First, they rely on predefined objectives, and, second, exhibit linear convergence guarantees. In this paper, we improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees.	architecture as topic;behavior;computation;computer multitasking;convergence (action);experiment;graph (discrete mathematics);graph - visual representation;increment;inference;large;mental suffering;reinforcement learning;scalability;solutions;solver;variational principle;algorithm	Rasul Tutunov;DongHo Kim;Haitham Bou-Ammar	2018			artificial intelligence;rate of convergence;computer science;mathematical optimization;machine learning;scalability;reinforcement learning;inference;solver	ML	24.554618416114597	-34.08483837278547	187452
4ec382f2caea2a947876c8e630cb13b68b43c646	a method for regularization of evolutionary polynomial regression	regularization;feature extraction;evolutionary polynomial regression	While many applications require models that have no acceptable linear approximation, the simpler nonlinear models are defined by polynomials. The use of genetic algorithms to find polynomial models from data is known as Evolutionary Polynomial Regression (EPR). This paper introduces Evolutionary Polynomial Regression with Regularization, an algorithm extending EPR with a regularization term to control polynomial complexity. The article also describes a set of experiences to compare both flavors of EPR against other methods including Linear Regression, Regression Trees and Support Vector Regression. These experiments show that Evolutionary Polynomial Regression with Regularization is able to achieve better fitting and needs less computation time than plain EPR.	polynomial	Francisco Coelho;João Pedro Neto	2017	Appl. Soft Comput.	10.1016/j.asoc.2017.05.047	regularization;econometrics;mathematical optimization;feature extraction;computer science;machine learning;polynomial regression;mathematics;square-free polynomial;statistics	ECom	21.889999143587936	-36.89005779914874	187618
fb0c10c1a54fd41c3764aebf042b783ce9eedf0d	the minimum volume ellipsoid metric	learning algorithm;supervised learning;convex optimization;covariance matrix	We propose an unsupervised “local learning” algorithm for learning a metric in the input space. Geometrically, for a given query point, the algorithm finds the minimum volume ellipsoid (MVE) covering its neighborhood which characterizes the correlations and variances of its neighborhood variables. Algebraically, the algorithm maximizes the determinant of the local covariance matrix which amounts to a convex optimization problem. The final matrix parameterizes a Mahalanobis metric yielding the MVE metric (MVEM). The proposed metric was tested in a supervised learning task and showed promising and competitive results when compared with state of the art metrics in the literature.	algorithm;convex optimization;machine learning;mathematical optimization;optimization problem;supervised learning;unsupervised learning	Karim T. Abou-Moustafa;Frank P. Ferrie	2007		10.1007/978-3-540-74936-3_34	convex metric space;mathematical optimization;combinatorics;intrinsic metric;machine learning;mathematics	ML	24.423187926775807	-37.86592877969421	188147
5ab2e91d5ac315d2bb1c56058aa9e082b2748c26	a fast training algorithm for least squares svm	least squares approximations;mathematics computing;support vector machines;matrix inversion;support vector;support vector machines learning artificial intelligence least squares approximations mathematics computing pattern classification;least squares methods support vector machines support vector machine classification iterative algorithms large scale systems quadratic programming lagrangian functions chaos sun automatic testing;large scale;incremental learning theory least square svm classifier support vector machine fast training algorithm decremental learning theory;least square;pattern classification;learning artificial intelligence;learning theory;training algorithm	A fast training algorithm for Least Squares SVM (LS-SVM) classifiers was proposed, which is based on incremental and decremental learning theory. When a SV (Support Vector) is added or removed, computation based on previous training result replaces large-scale matrix inverse, thus the computation cost is reduced. The innovation is that by reasonable use of incremental and decremental learning the proposed algorithm can adaptively adjust the size of training sets (number of SVs) according to the specific classification problem. Finally several experiments show the validity of proposed algorithm.	algorithm;algorithmic efficiency;computation;experiment;least squares;neural coding;systemverilog	Shouda Jiang;Lianlei Lin;Chao Sun	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.18	support vector machine;least squares support vector machine;mathematical optimization;non-linear iterative partial least squares;computer science;online machine learning;machine learning;pattern recognition;mathematics;stability;relevance vector machine;computational learning theory;active learning;structured support vector machine;population-based incremental learning	Robotics	20.56837943466387	-37.2709001045151	188685
7ea8bd8e5c0d6bc425359df3a8fd778e79713b5f	guest editorial: regularisation methods in regression and classification	seminar fur angewandte stochastik;statistik;ddc 510	Regularisation has become an important tool in statistical modelling. In particular the challenge of high dimensional data boosted the fitting of more and more complex models that can not be fitted without appropriate regularisation. The need for regularisation, however, is not restricted to the modelling of high dimensional data, it is mainly driven by the complexity of the model. When the model includes nonparametric function estimation regularisation restricts the class of functions that are fitted. In regression and classification complexity of the model is usually determined by the structuring of the predictor. Regularisation helps to identify the relevant parts, which can consist of simple linear terms, functions, parametric or nonparametric interaction terms, or complex spatially and temporally structured terms. Regularisation can be made explicit by using penalty terms that restrict estimates or can be implicitly determined by the algorithm as for example in boosting methods. This issue of Statistics and Computing collects ten papers that focus on regularisation methods in different areas and with different methodology. Two papers are devoted to the extension of boosting techniques. In the paper “Twin Boosting: Improved Feature Selection and Prediction” P. Bühlmann and T. Hothorn propose a boosting method that consists of two rounds of common boosting with the second boosting process being forced to resemble the first round of boosting. The method shows much better feature selection behaviour than common boosting, in particular as far as false positives are concerned. The paper “Estimation and Regularization Techniques for Regression Models with Multidimensional Prediction Functions”	algorithm;boosting (machine learning);categorization;complexity;feature selection;kerrison predictor;statistical model;temporal logic	Gerhard Tutz	2010	Statistics and Computing	10.1007/s11222-010-9171-6	computer science;artificial intelligence;operations research	ML	20.442889416413987	-35.65734182293154	188711
da3da2f9bae83108926fd30cf1a6f85698175d7c	deep stacked stochastic configuration networks for non-stationary data streams		The concept of stochastic configuration networks (SCNs) offers a solid framework for fast implementation of feedforward neural networks through randomized learning. Unlike conventional randomized approaches, SCNs provide an avenue to select appropriate scope of random parameters to ensure the universal approximation property. In this paper, a deep version of stochastic configuration networks, namely deep stacked stochastic configuration network (DSSCN), is proposed for modeling non-stationary data streams. As an extension of evolving stochastic configuration networks (eSCNs), this work contributes a way to grow and shrink the structure of deep stochastic configuration networks autonomously from data streams. The performance of DSSCN is evaluated by six benchmark datasets. Simulation results, compared with prominent data stream algorithms, show that the proposed method is capable of achieving comparable accuracy and evolving compact and parsimonious deep stacked network architecture.	approximation;artificial neural network;benchmark (computing);concept drift;data point;deep learning;ensemble learning;feedforward neural network;futures studies;latex;long short-term memory;multitier architecture;mutual information;network architecture;network interface device;numerical analysis;occam's razor;on the fly;organizing (structure);randomized algorithm;reduction (complexity);relevance;self-organization;simulation;stationary process;stochastic process;streaming algorithm;synthetic intelligence;tier 1 network	Mahardhika Pratama;Dianhui Wang	2018	CoRR		streams;data stream;machine learning;stationary process;approximation property;network architecture;data stream mining;artificial intelligence;feedforward neural network;mathematics	ML	18.454756248349838	-32.16908060925153	189317
53e0edc88fb917968e93c23cefcfb98a4c9c11dd	a randomized heuristic for kernel parameter selection with large-scale multi-class data	minimum enclosing ball;hilbert spaces;time complexity;time complexity randomized heuristic kernel parameter selection large scale multiclass data linear algorithms multiplicative scaling ad hoc estimates minimum enclosing ball reproducing kernel hilbert spaces computer vision paradigm;kernel complexity theory support vector machines approximation methods accuracy clustering algorithms approximation algorithms;computer vision;multiple scales;large scale;parameter selection;computational complexity;reproducing kernel hilbert space;pattern classification;distance metric;kernel method;quality measures;pattern classification computational complexity computer vision hilbert spaces;large scale problem	Over the past few years kernel methods have gained a tremendous amount of attention as existing linear algorithms can easily be extended to account for highly non-linear data in a computationally efficient manner. Unfortunately most kernels require careful tuning of intrinsic parameters to correctly model the distribution of the underlying data. For large-scale problems the multiplicative scaling in time complexity imposed by introducing free parameters in a cross-validation setup will prove computationally infeasible, often leaving pure ad-hoc estimates as the only option. In this contribution we investigate a novel randomized approach for kernel parameter selection in large-scale multi-class data. We fit a minimum enclosing ball to the class means in Reproducing Kernel Hilbert Spaces (RKHS), and use the radius as a quality measure of the space, defined by the kernel parameter. We apply the developed algorithm to a computer vision paradigm where the objective is to recognize 72.000 objects among 1.000 classes. Compared to other distance metrics in the RKHS we find that our randomized approach provides better results together with a highly competitive time complexity.	algorithmic efficiency;camera resectioning;computational complexity theory;computer vision;cross-validation (statistics);heuristic;hilbert space;hoc (programming language);image scaling;kernel (operating system);kernel method;nonlinear system;programming paradigm;randomized algorithm;spaces;time complexity	Toke Jansen Hansen;Trine Julie Abrahamsen;Lars Kai Hansen	2011	2011 IEEE International Workshop on Machine Learning for Signal Processing	10.1109/MLSP.2011.6064582	mathematical optimization;combinatorics;discrete mathematics;kernel embedding of distributions;mathematics;tree kernel;variable kernel density estimation	Vision	24.28696702994432	-37.094620978114285	189795
a4617a9d8be6ba22498e4563c861c252abe7357d	online learning for distribution-free prediction		We develop an online learning method for prediction, which is important in problems with large and/or streaming data sets. We formulate the learning approach using a covariance-fitting methodology, and show that the resulting predictor has desirable computational and distribution-free properties: It is implemented online with a runtime that scales linearly in the number of samples; has a constant memory requirement; avoids local minima problems; and prunes away redundant feature dimensions without relying on restrictive assumptions on the data distribution. In conjunction with the split conformal approach, it also produces distribution-free prediction confidence intervals in a computationally efficient manner. The method is demonstrated on both real and synthetic datasets.	algorithmic efficiency;computation;kerrison predictor;maxima and minima;online machine learning;streaming media;synthetic intelligence	Dave Zachariah;Petre Stoica;Thomas B. Schön	2017	CoRR		mathematical optimization;computer science;online machine learning;machine learning;data mining;statistics	ML	22.39724813884457	-35.03564779818317	190238
196f40f0db2ea1736ed4b0bc18a7be2aed27a54c	multi-class linear dimension reduction by generalized fisher criteria	dimension reduction;feature space;covariance matrices	Linear Disciminant Analysis is in general unable to find the lower-dimensional feature space which maximizes the class discrimination, even if the class distributions can be assumed to be very simple, e.g. Gaussians with identical covariance matrices. In this paper we reformulate the -class Fisher criterion as a sum of -class Fisher criteria. This formulation allows to weigh class pair contributions according to their relevance for classification. Further it offers an obvious way how to cope with heteroscedastic models. We propose a particular weighting scheme which attempts to approximate the pairwise Bayes error. Moderate improvements are obtained on the TIMIT phoneme classification task.	approximation algorithm;common criteria;dimensionality reduction;feature vector;relevance;timit	Marco Loog;Reinhold Häb-Umbach	2000			sufficient dimension reduction;correlation dimension;feature vector;computer science;machine learning;dimensionality reduction	ML	19.65867439345234	-35.9703064348946	190244
db26e1f1318220a3015008346e77e68d86b9f0c4	smooth approximation method for non-smooth empirical risk minimization based distance metric learning	distance metric learning;smooth approximation;empirical risk minimization;nesterov s optimal first order method	Distance metric learning (DML) has become a very active research field in recent years. Bian and Tao (IEEE Trans. Neural Netw. Learn. Syst. 23(8) (2012) 1194-1205) presented a constrained empirical risk minimization (ERM) framework for DML. In this paper, we utilize smooth approximation method to make their algorithm applicable to the non-differentiable hinge loss function. We show that the objective function with hinge loss is equivalent to a non-smooth min-max representation, from which an approximate objective function is derived. Compared to the original objective function, the approximate one becomes differentiable with Lipschitz-continuous gradient. Consequently, Nesterov's optimal first-order method can be directly used. Finally, the effectiveness of our method is evaluated on various UCI datasets.	approximation;empirical risk minimization	Ya Shi;Hongbing Ji	2014	Neurocomputing	10.1016/j.neucom.2013.08.030	mathematical optimization;combinatorics;empirical risk minimization;computer science;machine learning;random coordinate descent;mathematics;statistics	ML	22.2310914316159	-37.06749409323056	190667
2572f770041c77aac75011eb456812cba925a4e2	online distributed passive-aggressive algorithm for structured learning		The training phase is time-consuming for structured learning, especially for supper-tagging tasks. In this paper, we propose an online distributed Passive-Aggression (PA) by averaging parameters for parallel training, which can reduce the training time significantly. We also give theoretic analysis for its convergence. Experimental results show that our method can accelerate the training process significantly with comparable or even better accuracy. © Springer-Verlag 2013.	algorithm	Jiayi Zhao;Xipeng Qiu;Zhao Liu;Xuanjing Huang	2013		10.1007/978-3-642-41491-6_12	active learning	ML	20.11551767077168	-37.084294398372734	191373
2658c43714cacd94acd9649b991f06ce56721fe5	improving reinforcement learning in stochastic ram-based neural networks	reinforcement learning;ram based nodes;reward penalty;training algorithm;hardware realisation;neural network	RAM-based neural networks are designed to be efficiently implemented in hardware. The desire to retain this property influences the training algorithms used, and has led to the use of reinforcement (reward-penalty) learning. An analysis of the reinforcement algorithm applied to RAM-based nodes has shown the ease with which unlearning can occur. An amended algorithm is proposed which demonstrates improved learning performance compared to previously published reinforcement regimes.	algorithm;artificial neural network;random-access memory;reinforcement learning	Alistair Ferguson;Hamid Bolouri	1996	Neural Processing Letters	10.1007/BF00417784	simulation;computer science;artificial intelligence;machine learning;learning classifier system;reinforcement learning;artificial neural network	ML	17.422527037771584	-31.12367624125027	192564
84bce3bdced8dad68a271d8f51f4506da08188af	boosting density estimation	density estimation	Several authors have suggested viewing boosting as a gradient descent search for a good fit in function space. We apply gradient-based boosting methodology to the unsupervised learning problem of density estimation. We show convergence properties of the algorithm and prove that a strength of weak learnability property applies to this problem as well. We illustrate the potential of this approach through experiments with boosting Bayesian networks to learn density models.	algorithm;bayesian network;boosting (machine learning);experiment;gradient descent;learnability;unsupervised learning	Saharon Rosset;Eran Segal	2002			econometrics;density estimation;computer science;machine learning;pattern recognition;mathematics;gradient boosting;statistics	ML	21.181806030905115	-34.743363852262114	192655
79a011d6d98b4209f21264fa9dbae7f89a6c6561	l4: practical loss-based stepsize adaptation for deep learning		We propose a stepsize adaptation scheme for stochastic gradient descent. It operates directly with the loss function and rescales the gradient in order to make fixed predicted progress on the loss. We demonstrate its capabilities by conclusively improving the performance of Adam and Momentum optimizers. The enhanced optimizers with default hyperparameters consistently outperform their constant stepsize counterparts, even the best ones, without a measurable increase in computational cost. The performance is validated on multiple architectures including dense nets, CNNs, ResNets, and the recurrent Differential Neural Computer on classical datasets MNIST, fashion MNIST, CIFAR10 and others.	algorithmic efficiency;computation;deep learning;loss function;mnist database;stochastic gradient descent	Michal Rolinek;Georg Martius	2018			mathematical optimization;machine learning;deep learning;hyperparameter;stochastic gradient descent;mnist database;mathematics;artificial intelligence;measure (mathematics);momentum	ML	22.79993587490484	-31.940718876057083	193015
1eba8afac8fec85fe678d90675f85cda45b236ae	the kernel least-mean-square algorithm	espace hilbert;traitement signal;on line systems;kernel hilbert spaces;least mean square;tratamiento transaccion;least mean squares methods adaptive filters hilbert spaces learning artificial intelligence;learning algorithm;methode moindre carre moyen;mise a jour;algorithm performance;lms algorithm;high dimensionality;espacio hilbert;least mean squares methods;learning;hilbert spaces;methode noyau;kernel methods;tikhonov regularization kernel methods least mean square;procede discontinu;algorithme apprentissage;indexing terms;aprendizaje;actualizacion;hilbert space;apprentissage;adaptive filters;machine learning;resultado algoritmo;systeme en ligne;reproducing kernel hilbert space;signal processing;kernel least mean square algorithm;metodo nucleo;kernel signal processing algorithms least squares approximation machine learning algorithms principal component analysis hilbert space training data algorithm design and analysis radio access networks adaptive filters;batch process;performance algorithme;filtro adaptable;kernel method;procedimiento discontinuo;tikhonov regularization;filtre adaptatif;learning artificial intelligence;sample by sample update;transaction processing;machine learning kernel least mean square algorithm sample by sample update adaptive filter kernel hilbert spaces;algoritmo aprendizaje;procesamiento senal;adaptive filter;updating;traitement transaction	The combination of the famed kernel trick and the least-mean-square (LMS) algorithm provides an interesting sample-by-sample update for an adaptive filter in reproducing kernel Hilbert spaces (RKHS), which is named in this paper the KLMS. Unlike the accepted view in kernel methods, this paper shows that in the finite training data case, the KLMS algorithm is well posed in RKHS without the addition of an extra regularization term to penalize solution norms as was suggested by Kivinen [Kivinen, Smola and Williamson, ldquoOnline Learning With Kernels,rdquo IEEE Transactions on Signal Processing, vol. 52, no. 8, pp. 2165-2176, Aug. 2004] and Smale [Smale and Yao, ldquoOnline Learning Algorithms,rdquo Foundations in Computational Mathematics, vol. 6, no. 2, pp. 145-176, 2006]. This result is the main contribution of the paper and enhances the present understanding of the LMS algorithm with a machine learning perspective. The effect of the KLMS step size is also studied from the viewpoint of regularization. Two experiments are presented to support our conclusion that with finite data the KLMS algorithm can be readily used in high dimensional spaces and particularly in RKHS to derive nonlinear, stable algorithms with comparable performance to batch, regularized solutions.	adaptive filter;algorithm;computation;computational mathematics;experiment;hilbert space;kernel (operating system);kernel method;least mean squares filter;machine learning;matrix regularization;nonlinear system;signal processing;yao graph	Weifeng Liu;Puskal P. Pokharel;José Carlos Príncipe	2008	IEEE Transactions on Signal Processing	10.1109/TSP.2007.907881	adaptive filter;kernel method;mathematical optimization;least mean squares filter;computer science;machine learning;signal processing;mathematics;algorithm;statistics;hilbert space	Vision	22.4232571586245	-34.1889238593417	193253
7181c13517f701e3b9ebf6d4212de88353269c8c	generative structure learning for markov logic networks	structure learning	In this paper, we present a generative algorithm to learn Markov Logic Network (MLN) structures automatically, directly from a training dataset. The algorithm follows a bottom-up approach by first heuristically transforming the training dataset into boolean tables, then creating candidate clauses using these boolean tables and finally choosing the best clauses to build the MLN. Comparisons to the state-of-the-art structure learning algorithms for MLNs in two real-world domains show that the proposed algorithm outperforms them in terms of the conditional log likelihood (CLL), and the area under the precision-recall curve (AUC).	markov chain;markov logic network	Quang-Thang Dinh;Matthieu Exbrayat;Christel Vrain	2010		10.3233/978-1-60750-676-8-63	computer science;artificial intelligence;machine learning;pattern recognition	ML	17.596767190192285	-35.96872532333656	193499
9e00bfc189e54758300cf208a3204759dd5a6377	consistent multiclass algorithms for complex performance measures		This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special case all loss-based performance measures, which are simply linear functions of the confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro F1 measures. We give a general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices, and give two specific instantiations based on the Frank-Wolfe method for concave performance measures and on the bisection method for ratioof-linear performance measures. The resulting algorithms are provably consistent and outperform a multiclass version of the state-of-the-art SVMperf method in experiments; for large multiclass problems, the algorithms are also orders of magnitude faster than SVMperf.	bisection method;concave function;confusion matrix;experiment;frank–wolfe algorithm;linear function;mathematical optimization;multiclass classification;optimization problem	Harikrishna Narasimhan;Harish G. Ramaswamy;Aadirupa Saha;Shivani Agarwal	2015			multiclass classification	ML	20.514325762829934	-36.33868558118083	193578
1883a5dd20cceed67420a60fb45c914218ef21f9	learning to aggregate information for sequential inferences		We consider the problem of training a binary sequential classifier under an error rate constraint. It is well known that for known densities, accumulating the likelihood ratio statistics is time optimal under a fixed error rate constraint. For the case of unknown densities, we formulate the learning for sequential detection problem as a constrained density ratio estimation problem. Specifically, we show that the problem can be posed as a convex optimization problem using a Reproducing Kernel Hilbert Space representation for the log-density ratio function. The proposed binary sequential classifier is tested on synthetic data set and UC Irvine human activity recognition data set, together with previous approaches for density ratio estimation. Our empirical results show that the classifier trained through the proposed technique achieves smaller average sampling cost than previous classifiers proposed in the literature for the same error rate.	activity recognition;aggregate function;algorithm;convex optimization;hilbert space;mathematical optimization;optimization problem;sampling (signal processing);synthetic data;uc browser;variational principle	Diyan Teng;Emre Ertin	2014	CoRR		sequential estimation;machine learning;pattern recognition;mathematics;statistics	ML	23.858000086042747	-32.58713132513041	194534
3efd2c862d94f4041880cbfe6b810153a2dc0005	stability conditions for online learnability	online algorithm;learning algorithm;statistical machine learning;uniform convergence;science learning;stability condition;randomized algorithm;binary classification;large classes;leave one out	Stability is a general notion that quantifies the sensitivit y of a learning algorithm’s output to small change in the training dataset (e.g. deletion or replacemen t of a single training sample). Such conditions have recently been shown to be more powerful to chara cte ize learnability in the general learning setting under i.i.d. samples where uniform conver gence is not necessary for learnability, but where stability is both sufficient and necessary for learnability. We here show that similar stability conditions are also sufficient for online learnab ility, i.e. whether there exists a learning algorithm such that under any sequence of examples (potential ly chosen adversarially) produces a sequence of hypotheses that has no regret in the limit with resp ect to the best hypothesis in hindsight. We introduce online stability, a stability condition relat ed to uniform-leave-one-out stability in the batch setting, that is sufficient for online learnability. I n particular we show that popular classes of online learners, namely algorithms that fall in the categ ory of Follow-the-(Regularized)-Leader, Mirror Descent, gradient-based methods and randomized alg orithms like Weighted Majority and Hedge, are guaranteed to have no regret if they have such onli ne stability property. We provide examples that suggest the existence of an algorithm with suc tability condition might in fact be necessary for online learnability. For the more restricted binary classification setting, we establish that such stability condition is in fact both sufficient and n ecessary. We also show that for a large class of online learnable problems in the general learning s etting, namely those with a notion of sub-exponential covering, no-regret online algorithms th at have such stability condition exists.	binary classification;descent;ftl: faster than light;gradient;learnability;non-functional requirement;online algorithm;oxford spelling;randomized algorithm;regret (decision theory);time complexity	Stéphane Ross;J. Andrew Bagnell	2011	CoRR		binary classification;online algorithm;mathematical optimization;uniform convergence;computer science;machine learning;mathematics;randomized algorithm;stability;algorithm	ML	20.18306871752757	-32.298385226604665	194671
3ac24187b4cf34883e367cada015525d76dbfefa	investigating the distribution assumptions in the pac learning model	distribution assumption;pac learning	In this paper, we question two assumptions of the pac learning model related to the distribution of examples: that the learning algorithm must learn for an arbitrary distribution (the distribution-independence assumption), and that the distribution of training examples is identical to the distribution of examples that the learning system sees in use (the distribution-invariance assumption). We argue that the distribution-independence assumption is too stringent, and we propose a learning model in which the distributions are required to satisfy a parametrized “reasonableness” criterion. This model allows us to extend results for learning functions under uniform distributions to non-uniform distributions. As an example, a bound is given for the time to learn an arbitrary halfspace in this model using the perceptron algorithm. We also argue that the distribution-invariance assumption in the pac model is unrealistic and we give bounds on the sample complexity when the distributions of training and testing examples differ.	probably approximately correct learning	Peter L. Bartlett;Robert C. Williamson	1991			computer science;artificial intelligence;machine learning;mathematics;stability;probably approximately correct learning;statistics	NLP	19.903880156946634	-32.70049800470405	194838
c56fc8564b057ac55800fd6269247f79e93fb864	differentially-private learning of low dimensional manifolds	random projection tree;doubling dimension;low dimensional manifolds;differential privacy	In this paper, we study the problem of differentially-private learning of low dimensional manifolds embedded in high dimensional spaces. The problems one faces in learning in high dimensional spaces are compounded in a differentially-private learning. We achieve the dual goals of learning the manifold while maintaining the privacy of the dataset by constructing a differentially-private data structure that adapts to the doubling dimension of the dataset. Our differentially-private manifold learning algorithm extends random projection trees of Dasgupta and Freund. A naive construction of differentially-private random projection trees could involve queries with high global sensitivity that would affect the usefulness of the trees. Instead, we present an alternate way of constructing differentially-private random projection trees that uses low sensitivity queries that are precise enough for learning the low dimensional manifolds. We prove that the size of the tree depends only on the doubling dimension of the dataset and not its extrinsic dimension.	algorithm;data structure;embedded system;information privacy;nonlinear dimensionality reduction;period-doubling bifurcation;random projection	Anna Choromanska;Krzysztof Choromanski;Geetha Jagannathan;Claire Monteleoni	2016	Theor. Comput. Sci.	10.1016/j.tcs.2015.10.039	combinatorics;discrete mathematics;topology;computer science;mathematics;differential privacy	Theory	20.577517857269864	-33.02155194711741	195272
6eb0ca83c28f2c4d7563122be9d3d8adb3e54fc5	halfspace learning, linear programming, and nonmalicious distributions	algorithm analysis;learning;computational theory;aprendizaje;analysis of algorithms;apprentissage;programacion lineal;linear programming;programmation lineaire;linear program;analyse algorithme;computational learning theory;analisis algoritmo	We study the application of a linear programming algorithm due to Vaidya to the problem of learning halfspaces in Baum’s nonmalicious distribution model. We prove that, in n dimensions, this algorithm learns up to c accuracy with probability 1 E in O((~*/E) log’(n/s) + n3.3R log(n/s)) time. For many combinations of the parameters, this compares favorably with the best known bound for the perceptron algorithm, which is 0(n2/c5).	baum–welch algorithm;linear programming;perceptron	Philip M. Long	1994	Inf. Process. Lett.	10.1016/0020-0190(94)90003-5	combinatorics;computer science;linear programming;analysis of algorithms;mathematics;algorithm	Theory	19.92170081250817	-31.106379295068443	195431
aeeeccf925e0eea03f533dd3592ab1d110ab0b3c	cubic regularization with momentum for nonconvex optimization		Momentum is a popular technique to accelerate the convergence in practical training, and its impact on convergence guarantee has been well-studied for first-order algorithms. However, such a successful acceleration technique has not yet been proposed for second-order algorithms in nonconvex optimization. In this paper, we apply the momentum scheme to cubic regularized (CR) Newton’s method and explore the potential for acceleration. Our numerical experiments on various nonconvex optimization problems demonstrate that the momentum scheme can substantially facilitate the convergence of cubic regularization, and perform even better than the Nesterov’s acceleration scheme for CR. Theoretically, we prove that CR under momentum achieves the best possible convergence rate to a secondorder stationary point for nonconvex optimization. Moreover, we study the proposed algorithm for solving problems satisfying an error bound condition and establish a local quadratic convergence rate. Then, particularly for finite-sum problems, we show that the proposed algorithm can allow computational inexactness that reduces the overall sample complexity without degrading the convergence rate.	algorithm;computation;cubic function;experiment;first-order predicate;manifold regularization;mathematical optimization;newton;newton's method;numerical analysis;rate of convergence;sample complexity;stationary process	Zhe Wang;Yi Zhou;Yingbin Liang;Guanghui Lan	2018	CoRR		rate of convergence;mathematical optimization;acceleration;mathematics;sample complexity;regularization (mathematics);stationary point;optimization problem;momentum;convergence (routing)	ML	24.20688818285268	-33.72673453770773	195708
3d56d7c50b291e1c473d2134ea378def7f216ee9	sequential learning of classifiers for structured prediction problems	independent learning;structure prediction	Many classification problems with structured outputs can be regarded as a set of interrelated sub-problems where constraints dictate valid variable assignments. The standard approaches to these problems include either independent learning of individual classifiers for each of the sub-problems or joint learning of the entire set of classifiers with the constraints enforced during learning. We propose an intermediate approach where we learn these classifiers in a sequence using previously learned classifiers to guide learning of the next classifier by enforcing constraints between their outputs. We provide a theoretical motivation to explain why this learning protocol is expected to outperform both alternatives when individual problems have different ‘complexity’. This analysis motivates an algorithm for choosing a preferred order of classifier learning. We evaluate our technique on artificial experiments and on the entity and relation identification problem where the proposed method outperforms both joint and independent learning.	algorithm;bootstrapping (statistics);computer vision;experiment;ibm notes;natural language processing;structured prediction;switzerland;synthetic data	Dan Roth;Kevin Small;Ivan Titov	2009			semi-supervised learning;online machine learning;machine learning;pattern recognition;data mining;structured prediction;generalization error	ML	17.64180138922385	-37.156180063473336	196350
0a5c184532fb42286224541fb89bc7ae028f3e29	stochastic weighted function norm regularization		Deep neural networks (DNNs) have become increasingly important due to their excellent empirical performance on a wide range of problems. However, regularization is generally achieved by indirect means, largely due to the complex set of functions defined by a network and the difficulty in measuring function complexity. There exists no method in the literature for additive regularization based on a norm of the function, as is classically considered in statistical learning theory. In this work, we propose sampling-based approximations to weighted function norms as regularizers for deep neural networks. We provide, to the best of our knowledge, the first proof in the literature of the NP-hardness of computing function norms of DNNs, motivating the necessity of a stochastic optimization strategy. Based on our proposed regularization scheme, stability-based bounds yield a O(N− 1 2 ) generalization error for our proposed regularizer when applied to convex function sets. We demonstrate broad conditions for the convergence of stochastic gradient descent on our objective, including for non-convex function sets such as those defined by DNNs. Finally, we empirically validate the improved performance of the proposed regularization strategy for both convex function sets as well as DNNs on real-world classification and segmentation tasks.	approximation;artificial neural network;convex function;deep learning;generalization error;machine learning;mathematical optimization;matrix regularization;np-hardness;sampling (signal processing);statistical learning theory;stochastic gradient descent;stochastic optimization;utility functions on indivisible goods	Amal Rannen Triki;Maxim Berman;Matthew B. Blaschko	2017	CoRR		statistical learning theory;proximal gradient methods for learning;stochastic optimization;regularization perspectives on support vector machines;artificial neural network;regularization (mathematics);stochastic gradient descent;mathematics;mathematical optimization;convex function	ML	22.58730277658426	-32.44891303379858	196797
bca78f53f12d7a56a63cd5c01bc191acdf469d59	on computability of pattern recognition problems	analisis estadistico;data compression;computability;intelligence artificielle;computational method;probabilistic approach;etiquetage;etiquetaje;computer sciences;statistical analysis;enfoque probabilista;approche probabiliste;calculabilite;analyse statistique;pattern recognition;labelling;learning problems;artificial intelligence;compresion dato;inteligencia artificial;reconnaissance forme;reconocimiento patron;sample complexity;vc dimension;compression donnee;calculabilidad	In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if consider only computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples. The task of pattern recognition is considered in conjunction with another learning problem — data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.	approximation algorithm;computability;computable function;data compression;pattern recognition;sample complexity;vc dimension	Daniil Ryabko	2005		10.1007/11564089_13	data compression;vc dimension;computer science;artificial intelligence;machine learning;mathematics;computability;computable function;computable number;algorithm;statistics;computable analysis	Vision	19.9659282424473	-31.424356793290478	197206
008f6dbaad60988a846ee92805bdf3f51de1d088	composing fisher kernels from deep neural models		This chapter is not aimed at replacing literature on introduction to kernel methods or Fisher kernels. There are some excellent text books and tutorials on the topic by Schölkopf and Smola (Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT Press (2002), [1]), Shawe-Taylor, Cristianini (Kernel methods for pattern analysis. Cambridge University Press (2004), [2]), Kung (Kernel methods and machine learning. Cambridge University Press, Princeton University (2014), [3])). In contrast to formal theory and proofs, this chapter briefly describes the evolution of kernel methods and the heuristics andmethods that have helped kernel methods evolve over the past many years for solving the challenges faced by current machine learning practitioners and applied scientists.	book;fisher information;heuristic (computer science);kernel method;machine learning;mathematical optimization;pattern recognition;support vector machine	Tayyaba Azim;Sarah Ahmed	2018		10.1007/978-3-319-98524-4		ML	20.36299162020249	-36.295811272061584	197452
b06f06394767b46b7d81297b3451be7e9af4c6a8	cross-validation for binary classification by real-valued functions: theoretical analysis	supervised learning;pac learning;theoretical analysis;neural net work;value function;binary classification;cross validation;error estimate	This paper concerns the use of real-valued functions for binary classification problems. Previous work in this area has concentrated on using as an error estimate the ‘resubstitution’ error (that is, the empirical error of a classifier on the training sample) or its derivatives. However, in practice, cross-validation and related techniques are more popular. Here, we devise new holdout and cross-validation estimators for the case where real-valued functions are used as classifiers, and we analyse theoretically the accuracy of these.	binary classification;cross-validation (statistics);statistical classification;test set	Martin Anthony;Sean B. Holden	1998		10.1145/279943.279987	binary classification;computer science;machine learning;pattern recognition;data mining;bellman equation;supervised learning;probably approximately correct learning;cross-validation;generalization error	ML	20.35211262016591	-35.06927995567355	198550
e5d3f93f9c0cd93c50ca8e6a802da124b03774e5	fast committee-based structure learning		Current methods for causal structure learning tend to be computationally intensive or intractable for large datasets. Some recent approaches have speeded up the process by first making hard decisions about the set of parents and children for each variable, in order to break large-scale problems into sets of tractable local neighbourhoods. We use this principle in order to apply a structure learning committee for orientating edges between variables. We find that a combination of weak structure learners can be effective in recovering causal dependencies. Though such a formulation would be intractable for large problems at the global level, we show that it can run quickly when processing local neighbourhoods in turn. Experimental results show that this localized, committee-based approach has advantages over standard causal discovery algorithms both in terms of speed and accuracy.	algorithm;causal filter;cobham's thesis	Ernest Mwebaze;John A. Quinn	2010			artificial intelligence;machine learning;data mining;mathematics	ML	17.534488018681202	-36.505101236504814	198571
