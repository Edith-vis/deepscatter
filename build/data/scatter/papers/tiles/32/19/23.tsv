id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
35acbb03c025b45aa7b3428b63a77e44b45f25d6	duty cycle shift under static/dynamic aging in 28nm hk-mg technology	stress;size 28 nm duty cycle shift static dynamic aging hk mg technology bias temperature instability logic circuit power efficient technique clock gating dynamic voltage scaling asymmetric aging synchronous circuit data path delay nbti pbti latch based circuit calibration chc data static stress condition;degradation;reliability;nbti;aging;duty cycle shift;synchronization;pbti;integrated circuit modeling;duty cycle shift aging nbti pbti;stress aging degradation integrated circuit modeling synchronization data models reliability;stress analysis ageing calibration failure analysis logic circuits negative bias temperature instability;data models	Aging due to bias-temperature-instability (BTI) is the dominant cause of functional failure in large scale logic circuits. Power efficient techniques such as clock gating or dynamic voltage scaling exacerbate the problem of asymmetric aging. Traditional analysis on synchronous circuits focuses on shift in data path delay and neglects the change in duty cycle. This work highlights the impact of NBTI and PBTI at advanced technology node on duty cycle shift which is important for edge triggered designs, such as latch based circuits. The contributions of this work are: (1) characterization, decoupling and model calibration of NBTI, PBTI and CHC data at 28nm HK-MG technology; (2) demonstration of monotonic shift of duty cycle under static stress condition and non-monotonic shift under dynamic stress, in which duty cycle converges to 50%. Additional PBTI component at 28nm HK-MG causes faster shift in duty cycle compared to conventional NBTI aging; (3) the sensitivity of long-term aging to the ratio between static and dynamic stress conditions. With PBTI, duty cycle shift is effectively reduced by dynamic stress.	btrieve;clock gating;coupling (computer programming);die shrink;duty cycle;dynamic voltage scaling;failure analysis;image scaling;logic gate;mg (editor);negative-bias temperature instability;non-monotonic logic;semiconductor device fabrication;simulation	Ketul Sutaria;Pengpeng Ren;Abinash Mohanty;Xixiang Feng;Runsheng Wang;Ru Huang;Yu Cao	2015	2015 IEEE International Reliability Physics Symposium	10.1109/IRPS.2015.7112785	embedded system;electronic engineering;real-time computing;engineering	EDA	20.43129049874683	58.836085435407924	135140
b3c29453cbb81e5a9229e4b44fd62e62807414c2	optimization of advanced mos technologies for narrow distribution of circuit performance	ring oscillators;mosfet circuits;simulation ordinateur;analytical models;modelizacion;substrate doping concentration;concepcion asistida;cmos integrated circuits;oscillations;computer aided design;optimisation;low voltage application advanced mos technologies circuit performance distribution technological parameters optimisation analytical mosfet model circuit simulations effective channel length oxide thickness substrate doping concentration narrow delay distribution;circuit performance distribution;metodologia;optimizacion;integrated circuit;circuit mos;circuit simulations;semiconductor process modeling;optimal method;ring oscillator;narrow delay distribution;integrated circuit design circuit optimisation circuit analysis computing cmos integrated circuits delays parameter estimation monte carlo methods mosfet semiconductor device models circuit cad;circuito integrado;circuito mos;indexing terms;methodologie;network analysis;oxide thickness;modelisation;doping;optimization methods semiconductor process modeling circuit optimization mosfet circuits analytical models circuit simulation doping ring oscillators delay low voltage;integrated circuit design;circuit simulation;low voltage;semiconductor device modeling;semiconductor device models;analyse performance;performance analysis;estimacion parametro;conception assistee;mosfet;optimization;circuit cad;simulacion computadora;parameter estimation;estimation parametre;mos circuit;analytical mosfet model;methodology;circuit optimisation;advanced mos technologies;low voltage application;circuit analysis computing;analyse circuit;technological parameters optimisation;modeling;computer simulation;monte carlo methods;analisis circuito;circuit integre;delays;circuit optimization;effective channel length;optimization methods;analisis eficacia	A methodology is investigated to optimize technological parameters to reduce distribution of circuit performance. Major attention is paid to treat all characteristics from device level to circuit level on one basis, namely technological parameters. The key point of the whole procedure is the quality of the analytical MOSFET model used for circuit simulations. Due to the physical description of our model, only three independent technological parameters have to be studied, namely the effective channel length, the oxide thickness, and the substrate doping concentration. These values are available directly or indirectly from measurements. The methodology is applied to optimize the oxide thickness for a 23-stage ringoscillator to get narrow delay distribution for low voltage application.	doping (semiconductor);simulation;thickness (graph theory)	Heinz Hoenigschmid;Mitiko Miura-Manausch;Odin Prigge;Alexander Rahm;Dominique Savignac	1997	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.573834	computer simulation;control engineering;electronic engineering;semiconductor device modeling;systems modeling;index term;network analysis;computer science;engineering;electrical engineering;integrated circuit;ring oscillator;computer aided design;methodology;doping;low voltage;estimation theory;oscillation;cmos;statistics;monte carlo method;integrated circuit design	EDA	23.537847722337037	56.872377346979796	135287
dc72805090d36cbecb3f3b70296c450f90b90b72	constant power consumption design of novel differential logic gate for immunity against differential power analysis		Differential power analysis (DPA) method is frequently used for the non-invasive side-channel attack to hack into the system. This study proposes a novel DPA immune design of basic gates, which show the dense distribution of autocorrelation and strong salience strength around 60%. The design has a highly regular structure with exactly similar evaluation path for both differential outputs, AND-NAND, and OR-NOR which can be easily extended for n n-bit inputs. The design effort is minimal as the structure is such that AND-NAND design can be used to obtain OR-NOR function by just changing the placement of inputs. These gates have 0.46× less propagation delay, and 3.7× higher power consumption in comparison to other published work. The designs are simulated using Cadence tool with TowerJazz CMOS 180 nm technology with a power supply of 1.8 V.		Harjap Saini;Anu Gupta	2019	IET Circuits, Devices & Systems	10.1049/iet-cds.2018.5093	electronic engineering;power analysis;propagation delay;mathematics;autocorrelation;cadence;logic gate;cmos	EDA	19.986856045014335	55.73101512653277	135726
754e2dc61eb05b239f35a0e8c14fd8e75c3e3aa8	design methodology for the s/390 parallel enterprise server g4 microprocessors	verification;modelizacion;microprocessor;synthese circuit;concepcion circuito;interconnection;metodologia;circuit design;serveur informatique;ruido;methodologie;interconexion;modelisation;bruit;interconnexion;servidor informatico;sintesis circuito;conception circuit;microprocesseur;systeme parallele;parallel system;verificacion;methodology;modeling;microprocesador;circuit synthesis;sistema paralelo;noise;computer server;design methodology;timing	ions of the global environment are brought down to the macro level to guide macro-level implementation. Shadow This is a representation of the global wires overlaying a macro that is used to guide macro physical design and for macro extraction. Timing assertions This is information on the global timing at macro interfaces-arrival times with phase tags on inputs, required arrival times with phase tags on outputs, primary input resistances, and primary output	global variable;microprocessor;physical design (electronics)	Kenneth L. Shepard;Sean M. Carey;Ee Kin Cho;Brian W. Curran;Robert F. Hatch;Dale E. Hoffman;Scott A. McCabe;Gregory A. Northrop;A. E. Seigler	1997	IBM Journal of Research and Development	10.1147/rd.414.0515	embedded system;electronic engineering;real-time computing;verification;systems modeling;probabilistic design;design methods;telecommunications;computer science;engineering;noise;interconnection;circuit design;methodology;server	EDA	18.323480096058137	54.79638215342942	136363
dae7d6abb73874e5d1730821bb0856b51d591309	low voltage srams and the scalability of the 9t supply feedback sram	random access memory low voltage transistors predictive models semiconductor device modeling scalability nanoscale devices;nanotechnology;integrated circuit design;cmos logic circuits;size 22 nm low voltage sram 9t supply feedback sram scalability minimum energy operation digital circuits subthreshold region process variation device mismatch nanoscale technology strong inversion operation supply feedback mechanism cmos technology size 40 nm;low power electronics;sram chips cmos logic circuits integrated circuit design low power electronics nanotechnology;sram chips	Recent research has shown that minimum energy operation of digital circuits is in the sub-threshold region, and a good trade-off between power and performance can be achieved through operation at near threshold supply voltages. However, due to process variations and device mismatch at nanoscale technology nodes, voltage scaling of standard SRAMs is limited to strong-inversion operation. One of the techniques for enabling operation at low voltages is implementation of a Supply Feedback mechanism that internally weakens the pull-up current during write operations. This concept was recently implemented in a 9T Supply Feedback SRAM (SF-SRAM) cell, fabricated and successfully tested in a 40nm CMOS technology. In this paper, we review existing low voltage SRAM solutions, overview the SF-SRAM cell, and show its scalability into deep nanoscale technologies by using the 22nm predictive model.	cmos;cell (microprocessor);digital electronics;dynamic voltage scaling;image scaling;scalability;static random-access memory	Janna Mezhibovsky;Adam Teman;Alexander Fish	2011	2011 IEEE International SOC Conference	10.1109/SOCC.2011.6085135	electronic engineering;real-time computing;computer science;engineering;electrical engineering;operating system;low-power electronics;integrated circuit design	EDA	18.13238247910933	58.98092936912788	136799
b238a2d80f2e42390d44d863be5772ff91a6483f	g-vector: a new model for glitch analysis in logic circuits	switching activity;power estimation;logic circuits;synthesis;glitches;power consumption	One of the major factors which contribute to the power consumption in CMOS combinational logic circuits is the switching activities in the circuits. Many of such switching activities are due to spurious pulses, called glitches. In this paper, we propose a new model for describing signals that contain glitches, called G-vector. Unlike the previous works in which their primary concern is modeling the propagation of glitches to count the number of glitches in the circuits, our G-vector provides a general, but effective model for generation, propagation and elimination of glitches, enabling us to not only count the number of glitches but also locate the glitches so that such information can be utilized by system tools for the reduction of the number of glitches in the circuits. We provide a set of experimental results to demonstrate the effectiveness of our model.	glitch	Ki-Seok Chung;Taewhan Kim;C. L. Liu	2001	VLSI Signal Processing	10.1023/A:1008139232134	real-time computing;logic gate;computer science;glitch;algorithm	EDA	19.70769220857622	55.69849461224955	136898
60bd6aba6b57ef67a837e769286bcb307dee73d8	optimizing the operating voltage of tunnel fet-based sram arrays equipped with read/write assist circuitry	read and write assist techniques;read and write assist techniques sram cells tfet fdsoi negative gnd technique;sram cells;negative gnd technique;fdsoi;uhf transistors field effect transistors low power electronics microwave transistors power supply circuits sram chips tunnel transistors;tfet;tfets sram cells power demand stability analysis performance evaluation;frequency 4 55 ghz tunnel fet sram arrays read write assist circuitry minimum operating voltage memory arrays tfet sram cells spice simulations tfet devices power supply voltage levels static noise margin tfet structures long read access latency ultralow supply voltages negative gnd read assist energy consumption voltage 200 mv voltage 300 mv frequency 1 32 ghz	This paper deals with obtaining the minimum operating voltage of memory arrays based on TFET SRAM cells. First, we compare the I-V characteristics of two TFETs and one FDSOI using SPICE simulations based on 20nm technology models. The results reveal that TFET devices exhibit high ON/OFF current ratios at different power supply voltage levels. This observation suggests a higher stability for SRAM cells based on these devices. Next, the characteristics of 6T SRAM cells implemented using minimum sized transistors based on these three device structures are compared. The comparison, which considers two TFET cell structures, i.e., inward and outward SRAMs, is performed at different supply voltages. The results for the hold static noise margin show that at low supply voltages (i.e., below 300mV), the FDSOI SRAM cell cannot hold data whereas both the inward and outward structures of TFET have acceptable noise margins at all supply voltages. Among the two TFET structures, the outward cell is selected because of higher speed especially for the write operation. TFET SRAMs suffer from long read access latency at ultra-low supply voltages (e.g., 150mV). The problem, however, may be overcome by using the negative GND read-assist technique. The results show that for a 32×32 TFET outward SRAM array, the minimum energy consumption (energy-delay product) may be achieved at the supply voltage of 200mV (300mV) with 1.32GHz (4.55GHz) as the read access frequency	cell (microprocessor);direct inward dial;electronic circuit;noise margin;optimizing compiler;power supply;spice;simulation;static random-access memory;transistor	Hassan Afzali-Kusha;Alireza Shafaei;Massoud Pedram	2016	2016 International Great Lakes Symposium on VLSI (GLSVLSI)	10.1145/2902961.2903031	embedded system;electronic engineering;engineering;electrical engineering	EDA	17.736361012668784	58.82406881570043	136940
c6607de55baa385a92210d4e7db549c34e05fc90	layout-based soft error rate estimation framework considering multiple transient faults—from device to circuit level	databases;reliability;circuit faults;layout based analysis;propagation system;integrated circuit layout;single transient faults;logic design;masking mechanisms;high energy neutrons;multiple transient faults mtfs;generation system;layout;transient analysis;layout based soft error rate estimation framework masking mechanisms propagation system voltage transformation charge collection generation system netlist based stf analysis single transient faults mtf multiple transient faults nuclear reactions high energy neutrons ser;ser;neutrons;error analysis;multiple transient faults;voltage transformation;netlist based stf analysis;mtf;estimation;layout based soft error rate estimation framework;radiation hardening electronics cmos logic circuits integrated circuit layout logic design;cmos logic circuits;radiation hardening electronics;charge collection;reliability multiple transient faults soft error layout based analysis;nuclear reactions;soft error;transient analysis circuit faults neutrons error analysis estimation layout databases	This paper investigated the soft errors caused by particle strikes, such as high-energy neutrons, extending beyond the deep submicrometer era. Considering the structure of the layout and resulting nuclear reactions, multiple transient faults (MTFs) tend to be induced more frequently than do single transient faults (STFs), due to the effects of technology scaling. This means that the soft error rates (SER) are beyond traditional netlist-based STF analysis, which can result in serious mis-estimations. This paper proposes a layout-based soft error estimation framework, which takes into account MTFs from the device level to the circuit level. This framework comprises two systems: 1) generation and 2) propagation. In the generation system, transient faults are modeled through nuclear reactions, charge collection, and voltage transformation at the device level. The propagation system abstracts these effects from the device level to the circuit level, taking into account three masking mechanisms associated with the propagation of transient faults. Experiment results demonstrate that the SER can be underestimated by an average of 15.72% if only single (rather than multiple) transient faults are taken into account. Our results indicate that netlist-based analysis for the estimation of SERs is no longer sufficient, due to the overwhelming influence of the structural layout. Thus, using benchmark c432, a tighter layout will result in an SER 34% higher than that generated in a looser layout.	benchmark (computing);computation;experiment;glitch;image scaling;loose coupling;netlist;soft error;software propagation	Hsuan-Ming Huang;Charles H.-P. Wen	2016	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2474355	layout;embedded system;estimation;electronic engineering;logic synthesis;real-time computing;soft error;computer science;engineering;electrical engineering;nuclear reaction;reliability;integrated circuit layout;neutron;statistics	EDA	21.405338684123414	57.09679564097417	137104
69590fb0d57678e79fd5a916bdbe22b9fbd30441	stt-ram cell design optimization for persistent and non-persistent error rate reduction: a statistical design view	mtj process variation;process variation;conventional memory technology;non-persistent error rate reduction;next generation memory technology;features fast access time;stt-ram cell design;good cmos process compatibility;spintransfer torque random access;statistical design view;spin-transfer torque random access;stt-ram cell;stt-ram cell operation;stt-ram cell design optimization;combinational circuit;electronics industry;error rate;design optimization;integrated circuit design;statistical analysis	The rapidly increased demands for memory in electronic industry and the significant technical scaling challenges of all conventional memory technologies motivated the researches on the next generation memory technology. As one promising candidate, spintransfer torque random access memory (STT-RAM) features fast access time, high density, non-volatility, and good CMOS process compatibility. However, like all other nano-scale devices, the performance and reliability of STT-RAM cells are severely affected by process variations, intrinsic device operating uncertainties and environmental fluctuations. In this work, we systematically analyze the impacts of CMOS and MTJ process variations, MTJ switching uncertainties induced by thermal fluctuations and working temperature on the performance and reliability of STT-RAM cells. A combined circuit and magnetic simulation platform is also established to quantitatively analyze the persistent and non-persistent error rates during the STT-RAM cell operations. Finally, an optimization flow and its effectiveness are depicted by using some STT-RAM cell designs as case study.	access time;cmos;environmental resource management;gnu nano;image scaling;mathematical optimization;non-volatile memory;persistent data structure;random access;random-access memory;simulation;volatility	Yaojun Zhang;Xiaobin Wang;Yiran Chen	2011	2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		embedded system;electronic engineering;real-time computing;multidisciplinary design optimization;word error rate;computer science;engineering;electrical engineering;combinational logic;process variation;algorithm;statistics;integrated circuit design	EDA	19.072336170498758	60.02994433691107	137760
3ee6ac2f53678b658acf43adf9542e838838c43e	sleep transistor design in 28nm cmos technology	cmos integrated circuits;mosfet cmos integrated circuits;size 28 nm cmos technology transistor power performance characteristic cross corner variations back bias gate length sleep transistor design quality production design;mosfet;switches cmos integrated circuits cmos technology logic gates abstracts fingers mos devices	Significant changes in transistor's power-performance characteristic and cross-corner variations in 28nm CMOS technology prompt the need for a new look at sleep transistor design guidelines. This paper evaluated impacts of back-bias, Vt, and gate length and width on sleep transistor design quality. Recommendations were proposed for production design considerations.	and gate;cmos;die shrink;recommender system;transistor;transistor–transistor logic	Kaijian Shi	2013	2013 IEEE International SOC Conference	10.1109/SOCC.2013.6749701	transistor count;embedded system;electronic engineering;computer science;engineering;electrical engineering;pass transistor logic;integrated injection logic;cmos;bicmos	EDA	17.936924736075568	58.72239320490665	138090
ccd4d55402429dd2d6b95a222c218cc0efee8926	detectability challenges of bridge defects in finfet based logic cells	bridge defects;finfet technology;critical resistance;lateral capacitance;bridge defect criticality;test	Since 22nm technology node, FinFET technology is an attractive candidate for high-performance and power-efficient applications. This is achieved due to better channel control in FinFET devices obtained by wrapping a metal gate around a thin fin. In this paper, we investigate the detectability of bridge defects in FinFET based logic cells that make use of Middle-Of-Line (MOL) interconnections and multi-fin and multi-finger design strategies. The use of MOL to build the logic cells impacts the possible bridge defect locations and the likelihood of occurrence of the defect. Some defect locations unlikely to appear in planar CMOS now become more likely to occur due to the use of MOL. It is shown that these defects are difficult to be detected. The detectability of bridge defects has been analyzed for gates with different drive strengths and fan-in, and also extended to the different type of gates. A metric called Bridge Defect Criticality (BDC) is used to identify the most harmful bridge defects. This metric depends on the degree of detectability and likelihood of occurrence of a bridge defect. More design and/or test effort may be dedicated to those defects with higher a value of the BDC metric to improve product quality.		Freddy Forero;Jean Marc Gallière;Michel Renovell;Víctor H. Champac	2018	J. Electronic Testing	10.1007/s10836-018-5714-0	computer science;metal gate;electronic engineering;test effort;communication channel;cmos	EDA	21.37663069264372	55.70489861424503	138228
03921c7734551bcc41c0a5b6c08d38fd44be9ad4	a learning-based autoregressive model for fast transient thermal analysis of chip-multiprocessors	ar model;accuracy predictive models transient analysis correlation integrated circuit modeling fitting adaptation models;thermal management packaging;estimation theory;thermal analysis;integrated circuit;estimation method;chip multiprocessor;transient analysis;autoregressive model;fitting;accuracy;autoregressive processes;integrated circuit modeling;predictive models;prediction model;multiprocessing systems;correlation;model fitting;thermal management packaging autoregressive processes estimation theory microprocessor chips multiprocessing systems;adaptation models;microprocessor chips;thermal profile estimation method learning based autoregressive model transient thermal analysis chip multiprocessor cmp ar model transient temperature	Thermal issues have become critical roadblocks for the development of advanced chip-multiprocessors (CMPs). In this paper, we introduce a new angle to view transient thermal analysis - based on predicting thermal profile, instead of calculating it. We develop a systematic framework that can learn different thermal profiles of a CMP by using an autoregressive (AR) model. The proposed AR model can serve as a fast alternative for predicting the transient temperature of a CMP with reasonably good accuracy. Experimental results show that the proposed AR model can achieve approximately 113X speed-up over existing thermal profile estimation methods, while introducing an error of only 0.8°C on average.	autoregressive model;lasso;thermal profiling	Da-Cheng Juan;Huapeng Zhou;Diana Marculescu;Xin Li	2012	17th Asia and South Pacific Design Automation Conference	10.1109/ASPDAC.2012.6165027	embedded system;electronic engineering;real-time computing;computer science;predictive modelling;autoregressive model;statistics	EDA	22.764890885982773	59.71112283685313	138320
84ed26f74e0c5cccf17cb82e954a6e6230f5a466	test development through defect and test escape level estimation for data converters	test and post test data analysis;process variation;integrated circuit;and mixed signal;indirect cost;data analysis;mixed signal and analog test;yield loss;process model;economics of test	Testing integrated circuits (ICs) is understood as the task of filtering out defective ICs that violate data sheet specifications. The costs of this filter comprise both the direct cost of testing a device and the indirect cost of test escapes and test yield–loss. For analog and mixed-signal devices, such as data converters, traditional methods of estimating the defect and test escape levels require large sample sets of devices. This is because the defect level induced by manufacturing process variations is typically low. In this work, a model-based method of estimating defect and test escape levels is described. For this method, a small set of sample devices is sufficient, as we first derive a manufacturing process model which is then used to simulate the manufacturing of a large number of devices. These simulation results are subsequently used for the purposes of estimating the defect and test escape levels, as well as the test-related yield–loss when applying a given test. With these estimates, the quality and indirect costs of a test can be determined as a function of the test limits and guard-bands applied in production test.	datasheet;mixed-signal integrated circuit;process modeling;simulation;software bug;test case	Carsten Wegener;Michael Peter Kennedy	2006	J. Electronic Testing	10.1007/s10836-006-9457-y	reliability engineering;embedded system;electronic engineering;simulation;computer science;engineering;automatic test pattern generation;integrated circuit;test compression;process modeling;indirect costs;data analysis;process variation;statistics	SE	23.164848150186838	55.48659311780786	138620
90719d20bcf4eec604fc7675d04662b6d08f64e9	invited: optimizing device reliability effects at the intersection of physics, circuits, and architecture	stress;reliability;cross layer optimization reliability bias temperature instability hot carriers oxide breakdown electromigration;cross layer optimization;wires;chip level design device reliability effects reliability modeling cross layer optimization device reliability phenomena circuit models;electric breakdown;oxide breakdown;logic gates;integrated circuit modeling;bias temperature instability;hot carriers;electromigration;integrated circuit reliability;logic gates integrated circuit reliability integrated circuit modeling electric breakdown wires stress;transistor circuits circuit optimisation integrated circuit design integrated circuit interconnections integrated circuit modelling integrated circuit reliability	Over the years, there has been tremendous progress in developing new methods for modeling and diagnosing reliability at the level of individual transistors and interconnects. The thrust to propagate these models to higher levels of abstraction to predict the reliability of larger circuits is much more recent. This paper addresses the intersection of physics, circuits, and architecture for reliability modeling and optimization that must come together for cross-layer optimization. For various device reliability phenomena, this paper shows how physical models can be leveraged at the circuit level, or circuit models at the architecture level, to deliver composite solutions that comprehend chip-level design goals.	electrical connection;level design;mathematical optimization;optimizing compiler;principle of abstraction;reliability engineering;thrust;transistor	Deepashree Sengupta;Vivek Mishra;Sachin S. Sapatnekar	2016	2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2897937.2905016	reliability engineering;electromigration;electronic engineering;logic gate;engineering;electrical engineering;reliability;hot-carrier injection;stress;statistics	EDA	21.37833612165576	57.490449788769716	139101
bdca92e2ec271a1b874b2a6abb63c40297d9eec3	self-test and adaptation for random variations in reliability	noise injection experiments;mean time between failure;self optimization;field programmable gate array;reliability;mtbf self test random physical variations advanced electronic systems field programmable systems subsequent optimization on chip noise emulation noise injection experiments latch reliability field programmable gate arrays fpga self optimization mean time between failures;random physical variations;mtbf;subsequent optimization;coefficient of variation;reconfigurable computing;automatic testing;flip flops;latches noise flip flops built in self test reliability field programmable gate arrays optimization;fpga;latch reliability;mean time between failures;self adaptation;chip;variations;logic testing automatic testing circuit optimisation field programmable gate arrays flip flops integrated circuit reliability;built in self test;transient faults;logic testing;fpgas;reconfigurable computing self adaptation self test self optimization fpgas variations transient faults;transient fault;optimization;latches;field programmable gate arrays;integrated circuit reliability;field programmable systems;circuit optimisation;self test;advanced electronic systems;on chip noise emulation;noise	Random physical variations and noise are growing challenges for advanced electronic systems. Field programmable systems can, in principle, adapt to these phenomena, but two main problems must be addressed: how to efficiently characterize random variations and how to perform subsequent optimization. This paper addresses both of these questions. First, an approach to self-test is presented that uses on-chip noise emulation to quickly characterize some of the hidden variations in latches. Our noise-injection experiments demonstrate that there can be significant spreads in latch reliability even with current 65nm field-programmable gate arrays (FPGAs). We detected coefficients of variation as high as 77%. Second, we propose an approach to self-optimization using local resource swapping. Experiments on two FPGAs show improvements in mean-time-between-failures (MTBF) of up to 60%.	coefficient;emulator;experiment;field-programmability;field-programmable gate array;image noise;mathematical optimization;mean time between failures;paging	Kenneth M. Zick;John P. Hayes	2010	2010 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2010.47	embedded system;real-time computing;mean time between failures;computer science;field-programmable gate array	EDA	21.64033271780041	57.32084302301302	139557
4a70e30e88d446eac50665bb226d1d48d752fb8c	5v tolerant power clamps for mixed-voltage ic's in 65nm 2.5v salicided cmos technology	power integrated circuits clamps cmos integrated circuits electrostatic discharge mixed analogue digital integrated circuits mosfet;cmos integrated circuits;voltage 2 5 v salicided cmos technology mixed voltage ic power pins esd protection salicided transistors cascaded power clamp technique effective power to ground cascade esd clamp circuits dual well processes low impedance path ground lines hbm level voltage 5 v size 65 nm;power supply;electrostatic discharge;electrostatic discharge clamps transistors integrated circuits simulation resistance logic gates;mixed analogue digital integrated circuits;mosfet;clamps;power integrated circuits	A cascaded power clamp technique is proposed for mixed-voltage IC's power pins ESD protection using 65nm 2.5V salicided transistors for dual well processes. The effective power-to-ground cascade ESD clamp circuits for mixed 2.5V and 5V powers have been designed to provide low-impedance path between mixed powers and ground lines of IC during the ESD stress. The stacked power clamp also can be used only for 5V power supply without any electrical overstress on transistors. The discussed ESD protection method is easy simulate able and allow having predictable HBM ESD level. The HBM level of this clamp is up to 5kV with allowable layout area.	cmos;characteristic impedance;clamping (graphics);high bandwidth memory;power supply;simulation;transistor	Vazgen Melikyan;Karen Sahakyan;Armen Nazaryan	2010	2010 East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2010.5742097	embedded system;electronic engineering;engineering;electrical engineering	EDA	18.934261661009156	56.52176422201907	140123
c86f40da8e235e95f96144d1b7957fc50171e347	sram stability design comprehending 14nm finfet reliability	stress;reliability;finfet reliability;vmin;aging finfets stress sram cells reliability;semiconductor device reliability;statistical sram cell design;z score method;aging;sram chips mosfet semiconductor device reliability;sram stability design;size 14 nm vmin behavior z score method statistical sram cell design end of life aging finfet reliability sram stability design;finfets;size 14 nm;sram cells;bias temperature instability sram htol vmin;bias temperature instability;mosfet;sram;end of life aging;vmin behavior;htol;sram chips	Importance of low voltage operation of SRAM in mobile application is ever increasing for longer battery life. SRAM occupies a significant portion of the total area and power for the SOC ICs (>10-30MByte used in AP/CPUs). For the operation of SRAM at low voltage, proper noise margin for read, disturb and write operation is important since noise margin reduces with technology scaling and low voltage operation. Previously, we presented a method on SRAM Vmin design and characterization before and after High Temperature Operating Life (HTOL) stress test [1-3]. In this work, we extend our method to account for end-of-life aging into statistical SRAM cell design with Z-score method on 14nm FinFET technology. Excellent Vmin behavior at both time0 and EOL satisfying 10yrs was demonstrated at the product level.	btrieve;cell (microprocessor);central processing unit;doping (semiconductor);end-of-life (product);flash memory;high-temperature operating life;image scaling;mobile app;noise margin;spatial variability;static random-access memory;system on a chip	Choelhwyi Bae;Sangwoo Pae;Cheong-sik Yu;Kangjung Kim;Yongshik Kim;Jongwoo Park	2015	2015 IEEE International Reliability Physics Symposium	10.1109/IRPS.2015.7112815	reliability engineering;electronic engineering;real-time computing;engineering	EDA	19.561238565682665	59.31600679057526	140593
7453199de8f91bb3a6053cbdd9f9702b4eb58e6c	a double-edge implicit-pulsed level convert flip-flop	flip flops;trigger circuits flip flops power consumption delay systems;flip flops energy consumption delay voltage clocks very large scale integration circuits switches master slave power dissipation;power dissipation;delay systems;trigger circuits;power consumption;voltage scaling;flip flop;power consumption double edge flip flop implicit pulsed level convert flip flop clustered voltage scaling systems power dissipation efficient level converter power overhead delay overhead circuit techniques conditional discharge overhead reduction level conversion flip flops double edge triggering	Clustered voltage scaling (CVS) systems is a technique to decrease power dissipation. One of the design challenges in CVS is the efficient level converter with fewer overheads in power and delay. In this paper, we propose a novel implicit-pulsed level convert flip-flop that uses circuit techniques such as conditional discharge to reduce the overhead incurred with level conversion flip-flops (LCFF). Double-edge triggering is also used to further decrease the power consumption. In view of power, the new LCFF outperform previous published designs about 18%-56% and exhibit smaller delay and PDP.	concurrent versions system;discharger;dynamic voltage scaling;flops;flip-flop (electronics);image scaling;overhead (computing);programmed data processor	Peiyi Zhao;Pradeep Golconda;C. Archana;Magdy A. Bayoumi	2004	IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2004.1339521	embedded system;electronic engineering;real-time computing;engineering	Arch	17.592872627445114	57.088911106402975	140761
3ebd93c63cf4d2dab128fd40f85c992c638bda42	copula models of correlation: a dram case study	and fault tolerance;reliability;dynamic random access memory dram;performance and reliability;memory structures;semiconductor memories;size 65 nm dynamic random access memory dram copula models variable bit retention time retention times financial modeling producer oriented figures of merit customer oriented figures of merit array sizes fault tolerance schemes;testing;performance analysis and design aids;yield models integrated circuits dynamic random access memory dram testing fault tolerance reliability;fault tolerance;yield models;data models arrays random access memory computational modeling correlation mathematical model gold;fault tolerance dram chips;dram;integrated circuits;hardware	Variable bit retention time observed in a 65-nm dynamic random access memory (DRAM) case study will cause miscorrelation between retention times occurring in Test and Use. Conventional multivariate normal statistics cannot adequately model this miscorrelation. A more general copula-based modeling approach, widely used in financial and actuarial modeling, solves this problem. The DRAM case study shows by example how to use copula models in test applications. The method includes acquiring data using a test vehicle, fitting the data to a copula-based statistical model, and then using the model to compute producer- and customer-oriented figures of merit of a product, different from the test vehicle. Different array sizes, fault tolerance schemes, test coverage, end-use (datasheet), and test condition specifications of the product are modeled.	clutter;consistency model;datasheet;dimensionality reduction;dynamic random-access memory;fault coverage;fault tolerance;marginal model;model selection;random access;semiconductor research corporation;statistical model;test case;truncation;yet another	C. Glenn Shirley;W. Robert Daasch	2014	IEEE Transactions on Computers	10.1109/TC.2013.129	fault tolerance;parallel computing;real-time computing;computer science;reliability;software testing;dram;statistics	SE	22.63331337284465	58.92177852741045	140769
20a0012b18121215a95a096cf23232d47d191224	fpga power reduction by guarded evaluation	switching activity;field programmable gate array;logic synthesis;digital design;graph representation;fpgas;optimization;low power design;power reduction;power consumption;field programmable gate arrays;technology mapping;power	"""Guarded evaluation is a power reduction technique that involves identifying sub-circuits (within a larger circuit) whose inputs can be held constant (guarded) at specific times during circuit operation, thereby reducing switching activity and lowering dynamic power. The concept is rooted in the property that under certain conditions, some signals within digital designs are not """"observable"""" at design outputs, making the circuitry that generates such signals a candidate for guarding. Guarded evaluation has been demonstrated successfully for custom ASICs; in this paper, we apply the technique to FPGAs. In ASICs, guarded evaluation entails adding additional hardware to the design, increasing silicon area and cost. Here, we apply the technique in a way that imposes minimal area overhead by leveraging existing unused circuitry within the FPGA. The primary challenge in guarded evaluation is in determining the specific conditions under which a sub-circuit's inputs can be held constant without impacting the larger circuit's functional correctness. We propose a simple solution to this problem based on discovering """"non-inverting paths"""" in the circuit's AND-inverter graph representation. Experimental results show that guarded evaluation can reduce switching activity by 22%, on average, and can reduce power consumption in the FPGA interconnect by 14%."""	application-specific integrated circuit;correctness (computer science);electronic circuit;field-programmable gate array;graph (abstract data type);observable;overhead (computing);power inverter	Jason Helge Anderson;Chirag Ravishankar	2010		10.1145/1723112.1723141	embedded system;parallel computing;real-time computing;computer science;field-programmable gate array	EDA	18.853303723235094	56.665122250933734	140787
692e499d003d43f28ca36c0d2ba5642c19e9fce3	efficient multilevel formal analysis and estimation of design vulnerability to single event transients	statistical analysis adders error analysis markov processes radiation hardening electronics;logic gates analytical models probabilistic logic probability markov processes adders adaptation models;mdp models multilevel formal analysis single event transients set ultra deep submicron technology soft errors error analysis rasvas hierarchical statistical method gate level propagation tables rtl markov decision process models	The progressive shrinking of device size in advanced technologies leads to miniaturization and performance improvements. However, ultra-deep sub-micron technologies are more vulnerable to soft errors. Error analysis of a complex system with a sufficiently large sample of vulnerable nodes takes a large amount of time. In this paper we propose RASVAS, a hierarchical statistical method to model, analyze, and estimate the behavior of a system in the presence of Single Event Transients (SETs) modeled at different abstraction levels. Gate level propagation tables are developed to abstract SET propagation conditions and probabilities from gate level models. At RTL, these tables are utilized to model the underlying probabilistic behavior as Markov Decision Process (MDP) models. Experimental results demonstrate that RASVAS is orders of magnitude faster than contemporary techniques and also handle designs as large as 256-bit adders while maintaining accuracy.	abstraction layer;complex system;fault injection;formal methods;markov chain;markov decision process;model checking;out of memory;soft error;software propagation;speedup;statistical model;unit propagation	Ghaith Bany Hamad;Otmane Aït Mohamed;Yvon Savaria	2015	2015 IEEE 21st International On-Line Testing Symposium (IOLTS)	10.1109/IOLTS.2015.7229818	reliability engineering;electronic engineering;real-time computing;computer science;theoretical computer science;algorithm;statistics	Embedded	22.390013588069696	58.497825806804904	141452
20428c85b66c79c638336d883ff83e698c8c5260	an integrated approach to realistic worst-case design optimization of mos analog circuits	mos integrated circuits;analogue circuits;circuit cad;nonlinear programming;mos analog circuits;biquad;high parametric yields;nonlinear programming;optimal performance;performance measure;worst-case design optimization;worst-case device parameter files	In this paper we present a new integrated approach for the optimization of MOS analog circuit performances using realistic worst-case device parameter files, each corresponding to a performance measure. Nonlinear response surfaces are constructed for the performance measures of interest, and the worst-case device parameter files are identified b y solving a set of suitably cast nonlinear programming problems. The worst-case files are shown to depend on the values of the designable parameters. An efficient method of incorporating this dependence during worst-case design optimization has been developed. This method enables the design of circuits with optimal performances and high parametric yields. Some illustrative analog circuit ezamples are given to demonstrate the application of the worst-case design optimization procedure.	analogue electronics;best, worst and average case;computer-aided design;integrated circuit;mathematical optimization;nonlinear programming;nonlinear system;performance;response surface methodology	Abhijit Dharchoudhury;Sung-Mo Kang	1992			mixed-signal integrated circuit;control engineering;physical design;mathematical optimization;electronic engineering;multidisciplinary design optimization;analogue electronics;nonlinear programming;computer science;control theory	EDA	24.355145206432933	57.59318809794044	141456
c0110cceac9275b717b3efe1c89f6cbfe67a86f8	statistical analysis of clock skew variation in h-tree structure	tecnologia electronica telecomunicaciones;temperature gradient;transition time constraint;clock distribution;manufacturing variability;statistical analysis;power dissipation;tree structure;tecnologias;environmental change;grupo a;clock skew;environmental variability;time constraint	This paper discusses clock skew due to manufacturing variability and environmental change. In clock tree design, transition time constraint is an important design parameter that controls clock skew and power dissipation. In this paper, we evaluate clock skew under several variability models, and demonstrate relationship among clock skew, transition time constraint and power dissipation. Experimental results show that constraint of small transition time reduces clock skew under manufacturing and supply voltage variabilities, whereas there is an optimum constraint value for temperature gradient. Our experiments in a 0.18 μm technology indicate that clock skew is minimized when clock buffer is sized such that the ratio of output and input capacitance is four.	clock skew;h tree;tree structure	Masanori Hashimoto;Tomonori Yamamoto;Hidetoshi Onodera	2005	IEICE Transactions	10.1093/ietfec/e88-a.12.3375	real-time computing;environmental change;clock skew;computer science;dissipation;control theory;timing failure;tree structure;static timing analysis;temperature gradient;statistics	Visualization	23.322881133029206	56.88822169119532	142201
b5dd8681b05dde9ed97b86e221c067903e0dca5f	a 65 nm gate-level pipelined self-synchronous fpga for high performance and variation robust operation	table lookup field programmable gate arrays;field programmable gate array;variation robust dynamic logic gate level dual pipeline high throughput power supply bounce reconfigurable vlsi self synchronous field programmable gate array ssfpga;gate level dual pipeline;temperature 0 degc to 120 degc gate level pipelined self synchronous fpga variation robust operation delay insensitive operation pipeline granularity self synchronous configurable logic blocks power supply noise frequency accelerated stress cycle chip malfunction power supply bounce size 65 nm frequency 2 97 ghz voltage 1 2 v voltage 728 mv to 2 v;variation robust;reconfigurable vlsi;failure mode;delay insensitive;indexing terms;self synchronous field programmable gate array ssfpga;synchronous system;power supply;chip;dynamic logic;computer architecture;power supply bounce;logic gates;power supply noise;pipelines;delay logic gates pipelines field programmable gate arrays throughput pipeline processing computer architecture;process voltage and temperature;field programmable gate arrays;high throughput;table lookup;logic gate;high performance;pipeline processing;throughput	A 65 nm self-synchronous field programmable gate array (SSFPGA) with delay insensitive operation and pipeline granularity at the gate level, is shown to be robust to process voltage and temperature (PVT) variations. The proposed SSFPGA employs a 38 × 38 array of four-input, three-stage self-synchronous configurable logic blocks, with the introduction of a new dual tree-divider four-input, three-pipeline stage LUT to achieve a 2.97 GHz throughput at 1.2 V. Correct operation is measured with 500 mVp-p, 1.12 GHz externally introduced power supply noise at 1.2 V power supply, equivalent to 42% power supply bounce. Sensitivity against power supply noise frequency has been measured, and confirmed with simulation results to show a strong correlation with the average operating frequency. Correct operation was shown over 10 chips with 16% performance variation, with VDD change from 728 mV to 2 V, and temperature change from 0°C to 120°C, without tuning any input parameters such as clock frequency, supply voltages and biases. Results show the SSFPGA can adapt and is inherently robust to these variations with internal throughput measured from 300 MHz to 4.07 GHz, while maintaining correct operation. The operation under noisey power supply conditions is compared to a conventional synchronous FPGA, which show the SSFPGA has 4.2 times error free operation. The failure mode is also measured on the SSFPGA using an accelerated stress cycle between 0°C and 120°C at 2 V, showing the SSFPGA has 8% longer correct operation before chip malfunction past a 10% delay margin commonly used in synchronous systems.	btrieve;cmos;clock rate;elegant degradation;failure cause;field-programmable gate array;power supply;simulation;stress testing (software);throughput;value-driven design	Benjamin Stefan Devlin;Makoto Ikeda;Kunihiro Asada	2011	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2011.2164024	embedded system;electronic engineering;real-time computing;logic gate;computer science;engineering;field-programmable gate array	Arch	18.860945190940814	58.02675739203969	143277
037cb9c5c3459534caee8d60b33676f86d4a411e	test and diagnosis of power switches	manufacturing defect;test quality;design for test diagnosis;design for testability;circuit faults switches discharges electric transistors power demand monitoring integrated circuits;design for test diagnosis power switch power management;circuit faults;ic;power switch;spice simulation;monitoring;static power consumption;transistors;spice design for testability integrated circuit testing power integrated circuits power semiconductor switches;power management;integrated circuit testing;itc 99 benchmark circuit power switches power gating technique manufacturing defect static power consumption ic design for test diagnosis test quality diagnosis accuracy spice simulation;discharges electric;diagnosis accuracy;power gating technique;itc 99 benchmark circuit;power semiconductor switches;switches;power demand;spice;integrated circuits;power integrated circuits;power switches	Power-gating techniques have been adopted so far to reduce the static power consumption of an Integrated Circuit (IC). Power gating is usually implemented by means of several power switches. Manufacturing defects affecting power switches can lead to increase the actual static power consumption and, in the worst case they can completely isolate a functional block of the IC. In this paper we present a novel Design for Test & Diagnosis to increase the test quality and diagnosis accuracy of power switches. The proposed approach has been validated through SPICE simulations on ITC'99 benchmark circuits.	benchmark (computing);best, worst and average case;design for testing;fault coverage;integrated circuit;iteration;network switch;power gating;spice;simulation	Miroslav Valka;Alberto Bosio;Luigi Dilillo;Aida Todri;Arnaud Virazel;Patrick Girard;Philippe Debaud;Stephane Guilhot	2014	17th International Symposium on Design and Diagnostics of Electronic Circuits & Systems	10.1109/DDECS.2014.6868792	embedded system;electronic engineering;network switch;computer science;engineering;electrical engineering;design for testing;transistor	EDA	20.925342133456468	54.99665753651412	143312
a79015ef19da74b49cc53f853aa7fa07ac1b3341	a quantitative approach to nonlinear process design rule scaling	delay estimation vlsi integrated circuit design cost benefit analysis gallium arsenide monolithic integrated circuits sram chips circuit layout cad integrated circuit economics;integrated circuit;monolithic integrated circuits;design criteria;gaas nonlinear process design rule scaling quantitative methodology scale factors ic performance ic area power improvement data delay improvement data die cost estimates cost benefit ratio quantitative metric design rule scaling cost efficiency cost benefit analysis methodology embedded static rams complementary gaas process cgaas design rules process independent optimizing sram compiler sram macrocells source drain area via metal pitch gate metal to ohmic spacing gate metal spacing design rules vlsi design cad tools 0 5 micron;theses;integrated circuit economics;design space;process design performance analysis delay estimation cost benefit analysis random access memory design engineering power engineering and energy gallium arsenide optimizing compilers macrocell networks;process design;design rules;integrated circuit design;threshold effects;performance improvement;nonlinear systems;cost estimates;linear process;gallium arsenide;threshold voltage;ohmic contact;transistors;voltage;scaling factor;cost efficiency;vlsi;quantitative analysis;cost effectiveness;algorithms;circuit layout cad;optimization;trade off analysis;cost estimation;process engineering;cost benefit analysis;delay estimation;systems approach;sram chips	This paper describes a quantitative methodology for selecting nonlinear design rule scale factors that provide the most cost-efficient improvements in IC performance and area. The methodology includes identifying the design rules which have the greatest impact on the scaling objective and analyzing the area and performance improvements as these rules are scaled through a range of practical scale factors. Power and delay improvement data is then combined with die cost estimates to produce a cost/benefit ratio, a quantitative metric for design rule scaling cost-efficiency. The slopes and inflection points of cost/benefit vs. scale factor plots will guide process engineers in selecting scale factors for the various design rules. This procedure is repeated, using the results of one iteration as the starting point for the next. The cost/benefit analysis methodology is demonstrated by comparing embedded static RAMs implemented in a complementary GaAs (CGaAs TM) process. The SRAMs are generated by a process-independent, optimizing SRAM compiler which can create SRAM macrocells for any complementary technology. A cost/benefit analysis of the CGaAs design rules shows that when operating under a fixed spending cap for process scaling research, development, and capital equipment acquisition, nonlinear scaling can provide greater improvements in area and performance than linear scaling. The results also show that for a 0.5μm CGaAs process to be fabricated in high volume, the first scaling step should be a 30% reduction of the source drain area and via/metal pitch, gate metal to ohmic spacing, and gate metal spacing design rules.	and gate;ab initio quantum chemistry methods;compiler;cost efficiency;embedded system;image scaling;iteration;nonlinear system;scalability;static random-access memory	Spencer M. Gold;Richard B. Brown;Bruce Bernhardt	1999		10.1109/ARVLSI.1999.756041	electronic engineering;real-time computing;engineering;electrical engineering	EDA	18.961711928805936	57.10897729679846	143373
d8eb4f9f27109fa749bcaf993be8a0b6b0989d97	charge recycling mtcmos for low energy active/sleep mode transitions	cmos integrated circuits;multithreshold voltage cmos;gated power;power supply circuits;cmos technology;parasitic capacitance;leakage current;low energy;32 bit;energy recycling;high energy;logic circuits;multi threshold voltage cmos;indexing terms;65 nm;subthreshold leakage multi threshold voltage cmos energy recycling gated power gated ground sleep switch;active mode sleep mode transitions;recycling energy consumption cmos technology leakage current parasitic capacitance threshold voltage logic gates logic circuits adders switches;circuit technique;32 bit charge recycling mtcmos active mode sleep mode transitions multithreshold voltage cmos circuit technique brent kung adder gated power gated ground sleep switch subthreshold leakage 65 nm;logic gates;energy consumption;threshold voltage;brent kung adder;adders;charge recycling mtcmos;low power electronics;gated ground;sleep switch;subthreshold leakage;power consumption;switches;power supply circuits adders cmos integrated circuits low power electronics;recycling	Multi-threshold voltage CMOS (MTCMOS) has emerged as an increasingly popular technique for reducing the leakage energy consumption of idle circuits. The MTCMOS circuits, however, suffer from high energy overhead during the transitions between the active and standby modes. A new circuit technique is proposed in this paper to lower the energy overhead of these mode transitions for effective energy reduction with the MTCMOS circuits. The charge stored at the virtual lines is recycled during the active-to-sleep-to-active mode transitions with the proposed technique. Applying the charge recycling MTCMOS circuit technique to a 32-bit Brent-Kung adder reduces the energy overhead of mode transitions by up to 36.3% as compared to the conventional MTCMOS circuits. Furthermore, the standby mode power consumption is reduced by 91.1% as compared to a standard Brent-Kung adder in a 65nm CMOS technology.	32-bit;adder (electronics);brent's method;computer recycling;multi-threshold cmos;overhead (computing);sleep mode;spectral leakage;transistor;value-driven design	Zhiyu Liu;Volkan Kursun	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378487	embedded system;electronic engineering;logic gate;computer science;engineering;electrical engineering;cmos	EDA	17.450315294997974	58.10822710088682	143381
14ab4d10289256987e3283700b290e9a85a63bd8	a three-step power-gating turn-on technique for controlling ground bounce noise	power supplies;rails;switching circuits;system on a chip soc design power gating ground bounce inductive noise mode transition;mode transition;power gating;system on a chip;power supply;inductive noise;system on a chip soc design;capacitors;transistors;noise transistors power supplies capacitors switches rails switching circuits;ground bounce;switches;noise	To suppress the ground bounce noise with a minimal wake-up time penalty, a three-step turn-on strategy and its corresponding power-gating structure are proposed. During the circuit's meta-stable region of operation, specifically, the amount of current flowing through the sleep transistors is precisely controlled while the virtual or circuit power supply is quickly boosted when the internal nodes of the circuit are stable. In 65 nm CMOS technology, simulation results demonstrate that our technique reduces the peak amplitude of the ground bouncing noise by up to 94% as compared to the conventional abrupt turn-on technique.	arithmetic logic unit;cmos;ground bounce;logic block;power gating;power supply;simulation;transistor;uptime;value-driven design	Rahul Singh;AhReum Kim;Soyoung Kim;Suhwan Kim	2010	2010 ACM/IEEE International Symposium on Low-Power Electronics and Design (ISLPED)	10.1145/1840845.1840880	system on a chip;embedded system;electronic engineering;capacitor;network switch;computer science;engineering;noise;electrical engineering;ground bounce;transistor	EDA	18.195260882621444	56.8825795975254	143740
34420374ed5876f7b63a393658e8e6bc38f75dbd	analysis of within-die process variation in 65nm fpgas	silicon;within die variation;within die delay variation;field programmable gate array;oscillations;process variation;systematics;variation aware design;densely distributed test oscillator;oscillators;computer model;reconfigurable logic;oscillators systematics field programmable gate arrays delay computational modeling silicon;fpga;systematic variation;product silicon;integrated circuit design;computational modeling;spatial correlation;within die performance variation;random components;integrated circuit testing;spatially correlated systematic component;static timing analysis;within die process variation analysis;size 65 nm within die process variation analysis fpga test structure product silicon reconfigurable logic within die delay variation densely distributed test oscillator within die performance variation spatially correlated systematic component random components static timing analysis field programmable gate arrays;test structure;field programmable gate arrays;size 65 nm;variation aware design field programmable gate arrays within die variation systematic variation;timing field programmable gate arrays integrated circuit design integrated circuit testing oscillators;timing	FPGAs are a great platform for studying within-die process variation because test structures can be implemented in product silicon using reconfigurable logic. This approach can achieve very high coverage without wasting otherwise useful silicon area. In this paper, we present a detailed analysis of within-die delay variation in a 65nm FPGA. We use densely distributed test oscillators to measure within-die performance variation across a large sample of dies, and identify both random and spatially correlated systematic components through post-processing. Finally, we evaluate the benefit of modeling within-die systematic variation in static timing analysis.	field-programmable gate array;population;reconfigurable computing;static timing analysis;video post-processing	Tim Tuan;Austin Lesea;Chris Kingsley;Steven Trimberger	2011	2011 12th International Symposium on Quality Electronic Design	10.1109/ISQED.2011.5770808	computer simulation;embedded system;electronic engineering;real-time computing;computer science;engineering;electrical engineering;field-programmable gate array	EDA	22.283073802065775	56.89094623181689	144852
8a455909be65e73d825d313c683317d2fc2a6b9a	optimization of vdd and vth for low-power and high speed applications	circuit optimisation vlsi integrated circuit design low power electronics high speed integrated circuits;process variation;energy consumption constraint optimization threshold voltage power dissipation temperature very large scale integration circuits nose fluctuations delay;very large scale integration;low power high speed design;vlsi design;vlsi optimization low power high speed design supply voltage threshold voltage power dissipation short channel effect;integrated circuit design;total power;low power;leakage power;threshold voltage;power dissipation;supply voltage;low power electronics;short channel effect;vlsi;optimization;power consumption;circuit optimisation;high speed;high speed integrated circuits	Closed-form formulas are presented for optimum supply voltage (V/sub DD/) and threshold voltage (V/sub TH/) that minimize power dissipation when technology parameters and required speed are given. The formulas take into account short-channel effects and the variation of V/sub TH/ and temperature. Using typical device parameters, it is shown that a simple guideline to optimize the power consumption is to set the ratio of maximum leakage power to total power about 30%. Extending the analysis, the future VLSI design trend is discussed. The optimum V/sub DD/ coincides with the SIA roadmap and the optimum V/sub TH/ for logic blocks at the highest temperature and at the lowest process variation corner is in the range of 0 V-0.1 V over generations.	low-power broadcasting;value-driven design	Koichi Nose;Takayasu Sakurai	2000		10.1109/ASPDAC.2000.835145	control engineering;electronic engineering;engineering;electrical engineering;very-large-scale integration	Arch	19.23099399604029	57.94229854282187	145068
31c7906479f6e6165d7899e0f4b137e5476683c8	computing stress tests for gate oxide shorts	gate oxide shorts;burn in;stress circuit testing integrated circuit testing very large scale integration integrated circuit interconnections low voltage computer science temperature production facilities mos devices;infant mortality stress test gate oxide short reliability screen;infant mortality;integrated circuit testing;integrated circuit reliability integrated circuit testing;integrated circuit reliability;stress tests;stress testing	Reliability screens are used to reduce infant mortality. The quality of the stress test set used during the screening process has a direct bearing on the effectiveness of the screen. We present a formal study of the problem of computing good quality stress tests for gate-oxide shorts which is the cause of much of the reliability problems. A method to compute stress test which is better than the popular method of using I/sub DDQ/ vectors is presented.	gate oxide	Vinay Dabholkar;Sreejit Chakravarty	1998		10.1109/ICVD.1998.646637	reliability engineering;electronic engineering;engineering;electrical engineering;infant mortality;burn-in;stress testing	EDA	22.669702554917063	54.602823673577724	145140
c664c1601788bad047fe99c18b5e59f4cdbdd351	measures to improve delay fault testing on low-cost testers - a case study	microcontrollers integrated circuit testing system on chip delays fault diagnosis clocks;microcontrollers;pattern count delay fault testing soc devices at speed testing microcontroller device on chip high speed clock generator dft techniques fault coverage;clocks;null;delay computer aided software engineering circuit faults circuit testing automatic test pattern generation clocks automatic testing production phase locked loops pulse generation;chip;system on chip;integrated circuit testing;test generation;fault coverage;high speed;delays;fault diagnosis	This paper addresses delay test for SOC devices on low-cost testers. The case study focuses on the at-speed testing for a state-of the-art microcontroller device by using an on-chip high-speed clock generator. The experimental results show that the simple on-chip high-speed clock generator is not sufficient to reach both high fault coverage and acceptable pattern count. Meanwhile, at-speed test constraints, required to enable the delay test on low cost testers, have a significant impact on test generation results. DFT techniques to increase fault coverage and to reduce pattern count are discussed.	clock generator;clock rate;experiment;fault coverage;microcontroller;software testability;test card	Matthias Beck;Olivier Barondeau;Frank Poehl;Xijiang Lin;Ron Press	2005	23rd IEEE VLSI Test Symposium (VTS'05)	10.1109/VTS.2005.54	chip;system on a chip;microcontroller;embedded system;electronic engineering;real-time computing;fault coverage;telecommunications;computer science;engineering;stuck-at fault;automatic test pattern generation	EDA	22.108245321199412	53.496147982466624	145252
59c7d15dd823d4c4208ce66ce30356a6acfdd439	stadium: a new tool for high assurance in systems design	stadium high assurance tool;system engineering;statistically based system level simulation;software tool;design tool;design process;design engineering;complex system development;statistically based subsystem level simulation;systems design;system level variabilities;search effects;systems engineering;simulation;systems engineering phase;worst case system design;operational variabilities;coupling circuits;circuit level variabilities;system performance;dual use technologies;manufacturing modeling process design design engineering systems engineering and theory system performance costs monte carlo methods coupling circuits design methodology;systems engineering and theory;process design;subsystem level variabilities;large system development;system performances;design of experiments;design of experiments methodology;simulation design of experiments systems engineering systems analysis software tools;complex system;systems analysis;system design;manufacturing;system performance levels;complex commercial systems;software tools;manufacturing variabilities;statistical techniques;system level variabilities systems design stadium high assurance tool large system development complex system development systems engineering phase operational variabilities manufacturing variabilities system performance levels search effects worst case system design statistically based system level simulation statistically based subsystem level simulation dual use technologies defense conversions complex commercial systems design of experiments methodology software tool system performances circuit level variabilities subsystem level variabilities;system simulation;modeling;monte carlo methods;monte carlo technique;defense conversions;design methodology;design of experiment	An issue in the development of large and complex systems is that thle design process is essentially open-ended The systems engmeering phase j n h an acceptable theoretical design andpasses it on for manufacturing. Little attention is paid to “real-world” detrimental effects of manufacturing and operational variabilities u p n the “designed” system perfonname levels. Alternatively to try to account for such effects, a “worst-case ’’ system design is often attempted with predictable impact to system costs. As a result of the above issues, the importance of running statistically based system and subsystem level simulatiom has rbecome particularly critical in this period of increasingly complex commercial systems, defense conversions and dual-use technologies. Monte Carlo techniques could be considered, but will ofren lead to excessively time-consuming and ineficient design tools. In this papel: we present a means of coupling more eficient statistical techniques (based on a Design of Experiments methodology) to conventional systems simulators. The resulting somare tool, called S T m I W , is currently in use within industry to characterize system performances in terms of circuit, subsystem and system level variabilitiex	best, worst and average case;complex systems;design of experiments;misuse case;monte carlo method;nonlinear gameplay;performance;simulation;systems design	Rufus H. Cofer;T. J. Sanders	1995		10.1109/ICECCS.1995.479324	complex systems;real-time computing;simulation;system of systems;systems engineering;engineering;computer performance;design of experiments;monte carlo method;systems design	EDA	24.218500500553976	57.27368470197853	145296
379a57cc6d869f39de8b8347f22853ad08cc9800	sleep mode analysis and optimization with minimal-sized power gating switch for ultra-low  ${v}_{\rm dd}$ operation	cmos integrated circuits;voltage drop;ultra low power;sleep mode energy consumption;power gating;inverters;subthreshold operation;sleep mode analysis;ultra low power microprocessor;ultra low power mtcmos power gating switch sleeps mode standby mode subthreshold operation;low power electronics circuit optimisation cmos integrated circuits integrated circuit measurement;power gating switch;ultra low voltage cmos circuits;energy consumption delay inverters threshold voltage optimization switches spice;standby mode;energy consumption;threshold voltage;low power electronics;silicon measurements sleep mode analysis minimal sized power gating switch sleep mode energy consumption ultra low voltage cmos circuits voltage drop spice simulations ultra low power microprocessor;mtcmos;spice simulations;optimization;circuit optimisation;switches;sleeps mode;spice;minimal sized power gating switch;integrated circuit measurement;silicon measurements	This paper investigates the optimization of sleep mode energy consumption for ultra-low Vdd CMOS circuits, which is motivated by our findings that minimization of sleep mode energy holds great potential for reducing total energy consumption. We propose a unique approach of using a power gating switch (PGS) in ultra-low Vdd regimes. Unlike the conventional manner of using PGSs, our optimization suggests using minimal-sized PGSs with a slightly higher Vdd to compensate for voltage drop across the PGS. In SPICE simulations, this reduces total energy consumption by ~125× compared to conventional approaches. The effectiveness of the proposed optimization is also confirmed by measurements taken from an ultra-low power microprocessor. Additionally, the feasibility of using minimal PGSs in ultra-low Vdd regimes is investigated using SPICE simulations and silicon measurements.	cmos;mathematical optimization;microprocessor;power gating;spice;simulation;sleep mode	Mingoo Seok;Scott Hanson;David Blaauw;Dennis Sylvester	2012	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2011.2109069	electronic engineering;real-time computing;network switch;computer science;engineering;electrical engineering;operating system;threshold voltage;cmos;standby power;voltage drop;low-power electronics	EDA	18.15360904873874	58.41005270354285	145451
d578acce7f514bc90345862d158b9b31ec430ced	power delivery performance of probe test systems for semiconductor wafers	power supplies;power distribution planning;contact resistance;probes integrated circuit interconnections integrated circuit testing;voltage fluctuation power delivery performance probe test system semiconductor wafer interconnect design;testing;transient analysis;semiconductor device modeling;capacitors;inductance;power distribution planning capacitors semiconductor device modeling transient analysis power supplies inductance contact resistance testing	The focus of this article is on power delivery of probe testers for wafers. When the interconnect design is optimized, a significant reduction in voltage fluctuations is possible as the authors demonstrate.	semiconductor;wafer (electronics)	Bahadir Tunaboylu	2016	IEEE Design & Test	10.1109/MDAT.2015.2501297	control engineering;power module;electronic engineering;semiconductor device modeling;capacitor;power factor;inductance;engineering;electrical engineering;contact resistance;switched-mode power supply;software testing;power semiconductor device	EDA	22.394629839910543	54.87866933773862	145615
6a25e27086896f665f45030fc628b68ed19842fb	hysteresis of intrinsic i/sub ddq/ currents	analytical models;hysteresis testing analytical models shape measurement power measurement velocity measurement condition monitoring current measurement history clocks;history;hysteresis;clocks;testing;shape measurement;current measurement;condition monitoring;velocity measurement;power measurement	Empirical analyses of the IDDQ signatures of 0.18 μm devices indicate that IDDQ currents exhibit hysteresis. A newly proposed test method, SPIRIT (Single Pattern Iteration IDDQ Test), demonstrates that the test pattern and the device clock speed before measurements must be maintained to assure the integrity of IDDQ measurements, which is the fundamental assumption of IDDQ applications: testing, diagnosis, monitoring, and static power estimation. Newly proposed IDDQ signature and hysteresis models show that hysteresis phenomena are caused by the global transient threshold voltage shifts induced by the direct tunnel charges to the pre-existing border traps under nominal operating conditions.	antivirus software;clock rate;hysteresis;iddq testing;iteration;sensor;spectral leakage;test card;time complexity;transistor;tunneling protocol	Yukio Okuda;Nobuyuki Furukawa	2003		10.1109/TEST.2003.1270882	control engineering;electronic engineering;hysteresis;engineering;software engineering;control theory;software testing	EDA	23.1817847753683	54.79788907020297	145802
3055146fcc8ba05e842e841c717847e6b30a1a5e	predicting electromigration mortality under temperature and product lifetime specifications	silicon;stress;cathodes;mortal wires electromigration mortality product lifetime specifications current density blech criterion temperature conditions power grid benchmarks;wires electric electromigration;wires;force;electromigration;stress electromigration blech length steady state;wires stress current density steady state cathodes force silicon;steady state;current density;blech length	Today's methodologies for electromigration (EM) identify EM-susceptible wires based on their current density, using the Blech criterion to filter out wires that are EM-immortal. The Blech criterion is agnostic to the product lifetime and temperature conditions: many Blech-mortal wires may never experience EM during the product lifetime. We develop new methods that evaluate the transient evolution of stress, relative to the product lifetime, and present an improved set of simple and practical mortality criteria. On a set of power grid benchmarks, we demonstrate that the actual number of mortal wires may depend strongly on the lifetime and reliability conditions.	electromigration;the immortal	Vivek Mishra;Sachin S. Sapatnekar	2016	2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2897937.2898070	electromigration;electronic engineering;electrical engineering;cathode;stress;silicon;forensic engineering;steady state;force;current density	EDA	22.54529989779194	56.005732200013526	145953
5bc3626b9b8faf82dd1b0cb567974b0fc3859bc8	analysis of non-uniform temperature-dependent interconnect performance in high performance ics	thermal analysis;thermal gradient;signal integrity issues nonuniform temperature dependent interconnect performance temperature profiles global interconnect lines thermal gradients rc interconnect delay model interconnect layouts temperature distributions;signal integrity;temperature dependence;boolean satisfiability;temperature profile;thermal analysis integrated circuit interconnections temperature distribution delays integrated circuit modelling vlsi;integrated circuit modelling;integrated circuit interconnections;vlsi;performance analysis integrated circuit interconnections clocks thermal management very large scale integration rapid thermal processing temperature distribution current density permission thermal resistance;design verification;high performance;clock skew;temperature distribution;delays	Non-uniform temperature profiles along global interconnect lines in high-performance ICs can significantly impact the performance of these lines. This paper presents a detailed analysis and modeling of the interconnect performance degradation due to non-uniform temperature profiles that exist along their lengths, which in turn arise due to the thermal gradients in the underlying substrate. A non-uniform temperature-dependent distributed RC interconnect delay model is proposed for the first time. The model has been applied to a wide variety of interconnect layouts and temperature distributions to quantify the impact on signal integrity issues including clock skew fluctuations.	clock skew;elegant degradation;gradient;signal integrity	Amir H. Ajami;Kaustav Banerjee;Massoud Pedram;Lukas P. P. P. van Ginneken	2001		10.1145/378239.379025	embedded system;electronic engineering;clock skew;computer science;signal integrity;engineering;electrical engineering;very-large-scale integration;boolean satisfiability problem;interconnect bottleneck;temperature gradient;thermal analysis	HPC	21.62824776506026	58.02654248145523	145960
bdaa140d8b0d3a6b60790d01222349cdc92c47c1	a 3.4-pj feram-enabled d flip-flop in 0.13-µm cmos for nonvolatile processing in digital systems	cmos logic circuits;ferroelectric capacitors;ferroelectric storage;flip-flops;random-access storage;asic design flow;cmos integrated circuit;feram enabled d flip-flop;energy 3.4 pj;ferroelectric capacitor;nonvolatile d-flip-flop;nonvolatile fir filter;nonvolatile memory element;nonvolatile processing;size 0.13 mum;sporadic power loss;time 4.8 mus to 1 d;digital electronics;embedded systems;energy harvesting;ferroelectric random access memory (feram);latches;low-power electronics;nonvolatile memory;registers	In order to realize a digital system with no distinction between “on” and “off,” computational state must be stored in non-volatile memory elements. If the energy cost and time cost of managing computational state in nonvolatile memory can be lowered to the microsecond and picojoule per bit level, such a system could operate from unreliable harvested energy, never requiring a reboot. This work presents a nonvolatile D flip-flop (NVDFF) designed in 0.13 μm CMOS that retains state in ferroelectric capacitors during sporadic power loss. The NVDFF is integrated into an ASIC design flow, and a test-case nonvolatile FIR filter with an accompanying power management unit automatically saves and restores state based on the status of a one-bit indicator of energy availability. Correct operation has been verified over power cycle intervals from 4.8 μs to 1 day. The round-trip save-restore energy is 3.4 pJ per NVDFF. Also presented are statistical measurements across 21,000 NVDFFs to validate the capability of the circuit to achieve the requisite 10 ppm failure rate for embedded system applications.		Masood Qazi;Ajith Amerasekera;Anantha Chandrakasan	2014	J. Solid-State Circuits	10.1109/JSSC.2013.2282112		Arch	18.712724278025156	60.252158263777915	146217
eabd4c44f30708782e6235314a2cddebeb47b6ba	automated shmoo data analysis: a machine learning approach	silicon;silicon electronic engineering computing elemental semiconductors learning artificial intelligence semiconductor device manufacture semiconductor device testing;semiconductor device testing;machine learning hvm silicon testing shmoo experiment;elemental semiconductors;semiconductor device manufacture;classification algorithms algorithm design and analysis decision trees silicon training accuracy prediction algorithms;electronic engineering computing;learning artificial intelligence;hvm test flow automated shmoo data analysis machine learning approach silicon testing shmoo plot silicon manufacturing development health silicon characterization data silicon data analysis hvm test content development supervised learning model vmin estimation automated tester software silicon characterization	In silicon testing, a Shmoo plot is commonly used to give us an insight into the silicon manufacturing development health. Shmoo plots and other silicon characterization data has high value, however, analysis of them is a time-consuming work. This paper establishes a machine learning based model to improve and automate the procedure in silicon data analysis for HVM test content development. Our experiment shows that the supervised learning model has good accuracy on VMIN estimation across various kinds of Shmoo issues (crack/sprinkle/ceiling). The accuracy attained is greatly improved over previous tools. The framework can be easily integrated into any automated tester software and would save time to market during first silicon characterization. Additionally, the methodology discussed in this work can be extended to the HVM test flow for silicon behavior.	hardware-assisted virtualization;machine learning;recurrence plot;supervised learning;web content development	Wei Wang	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783327	electronic engineering;simulation;engineering;electrical engineering;silicon;computer engineering	SE	23.276984186443425	55.672908176285716	146307
67ec078b653e0de20edf1fbced4d5264fb875f78	statistical leakage estimation based on sequential addition of cell leakage currents	evaluation performance;process variation;desviacion tipica;metodo monte carlo;leakage current;performance evaluation;full chip statistical leakage estimation;integrated circuit;very large scale integration;corriente escape;wilkinson method cell leakage currents sequential addition full chip statistical leakage estimation virtual cell approximation iscas benchmarks average leakage mean standard deviation errors monte carlo simulation;standard deviation;evaluacion prestacion;iscas benchmarks;simulacion numerica;methode monte carlo;circuito integrado;vlsi approximation theory leakage currents monte carlo methods;yield estimation;state dependence;chip;wilkinson method;yield estimation leakage current process variation statistical leakage estimation;approximation theory;circuit simulation;computational modeling;courant fuite;leakage currents;statistical leakage estimation;energy consumption;integrated circuit technology;cell leakage currents;average leakage mean;monte carlo method;sequential addition;simulation numerique;ecart type;estimacion parametro;standard deviation errors;vlsi;leakage current circuit simulation yield estimation monte carlo methods very large scale integration frequency energy consumption circuit optimization integrated circuit technology computational modeling;parameter estimation;estimation parametre;estimation statistique;monte carlo;virtual cell approximation;frequency;monte carlo simulation;estimacion estadistica;statistical estimation;monte carlo methods;circuit integre;circuit optimization;numerical simulation	This paper presents a novel method for full-chip statistical leakage estimation that considers the impact of process variation. The proposed method considers the correlations among leakage currents in a chip and the state dependence of the leakage current of a cell for an accurate analysis. For an efficient addition of the cell leakage currents, we propose the virtual-cell approximation (VCA), which sums cell leakage currents sequentially by approximating their sum as the leakage current of a single virtual cell while preserving the correlations among leakage currents. By the use of the VCA, the proposed method efficiently calculates a full-chip leakage current. Experimental results using ISCAS benchmarks at various process variation levels showed that the proposed method provides an accurate result by demonstrating average leakage mean and standard deviation errors of 3.12% and 2.22%, respectively, when compared with the results of a Monte Carlo (MC) simulation-based leakage estimation. In efficiency, the proposed method also demonstrated to be 5000 times faster than MC simulation-based leakage estimations and 9000 times faster than the Wilkinson's method-based leakage estimation.	and gate;approximation;computational complexity theory;image scaling;monte carlo method;multiprotocol label switching;newton's method;norm (social);simulation;spectral leakage;time complexity;total variation diminishing;transistor model;tunneling protocol;video content analysis;virtual cell	Wook Kim;Kyung Tae Do;Young Hwan Kim	2010	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2009.2013956	computer simulation;econometrics;electronic engineering;engineering;statistics;monte carlo method	EDA	23.35406273325235	59.22346427646201	146495
9f965a295545a432da7dad48302b3c9be8d27f34	analyzing the impact of double patterning lithography on sram variability in 45nm cmos	cmos integrated circuits;sram chips cmos integrated circuits lithography;double patterning lithography;random access memory;test chip;semiconductor device measurement;cmos process;chip;sram variability;arrays;lithography;transistors;sram cell;yield loss;robustness;optimization;random access memory lithography robustness optimization transistors semiconductor device measurement arrays;sram cell double patterning lithography sram variability test chip cmos process;sram chips	This paper analyzes the impact of Double Patterning Lithography (DPL) on 6T SRAM variability. A test chip is implemented in a 45nm CMOS process that uses DPL. Measurements from 75 dies demonstrate a significant impact of DPL on SRAM failures. Extensive analysis demonstrates that DPL induced mismatch considerably increases functional failures in SRAM cells, and degrades yield. We also propose a DPL-aware sizing technique to mitigate yield losses.	cmos;koutetsu no kishi;privilege level;semiconductor device fabrication;spatial variability;static random-access memory	Vivek Joshi;Michael Wieckowski;Gregory K. Chen;David Blaauw;Dennis Sylvester	2010	IEEE Custom Integrated Circuits Conference 2010	10.1109/CICC.2010.5617623	lithography;chip;embedded system;electronic engineering;parallel computing;telecommunications;computer science;cmos;transistor;robustness	EDA	19.36304465064298	58.78109617142244	146749
383414bf17ab8ba2a4ad677621b214fff8cda823	logical diagnosis solutions must drive yield improvement	design for testability;yield analysis yield improvement design for test dft full scan defect behavior modeling simulation logic diagnosis failure diagnosis;integrated circuit yield;logic design;behavior modeling;automatic testing;design for testability logic design logic testing fault diagnosis production testing integrated circuit yield integrated circuit testing automatic testing;research and development;logic testing;integrated circuit testing;high volume manufacturing;design for test;failure analysis silicon manufacturing throughput logic testing production stress costs fading geometry;production testing;fault diagnosis	Introduction Can automated solutions for logic diagnosis become a fundamental driver of yield improvement’? or are they doomed to remain an “occasional entertaining diversion’?” As we begin driving a transition in diagnosis from science projects to industrial solutions, we need to consider the differences between failure analysis (FA) and low yield analysis (LYA). We need to comprehend that this technology is critical, not just “nice to have.” And we need to drive development based on cutting edge research, supporting design features and validation in production silicon	failure analysis;nice (unix)	Paul G. Ryan	1997		10.1109/TEST.1997.639647	reliability engineering;embedded system;electronic engineering;white-box testing;computer science;engineering;test compression;design for testing;system testing;register-transfer level;computer engineering	AI	22.648224186024805	53.60580401943642	147048
6ab5632e1ce4c538a3740b03f98f92baafada061	rt level timing modeling for aging prediction		The simulation of aging related degradation mechanisms is a challenging task for timing and reliability estimations during all design phases of digital systems. Some good approaches towards accurate, efficient and applicable timing models at the register transfer level (RTL) have already been made. However recent state-of-the-art models often have to access lower levels of abstraction, such as the underlying gate-level netlist for each timing estimation and require to repeat every analyzing step if parameters, input signals or designs are changed. This work introduces a new RTL timing model concept that provides a separation of design analysis and aging estimation. It allows more efficient design evaluations with respect to aging. Although this is work in progress and systematic evaluations are still ongoing, early results indicate the applicability and capability of the approach to compete with recent models both in accuracy and efficiency.	digital electronics;elegant degradation;netlist;principle of abstraction;register-transfer level;simulation	Nils Koppaetzky;Malte Metzdorf;Reef Eilers;Domenik Helms;Wolfgang Nebel	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		reliability engineering;negative-bias temperature instability;electronic engineering;real-time computing;engineering	EDA	21.822713539607573	57.389548351918705	147685
5e3594f154ff6726e0f35762b86cc0f6dca8840e	design and performance analysis of ultra low power 6t sram using adiabatic technique		ABSTRACT: Power consumption has become a critical concern in both high performance and portable applications. Methods for power reduction based on the application of adiabatic techniques to CMOS circuits have recently come under renewed investigation. In thermodynamics, an adiabatic energy transfer through a dissipative medium is one in which losses are made arbitrarily small by causing the transfer to occur sufficiently slowly. In this work adiabatic technique is used for reduction of average power dissipation. Simulation of 6T SRAM cell has been done for 180nm CMOS technology. It shows that average power dissipation is reduced up to 75% using adiabatic technique and also shows the effect on static noise margin.	cmos;cpu power dissipation;cell (microprocessor);elegant degradation;noise margin;profiling (computer programming);simulation;static random-access memory	Sunil Jadav;Vikrant;Munish Vashisath	2012	CoRR		electronic engineering;real-time computing;adiabatic circuit;engineering;electrical engineering	EDA	18.70559302763238	59.70235267044131	147883
93aa13b745b52de019704b23e3bb8425277bac64	an illustrated methodology for analysis of error tolerance	flash memory;process variation;codecs;performance evaluation;converters;digital telephone answering device;mean opinion score error tolerance telephone answering machine yield defective flash memory;error tolerance;digital telephone answering device error tolerance analysis process variations;speech;error analysis performance analysis digital systems circuit testing computer errors error correction fault tolerance redundancy frequency response signal to noise ratio;yield;frequency response;telephone answering machine;error analysis;process variations;redundancy;fault tolerant systems;error correction;digital systems;fault tolerance;mean opinion score;performance analysis;defective flash memory;error tolerance analysis;telephone equipment;circuit testing;signal to noise ratio;computer errors	Noise, defects, and process variations are likely to cause very unpredictable circuit performance in future billion-transistor dies, hence decreasing raw yield. Error tolerance is one of several techniques that can increase effective yield. This article presents a methodology for analyzing the suitability of error tolerance for a particular application and implementation. The methodology, illustrated here by a digital telephone-answering device, is applicable to a broad class of systems.	die (integrated circuit);error-tolerant design;transistor	Melvin A. Breuer;Haiyang Zhu	2008	IEEE Design & Test of Computers	10.1109/MDT.2008.30	mean opinion score;reliability engineering;embedded system;yield;fault tolerance;frequency response;electronic engineering;codec;real-time computing;error detection and correction;speech recognition;computer science;electrical engineering;speech;redundancy;process variation;signal-to-noise ratio	EDA	22.58075180507881	54.66056470973596	148001
25cf810b081a15c6734af1e9b51d3c2d46196065	mosgrad-a tool for simulating the effects of systematic and random channel parameter variations	cmos integrated circuits;integrated circuit layout;mosfets random channel parameter variations cad tool mosgrad distributed two dimensional systematic variations device parameters matching critical circuits rectangular transistors nonconventional circuit structures multiple drain regions multiple source regions common channel region nonconventional layouts nonrectangular transistors segmented transistors;cmos integrated circuits mosfet mos integrated circuits integrated circuit layout circuit layout cad semiconductor device models circuit simulation;circuit simulation;circuit simulation random variables computational modeling mosfets predictive models application software silicon circuit optimization mirrors semiconductor device modeling;semiconductor device models;mos integrated circuits;circuit layout cad;mosfet	A CAD tool, MOSGRAD, that can be used to simulate the effects of distributed two-dimensional systematic and random variations in device parameters on the performance of matchingcritical circuits has been developed. In addition to applications for layouts with conventional rectangular transistors, this tool can predict the performance of non-conventional circuit structures in which multiple drain and/or source regions share a common channel region as well as predict the performance of nonconventional layouts that may incorporate nonrectangular transistors or segmented transistors.	computer-aided design;gradient;spice 2;semiconductor;simulation;transistor	Mao-Feng Lan;Randall L. Geiger	2001		10.1109/ISCAS.2001.921795	physical design;electronic engineering;ic layout editor;computer science;engineering;electrical engineering;integrated circuit;circuit design;integrated circuit layout;circuit extraction;cmos;engineering drawing;discrete circuit	EDA	23.224663777796827	56.91671539673901	148107
7f69ee88451bfe5c1fa67cc716fdd6e1477e9d75	powerfield: a probabilistic approach for temperature-to-power conversion based on markov random field theory	fpga device temperature to power conversion post silicon power model validation powerfield transient analysis steady state analysis thermal images power distribution power map maximum a posteriori markov random field map mrf spatial thermal system approximated transient heat transfer equation thermal node binary power patterns;transient analysis heat transfer infrared imaging markov processes maximum likelihood estimation power conversion power measurement;maximum likelihood estimation;transient analysis;infrared imaging;heat transfer;temperature measurement steady state transient analysis heating mathematical model thermal resistance equations;hotspot;circuits;markov processes;temperature to power graph cuts markov random field post silicon power validation;management;article;power conversion;power measurement	Temperature-to-power technique is useful for post-silicon power model validation. However, the previous works were applicable only to the steady-state analysis. In this paper, we propose a new temperature-to-power technique, named PowerField, supporting both transient and steady-state analysis based on a probabilistic approach. Unlike the previous works, PowerField uses two consecutive thermal images to find the most feasible power distribution that causes the change between the two input images. To obtain the power map with the highest probability, we adopted maximum a posteriori Markov random field (MAP-MRF). For MAP-MRF framework, we modeled the spatial thermal system as a set of thermal nodes and derived an approximated transient heat transfer equation that requires only the local information of each thermal node. Experimental results with a thermal simulator show that PowerField outperforms the previous method in transient analysis reducing the error by half on average. We also show that our framework works well for steady-state analysis by using two identical steady-state thermal maps as inputs. Lastly, an application to determining the binary power patterns of an FPGA device is presented achieving 90.7% average accuracy.	approximation algorithm;cut (graph theory);field-programmable gate array;map;markov chain;markov random field;mathematical optimization;simulation;steady state;transient state	Seungwook Paek;Wongyu Shin;Jaehyeong Sim;Lee-Sup Kim	2013	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2013.2272542	mathematical optimization;electronic circuit;electronic engineering;hotspot;computer science;mathematics;maximum likelihood;markov process;heat transfer;statistics	EDA	23.477734602318296	59.24424999165588	148368
e20690c72ca01578407133fcc30dfa364a4cd789	ultralow-power design in near-threshold region	modelizacion;dimensionnement;evaluation performance;optimisation;cmos logic circuits design optimization integrated circuit yield integrated circuit synthesis cmos digital integrated circuits integrated circuit modeling logic design space technology system performance voltage;leakage current;transistors cmos digital integrated circuits logic circuits logic design low power electronics;performance evaluation;optimizacion;integrated circuit;logic design;ajustamiento curva;corriente escape;evaluacion prestacion;optimal method;dimensioning;logic circuits;chip synthesis;circuito integrado;transistor sizing;time multiplexing;tecnologia mos complementario;pass transistor based logic family;conception circuit integre;chip;modelisation;integrated circuit design;sensitivity;multiplaje tiempo;design technique;chip synthesis near threshold region minimum energy point energy delay modeling framework transistor sizing supply voltage pass transistor based logic family cmos leakage dominated ultralow power designs time multiplexing energy reduction;conception logique;logic gates;cmos digital integrated circuits;courant fuite;leakage currents;semiconductor device modeling;transistors;pipelines;supply voltage;energy reduction;integrated circuit modeling;low power electronics;ajustement courbe;energy delay modeling framework;sensitivity circuit optimization cmos digital integrated circuits curve fitting design methodology integrated circuit design integrated circuit modeling leakage currents logic design optimization methods pipelines;multiplexage temps;optimization;temps retard;time division multiplexing;delay time;circuit integre numerique cmos;curve fitting;technologie mos complementaire;minimum energy point;dimensionamiento;modeling;concepcion logica;electronique faible puissance;tiempo retardo;leakage dominated ultralow power designs;cmos;circuit integre;complementary mos technology;near threshold region;circuit optimization;transistor;optimization methods;design methodology	Operation in the subthreshold region most often is synonymous to minimum-energy operation. Yet, the penalty in performance is huge. In this paper, we explore how design in the moderate inversion region helps to recover some of that lost performance, while staying quite close to the minimum-energy point. An energy-delay modeling framework that extends over the weak, moderate, and strong inversion regions is developed. The impact of activity and design parameters such as supply voltage and transistor sizing on the energy and performance in this operational region is derived. The quantitative benefits of operating in near-threshold region are established using some simple examples. The paper shows that a 20% increase in energy from the minimum-energy point gives back ten times in performance. Based on these observations, a pass-transistor based logic family that excels in this operational region is introduced. The logic family operates most of its logic in the above-threshold mode (using low-threshold transistors), yet containing leakage to only those in subthreshold. Operation below minimum-energy point of CMOS is demonstrated. In leakage-dominated ultralow-power designs, time-multiplexing will be shown to yield not only area, but also energy reduction due to lower leakage. Finally, the paper demonstrates the use of ultralow-power design techniques in chip synthesis.	cmos;gene expression programming;logic family;multiplexing;spectral leakage;transistor	Dejan Markovic;Cheng C. Wang;Louis P. Alarcón;Tsung-Te Liu;Jan M. Rabaey	2010	Proceedings of the IEEE	10.1109/JPROC.2009.2035453	control engineering;electronic engineering;logic gate;engineering;electrical engineering;transistor	EDA	18.524118406893983	55.36570653471691	148471
73c2016f86f32c10f01efd0f1ca7a3eeb2654ce6	system-level modeling and microprocessor reliability analysis for backend wearout mechanisms	aging;siv;em;state-of-art microprocessor lifetime;sm;functional unit;system-level modeling;backend time-dependent dielectric breakdown;wearout mechanisms;backend wearout mechanism;reliability;microprocessor;lifetime-limiting wearout mechanism;reliability-critical microprocessor;detailed electrical stress;microprocessor reliability analysis;cross-sectional area;tddb;circuit layout geometries;microprocessor system;dielectrics;stress;benchmark testing;layout	Backend wearout mechanisms are major reliability concerns for modern microprocessors. In this paper, a framework which contains modules for backend time-dependent dielectric breakdown (BTDDB), electromigration (EM), and stress-induced voiding (SIV) is proposed to analyze circuit layout geometries and interconnects to accurately estimate state-of-art microprocessor lifetime due to each mechanism. Our methodology incorporates the detailed electrical stress, temperature, linewidth and cross-sectional areas of each interconnect within the microprocessor system. We analyze several layouts using our methodology and highlight the lifetime-limiting wearout mechanisms, along with the reliability-critical microprocessor functional units, using standard benchmarks.	benchmark (computing);circuit diagram;cross-sectional data;electrical connection;electromigration;front and back ends;microprocessor;reliability engineering;stress ball	Chang-Chih Chen;Linda S. Milor	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;real-time computing;engineering;reliability;em;statistics;time-dependent gate oxide breakdown	EDA	21.954441390956458	57.10232223008278	148485
dbc0e00399a8fc90d8a3487dff8c9a42d47a2c06	temperature-insensitive dual- $v_{\rm th}$ synthesis for nanometer cmos technologies under inverse temperature dependence	dependance temperature;leakage;optimisation;mos devices;floating point unit;network synthesis;voltage threshold;optimizacion;logic design;leakage reduction;circuit design;tecnologia mos complementario;temperature dependence;procesador 32 bits;algorithme;algorithm;power aware computing;consumo electricidad;total power;low power;conception logique;logic synthesis;logic gates;estudio caso;system synthesis;threshold voltage;synthese systeme;processeur 32 bits;nanoelectronics;electric power consumption;low power electronics;power optimization;etude cas;sintesis sistema;temperature aware design;timing analysis;mosfet;seuil tension;optimization;floating point;coma flotante;temps retard;delay time;nanometer cmos technologies;temperature;inverse temperature dependence;power aware logic synthesis;technologie mos complementaire;temperature insensitive synthesis;nanometer circuits;concepcion logica;electronique faible puissance;tiempo retardo;consommation electricite;complementary mos technology;dual threshold voltage;umbral tension;32 bit processor;virgule flottante;algoritmo	With the scaling of CMOS technologies, the gap between nominal supply voltage and threshold voltage has decreased significantly. This trend is further amplified in low-power nanometer libraries, which feature cells with identical size and functionality, but different threshold voltages. As a consequence, different cells may have different delay behaviors as the temperature varies within a circuit. For instance, cells with low-threshold devices may experience an increase in delay when temperature increases, whereas cells using high-threshold devices may experience the opposite behavior. The latter effect, also known as inverse temperature dependence (ITD), poses new challenges to circuit designers. Besides making timing analysis more difficult, ITD has important and unforeseeable consequences for power-aware logic synthesis. This paper describes the impact that ITD may have on the design of nanometer circuits. We also provide a threshold voltage assignment algorithm for dual threshold voltage synthesis, which guarantees temperature-insensitive operation of the circuits, together with a significant reduction of both leakage and total power consumption. Experiments performed on a set of standard benchmarks show timing compliance at any operating temperature, and an average leakage reduction around 28% compared to circuits synthesized with a standard synthesis flow that does not take ITD into account. We also apply our proposed synthesis algorithm to a realistic case study consisting of a 32-bit, IEEE-754 floating point unit.	32-bit;algorithm;benchmark (computing);cmos;experiment;floating-point unit;image scaling;library (computing);logic synthesis;low-power broadcasting;netlist;spectral leakage;static timing analysis	Andrea Calimera;R. Iris Bahar;Enrico Macii;Massimo Poncino	2010	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2009.2025884	electronic engineering;logic synthesis;telecommunications;computer science;engineering;electrical engineering;operating system	EDA	18.434206137360707	55.000659681227866	149078
b2a4a27f810222a1d6dbac05c18b09a64b4b0fbd	error-resilient low-power dsp via path-delay shaping	error detection and correction;energy efficiency;energy conservation;size 32 nm;dynamic voltage scaling control loop;voltage control;fir;lsb result registers;power saving;voltage scaling low power vlsi dsp fir razor error tolerance;size 32 nm error resilient low power dsp path delay shaping circuit level timing error mitigation technique energy efficiency razor flip flops dvs control loop dynamic voltage scaling control loop lsb result registers tool based device sizing cmos digital filter;energy efficient;razor flip flops;tool based device sizing;error tolerance;finite impulse response filter;power aware computing cmos digital integrated circuits digital signal processing chips energy conservation;dynamic voltage scaling;error resilient low power dsp;digital filter;razor;power aware computing;finite impulse response;low power;cmos digital integrated circuits;dvs control loop;delay noise adders voltage control finite impulse response filter;adders;critical path;vlsi;error rate;error resilience;digital signal processing chips;path delay shaping;circuit level timing error mitigation technique;voltage scaling;cmos;flip flop;dsp;noise	In this paper, we present a novel circuit-level timing error mitigation technique, which aims to increase energy-efficiency when applying a known in situ error-detection and correction technique, called Razor, to DSP datapaths. Timing errors are detected using Razor flip-flops at critical-path endpoints and the error-rate feedback is used to control a dynamic voltage scaling (DVS) control loop. We propose a new approach to bound the magnitude of intermittent timing errors at the circuit level by introducing a guard-band over which timing errors are safely mitigated. The guard-band is achieved by shaping the path delay distribution such that the critical paths correspond to a group of LSB result registers. These end-points are ensured to be critical by modifying the topology of the final stage carry-merge adder and by using tool-based device sizing. Hence, timing violations lead to weakly correlated logical errors of small magnitude in a mean-squared-error sense. We applied this approach to a digital filter in 32nm CMOS. Power saving compared to a conventional design was 23%, over worst-case process and temperature corners.	adder (electronics);apache ant (another neat tool);best, worst and average case;cmos;control system;digital filter;digital signal processor;dynamic voltage scaling;elegant degradation;error detection and correction;flops;finite impulse response;flip-flop (electronics);image scaling;least significant bit;linear phase;low-power broadcasting;mean squared error;noise shaping;occam's razor;razor;simulation;throughput	Paul N. Whatmough;Shidhartha Das;David M. Bull;Izzat Darwazeh	2011	2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2024724.2024946	embedded system;electronic engineering;real-time computing;computer science;engineering;electrical engineering;finite impulse response	EDA	19.064486289760428	58.61581117771395	149350
419c05f37cbe25893f59267c61ea625b6d1db920	a direct search method for worst case analysis and yield optimization of integrated circuits	mos transistor mismatch;integrated circuit;yield maximization;worst case analysis;direct search method	Yield maximization is an important aspect in the design of integrated circuits. A prerequisite for its automation is a reliable and fast worst performance analysis which results in corners that can be used in the process of circuit optimization. We formulate the constrained optimization problem for finding the worst performance of an integrated circuit and develop a direct search method for solving it. The algorithm uses radial steps and rotations for enforcing the inequality constraint. We demonstrate the performance of the proposed algorithm on real world design examples of integrated circuits. The results indicate that the algorithm solves the worst performance problem in an efficient manner. The proposed algorithm was also successfully used in the process of yield maximization, resulting in a 99.65% yield.	best, worst and average case;constrained optimization;constraint (mathematics);entropy maximization;expectation–maximization algorithm;integrated circuit;line search;mathematical optimization;optimization problem;radial (radio);social inequality	Gregor Cijan;Tadej Tuma;Árpád Bürmen	2009	Journal of Circuits, Systems, and Computers	10.1142/S0218126609005617	embedded system;mathematical optimization;real-time computing;computer science;engineering;electrical engineering;integrated circuit	EDA	17.557963998878392	53.75084868892995	149525
61ac0fc25f4a34a986210dad69dd5523a94eb171	sram cell with data-aware power-gating write-asist for near-threshold operation		This paper proposes a FinFET-based SRAM cell with data-aware power-gating write-assist to achieve both high read stability and write ability by using read-decoupled access transistors and power-gating PMOSs, respectively, for near-threshold operation. By adaptively cutting off the power-gating PMOS depending on the written data, the write disturbance from power supply can be eliminated, which facilitates more reliable write operation without any additional write assist circuit. Bit-interleaving scheme can be implemented in the proposed SRAM for soft error immunity while ensuring sufficient hold stability in half-selected cells during write operation. The proposed SRAM achieves read stability yield of 8.4σ and write ability yield of 6.1σ and consumes 0.47 pJ energy per operation at supply voltage of 0.4 V, a near-threshold voltage, in a 22-nm FinFET technology.	cell (microprocessor);discharger;forward error correction;gradient boosting;memory cell (binary);pmos logic;power gating;power supply;soft error;static random-access memory;transistor	Tae Woo Oh;Seong-ook Jung	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351146	static random-access memory;pmos logic;power gating;electronic engineering;voltage;transistor;computer science;soft error	Arch	18.058202476642254	59.36799262412087	149581
3cf6860038c2be66fc24d187557eecb6f98fc2e5	hierarchical critical area extraction with the eye tool	defect sensitivity visualisation aids;fabrication;probability integrated circuit layout circuit layout cad integrated circuit yield;control systems;yield prediction;software tool;edinburgh yield estimator;probability;circuit faults;integrated circuit yield;integrated circuit layout;computational geometry;data mining fabrication control systems yield estimation integrated circuit yield circuit faults software tools computational geometry design optimization integrated circuit manufacture;fault probability maps;yield estimation;data mining;design optimization;ic layout;commercial ic mask data;circuit layout cad;software tools;hierarchical critical area extraction;fast critical area algorithms;fault probability maps hierarchical critical area extraction eye tool software tool commercial ic mask data edinburgh yield estimator fast critical area algorithms large devices yield prediction ic layout defect sensitivity visualisation aids;large devices;integrated circuit manufacture;eye tool	A software tool to extract critical areas from commercial IC mask data is reported. The EYE (Edinburgh Yield Estimator) tool uses fast O(NlogN) critical area algorithms and is able to perform operations hierarchically making it suitable for use on large devices. The tool has applications in yield prediction, optimising the manufacturability of IC layout, and the generation of defect sensitivity visualisation aids in the form of fault probability maps.		Gerard A. Allan;Anthony J. Walton	1995		10.1109/DFTVS.1995.476934	electronic engineering;computational geometry;engineering;electrical engineering;integrated circuit layout;engineering drawing;statistics;computer engineering	EDA	23.643636470369056	53.643216480134654	149657
71c3aabbdf2cf7b537fbef59b61dfa2dad5c3814	co-simulations of electromagnetic and thermal effects in electronic circuits using non-conformal numerical methods	silicon;memory partitioning;behavioral synthesis;metals;three dimensional integrated circuits high speed integrated circuits integrated circuit interconnections integrated circuit packaging numerical analysis;memory scheduling;silicon integrated circuit interconnections metals educational institutions conductivity couplings;numerical analysis;conductivity;integrated circuit interconnections;couplings;integrated circuit packaging;deep sub micrometer cmos technologies electromagnetic effects thermal effects electronic circuits nonconformal numerical methods metal layers 3d stacking technique integrated circuits package products clock frequency edge rates feature size 3d interconnect technologies high speed systems signal integrity effects signal delay;high speed integrated circuits;three dimensional integrated circuits	Advances in interconnect technologies, such as the increase of the number of metal layers and 3-D stacking technique, have paved the way for higher functionality and superior performance while reducing size, power, and cost in today's integrated circuits and package products. With the increase of clock frequency and edge rates as well as the continuously downscaling of feature size and 3-D interconnect technologies in high-speed systems, signal integrity (SI) effects such as signal delay, reflection, attenuation, dispersion and crosstalk have become one of the dominant factors in current deep sub-micrometer CMOS technologies limiting overall performance of high-speed systems.  Another major difficulty is the power integrity (PI) issue. For example, IR drop is caused by the finite resistivity (R) of metals and current (I) drawn off from the power/ground planes. Chip designs are susceptible to IR drop, especially when the integrated circuit (IC) supply voltage Vdd decreases with the scaling of silicon processes. IR drop is proportional to the resistance of the power/ground plane. When the resistance of power/ground plane increases, with the shrinking of complex geometries, the IR drops will in turn increase as well. To further confound the difficult issue, the rise of the thermal temperature due to the current carrying interconnects also has tremendous impact on the IC performance and reliability. Current flow in a VLSI interconnect can cause a power dissipation, which is referred to as Joule heating or self-heating. The Joule heating effects are becoming increasingly significant with the shrinking scale of silicon process because of the increase of on-chip power density, inclusion of more metal layers with higher densities, and the use of dielectric materials with lower thermal conductivities. Subsequently, the temperature effects on the electrical performance of 3D systems should be carefully considered as well in the electrical designs.  Unfortunately, almost all practical SI and PI problems are multi-physics in nature, and various physical phenomena are usually interacting and coupling to each other. For instance, the resistivity of most conducting metals increases linearly with the increases of the surrounding temperature resulting from the Joule heating by electric currents flowing through the conductors. Therefore, in order to accurately characterize the performance of high power integrated circuits (ICs), packages and printed circuit boards (PCBs), it is essential to account for both electric and thermal effects and the intimate couplings between them.  In this paper, we propose to use non-conformal, non-overlapping domain decomposition methods (DDMs) for the signal integrity analyses and thermal-aware DC IR drop co-analysis of high-power IC-Package-PCBs. The proposed DDM starts by partitioning the composite device into inhomogeneous sub-regions with temperature dependent material properties. Subsequently, each sub-domain is meshed independently according to its own characteristic features. As a result, the troublesome mesh generation task for complex ICs can be greatly subdued. Moreover, the proposed thermal-aware DC IR drop co-analysis applies the non-conformal DDMs for both conduction and steady state heat transfer analyses with a two-way coupling between them. Furthermore, we extend the non-conformal DDM to study transient thermal temperature distribution within complex ICs. Numerical examples, including an IC package and an IC-Package-PCB, demonstrate the flexibility and potentials of the proposed non-conformal DDMs for SI analyses and thermal-aware DC IR-drop co-analysis.	cmos;clock rate;crosstalk;domain decomposition methods;downscaling;dual in-line package;electrical connection;electronic circuit;image scaling;integrated circuit;interaction;joule;mesh generation;numerical method;power semiconductor device;printed circuit board;printing;signal integrity;simulation;stacking;steady state;value-driven design;very-large-scale integration	Jin-Fa Lee;Yang Shao;Zhen Peng	2012	2012 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1145/2429384.2429489	embedded system;electronic engineering;telecommunications;numerical analysis;integrated circuit packaging;signal integrity;electrical engineering;conductivity;coupling;silicon;algorithm	EDA	22.248642634967293	60.07928575310912	150340
eb0b6de99d871246f6815116d6f12095fd3354af	statistical circuit performance variability minimization under manufacturing variations	performance measure;circuit performance variability minimization;performance evaluation;circuit design;circuit optimization minimization principal component analysis manufacturing processes random variables response surface methodology mos devices pulp manufacturing electronic components fluid flow measurement;response surface models;manufacturing testing;response surface model;manufacturing processes;design of experiments;manufacturing variation;production testing circuit optimisation design of experiments integrated circuit manufacture integrated circuit reliability manufacturing processes performance evaluation;0 18 micron;circuit variability;random variable;statistical design;integrated circuit reliability;0 18 micron circuit performance variability minimization manufacturing variation statistical variations statistical design response surface models rsm circuit variability circuit designable parameters;production testing;circuit optimisation;rsm;circuit designable parameters;statistical variations;integrated circuit manufacture;circuit optimization;device modeling;design of experiment	In this paper, we present a flow to minimize the variability of a circuit performance measure due to the statistical variations in the manufacturing process. The flow starts with 0.18mum worst case corner device model parameters and transforms them to uncorrelated random variables for the statistical design of experiments which generate response surface models (RSM) of the circuit variability in terms of the circuit designable parameters. The minimization of variance due to process fluctuations is performed on the RSM functions by changing the circuit designable parameters, such as channel widths of the devices without causing excessive mean response shifts. Finally, the mean response is re-adjusted using the designable parameters that are not used in the variability minimization	best, worst and average case;design of experiments;experiment;heart rate variability;response surface methodology;spatial variability	Ayhan A. Mutlu;Charles Kwong;Abir Mukherjee;Mahmudur Rahman	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693262	electronic engineering;engineering;mathematics;design of experiments;engineering drawing;statistics	EDA	23.40184449016607	56.80931566640013	152014
10908577a8ef7ed5b45e620ad5879720461a3c2b	considering the effect of process variations during the isa extension design flow	isa extension design flow;conventional merit;systematic variation modeling;pruning technique;process variation;performance yield;identification phase;branch-and-bound approach;cis extension;technique bridge;selection phase;effective pruning technique	In this paper, we present a technique for custom instruction (CI) extension considering process variations. The technique bridges the gap between the high level custom instruction extension and chip fabrication in nanotechnologies. In particular, instead of using the conventional static timing analysis (STA), it utilizes statistical static timing analysis (SSTA). Therefore, the approach becomes probabilistic where the delay of each CI is modeled by a Probability Density Function (PDF). Using this probabilistic approach, different subsets of the CIs extension are identified to meet predefined constraints (identification phase) and eventually selected for realization to improve a given merit function (selection phase). In the identification phase, performance yield under both random and systematic variations is added as a constraint. Also, a pruning technique is proposed to decrease the runtime of the systematic variation modeling. The results show that the technique reduces the number of the CIs which need systematic variation modeling by about 24.6% for the cases studied in this work. In the selection phase, both greedy and branch-and-bound approaches are used. In the greedy approach, the conventional merit function based on the cycle saving and area is modified to include the performance yield. The results show the proposed merit function leads to about 3.2% increasing in the speedup. In the branch-and-bound method an effective pruning technique is described to reduce the runtime. The pruning technique is able to reduce the search space about 62%.	branch and bound;design flow (eda);greedy algorithm;high-level programming language;portable document format;semiconductor device fabrication;speedup;statistical static timing analysis	Mehdi Kamal;Ali Afzali-Kusha;Saeed Safari;Massoud Pedram	2013	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2012.09.014	embedded system;parallel computing;real-time computing;simulation;computer science;electrical engineering;operating system;algorithm	EDA	22.596405160690793	57.4190335412734	152026
4fdccff386df1d384aa051167e5a51a23efcb892	a simple piecewise model of reset/set transitions in bipolar reram memristive devices		In this paper, we develop and test a piecewise model for the reset and set transitions of a bipolar ReRAM memristive device in the flux-charge space, instead of the usual voltage–current one. To do so, we consider the devices as memristors. The model used is very simple and provides accurate simulation results. It also allows the development of simple expressions for the conductance and power consumption, as well as the characterization of the ReRAM memristive device in voltage–current domain by using two points for any reset or set cycle. We consider the case of a ramp input signal with different slopes to obtain the model parameters, and we compare the predictions of our model with experimental results.	conductance (graph);electronic circuit simulation;memristor;ramp simulation software for modelling reliability, availability and maintainability;resistive random-access memory;segmented regression	Mohamad Moner Al Chawa;Carol de Benito;Rodrigo Picos	2018	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2018.2830412	memristor;memory management;piecewise;conductance;electronic engineering;expression (mathematics);non-volatile memory;resistive random-access memory;mathematics	EDA	22.203830225397247	59.72983034340674	152104
6b99a7bc3e4eeec96985b5aa65d4f8bd300155f4	mdsp: a high-performance low-power dsp architecture	digital circuit;diseno circuito;haute performance;arquitectura circuito;integrated circuit;multiprocessor;etude experimentale;estudio comparativo;flot donnee;circuit design;circuit architecture;circuito integrado;flujo datos;asynchronous circuit;experimental result;circuit numerique;puce traitement signal numerique;etude comparative;jeu instruction;low power;circuit asynchrone;flot commande;comparative study;control flow;architecture circuit;circuito numerico;resultado experimental;puissance faible;alto rendimiento;circuito asincrono;digital signal processing chips;conception circuit;flujo control;multiprocesador;data flow;resultat experimental;high performance;embedded processor;estudio experimental;concurrent process;circuit integre;instruction sets;potencia debil;multiprocesseur	The Multi-process DSP architecture (MDSP) is presented and evaluated for high-performance low-power embedded processors. The proposed architecture extends the standard control-flow DSP architecture with simple data-flow primitives. Such primitives are used to generate concurrent processes at run-time, which independently generate and consume data without accessing the instruction flow. We have evaluated the MDSP proposal by designing an asynchronous DSP core, since previous studies showed it to be better suited as an implementation technique. The experiment showed interesting improvements in overall performance and external device management compared to the current commercially available DSP cores. It also showed good scalability and compiler-friendliness with respect to alternative approaches.	digital signal processor;mdsp	Francesco Pessolano;Joep L. W. Kessels;Ad M. G. Peeters	2002		10.1007/3-540-45716-X_4	embedded system;data flow diagram;multiprocessing;asynchronous circuit;telecommunications;computer science;electrical engineering;integrated circuit;circuit design;comparative research;instruction set;control flow;digital electronics	EDA	18.203450991366267	54.64971676865257	152185
251f68690bae400c0b8ce4861adc5349439c6de4	design and reliability analysis of multiple valued logic gates using carbon nanotube fets	carbon nanotube field effect transistors cntfets;reliability;carbon nanotube field effect transistors;semiconductor device reliability carbon nanotube field effect transistors logic gates;stochastic computational models scms multiple valued logic mvl carbon nanotube field effect transistors cntfets reliability;semiconductor device reliability;multiple valued logic mvl;cntfets logic gates inverters integrated circuit reliability integrated circuit modeling probabilistic logic;stochastic computational models scms;logic gates;c multiple valued logic gates carbon nanotube fet cntfet nanometric technologies multiple valued logic circuits mvl circuits design information density pseudo complementary mvl design carbon nanotube field effect transistors transistor level reliability analysis mvl gates logic level analysis scalability stochastic computational models	With emerging nanometric technologies, multiple valued logic (MVL) circuits have attracted significant attention due to advantages in information density and operating speed. In this paper, a pseudo complementary MVL design is initially proposed for implementations using carbon nanotube field effect transistors (CNTFETs). This design utilizes no resistors in its operation. To account for the properties and fabrication non-idealities of CNTFETs, a transistor-level reliability analysis is proposed to accurately estimate the error rates of MVL gates. This approach considers gate structures and their operation, so it yields a more realistic framework than a logic-level analysis of reliability. To achieve scalability, stochastic computational models are developed to accurately and efficiently analyze MVL gates; the extension of these models to circuits is briefly discussed.	computational model;field effect (semiconductor);information design;logic gate;logic level;reliability engineering;scalability;transistor	Jinghang Liang;Jie Han;Linbin Chen;Fabrizio Lombardi	2012	2012 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)	10.1145/2765491.2765515	electronic engineering;nmos logic;logic family;electrical engineering;pass transistor logic;nanotechnology;mathematics;pmos logic	EDA	21.80517794586368	58.632545914223336	152396
2b6b8f4ea628ca5d15188f336edbef414a671ee4	very-low voltage (vlv) and vlv ratio (vlvr) testing for quality, reliability, and outlier detection	integrated circuit testing stress low voltage random access memory delay qualifications temperature fabrication failure analysis cmos process;cmos integrated circuits;low power electronics cmos integrated circuits failure analysis fault diagnosis integrated circuit reliability integrated circuit testing;outlier detection;failure analysis;low voltage;low power;low power cmos process very low voltage vlv ratio testing outlier detection minimum voltage testing product quality product reliability stress induced failure mechanism;low power electronics;failure mechanism;integrated circuit testing;integrated circuit reliability;product quality;fault diagnosis	A new implementation of very-low voltage (VLV) and minimum voltage (Min_Vdd) testing, the VLV ratio test (VLVR) is proposed to improve product quality and reliability by detecting fabrication and test outliers. The VLVR technique was also used to assist in the diagnosis of a stress induced failure mechanism in a 0.13 micron low power (LP) CMOS process	anomaly detection;cmos;failure cause;sensor;turing test	Jeffrey L. Roehr	2006	2006 IEEE International Test Conference	10.1109/TEST.2006.297690	reliability engineering;failure analysis;anomaly detection;electronic engineering;real-time computing;computer science;engineering;electrical engineering;operating system;low voltage;cmos;low-power electronics	EDA	22.65209239821954	54.874006996925935	152513
39e826b5d4148a14be501a5a810d276a133106ac	leakage power analysis and reduction during behavioral synthesis	high level leakage power analysis;libraries;modelizacion;rtl module library;switching activity;power analysis;algorithm performance;behavioral synthesis;voltage threshold;integrated circuit;registro rtl;implementation;concepcion optimal;leakage reduction;conception optimale;circuit vlsi;optimization method;datapath;circuito integrado;leak detection;synthese haut niveau;indexing terms;metodo optimizacion;sintesis alto nivel;experimental result;champ fuite;modelisation;ejecucion;high level synthesis;integrated circuit design;low leakage optimization;vlsi circuit;total power;low power;dual threshold voltage technology;leakage power;leakage currents high level synthesis low power electronics vlsi circuit optimisation circuit cad circuit analysis computing integrated circuit design;leakage currents;energy consumption;resultado algoritmo;threshold voltage;power consumption estimation;analyse energetique;low power electronics;power optimization;performance algorithme;methode optimisation;energy analysis;resultado experimental;niveau transfert registre;leakage flux;vlsi;leakage power reduction algorithm;optimal design;device level models;power generation;register transfer level module library;seuil tension;circuit cad;power consumption;circuito vlsi;consommation energie electrique;circuit optimisation;resultat experimental;circuit analysis computing;modeling;register transfer level;electronique faible puissance;algorithm design and analysis;analisis energetico;benchmark testing;circuit synthesis;circuit integre;campo fuga;dual threshold voltage technology high level leakage power analysis leakage power reduction algorithm behavioral synthesis high level synthesis power optimization device level models register transfer level module library rtl module library power consumption estimation frequently idle module extraction datapath low leakage optimization;frequently idle module extraction;umbral tension	This paper presents a high-level leakage power analysis and reduction algorithm. The algorithm uses device-level models for leakage to precharacterize a given register-transfer level module library. This is used to estimate the power consumption of a circuit due to leakage. The algorithm can also identify and extract the frequently idle modules in the datapath, which may be targeted for low-leakage optimization. Leakage optimization is based on the use of dual threshold voltage (V/sub T/) technology. The algorithm prioritizes modules giving a high-level synthesis system an indication of where most gains for leakage reduction may be found. We tested our algorithm using a number of benchmarks from various sources. We ran a series of experiments by integrating our algorithm into a low-power high-level synthesis system. In addition to reducing the power consumption due to switching activity, our algorithm provides the high-level synthesis system with the ability to detect and reduce leakage power consumption, hence, further reducing total power consumption. This is shown over a number of technology generations. The trend in these generations indicates that leakage becomes the dominant component of power at smaller feature size and lower supply voltages. Results show that using a dual-V/sub T/ library during high-level synthesis can reduce leakage power by an average of 58% for the different technology generations. Total power can be reduced by an average of 15.0%-45.0% for 0.18-0.07 /spl mu/m technologies, respectively. The contribution of leakage power to overall power consumption ranges from 22.6% to 56.2%. Our approach reduced these values to 11.7%-26.9%.		Kamal S. Khouri;Niraj K. Jha	2002	IEEE Trans. VLSI Syst.	10.1109/TVLSI.2002.808436	electricity generation;embedded system;algorithm design;benchmark;electronic engineering;systems modeling;power analysis;index term;magnetic flux leakage;computer science;engineering;electrical engineering;optimal design;integrated circuit;very-large-scale integration;threshold voltage;high-level synthesis;implementation;power optimization;register-transfer level;low-power electronics;statistics;integrated circuit design	EDA	17.901480593925235	54.60407767410783	153120
a5d78aa06296442d7d5c68fd9b514e79db5ea4d5	power delivery dynamics and its impact on silicon validation	silicon;microprocessors;voltage droop dynamics;microprocessor;voltage fluctuations;charge sequence optimization;silicon validation;functional testing;very large scale integration;transistor currents;power delivery dynamics;power system dynamics;testing;packaging;power system dynamics silicon testing frequency very large scale integration voltage fluctuations capacitors packaging microprocessors voltage measurement;decoupling capacitor placement;post silicon functional speed paths;post silicon voltage measurements;functional testing mode;low power;global voltage fluctuations;capacitors;transistors;transistors integrated circuit testing microprocessor chips;integrated circuit testing;local voltage fluctuations;decoupling capacitor;test generation;scan based testing;post silicon voltage measurements power delivery dynamics silicon validation transistor currents functional testing mode scan based testing local voltage fluctuations global voltage fluctuations decoupling capacitor placement post silicon functional speed paths voltage droop dynamics charge sequence optimization microprocessor;frequency;voltage measurement;microprocessor chips	The drive to low power regimes has highlighted the impact of power delivery dynamics on the operation and testing of the die. In normal functional mode, the transient draw of transistor currents will create scenarios that, via voltage droop, may impact the maximum operational frequency of the die. The same is true for functional testing. Even a need to replicate these effects for scan-based testing is not easily done without understanding both the dynamics of the power delivery as well as its impact on frequency during functional operation and functional testing. In this talk, power delivery dynamics will be discussed. Global and local voltage fluctuations will be mapped into the roughly four different kinds of voltage droops including the frequencies associated with each. It will be shown how decoupling capacitor placement either on a die or externally on the package will affect each of the kinds of voltage droops described. A description of the different die events, in either functional or scan mode, which can set up frequencies to impact each of the kinds of voltage droops, will be given. It will be shown that postsilicon functional speed-paths are often a clear function of the voltage droop dynamics. The second part of the problem consists in creating tests to excite the correct sequence of voltage droops to cause impact on the total droop seen by the transistors. We will discuss an early flow based on machine abstraction, charge assignment to reflect current draw, and charge sequence optimization to enable one to accomplish test-generation for droop post-silicon and the application of this flow on a major microprocessor. Post-silicon voltage measurements will be used to demonstrate the complexity needed to achieve a large droop impact in a short test which would be equivalent to running multiple functional tests.	verification and validation	Eli Chiprout	2010		10.1109/VTS.2010.5469552	control engineering;packaging and labeling;electronic engineering;capacitor;engineering;electrical engineering;frequency;functional testing;software testing;very-large-scale integration;silicon;transistor;quantum mechanics;decoupling capacitor	Robotics	22.155050230335714	54.671023169713635	153238
0baee18946caab409b465292fb8860e1b883f68e	new electromigration modeling and analysis considering time-varying temperature and current densities	stress current density mathematical model wires metals equations numerical models;vlsi current density electromigration integrated circuit modelling integrated circuit reliability nucleation three dimensional integrated circuits;fast stress calculation method electromigration modeling time varying temperature reliability issue vlsi technologies em models constant current density system level run time physics based em model 3d ic temperature profiles em induced lifetime nucleation phase growth phase	Electromigration (EM) is projected to be the major reliability issue for current and future VLSI technologies. However, existing EM models and assessment techniques are mainly based on the constant current density and temperature. Such models will not work well at the system level as the current density (power) and temperature are changing with time due to different tasks (their loans) applied at run time. Existing EM approaches using average current density or temperature, however, will lead to significant errors as shown in this work. In this paper, we propose a new physics-based EM model considering time-varying temperature and current density, which reflects a more practical chip working conditions especially for multi-core and emerging 3D ICs. We study the impacts of the time-varying current densities and temperature profiles on EM-induced lifetime of a wire for both nucleation phase and growth phase. We propose a fast stress calculation method for given time-varying temperature and current densities for the nucleation phase. We further develop new formulae to compute the resistance changes in growth phase due to changing temperature and current densities. Experimental results show that the proposed method shows an excellent agreement with the detailed numerical analysis but with much improved efficiency.	central processing unit;constant current;electromigration;multi-core processor;numerical analysis;run time (program lifecycle phase);very-large-scale integration	Hai-Bao Chen;Sheldon X.-D. Tan;Xin Huang;Valeriy Sukharev	2015	The 20th Asia and South Pacific Design Automation Conference	10.1109/ASPDAC.2015.7059030	electromigration;electronic engineering;electrical engineering;black's equation	EDA	22.34549959053443	59.366084122661	154050
9473cd8d4c279fb6518f593e3be3341b8767c349	fast wat test structure for measuring vt variance based on latch-based comparators	semiconductor device measurement;current measurement;transistors;mathematical model;electrical resistance measurement;correlation;voltage measurement	As the technology of IC manufacturing continually scales down, process variations become more and more crucial than before. To statistically characterize local process variations, the traditional array-based test structure measures threshold voltage (Vt) for a sufficiently large number of devices-under-test (DUTs). However, the array-based test structure requires long time for DUT-by-DUT measurement; furthermore, it suffers from significant IR drop or leakage current due to the large number of DUTs, which results in the loss of measurement accuracy. In this paper, we present a novel sense-amplifier-based test structure that can monitor process variations based on rapid characterization of Vt variance, with marginal error of accuracy. A test-chip containing 120 NMOS and 120 PMOS DUTs has been implemented in 28nm CMOS process technology. Various experiments reveal promising efficiency and accuracy of the proposed test structure, for characterizing Vt variance.	cmos;comparator;device under test;die shrink;experiment;marginal model;nmos logic;pmos logic;sense amplifier;spectral leakage;synergy	Kao-Chi Lee;Kai-Chiang Wu;Chih-Ying Tsai;Mango Chia-Tso Chao	2017	2017 IEEE 35th VLSI Test Symposium (VTS)	10.1109/VTS.2017.7928928	electronic engineering;engineering;electrical engineering;mathematical model;forensic engineering;correlation;transistor;statistics	EDA	22.56857557401074	55.84877202766491	154420
29b539e04f2365abfad39af582828e466016c1ca	process variation tolerant 9t sram bitcell design	size 32 nm process variation tolerant 9t sram bitcell design nine transistor static random access memory bitcell energy constraint low voltage constraint reliability standard six transistor sram bitcell 6t sram bitcell read stability read static noise margin write noise margin;process variation;static random access memory;sram chips integrated circuit design integrated circuit reliability;static noise margin;integrated circuit design;low voltage;integrated circuit reliability;random access memory transistors stability analysis sensors noise integrated circuit modeling circuit stability;reading and writing;sram chips	In this paper, a nine-transistor (9T) Static Random Access Memory (SRAM) bitcell for the low voltage and energy constraint applications is proposed. It is well known that in sub-threshold regime, reliability and process variations are the main design challenges, and standard six-transistor (6T) SRAM bitcell fails to operate in sub-VTH. The proposed design has better read stability and improved process variation tolerant as compared to standard 6T SRAM at low voltage. Simulation results based on 32nm technology node shows that there is 37% improvement in the read stability as compared to standard 6T SRAM bitcell. The proposed design also address the conflicting read and write requirements, therefore, one can optimize the read static noise margin (SNM), write noise margin and write speed for a particular application by selecting the bitcell ratios for read and write operations.	access time;bit cell;gnu nano;noise margin;random access;requirement;scalability;semiconductor device fabrication;simulation;static random-access memory;transistor	G. K. Reddy;Kapil Jainwal;Jawar Singh;Saraju P. Mohanty	2012	Thirteenth International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2012.6187539	electronic engineering;parallel computing;real-time computing;static random-access memory;computer science;engineering;electrical engineering;low voltage;process variation;integrated circuit design	EDA	18.464340627380658	59.199007828526355	154554
6a112f9ff1a882c5cb721797f99f35c91e7da268	systematic approximate logic optimization using don't care conditions	erbium;circuit faults;boolean functions;approximation algorithms;approximate computing error tolerable latency;logic gates;erbium approximation algorithms logic gates boolean functions approximate computing circuit faults hardware;approximate computing;hardware	Approximate computing has emerged as a new design paradigm in wide variety error resilient applications to decrease the hardware cost in an acceptable deviation from the nominal output values. In this paper, a logic approximation technique is proposed which takes the advantage of error budget in error tolerant applications to achieve an approximation circuit with smaller area and latency. Moreover, a heuristic is proposed to prune the search space, which increases the scalability of the proposed technique. The results show 38.3% reduction in area and 26.6× speed up compared to the state-of-the-art approximation techniques.	approximate computing;approximation algorithm;don't-care term;error-tolerant design;heuristic;logic optimization;mathematical optimization;programming paradigm;scalability;speedup	Sahand Salamat;Mehrnaz Ahmadi;Bijan Alizadeh;Masahiro Fujita	2017	2017 18th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2017.7918352	boolean circuit;mathematical optimization;electronic engineering;erbium;logic gate;computer science;theoretical computer science;mathematics;boolean function;register-transfer level;approximation algorithm;algorithm	Arch	19.569434118274692	56.1673595090308	155520
5ab1114bec3c8b2dfd0094648b9798b066376101	energy optimization of pipelined digital systems using circuit sizing and supply scaling	energy economy;traitement pipeline;digital circuit;economie energie;optimal solution;optimal energy criteria;economia energia;dimensionnement;evaluation performance;optimisation;solution optimale;energy delay;performance evaluation;supply voltage effects circuit sizing digital system energy delay characteristics optimal criteria optimization methodology pipelined stage;optimizacion;logic design;digital systems circuits pipelines delay optimization methods voltage design optimization power engineering and energy throughput displays;characteristics;evaluacion prestacion;delay characteristics;dimensioning;systeme numerique;design optimization;supply voltage effects;pipeline stages;energy delay curve;circuit numerique;power engineering and energy;digital design energy optimization pipelined digital systems circuit sizing supply scaling pipeline stages optimal energy criteria media datapath energy delay curve voltage scaling;low voltage;digital system;energy optimization;baja tension;circuit optimisation logic design pipeline arithmetic;pipelined stage;pipelined digital systems;media datapath;solucion optima;digital systems;displays;pipelines;voltage;digital design;optimal criteria;circuito numerico;basse tension;optimality criteria;circuit sizing;energy delay characteristics;sistema numerico;optimization;circuits;supply scaling;temps retard;delay time;optimization methodology;energy 8211;circuit optimisation;dimensionamiento;voltage scaling;tiempo retardo;pipeline arithmetic;pipeline processing;energy saving;throughput;optimization methods	We present a systematic method for minimizing the energy of pipelined digital systems, through joint optimization of each pipeline stage and the system. A pipeline stage with a constant load can either be optimized for delay at a given input size, minimized for energy at a fixed delay, or have delay traded off for energy at a fixed input size. The results of these optimizations are combined to yield the design region for energy and delay. At the system level with a fixed throughput constraint, the sensitivities to input size and output load of all pipeline stages form the optimal energy criteria that provide a systematic method to minimize the total system energy. This method is applied to a media datapath, where we show up to 37% energy saving for a fixed performance. The minimal energy-delay curve of the system obtained through application of this method demonstrates similar characteristics as that of a single pipeline stage. With voltage scaling, the optimal solution displays a strong dependency between delay, energy, and supply voltage. The proper tradeoff between these entities makes a fundamental impact on efficient digital design.	common criteria;datapath;diagram;digital electronics;dynamic voltage scaling;entity;image scaling;information;input/output;interdependence;logic gate;logic synthesis;mathematical optimization;pipeline (computing);systems design;throughput;trusted computer system evaluation criteria	Hoang Q. Dao;Bart R. Zeydel;Vojin G. Oklobdzija	2006	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2005.863760	embedded system;throughput;electronic engineering;logic synthesis;real-time computing;voltage;simulation;computer science;engineering;electrical engineering;energy economics;low voltage;dimensioning	EDA	17.79410140662663	54.73869597640909	155555
62f2a9cbb1769aa6a76afe6610f0b46c48e726d1	temperature-aware nbti modeling techniques in digital circuits	negative bias temperature instability (nbti);reliability;temperature;negative bias temperature instability;digital circuits	Negative bias temperature instability (NBTI) has become a critical reliability phenomena in advanced CMOS technology. In this paper, we propose an analytical temperature-aware dynamic NBTI model, which can be used in two circuit operation cases: executing tasks with different temperatures, and switching between active and standby mode. A PMOS Vth degradation model and a digital circuits’ temporal performance degradation estimation method are developed based on our NBTI model. The simulation results show that: 1) the execution of a low temperature task can decrease ΔVth due to NBTI by 24.5%; 2) switching to standby mode can decrease ΔVth by 52.3%; 3) for ISCAS85 benchmark circuits, the delay degradation can decrease significantly if the circuit execute low temperature task or switch to standby mode; 4) we have also observed the execution time’s ratio of different tasks and the ratio of active to standby time both have a considerable impact on NBTI effect. key words: negative bias temperature instability (NBTI), temperature, reliability	benchmark (computing);cmos;digital electronics;elegant degradation;negative-bias temperature instability;pmos logic;run time (program lifecycle phase);simulation;sleep mode	Hong Luo;Yu Wang;Rong Luo;Huazhong Yang;Yuan Xie	2009	IEICE Transactions		negative-bias temperature instability;electronic engineering;real-time computing;temperature;engineering;electrical engineering;reliability;digital electronics;physics	EDA	20.03751393684915	58.78371793813538	156258
479a0f98d1d0b68d419ba24936d6c28fe5298c79	test pattern length required to reach the desired fault coverage	design for testability very large scale integration integrated circuit testing;design for testability;pattern compression coefficient test pattern length desired fault coverage execution test pattern single stuck at fault coverage atpg processing time vlsi testing negative binominal distribution;automatic test pattern generation;integrated circuit testing automatic test pattern generation design for testability vlsi;integrated circuit testing;vlsi;fault coverage	An essential point in VLSI (very large scale integration) testing is to build a testing strategy that can produce the best solution for the selection of a testing method which takes into account such factors as quality, cost (chip area, yield, testing cost, etc.), performances, power dissipation and time for the development project, as well as for the determination of the role of the tester. For this purpose, it is important to know the relationship between the fault coverage F and the pattern length L in order to obtain the testing cost and the defect level that will serve as the quality indicator. Some studies have already disclosed the relationship between the fault coverage of pseudo random-number generation patterns and the pattern length in BIST. In these studies, the discussion uses a delta function for the distribution function of the detected faults. However we discuss conventional testing methods, with the exception of BIST, as well as the relationship between the fault coverage and the pattern length based on the assumption that the distribution function of the detected faults is a gamma distribution. Under this assumption, the probability of fault detection becomes the negative binominal distribution from the binominal distribution and is approximated as the negative hypergeometric distribution. Since, based on the definition of the negative hypergeometrical distribution and on the definition of the negative binominal distribution, the clustering parameter k plus the number of detected faults M represents the number of trials, the subject of our discussion therefore indicates the length of the execution test pattern. Then we assume that pattern compression and expansion are in proportion to the total fault number N and to the detected fault number. And we assume that pattern compression and expansion are in inverse proportion to the number of terminals P. As we can assume that the total number of detected faults is given by the greatest value of detected faults, we assume that the total number of faults detected by the test pattern is the mean value plus 3 sigmas of the negative binominal distribution. Consequently we can get the execution test pattern length. Here, with C as the pattern compression coefficient, the execution test pattern length L is given by the following equation. L=CN/P M(k+M) =CN/P [kF/(1-F)+3(kF)/(1-F)] [k+kF/(1-F)+3(kF)/(1-F)]	approximation algorithm;built-in self-test;cluster analysis;coefficient;dirac delta function;fault coverage;fault detection and isolation;integrated circuit;performance;pseudorandomness;random number generation;software bug;test card;very-large-scale integration	Junichi Hirase	2003		10.1109/ATS.2003.1250878	reliability engineering;electronic engineering;real-time computing;fault coverage;engineering;stuck-at fault;automatic test pattern generation;test compression;design for testing;very-large-scale integration	HPC	22.746606922793287	54.37161608056569	156344
63871542c13a28f82a7ec747c3dc99b824db3bc6	a new asymmetric skewed buffer design for runtime leakage power reduction	0 13 micron asymmetric skewed buffer design runtime leakage power reduction subthreshold leakage cmos noninverting buffer inverters nmos pmos transistors high vt devices dynamic power dissipation propagation delay;cmos integrated circuits;leakage current;time window;buffer storage;runtime mos devices threshold voltage inverters switches transistors degradation delay subthreshold current leakage current;leakage power reduction;transistors invertors leakage currents buffer storage cmos memory circuits cmos integrated circuits;cmos memory circuits;leakage currents;propagation delay;transistors;power dissipation;invertors	A novel asymmetric skewed buffer is proposed to reduce the subthreshold leakage of standard CMOS noninverting buffers. Using oppositely skewed inverters to drive the NMOS and PMOS of the second inverter creates a small time window during which both transistors are conducting, enhancing speed. Given this performance advantage over traditional CMOS buffers, the leakage current can then be suppressed by either downsizing transistors or by assigning high-Vt devices. Based on simulation results for a 0.13 /spl mu/m technology, leakage is reduced by up to 4.4 times when the input is high while maintaining fixed dynamic power dissipation and propagation delay compared to CMOS.	best, worst and average case;cmos;inverter (logic gate);mumps;nmos logic;pmos logic;power inverter;propagation delay;simulation;software propagation;spectral leakage;transistor	Yu-Shiang Lin;Dennis Sylvester	2005	18th International Conference on VLSI Design held jointly with 4th International Conference on Embedded Systems Design	10.1109/ICVD.2005.23	propagation delay;electronic engineering;real-time computing;computer science;engineering;electrical engineering;dissipation;leakage;cmos;transistor;computer network	EDA	17.95311538020537	58.45052933333681	156355
07cc0865764d4974795fdcddd0d672dd0c727e2e	leakage in nano-scale technologies: mechanisms, impact and design considerations	process variation;cmos technology;leakage current;circuit design;leakage current delay cmos technology frequency circuit synthesis permission gate leakage doping profiles voltage energy consumption;doping profiles;permission;process parameters;energy consumption;threshold voltage;power dissipation;voltage;frequency;gate leakage;circuit synthesis	The high leakage current in nano-meter regimes is becoming a significant portion of power dissipation in CMOS circuits as threshold voltage, channel length, and gate oxide thickness are scaled. Consequently, the identification of different leakage components is very important for estimation and reduction of leakage. Moreover, the increasing statistical variation in the process parameters has led to significant variation in the transistor leakage current across and within different dies. Designing with the worst case leakage may cause excessive guard-banding, resulting in a lower performance. This paper explores various intrinsic leakage mechanisms including weak inversion, gate-oxide tunneling and junction leakage etc. Various circuit level techniques to reduce leakage energy and their design trade-off are discussed. We also explore process variation compensating techniques to reduce delay and leakage spread, while meeting power constraint and yield.	and gate;best, worst and average case;cmos;colour banding;darpa network challenge;estimation theory;gnu nano;gate oxide;guard (information security);image scaling;information leakage;semiconductor research corporation;spectral leakage;thickness (graph theory);transistor;tunneling protocol	Amit Agarwal;Chris H. Kim;Saibal Mukhopadhyay;Kaushik Roy	2004	Proceedings. 41st Design Automation Conference, 2004.	10.1145/996566.996571	control engineering;electronic engineering;voltage;engineering;electrical engineering;dissipation;frequency;circuit design;leakage;threshold voltage;process variation;cmos	EDA	19.85897576738664	58.588368273302166	156908
a5343d199ea3f9a88a5f588d273c518d56afb31c	on the robustness of memristor based logic gates	memristors;resistance;biological system modeling;logic gates;threshold voltage;nanowires;switches	As today's CMOS technology is scaling down to its physical limits, it suffers from major challenges such as increased leakage power and reduced reliability. Novel technologies, such as memristors, nanotube, and graphene transistors, are under research as alternatives. Among these technologies, memristor is a promising candidate due to its great scalability, high integration density and near-zero standby power. However, memristor-based logic circuits are facing robustness challenges mainly due to improper values of design parameters (e.g., OFF/ON ratio, control voltages). Moreover, process variation, sneak path currents and parasitic resistance of nanowires also impact the robustness. To realize a robust design, this paper formulates proper constraints for design parameters to guarantee correct functionality of logic gates (e.g., AND). Our proposal is verified with SPICE simulations while taking both device variation and parasitic effects into account. It is observed that the errors due to analytical parameter constraints are typically within 4.5% as compared to simulations.	cmos;graphene;image scaling;logic gate;memristor;parasitic element (electrical networks);spice;scalability;simulation;spectral leakage;transistor	Lei Xie;Hoang Anh Du Nguyen;Jintao Yu;Mottaqiallah Taouil;Said Hamdioui	2017	2017 IEEE 20th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)	10.1109/DDECS.2017.7934559	electronic engineering;memristor;logic gate;network switch;computer science;engineering;electrical engineering;pass transistor logic;threshold voltage;resistance;nanowire	EDA	20.094003412190844	59.00041611780066	157366
380e31d0aca3f122cf24848a1aac042b39b98d99	pre-bond testing of the silicon interposer in 2.5d ics		In interposer-based 2.5D integrated circuits, the passive silicon interposer is the least expensive component in the chip. Thus, it is desirable to test the interposer before bonding to ensure that more expensive and defect-free dies are not stacked on a faulty interposer. We present an efficient method to locate defects in a passive interposer before stacking. The proposed test architecture uses e-fuses that can be programmed to connect or disconnect functional paths inside the interposer. The concept of die footprint is utilized for interconnect testing, and the overall assembly and test flow is described. In order to reduce test time, the concept of weighted critical area is defined and utilized. We present HSPICE simulation results to demonstrate the effectiveness of the pre-bond test solution. The benefit of using weighted critical area is demonstrated using a commercial interposer from GLOBALFOUNDRIES.	2.5d;electrical connection;integrated circuit;interposer;spice 2;simulation;software bug;stacking;test case	Ran Wang;Zipeng Li;Sukeshwar Kannan;Krishnendu Chakrabarty	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;interposer;engineering;electrical engineering;stacking;glass;software testing;silicon	EDA	21.87599105790464	54.07126514137546	157496
064f5f5d22faa832da147d3185dad6ff70f248a7	detectability conditions for interconnection open defect	automatic testing;test generation process detectability conditions interconnection open defects logic testing i sub ddq testing interconnection paths electrical model reliable detection full controllability coupling lines detectability dependency;cmos digital integrated circuits;automatic testing integrated circuit interconnections integrated circuit testing logic testing cmos digital integrated circuits;integrated circuit interconnections;logic testing;integrated circuit testing;test generation;integrated circuit interconnections logic testing controllability fabrication circuit testing voltage predictive models frequency astrophysics optical interconnections	The detectability of interconnection opens by logic and IDDQ testing is investigated. Opens in interconnection paths disconnect the driven gate(s) from the driving gate. An electric& model for interconnection opens is used to predict the detectubility of this type of opens. Using the proposed model, explicit analyticul expressions have been obtained to determine the conditions for reliuble detection of this defect by logic and IDDQ testing. The cases offull controllability and non-full controllability of the signals at the coupling lines have been analyzed. The effect of the trapped charge during fabrication has also been investigated. In addition, it has been found thut the detectability of interconnection opens depends on the metal level where the signals are laid-out. The detectubility dependency of interconnection opens on the test generation process has been analyzed.	iddq testing;interconnection;software bug	Víctor H. Champac;Antonio Zenteno	2000		10.1109/VTEST.2000.843859	embedded system;electronic engineering;engineering;electrical engineering	SE	22.812427533036196	53.65484976892278	157589
2f3205b329fc938e730cde57794f4fcada64ab90	set propagation in micropipelines	pipelines inverters integrated circuit modeling pipeline processing spice transistors load modeling;set propagation asynchronous circuits metastability typical handshake cycle pulse propagation spice simulations pipeline implementation output buffer design matched threshold design high threshold design single sided threshold design schmitt trigger output radiation induced single event transients micropipelines;logic design;buffer circuits;trigger circuits asynchronous circuits buffer circuits logic design radiation hardening electronics;radiation hardening electronics;asynchronous circuits;trigger circuits	Radiation-induced Single Event Transients (SETs) have the potential to create metastability in asynchronous circuits, as they are much shorter than the typical handshake cycle and do not respect the timing closure. Micropipelines have been shown to be effective in filtering those pulses. In this paper1 we investigate the propagation of pulses of critical width through a micropipeline in SPICE simulations. We study how the pipeline implementation, especially the output buffer design (matched threshold, high threshold, Schmitt-trigger) influences the propagation behavior. For the solutions with single sided thresholds we observe a considerable propagation potential of critical pulses that strongly depends, however, on the degree of threshold matching. The Schmitt trigger output, in contrast, reliably filters all pulses which are shorter than a certain threshold while propagating all others securely. At the same time our respective analysis reveals that the cost of the Schmitt trigger stage in terms of performance overheads is also significant, so the choice needs to be carefully balanced with the application requirements.	c-element;metastability in electronics;overhead (computing);power inverter;requirement;spice;schmitt trigger;simulation;soft error;software propagation;time complexity;timing closure;value-driven design;window of opportunity	Thomas Polzer;Andreas Steininger	2013	2013 23rd International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)	10.1109/PATMOS.2013.6662165	embedded system;electronic engineering;logic synthesis;real-time computing;computer science;engineering;electrical engineering;schmitt trigger	EDA	19.937088496937886	56.45708901440781	157839
25d96424d82e2f51f9d9f3db51e62a1f7a27a4f9	a 2.5d integrated voltage regulator using coupled-magnetic-core inductors on silicon interposer delivering 10.8a/mm2	silicon;capa fina;buck converter;couche mince;evaluation performance;switching;convertisseur abaisseur;apilamiento;thin film;densidad corriente;performance evaluation;printed circuit;2 5d integration;bobine inductance;evaluacion prestacion;factor conversion;switched inductor;elemental semiconductors;bobina inductancia;electronica potencia;power inductors;2 5d integration buck converter dc dc power conversion integrated voltage regulator ivr switched inductor;inductors voltage control saturation magnetization perpendicular magnetic anisotropy magnetic separation integrated circuits;power electronics;prototipo;electronique puissance;densite courant;stacking;si integrated voltage regulator coupled magnetic core inductors silicon interposer ivr thin film magnetic power inductors multi phase buck converter ic chip stacking;step down convertor;inductor;convertisseur puissance;conversion rate;regulateur tension;conmutacion;power converter;voltage regulators;power convertors;noyau magnetique;dc dc power conversion;magnetic core;nucleo magnetico;voltage regulators elemental semiconductors magnetic cores power convertors power inductors silicon;silicium;magnetic cores;convertidor reductor;prototype;circuit imprime;commutation;silicio;circuito imprimido;empilement;taux conversion;convertidor potencia;current density;integrated voltage regulator ivr	Energy consumption is a dominant constraint on the performance of modern microprocessors and systems-on-chip. Dynamic voltage and frequency scaling (DVFS) is a promising technique for performing “on-the-fly” energy-performance optimization in the presence of workload variability. Effective implementation of DVFS requires voltage regulators that can provide many independent power supplies and can transition power supply levels on nanosecond timescales, which is not possible with modern board-level voltage regulator modules (VRMs) [1]. Switched-inductor integrated voltage regulators (IVRs) can enable effective implementation of DVFS, eliminating the need for separate VRMs and reducing power distribution network (PDN) impedance requirements by performing dc-dc conversion close to the load while supporting high peak current densities [2-3]. The primary obstacle facing development of IVRs is integration of suitable power inductors. This work presents an early prototype switched-inductor IVR using 2.5D chip stacking for inductor integration.	2.5d;buck converter;characteristic impedance;computation;computer;dynamic frequency scaling;dynamic voltage scaling;electric power conversion;frequency scaling;heart rate variability;image scaling;interactive voice response;interposer;mathematical optimization;microprocessor;performance per watt;power supply;prototype;requirement;stacking;system on a chip;voltage regulator module	Noah Sturcken;Eugene J. O'Sullivan;Naigang Wang;Philipp Herget;Bucknell C. Webb;Lubomyr T. Romankiw;Michele Petracca;Ryan Davies;Robert E. Fontana;Gary M. Decad;Ioannis Kymissis;Angel V. Peterchev;Luca P. Carloni;William J. Gallagher;Kenneth L. Shepard	2012	2012 IEEE International Solid-State Circuits Conference	10.1109/JSSC.2012.2221237	magnetic core;embedded system;voltage regulator;electronic engineering;buck converter;engineering;electrical engineering;stacking;power electronics;prototype;silicon;printed circuit board;inductor;thin film;current density;conversion marketing	Arch	18.5420856336892	55.67086990631611	158461
6847ca581c52764c6bae77465a8a8b163fb704c3	a compact and accurate temperature-dependent model for cmos circuit delay	electron mobility;temperature variations;cmos circuit delay;power density;mosfets;bsim3 cmos circuit delay power density temperature variations compact temperature dependent model cmos inverter alpha power law;delay effects;inverters;compact temperature dependent model;temperature dependence;transient analysis;chip;alpha power law;temperature effect;semiconductor device modeling temperature dependence transient analysis inverters mosfets delay effects electron mobility integrated circuit technology microprocessor chips spice;logic gates;integrated circuit modelling;integrated circuit technology;semiconductor device modeling;cmos logic circuits;logic gates cmos logic circuits integrated circuit modelling;power law;cmos inverter;spice;microprocessor chips;bsim3;transient behavior	With ever increasing power density and temperature variations within chips, it is very important to correctly model temperature effects on the devices in a compact way. In this paper, it is first shown that the temperature dependencies of the mobility and the saturation velocity need to be treated separately in modeling the current with temperature effects. Then, a new compact temperature-dependent model is presented for the on-current and transient behavior of a CMOS inverter based on the alpha-power law. The proposed model is shown to have an excellent agreement with BSIM3.	cmos;power inverter;velocity (software development)	Ja Chun Ku;Yehea I. Ismail	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378655	chip;embedded system;power law;electronic engineering;electron mobility;semiconductor device modeling;logic gate;telecommunications;computer science;engineering;electrical engineering;power density;inverter	Arch	21.66503910373724	58.52510237400613	158485
cb324ed626477f3429203a333e79685b8fd67faf	state space modeling for sub-threshold sram stability analysis	vertical science platform;random access memory;standards;circuit stability;random access memory stability analysis circuit stability solid modeling mathematical model standards integrated circuit modeling;research paper;patents;solid modeling;integrated circuit modeling;stability analysis;mathematical model;research platform;journals;researchers network	Continuous technology scaling has made traditional Static Noise Margin metrics for stability analysis of SRAM bitcells insufficient. Today, Dynamic Noise Margin analyses and metrics are necessary for state-of-the-art bitcell design, especially under problematic low-voltage operation. In this paper, we overview the concept of state-space modeling for dynamic stability analysis, and then develop an analytical method for evaluating SRAM bitcell operation in the sub-threshold regime. An algorithm for state-space and phase-portrait plotting is proposed and shown to correctly predict subthreshold hold and write behavior of standard bitcells in a 40nm CMOS technology. Implementation of the presented technique in mathematical CAD tools provides orders of magnitude faster evaluation than using traditional brute force approaches.	algorithm;bit cell;brute-force search;cmos;computer-aided design;image scaling;noise margin;state space;static random-access memory	Janna Mezhibovsky;Adam Teman;Alexander Fish	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271622	embedded system;electronic engineering;von neumann stability analysis;telecommunications;computer science;engineering;electrical engineering;theoretical computer science;mathematical model;mathematics;solid modeling;algorithm;statistics	EDA	22.3954049846675	59.04001972685463	158998
2a2663deedc5e99fa94d8d42f73f5c1d17fb4380	an evaluation of the impact of gate oxide tunneling on dual-vt-based leakage reduction techniques	multiple channels;leakage reduction;gate oxide tunneling;circuit design;low power;leakage power;threshold voltage;subthreshold leakage;dual threshold voltage;low power circuit design	We evaluate the effectiveness of dual-Vt design in the presence of both subthreshold leakage and leakage due to gate oxide tunneling. At the device level, we use detailed HSPICE simulation to investigate the total leakage impact of three methods of dual-Vt implementation: multiple channel doping, channel length, and oxide thickness. At the system level, we generate and characterize a standard cell library and apply three representative delay-constrained leakage minimization dual-Vt assignment algorithms to the ISCAS'85 combinational benchmark circuits. Results show that oxide thickness modulation effectively reduces total leakage power consumption, but channel doping and channel length modulation are less effective.	algorithm;benchmark (computing);channel length modulation;combinational logic;doping (semiconductor);gate oxide;spice 2;simulation;spectral leakage;standard cell;thickness (graph theory);tunneling protocol	Lara D. Oliver;Krishnendu Chakrabarty;Hisham Z. Massoud	2006		10.1145/1127908.1127935	electronic engineering;real-time computing;engineering;electrical engineering;circuit design;leakage;threshold voltage	EDA	17.899456942528403	56.49780997888227	159405
724b52148c6a0fdf6ba1203f7134b59b04109129	a lightweight write-assist scheme for reduced rram variability and power		Abstract Common problems with Oxide-based Resistive RAM are related to high variability in operating conditions and high programming currents during FORMING, SET and RESET operations. Although research has taken steps to resolve these issues, variability combined with high programming currents remains an important characteristic for RRAMs. In a conventional write scheme with fixed duration and amplitude, the programming current is not controlled, which degrades the cell performance (power consumption and variability) due to over-programming. In this paper, a self-adaptive write driver is proposed to control the write current. A feedback mechanism based on current comparison is used to switch off the write stimulus as soon as the preferred write current is reached. Compared to conventional write schemes, in the proposed write-assist circuit, the write energy per bit is reduced by 27% and the standard deviation of post-FORMING distributions is reduced by 57%.	resistive random-access memory;spatial variability	Hassen Aziza;Basma Hajri;Mohammad M. Mansour;Ali Chehab;A. Perez	2018	Microelectronics Reliability	10.1016/j.microrel.2018.07.065	electronic engineering;standard deviation;amplitude;engineering;resistive random-access memory	EDA	18.773693101449002	60.02209514333429	159984
d6e444c884653452e0da7ab274f7216b4c021b35	testing ics: getting to the core of the problem	pins;technology trends;circuit faults;integrated circuit;sequential circuits;online testing ic testing technology trends integrated circuits predesigned components cores built in self test manufacturing testing;sequential analysis;manufacturing testing;built in self test;computer testing;error correction;signal processing;fault detection;integrated circuit testing;circuit testing circuit faults combinational circuits signal processing sequential circuits sequential analysis error correction semiconductor impurities fault detection pins;predesigned components;cores;circuit testing;integrated circuit testing integrated circuit manufacture built in self test computer testing;online testing;integrated circuits;integrated circuit manufacture;ic testing;semiconductor impurities;combinational circuits	The article examines the market and technology trends affecting the testing of integrated circuits, with emphasis on the role of predesigned components-cores-and built in self test. We explain manufacturing testing, as opposed to design testing, which happens before manufacturing, and online testing, which happens after.		Brian T. Murray;John P. Hayes	1996	IEEE Computer	10.1109/2.544235	test strategy;white-box testing;computer science;integrated circuit;signal processing;statistics	Visualization	22.784434981592877	53.59248141970778	160171
8508832e6dcafd97714355900e42aedadfe3e000	in situ sram static stability estimation in 65-nm cmos	estimation theory;sensors;circuit stability;voltage 1 mv in situ sram static stability estimation sram cell array read bit line current write bit line current write stability estimation read stability estimation regression analysis stability measurement low area sensing circuit on chip time based adc measurement estimation error sigma cmos technology size 65 nm voltage 100 mv voltage 25 mv voltage 1 mv;cmos memory circuits;analogue digital conversion;integrated circuit testing;sram chips analogue digital conversion circuit stability cmos memory circuits estimation theory integrated circuit measurement integrated circuit testing regression analysis sensors;regression analysis;current measurement circuit stability estimation stability analysis arrays sensors voltage measurement;wtv process variation regression rrv sram stability estimation;integrated circuit measurement;sram chips	This paper presents a method to rapidly estimate the read and write stability of cells within an SRAM array without modifying the cell structure. The approach measures the read or write bit-line current for a few supply levels and apply the measurements to an estimation function to determine the read stability and write ability of the cell. The estimation function can be determined by regression using stability measurements for a subset of the memory. A low-area sensing circuit and an on-chip time-based ADC measures the bit-line current. The step size of the supply voltage for estimating the stability is 100 mV and for determining the estimation function is 25 mV with an accuracy of ±1 mV. Measurement data show that the estimation error sigma is as small as 4.77% for the read stability and 1.3% for the write ability estimation. The validity of the approach is verified in measurements across multiple dice from a test chip fabricated in a 65-nm CMOS technology.	built-in self-test;cmos;cell (microprocessor);elegant degradation;embedded system;static random-access memory;transponder timing	Henry S Park;Chih-Kong Ken Yang	2013	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2013.2275653	embedded system;electronic engineering;engineering;sensor;electrical engineering;estimation theory;regression analysis	EDA	22.481428332810577	55.54032575543652	160315
98878e41414b89889cef3efbfa50beb7d65498a5	dataline isolated differential current feed/mode sense amplifier for small icell sram using finfet	process variation;static random access memory sram;read yield;cell current	This paper for the first time presents a novel, high-performance and robust current feed sense amplifiers (CF-SA) design for small ICell SRAM in 20nm Fin-shaped field effect transistor (FinFET) technology. The CFSA incorporates isolated DL current sensing approach which provides the higher Current Ratio Amplification (CRA) factor. The CF-SA significantly outperforms with 66.89% and 31.47% lower sensing delay than CCSA [13] and HSA [8] respectively under similar ICell and bit-line and data-line capacitance. Our results show that even at the worst corner the CF-SA demonstrates 2.15x and 3.02x higher differential current and 2.23x and 1.7x higher data-line differential voltage with 66.6% and 34.32% higher mean (μ) than those of the best prior arts. Furthermore, failure probability of the proposed design against process parameter variations is rigorously analyzed through Monte Carlo simulations.	credit bureau;field effect (semiconductor);heterogeneous system architecture;monte carlo method;sense amplifier;simulation;static random-access memory;transistor	Bhupendra Singh Reniwal;Vikas Vijayvargiya;Pooran Singh;Santosh Kumar Vishvakarma;Devesh Dwivedi	2015		10.1145/2742060.2742104	electronic engineering;real-time computing;engineering;electrical engineering;process variation	HCI	19.505952246228166	59.07139894651831	161226
7f437c767549545d0d568cc82dca9b30edff48a1	an automated design approach for cmos ldo regulators	design variable;ldo regulator;geometric program;optimal sizing;competing performance metrics;cmos low drop;regulator circuit;design problem;cmos ldo regulator;automated design approach;performance metrics;optimal trade;transistors;geometric programming;stability analysis;cmos integrated circuits;probability density function;gain;scheduling;measurement;data mining	This paper presents a method for optimal sizing of CMOS low drop out regulator circuits. The technique relies on the observation that many of the performance metrics of a LDO regulator can be approximated as posynomial functions of design variables. This allows the design problem to be cast as a geometric program. Geometric program is particularly attractive as the tool for optimization as --1)it can be solved very efficiently, 2)it always finds the global minima, 3)infeasible specifications are readily determined and 4)the final solution is completely independent of the initial guess. As a result CMOS LDOs may be conveniently synthesized; moreover the optimal trade off curves between the competing performance metrics, can be obtained very fast.	approximation algorithm;cmos;geometric programming;low-dropout regulator;mathematical optimization;maxima and minima;posynomial	Samiran DasGupta;Pradip Mandal	2009	2009 Asia and South Pacific Design Automation Conference		control engineering;embedded system;mathematical optimization;probability density function;electronic engineering;von neumann stability analysis;real-time computing;geometric programming;gain;computer science;engineering;electrical engineering;cmos;scheduling;transistor;measurement;statistics	EDA	24.340729405096354	57.739846614616724	161456
d30ed7b72bfdc2aba4d7ed42c9f523b7b1f992d9	a hybrid stochastic-deterministic input design method for active fault diagnosis	probability;conservatism reduction hybrid stochastic deterministic input design method active fault diagnosis fault detectability improvement worst case diagnosis guarantee diagnosis probability maximization;stochastic processes;stochastic processes fault diagnosis probability;noise noise measurement fault diagnosis design methodology stochastic processes optimization generators;fault diagnosis	The problem of designing an input to improve the detectability of faults has been addressed previously using both stochastic and deterministic formulations. This article presents a hybrid approach that provides a worst-case guarantee of diagnosis within a time interval [0,N], while maximizing the probability of diagnosis at some earlier time M <; N. Compared to purely deterministic methods, this strategy reduces the average time required for diagnosis. Moreover, a high probability of diagnosis at M enables N to be chosen large, thus reducing the conservatism of the required input.	best, worst and average case	Joseph K. Scott;Giuseppe Roberto Marseglia;Lalo Magni;Richard D. Braatz;Davide Martino Raimondo	2013	52nd IEEE Conference on Decision and Control	10.1109/CDC.2013.6760780	control engineering;stochastic process;mathematical optimization;engineering;probability;control theory;mathematics;statistics	Robotics	24.361537763690965	56.79136562946038	162091
561e5e3394e5995e5f347e8c5e91a55856e401de	improved iddq testing with empirical linear prediction	mosfet circuits;fabrication;prediction model fitting;electric current;leakage current;delta i ddq test method;current supplies;digital cmos integrated circuits i sub ddq testing empirical linear prediction method test effectiveness statistical pre processing training device measurements principal device i sub ddq behavior patterns prediction model fitting fabrication process variations delta i sub ddq test method sematech s 121 data defective parts defect free parts;curve fitting cmos digital integrated circuits mosfet integrated circuit testing vlsi fault location integrated circuit reliability statistical analysis electric current integrated circuit modelling;defective parts;data mining;linear predictive;i ddq testing;test effectiveness;prediction methods;statistical analysis;cmos digital integrated circuits;digital cmos integrated circuits;statistical pre processing;integrated circuit modelling;training device measurements;fabrication process variations;sematech s 121 data;test methods;integrated circuit testing;vlsi;predictive models;mosfet;circuit testing;prediction model;integrated circuit reliability;curve fitting;defect free parts;principal device i ddq behavior patterns;circuit testing prediction methods fabrication integrated circuit testing mosfet circuits leakage current data mining predictive models current supplies electrical fault detection;electrical fault detection;empirical linear prediction method;fault location	A new linear prediction method that improves IDDQ test effectiveness is described. The method uses statistical pre-processing of exhaustive measurements on training devices to extract principal patterns in the device IDDQbehavior and to generate a prediction model. Fitting the model to device measurements accommodates variations in the fabrication process. Comparison with the Delta IODQtest method using the SEMATECH S-121 data shows that for nearly equal numbers of defective parts passed, the new method fails fewer defect-tree parts.	curve fitting;iddq testing;preprocessor;semiconductor device fabrication;software bug	David I. Bergman;Hans Engler	2002		10.1109/TEST.2002.1041851	reliability engineering;electronic engineering;computer science;engineering;electrical engineering;predictive modelling;statistics	SE	23.111274917986254	55.07205474654306	162169
3310ea35a36b2bea900a51ba1cddb3f04bc22461	nbti/pbti-aware wwl voltage control for half-selected cell stability improvement	voltage control negative bias temperature instability signal processing equipment;voltage control;negative bias temperature instability;circuit stability voltage control stress stability analysis degradation sram cells;positive bias temperature instability pbti half selected static noise margin hs snm negative bias temperature instability nbti;signal processing equipment;nbti pbti aware wwl voltage control sample circuit write margin cell stability degradation power consumption bti aging power consumption positive bti aware write wordline voltage control technique negative bias temperature instability half selected cell stability improvement	This brief presents a negative bias temperature instability (BTI)/positive BTI-aware write-wordline (WWL) voltage control technique for improving degraded cell stability of half-selected cells without extra power consumption. After BTI aging, the proposed lowering WWL voltage recovers the degraded cell stability without scarifying the write margin. Finally, we also present a sample circuit implementation of the proposed WWL voltage control scheme.	btrieve;negative-bias temperature instability	Zhao Chuan Lee;Kam Chew Leong;Zhi-Hui Kong;Tony Tae-Hyoung Kim	2013	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2013.2273731	negative-bias temperature instability;electronic engineering;engineering;electrical engineering;control theory	EDA	19.096986575607154	59.591944486287744	162258
a01b80eb141d42e57d931d1f7052ffb59db91f6f	a layout sensitivity model for estimating electromigration-vulnerable narrow interconnects	yield prediction;mean time to failure;chip;yield modeling;electromigration;prediction model;layout sensitivity;narrow defects;critical area;spot defects	During the back-end manufacturing process of IC, intervention of spot defects induces extra and missing material of interconnects causing circuit failures. In this paper, a new type of spot defects called interconnect “narrowing defect” is defined. Interconnect narrowing occurs when spot defects induce missing material of interconnects without resulting in a complete cut. The narrow sites of defective interconnects favor electromigration that makes narrow interconnects more likely to induce a chip failure than regular interconnects. In this paper, a layout sensitivity model accounting for narrowing defects is derived. A methodology for predicting the probability of narrow interconnects using the sensitivity model is then proposed. The layout sensitivity model for narrow interconnects is tested and compared to actual and simulated data. Our layout sensitivity model for narrow interconnects predicts the probability of narrowing with 3.1% error, on average. The model is then combined with electromigration constraints to predict mean-time-to-failure of chips manufactured in future technology down to 32nm node. The paper concludes with some other possible applications of the narrow interconnect predictive model.	electrical connection;electromigration;futures studies;mean time between failures;predictive modelling;software bug;speaker wire	Rani S. Ghaida;Payman Zarkesh-Ha	2009	J. Electronic Testing	10.1007/s10836-008-5079-x	chip;reliability engineering;electromigration;electronic engineering;mean time between failures;telecommunications;computer science;engineering;predictive modelling;forensic engineering;engineering drawing	EDA	22.48070820985075	55.031297899323604	162516
4a80c6d10659782f05083949c05a4943f44ec3ff	set and noise fault tolerant circuit design techniques: application to 7 nm finfet	article	In the near future of high component density and low-power technologies, soft errors occurring not only in memory systems and latches but also in the combinational parts of logic circuits will seriously affect the reliable operation of integrated circuits. This paper presents a novel design style which reduces the impact of radiation-induced single event transients (SET) on logic circuits, and enhances the robustness in noisy environments. The independent design style of this method achieves SET mitigation and noise immunity by strengthening the sensitive nodes using a technique similar to feedback. A realization for this methodology is presented in 7 nm FinFET and in order to check the accuracy of our proposal, we compare it with others techniques for hardening radiation at the transistor level against a single event transient. Simulation results show that the proposed method has a good soft error tolerance capability as well as better noise immunity. 2013 Elsevier Ltd. All rights reserved.	circuit design;combinational logic;error-tolerant design;fault tolerance;gnu nano;hardening (computing);image noise;integrated circuit;logic gate;low-power broadcasting;simulation;soft error;transistor	Antonio Calomarde;Esteve Amat;Francesc Moll;Julio Vigara;Antonio Rubio	2014	Microelectronics Reliability	10.1016/j.microrel.2013.12.018	electronic engineering;real-time computing;engineering;electrical engineering;physics	EDA	20.45311750124749	56.19586891183444	163547
0af86cf5bc9e9b28886109469765d7eb984e98c0	robust estimation of timing yield with partial statistical information on process variations	partial statistical information;process variation;distributional robustness theory;robust estimator;functional form;integrated circuit yield;statistical process control integrated circuit design integrated circuit yield statistical distributions;correlations;robustness yield estimation timing shape uncertainty integrated circuit yield log normal distribution delay estimation distributed computing probability distribution;probability density function;probability distributions;statistical process control;robust estimation;within die wid and die to die d2d variations class of distributions convexity correlations robust estimation timing yield;integrated circuit design;distribution function;statistical distributions;process variations;timing yield;integrated circuit technology;probability distribution;joint distribution function timing yield partial statistical information process variations distributional robustness theory probability distributions;robustness;convexity;class of distributions;correlation coefficient;within die wid and die to die d2d variations;joint distribution function;time constraint	This paper illustrates the application of a novel theory, namely, the distributional robustness theory (DRT), to compute the worst-case timing yield of a circuit. The assumption is that the probability distributions of the process variables are unknown, and only their intervals and their ldquoclassrdquo of distributions are available. This paper considers practical classes to describe potential distributions which match with partial statistical information that might be available. Some classes are suitable for independent distributions that have symmetrical or asymmetrical shapes, while others can account for correlations. These classes have high flexibility to include various shapes of the distributions of the process variations. At a higher level, they can also capture the case when uncertainty in their correlation coefficients exists. The contributions of this paper are on formulating the DRT for different cases of variations and in deriving conditions (e.g., acceptable bounds on timing constraint, acceptable intervals of variations) that allow applying the results of the DRT. Compared with other recent works, the presented approach can include correlations among process variations and does not require knowledge of the exact function form of their joint distribution function. The presented approach is also applicable to other types of parametric yield.	best, worst and average case;coefficient	Lin Xie;Azadeh Davoodi	2008	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2008.2006146	probability distribution;econometrics;mathematical optimization;mathematics;statistics	EDA	22.88985223940123	57.474590102169415	163775
7773f31d843e54fa13b0153bd8de8ef57fe048d7	experimental validation of minimum operating-voltage-estimation for low supply voltage circuits	cmos integrated circuits;discrete cosine transforms semiconductor device measurement clocks voltage measurement mosfet estimation;low power electronics cmos integrated circuits integrated circuit design large scale integration;integrated circuit design;large scale integration;low power electronics;low power design subthreshold circuit minimum operating voltage;voltage 10 mv minimum operating voltage estimation low supply voltage circuits near threshold circuits sub threshold circuits low energy circuits low power circuit lognormal model approximation dct circuit measurements silicon data lsis cmos process flip flop	Sub- and near-threshold circuits have been attracting growing interests because they are suitable for realizing extremely low power and low energy circuits. The estimation of the minimum operating voltage (VDDmin), under which the circuit does not function correctly, is one of the most important issues in their design. In this paper, the distribution of VDDmin is explored through simulations and measurements. Lognormal model-approximation and a quick VDDmin estimation method are validated by the measurements of 124k FFs. Assuming that the VDDmin of a circuit is limited by that of the FFs, VDDmin distribution for any circuits can be efficiently estimated. The measurements of 192 DCT circuits show that the estimation matches with silicon data very well within 10 mV error.	approximation;biasing;discrete cosine transform;fits;nmos logic;newton's method;simulation;transistor	Takashi Sato;Junya Kawashima;Hiroshi Tsutsui;Hiroyuki Ochi	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783356	electronic engineering;real-time computing;high impedance;computer science;engineering;78xx;electrical engineering;operating system;cmos;low-power electronics;integrated circuit design	EDA	24.452867863383798	59.544346596399265	163783
7725fbc3b004b8026ab4f03fd6bff3bf64809976	using bayesian networks to accurately calculate the reliability of complementary metal oxide semiconductor gates	digital circuit;reliability engineering;cmos integrated circuits;complementary metal oxide semiconductor;bayesian network;design automation;integrated circuit;bayes methods;reliability modeling bayesian network cmos transistors design automation digital circuit nanotechnology;reliability modeling;semiconductor device reliability;nanotechnology;very large scale integrated;vlsi design;vlsi bayes methods cmos integrated circuits electronic design automation semiconductor device reliability;massive nanoscaled design bayesian networks reliability complementary metal oxide semiconductor gates very large scale integrated designs electronic design automation tools eda tool vlsi design cmos gate noise margins;logic gates cmos integrated circuits integrated circuit reliability threshold voltage transistors reliability engineering;logic gates;threshold voltage;transistors;vlsi;integrated circuit reliability;digital circuits;logic gate;cmos transistors;electronic design automation	Scaling complementary metal oxide semiconductor (CMOS) devices has been a method used very successfully over the last four decades to improve the performance and the functionality of very large scale integrated (VLSI) designs. Still, scaling is heading towards several fundamental limits as the feature size is being decreased towards 10 nm and less. One of the challenges associated with scaling is the expected increase of static and dynamic parameter fluctuations and variations, as well as intrinsic and extrinsic noises, with significant effects on reliability. Therefore, there is a clear, growing need for electronic design automation (EDA) tools that can predict the reliability of future massive nano-scaled designs with very high accuracy. Such tools are essential to help VLSI designers optimize the conflicting tradeoffs between area-power-delay and reliability requirements. In this paper, we introduce an EDA tool that quickly and accurately estimates the reliability of any CMOS gate. The tool improves the accuracy of the reliability calculation at the gate level by taking into consideration the gate's topology, the reliability of the individual devices, the applied input vector, as well as the noise margins. It can also be used to estimate the effect on different types of faults and defects, and to estimate the effects of enhancing the reliability of individual devices on the gate's overall reliability.	and gate;adder (electronics);algorithm;bayesian network;cmos;course (navigation);electronic design automation;gnu nano;image scaling;molecular dynamics;monte carlo method;multidimensional digital pre-distortion;multiplexing;noise margin;reliability engineering;requirement;semiconductor;simulation;very-large-scale integration;von neumann architecture	Walid Ibrahim;Valeriu Beiu	2011	IEEE Transactions on Reliability	10.1109/TR.2011.2161032	electronic engineering;electronic design automation;logic gate;engineering;electrical engineering;very-large-scale integration;cmos;digital electronics;computer engineering	EDA	21.462834409597946	58.03651105470056	163937
14d95fd20516a639e584b97da341fe29a6e09dc4	a generic low power scan chain wrapper for designs using scan compression	peak current reduction;design for testability;test power consumption;scan compression solution;codecs;test data compression hardware;design engineering;clocks;test data compression;test power reduction;automatic test pattern generation;size 65 nm generic low power scan chain wrapper power budgets low power system on chips test power consumption chip design test engineering teams automatic test pattern generation atpg design for test dft techniques test power reduction test data compression hardware physical design cycle peak current reduction scan based testing chip clocking test compression logic bypassed flip flops scan compression solution silicon power measurements;flip flops;power systems;low power system on chips;physical design;test engineering teams;dft techniques;system on a chip;chip design;boundary scan testing;chip;power engineering and energy;physical design cycle;test compression logic;chip clocking;low power;bypassed flip flops;system on chip;energy consumption;empirical validation;logic testing;low power electronics;circuit testing automatic test pattern generation system testing design for testability logic testing power systems energy consumption chip scale packaging design engineering power engineering and energy;system testing;circuit testing;design for test;power reduction;generic low power scan chain wrapper;power consumption;scan based testing;switches;automatic test pattern generator;power demand;silicon power measurements;size 65 nm;flip flop;system on chip automatic test pattern generation boundary scan testing design for testability low power electronics;power budgets;chip scale packaging;power measurement;atpg	Shrinking power budgets in low power system-on-chips (SoCs) have elevated test power consumption as a major consideration for chip design and test engineering teams. Many traditional automatic test pattern generation (ATPG) and design-for-test (DFT) techniques for test power reduction are either effective for circuits not using test data compression hardware or have implications on the physical design cycle. This paper describes a technique for reducing peak current during scan based testing that can work in the presence of compression, and impose no restrictions on physical design, e.g. related to chip clocking. We propose low-design effort modifications to the test compression logic (wrapper-like changes) that enable us to (a) bypass scan chains or groups of them and (b) shift in constant values into the bypassed flip-flops for lowering the instantaneous current drawn. The modifications are easily localized to a scan chain wrapper that can be used with any scan compression solution. An SoC using lowpower scan chain wrappers provides sufficient configurability (scan chains bypassed or scan chains included) to explore different power reductions with test cost trade-offs. We describe a methodology that allows us to manage the inherent configurability available in our solution. For empirical validation, we have implemented low-power scan chain wrappers for a subset of scan chains in a recently taped-out 65nm low-power SoC. We present experimental data from ATPG and initial silicon power measurements for this chip to demonstrate the benefits and limitations of the proposal.	clock rate;data compression;design for testing;emoticon;flops;flip-flop (electronics);low-power broadcasting;physical design (electronics);scalability;system on a chip;tape-out;test card;test compression;test data	Amit Sabne;Rajesh Tiwari;Abhijeet Shrivastava;Srivaths Ravi;Rubin A. Parekhji	2010	2010 28th VLSI Test Symposium (VTS)	10.1109/VTS.2010.5469593	system on a chip;embedded system;electronic engineering;scan chain;real-time computing;computer science;engineering;automatic test pattern generation;test compression;design for testing	EDA	20.111764981799716	54.46210003294492	163979
409a1bf213e2729e8e3049827a91f2d1acac16e1	on nor-2 von neumann multiplexing	mosfet circuits;cmos integrated circuits;semiconductor device reliability mosfet semiconductor device models;logic gates multiplexing mosfet circuits lead mosfets cmos integrated circuits bayesian methods;mosfets;semiconductor device reliability;bayesian methods;multiplexing;logic gates;lead;threshold voltage;semiconductor device models;device level analysis nor 2 von neumann multiplexing threshold voltage variation bulk mosfet transistor transistor sizing cmos gate;mosfet	This paper provides a detailed analysis of the effects threshold voltage variations play on the reliability of bulk MOSFET transistors. It also investigates the consequences of transistor sizing on the reliability of both devices and CMOS gates. These are followed by very accurate device-level (CMOS technology specific) analyses of NOR-2 von Neumann multiplexing with respect to threshold voltage variations, taking into account both the gates' schematic as well as the input vectors. The simulation results reported here show clearly that improving the reliability at the device-level does not necessarily lead to reliability improvement at the gate- and system-level. They also reveal that the effectiveness of von Neumann multiplexing schemes depend to a great extend not only on devices, but also on the gate types (i.e., gates' topologies).	cmos;multiplexer;multiplexing;nand gate;nmos logic;nor gate;schematic;simulation;software propagation;transistor;von neumann architecture	Walid Ibrahim;Valeriu Beiu;Azam Beg	2010	2010 5th International Design and Test Workshop	10.1109/IDT.2010.5724410	lead;electronic engineering;logic gate;telecommunications;bayesian probability;computer science;engineering;electrical engineering;threshold voltage;cmos;multiplexing	EDA	20.70660923714093	58.752746539601475	164092
76c3bab6c6c258b2b31cb8f4803771f173ded833	a new methodology for realistic open defect detection probability evaluation under process variations	cmos integrated circuits;software tool open defect detection probability evaluation process variations cmos ic scaling electronic circuit performance test methodologies failure mechanism nanometer process process variability defective floating node;process variation;software tool;cmos ic scaling;integrated circuit testing cmos integrated circuits;process variations;logic gates;test methodologies;integrated circuit interconnections;transistors;failure mechanism;logic gates capacitance correlation transistors integrated circuit interconnections couplings gaussian distribution;integrated circuit testing;process variability;nanometer process;capacitance;correlation;couplings;open defect detection probability evaluation;defective floating node;electronic circuit performance;gaussian distribution;defect detection	CMOS IC scaling has provided significant improvements in electronic circuit performance. Advances in test methodologies to deal with new failure mechanisms and nanometer issues are required. Interconnect opens are an important defect mechanism that requires detailed knowledge of its physical properties. In nanometer process, variability is predominant and considering only nominal value of parameters is not realistic. In this work, a model for computing a realistic coverage of via open defect that takes into account the process variability is proposed. Correlation between parameters of the affected gates is considered. Furthermore, spatial correlation of the parameters for those gates tied to the defective floating node can also influence the detectability of the defect. The proposed methodology is implemented in a software tool to determine the probability of detection of via opens for some ISCAS benchmark circuits. The proposed detection probability evaluation together with a test methodology to generate favorable logic conditions at the coupling lines can allow a better test quality leading to higher product reliability.	benchmark (computing);cmos;electronic circuit;failure cause;fault coverage;image scaling;programming tool;software bug;spatial variability	Jesus Moreno;Víctor H. Champac;Michel Renovell	2011	29th VLSI Test Symposium	10.1109/VTS.2011.5783781	normal distribution;reliability engineering;electronic engineering;logic gate;engineering;electrical engineering;capacitance;coupling;process variation;cmos;correlation;transistor	EDA	22.09324471417698	56.918679925838504	164112
3d19f25dc1301ef494f683655d86cb8fa565adcd	confronting the variability issues affecting the performance of next-generation sram design to optimize and predict the speed and yield	energy efficiency;engineering;reliability engineering;next generation networking threshold voltage performance evaluation energy efficiency integrated circuit reliability reliability engineering voltage measurement sram chips;performance evaluation;sram chips ageing integrated circuit modelling integrated circuit reliability integrated circuit yield negative bias temperature instability;threshold voltage;size 16 nm next generation sram design integrated circuit yield circuit parameter variations integrated circuit reliability six transistor cell static random access memory 6t sram var tx model interdie process variation intradie process variation negative bias temperature instability;integrated circuit reliability;voltage measurement;next generation networking;sram chips	Effectively confronting device and circuit parameter variations to maintain or improve the design of high performance and energy efficient systems while satisfying historical standards for reliability and lower costs is increasingly challenging with the scaling of technology. In this paper, we develop methods for robust and resilient six-transistor-cell static random access memory (6T-SRAM) designs that mitigate the effects of device and circuit parameter variations. Our interdisciplinary effort involves: 1) using our own recently developed VAR-TX model [1] to illustrate the impact of interdie (also known as die-to-die, D2D) and intradie (also know as within-die, WID) process and operation variations - namely threshold voltage (Vth), gate length (L), and supply voltage (Vdd) - on future different 16-nm architectures and 2) using modified versions of other well-received models to illustrate the impact of variability due to temperature, negative bias temperature instability, aging, and so forth, on existing and next-generation technology nodes. Our goal in combining modeling techniques is to help minimize all major types of variability and to consequently predict and optimize speed and yield for the next generation 6T-SRAMs.	heart rate variability;image scaling;negative-bias temperature instability;next-generation network;random access;spatial variability;static random-access memory;transistor	Jeren Samandari-Rad;Matthew R. Guthaus;Richard Hughey	2014	IEEE Access	10.1109/ACCESS.2014.2323233	real-time computing;electrical engineering;efficient energy use;threshold voltage	EDA	19.296498162506662	59.94847108657562	164178
20b3ff6e69dd8cdec290ee3cd19c26035d90dbf6	on reducing circuit malfunctions caused by soft errors	computed tomography;circuit faults;current transformers;fault currents;nanoelectronic circuits;iscas 89 benchmark circuits reducing circuit malfunctions soft errors nanoelectronic circuits redundancy circuit elements partial intrinsic immunity;argon;circuit hardening;rail to rail outputs;argon integrated circuits rail to rail outputs fault currents circuit faults current transformers computed tomography;redundancy;iscas 89 benchmark circuits;redundancy nanoelectronics radiation hardening electronics;circuit hardening soft errors;nanoelectronics;reducing circuit malfunctions;radiation hardening electronics;single event upset;soft errors;integrated circuits;soft error;circuit elements;partial intrinsic immunity	Soft errors due to radiation are expected to increase in nanoelectronic circuits. Methods to reduce system failures due to soft errors include use of redundancy and making circuit elements robust such that soft errors do not upset signal values. Recent works have noted that electronic circuits have partial intrinsic immunity to soft errors since single event upsets on a large percentage of signal lines do not cause errors on circuit outputs. Using ISCAS-89 benchmark circuits we present experimental evidence that the partial immunity to single event upsets is in most cases due to redundancy in the circuits and thus immunity to soft errors may not be available in irredundant circuits. Thus goals on immunity to soft errors may not be achievable in highly optimized circuits without adding circuit redundancy and/or relaxing the requirements on system failures due to soft errors.	benchmark (computing);electronic circuit;fault tolerance;headroom (audio signal processing);redundancy (engineering);requirement;single event upset;soft error	Ilia Polian;Sudhakar M. Reddy;Irith Pomeranz;Xun Tang;Bernd Becker	2008	2008 IEEE International Symposium on Defect and Fault Tolerance of VLSI Systems	10.1109/DFT.2008.20	nanoelectronics;current transformer;electronic engineering;real-time computing;soft error;engineering;electrical engineering;computed tomography;redundancy;argon;electrical element	EDA	21.371066335434858	53.97973384467946	164532
8106e9864327ad35dc4c72c8f875bc8f5805a38f	connecting the physical and application level towards grasping aging effects	stress;aging;temperature dependence;bias temperature instability technology scaling transistor on chip system reliability estimation operating system temperature voltage stress waveform calibration experimental model parameter derivation complex physics based model tddb time dependent dielectric breakdown bti aging phenomena;registers;transistors;transistors aging temperature measurement registers stress temperature distribution temperature dependence;transistors ageing calibration estimation theory semiconductor device reliability;temperature measurement;temperature distribution	Technology scaling noticeably increases the susceptibility of transistors to varied degradations induced by aging phenomena like Bias Temperature Instability (BTI) and Time-Dependent-Dielectric Breakdown (TDDB). Therefore, estimating the reliability of an entire computational system necessitates investigating how such phenomena will ultimately lead to failures - considering that aging starts from the physical level and ends up at the application level, where workloads (i.e. software programs) run. The key challenge is that an accurate estimation imposes analyzing the impact of aging on each individual transistor within a sophisticated on-chip system using complex physics-based models. The latter requires both a careful experimental model parameter derivation for calibration and precise information regarding the actual temperature voltage-stress waveforms that may be applied to the transistors during lifetime. These waveforms are directly driven by the running workloads creating the inevitable necessity to connect the physical and application level. As a matter of fact, this challenge is exacerbated in the nano era, due to the typical workloads (i.e. multiple applications running in parallel along with an operating system) that may run on top of a tremendous number of transistors. This paper investigates this challenge to provide designers with an abstracted, yet sufficiently accurate reliability estimation that takes into account the interrelations between the physical and application level towards grasping how aging actually degrades the reliability of on-chip systems.	btrieve;computation;gnu nano;image scaling;instability;map;operating system;reliability engineering;transistor	Hussam Amrouch;Javier Martín-Martínez;Victor M. van Santen;Miquel Moras;Rosana Rodríguez;Montserrat Nafría;Jörg Henkel	2015	2015 IEEE International Reliability Physics Symposium	10.1109/IRPS.2015.7112711	electronic engineering;simulation;engineering;electrical engineering	Arch	21.224624532185434	59.563568112937865	164734
151cb281d6486dcc231b6da3d32304e2b08b2fe0	design methodology for fine-grained leakage control in mtcmos	leakage;logic design;sequential circuits;circuit design;integrated circuit design;0 13 micron design methodology fine grained leakage control multi threshold cmos standby leakage power delay overhead mtcmos designs sleep devices block level standby leakage sneak leakage paths gate level sleep device insertion sequential circuits combinational circuits dual threshold voltage test chip low power fpga core gate level sleep fet clb configurations active configurable logic block;low power;leakage power;logic design field programmable gate arrays cmos logic circuits sequential circuits combinational circuits leakage currents integrated circuit design integrated circuit testing logic testing;sleep mode;leakage currents;cmos logic circuits;logic testing;integrated circuit testing;mtcmos;sneak leakage;field programmable gate arrays;combinational circuit;design methodology sleep semiconductor device measurement delay combinational circuits circuit testing field programmable gate arrays fets current measurement logic;fine grain sleep regions;design methodology;combinational circuits	Multi-threshold CMOS is a popular technique for reducing standby leakage power with low delay overhead. MTCMOS designs typically use large sleep devices to reduce standby leakage at the block level. We provide a formal examination of sneak leakage paths and a design methodology that enables gate-level insertion of sleep devices for sequential and combinational circuits. A fabricated 0.13 μm, dual V T testchip employs this methodology to implement a low-power FPGA core with gate-level sleep FETs and over 8X measured standby current reduction. The methodology allows local sleep regions that reduce leakage in active CLBs by up to 2.2X (measured) for some CLB configurations.	combinational logic;field-programmable gate array;low-power broadcasting;multi-threshold cmos;overhead (computing);spectral leakage	Benton H. Calhoun;Frank Honoré;Anantha Chandrakasan	2003		10.1145/871506.871535	embedded system;electronic engineering;real-time computing;computer science;engineering;combinational logic	Arch	19.625387816264336	54.44298258722107	164897
8479b541e793be8e2d824772d60f5cd868bd0c7a	dual-vt assignment policies in itd-aware synthesis	norma ensayo;dependance temperature;effet temperature;evaluation performance;leakage current;performance evaluation;voltage threshold;logic design;scaling law;corriente escape;test standard;evaluacion prestacion;multi vt synthesis;bande interdite;tecnologia mos complementario;temperature dependence;energy gap;algorithme;temperature effect;algorithm;consumo electricidad;conception logique;logic synthesis;leakage power;threshold current;norme essai;courant fuite;ley escala;fluctuation temperature;electric power consumption;low power electronics;temperature aware design;temperature fluctuation;fluctuacion temperatura;seuil tension;efecto temperatura;banda prohibida;temps retard;low power design;delay time;multi v t;loi echelle;technologie mos complementaire;concepcion logica;electronique faible puissance;tiempo retardo;consommation electricite;courant seuil;complementary mos technology;umbral tension;algoritmo	"""Traditionally, the effects of temperature on delay of CMOS devices have been evaluated using the highest operating temperature as a worst-case corner. This conservative approach was based on the fact that, in older technologies, CMOS devices systematically degraded their performance as temperature increases. With the progressive scaling of technology, however, there has been a continuous reduction of the gap between supply and threshold voltages of devices, mostly due to low-power constraints. The latter have accelerated this trend by using libraries containing multiple instances of a cell with different ranges of threshold voltages; in particular, the use of high-V""""t cells to control sub-threshold leakage currents has made this gap smaller and smaller. The consequence of this trend is the occurrence of the so-called inverted temperature dependence (ITD), under which cells get faster as temperature increases. This new thermal dependence has made the old worst-case design approach obsolete, posing new EDA challenges. Beside complicating timing analysis, in particular, ITD has important and unforeseeable consequences for power-aware design, especially in dual-V""""t logic synthesis. Due to a contrasting temperature dependence between low-V""""t cells (which enjoy the classical, direct temperature dependence) and high-V""""t cells (for which an inverted temperature dependence holds), a single-temperature worst-case design approach fails to generate netlists that are compliant with timing constraints for the entire temperature range. In this work, we first validate the relevance of ITD on an industrial 65nm CMOS multi-V""""t library. Then, we describe an ITD-aware, dual-V""""t assignment algorithm that guarantees temperature-insensitive operation of the circuits, together with a significant reduction of both leakage and total power consumption. The algorithm has been tested over standard benchmarks using three different replacement policies. Experimental results show an average leakage power savings of 50% w.r.t. circuits synthesized with a standard, commercial flow that does not take ITD into account and thus, to ensure that no temperature-induced timing faults occur, needs to resort to over-design (i.e., over-constraining the timing bound so as to make sure that temperature fluctuations never make the circuits violating the specified required time for all paths)."""		Andrea Calimera;R. Iris Bahar;Enrico Macii;Massimo Poncino	2010	Microelectronics Journal	10.1016/j.mejo.2009.12.004	electronic engineering;logic synthesis;telecommunications;engineering;electrical engineering	EDA	18.7989068516625	55.32459105946208	165246
1b764c6c033e0a5a9d537132655d18df95a9924a	fast statistical analysis of rare circuit failure events via scaled-sigma sampling for high-dimensional variation space	failure analysis importance sampling integrated circuit modelling integrated circuit reliability nanoelectronics;random access memory;random variables monte carlo methods estimation gaussian distribution random access memory standards integrated circuit modeling;standards;process variation parametric yield monte carlo analysis importance sampling;random variables;estimation;integrated circuit modeling;fast statistical analysis rare circuit failure events scaled sigma sampling high dimensional variation space importance sampling technique transistor level simulations soft maximum theorem analytical model sss method nanoscale circuit blocks rare failure rate estimation;rare failure rate estimation fast statistical analysis rare circuit failure events scaled sigma sampling high dimensional variation space importance sampling technique transistor level simulations soft maximum theorem analytical model sss method nanoscale circuit blocks;nanoelectronics failure analysis importance sampling integrated circuit modelling integrated circuit reliability;gaussian distribution;monte carlo methods;importance sampling process variation parametric yield monte carlo analysis	"""Accurately estimating the rare failure rates for nanoscale circuit blocks (e.g., SRAM, DFF, etc.) is a challenging task, especially when the variation space is high-dimensional. In this paper, we propose a novel scaled-sigma sampling (SSS) method to address this technical challenge. The key idea of SSS is to generate random samples from a distorted distribution for which the standard deviation (i.e., sigma) is scaled up. Next, the failure rate is accurately estimated from these scaled random samples by using an analytical model derived from the theorem of """"soft maximum"""". Several circuit examples designed in nanoscale technologies demonstrate that the proposed SSS method achieves superior accuracy over the traditional importance sampling technique when the dimensionality of the variation space is more than a few hundred."""	digital forensics framework (dff);experiment;failure rate;importance sampling;microsoft customer care framework;numerical analysis;sampling (signal processing);simulation;static random-access memory;transistor	Shupeng Sun;Xin Li;Hongzhou Liu;Kangsheng Luo;Ben Gu	2013	2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/TCAD.2015.2404895	normal distribution;random variable;econometrics;estimation;electronic engineering;importance sampling;computer science;mathematics;statistics;monte carlo method	EDA	22.65232971683107	59.01119604522125	165552
8ea20beeb507592eab7a3a34cf794be02f202fc0	a fast process-variation-aware mask optimization algorithm with a novel intensity modeling	kernel;proximity effect lithography masks optimisation;optical imaging;estimation;semiconductor device modeling;image quality;manufacturing;wafer image quality optical proximity correction opc opc computation time process variation;image quality semiconductor device modeling adaptive optics optical imaging kernel manufacturing estimation;adaptive optics;mask notch rule fast process variation aware mask optimization algorithm intensity modeling continuous shrinkage advanced technology nodes optical proximity correction wafer image quality lithographic process variations novel intensity based opc algorithm minimal edge placement error process variability band area intensity estimation model acceptable estimation accuracy two fragment shifting corner hammering subresolution assist feature insertion	With the continuous shrinkage of advanced technology nodes into the sub-16-nm regime, optical proximity correction (OPC) is still the main stream to preserve acceptable wafer image quality under lithographic process variations in the foreseeable future. However, OPC is getting more aggressive to keep pace with advanced technology nodes. This results in complex mask solutions and long computation time. In this paper, we propose a novel-intensity-based OPC algorithm to find mask solutions with minimal edge placement error and process variability band area within a short computation time. This is achieved through exploiting a fast novel intensity estimation model with acceptable estimation accuracy to guide the OPC response including two-fragment shifting, corner hammering, and subresolution assist feature insertion for better convergence. Moreover, our algorithm is extended to satisfy the mask notch rule and reduce shot count for a lower mask manufacturing cost. The experimental results show that our algorithm outperforms recently published algorithms on the public benchmarks.	algorithm;benchmark (computing);computation;elegant degradation;image quality;microsoft windows;open platform communications;scalability;spatial variability;time complexity;wafer (electronics)	A. S. A. Awad;Atsushi Takahashi;Satoshi Tanaka;Chikaaki Kodama	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2016.2616840	image quality;estimation;electronic engineering;semiconductor device modeling;kernel;simulation;engineering;optical imaging;manufacturing;adaptive optics;engineering drawing;statistics	EDA	20.374466363903565	56.98054402840538	165627
27184435697c64234b89f669cc31a0a4bd073d61	impact of variability effects on finfet transistors and combinational cells	standards;metals;fluctuations;finfets;logic gates	This work provides a predictive evaluation of PVT variability impact on sub-22nm FinFET devices and a set of combinational cells. Results show that metal gate workfunction fluctuation is the main source of process variability, severely affecting the OFF current especially on NFET devices. Combinational cells suffer from temperature impact on the power consumption and also from voltage drop that increases more than three times the timing results of the majority of cells.	combinational logic;heart rate variability;metal gate;quantum fluctuation;spatial variability;transistor	Alexandra L. Zimpeck;Ricardo Augusto da Luz Reis	2016	2016 IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2016.7841231	electronic engineering;engineering;electrical engineering;nanotechnology	EDA	19.94592306402992	58.8007961407383	165668
0a5ec5c2807e25fb7719845e165ab6c56eae337c	flash memory die sort by a sample classification method	flash memory;design for testability;process variation;high density;memory testing;similar test;die sort;chip;test flow;wafer probe;wafer probe die sort flash memory memory testing test flow;flash memory circuit testing uncertainty semiconductor device reliability sorting flash memory cells design for testability semiconductor device testing probes random access memory	As the memory cells keep scaling down and designs are getting bigger and faster, uncertainty is becoming one of the greatest challenges for the semiconductor industry. Unexpected and unpredictable behaviors of devices usually lead to poor quality and reliability. Low-cost test techniques that improves die sorting accuracy thus are critical for advanced devices. Flash memory is more prone to such problem compared with others. Large capacity, high density, and complicated cell structure makes flash memory cell behavior difficult to predict precisely. Even when we test the dies on the same wafer it can be bothering, as each of them may ask for different test condition due to geometric process variation. As a fast and easy-to-use method to solve the problem, we propose a sample classification method. It is not only effective for flash memory testing, but also for other types of circuits that face similar test problem. Experimental result shows that the method solves the flash memory die sort problem efficiently and accurately. The test time is greatly reduced-for an industrial chip, the test time is reduced from 8,817 ms to 718 ms. Moreover, the proposed approach is also suitable for design-for-testability (DFT) implementation that can easily be integrated with a commodity or embedded memory.	built-in self-test;design for testing;die (integrated circuit);effective method;embedded system;flash memory;image scaling;memory cell (binary);memory tester;ramp simulation software for modelling reliability, availability and maintainability;semiconductor industry;sorting;statistical classification	Yu-Chun Dawn;Jen-Chieh Yeh;Cheng-Wen Wu;Chia-Ching Wang;Yung-Chen Lin;Chao-Hsun Chen	2005	14th Asian Test Symposium (ATS'05)	10.1109/ATS.2005.61	chip;embedded system;interleaved memory;semiconductor memory;parallel computing;dynamic random-access memory;sense amplifier;wafer testing;memory refresh;computer hardware;telecommunications;computer science;flash memory emulator;design for testing;computer memory;universal memory;flat memory model;process variation;registered memory	EDA	21.845764322468742	54.67391106027495	166303
7ccfbbd5b9a86e51107f9b07026995afb2a36d72	closed form expressions for extending step delay and slew metrics to ramp inputs	median;elmore;moments;interconnects;slew;pdf;skewness;probability distribution function;timing	Recent years have seen significant research in finding closed form expressions for the delay of an RC circuit that improves upon the Elmore delay model. However, several of these formulae assume a step excitation, leaving it to the reader to find a suitable extension to ramp inputs (we always assume a saturated ramp in this paper). The few works that do consider ramp inputs do not present a closed-form formula that works for a wide range of possible input slews. We propose the PERI (Probability distribution function Extension for Ramp Inputs) technique, that extends delay metrics for step inputs to the more general and realistic non-step inputs. Although there has been little work done in finding good slew - which is also referred as signal transition time - metrics, we also show how one can extend a slew metric for step inputs to the non-step case. We validate the efficacy of our approach through experimental results from several hundred RC dominated nets extracted from an industry ASIC design.	application-specific integrated circuit;elmore delay;ramp simulation software for modelling reliability, availability and maintainability;rc circuit;rise time;signal transition	Chandramouli V. Kashyap;Charles J. Alpert;Frank Liu;Anirudh Devgan	2003		10.1145/640000.640009	skewness;probability density function;real-time computing;control theory;mathematics;moment;median;statistics	EDA	24.04455578406915	59.06149526142586	166560
8b22d14aed4d5fa28bc41986003ee63ca005b6df	closing the smoothness and uniformity gap in area fill synthesis	chemical mechanical polishing;density analysis;monte carlo method;vlsi manufacturability;dummy fill problem;linear program;lipschitz condition;monte carlo;copper;chemical mechanical planarization;density analysis monte carlo	"""Control of variability in the back end of the line, and hence in interconnect performance as well, has become extremely difficult with the introduction of new materials such as copper and low-k dielectrics. Uniformity of chemical-mechanical planarization (CMP) requires the addition of area fill geometries into the layout, in order to smoothen the variation of feature densities across the die. Our work addresses the following smoothness gap in the recent literature on area fill synthesis. (1)The very first paper on the filling problem (Kahng et al., ISPD98 [7]) noted that there is potentially a large difference between the optimum window densities in fixed dissections vs. when all possible windows in the layout are considered. (2)Despite this observation, all filling methods since 1998 minimize and evaluate density variation only with respect to a fixed dissection. This paper gives the first evaluation of existing filling algorithms with respect to """"gridless"""" (""""floating-window"""") mode, according to both the effective and spatial density models. Our experiments indicate surprising advantages of Monte-Carlo and greedy strategies over """"optimal"""" linear programming (LP) based methods. Second, we suggest new, more relevant methods of measuring a local uniformity based on Lipschitz conditions, and empirically demonstrate that Monte-Carlo methods are inherently better than LP with respect to the new criteria. Finally, we propose new LP-based filling methods that are directly driven by the new criteria, and show that these methods indeed help close the """"smoothness gap""""."""	chemical-mechanical planarization;circuit complexity;closing (morphology);experiment;greedy algorithm;linear programming;microsoft windows;monte carlo method;spatial variability	Yu Chen;Andrew B. Kahng;Gabriel Robins;Alex Zelikovsky	2002		10.1145/505388.505422	mathematical optimization;linear programming;mathematics;chemical-mechanical planarization;physics;statistics;monte carlo method	ML	23.333898802495792	58.337371780246386	166806
68dcc80be54634faceb107c7e5cf2c6cd1c67323	stochastic behavioral modeling and analysis for analog/mixed-signal circuits	process variation;behavior modeling;behavioral modeling;journal article;latin hypercube sampling;statistical distributions;correlation integrated circuit modeling probabilistic logic approximation methods accuracy random variables argon;stochastic processes;integrated circuit modelling;stochastic processes integrated circuit modelling mixed analogue digital integrated circuits statistical distributions;monte carlo method;drntu engineering electrical and electronic engineering integrated circuits;mixed analogue digital integrated circuits;moment matching method stochastic behavioral modeling analog mixed signal circuit moment matching based method probabilistic behavioral distribution latin hypercube sampling coupling correlation control technique high order moments;process variation behavioral modeling latin hypercube sampling monte carlo method	It has become increasingly challenging to model the stochastic behavior of analog/mixed-signal (AMS) circuits under large-scale process variations. In this paper, a novel moment-matching-based method has been proposed to accurately extract the probabilistic behavioral distributions of AMS circuits. This method first utilizes Latin hypercube sampling coupling with a correlation control technique to generate a few samples (e.g., sample size is linear with number of variable parameters) and further analytically evaluate the high-order moments of the circuit behavior with high accuracy. In this way, the arbitrary probabilistic distributions of the circuit behavior can be extracted using moment-matching method. More importantly, the proposed method has been successfully applied to high-dimensional problems with linear complexity. The experiments demonstrate that the proposed method can provide up to 1666X speedup over crude Monte Carlo method for the same accuracy.	algorithm;behavioral modeling;experiment;instability;linear programming;mixed-signal integrated circuit;monte carlo method;numerical analysis;sampling (signal processing);speedup;vhdl-ams	Fang Gong;Sina Basir-Kazeruni;Lei He;Hao Yu	2013	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2012.2217961	behavioral modeling;stochastic process;econometrics;mathematical optimization;electronic engineering;computer science;mathematics;statistics	EDA	23.564277494025866	58.35882413966888	166832
06b8c781f6d318ce31c55bd27c00491b853dc5dd	computing with subthreshold leakage: device/circuit/architecture co-design for ultralow-power subthreshold operation	traitement pipeline;medium frequency;evaluation performance;conception conjointe;diseno circuito;arquitectura circuito;five tap finite impulse response filter;ultralow power subthreshold operation;subthreshold current circuits computer architecture logic devices capacitance energy consumption application software portable computers logic design inverters;performance evaluation;filtre reponse impulsion finie;subthreshold current;logic design;diseno conjunto;application software;evaluacion prestacion;finite impulse response filter;circuit design;circuit architecture;inverters;paralelisacion;threshold logic;nmos technology;computer architecture;high level synthesis;subthreshold logic subthreshold leakage device circuit architecture co design ultralow power subthreshold operation ultralow power design power consumption five tap finite impulse response filter pseudo nmos logic;design technique;filtro respuesta impulsion acabada;finite impulse response;technologie nmos;parallel architectures;codesign;leakage currents;energy consumption;portable computers;logica umbral;tecnologia nmos;parallelisation;low power electronics;device circuit architecture co design;architecture circuit;parallelization;low power electronics logic design high level synthesis leakage currents parallel architectures fir filters;subthreshold leakage;conception circuit;circuits;ultralow power design;capacitance;fir filters;power consumption;logique seuil;consommation energie electrique;parallelization and pipelining;electronique faible puissance;subthreshold logic;subthreshold logic parallelization and pipelining pseudo nmos logic;logic devices;pipeline processing;pseudo nmos logic;design methodology	This paper presents a novel design methodology for ultralow-power design using subthreshold leakage as the operating current (suitable for medium frequency of operation: tens to hundreds of millihertz). Standard design techniques suitable for super-threshold design can be used in the subthreshold region. However, in this study, it has been shown that a complete co-design at all levels of hierarchy (device, circuit, and architecture) is necessary to reduce the overall power consumption while achieving acceptable performance (hundreds of millihertz) in the subthreshold regime of operation. Simulation results of co-design on a five-tap finite-impulse-response filter shows /spl sim/2.5/spl times/ improvement in throughput at iso-power compared to a conventional design.	simulation;spectral leakage;throughput	Arijit Raychowdhury;Bipul Chandra Paul;Swarup Bhunia;Kaushik Roy	2005	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2005.859590	embedded system;electronic engineering;computer science;engineering;electrical engineering;finite impulse response	EDA	18.098487067338244	54.99781016470997	166967
717b1c1c0258207c7a7dcf370a6706d147ad0123	a novel power-managed scan architecture for test power and test time reduction	desplazamiento frecuencia;metodo adaptativo;diseno circuito;implementation;circuit design;methode adaptative;electronica potencia;power supply;power electronics;low voltage;test power;electronique puissance;baja tension;leakage power;alimentation electrique;deplacement frequence;adaptive method;low power electronics;power management;basse tension;conception circuit;test time;power consumption;consommation energie electrique;implementacion;alimentacion electrica;frequency shift;voltage scaling;electronique faible puissance;hardware system design	In sub-70 nm technologies, leakage power becomes a significant component of the total power. Designers address this concern by extensive use of adaptive voltage scaling techniques to reduce dynamic as well as leakage power. Low-power scan test schemes that have evolved in the past primarily address dynamic power reduction, and are less effective in reducing the total power. This paper proposes a Power-Managed Scan (PMScan) scheme which exploits the presence of adaptive voltage scaling logic to reduce test power. Some practical implementation challenges that arise when the proposed scheme is employed on industrial designs are also discussed. Experimental results on benchmark circuits and industrial designs show that employing the proposed technique leads to a significant reduction in dynamic and leakage power. The proposed method can also be used as a vehicle to trade-off test application time with test power by suitably adjusting the scan shift frequency and scan-mode power supplies.		V. R. Devanathan;C. P. Ravikumar;Rajat Mehrotra;V. Kamakoti	2008	J. Low Power Electronics	10.1166/jolpe.2008.150	electronic engineering;telecommunications;computer science;engineering;electrical engineering;circuit design;power electronics;low voltage;implementation;low-power electronics	EDA	18.765032887445738	55.02209491899001	167145
15c59b2634ed5cfcb90ec63b8209f50efaa6680f	designs for reducing test time of distributed small embedded srams	efficient bist method;lra;integrated circuit testing;distributed small embedded sram;xwrtm;parallel local response analyzer;soc;drf;small buffer;dft technique;proposed test architecture;sram chips;defect coverage;no write recovery test mode;test time;e-sram;small embedded srams;data retention faults;better defect coverage;design for testability;sram test time reduction;write recovery test mode;small e-srams;test architecture;logic testing	"""This paper proposes a test architecture aimed at reducing test time of distributed small embedded SRAMs (e-SRAMs). This architecture improves the one proposed in (W. B. Jone et al, Proc. 17th IEEE VLSI Test Symp., p.246- 251, 1999 and also IEEE Transact. VLSI Syst., vol.10, no.4, p.512-515, 2002). The improvements are mainly two-fold. On one hand, the testing of time-consuming data retention faults (DRFs), that is neglected by the previously proposed test architecture, is now considered and performed via a DFT technique referred to as the """"no write recovery test mode (XWRTM)"""". On the other hand, a parallel local response analyzer (LRA), instead of a serial response analyzer, is used to reduce the test time of these distributed small e-SRAMs. Results from our evaluations show that the proposed test architecture can achieve a better defect coverage and test time compared to those obtained previously, with a negligible area cost."""	embedded system;software bug;static random-access memory;turing test;very-large-scale integration	Baosheng Wang;Yuejian Wu;André Ivanov	2004	19th IEEE International Symposium on Defect and Fault Tolerance in VLSI Systems, 2004. DFT 2004. Proceedings.	10.1109/DFT.2004.25	electronic engineering;parallel computing;real-time computing;computer science;engineering;automatic test pattern generation;test compression;design for testing	Embedded	20.04964158224239	53.831103338790435	167345
1dca26c3069e8ffb60d494aeb2d8b335c1bd7ab2	eliminating data invalidation in debugging multiple-clock chips	monolithic integrated circuits;clocks;size 0 18 mum data invalidation elimination multiple clock chip debugging post silicon debug run stop control clock gating data repetition data loss design for debug circuit smic technology spice tool;spice clocks microprocessor chips monolithic integrated circuits;chip;simulation experiment;theoretical analysis;clock synchronization;clocks synchronization debugging latches processor scheduling manganese;spice;data transfer;microprocessor chips	A critical concern for post-silicon debug is the need to control the chip at clock cycle level. In a single clock chip, run-stop control can be implemented by gating the clock signal using a stop signal. However, data invalidation might occur when it comes to multiple-clock chips. In this paper, we analyze the possible data invalidation, including data repetition and data loss, when stopping and resuming a multiple-clock chip. Furthermore, we propose an efficient solution to eliminate data repetition and data loss. Theoretical analysis and simulation experiments are both conducted for the proposed solution. We implement the proposed Design-for-Debug (DfD) circuit with SMIC 0.18/Ltm technology and simulate the data transfer across clock domains using SPICE tool. The results show that both data repetition and data loss can be avoided with the proposed solution, even if metastability occurs.	clock signal;debug;experiment;metastability in electronics;spice;simulation	Jianliang Gao;Yinhe Han;Xiaowei Li	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763117	chip;clock synchronization;embedded system;electronic engineering;parallel computing;real-time computing;telecommunications;clock domain crossing;computer science;clock gating	EDA	20.14660703788024	54.119010643279275	167472
3ccaae1ac8191d57d7d92f3dbb19ab4d55184af1	gate sizing using incremental parameterized statistical timing analysis	delays;integrated circuit design;integrated circuit yield;statistical analysis;timing jitter;ic design;ic yield;circuit simulation;correlated parameter;fitted process sensitivity;gate sizing;linear delay;run-time;statistical static timing analysis	As technology scales into the sub-90 nm domain, manufacturing variations become an increasingly significant portion of circuit delay. As a result, delays must be modeled as statistical distributions during both analysis and optimization. This paper uses incremental, parametric statistical static timing analysis (SSTA) to perform gate sizing with a required yield target. Both correlated and uncorrelated process parameters are considered by using a first-order linear delay model with fitted process sensitivities. The fitted sensitivities are verified to be accurate with circuit simulations. Statistical information in the form of criticality probabilities are used to actively guide the optimization process which reduces run-time and improves area and performance. The gate sizing results show a significant improvement in worst slack at 99.86% yield over deterministic optimization.	first-order predicate;mathematical optimization;self-organized criticality;simulation;slack variable;statistical static timing analysis	Matthew R. Guthaus;Natesan Venkateswaran;Chandu Visweswariah;Vladimir Zolotov	2005	ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.		probability distribution;electronic engineering;real-time computing;simulation;delay calculation;computer science;first-order logic;mathematics;static timing analysis;statistics;integrated circuit design	EDA	22.650378429311377	57.74570659887896	167625
744af5c16ea62980ee4edc8a3d8f944d4081c90d	simultaneous opc- and cmp-aware routing based on accurate closed-form modeling	chemical mechanical polishing;routing;optical proximity correction	As the process technology advances to the nanometer nodes, Optical Proximity Correction (OPC) is the most popular Resolution-Enhancement Technique (RET) in industry for subwavelength lithography, and the inter-level dielectric (ILD) thickness variation caused by the planarization step of the Chemical-Mechanical Polishing (CMP) process also plays a key role for interconnect yield. Considering the OPC and CMP effects simultaneously during the routing stage can significantly alleviate the width and thickness variations (and thus the whole 3D geometry variations) of post-layout RET and CMP operations. In this paper, we first present an efficient, yet sufficiently accurate closed-form formula for printed width computation and dummy-insertion-aware routing cost derivation. The formula provides a cost modeling for post-layout OPC and CMP optimization during routing. Incorporating the OPC and CMP costs, the router can be guided to optimize the effects of layout correction and planarization. Compared with the state-of-the-art OPC-friendly router, QL-MGR (which does not consider CMP), the experimental results show that our approach can achieve respective 19% and 6% reductions in the maximum and average layout distortions. Compared with the state-of-the-art CMP-aware router, TTR (which does not consider OPC), the experimental results show that our approach can achieve respective 19% and 25% reductions in the peak-to-peak thickness and thickness variance. These results indicate that our simultaneous OPC- and CMP-aware router contributes a significant improvement for layout integrity.	approximation;chemical-mechanical planarization;computation;distortion;dummy variable (statistics);internet listing display;mathematical optimization;open platform communications;printing;resolution enhancement technology;router (computing);routing;thickness (graph theory)	Shao-Yun Fang;Chung-Wei Lin;Guang-Wan Liao;Yao-Wen Chang	2013		10.1145/2451916.2451938	routing;parallel computing;chemical-mechanical planarization;physics;optical proximity correction	EDA	21.508473656078355	57.6329189504104	167632
67077370c83717fe7a48f37373d89ed33b1adf66	extended dynamic voltage scaling for low power design	cmos integrated circuits;dynamic voltage scaling energy efficiency circuits voltage control frequency threshold voltage delay process design low voltage microprocessors;integrated circuit;energy efficient;dynamic voltage scaling;processor design dynamic voltage scaling low power design energy reduction integrated circuits subthreshold supply voltages leakage energy minimum energy optimal voltage mtcmos;integrated circuit design;technology scaling;low power;energy optimization;low power electronics;low power electronics microprocessor chips integrated circuit design cmos integrated circuits;just in time;low power design;point of view;analytical model;microprocessor chips	"""Dynamic voltage scaling (DVS) is a popular approach for energy reduction of integrated circuits. Current processors that use DVS typically have an operating voltage range from full to half of the maximum V/sub dd/. However, it is possible to construct designs that operate over a much larger voltage range: from full Vdd to subthreshold voltages. This possibility raises the question of whether a larger voltage range improves the energy efficiency of DVS. First, from a theoretical point of view, we show that for subthreshold supply voltages leakage energy becomes dominant, making """"just in time completion"""" energy inefficient. We derive an analytical model for the minimum energy optimal voltage and study its trends with technology scaling. Second, we compare several different low-power approaches including MTCMOS, standard DVS and extended DVS to subthreshold operation. Study of real applications on commercial processor shows that extended DVS has the best energy efficiency. Therefore, we conclude that extending the voltage range below V/sub dd//2 will improve the energy efficiency for most processor designs."""	central processing unit;dynamic voltage scaling;ibm notes;image scaling;integrated circuit;just-in-time compilation;low-power broadcasting;multi-threshold cmos;power gating;rise time;spice;semiconductor research corporation;simulation;spectral leakage;tracing (software);value-driven design	Bo Zhai;David Blaauw;Dennis Sylvester;Krisztián Flautner	2004	IEEE International SOC Conference, 2004. Proceedings.	10.1109/SOCC.2004.1362475	embedded system;electronic engineering;real-time computing;computer science;engineering;operating system;integrated circuit;efficient energy use;cmos;low-power electronics;integrated circuit design	EDA	18.848267286052447	58.322913915838	167728
1155cd65d8dfc0dc4666c86942f67410ac9d32d4	modeling of retention time for high-speed embedded dynamic random access memories	couplings noise transistors random access memory capacitance logic gates timing;integrated circuit noise cache storage dram chips embedded systems high speed integrated circuits integrated circuit design integrated circuit modelling;size 22 nm retention time modeling high speed embedded dynamic random access memory large cache applications density benefits speed benefits power benefits edram array design logic process commodity dram scaled technology bitcell leakage reference voltage variations effects frequency dependent writeback voltage pattern dependent coupling noise strict frequency power budgets second order mechanisms array retention time noise sources noise variations predictive technology;retention modeling embedded dynamic random access memories edram low power memory	Embedded dynamic random access memory (eDRAM) is becoming a popular choice for large cache applications due to its density, speed, and power benefits. One of the crucial challenges in eDRAM design is meeting the retention time specification. Due to implementation in logic process, usually eDRAM suffers from poor retention time compared to commodity DRAM. The retention time of eDRAM designed in scaled technologies not only depends on bitcell leakage but also on effects such as reference voltage variations, frequency-dependent writeback voltage, and various pattern-dependent coupling noise. Under the strict frequency and power budgets, these second-order mechanisms start playing a major role in determining the array retention time. Designing eDRAM array for certain retention time requires detailed modeling and understanding of the noise sources and variations. This paper investigates these components and provides a model of eDRAM retention time. Our results in 22 nm predictive technology shows that retention time can be impacted by as much as 10-16 × if the noise and variations are not contained in the design.	cache (computing);dynamic random-access memory;edram;embedded system;random access;spectral leakage	Swaroop Ghosh	2014	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2014.2312481	electronic engineering;parallel computing;real-time computing;computer science	Arch	19.802860643779443	58.43113427406904	167730
6c362abdcefb50407d1e03bcef87779fa104ecd7	impact of capacitance correlation on yield enhancement of mixed-signal/analog integrated circuits	process variation;device mismatch capacitance correlation mixed signal analog integrated circuits;integrated circuit yield;capacitance integrated circuit yield analog integrated circuits circuit simulation information analysis process design performance evaluation fluctuations analog circuits costs;analog digital conversion;layout;indexing terms;chip;analog circuits;yield analysis;mixed analogue digital integrated circuits capacitance integrated circuit yield;spatial correlation;analog integrated circuits;mixed analogue digital integrated circuits;capacitance ratio;capacitance;time to market;correlation;mismatch;common centroid;yield analysis capacitance ratio common centroid mismatch process variation spatial correlation;modulation	Random fluctuations in process conditions change the physical properties of parameters on a chip. The correlation of device parameters depends on spatial locations. In general, the closer devices most likely have the similar parameter variation. The key performance of many analog circuits is directly related to accurate capacitance ratios. Parallel unit capacitances have a great effect on reducing ratio mismatch. This paper addresses the impact of capacitance correlation on the yield enhancement of mixed-signal/analog integrated circuits. The relationship between correlation and variation of capacitance ratio is also presented. Therefore, both mismatch and variation of capacitance ratio can be expressed in terms of capacitance correlation. Furthermore, both process variation and device mismatch are considered in the early design phase to reduce the design costs and speed up the time to market.	analogue electronics;coefficient;integrated circuit design;linear integrated circuit;mixed-signal integrated circuit;modulation;simulation	Pei-Wen Luo;Jwu-E Chen;Chin-Long Wey;Liang-Chia Cheng;Ji-Jan Chen;Wen Ching Wu	2008	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2008.2006139	chip;control engineering;layout;yield spread;electronic engineering;spatial correlation;index term;analogue electronics;computer science;engineering;electrical engineering;capacitance;process variation;correlation;modulation	EDA	23.03444692934015	56.970097769142654	167742
3a89445b76c7f2086c5e01fc54e1dd5557ebdd75	leakage power reduction in data driven dynamic logic circuits	power transistors adders carry logic leakage currents logic circuits logic design logic gates low power electronics;power delay dynamic power consumption static power consumption low power methodology current leakage power gating transistor short circuit paths dual v t sleep switch transistors ripple carry adders power leakage split path data driven dynamic logic circuit;transistors adders delays logic gates logic circuits clocks;power gating data driven dynamic logic leakage low power sleep switch	In this paper, data-driven dynamic logic and its' variant split-path data-driven dynamic logic circuits are analyzed for dynamic and static power consumption. A new low-power methodology is introduced in order to reduce the leakage current while maintaining the speed advantages of the data driven dynamic logic. A sleep switch transistor is used in the data driven dynamic circuits in order to force a sleep mode asynchronously. An additional power gating transistor is used to avoid the possible short-circuit paths during idle mode. The proposed circuits are compared against the conventional dynamic logic that uses dual-Vt sleep switch transistors for reduced leakage. 45nm technology has been used to implement the designs and they have been tested using ripple carry adders. The results demonstrate that, with the proposed circuits, leakage power can be reduced by more than 90%, with minimal impact on the speed and dynamic power consumption. Split-path data-driven dynamic logic offers the best speed and power-delay product among the dynamic logic circuits.	adder (electronics);clock network;domino logic;dynamic circuit network;dynamic logic (modal logic);logic gate;low-power broadcasting;mathematical optimization;network switch;power gating;power–delay product;ripple effect;simulation;sleep mode;spectral leakage;transistor	Cecilia García Martin;Erdal Oruklu	2014	IEEE International Conference on Electro/Information Technology	10.1109/EIT.2014.6871773	and-or-invert;electronic engineering;nmos logic;real-time computing;diode–transistor logic;adiabatic circuit;logic level;logic gate;logic family;depletion-load nmos logic;electrical engineering;pass transistor logic;sequential logic;diode logic;integrated injection logic;pull-up resistor;clock gating;digital electronics;pmos logic;low-power electronics;resistor–transistor logic;emitter-coupled logic	EDA	17.776049781771416	57.316851105472004	167890
9ab3e4a1d7408c9178e8fea035f41efd8f590da9	unconventional layout techniques for a high performance, low variability subthreshold standard cell library		A novel subthreshold sizing strategy utilizing the Inverse Narrow Width Effect is demonstrated that has the largest range of propagation delays within the same cell footprint and lowest variability of any subthreshold sizing strategy thus far proposed. Simulation results and ring oscillators implemented in a commercial 65nm low power process confirm a propagation delay improvement of up to 1.95X over the standard superthreshold sizing strategy at 300mV and below. 32 bit multipliers are then synthesized and static timing analysis performed at several subthreshold voltage corners to illustrate applicability to real designs in conventional EDA design flows.	32-bit;design flow (eda);heart rate variability;propagation delay;simulation;software propagation;spatial variability;standard cell;static timing analysis;tape-out	Jordan Morris;Pranay Prabhat;James Myers;Alexandre Yakovlev	2017	2017 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)	10.1109/ISVLSI.2017.14	design flow;standard cell;propagation delay;subthreshold conduction;logic gate;electronic engineering;static timing analysis;capacitance;engineering	EDA	19.38090928803469	57.575874094004924	168399
d03baf2b4545cde99724a2a588ed7e15b5259682	an optimal round-robin arbiter design for noc	fonction booleenne;optimisation;routeur;haute performance;optimizacion;diminution cout;execution time;intellectual property;multiprocessor;arbitrage;concepcion optimal;conception optimale;boolean function;system on a chip;round robin;patent rights;sistema sobre pastilla;propiedad industrial;arbitration;patents;funcion booliana;arbiter;propiedad intelectual;alto rendimiento;optimal design;router;temps execution;optimization;systeme sur puce;propriete industrielle;patente;noc;multiprocesador;tiempo ejecucion;reduccion costes;brevet;high performance;propriete intellectuelle;cost lowering;network;multiprocesseur	A new optimal arbiter is designed. We proposed a set of optimal Boolean functions and the corresponding circuit for it, and showed that the arbitration Boolean functions derived are optimal (simplest). This new arbiter is fair for any input combinations and faster than all previous arbiters we knew. Using Synopsys design tools with TSMC 0.18μm technology, the design results have shown that our arbiter has 22.8% improvement of execution time and 39.1% of cost (area) reduction compared with the existing fastest arbiter, SA [7]. Because of this small arbiter’s the high-performance, it is extremely useful for the realizations of NoC routers, MPSoC arbitration, and ultra-highspeed switches. This new arbiter is being applied for a patent of the R.O.C. (application No.: 0972020612-0).	arbiter (electronics);boolean algebra;broadcast delay;circuit complexity;fairness measure;fastest;logic gate;mpsoc;network on a chip;network switch;ppa (complexity);propagation delay;round-robin scheduling;router (computing);run time (program lifecycle phase);state diagram	Jer-Min Jou;Yun-Lung Lee	2010	J. Inf. Sci. Eng.		system on a chip;embedded system;parallel computing;multiprocessing;telecommunications;computer science;optimal design;network operations center;boolean function;arbitration;intellectual property;arbitrage	EDA	18.361155955894972	54.15359914092745	168822
dbc1bab2c6e1cd2281b168d748771b589fa1d56b	mtf test system with ac based dynamic joule correction for electromigration tests on interconnects		For accelerated electromigration tests to be accurately measured and extrapolated, the sample temperature has to remain constant during the entire test. Conventional Median Time to Failure (MTF) test systems take the joule heating into account only at the beginning of the test, which is not sufficient. A solution to this problem was formulated by Scandurra et al. by introducing a DC-current based dynamic joule correction. In this paper, a new test system has been developed which makes use of an AC-current based dynamic joule correction. In this way, no electromigration effects take place during the determination of the thermal resistance.	ac (complexity);electrical connection;electromigration;joule;move-to-front transform	Leen Biesemans;K. Schepers;Kris Vanstreels;Jan D'Haen;Ward De Ceuninck;Marc D'Olieslaeger	2004	Microelectronics Reliability	10.1016/j.microrel.2004.07.096	joule effect;engineering;joule;electronic engineering;electromigration;joule heating;integrated circuit;thermal resistance	EDA	23.7159003651921	55.81445100350267	169118
40dc8d32ae3b8c292b22a836592b54eff39c0011	a low-leakage single-ended 6t sram cell	voltage 1 v low leakage 6t sram cell single ended 6t sram cell six transistor sram cell dual threshold voltage dual power supply leakage power dissipation leakage power consumption voltage 0 6 v;microprocessors;random access memory;low leakage;leakage current;random access memory transistors threshold voltage delay microprocessors leakage current computer architecture;power supply;computer architecture;leakage power;threshold voltage;transistors;sram cell;low leakage sram cell;sram chips	A new six transistor (6T) SRAM cell is presented in this paper for low leakage design. In the proposed SRAM cell, dual threshold voltage and dual power supply techniques are used in order to reduce leakage power dissipation. The new cell operates at 0.6 V in standby mode and at 1V during read operation. The proposed cell has been compared to the conventional 6T-SRAM, using the 65 nm technology. Experimental results show leakage power consumption is reduced by up to 72.7%.	cell (microprocessor);power supply;simulation;single-ended signaling;sleep mode;spectral leakage;static random-access memory;transistor	Sohit Solanki;Fabio Frustaci;Pasquale Corsonello	2010	2010 3rd International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2010.102	embedded system;electronic engineering;real-time computing	EDA	17.454339254096894	58.50247590497167	169329
5ba13fbb7f2f807cafe540fcf7bc25c2069bc031	evaluation of differential vs. single-ended sensing and asymmetric cells in 90 nm logic technology for on-chip caches	microprocessors;cache storage;logic arrays;random access memory;power saving;degradation;delay voltage random access memory degradation frequency logic arrays microprocessors differential amplifiers inverters driver circuits;differential sensing;single ended sensing;logic design;sram arrays;350 mv differential sensing single ended sensing asymmetric cells logic technology on chip caches sram arrays delay degradation single ended large signal arrays integrated circuit design bit line leakage 90 nm;sram chips cache storage integrated circuit design integrated logic circuits logic design nanoelectronics;inverters;differential amplifiers;logic technology;delay degradation;chip;integrated circuit design;on chip caches;90 nm;voltage;nanoelectronics;single ended large signal arrays;asymmetric cells;driver circuits;integrated logic circuits;power consumption;bit line leakage;frequency;cache memories;350 mv;sram chips	SRAM arrays using differential sensing (DS) and single-ended sensing (SE) are designed and fabricated in a test chip and their power and performance behaviors are studied in this paper. Sense amplifier offset (DC condition), which is one of the main criterion to determine the required bit-line differential, is measured. A novel SE scheme is proposed to overcome the delay degradation due to large bit line leakage in scaled technology. With marginal switching power savings, the SE array is 56% slower than the DS array in 90 nm technology with a single high-Vt at 350 mV. The difference narrows down to 30% in low-Vt case. Using asymmetric cells, instead of symmetric cells, in single-ended large signal arrays improves delay by 3%, while the power consumption remains approximately the same	eisenstein's criterion;elegant degradation;large-signal model;marginal model;sense amplifier;sensor;single-ended signaling;spectral leakage;static random-access memory	Yibin Ye;Muhammad M. Khellah;Dinesh Somasekhar;Vivek De	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1692747	nanoelectronics;chip;embedded system;electronic engineering;logic synthesis;real-time computing;voltage;degradation;telecommunications;computer science;engineering;frequency;quantum mechanics;integrated circuit design	Arch	17.33732626517814	59.270735417811736	169371
0fbf03059c47cab31bb4f83cf3f7850f71e4c5fa	comparative analysis of timing yield improvement under process variations of flip-flops circuits	power overhead;cmos integrated circuits;process variation;comparative analysis;statistical gate sizing;cmos technology;timing flip flops circuits clocks cmos technology delay effects master slave sampling methods very large scale integration cmos process;sense amplifier;clocks;statistical gate sizing process variations flip flops timing yield;very large scale integration;probability density function;flip flops;timing circuits;delay effects;gate sizing algorithms;cmos process;synchronous system;process variations;logic gates;timing yield;timing yield improvement;timing circuits cmos integrated circuits flip flops;flip flop delay;stmicroelectronics;circuits;size 65 nm timing yield improvement process variations flip flops circuits flip flop delay gate sizing algorithms stmicroelectronics cmos technology power delay product sense amplifier power overhead transmission gate master slave flip flop;power delay product;transmission gate master slave flip flop;sampling methods;master slave;size 65 nm;flip flops circuits;flip flop;monte carlo methods;timing;time constraint	In synchronous systems, any violation of the timing constraints of the flip-flops can cause the overall system to malfunction. Moreover, the process variations create a large variability in the flip-flop delay in scaled technologies impacting the timing yield. Overtime, many gate sizing algorithms have been introduced to improve the timing yield. This paper presents an analysis of timing yield improvement of four commonly used flip-flops under process variations. These flip-flops are designed using STMicroelectronics 65-nm CMOS technology. The analyzed flip-flops are compared for power and power-delay product (PDP) overheads to achieve this timing yield improvement. The analysis shows that the sense amplifier based flip flop (SA-FF) has a power overhead and PDP overhead of 1.7X and 2.8X, respectively, much higher than that of the transmission-gate master-slave flip flop(TG-MSFF) . The TG-MSFF exhibits the lowest relative power and PDP overheads of 30.87% and 9% ,respectively.	algorithm;cmos;flops;flip-flop (electronics);heart rate variability;iteration;logic gate;overhead (computing);power–delay product;qualitative comparative analysis;sense amplifier	Hassan Mostafa;Mohab Anis;Mohamed I. Elmasry	2009	2009 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2009.23	embedded system;electronic engineering;real-time computing;engineering	EDA	19.21049698297594	58.14558899160666	169472
9c951c5c11233f5f8c3bc9829082f4df4c740cd0	derivation of signal flow for switch-level simulation	mos integrated circuits;circuit cad;mos circuits;microprocessor;node capacitor sizes;signal flow derivation;static analysis;switch-level characteristics;switch-level simulation;transistor strengths;unidirectional transistors	This paper presents a new algorithm for deriving the direction of signal flow in MOS circuits. The algorithm detects so-called unidirectional transistors. In a unidirectional transistor, signal flow is restricted to one direction during switch-level simulation, without compromising the simulation results. The algorithm uses a static analysis of the switch-level characteristics of the circuit, such as the transistor strengths and node capacitor sizes. It was implemented and used in the simulation of a large, commercial microprocessor. For this processor, 98.5% of the transistors were determined to be unidirectional by the algorithm. The simulation time for this processor decreased significantly when unidirectional transistors were detected.	algorithm;microprocessor;simulation;static program analysis;transistor	David Blaauw;Daniel G. Saab;Junsheng Long;Jacob A. Abraham	1990			algorithm design;switched capacitor;network switch;computer science;electrical engineering;discrete event simulation;computational model;static analysis;algorithm	Arch	20.689006189669023	55.36740819977153	169808
b4333735abcfa3018b98c8bfdf2a351b3fb40295	a 1.1ghz 12μa/mb-leakage sram design in 65nm ultra-low-power cmos with integrated leakage reduction for mobile applications	0 5 to 1 2 v;ultra low power;low leakage memory cell;leakage reduction;integrated leakage reduction;250 mhz;65 nm;8m cmos;0 5 to 1 2 v sram design integrated leakage reduction mobile application low leakage memory cell 8m cmos 1 1 ghz 250 mhz 65 nm;low power;cmos memory circuits;sram design;mobile computing;random access memory circuits cmos technology mos devices subthreshold current leakage current dynamic voltage scaling voltage control frequency energy consumption;1 1 ghz;high speed;sram chips cmos memory circuits mobile computing;mobile application;sram chips	A low-power high-speed SRAM macro is implemented in an ultra-low-power 8M 65nm CMOS for mobile applications. The 1Mb macro features a 0.667μm2 low-leakage memory cell and operates with supply voltage from 0.5V to 1.2V. It operates at a frequency of 1.1 GHz at 1.2V and 250MHz at 0.7V. Leakage is reduced to 12μA/Mb at the data retention voltage of 0.5V. The measured bitcell leakage from the SRAM array is ~2pA/b at retention voltage with integrated leakage reduction schemes.	cmos;low-power broadcasting;memory cell (binary);mobile app;spectral leakage;static random-access memory	Yih Wang;Hong Jo Ahn;Uddalak Bhattacharya;T. Coan;Fatih Hamzaoglu;W. Hafez;C.-H. Jan;Pramod Kolar;Sarvesh H. Kulkarni;Jimmy Lin;Yong-Gee Ng;I. Post;Liqiong Wei;Yih Zhang;Kevin Zhang;Mark Bohr	2007	2007 IEEE International Solid-State Circuits Conference. Digest of Technical Papers	10.1109/ISSCC.2007.373425	embedded system;electronic engineering;real-time computing;computer science;operating system;mobile computing	EDA	17.42542586889099	58.748338089244776	170066
44f6b8afff6f0d2bdbd6308df1d4fe5bfd096ab2	low power integrated scan-retention mechanism	low power design integrated scan retention mechanism data retention scan mechanism latch scannable latch power gating sleep mode retention latch active mode leakage control;latches flip flops sequential circuits cmos technology threshold voltage cmos logic circuits power supplies leakage current permission art;leakage;subthreshold;leakage currents flip flops low power electronics integrated circuit design;flip flops;power gating;data retention;integrated circuit design;low power;leakage currents;balloon latch;scan;low power electronics;mtcmos;latch	This paper presents a methodology for unifying the scan mechanism and data retention in latches which leads to scannable latches with the data retention capability achieved at a very low power overhead during the active mode. A detailed analysis of power and area overhead is presented, with layout examples for various common latch styles. Implications of using different power gating techniques for reducing leakage during sleep mode on the design of retention latches are considered, including well biasing for leakage control and sharing wells between gated logic and retention latch devices.	biasing;evernote;overhead (computing);power gating;sleep mode;spectral leakage	Victor V. Zyuban;Stephen V. Kosonocky	2002		10.1145/566408.566436	embedded system;electronic engineering;real-time computing;computer science;engineering;electrical engineering;operating system;leakage;subthreshold conduction;low-power electronics;integrated circuit design	EDA	17.78997494921013	58.00283301853998	170545
36f40e09c1e5c8e48fb4af1aa6e0daae794831b4	4.7 a 409gops/w adaptive and resilient domino register file in 22nm tri-gate cmos featuring in-situ timing margin and error detection for tolerance to within-die variation, voltage droop, temperature and aging	delays radio frequency arrays measurement uncertainty frequency measurement registers;frequency measurement;measurement uncertainty;arrays;radio frequency;registers;scaled cmos process domino register file tri gate cmos in situ timing margin error detection within die variation voltage droop high performance microprocessor datapaths precharge evaluate read critical path energy efficiency;microprocessor chips cmos logic circuits;delays	8-Transistor (8T) cell 1-read/1-write (1R1W) register files (RF) with domino read and static differential write are critical performance-limiting building blocks in high-performance microprocessor datapaths. The RF operating voltage (V) and frequency (F) are limited by the delay of the precharge-evaluate read critical path. Traditionally, the operating V/F is set to ensure no read timing error across all data access patterns in the RF array in the presence of within-die (WID) parameter (P) variations, and worst-case voltage droops, temperature (T) changes and transistor-aging-induced delay degradations. However, many of these worst-case conditions and events are rare during normal operation. Therefore, these V/F guardbands can severely limit the best-achievable performance and energy efficiency in scaled CMOS process.	best, worst and average case;cmos;critical path method;data access;datapath;error detection and correction;microprocessor;radio frequency;register file;transistor;triangular function	Jaydeep P. Kulkarni;Carlos Tokunaga;Paolo A. Aseron;Trang Nguyen;Charles Augustine;James W. Tschanz;Vivek De	2015	2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers	10.1109/ISSCC.2015.7062936	embedded system;electronic engineering;real-time computing;computer science;processor register;radio frequency;measurement uncertainty	Arch	18.944502947941604	58.84948905438309	171389
eee2cd9569bd89824491e1f893619ef83e768f99	integrated temperature sensors for on-line thermal monitoring of microelectronic structures	bist;oscillations;system reliability;circuit under test;on line testing;signal integrity;oscillation test strategy;temperature sensor;chip;thermal monitoring;power dissipation;test methods	Built-in temperature sensors increase the system reliability by predicting eventual faults caused by excessive chip temperatures. In this paper, simple and efficient built-in temperature sensors for the on-line thermal monitoring of microelectronics structures are introduced. The proposed temperature sensors produce a signal oscillating at a frequency proportional to the temperature of the microelectronics structure and therefore they are compatible to the oscillation-test method. The oscillation-test method is a low-cost and robust test method for mixed-signal integrated circuits based on transforming the circuit under test (CUT) to an oscillator. This paper presents the design and detailed characteristics of the sensors proposed based on the CMOS 1.2 μm technology parameters of Mitel S.C.C. Extensive post-layout simulations show that the oscillation frequency is very sensitive to temperature variations. The sensors proposed require very small power dissipation and silicon area.	cmos;cpu power dissipation;canonical account;mixed-signal integrated circuit;online and offline;sensor;simulation;test strategy	Karim Arabi;Bozena Kaminska	1998	J. Electronic Testing	10.1023/A:1008285907945	chip;embedded system;electronic engineering;telecommunications;signal integrity;engineering;electrical engineering;dissipation;test method;oscillation;statistics	EDA	21.68301495019923	55.60996789759772	171835
4bab6bbb3af78dc40e81583abfb2fbf1ae3a2a49	variation aware guard -banding for soc static timing analysis	sta;program diagnostics;logic design;systematics;clocks;random number generation;constant guard band;variation aware guard banding;statistical methods;statistical method;design quality;system on a chip;integrated circuit design;electronic design automation and methodology;performance verification;statistical analysis;timing clocks delay sampling methods logic design electronic design automation and methodology random number generation microelectronics statistical analysis performance analysis;system on chip;design quality variation aware guard banding soc static timing analysis sta constant guard band parameter variation statistical methods;parameter variation;performance analysis;soc static timing analysis;static timing analysis;performance verification static timing analysis variation;approximation methods;electronic engineering computing;system on chip electronic engineering computing integrated circuit design program diagnostics statistical analysis;variation;microelectronics;sampling methods;timing	The conventional approach to static timing analysis (STA) is to apply a constant guard-band against parameter variation. This can lead to costly and inefficient design. In this paper, we show that the guard-band can be reduced through statistical methods applied using an industry standard STA tool while preserving pessimism to ensure design quality and yield.	static timing analysis;technical standard	Vee Kin Wong;Siong Kiong Teng	2010	2010 11th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2010.5450541	system on a chip;embedded system;electronic engineering;real-time computing;computer science;electrical engineering;operating system;statistics	EDA	22.795266814866558	57.43449061115447	172746
5d0bb719d4aa0f93701c2b16f9734b17f0b49838	sensitivity of a power supply damping method to resistance and current waveform variations	digital circuit;resistance electrique;cmos integrated circuits;damping;resistencia electrica componente;circuit integre cmos;optimisation;optimizacion;simulated annealing;power supply;chip;circuit numerique;recuit simule;electrodiffusion;alimentation electrique;power supply noise;electrodifusion;circuito numerico;recocido simulado;optimization;electromigration;resistor;alimentacion electrica;amortiguacion;amortissement	In this paper the influence of parameter variations to the effect of power supply noise damping in digital CMOS circuits is investigated. Splitting up the power supply path and using different additional resistors in each path combined with a slight increase of the off-chip supply voltage is found to reduce significantly power supply noise. The damping resistors are optimized using a simulated annealing schedule for the worst-case current waveform. The dependency of this approach to current waveform variations and an increased resistance due to electromigration or a higher operating temperature is examined.		Jürgen Rauscher;Hans-Jörg Pfleiderer	2006		10.1007/11847083_48	resistor;chip;damping;power supply rejection ratio;embedded system;electromigration;simulated annealing;computer science;engineering;electrical engineering;switched-mode power supply;cmos;digital electronics	EDA	18.272624234354783	55.13851272553514	172890
20a27b59d95cb24d2d0caaea27ff128d85450857	feasibility of monolithic and 3d-stacked dc-dc converters for microprocessors in 90nm technology generation	microprocessors;convertisseur courant continu;carte electronique;cmos integrated circuits;microprocessor;evaluation performance;output voltage;direct current convertor;micromontaje;3 d integration;power supply circuits;cmos technology;capacitancia;thin film;performance evaluation;real estate;integrated circuit;thin film inductors;printed circuit;routing;desacoplamiento;switching convertors;evaluacion prestacion;inducteur couche mince;power capacitors;90 to 180 nm 3d stacked dc dc converters monolithic dc dc converters microprocessor power delivery on die switching converter back end thin film inductor module cmos process smaller external decoupling peak supply current high performance microprocessors cascode bridge high voltage switching low voltage process buck converter 85 percent;decouplage;condensador potencia;circuito integrado;tension sortie;cmos process;power inductors dc dc power convertors power supply circuits low power electronics switching convertors microprocessor chips computer power supplies cmos integrated circuits;power inductors;power supply;dc dc power convertors;dc dc power converters;convertidor corriente continua;switching converters;condensateur puissance;microassemblage;alimentation electrique;matriz formadora;circuit integre monolithique;dc dc converter;voltage;power delivery;monolithic integrated circuit;tarjeta electronica;low power electronics;integrated magnetics;die;decoupling capacitor;microassembling;dc dc power converters microprocessors cmos technology costs power capacitors routing switching converters cmos process thin film inductors voltage;decoupling;microprocesseur;capacitance;printed circuit board;convertisseur commutation;circuito integrado monolitico;alimentacion electrica;matrice formage;electronique faible puissance;power capacitor;microprocesador;circuit imprime;on die switching converter;circuit integre;circuito imprimido;computer power supplies;microprocessor chips;capacite electrique;voltage salida	Rapidly increasing input current of microprocessors resulted in rising cost and motherboard real estate occupied by decoupling capacitors and power routing. We show by analysis that an on-die switching DC-DC converter is feasible for future microprocessor power delivery. The DC-DC converter can be fabricated in an existing CMOS process (90nm-180nm) with a back-end thin-film inductor module. We show that 85% efficiency and 10% output voltage droop can be achieved for 4:1, 3:1, and 2:1 conversion ratios, area overhead of 5% and no additional on-die decoupling capacitance. A 4:1 conversion results in 3.4x smaller input current and 6.8x smaller external decoupling.	british undergraduate degree classification;cmos;coupling (computer programming);microprocessor;motherboard;overhead (computing);routing	Gerhard Schrom;Peter Hazucha;Jae-Hong Hahn;Volkan Kursun;Donald S. Gardner;Siva G. Narendra;Tanay Karnik;Vivek De	2004	Proceedings of the 2004 International Symposium on Low Power Electronics and Design (IEEE Cat. No.04TH8758)	10.1145/1013235.1013302	embedded system;electronic engineering;computer science;engineering;electrical engineering;integrated circuit;decoupling;printed circuit board;cmos	Arch	18.375779800624503	55.46664943908774	172920
3dac8a216f2895ae21903bc269d61271110a4969	efficient post-silicon validation via segmentation of process variation envelope — global vs local variations	silicon;vector spaces delay marginalities post silicon validation local and global process variations segmentation of variation envelopes variability;silicon elemental semiconductors integrated circuit design;elemental semiconductors;delays vectors testing logic gates silicon process control;integrated circuit design;si post silicon validation delay marginalities full global plus local process variation envelope sub envelopes worst case full local only variations on die variations across wafer lots across wafers across die variability aware approach multiple vectors generation vector spaces segment by segment manner worst case delay design process	In this paper, we propose an efficient method for generating validation vectors for identifying delay marginalities under increasing levels of process-variations-across die, across wafers, across wafer lots. With the goal of significantly reducing the number of vectors required for validation, we propose an approach for segmenting the full global plus local process variation envelope into sub-envelopes, where each sub-envelope is guaranteed to capture worst-case full local-only (worst case on-die) and partial global-only (worst case of across die, across wafers, and across wafer lots) variations, and where all sub-envelopes collectively capture the worst-case full global plus local variations. We then use our recent variability aware approach for generating multiple vectors (captured as vector-spaces) in a segment-by-segment manner to guarantee the invocation of the worst-case delay of the chips in the first-silicon batch. We present extensive experimental results to demonstrate the effectiveness of our approach, especially in the context of increasing process variations.	acceptance testing;best, worst and average case;spatial variability;test set;wafer (electronics)	Prasanjeet Das;Sandeep K. Gupta	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783314	electronic engineering;real-time computing;engineering;silicon;integrated circuit design	Arch	22.554440375087644	56.791699666216935	173181
35ccd74afd8ad84f95914d3a405c8afbf9b7b471	modeling process variability in scaled cmos technology	model design;high dielectric;statistical circuit analysis;cmos integrated circuits;complementary metal oxide semiconductor;scaled cmos technology;polysilicon granularity;modeling process variability;random variables;network analysis;statistical compact modeling;statistical distributions;statistical analysis cmos integrated circuits network analysis;statistical analysis;high k dielectric;complementary metal oxide semiconductor modeling process variability scaled cmos technology scaled cmos design device performance circuit performance statistical circuit analysis;metal gate;process variability;statistical compact modeling compact variability modeling design and test gate oxide thickness variability high k dielectric line edge roughness metal gate polysilicon granularity process variability random discrete dopants scaled cmos technology;compact variability modeling;gate oxide thickness variability;device performance;dielectrics;line edge roughness;design and test;modeling;scaled cmos design;cmos technology semiconductor device modeling cmos process very large scale integration circuit synthesis circuit optimization mosfet circuits design methodology doping power mosfet;random discrete dopants;compact model;circuit performance	Process variability has become a critical issue in scaled CMOS design. This article provides a comprehensive view on the predominant variation sources in sub-90-nm devices, their impact on device and circuit performance, and various modeling approaches for statistical circuit analysis.	cmos;heart rate variability;network analysis (electrical circuits);spatial variability	Samar K. Saha	2010	IEEE Design & Test of Computers	10.1109/MDT.2010.50	electronic engineering;computer science;electrical engineering;cmos;statistics;computer engineering	EDA	23.425771492701394	56.85027800888957	173348
d17d8eacf24de453c80a782e7daa3bb574d5eb76	a metric to target small-delay defects in industrial circuits	output deviations small delay defects industrial circuits timing related defects very deep submicron integrated circuits crosstalk process variations power supply noise resistive opens resistive shorts timing failures test grading technique;circuit faults;delay test;resistive opens;crosstalk;timing related defects;timing failures;screening;automatic test pattern generation;very deep submicron;defects;runtime;resistive shorts;integrated circuit design;process variations;output deviations;logic gates;power supply noise;delay test design and test atpg dft very deep submicron defects output deviations screening small delay defects;integrated circuit testing;industrial circuits;integrated circuit reliability;integrated circuit testing integrated circuit design integrated circuit reliability;automatic test pattern generator;design and test;logic gate;delay logic gates automatic test pattern generation circuit faults runtime;dft;small delay defects;very deep submicron integrated circuits;test grading technique;atpg	Timing-related defects are a major cause of test escapes and field returns for very deep-submicron (VDSM) integrated circuits. Small-delay variations induced by crosstalk, process variations, power supply noise, and resistive opens and shorts can cause timing failures in a design, leading to quality and reliability concerns. This article describes the authors' work with a previously proposed test-grading technique that uses output deviations for screening small-delay defects.	crosstalk;integrated circuit;power supply;very-large-scale integration	Mahmut Yilmaz;Mark Mohammad Tehranipoor;Krishnendu Chakrabarty	2011	IEEE Design & Test of Computers	10.1109/MDT.2011.26	electronic engineering;real-time computing;logic gate;engineering;electrical engineering;automatic test pattern generation	EDA	22.135016121650548	54.04206524517249	173859
f836e9cb6042e45e593a2d9c7bd39e8fb12625d2	impact of power supply noise on clock jitter in high-speed ddr memory interfaces	power supplies;digital signal processing;timing jitter clocks dram chips failure analysis integrated circuit design integrated circuit noise power supply circuits;impedance;power supply circuits;clocks;clocks jitter noise power supplies digital signal processing impedance switches;failure analysis;integrated circuit design;power delivery network clock jitter power supply noise;physical design high speed ddr memory interfaces power supply noise clock jitter random system failures ddr output clock;power supply noise;clock jitter;jitter;switches;integrated circuit noise;power delivery network;timing jitter;dram chips;noise	In this paper we analyze the impact of power supply noise on clock jitter in high-speed DDR memory interfaces. Random system failures on a custom IC were traced to excessive clock jitter on the DDR output clock, which when debugged were attributed to power supply noise caused at certain frequency bands (between ~30 and ~100MHz). We present methods that have been used at the architectural and system levels and in physical design to alleviate the effect of the supply noise on the DDR clock.	best, worst and average case;built-in test equipment;characteristic impedance;data rate units;debugging;frequency band;physical design (electronics);power supply;spectral density;stochastic process	Jim Monthie;Vineet Sreekumar;Ranjit Yashwante	2013	2013 26th International Conference on VLSI Design and 2013 12th International Conference on Embedded Systems	10.1109/VLSID.2013.198	embedded system;failure analysis;electronic engineering;real-time computing;jitter;telecommunications;clock domain crossing;clock skew;network switch;computer science;engineering;noise;electrical engineering;digital signal processing;electrical impedance;clock gating;digital clock manager;integrated circuit design;cpu multiplier	EDA	21.014624711968782	55.06771474427819	174090
5ff348e2b320ae07350e5fec5e5f6f6b5f01cbf0	simulation of system backend dielectric reliability	backend dielectric breakdown;reliability;time dependent dielectric breakdown	Backend dielectric breakdown degrades the reliability of circuits. A methodology to estimate chip lifetime due to backend dielectric breakdown is presented. It incorporates failures due to parallel tracks, the width effect, field enhancement due to line ends, and variation in activity and temperature. Different workloads are considered as well, in order to evaluate aging effects in microprocessors running realworld applications with realistic use conditions. & 2014 Elsevier Ltd. All rights reserved.	front and back ends;microprocessor;simulation	Chang-Chih Chen;Muhammad Bashir;Linda S. Milor;Daehyun Kim;Sung Kyu Lim	2014	Microelectronics Journal	10.1016/j.mejo.2014.01.008	electronic engineering;electrical engineering;reliability	EDA	21.351354329971436	59.725249502959905	174338
7cb3402533447cac9fbcc7b76c4a230c9fc9a321	buffering carbon nanotube interconnects considering inductive effects	inductive effects;carbon nanotube;timing constraint;buffer insertion;rlc model	While copper interconnect scaling is approaching its fundamental physical limit, increasing wire resistivity and delay have greatly limited the circuit miniaturization. The emerging carbon nanotube (CNT) interconnects, especially single-walled CNTs (SWCNTs) bundle interconnects, have become a promising replacement material. Nevertheless, physical design optimization techniques are still needed to allow them achieving the desired performances. While the preliminary conference version of this work [L. Liu, Y. Zhou and S. Hu, Proc. IEEE Computer Society Annual Symp. on VLSI (ISVLSI), 2014] designs the ̄rst timing driven bu®er insertion technique for SWCNT interconnects, it only considers resistive and capacitive e®ects but not inductive e®ects. Although inductance could be negligible for prevailing CNT-based circuit designs, it becomes important when designing ultra-high performance chips in the future. Thus, this paper considers bu®ering inductive bundled SWCNTs interconnects through developing a dynamic programming algorithm for bu®er insertion using the RLC tree delay model. Our experiments demonstrate that bundled SWCNTs interconnect-based bu®ering can e®ectively reduce the delay by over 3 when inductive e®ects are considered. With the same timing constraint, bundled SWCNTs interconnect-based bu®ering can save over 20% bu®er area compared to copper interconnect based bu®ering, while still running about 2 faster.	algorithm;copper interconnect;dynamic programming;electrical connection;elmore delay;experiment;holographic principle;image scaling;inductive reasoning;mathematical optimization;performance;physical design (electronics);rlc circuit;speaker wire;very-large-scale integration	Jia Wang;Lin Liu;Yuchen Zhou;Shiyan Hu	2016	Journal of Circuits, Systems, and Computers	10.1142/S0218126616500936	electronic engineering;carbon nanotube;electrical engineering;inductive effect	EDA	18.53293421431814	59.70884682874959	174421
6deb6c224f87ee2175b9d94fd05cc2a4d405d984	a novel mechanism for delay-insensitive data transfer based on current-mode multiple valued logic	32 bits processor;data transmission;mode courant;logica multivalente;current mode;circuito logico;delay insensitive;tecnologia mos complementario;procesador 32 bits;chip;logique multivalente;large scale;circuit logique;transmission donnee;processeur 32 bits;modo corriente;technologie mos complementaire;multivalued logic;logic circuit;transmision datos;data transfer;complementary mos technology;multiple valued logic	By conventional delay-insensitive data encodings, the num- ber of required wires for transferring N-bit data is 2N+1. To reduce the required number of wires to N+1, and thus, reducing complexity in designing a large scaled chip, a novel data transfer mechanism based on current-mode multiple valued logic is proposed. Effectiveness of pro- posed data transfer mechanism is validated by comparisons with conven- tional data transfer mechanisms using dual-rail and 1-of-4 encodings at the 0.25-µm CMOS technology. Simulation results of 32-bit data trans- fer with 10 mm wire length demonstrate that proposed data transfer mechanism reduces the time-power product values of dual-rail and 1-of- 4 encoding by 55.5% and 8.5%, respectively, in addition to the reduction of the number of wire by about half.		Myeong-Hoon Oh;Dong-Soo Har	2004		10.1007/978-3-540-30205-6_71	telecommunications;computer science;theoretical computer science;mathematics;algorithm;data transmission	EDA	18.101977224191245	55.52911418777364	174868
565a114818269ad0181a006ec64cd44412bc9a16	bp-1992 a comparison of defect models for fault location with iddq measurements	cmos integrated circuits;static cmos circuits;single standard cell inter gate shorts fault location quiescent current static cmos circuits fault detection diagnosis intra gate shorts fault models standard cell asic;inter gate shorts;circuit faults;fault models;short circuit currents;integrated circuit reliability cmos integrated circuits electric current measurement fault location integrated circuit testing application specific integrated circuits short circuit currents vlsi fault diagnosis logic testing failure analysis semiconductor device models;single standard cell;chip;electric current measurement;standard cell asic;failure analysis;current measurement;application specific integrated circuits;semiconductor device modeling;semiconductor device models;fault detection;voltage;logic testing;integrated circuit testing;vlsi;system testing;intra gate shorts;predictive models;circuit testing;fault location circuit testing circuit faults semiconductor device modeling predictive models fault diagnosis system testing voltage current measurement electrical fault detection;integrated circuit reliability;quiescent current;fault model;diagnosis;electrical fault detection;fault diagnosis;fault location	Iddq testing, where quiescent current is measured for a variety of states in static CMOS circuits, has emerged as a useful fault detection technique. In this paper it is shown that Iddq tests may be used for precise diagnosis of defects, using both inter and intra-gate shorts as fault models. The effects of these models are compared, using chips from a standard cell ASIC run. Of the 151 parts in the sample, diagnoses have been obtained for 135 of them. In many of these cases, the predicted defects are confined to a single standard cell.	application-specific integrated circuit;biasing;cmos;fault detection and isolation;fault model;iddq testing;software bug;standard cell	Robert C. Aitken	1993		10.1109/TEST.1993.470593	chip;embedded system;biasing;failure analysis;electronic engineering;semiconductor device modeling;real-time computing;voltage;computer science;engineering;electrical engineering;fault model;iddq testing;predictive modelling;application-specific integrated circuit;very-large-scale integration;cmos;system testing;fault detection and isolation	SE	22.579426608987156	53.854520972061486	174907
15a1e92906ba9469cbdb07c7f94307e7db83b36c	single-ended sub-threshold finfet 7t sram cell without boosted supply	finfets sram cells delays circuit stability stability analysis;circuit stability;wtp finfet hsnm sram sub threshold ultra low power ulp writability;finfets;sram cells;stability analysis;delays	The proposed novel FinFET 7T cell involves the breaking-up of feedback between the true storing nodes that enhances the writability of the cell at ultra-low voltage (ULV) power supply without boosted supply and write assist at 20nm technology node. Proposed 7T achieves improved hold static noise margin (HSNM) as compared to the conventional upsized 5T(U5T) cell. The write trip point (WTP) is 16.2% lower than the U5T WTP at 100mV. The read power consumption is reduced by 13.7% with similar write power consumption of U5T. The read decoupling and feedback cutting makes proposed 7T more immune to process variations in sub-threshold regime.	cell (microprocessor);coupling (computer programming);eclipse;noise margin;power supply;semiconductor device fabrication;single-ended signaling;static random-access memory;ultra-low-voltage processor	C. B. Kushwah;Santosh Kumar Vishvakarma;Devesh Dwivedi	2014	2014 IEEE International Conference on IC Design & Technology	10.1109/ICICDT.2014.6838593	embedded system;electronic engineering;real-time computing;engineering	Robotics	17.893447461780735	58.59559940052538	175303
2598db948ea0e528af535b7a5e38ce95c89df4f2	adaptive performance compensation with in-situ timing error prediction for subthreshold circuits	voltage control;cmos integrated circuits;in situ timing error prediction;guardbanding adaptive performance compensation in situ timing error prediction subthreshold circuits environmental variability canary flip flop timing errors kogge stone adder pvt variations;velocity control;environmental variables;energy efficient;canary flip flop;adaptive control;flip flops;kogge stone adder;pvt variations;timing errors;monitoring;timing circuits pulp manufacturing adders cmos process energy measurement adaptive control energy efficiency;adders;power dissipation;adaptive performance compensation;subthreshold circuits;guardbanding;flip flops adders cmos integrated circuits;environmental variability	This paper presents an adaptive technique for compensating manufacturing and environmental variability in subthreshold circuits using “canary Flip-Flop” that can predict timing errors. A 32-bit Kogge-Stone adder whose performance was controlled by body-biasing was fabricated in a 65 nm CMOS process. Measurement results show that the adaptive control can compensate PVT variations and improve energy-efficiency of subthreshold circuits significantly compared to worst-case design and operation with guardbanding.	32-bit;adder (electronics);best, worst and average case;biasing;cmos;flip-flop (electronics);kogge–stone adder;spatial variability	Hiroshi Fuketa;Masanori Hashimoto;Yukio Mitsuyama;Takao Onoye	2009	2009 IEEE Custom Integrated Circuits Conference	10.1109/CICC.2009.5280882	embedded system;electronic engineering;real-time computing;adaptive control;engineering;dissipation;efficient energy use;cmos;adder	EDA	19.223710666551852	58.92605091622073	175467
86ab499d0fa8037fa6617e2c1b1f16a4e36ec9d8	lookup table based discrete gate sizing for delay minimization with modified elmore delay model	delay minimization;gate model;elmore delay model;lookup table;discrete gate sizing	Gate sizing is one of the most important techniques for circuit optimization. Over the years, Elmore delay model (EDM) has been the predominant timing model used in gate sizing due to its simplicity. However, EDM is no longer effective in meeting the increasing demand of timing accuracy. In this paper, we propose a new gate delay model, which characterizes the timing information of lookup tables and creates a model which is mathematically similar to EDM, and can be easily incorporated into well-known EDM based gate sizing techniques using Lagrangian Relaxation (LR) with minor modifications. Experimental data show that it can produce even better results than those directly based on lookup tables, while keeping the benefit of the simplicity of EDM.	elmore delay;lr parser;lagrangian relaxation;lookup table;mathematical optimization;propagation delay;relaxation (approximation)	Jiani Xie;C. Y. Roger Chen	2015		10.1145/2742060.2742094	electronic engineering;real-time computing;delay calculation;lookup table;computer science;engineering;electrical engineering;elmore delay;algorithm	EDA	23.960368667483372	58.6592173863583	176557
f7fb5b42fe34c2bef0922a2dd3401098e09c7d74	a subthreshold symmetric sram cell with high read stability	sram chips cmos memory circuits integrated circuit design monte carlo methods network topology;voltage 27 mv subthreshold symmetric sram cell read stability differential eight transistor static random access memory cell symmetric topology area overhead read operation cell storage nodes read operation path cell data stability read operation delay postlayout monte carlo cmos technology read noise margin write noise margin conventional six transistor cell conventional 6t sram cell area size 45 nm voltage 92 mv voltage 74 mv voltage 0 35 v voltage 18 mv;network topology;integrated circuit design;transistors circuit stability sram cells leakage currents stability criteria;cmos memory circuits;subthreshold static random access memory sram data stability iso area analysis;monte carlo methods;sram chips	This brief introduces a differential eight-transistor static random access memory (SRAM) cell for subthreshold SRAM applications. The symmetric topology offers a smaller area overhead compared with other symmetric cells for the same stability in the read operation. Two transistors isolate the cell storage nodes from the read operation path to maintain the data stability of the cell. This topology improves the data stability at the expense of read operation delay. Thorough postlayout Monte Carlo worst corner simulations in 45-nm CMOS technology are conducted. The proposed cell operates down to 0.35 V with a read noise margin of 74 mV and a write noise margin of 92 mV. Under this condition, the read and write noise margins of the conventional six-transistor (6T) cell are 18 and 27 mV, respectively. The cell area is 1.57× the conventional 6T SRAM cell area in 45-nm design rules.	cmos;cell (microprocessor);monte carlo method;noise margin;overhead (computing);random access;simulation;static random-access memory;symmetric multiprocessing;transistor	Roghayeh Saeidi;Mohammad Sharifkhani;Khosrow Hajsadeghi	2014	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2013.2291064	electronic engineering;parallel computing;real-time computing;computer science;network topology;monte carlo method;integrated circuit design	EDA	17.84861214299459	59.42199752752814	176961
0f3a70180f1f11f544117cb955280590a66c8e3a	mom - a process variation aware statistical capacitance extractor	process variation;monte carlo sampling;sampling methods integrated circuit interconnections capacitance measurement integrated circuit modelling monte carlo methods;statistical model;expected value;integrated circuit modelling;integrated circuit interconnections;capacitance measurement;monte carlo sampling statistical capacitance extractor process variation interconnect capacitances mask monte carlo capacitance extractor statistical models;sampling methods;monte carlo;monte carlo methods;message oriented middleware computational geometry delay chemical technology parasitic capacitance character generation manufacturing processes scalability conductors sampling methods	With the advent of newer technologies, the underlying manufacturing processes are becoming more complicated. This results in substantial variations between mask and actual fabricated geometries of the conductors. These variations in geometries can lead to significant differences in the interconnect capacitances computed using mask and actual geometries. The sources for these variations could be many and the variations could be independent or correlated. We present a novel extension to the statistical Monte-Carlo capacitance extractor to take into account the statistical models of these variations. The variations are handled by an additional Monte-Carlo sampling; hence termed Monte-Carlo over Monte-Carlo (MoM). Our technique reports the expected value and the expected spread of the capacitances along with the co-variance among the different capacitances. We demonstrate the correctness, accuracy and scalability of our technique analytically and experimentally.	correctness (computer science);experiment;message-oriented middleware;metropolis–hastings algorithm;monte carlo method;randomness extractor;read-write memory;sampling (signal processing);scalability;statistical model	Rohit Ananthakrishna;Shabbir H. Batterywala	2006	19th International Conference on VLSI Design held jointly with 5th International Conference on Embedded Systems Design (VLSID'06)	10.1109/VLSID.2006.119	electronic engineering;simulation;computer science;electrical engineering;statistics;monte carlo method	EDA	22.932729009629057	57.97193359860506	177227
de7b47de71ace25a89204f0c777a20726d07385a	low-swing on-chip signaling techniques: effectiveness and robustness	circuito intercara;energy efficiency;quadratic energy savings;facteur efficacite;fiabilidad;reliability;factor de eficacia;interface circuit;interconnection;energy savings low swing on chip signaling techniques on chip interconnect schemes energy efficiency signal integrity reliability interconnect swing supply voltages quadratic energy savings benchmark interconnect circuit;clocks;energy efficient;circuit interface;metric;wires;indexing terms;test;pulga electronica;signal integrity;performance tradeoffs;interconnect swing;chip;ensayo;interconexion;on chip interconnect schemes;integrated circuit design;circuit simulation;essai;low voltage;low power;baja tension;energy consumption;integrated circuit interconnections;cmos logic circuits;fiabilite;robustesse;interconnexion;low power electronics;basse tension;puissance faible;energy savings;driver circuits;metrico;robustness;robustness integrated circuit interconnections wires energy consumption circuit testing benchmark testing energy efficiency circuit simulation driver circuits clocks;circuit testing;benchmark interconnect circuit;low power design;integrated circuit reliability;effectiveness factor;low swing on chip signaling techniques;logic cad;puce electronique;special low power 99;benchmark testing;metrique;energy saving;supply voltages;logic cad low power electronics integrated circuit interconnections integrated circuit reliability integrated circuit design cmos logic circuits;robustez;potencia debil;digital cmos	This paper reviews a number of low-swing on-chip interconnect schemes and presents a thorough analysis of their effectiveness and limitations, especially on energy efficiency and signal integrity. In addition, several new interface circuits presenting even more energy savings and better reliability are proposed. Some of these circuits not only reduce the interconnect swing, but also use very low supply voltages so as to obtain quadratic energy savings. The performance of each of the presented circuits is thoroughly examined using simulation on a benchmark interconnect circuit. Significant energy savings up to a factor of six have been observed.	benchmark (computing);robustness (computer science);signal integrity;simulation	Hui Zhang;George Varghese;Jan M. Rabaey	2000	IEEE Trans. VLSI Syst.	10.1109/92.845893	embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering;efficient energy use	EDA	18.410107765450796	54.92466411315653	177400
4374fc79b34c8e6a230308279a2b26e52cc369d4	in design dfm rule scoring and fixing method using icv		As compared to DRC rules, DFM rules are a list of selected recommended rules which aim to improve the design margins for better manufacturability. In GLOBALFOUNDRIES, we use DFM scoring methodology as an effective technique to analyze design quality in terms of manufacturability. Physical design engineers can perform our Manufacturability Check Deck (MCD) to asset their design quality during the sign-off stage. In the past, Synopsys users have to convert their design though milkyway database to GDSII format and execute the verification through the third party EDA tools. This method is costly and time-consuming for our Synopsys users. Today, we propose a new and easy-to-use integrated flow which leverages on the ICV engine to provide DFM scoring and in-design fixing techniques. The new methodology address DFM violations early in the design flow and achieve DFM compliance design during sign-off phase.	closing (morphology);design for manufacturability;electronic design automation;iteration;magnetic circular dichroism;physical design (electronics);requirement;tape-out	Vikas Tripathi;Yongfu Li;Zhao Chuan Lee;I-Lun Tseng;Jason Khaw;Jonathan Ong	2017	CoRR		computer science;theoretical computer science;physical design;computer architecture;design flow;design for manufacturability;electronic design automation	EDA	20.33945969017689	55.044093645759	177465
973133db04dee1926be6e15a0eaa4792c5db92fd	bottom-up digital system-level reliability modeling	negative bias temperature instability;bottom up;reaction diffusion;reliability modeling;model simplification;relative error;integrated circuit modelling;digital systems;degradation logic gates stress integrated circuit reliability delay;gate level models digital system level reliability modeling transistor level models;integrated circuit reliability;integrated circuit reliability integrated circuit modelling	We demonstrate here for the first time that it is possible by a bottom-up approach to build transistor- and gate-level models with enough accuracy to allow direct comparison with experimental degradations at system-level. This work opens new ways to optimize high level digital systems with respect to aging with great accuracy.	and gate;bottom-up parsing;digital electronics;high-level programming language;reliability engineering;top-down and bottom-up design;transistor	N. Ruiz Amador;V. Huard;E. Pion;Florian Cacho;Damien Croain;V. Robert;Sylvain Engels;Philippe Flatresse;Lorena Anghel	2011	2011 IEEE Custom Integrated Circuits Conference (CICC)	10.1109/CICC.2011.6055343	reliability engineering;negative-bias temperature instability;approximation error;electronic engineering;engineering;electrical engineering;circuit design;top-down and bottom-up design;circuit extraction;reaction–diffusion system	EDA	21.707412452987345	57.86746885234062	178312
44278f8f7291b48cfe88cff2f619342dd735f3dc	efficient determination of fault criticality for manufacturing test set optimization	analytical models;test quality;process variation;circuit faults;test set optimization;sequential circuits logic testing;test escape quality fault criticality test set optimization simulation elf md test escapes test quality;automatic test pattern generation;sequential circuits;simulation;flip flops;testing;test escape quality;sequential fault analysis;fault analysis;logic gates;circuit faults flip flops logic gates integrated circuit modeling analytical models automatic test pattern generation testing;sequential fault analysis fault criticality test set optimization combinational fault analysis;integrated circuit modeling;logic testing;combinational fault analysis;test escapes;efficient estimation;fault criticality;elf md	Defective part levels of zero are almost impossible to achieve in an era of complex defects, process variations, and limited testing resources. It is important to ensure that any defects missed during test will impact the end user as little as possible. However, optimizing test sets for superior detection of critical defects requires an understanding of relative fault criticality, and this determination may be very expensive. In this paper, we will present a method which efficiently estimates this criticality under realistic usage conditions using a translation between combinational and sequential fault analysis. We will show that the resulting criticalities are sufficiently accurate to select an optimized test set which is as effective as one created with perfect criticality information.	combinational logic;criticality matrix;self-organized criticality;test set	Yiwen Shi;Kellie DiPalma;Jennifer Dworak	2008	2008 IEEE International Symposium on Defect and Fault Tolerance of VLSI Systems	10.1109/DFT.2008.48	reliability engineering;electronic engineering;real-time computing;logic gate;engineering;automatic test pattern generation;sequential logic;software testing;process variation	Arch	22.380509374912872	54.675309263194926	178658
24782ba68aab84fbe92c40d8ee5223de20ae17f3	process aware ultra-high-speed hybrid sensing technique for low power near-threshold sram		Significant speed degradation is one of the severest issues encountered in low-voltage Static Random Access Memory (SRAM) operation. In addition, Sense Amplifier (SA) stability deterioration is another problem in low-voltage operation. These phenomena occur because the random transistor variation becomes larger as the process scaling progresses. In this work a highly robust and novel Ultra-High-Speed (UHS) hybrid current/voltage sensing technique is developed for low power and high speed SRAM. The Precisely sized Current Mode Circuit (CMC) is designed for local differential current mode sensing at bit-lines to achieve low power and high-speed. Local cross coupled inverters latch configuration is designed which convert differential voltage developed at data-lines to full logic swing at output. High speed SRAM sensing technique is designed using a 45nm CMOS standard process. With focus on the current sensing, we have shown that latch makes an excellent second-stage comparator after a local differential current sensing. Extensive post-layout simulation has been verified that our design operates down to 0.7V and achieves 95ps sensing delay at 1V supply voltage. Operating frequency is 1 GHz and power consumption is 2.18μW and 0.10μW at 1V and 0.7V respectively. The primary advantage of the proposed amplifier over previously reported sense amplifiers is the excellent immunity to inter-die and intra-die variations, making it more reliable against device mismatch and process variations.	static random-access memory	Bhupendra Singh Reniwal;Santosh Kumar Vishvakarma	2013		10.1007/978-3-642-42024-5_1	electronic engineering;parallel computing;real-time computing	HCI	19.60370641982527	59.87063114370815	178659
ec70e2024a8ccc895ddfc257bd5a51a5205f119f	reconfiguring cache associativity: adaptive cache design for wide-range reliable low-voltage operation using 7t/14t sram		This paper presents an adaptive cache architecture for wide-range reliable low-voltage operations. The proposed associativityreconfigurable cache consists of pairs of cache ways so that it can exploit the recovery feature of the novel 7T/14T SRAM cell. Each pair has two operating modes that can be selected based upon the required voltage level of current operating conditions: normal mode for high performance and dependable mode for reliable low-voltage operations. We can obtain reliable low-voltage operations by application of the dependable mode to weaker pairs that cannot operate reliably at low voltages. Meanwhile leaving stronger pairs in the normal mode, we can minimize performance losses. Our chip measurement results show that the proposed cache can trade off its associativity with the minimum operating voltage. Moreover, it can decrease the minimum operating voltage by 140 mV achieving 67.48% and 26.70% reduction of the power dissipation and energy per instruction. Processor simulation results show that designing the on-chip caches using the proposed scheme results in 2.95% maximum IPC losses, but it can be chosen various performance levels. Area estimation results show that the proposed cache adds area overhead of 1.61% and 5.49% in 32-KB and 256-KB caches, respectively. key words: low-voltage adaptive cache design, reconfiguring associativity, dynamic voltage frequency scaling, 7T/14T SRAM	cpu cache;cache (computing);cell (microprocessor);dependability;frequency scaling;normal mode;overhead (computing);simulation;static random-access memory	Jinwook Jung;Yohei Nakata;Shunsuke Okumura;Hiroshi Kawaguchi;Masahiko Yoshimoto	2013	IEICE Transactions		computer architecture;parallel computing;real-time computing;tag ram	Arch	18.261054378057445	59.812688380903566	178744
d163b121c0e3787d4b7ef4a19c8c8fc7a0e78b31	extending the viability of iddq testing in the deep submicron era	power supplies;silicon;cmos integrated circuits;circuit under test;sensing node;cmos technology;leakage current;transistor sub threshold leakage current;telecommunication computing;temperature dependence;fault location integrated circuit testing leakage currents cmos integrated circuits;current measurement;leakage currents;threshold voltage;i ddq testing viability extension;integrated circuit testing;circuit testing leakage current threshold voltage temperature dependence cmos technology telecommunication computing informatics power supplies current measurement silicon;informatics;circuit testing;defect detection technique;i ddq testing scheme;normal leakage current elimination;defect detection;sensing node i sub ddq testing viability extension defect detection technique cmos ics transistor sub threshold leakage current i sub ddq testing scheme normal leakage current elimination;cmos ics;fault location	IDDQ testing has become a widely accepted defect detection technique in CMOS ICs. However its effectiveness in deep submicron is threatened by the increased transistor sub-threshold leakage current. In this paper, a new IDDQ testing scheme is proposed. This scheme is based on the elimination, during IDDQ testing, of the normal leakage current from the sensing node of the circuit under test so that already known in the open literature IDDQ sensing techniques can be applied in deep submicron.	cmos;iddq testing;software bug;spectral leakage;transistor;very-large-scale integration	Yiorgos Tsiatouhas;Themistoklis Haniotakis;Dimitris Nikolos;Angela Arapoyanni	2002		10.1109/ISQED.2002.996706	embedded system;electronic engineering;computer science;engineering;electrical engineering;cmos	SE	22.579478251017242	54.39352029855766	179201
9c33f9de710d79332fea23655d092e21210e2141	static test compaction for low-power test sets by increasing the switching activity	circuit faults;clocks;very large scale integration;transition faults broadside tests low power test sets skewed load tests static test compaction;compaction;power dissipation;switches compaction circuit faults very large scale integration clocks power dissipation benchmark testing;switches;benchmark circuits static test compaction low power test sets switching activity signal transitions functional operation transition faults;benchmark testing;low power electronics integrated circuit testing	This brief describes a static test compaction procedure for low-power test sets, which is based on the observation that a higher switching activity leads to a smaller number of tests. To use this observation in the context of low-power test sets, this brief makes the following new observations. First, considering the number of tests in a given test set where a line g makes a 0 → 1 or a 1 → 0 transition, there are large variations in this number between different lines. Increasing the switching activity only for a subset G of lines that make the smallest numbers of signal transitions is sufficient for achieving test compaction. Second, the switching activity for the subset G can be increased in a way that the specific values that the lines assume can occur during functional operation, and the maximum switching activity for the test set does not increase. These observations allow the compacted test set to remain a low-power test set. Experimental results demonstrate significant reductions in the numbers of tests in low-power test sets for transition faults in benchmark circuits.	benchmark (computing);data compaction;low-power broadcasting;maximum cut;test set	Irith Pomeranz	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2345762	reliability engineering;compaction;benchmark;electronic engineering;real-time computing;network switch;computer science;engineering;dissipation;operating system;test compression;very-large-scale integration	EDA	19.497928251704455	53.46751203670148	179411
21ae0b65ca0d9fabe3542c846ef28e2963c0d695	monitor strategies for variability reduction considering correlation between power and timing variability	cmos integrated circuits;leakage;voltage scaling cmos digital design nanometer technologies leakage delay variability on chip sensing critical path body biasing;sensors;delay power demand correlation adders monte carlo methods sensors inverters;sensors cmos integrated circuits delays power consumption;cmos digital design;delay monitoring strategy variability reduction power variability timing variability cmos process variation voltage variation temperature variation power consumption electronic devices parametric yield control technique body biasing voltage scaling sensor static power dynamic power;body biasing;critical path;conference report;nanometer technologies;on chip sensing;power consumption;variability;voltage scaling;delays	As CMOS technology scales, Process, Voltage and Temperature (PVT) variations have an increasing impact on, performance and power consumption of the electronic devices. Variability causes an undesirable dispersion of performance parameters and a consequent reduction in parametric yield. Monitor and control techniques based on BB and VS can be used to reduce variability. This paper aims to determine which type of sensor provides a better overall variability reduction by taking into account the correlation between different performance magnitudes: static power, dynamic power and delay.	cmos;heart rate variability;spatial variability	Joan Mauricio;Francesc Moll;Josep Altet	2011	2011 IEEE International SOC Conference	10.1109/SOCC.2011.6085081	embedded system;electronic engineering;real-time computing;computer science;engineering;sensor;electrical engineering;critical path method;leakage;cmos	EDA	19.776818566619284	58.74651765195735	179839
21644770f80dca8dfd024b2f9bf5748fb26e557d	sleep transistor sizing for leakage power minimization considering temporal correlation	temporal correlation;microwave integrated circuits;cmos integrated circuits;design automation circuit modeling circuit optimization circuit reliability;design automation;leakage power minimization;decoupling capacitances sleep transistor sizing leakage power minimization temporal correlation power gating maximum instantaneous current ir drops distributed sleep transistor network design;ir drops;reduction;cmos circuits;power gating;sleep microwave integrated circuits cmos technology clustering algorithms minimization capacitance circuit optimization leakage current clocks algorithm design and analysis;circuit modeling;sleep;maximum instantaneous current;circuit reliability;leakage currents;transistors;sleep transistor sizing;clustering algorithms;decoupling capacitances;algorithm design and analysis;distributed sleep transistor network design;circuit optimization;partitioning algorithms;transistors leakage currents	Power gating is one of the most effective ways to reduce leakage power. In this paper, we introduce a new relationship among maximum instantaneous current, IR-drops and sleep transistor networks from a temporal viewpoint. Based on this relationship, we propose an algorithm to reduce the total sizes of sleep transistors in distributed sleep transistor network designs with the consideration of decoupling capacitances is taken. Our method achieves significantly better results than previous works on sleep transistor sizes.	algorithm;clock rate;coupling (computer programming);macromodel;power gating;sleep (system call);sleep mode;spectral leakage;transistor;transmission line	De-Shiuan Chiou;Yu-Ting Chen;Da-Cheng Juan;Shih-Chieh Chang	2010	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2010.2046812	control engineering;embedded system;algorithm design;electronic engineering;reduction;electronic design automation;computer science;engineering;electrical engineering;sleep;cluster analysis;cmos;transistor;algorithm	EDA	17.602013048967567	55.87692421180094	179904
61cbb47c3d651eac20062fc32b3621a66687d59f	throughput analysis and voltage-frequency island partitioning for streaming applications under process variation	system on chip integrated circuit design media streaming multiprocessing systems statistical distributions;integrated circuit design;statistical distributions;system on chip;media streaming;multiprocessing systems;throughput clocks vectors timing hardware probability distribution correlation;deterministic partitioning throughput analysis streaming applications process variation manufacturing process variability multiprocessor system on chip mpsoc statistical timing analysis system level design probability distribution voltage frequency island partitioning vfi partitions clock frequency levels within die variations die to die variations variation aware partitioning timing yield design cost circuit design margins	Variability in the manufacturing process results in variation in the maximum supported frequency of individual cores in a Multi-Processor System-on-Chip (MPSoC). This variation needs to be considered when performing statistical timing analysis in the system-level design. As our first contribution, we present a framework to estimate the probability distribution of application throughput (e.g. frames per second in video decoding) in a system with Voltage-Frequency Island (VFI) partitions in the presence of process variation. The novelty of the framework lies in the computation of the probability distribution of throughput, based on a user-specified set of clock-frequency levels per VFI domain considering both within-die and die-to-die variations of cores. As a second contribution, we provide a methodology to perform variation-aware partitioning of the cores of an MPSoC into VFIs for maximized timing yield (percentage of chips that satisfy a given throughput requirement). On a case study, we demonstrate how our methodology can be used by system designers for two purposes: 1) to make trade-offs between the number of VFI partitions (design cost) and timing yield; 2) to estimate the impact of reducing circuit design margins on the number of good dies on a wafer. We illustrate that the proposed variation-aware partitioning provides up to 18% improvements in the timing yield compared to a deterministic partitioning.	circuit design;computation;electronic system-level design and verification;heart rate variability;level design;mpsoc;multiprocessing;static timing analysis;throughput;video decoder	Davit Mirzoyan;Sander Stuijk;Benny Akesson;Kees G. W. Goossens	2013	The 11th IEEE Symposium on Embedded Systems for Real-time Multimedia	10.1109/ESTIMedia.2013.6704497	system on a chip;probability distribution;embedded system;parallel computing;real-time computing;computer science;statistics;integrated circuit design	Embedded	22.625479610497205	57.454266364402805	179990
d764ac633c4482e53df466c80ffc88ccdb3d3c6b	a 16 nm finfet heterogeneous nona-core soc supporting iso26262 asil b standard	standards;circuit faults;time factors;built in self test;monitoring;safety;hardware	It is getting mandatory to comply with ISO26262 in the recent automotive development. The implementation of safety mechanism to detect faults is one of the keys in ISO26262. The fault prediction will make the automotive system more reliable. The SoC in this paper introduces two features: hardware built-in self-test (BIST) for safety mechanism supporting automotive safety integrity level (ASIL) B and killer-droop monitor for fault prediction. In addition, time slicing is introduced to the testing with hardware BIST during runtime so that each test session can be shorter than the required interrupt response time in the application. The killer-droop monitor can predict the voltage droop and stop the clock supply for a certain period of time to mitigate the droop for preventing delay faults on the SoC. The monitor samples the voltage based on time-to-digital converter at CPU operation clock and predicts the voltage from the history of sampled voltage. With that prediction feature, the minimum operation voltage can be improved by 50 mV at 2.02 GHz CPU operation, and the maximum frequency can be improved by 140 MHz under 0.82 V power supply in 16 nm process.	built-in self-test;central processing unit;fail-safe;fault detection and isolation;mandatory integrity control;power supply;requirement;response time (technology);system on a chip;time slicing (digital broadcasting);time-to-digital converter	Shinichi Shibahara;Chikafumi Takahashi;Kazuki Fukuoka;Yuko Kitaji;Takahiro Irita;Hirotaka Hara;Yasuhisa Shimazaki;Jun Matsushima	2017	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2016.2623682	embedded system;electronic engineering;real-time computing;computer science;engineering;operating system	Mobile	20.983063986564076	54.662813104886766	180173
1c7210f81d2a54992e0542a04481d056649cb89f	relating integrated circuit yield and time-dependent reliability for various defect density distributions	multinomial distribution;yield burn in defect density distribution extrinsic failures;negative binomial;time dependent;integrated circuit yield;integrated circuit;defect density;failure analysis;negative binomial relationship integrated circuit yield time dependent reliability defect density distribution poisson relationship multinomial distribution optimal burn in time extrinsic failures;integrated circuit yield integrated circuit reliability manufacturing processes cost function integrated circuit manufacture virtual manufacturing continuous production mass production integrated circuit testing integrated circuit modeling;binomial distribution integrated circuit yield integrated circuit reliability failure analysis poisson distribution;binomial distribution;integrated circuit reliability;poisson distribution	The issue of developing a model to estimate field reliability from process yield has received a growing interest in recent years. Thus far, only Poisson and negative binomial relationships have been obtained, assuming that the number of yield defects is independent of the number of reliability defects in a device. In this paper, we derived explicit yield-reliability relationships for various defect density, such as Erlang, uniform, and triangle distributions, using a multinomial distribution to consider a correlation between the number of yield defects, and the number of reliability defects. The proposed model has advantages over previous models for any defect density distribution to determine the optimal burn-in time.	burn-in;erlang (programming language);integrated circuit;multinomial logistic regression;software bug	K. O. Kim	2006	IEEE Transactions on Reliability	10.1109/TR.2006.874930	reliability engineering;failure analysis;electronic engineering;engineering;binomial distribution;integrated circuit;mathematics;poisson distribution;negative binomial distribution;multinomial distribution;statistics	Embedded	23.43421570106508	55.39715142544247	180262
1194ca67b9515206773b79584096d2b5c552b6d4	extended-sakurai-newton mosfet model for ultra-deep-submicrometer cmos digital design	cmos integrated circuits;parameter extraction;circuit simulation;logic gates;semiconductor device modeling;cmos logic circuits;transistors;model matching;digital design;mathematical model;mosfet circuits semiconductor device modeling delay tin predictive models computational modeling cmos process low voltage cmos technology spice;mosfet;size 0 25 mum extended sakurai newton mosfet model ultra deep submicrometer cmos digital design power law model transistor drain current t spice simulations cmos processes elmore delay prediction cmos inverter centroid of current two input cmos nand gate digital cmos gates size 0 18 mum;power law;tin;mosfet circuit simulation cmos logic circuits logic gates	In this paper an extension of the Sakurai and Newton's Nth power law model, namely Extended-Sakurai-Newton Model, is proposed. The proposed model (henceforth referred to as the ESN model) preserves the simplicity and accuracy of the Sakurai Newton model for the estimation of drain current in deep submicron CMOS devices and extends it for varying device widths. Although the Modified Sakurai-Newton Current Model (MSN Model) also provides an estimation of transistor drain current with varying transistor widths, it suffers from the drawback of being more error prone and computation intensive in parameter extraction. The proposed model matches with BSIM3v3 level 49 T-SPICE simulations to within an error of 1.8%(3.67% maximum), in 0.18µm and 0.25µm CMOS processes for a wide range of transistor widths and input rise/fall times. The proposed model is further used to improve the Elmore Delay prediction of CMOS inverter operated at low supply voltages. The centroid-of-current and power based delay metrics [1] are modified based on the proposed model. The new delay metric is able to accurately predict the delay of CMOS inverter operated at low supply voltages. The proposed ESN Model is also applied to predict the delay of two-input CMOS NAND gate. Hence the proposed model can be effectively used in the design of digital CMOS gates involving varying device widths and supply voltages in the deep submicron region.	cmos;coefficient;cognitive dimensions of notations;computation;echo state network;elmore delay;newton;power inverter;ramp simulation software for modelling reliability, availability and maintainability;simulation;software propagation;transistor;very-large-scale integration	Nishant Chandra;Apoorva Kumar Yati;A. B. Bhattacharyya	2009	2009 22nd International Conference on VLSI Design	10.1109/VLSI.Design.2009.48	power law;electronic engineering;semiconductor device modeling;real-time computing;logic gate;tin;computer science;engineering;electrical engineering;fo4;mathematical model;cmos;transistor;statistics	EDA	24.355279721100057	59.08235898453876	180974
0a92abb9e1bf60814dfd1f2d84cb8a5147ab97af	low-power testing for low-power devices	low power devices;testing clocks power dissipation flip flops hardware delay book reviews;low power electronics flip flops integrated circuit testing;clocks;shift power;test power reduction;flip flops;testing;functional constraints;functional constraints low power testing low power devices electronic applications hardware software techniques functional power dissipation at speed scan testing flip flop functional clock pulse;low power device;test power;low power;functional clock pulse;power dissipation;low power electronics;integrated circuit testing;hardware software techniques;electronic applications;book reviews;power reduction;low power testing;functional power dissipation;low power device at speed scan testing test power shift power capture power test power reduction;flip flop;capture power;at speed scan testing;hardware	Low-power devices are indispensable for modern electronic applications, and numerous hardware/software techniques have been developed for drastically reducing functional power dissipation. However, the testing of such low-power devices has increasingly become a serious problem, especially in at-speed scan testing where a transition is launched at the output of a flip-flop and the corresponding circuit response is captured by a flip-flop with a functional clock pulse. The reason is that most or all of the functional constraints with respect to circuit operations and clocking are ignored in at-speed scan testing, which may make test power several times higher than functional power. Excessive test power may cause die/package damage due to excessive heat as well as undue yield loss due to excessive power supply noise, as illustrated in Figure 1. Therefore, it has become imperative to apply low-power testing to low-power devices. In other words, low-power devices cannot be successfully realized without effective and efficient low-power test solutions. This presentation first describes the basics of power dissipation in CMOS circuits. It goes on to highlight the difference between power dissipation in function mode and power dissipation in test mode, and lists the reasons why test power can be several times higher than functional power for low-power device. This presentation then describes the widely used clocking scheme used in at-speed scan testing, namely launch-on-capture (LOC), and shows the different characteristics of shift power and capture power in LOC-based at-speed scan testing. Based on that, a general low-power testing strategy is outlined, featuring the use of design-for-test (DFT) techniques for shift power reduction and the use of test data manipulation techniques for capture power reduction. This presentation then provides a comprehensive review of the state-of-the-art techniques for reducing test (shift or capture) power. Finally, future directions in the research and development of more advanced and sophisticated low-power testing are discussed.	altered level of consciousness;cmos;cpu power dissipation;clock rate;clock signal;design for testing;flops;flip-flop (electronics);imperative programming;low-power broadcasting;power semiconductor device;power supply;test data;turing test	Xiaoqing Wen	2010	2010 IEEE 25th International Symposium on Defect and Fault Tolerance in VLSI Systems	10.1109/DFT.2010.38	embedded system;electronic engineering;white-box testing;computer science;engineering;electrical engineering;dissipation;operating system;software testing;low-power electronics	EDA	20.111309471267013	54.92471930150184	181251
f364c02b9eca9d32364e8469535904b5832ef59e	design-time reliability evaluation for digital circuits		A new method of evaluating the reliability of combinational circuits is proposed, this method uses two levels of characterisation: a Stochastic Fault Model (SFM) of the component library and a design-specific Critical Vector Model (CVM). The idea is to move the high-complexity problem of stochastic characterisation of parameters into the generic part of the design process, and do it just once for a great number of the specific designs. The SFM captures variations of the vector of parameters of a library component fault model, those causing a transient fault at the component output; it is meant to be supplied by the foundry, similar to timing library files. The CVM is derived by a limited number of simulation runs on the specific design, and represents the boundary between the erroneous and error-free operation, w.r.t. the vector of parameters of each component. The probability of error-free operation is subsequently calculated by jointly using SFM and CVM. The method is demonstrated on a chain of inverters for simplicity, and subsequently applied to a 3-bit full adder. A complex three-way trade-off between energy, performance and reliability is explored. The method is meant to serve as a basis for design-time reliability evaluation and runtime power-reliability management. A slow stage is added to the circuit under test to improve its reliability.	adder (electronics);combinational logic;component-based software engineering;digital electronics;electronic design automation;experiment;fault model;frequency scaling;image scaling;interference (communication);inverter (logic gate);logic synthesis;monte carlo method;operating point;power management;reliability engineering;simulation;statistical relational learning;timing closure	Mohamed A. Abufalgha;Alex Bystrov	2017	2017 IEEE 23rd International Symposium on On-Line Testing and Robust System Design (IOLTS)	10.1109/IOLTS.2017.8046196	electronic engineering;real-time computing;digital electronics;engineering design process;fault model;computer science;logic gate;adder;combinational logic	EDA	21.133472283598852	57.98293976867124	181765
03ec5e34f8ec8b2df4f700f1d768483a2c7c5063	future performance challenges in nanometer design	feedback mechanism;integrated circuit;integrated circuit design nanotechnology integrated circuit packaging integrated circuit reliability low power electronics;nanotechnology;chip;integrated circuit design;low power electronics;itrs packaging predictions nanometer design feature sizes dynamic power scaling trends packaging problems thermal monitoring feedback mechanisms worst case dissipation ultrasmall mosfets global signaling strategies low swing drivers cross chip communication power delivery;temperature integrated circuit packaging energy consumption predictive models permission voltage thermal resistance cooling heat sinks leakage current;integrated circuit reliability;integrated circuit packaging;high performance	We highlight several fundamental challenges to designing high-performance integrated circuits in nanometer-scale technologies (i.e. draRita Glover, EDA Today, L.C.wn feature sizes< 100 nm). Dynamic power scaling trends lead to major packaging problems. To alleviate these concerns, tMarc Halpernhermal monitoring and feedback mechanisms can limit worst-case dissipation and reduce costs. Furthermore, a flexible multi-Vdd + multi-Vth + re-sizing approach is advocated to leverage the inherent properties of ultra-small MOSFETs and limit both dynamic and static power. Alternative global signaling strategies such as differential and low-swing drivers are recommended in order to curb the power requirements of cross-chip communication. Finally, potential power delivery challenges are addressed with respect to ITRS packaging predictions.	best, worst and average case;image scaling;integrated circuit;requirement	Dennis Sylvester;Himanshu Kaul	2001		10.1145/378239.378245	chip;embedded system;electronic engineering;integrated circuit packaging;computer science;engineering;electrical engineering;integrated circuit;circuit design;feedback;low-power electronics;integrated circuit design	Arch	19.99192169370859	58.42675534636241	182024
2fb0c0036d5ea25c449906fe4f2ccf87b0abdfd2	parameterized transient thermal behavioral modeling for chip multiprocessors	optimisation;thermal related design;thermal model;architecture-level parameterized transient thermal behavioral modeling algorithm;parameterized thermal performance model;heat sink;quad-core microprocessor;generalized pencil-of-function;thermal sensor;heat sink material;multiprocessing systems;polynomial time-varying coefficient;transient thermal behavioral modeling;optimization;heat sinks;heat spread;parthermpof;chip multiprocessors;high-performance chip-multiprocessor;thermal conductivity;new method;thermal sensors;optimization problem;behavior modeling;transfer function;temperature measurement;parameter space;copper;response surface methodology;computational modeling	In this paper, we propose a new architecture-level parameterized transient thermal behavioral modeling algorithm for emerging thermal related design and optimization problems for high-performance chip-multiprocessor (CMP) design. We propose a new approach, called ParThermPOF, to build the parameterized thermal performance models from the given architecture thermal and power information. The new method can include a number of parameters such as the locations of thermal sensors in a heat sink, different components (heat sink, heat spread, core, cache, etc.), thermal conductivity of heat sink materials, etc. The method consists of two steps: first, response surface method based on low-order polynomials is applied to build the parameterized models at each time point for all the given sampling nodes in the parameter space. Second, an improved generalized pencil-of-function (GPOF) method is employed to build the transfer-function based behavioral models for each time-varying coefficient of the polynomials generated in the first step. Experimental results on a practical quad-core microprocessor show that the generated parameterized thermal model matchs the given data very well. ParThermPOF is very suitable for design space exploration and optimization where both time and system parameters need to be considered.	algorithm;behavioral modeling;cpu cache;coefficient;design space exploration;heat sink;mathematical optimization;microprocessor;multi-core processor;multiprocessing;polynomial;response surface methodology;sampling (signal processing);sensor;transfer function	Duo Li;Sheldon X.-D. Tan;Eduardo H. Pacheco;Murli Tirumala	2008	2008 IEEE/ACM International Conference on Computer-Aided Design	10.1145/1509456.1509592	embedded system;electronic engineering;real-time computing;computer science;heat sink	EDA	24.151129858967614	59.25810589279727	182042
1719eb32f4219a1c77138a20dfdd26264177c2ca	using lcss algorithm for circuit level verification of analog designs	probability;delay analytical models analog circuits ring oscillators algorithm design and analysis testing;oscillators;fabrication process lcss algorithm analog design circuit level verification longest closest subsequence longest common subsequence lcs process variation parametric techniques statistical techniques closest matching probability circuit acceptance rejection bounded hypothesis testing rambus ring oscillator circuit;integrated circuit design;analogue integrated circuits;statistical analysis;statistical analysis analogue integrated circuits integrated circuit design oscillators probability	This paper relies on the longest closest subsequence (LCSS), a variant of the longest common subsequence (LCS), to account for process variation and mismatch in analog circuits. At circuit level, the effect of mismatch and process variation that results in offsets is analyzed by performing parametric and statistical techniques and then applying LCSS to estimate the probability of closest matching. The acceptance/rejection of a circuit is done using bounded hypothesis testing. The approach is illustrated on a Rambus ring oscillator circuit for a 90nm fabrication process. Advantages of the proposed methods are robustness and flexibility to account for a wide range of variations.	algorithm;analogue electronics;black box;frequency offset;image scaling;longest common subsequence problem;pattern matching;rejection sampling;ring oscillator;semiconductor device fabrication;simulation;test set;thread (computing);time complexity;tracing (software);verification and validation	Rajeev Narayanan;Alaeddine Daghar;Mohamed H. Zaki;Sofiène Tahar	2012	10th IEEE International NEWCAS Conference	10.1109/NEWCAS.2012.6328987	electronic engineering;theoretical computer science;mathematics;algorithm	EDA	23.05571204764856	57.73762546110124	182263
5c2daa4395d74ac2fef19d307b22b9c43d2ef274	fast and accurate parasitic capacitance models for layout-aware synthesis of analog circuits	look up table;parasitic estimation;design process;linear interpolation;layout aware;analog synthesis	Considering layout effects early in the analog design process is becoming increasingly important. We propose techniques for estimating parasitic capacitances based on look-up tables and multi-variate linear interpolation. These models enable fast and accurate estimation of parasitic capacitances and are very suitable for use in a synthesis flow. A layout aware methodology for synthesis of analog CMOS circuits using these parasitic models is presented. Results indicate that the proposed synthesis system is fast as compared to a layout-inclusive synthesis approach.	analogue electronics;cmos;linear interpolation;lookup table;parasitic element (electrical networks)	Anuradha Agarwal;Hemanth Sampath;Veena Yelamanchili;Ranga Vemuri	2004	Proceedings. 41st Design Automation Conference, 2004.	10.1145/996566.996610	control engineering;electronic engineering;design process;lookup table;computer science;theoretical computer science;parasitic extraction;linear interpolation;algorithm	EDA	17.336037854685596	54.36366513736117	182586
7c14622133154f729f06d2b09d846b7f3c12a9c8	fast and accurate statistical static timing analysis	vlsi integrated logic circuits nanoelectronics statistical analysis;statistical static timing analysis overestimate critical path delays true critical paths false path analysis critical path traversal fast ssta method pessimistic estimation digital vlsi nanotechnology;delays logic gates algorithm design and analysis design automation random variables central processing unit;false path analysis statistical static timing analysis critical path	The impact of process variation has been more prominent in nano-technology, and it poses great challenge to timing analysis for digital VLSI. Traditionally, this problem is solved by using statistical static timing analysis (SSTA). However, static timing analysis may lead to an overly pessimistic estimation, as many critical paths are not true paths. In this paper, we present a fast SSTA method, in which critical path traversal is combined with false path analysis so that true critical paths can be quickly identified. Experimental results show that a significant portion of the longest paths are actually false, which implies SSTA without false path analysis usually overestimate critical path delays.	critical path method;directory traversal attack;gnu nano;longest path problem;path analysis (statistics);statistical static timing analysis;tree traversal;very-large-scale integration	Sying-Jyan Wang;Tsung-Huei Tzeng;Katherine Shu-Min Li	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865694	parallel computing;real-time computing;computer science;theoretical computer science;static timing analysis	EDA	22.812718047629712	57.45394996573048	182598
c0b09f8ad15030b0df390df94e7291d5b985e75b	a data aware 9t static random access memory cell for low power consumption and improved stability	hold mode;t technology general;leakage current;low power;write margin;read stability;ta engineering general civil engineering general	Reducing the power consumption in static random access memory can significantly improve the system power efficiency, reliability, and performance. In this paper, we propose a data aware static random access memory cell to reduce the power consumption during read and write operation. The proposed cell contains nine transistors with separate read/write ports. The write operation in the proposed cell is controlled by an additional write signal instead of word line. Because of isolation of the storage nodes with bit lines, read signal-to-noise margin is equal to ideal hold signal-to-noise margin of the conventional cell. The proposed cell saves approximately more than 43% active power compared with the 6T cell and other published cells. The proposed cell gives faster write access and low leakage current compared with the conventional and other cells. About 99% standby column power reduction, with 128 cells, is observed in the proposed cell. Copyright © 2012 John Wiley & Sons, Ltd. Received 1 March 2012; Revised 27 August 2012; Accepted 7 December 2012	access time;file system permissions;john d. wiley;memory cell (binary);noise margin;performance per watt;random access;read-write memory;signal-to-noise ratio;spectral leakage;static random-access memory;transistor	Ajay Kumar Singh;Mah Meng Seong;C. M. R. Prabhu	2014	I. J. Circuit Theory and Applications	10.1002/cta.1897	electronic engineering;real-time computing;telecommunications;computer science;engineering;electrical engineering;leakage	EDA	17.346063419729226	59.23269328533657	182815
1b56ea485c9ceed3cb53d6b55752ca85131b4ddd	test structure for ic(vbe) parameter determination of low voltage applications	low voltage applications;parameter determination;accurate measurement;theclassical method;best fitting;temperature dependence;new method;operating temperature;accurate value;vbe voltageand;test structure;temperature dependence ofic;direct measurement ofdie temperature;drams;design methodology;photonic band gap;stability;telephony;low power electronics;low voltage	"""The temperature dependence of the IC(VBE) relationshipcan be characterised by two parameters: EG and XTI. Theclassical method to extract these parameters consists in a""""best fitting"""" from measured VBE(T) values, using leastsquare algorithm at constant collector current. Thismethod involves an accurate measurement of VBE voltageand an accurate value of the operating temperature. Wepropose in this paper, a configurable test structurededicated to the extraction of temperature dependence ofIC(VBE) characteristic for BJT designed with bipolar orBiCMOS processes. This allows a direct measurement ofdie temperature and consequently an accuratemeasurement of VBE(T). First, the classical extractionmethod is explained. Then, the implementation techniquesof the new method are discussed, the improvement of thedesign is presented."""	algorithm;eurographics;vesa bios extensions;x/open transport interface	Wenceslas Rahajandraibe;Christian Dufaza;Daniel Auvergne;B. Cialdella;B. Majoux;V. Chowdhury	2002				EDA	24.59543708307576	56.9745774711614	183309
93903018a543601bc497bb4b805e9df2a870353c	guest editors' introduction: defect-oriented testing in the deep-submicron era	circuit noise;crosstalk;noise generators;coupling circuits;noise level;integrated circuit interconnections;voltage;circuit testing;signal to noise ratio;integrated circuit noise;circuit testing integrated circuit interconnections crosstalk noise generators integrated circuit noise circuit noise coupling circuits noise level signal to noise ratio voltage	0740-7475/02/$17.00 © 2002 IEEE September–October 2002 CMOS IC SCALING increases device/interconnect density to allow more logic on a die at higher clock rates, enhancing overall performance. Improvements in process technology enable integration on a single die of circuits with different functions that require distinct manufacturing process steps. With added constraints of reduced time to market, the complexity of CMOS ICs has grown steadily during the past few decades. Shortened development times require shorter design, verification, and manufacturing cycles, as well as more efficient and accurate test and debugging methods. Despite impressive trends in the microelectronic industry, deep-submicron technologies—especially below 0.18 micron—face several important test-technology challenges. This special issue presents five articles that address some of these challenges. Defect-oriented testing emerged at the beginning of the 1990s as a solution to the limitations of traditional logic fault models. The lack of solutions offered by logic fault models was especially severe for high-reliability applications. Defect-oriented test strategies first analyze defect properties, and then determine the best test technique for detection. This IC test approach led to parametric test methods focusing on IC parameters other than the logic circuit behavior. Such strategies complemented logicbased tests and enhanced product quality, often serving as additional reliability screens. Deep-submicron devices brought new physical phenomena that pushed the limits of manufacturing capabilities. The miniaturization laws in this domain generated a new scaling approach—constant field scaling—as an alternative to constant voltage scaling. Constant field scaling dictates lower supply voltages for successive technology nodes to prevent the electric field across the MOSFET transistor gate oxide from surpassing reliability limits. Lowering the supply voltage affects circuit performance directly. Transistor threshold voltage reduction compensates for this performance change by maintaining acceptable gate overdrive voltages, and therefore adequate saturation currents. The threshold voltage reduction, in turn, affects transistor off-state current (also called leakage current), which depends exponentially on this parameter. Rising off-state current implies higher quiescent supply current at the circuit level, impacting the effectiveness of traditional single-threshold IDDQ testing—one of the most effective parametric test techniques used in defect-oriented testing. Several approaches extend IDDQ-based methods to deep-submicron technologies. These techniques correlate different IDDQ measurements from the same die or several dies to enhance IDDQ sensitivity. Some of these methods perform a pre-analysis on a few dies, whereas others require a post-test statistical analysis. The first article in this special issue, by Sabade and Walker, reviews these techniques and analyzes their merits and limitations, as well as their projections for future technologies. Traditional defect-oriented test methods have mainly targeted detection of bridges because this defect mechanism was considered the most probable in previous CMOS generations. Although researchers have made significant advances in the characterization and Guest Editors’ Introduction: Defect-Oriented Testing in the Deep-Submicron Era	amiga walker;cmos;constant-voltage speaker system;debugging;die (integrated circuit);dynamic voltage scaling;fault model;gate oxide;iddq testing;image scaling;logic gate;pentium overdrive;software bug;spectral leakage;transistor;very-large-scale integration	Jaume Segura;Peter C. Maxwell	2002	IEEE Design & Test of Computers	10.1109/MDT.2002.1033786	equivalent circuit;mixed-signal integrated circuit;embedded system;effective input noise temperature;substrate coupling;electronic engineering;voltage;crosstalk;telecommunications;computer science;signal integrity;engineering;electrical engineering;low-noise amplifier;circuit extraction;signal-to-noise ratio	EDA	22.45598403330333	54.651564856840274	183474
5e1039cb71f3a4ed7b18e1b126fbcb70f2c198e4	on-chip i–v variability and random telegraph noise characterization in 28 nm cmos	microprocessors;semiconductor device measurement;computer architecture;current measurement;logic gates;temperature measurement;voltage measurement	Building reliable mixed-signal circuits in advanced process technologies requires an accurate understanding of device performance and variability. This work presents an on-chip transistor characterization platform built on a digital focal plane array readout circuit framework that enables highly parallel device measurements to be taken in the digital domain. This technique is used to quickly assess large-scale transistor characteristics and study the impact of random telegraph noise (RTN) in deeply scaled technologies. A 28 nm HKMG bulk LP CMOS test chip containing over 80,000 NFETs and PFETs of multiple sizes and threshold voltages was fabricated and tested to study device parameters and RTN performance down to cryogenic temperatures. Results support previous studies of RTN temperature dependence and suggest that threshold voltage has minimal impact on RTN relative to device type and dimension.	burst noise;cmos;die shrink;focal (programming language);high-κ dielectric;mixed-signal integrated circuit;spatial variability;staring array;transistor	Amy Whitcombe;Scott Taylor;Martin Denham;Vladimir Milovanovic;Borivoje Nikolic	2016	2016 46th European Solid-State Device Research Conference (ESSDERC)	10.1109/ESSDERC.2016.7599632	electronic engineering;real-time computing;engineering;electrical engineering	EDA	22.56912231690477	56.162320763438	183490
4e09d6246141d51548bff77a8f85b61878eef8ee	two new techniques for identifying opens on printed circuit boards: analog junction test & radio frequency induction test	cycle time;opens;pins;printed circuits;bed of nails fixture;device under test;circuit faults;functional testing;fixtures;induced ac signal;analog junction test;parasitic diode;radio frequency induction test;electric resistance measurement;electric current measurement;assembly;protection;teradyne;radio frequency;production testing printed circuit testing;bed of nails fixture opens printed circuit boards analog junction test radio frequency induction test teradyne pin pairs parasitic diode device under test overhead inducer rf induction induced ac signal;diodes;circuit testing printed circuits radio frequency diodes pins fixtures assembly voltage electrical resistance measurement force measurement;fault coverage;printed circuit boards;circuit testing;printed circuit testing;printed circuits circuit testing circuit faults radio frequency assembly costs protection diodes pins fixtures;printed circuit board;economics;production testing;overhead inducer;electric current measurement printed circuit testing economics fault location fault diagnosis electric resistance measurement;fault diagnosis;capacitive coupling opens printed circuit boards junction test radio frequency induction test analog test vectorless test teradyne pc test test cost cycle time fault coverage;rf induction;pin pairs;fault location	The author describes the principles, implementation, strengths, weaknesses and applications of two vectorless test techniques developed by Teradyne. Examples drawn from users' experience demonstrate how unpowered testing can dramatically reduce test cost and cycle time while maintaining or improving fault coverage. Analog junction test (AJT) and RF induction test (RFIT) form part of Teradyne's vectorless test toolset. AJT uses simple analog characterization of pin pairs on the device, relying on the protection or parasitic diode in most devices to produce the signal that indicates correct board assembly. RFIT uses RF induction into the device-under-test from an overhead inducer. Detection of the induced AC signal on the device pins via the standard bed-of-nails fixture indicates good board assembly. Both AJT and RFIT offer entirely new capability to the test engineer in that they can detect resistive cold solder joints. These faults would otherwise pass a traditional vector test or even a functional test, while risking field failures.	printed circuit board;radio frequency	Joe Wrinn	1995		10.1109/TEST.1995.529936	embedded system;electronic engineering;computer science;engineering;electrical engineering;printed circuit board	EDA	23.4159336899087	53.71969490390353	183524
adf3896ada6cac358e4fd014d6995a4853c9631d	high-reliability, low-energy microarchitecture synthesis	concepcion asistida;computer aided design;fiabilidad;reliability;basse energie;microarchitecture integrated circuit synthesis integrated circuit interconnections acceleration power dissipation integrated circuit reliability degradation current density electromigration delay;microarchitecture;interconnection;synthese evoluee;densidad corriente;integrated circuit;low energy;dissipation energie;data flow graphs;baja energia;data transfer equipment;rt level microarchitecture synthesis power dissipation integrated circuit electromigration reliability submicrometer scaling control data flow graph bus failure energy optimization interconnect metal line algorithm;equipement transfert donnee;high level synthesis integrated circuit design integrated circuit reliability electromigration integrated circuit interconnections system buses failure analysis low power electronics data flow graphs;circuito integrado;energy dissipation;indexing terms;pulga electronica;chip;system buses;interconexion;failure analysis;high level synthesis;integrated circuit design;low power;densite courant;electrodiffusion;control data flow graph;equipo transferencia datos;integrated circuit interconnections;fiabilite;power dissipation;electrodifusion;interconnexion;low power electronics;conception assistee;disipacion energia;electromigration;integrated circuit reliability;puce electronique;circuit integre;data transfer;current density	Continuous scaling of device dimensions has accelerated the power dissipation and electromigration-induced reliability degradation in integrated circuits. Submicrometer scaling increases the fraction of on-chip energy dissipated on long interconnects and buses. In addition, submicrometer-level scaling increases current density in long interconnects and buses, causing structural damage in metal lines due to electromigration (a major failure phenomenon in integrated circuits). We present algorithms for synthesizing high-reliability, low-energy microarchitectures. This can be realized by judiciously binding and scheduling the data transfers of a control-data-flow graph representation of an application onto the buses in the microarchitecture. The algorithm considers (i) correlations between data transfers, (ii) constraints on the number of buses, and (iii) area and delay.	microarchitecture	Aurobindo Dasgupta;Ramesh Karri	1998	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.736567	embedded system;electronic engineering;computer science;engineering;electrical engineering;dissipation;computer aided design	EDA	18.579058146219598	55.34780837555511	183772
a4163746080484b41b95468b79ff05213436271f	a delay test architecture for tsv with resistive open defects in 3-d stacked memories	voltage divider 3 d stacked memories resistive open defects through silicon via tsv;voltage dividers delay circuits integrated circuit testing integrated memory circuits test equipment three dimensional integrated circuits;through silicon vias delays resistance testing clocks mosfet capacitance;voltage divider delay test architecture tsv testing resistive open defects 3d stacked memories through silicon via test circuit low frequency test equipment	The limits of technology scaling for smaller chip size, higher performance, and lower power consumption are being reached. For this reason, the memory semiconductor industry is searching for new technology. 3-D stacked memory using through-silicon via (TSV) has been considered as a promising solution for overcoming this challenge. However, to guarantee quality and yield for mass production of 3-D stacked memories, effective test techniques for TSV are required. In this paper, a new test architecture for testing TSVs in 3-D stacked memories is proposed. By comparing voltage changes generated due to resistive open defects with a reference voltage applied externally, the test circuit estimates delay across the TSV. This allows the possibility of a delay test with low-frequency test equipment. Experimental results demonstrate that the proposed test architecture can be effective in the testing of TSV with resistive open defects, and have lower area overhead and lower peak current consumption.	built-in test equipment;comparator;debugging;image scaling;overhead (computing);semiconductor industry;through-silicon via;turing test	Hyungsu Sung;Keewon Cho;Kunsang Yoon;Sungho Kang	2014	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2013.2289964	embedded system;electronic engineering;engineering;electrical engineering	EDA	20.086132837851924	56.23153479579586	184157
c93797251c22033133f407138a3463d16f86cc42	aging-leakage tradeoffs using multi-vth cell library	thermal variables control;negative bias temperature instability;multiple threshold voltage;nbti;aging;design optimization;leakage power;logic gates;threshold voltage;optimization;delays	Negative Bias Temperature Instability (NBTI) induced transistor aging has become one of the major reliability concerns in sub-micron circuit as technology scales. Transistor performance degrades over time and may eventually cause timing violations and circuit failure. Multi-Vth is known as a method to improve circuit performance with leakage and timing tradeoffs. In this paper, we propose a heuristic metric, which could be used to capture leakage and timing tradeoff during multi-Vth optimization. Our technique is an after-aging delay minimization problem under leakage power constraint. The experimental results on ISCAS85 benchmark circuits at 45nm node show up to 17% after-aging delay improvement within the predefined leakage power constraint.	algorithm;benchmark (computing);critical path method;heuristic (computer science);mathematical optimization;negative-bias temperature instability;spectral leakage;time complexity;transistor	Hao Luo;Mehrdad Heydarzadeh;Mehrdad Nourani	2016	2016 IEEE 25th Asian Test Symposium (ATS)	10.1109/ATS.2016.13	negative-bias temperature instability;electronic engineering;real-time computing;engineering;electrical engineering	EDA	19.874247548890573	57.765635811777116	184315
660da45f0afd945617d17809b5414374eb87fe12	dicer: distributed and cost-effective redundancy for variation tolerance	monte carlo methods;vlsi;delay estimation;fault tolerance;integrated circuit testing;redundancy;dicer;monte carlo simulation;vlsi technology;cost-effective redundancy;delay variability;distributed redundancy;dual-driver nets;fault tolerance technique;gate splitting method;spin-off gate placement heuristic;variation tolerance;variational effects	Increasingly prominent variational effects impose imminent threat to the progress of VLSI technology. This work explores redundancy, which is a well-known fault tolerance technique, for variation tolerance. It is observed that delay variability can be reduced by making redundant paths distributed or less correlated. Based on this observation, a gate splitting methodology is proposed for achieving distributed redundancy. We show how to avoid short circuit and estimate delay in dual-driver nets which are caused by gate splitting. A spin-off gate placement heuristic is developed to minimize redundancy cost. Monte Carlo simulation results on benchmark circuits show that our method can improve timing yield from 59% to 72% with only 03% increase on cell area and 2.2% increase on wirelength on average.	benchmark (computing);delay calculation;fault tolerance;heuristic;monte carlo method;redundancy (engineering);short circuit;simulation;spatial variability;variational principle;very-large-scale integration	Di Wu;Ganesh Venkataraman;Jiang Hu;Quiyang Li;Rabi N. Mahapatra	2005	ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.		triple modular redundancy;fault tolerance;electronic engineering;parallel computing;real-time computing;delay calculation;computer science;statistics;monte carlo method	EDA	20.246416263261526	57.51924183802807	184398
8ba77353bf2473ed26d066ce2cb73591b14ce773	iddq current dependency on test vectors and bridging resistance	automatic test pattern generation cmos digital integrated circuits integrated circuit testing logic testing vectors fault simulation spice electric current measurement;fault simulation;automatic test pattern generation;electric current measurement;pspice simulation cmos circuits i sub ddq current dependence test vectors bridging resistance bridging faults faulty nodes small channel resistance fault simulation external bridging nodes internal bridging nodes;vectors;bridging fault;cmos digital integrated circuits;fault detection;logic testing;integrated circuit testing;circuit faults circuit testing logic testing current supplies power supplies circuit simulation current measurement system testing steady state power measurement;iddq testing;spice	In this paper, we focus on how the I DDQ current value varies depending on the test vectors and the resistance of bridging faults. Detection of a large I DDQ is easier than a small I DDQ, because the value of I DDQ is normally of the order of microamperes. If suitable logic values are assigned to the faulty nodes, a small channel resistance will be obtained and then a large I DDQ will flow. We have performed the simulation to show that the value of I DDQ depends on the test vectors that applied to the faulty nodes for external and internal bridging nodes. As a result, I DDQ at the low resistance-bridging fault increased by 26%-36% for external bridging fault between different gates. For the internal bridging fault between any gates, the increased I DDQ is 27%-159% for all resistive bridging faults.	brainfuck;bridging (networking);bridging fault;downstream (software development);iddq testing;simulation;test vector;transistor	Arabi Keshk;Kozo Kinoshita;Yukiya Miura	1999		10.1109/ATS.1999.810745	embedded system;electronic engineering;engineering;automatic test pattern generation;iddq testing;fault detection and isolation;computer engineering	EDA	22.730634305283953	53.97113221892481	184481
f001d49ae3a7266f6a207add66a72105f5655eba	inductor optimization for active cell balancing using geometric programming	scalability;computer architecture;mathematical model;inductors;personalized medicine;energy dissipation;geometric programming;charge transfer;cyber physical systems;pulse width modulation;optimization	This paper proposes an optimization methodology for inductor components in active cell balancing architectures of electric vehicle battery packs. For this purpose, we introduce a new mathematical model to quantitatively describe the charge transfer of a family of inductor-based circuits. Utilizing worst case assumptions, this model yields a nonlinear program for designing the inductor and selecting the transfer current. In the next step, we transform this problem into a geometric program that can be efficiently solved. The optimized inductor reduces energy dissipation by at least 20% in various scenarios compared to a previous approach which selected an optimal off-the-shelf inductor.	best, worst and average case;cell (microprocessor);geometric programming;mathematical model;mathematical optimization;nonlinear programming;nonlinear system	Matthias Kauer;Swaminathan Narayanaswamy;Martin Lukasiewycz;Sebastian Steinhorst;Samarjit Chakraborty	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		control engineering;embedded system;electronic engineering;personalized medicine;real-time computing;scalability;computer science;engineering;electrical engineering;operating system;cyber-physical system	EDA	19.42866233859787	56.30783798871198	184823
6c4cdc6f66dba605fe0a63c3e37d1759224bfe5a	uncertainty-aware robust optimization of test-access architectures for 3d stacked ics	robustness three dimensional displays system on chip simulated annealing uncertainty power demand;three dimensional integrated circuits integer programming integrated circuit testing;integer programming;integrated circuit testing;average test time uncertainty aware robust optimization test access architectures 3d stacked ic through silicon vias tsv 3d integration input parameter space test power pattern count logic cores integer linear programming ilp model heuristic solution itc 02 soc benchmarks single point solutions;three dimensional integrated circuits	3D integration using through-silicon vias offers many benefits, such as high bandwidth, low power, and small footprint. However, test complexity and test cost are major concerns for 3D-SICs. Recent work on the optimization of 3D test architectures to reduce test cost suffer from the drawback that they ignore potential uncertainties in input parameters; they consider only a single point in the input-parameter space. In realistic scenarios, the assumed values for parameters such as test power and pattern count of logic cores, which are used for optimizing the test architecture for a die, may differ from the actual values that are known only after the design stage. In a 3D setting, a die can be used in multiple stacks each with different properties. As a result, the originally designed test architecture might no longer be optimal, which leads to an undesirable increase in the test cost. We propose an optimization approach that takes uncertainties in input parameters into account and provides a solution that is efficient in the presence of input-parameter variations. We use integer linear programming (ILP) to formulate the robust test-architecture optimization problem, and the resulting ILP model serves as the basis for a heuristic solution that scales well for large designs. The proposed optimization framework is evaluated using the ITC'02 SoC benchmarks and we show that robust solutions are superior to single-point solutions in terms of average test time when there are uncertainties in the values of input parameters.	central processing unit;die (integrated circuit);heuristic;ibm tivoli access manager;integer programming;linear programming;mathematical model;mathematical optimization;optimization problem;parameter (computer programming);robust optimization;scheduling (computing);simulated annealing;system on a chip;via (electronics)	Sergej Deutsch;Krishnendu Chakrabarty;Erik Jan Marinissen	2013	2013 IEEE International Test Conference (ITC)	10.1109/TEST.2013.6651905	embedded system;mathematical optimization;electronic engineering;real-time computing;integer programming;computer science;engineering	EDA	20.084309285966057	57.49910198603737	184934
e85a7fb35232934f9845d51db279dfc022956505	in-situ transistor reliability measurements through nanoprobing		Abstract In this study we examine the feasibility of performing transistor reliability measurements with the Hyperion II nanoprobing system. Proof-of-concept bias temperature instability (BTI) measurements were run on a commercially available Intel 14 nm FinFET processor. BTI degradation was found to closely follow the expected power law over 10 3  s stress in total at 2 V with characterization done ON reduction of 14.4% (σ = 6.6%) and 6.5% (σ = 2.5%) for pullups and pulldowns, respectively. The in-situ nature of the nanoprobing approach provides insight into transistor lifetime and performance as a function of layout as well as variations in aging between identically designed devices. This is a compelling reason to apply nanoprobing for a range of reliability measurements as a complement to the suite of established reliability testing techniques.	nanoprobing;transistor	O. Dixon-Luinenburg;J. Fine	2018	Microelectronics Reliability	10.1016/j.microrel.2018.06.100	electronic engineering;engineering;reliability engineering;transistor;temperature instability;nanoprobing	EDA	21.21930088795335	59.99579219540046	185220
47f68bf29360220c95c9544c1b7e7bc6d93410ee	test pattern development and evaluation for drams with fault simulator ramsim	random access memory circuit faults circuit simulation production circuit testing buffer storage logic arrays capacitors fault detection semiconductor device testing;semiconductor device testing;logic arrays;random access memory;circuit faults;fault simulation;buffer storage;circuit simulation;capacitors;fault detection;production;circuit testing	A fault simulator for DRAMS has been developed to determine the fault coverage of test patterns . The realistic fault models are derived from DRAM failure analysis and include complex pattern sensitivity mechanisms.	dynamic random-access memory;failure analysis;fault simulator;fault coverage;fault model;test card	H.-D. Oberle;Peter Muhmenthaler	1991		10.1109/TEST.1991.519717	embedded system;electronic engineering;real-time computing;capacitor;fault coverage;fault indicator;computer science;engineering;stuck-at fault;automatic test pattern generation;circuit extraction;fault detection and isolation	HPC	22.542234257013007	53.425127632744314	185508
4938f10b4d151296f7c224c6eda62bd4aa325f0b	state encoding based nbti optimization in finite state machines	silicon degradation optimization benchmark testing integrated circuit reliability power capacitors;flipping bits state encoding based nbti optimization finite state machines optimal state encoding techniques dynamic power optimization negative bias temperature instability simulated annealing sa based state code assignment algorithm pmos transistor nbti degradation minimization delay degradation combinational circuits sequential circuits state probability lgsynth93 benchmarks code swap code modification;simulated annealing circuit optimisation combinational circuits encoding finite state machines mosfet circuits negative bias temperature instability probability sequential circuits	Several works in literature have proposed optimal state encoding techniques for delay, leakage, and dynamic power optimization. In this work, we propose for the first time, NBTI (Negative Bias Temperature Instability) optimization based on state code optimization. We propose a simulated annealing (SA) based state code assignment algorithm that results in minimization of NBTI degradation in the synthesized circuit. A PMOS transistor when switched ON for a long period of time, will lead to delay degradation due to NBTI. Therefore, in combinational circuits, an NBTI friendly input vector that stress least number of PMOS transistors on the critical path can be applied. For sequential circuits, the state code can significantly influence the ON/OFF mode of pMOS transistors in the controller implementation. Therefore, we propose to focus on state encoding. As the problem is computational intractable, we will focus on encoding states with high state probability. The following SA moves are employed: (a) code swap; and (b) code modification by flipping bits. Experiments with LGSYNTH93 benchmarks resulted in 18.6% improvement in NBTI degradation on average with area and power improvements of 5.5% and 4.6% respectively.	algorithm;combinational logic;computation;critical path method;elegant degradation;emoticon;finite-state machine;mathematical optimization;negative-bias temperature instability;pmos logic;paging;power optimization (eda);program optimization;simulated annealing;spectral leakage;transistor;transistor–transistor logic	Shilpa Pendyala;Srinivas Katkoori	2016	2016 17th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2016.7479237	electronic engineering;parallel computing;real-time computing;engineering	EDA	19.235688830503594	57.14174645170712	185588
0ba03888233c3bc52eea8e4ca43085df1aa7a65a	hardware error likelihood induced by the operation of software	hardware software interaction;stress;software;software reliability cmos digital integrated circuits embedded systems hardware software codesign semiconductor device reliability;reliability;embedded software applications hardware error likelihood software operation hardware failures safety critical systems semiconductor devices complementary metal oxide semiconductor technology deep submicron regimes reliability analysis;hardware software codesign;integrated circuit;semiconductor device reliability;semiconductor devices;embedded system;embedded systems;circuit simulation;logic gates;cmos digital integrated circuits;transistors;integrated circuit modeling;failure propagation;safety critical system;reliability analysis;hardware integrated circuit modeling logic gates software reliability transistors stress;permanent hardware failures circuit simulation embedded systems failure propagation hardware software interaction;software reliability;logic gate;permanent hardware failures;embedded software;hardware	The influence of the software, and its interaction and interdependency with the hardware in the creation and propagation of hardware failures, are usually neglected in reliability analyses of safety critical systems. The software operation is responsible for the usage of semiconductor devices along the system lifetime. This usage consists of voltage changes and current flows that steadily degrade the materials of circuit devices until the degradation becomes permanent, and the device can no longer perform its intended function. At the circuit level, these failures manifest as stuck-at values, signal delays, or circuit functional changes. These failures are permanent in nature. Due to the extremely high scaling of complementary metal-oxide-semiconductor (CMOS) technology into deep submicron regimes, permanent hardware failures are a key concern, and can no longer be neglected compared to transient failures in radiation-intense applications. Our work proposes a methodology for the reliability analysis of permanent failure manifestations of hardware devices due to the usage induced by the execution of embedded software applications. The methodology is illustrated with a case study based on a safety critical application.	arithmetic logic unit;cmos;computer hardware;curve fitting;electromigration;elegant degradation;embedded software;failure cause;fan-out;fault injection;fault model;high- and low-level;hot-carrier injection;image scaling;interaction;interdependence;logic gate;negative-bias temperature instability;netlist;physical review a;spice;semiconductor device;simulation;software propagation;statistical model;transistor;vhdl;very-large-scale integration;wiring	Bing Huang;Manuel Rodríguez;Ming Li;Joseph B. Bernstein;Carol S. Smidts	2011	IEEE Transactions on Reliability	10.1109/TR.2011.2161699	reliability engineering;electronic engineering;real-time computing;logic gate;engineering;computer engineering	EDA	21.55608294711353	57.39703869542721	185877
195a9b5e7a01dccdfe2a919ca41e7a65ca8a5003	controlling ground bounce noise in power gating scheme for system-on-a-chip	substrate noise power gating sleep transistor leakage ground bounce system on a chip;cmos integrated circuits;control systems;leakage;rails;resonance;circuit noise;leakage current;clocks;control systems system on a chip circuit noise power system reliability rails integrated circuit noise leakage current voltage packaging resonance;peak current;power gating structures;power gating;ground bounce noise;packaging;system on a chip;power mode transition;power gating scheme;leakage currents;system on chip;sleep transistor;transistors;voltage;power system reliability;substrates;ground bounce;system on chip circuit noise leakage currents;integrated circuit noise;voltage glitches;power distribution network;power distribution network ground bounce noise power gating scheme system on a chip leakage currents power mode transition power gating structures peak current voltage glitches;noise;substrate noise	Conventional power gating techniques for minimizing leakage currents introduce ground bounce noise during power mode transition. Here an analysis of ground bounce due to power mode transition in power gating structures is presented. An innovative power gating approach is proposed, which in addition to targeting maximum reduction of major leakage currents will provide a way to control ground bounce during power mode transition. The proposed power gating technique will have an additional intermediate HOLD mode along with conventional CUTOFF and RUN modes. Its stepwise turning on feature will provide higher reduction of the magnitude of peak current and voltage glitches in the power distribution network as well as the minimum time required to stabilize power and ground as compared to other similar techniques.	glitch;ground bounce;power gating;spectral leakage;stepwise regression;system on a chip	Masud H. Chowdhury;Juliana Gjanci;Pervez Khaled	2008	2008 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2008.85	embedded system;electronic engineering;engineering;electrical engineering;ground bounce;clock gating	Arch	18.223243051871133	56.99420746116247	185968
8c6b72da11022af477db2acb416da0b0d97cc0e4	nbti-aware adaptive minimum leakage vector selection using a linear programming approach	negative bias temperature instability;负偏置温度不稳定都效应;input vector control;漏电功率;support vector regression;输入向量控制;leakage power;线性规划方法;期刊论文;支持向量回归;linear programming	Due to the circuit aging effect, the minimum leakage vector (MLV) found by the traditional input vector control method may not obtain the optimal leakage power reduction result when the circuit begins to degrade. To solve this problem, we present an adaptive MLV selection strategy based on a linear programming approach. The method divides the total lifetime of the circuit into a succession of time intervals, and the MLV used in each interval is periodically updated according to the transistor's threshold voltage degradation so that the best overall power reduction result can be achieved. Experimental results on various benchmark circuits show the effectiveness of our method. & 2016 Published by Elsevier B.V. 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94	benchmark (computing);elegant degradation;linear programming;negative-bias temperature instability;spectral leakage;succession;transistor	Zhi-Ming Yang;Yang Yu;Chengcheng Zhang;Xiyuan Peng	2016	Integration	10.1016/j.vlsi.2015.12.009	support vector machine;negative-bias temperature instability;mathematical optimization;electronic engineering;computer science;engineering;linear programming;control theory	EDA	19.490932026015102	60.087494009353556	186087
6f8ac6cb3b132adb707c2b1850e6c1fd0c8c3c8b	an efficient delay model for mos current-mode logic automated design and optimization	automated design;design automation;cmos technology;decoding;delay logic design design optimization cmos logic circuits design automation cmos technology semiconductor device modeling coupling circuits inverters logic gates;logic design;logic design adders cmos logic circuits current mode logic decoding;automated optimization;inverters;low noise alternative;coupling circuits;4 bit ripple carry adder;design optimization;size 0 18 mum;mos current mode logic;size 0 18 mum mos current mode logic automated design optimization low noise alternative 4 bit ripple carry adder 8 bit decoder delay model cmos technology word length 4 bit word length 8 bit;source coupled logic;current mode logic;low noise;ripple carry adder;logic gates;mathematical programming;adders;semiconductor device modeling;cmos logic circuits;source coupled logic automated optimization mos current mode logic mcml;8 bit decoder;optimization;word length 8 bit;word length 4 bit;logic gate;mos current mode logic mcml;delay model	MOS current-mode logic (MCML) is a low-noise alternative to CMOS logic. The lack of MCML automation tools, however, has deterred designers from applying MCML to complex digital functions. This paper presents an efficient MCML optimization program that can be used to properly size MCML gates. The delay model accuracy is adjusted by fitting measured gate delays by means of technology-dependent parameters. For an N number of logic gates, the proposed mathematical program has reduced the number of variables to N+1, in comparison to 7N+1 in the most recent work on this topic. The program has been implemented to efficiently optimize a 4-bit ripple carry adder and an 8-bit decoder in 0.18-μm CMOS technology.	4-bit;8-bit;adder (electronics);binary decoder;cmos;converge;current-mode logic;curve fitting;hp 48 series;logic gate;mathematical optimization;numerical analysis;propagation delay;quantum gate;ripple effect;simulation;transistor	Osman Musa Abdulkarim;Maitham Shams	2010	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2009.2039258	electronic engineering;real-time computing;electronic design automation;logic gate;computer science;engineering;theoretical computer science;adder	EDA	17.73349451616703	56.37220151524525	186134
4d9b87265edcb542931acbdc4b4dda36b6816acf	a high density and low power cache based on novel sram cell	leakage current;high density;cell size;positive feedback;cell leakage;indexing terms;static noise margin;design rules;low power;dynamic energy consumption;energy consumption;cell current;5t sram cell;cell area;read static noise margin free	Based on the observation that dynamic occurrence of zeros in the cache access stream and cacheresident memory values of ordinary programs exhibit a strong bias towards zero, this paper presents a novel CMOS five-transistor SRAM cell (5T SRAM cell) for very high density and low power cache applications. This cell retains its data with leakage current and positive feedback without refresh cycle. Novel 5T SRAM cell uses one word-line and one bit-line and extra read-line control. The new cell size is 17% smaller than a conventional six-transistor SRAM cell using same design rules with no performance degradation. Simulation and analytical results show purposed cell has correct operation during read/write and also the average dynamic energy consumption of new cell is 30% smaller than a six-transistor SRAM cell.	cmos;cell (microprocessor);elegant degradation;memory cell (binary);noise margin;performance;positive feedback;regular expression;simulation;spectral leakage;static random-access memory;transistor;value-driven design	Arash Azizi Mazreah;Mohammad Taghi Manzuri;Ali Mehrparvar	2009	JCP	10.4304/jcp.4.7.567-575	parallel computing;real-time computing;positive feedback;index term;computer science;leakage	HPC	17.81163326401885	59.179632403870094	186649
58b4e8c7ace65c2a7a68a685f028d7250e64785f	two modeling techniques for cmos circuits to enhance test generation and fault simulation for bridging faults	design for testability;inter gate faults;modeling technique;iddq testing cmos circuits efficient modeling techniques enhanced test generation performance fault simulation bridging faults fault modeling technique inter gate faults threshold determination method spice like accuracy digital logic gates logic testing;cmos technology;circuit faults;fault simulation;automatic testing;cmos circuits;spice like accuracy;design for testability cmos logic circuits fault diagnosis logic testing circuit analysis computing spice logic cad automatic testing integrated circuit modelling;circuit testing semiconductor device modeling cmos technology circuit faults circuit simulation voltage spice switches logic testing central processing unit;fault modeling technique;circuit simulation;digital logic gates;integrated circuit modelling;efficient modeling techniques;semiconductor device modeling;enhanced test generation performance;cmos logic circuits;voltage;logic testing;bridging faults;test generation;threshold determination method;circuit testing;iddq testing;fault model;switches;circuit analysis computing;logic cad;spice;central processing unit;fault diagnosis	In this paper we present two accurate and efficient modeling techniques for CMOS circuits to enhance the performance of test generation and fault simulation for bridging faults. The first one is a fault modeling technique for inter-gate bridging faults. The second one is an accurate threshold determination method. The accuracy of our model is achieved because all the following factors, including device parameters, voltage operation range of each logic value, resistance of ON-transistors, resistance of bridging faults, and test patterns are considered. The efficiency is achieved due to the simplicity of the solution methods that require no complex circuit level simulation. Experimental data show that SPICE like accuracy can be efficiently achieved.	bridging (networking);cmos;simulation	Kuen-Jong Lee;Jing-Jou Tang	1996		10.1109/ATS.1996.555154	electronic engineering;semiconductor device modeling;real-time computing;voltage;network switch;computer science;engineering;stuck-at fault;operating system;central processing unit;design for testing;fault model;iddq testing;cmos;computer engineering	EDA	22.05868938039575	54.86097023385391	186988
e7c38f444f3d4c38e80a812f41c759aa2aa1c2f3	mitigating “no trouble found” component returns	digital signal processing;very large scale integration no trouble found mitigation vlsi scaling dsp ntf component board level manufacturing testing soft defect hard defect on die process parameter;testing;vlsi integrated circuit testing;monitoring;manufacturing;fabrics;switches;integrated circuits;manufacturing integrated circuits testing monitoring fabrics switches digital signal processing	With VLSI scaling, “no trouble found” or NTF parts passing at the component level, but failing at board level manufacturing testing have increased due to the dominance of soft defects over hard defects. An analysis of networking and DSP NTF components shows outlying behavior in not only product parameters but also on-die process parameters revealing new mitigation opportunities. The resulting yield hit is demonstrated to be minor <;0.5% to catch NTFs that can be >50% component with high debug cost.	digital signal processor;failure;image scaling;national transfer format;very-large-scale integration	Amr Haggag;Nik Sumikawa;Aamer Shaukat;J. K. Jerry Lee;Nick Aghel;Charlie Slayman	2015	2015 IEEE International Reliability Physics Symposium	10.1109/IRPS.2015.7112710	embedded system;electronic engineering;engineering;electrical engineering	Embedded	21.847665499074928	53.96625502171846	187100
ff5ec156a80dda96e24cd0aa787e08fe280a2b54	a memristor-based compressive sensing architecture	memristors compressed sensing sensors fabrication hardware resistance titanium;random processes circuit complexity cmos digital integrated circuits compressed sensing image sampling integrated circuit design memristors performance evaluation;limited sampling speed memristor based compressive sensing architecture memristor based circuit design nonideal fabrication processes performance uncertainties randomness utilization random sensing matrix digital cmos circuits hardware complexity problems	Memristors are considered as one promising candidate for future memory and computing fabrics. However, the design of memristor-based circuits is under a critical challenge of inevitable variations due to non-ideal fabrication processes and the resulted performance uncertainties. This kind of randomness can be utilized in many other applications, such as compressive sensing based data acquisition, which is conducted by a random sensing matrix. Existing compressive sensing systems are usually implemented in digital CMOS circuits, which suffer the problems of high hardware complexity and limited sampling speed. In this paper, we exploit the inherent variations in memristor devices to generate random sensing matrices for compressive sensing and achieve low cost and high performance operations. Simulation results demonstrate the advantages of the proposed memristor-based compressive sensing architecture.	cmos;compressed sensing;data acquisition;memristor;randomness;sampling (signal processing);simulation	Fengyu Qian;Yanping Gong;Guoxian Huang;Kiarash Ahi;Mehdi Anwar;Lei Wang	2016	2016 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)	10.1145/2950067.2950081	control engineering;electronic engineering;engineering;electrical engineering;memistor	Arch	21.3410427256263	59.61859797784778	187782
f695fdb4748935058b1381a4e0d8daec4d145bc8	on the threat of metastability in an asynchronous fault-tolerant clock generation scheme	analytical models;metastasis fault tolerance clocks fault tolerant systems pipelines asynchronous circuits circuit faults performance analysis analytical models heart;muller c elements;circuit faults;fault tolerant;logic design;building block;radiation detectors;logic testing asynchronous circuits fault tolerance logic design;transient pulse asynchronous fault tolerant clock generation scheme handshake based flow control asynchronous circuits single event transients asynchronous building blocks muller c elements single metastable upset multiple byzantine fault tolerant system failure elastic pipeline simulation;multiple byzantine fault tolerant system failure;asynchronous circuit;metastable state;elastic pipeline simulation;asynchronous building blocks;single metastable upset;synchronization;pipelines;fault tolerance;transient pulse;single event transient;integrated circuit modeling;logic testing;byzantine fault tolerant;asynchronous circuits;asynchronous fault tolerant clock generation scheme;flow control;handshake based flow control;analytical model;single event transients	Due to their handshake-based flow control, asynchronous circuits generally do not suffer from metastability issues as much as synchronous circuits do. We will show, however, that fault effects like single-event transients can force (sequential) asynchronous building blocks such as Muller C-Elements into a metastable state. At the example of a fault-tolerant clock generation scheme, we will illustrate that metastability could overcome conventional error containment boundaries, and that, ultimately, a single metastable upset could cause even a multiple Byzantine fault-tolerant system to fail. In order to quantify this threat, we performed analytic modeling and simulation of the elastic pipelines, which are at the heart of our physical implementation of the fault-tolerant clocks. Our analysis results reveal that only transient pulses of some very specific width can trigger metastable behavior. So even without consideration of other masking effects the probability of a metastable upset to propagate through a pipeline is fairly small. Still, however, a thorough metastability analysis is mandatory for circuits employed in high-dependability applications.	byzantine fault tolerance;dependability;metastability in electronics;pipeline (software);simulation;single event upset	Gottfried Fuchs;Matthias Függer;Andreas Steininger	2009	2009 15th IEEE Symposium on Asynchronous Circuits and Systems	10.1109/ASYNC.2009.15	electronic engineering;real-time computing;engineering;distributed computing	EDA	20.120510082824687	56.32036832849536	187891
82b44666cd4a6ed53610ef9e6903f1e360feff43	a design model for random process variability	random process variability;analternative equation;circuit design;process variation;convenient posynomial model;new approach;hand analysis;existingstatistical method;current variation;design model;monte carlo;optimization problem;random processes;testing;integrated circuit design;metrology;statistical analysis;random process;gaussian distribution;monte carlo method;principal component analysis;modeling	A new approach to analyze process variation through measured current variation is introduced. The methodology concludes with a simple and convenient posynomial model for random process variability to bridge the gap between existing statistical methods and circuit design. The model contains only design variables: transistor sizes W and L, and operating points Vgs and Vds. Modeling random process variability in this way allows for adaptability to optimization problems, time efficient methods for gathering statistical information in comparison to Monte Carlo, and an alternative equation for hand analysis.	circuit design;hayes microcomputer products;heart rate variability;mathematical optimization;monte carlo method;optimization problem;posynomial;spice;semiconductor research corporation;spatial variability;stochastic process;transistor	Victoria Wang;Kanak Agarwal;Sani R. Nassif;Kevin J. Nowka;Dejan Markovic	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.111	stochastic process;econometrics;electronic engineering;computer science;statistics;monte carlo method	EDA	23.93483483855587	57.07046013649945	188066
a03c12314393e3150a5edfb68256a11895a48d2f	achieving accurate timing measurements on ttl/cmos devices	test system;large-scale integration;production ttl;achieving accurate timing measurements;new ttl;cmos devices;different test system;percent error factor;automatic test equipment system;cmos timing measurement;cmos technology;gate array;semiconductor device modeling;transmission lines;automatic test equipment;propagation delay;system testing	Production TTL and CMOS timing measurements obtained between different test systems and between test systems and the bench setup often do not correlate or do not appear to be accurate even though the automatic test equipment system has subnanosecond accuracy. Errors of as much as 2 ns can occur with small-, medium-, and large-scale integration and with gate arrays using the new TTL and CMOS technologies. This represents a 100- percent error factor for these emerging products.	approximation error;built-in test equipment;cmos;integrated circuit;transistor–transistor logic	Dennis Petrich	1985	IEEE Design & Test of Computers		embedded system;automatic test equipment;propagation delay;logic probe;electronic engineering;semiconductor device modeling;real-time computing;computer science;engineering;electric power transmission;cmos;system testing	EDA	22.837060422217032	54.13620818763349	189209
8d1ce64a34f686107fe98d139882d13bf51c0ad5	impacts of nbti/pbti on performance of domino logic circuits with high-k metal-gate devices in nanoscale cmos	reliability;positive bias temperature instability pbti;negative bias temperature instability nbti;high k metal gate	0026-2714/$ see front matter 2012 Elsevier Ltd. A http://dx.doi.org/10.1016/j.microrel.2012.03.012 ⇑ Corresponding author. Address: 1st floor, 1st unit Lahoori Street, Mashhad 9179895465, Khorasan 5115023121, mobile: +98 9155164374. E-mail addresses: ma_ho316@stu-mail.um.ac.ir ( rlotfi@ieee.org (R. Lotfi), khmafinezhad@gmail.com ( sfsu.edu (H. Mahmoodi). Negative-bias temperature instability (NBTI) and positive-bias temperature instability (PBTI) weaken PFETs and high-k metal-gate NFETs, respectively. This paper provides comprehensive analyses on the impacts of NBTI and PBTI on wide fan-in domino gates with high-k metal-gate devices. The delay degradation and power dissipation of domino logic, as well as the Unity Noise Gain (UNG) are analyzed in the presence of NBTI/PBTI degradation. It has been shown that the main concern is the degradation impact on delay which can increase up to 16.2% in a lifetime of 3 years. We have also proposed a degradation tolerant technique to compensate for the NBTI/PBTI-induced delay degradation in domino gates with a negligible impact on UNG and power. 2012 Elsevier Ltd. All rights reserved.	btrieve;cmos;domino logic;elegant degradation;fan-in;high-κ dielectric;keeper (password manager);logic gate;negative-bias temperature instability;polynomial texture mapping;power inverter;transistor;unity;upsizing (database)	Masaoud Houshmand Kaffashian;Reza Lotfi;Khalil Mafinezhad;Hamid Mahmoodi	2012	Microelectronics Reliability	10.1016/j.microrel.2012.03.012	high-κ dielectric;electronic engineering;real-time computing;electrical engineering;reliability;mathematics;physics;statistics	EDA	19.544789983396363	59.796131766688646	189400
1f36cb540920108ae855541d7acf35d0d0c6d279	a cell-driven multiplier generator with delay optimization of partial products compression and an efficient partition technique for the final addition	accumulator;circuito aritmetico;optimisation;multiplier;tecnologia electronica telecomunicaciones;multipliers;optimizacion;arithmetic functional units;tecnologia mos complementario;acumulador;multiplicateur;logica transistor transistor;final adder partition;analyse performance;transistor transistor logic;performance analysis;logique transistor transistor;optimization;temps retard;delay time;tecnologias;grupo a;technologie mos complementaire;accumulateur;datapath generation;tiempo retardo;multiplicador;circuit arithmetique;complementary mos technology;partial product compression;arithmetic circuit;multiplier and accumulator mac;analisis eficacia	In this paper, a cell-driven multiplier generator is developed that can produce high-performance gate-level netlists for multiplier-related arithmetic functional units, including multipliers, multiplier and accumulators (MAC) and dot product calculator. The generator optimizes the speed/area performance both in the partial product compression and in the final addition stage for the specified process technology. In addition to the conventional CMOS full adder cells, we have also designed fast compression elements based on pass-transistor logic for further performance improvement of the generated multipliers. Simulation results show that our proposed generator could produce better multiplier-related functional units compared to those generated using Synopsys Designware library or other previously proposed approaches.		Tso-Bing Juang;Shen-Fu Hsiao;Ming-Yu Tsai;Jenq-Shiun Jan	2005	IEICE Transactions	10.1093/ietisy/e88-d.7.1464	embedded system;accumulator;mathematics;transistor–transistor logic;multiplier;algorithm	EDA	18.183968005354792	55.354181122703274	189496
ae7045cd9fedae2afc45d68ddc4adc98b3b696be	industrial evaluation of transition fault testing for cost effective offline adaptive voltage scaling		Adaptive voltage scaling (AVS) has been used widely to compensate for process, voltage, and temperature variations as well as power optimization of integrated circuits. The current industrial state-of-the-art AVS approaches using Process Monitoring Boxes (PMBs) have shown several limitations such as huge characterization effort, which makes these approaches very expensive, and a low accuracy that results in extra margins, which consequently lead to yield loss and performance limitations. To overcome those limitations, in this paper we propose an alternative solution using transition fault test patterns, which is able to eliminate the need for PMBs, while improving the accuracy of voltage estimation. The paper shows, using simulation of ISCAS'99 benchmarks with 28nm FD-SOI library, that AVS using transition fault testing (TF-based AVS) results in an error as low as 5.33%. The paper also shows that the PMB approach can only account for 85% of the uncertainty in voltage measurements, which results in power waste, while the TF-based approach can account for 99% of that uncertainty.	central processing unit;die shrink;dynamic voltage scaling;graphics processing unit;image scaling;integrated circuit;mathematical optimization;online and offline;pmb;power optimization (eda);simulation;technical standard;test card;the industry standard	Mahroo Zandrahimi;Philippe Debaud;Armand Castillejo;Zaid Al-Ars	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342022	real-time computing;computer science;adaptive voltage scaling;voltage;benchmark (computing);integrated circuit;power optimization	EDA	20.367820991357075	57.56682213451198	189757
512d5b0cc2b007be343790a27006c8d9ee4fb13e	a fault tolerance mechanism for semiconductor equipment monitoring		As the semiconductor manufacturing technology advances, the size of a wafer becomes bigger and the critical dimension becomes smaller than before. This means a wafer can be used to produce more chips. However, the process of manufacturing chips is costly while using today's semiconductor manufacturing technology. Any defect on the wafer may fail the final product and cause large business loss. To reduce the chance of defects on the wafer, the parameters of the manufacturing environment must be precisely controlled. To achieve this goal, a monitoring system is usually used to collect real-time information, which helps shorten the decision time for changing the parameters of the manufacturing environment. For now, most of the semiconductor manufacturing machines support the SECS/GEM standard, which defines how to obtain the monitoring data of the machines via TCP/IP. The problem is that, the existing monitoring approach rarely supports failover and needs human intervention when the system crashes. This implies a long recovery time. Moreover, the failure may further cause other problems. For example, a manufacturing alarm system could generate a false alarm or overlook an important abnormality during the failure time, since the monitoring system fails to feed any data to the alarm system. To solve this problem, we introduce a new fault-tolerance monitoring mechanism based on the techniques of server redundancy and checkpointing. With the proposed approach, the monitoring system is able to achieve a very small downtime, and consequently helps the manufacturing process and the yield rate.	application checkpointing;downtime;failover;fault tolerance;internet protocol suite;real-time data;secs/gem;semiconductor device fabrication;server (computing);software bug;wafer (electronics)	Shao-Jui Chen;Hsueh-Wen Liu;Wei-Jen Wang	2017	2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)	10.1109/SC2.2017.33	redundancy (engineering);failover;false alarm;reliability engineering;downtime;fault tolerance;synchronization;semiconductor device fabrication;computer science;server	Embedded	24.386630103529374	53.732632538302965	189768
5cec47e7024806f17569acba77ecec41024b0fa7	floorplan-based crosstalk estimation for macrocell-based designs	crosstalk routing wire phase estimation phase measurement probability noise level capacitance silicon predictive models;integrated circuit layout;crosstalk;network routing;spice integrated circuit layout network routing circuit layout cad crosstalk circuit simulation;circuit simulation;global routing;circuit layout cad;hspice simulations floorplan based crosstalk estimation macrocell based designs crosstalk susceptibility global routing phase wire routes noise amplitude statistical estimation cross coupling capacitances placement routing cadence silicon ensemble;spice;statistical estimation	We propose an estimation technique to measure the crosstalk susceptibility of different nets in the post global routing phase, prior to detailed routing of designs. Global routing provides the approximate routes of the wires. This is used to compute the aggressors of a given victim wire along its route and its crosstalk susceptibility with respect to those aggressors. The crosstalk susceptibility of a victim wire is given by: (1) P/sub t/ the probability of crosstalk occurrence on the wire in different regions along its route; and (2) V/sub peak/ worst case noise amplitude experienced by the wire along its route. P/sub t/ is estimated using a very fast and accurate statistical estimator previously proposed by the authors. V/sub peak/ is estimated by predicting the cross-coupling capacitances between neighboring wires, using their global routing information. Placement and global routing are done using CADENCE silicon ensemble. The predicted crosstalk estimates are compared against those by detailed HSPICE simulations. Average errors are found to be less than 8% while the execution times are significantly reduced.	approximation algorithm;best, worst and average case;crosstalk;estimation theory;routing;spice 2;simulation;speedup;statistical model	Suvodeep Gupta;Srinivas Katkoori;Hariharan Sankaran	2005	18th International Conference on VLSI Design held jointly with 4th International Conference on Embedded Systems Design	10.1109/ICVD.2005.100	embedded system;routing;electronic engineering;real-time computing;crosstalk;computer science;engineering;electrical engineering;integrated circuit layout;routing	EDA	22.40446935659644	57.40267384158771	190224
84c7720196c9b4d3317261b51f0e69c82dec1358	effective launch-to-capture power reduction for los scheme with adjacent-probability-based x-filling	launch off shift scheme;ap fill technique;probability;power aware computing integrated circuit testing integrated logic circuits large scale integration;circuit faults;lsi testing;integrated circuit;flip flops;power supply noise test generation test power at speed scan testing launch off shift;launch to capture power reduction;test generation approach;testing;circuit faults vectors flip flops logic gates probability integrated circuit modeling testing;test vector count inflation launch to capture power reduction los scheme adjacent probability based x filling lsi testing at speed testing defect free logic ic test generation approach ir drop power supply noise launch off capture scheme launch off shift scheme post atpg test modification flow ap fill technique itc 99 circuit;launch off shift;defect free logic ic;power aware computing;test power;at speed testing;large scale integration;vectors;logic gates;adjacent probability based x filling;power supply noise;los scheme;post atpg test modification flow;integrated circuit modeling;integrated circuit testing;test vector count inflation;test generation;launch off capture scheme;fault coverage;model test;power reduction;integrated logic circuits;logic gate;itc 99 circuit;ir drop;flip flop;at speed scan testing	It has become necessary to reduce power during LSI testing. Particularly, during at-speed testing, excessive power consumed during the Launch-To-Capture (LTC) cycle causes serious issues that may lead to the overkill of defect-free logic ICs. Many successful test generation approaches to reduce IR-drop and/or power supply noise during LTC for the launch-off capture (LOC) scheme have previously been proposed, and several of X-filling techniques have proven especially effective. With X-filling in the launch-off shift (LOS) scheme, however, adjacent-fill (which was originally proposed for shift-in power reduction) is used frequently. In this work, we propose a novel X-filling technique for the LOS scheme, called Adjacent-Probability-based X-Filling (AP-fill), which can reduce more LTC power than adjacent-fill. We incorporate AP-fill into a post-ATPG test modification flow consisting of test relaxation and X-filling in order to avoid the fault coverage loss and the test vector count inflation. Experimental results for larger ITC'99 circuits show that the proposed AP-fill technique can achieve a higher power reduction ratio than 0-fill, 1-fill, and adjacent-fill.	ap computer science principles;altered level of consciousness;fault coverage;integrated circuit;linear programming relaxation;litecoin;power supply;software bug;space-filling tree;test vector	Kohei Miyase;Y. Uchinodan;Kazunari Enokimoto;Yuta Yamato;Xiaoqing Wen;Seiji Kajihara;Fangmei Wu;Luigi Dilillo;Alberto Bosio;Patrick Girard;Arnaud Virazel	2011	2011 Asian Test Symposium	10.1109/ATS.2011.35	embedded system;electronic engineering;real-time computing;logic gate;engineering;electrical engineering	EDA	20.363803881911387	53.54207483732093	190496
242c77775965b23a6f3f60d356869a017b8943c9	intrinsic mosfet parameter fluctuations due to random dopant placement	mosfet circuits;static power intrinsic parameter fluctuations random dopant placement threshold voltage subthreshold swing saturation drain current subthreshold leakage mosfet monte carlo simulation physical model power dissipation switching delay scaling supply voltage channel length cmos technology packing density dynamic power;complementary metal oxide semiconductor;transistor efecto campo;cmos technology;leakage current;voltage threshold;dopant fluctuations;fluctuations;parametro transistor;subthreshold current;semiconductor process modeling;switching delay;transistor effet champ;corriente escape;semiconductor doping;variation parametre;dopage;monte carlo methods mosfet semiconductor doping fluctuations semiconductor device models;modele physique;indexing terms;variacion parametro;mosfet circuits fluctuations threshold voltage subthreshold current cmos technology semiconductor process modeling monte carlo methods power dissipation power semiconductor switches delay;random fluctuations;chip;parametre transistor;doping;scaling;intrinsic parameter fluctuations;channel length;packing density;intrinsic threshold variations;courant fuite;integrated project;threshold voltage;power dissipation;semiconductor device models;supply voltage;parameter variation;field effect transistor;modelo fisico;mos transistor;dynamic power;mosfet parameter fluctuations;subthreshold leakage;mosfet;seuil tension;subthreshold swing;saturation drain current;physical model;power semiconductor switches;transistor parameter;monte carlo simulation;threshold voltage fluctuations;static power;transistor mos;monte carlo methods;random dopant placement;umbral tension	Intrinsic fluctuations in threshold voltage, subthreshold swing, saturation drain current and subthreshold leakage of ultrasmall-geometry MOSFETs due to random placement of dopant atoms in the channel are examined using novel physical models and a Monte Carlo simulator. These fluctuations are shown to pose severe barriers to the scaling of supply voltage and channel length and thus, to the minimization of power dissipation and switching delay in multibillion transistor chips of the future. In particular, using the device technology and the level of integration projections of the National Technology Roadmap for Semiconductors for the next 15 years, standard and maximum deviations of threshold voltage, drive current, subthreshold swing and subthreshold leakage are shown to escalate to 40 and 600 mV, 10 and 100%, 2 and 20 mV/dec, and 10 and 10/sup 8/%, respectively, in the 0.07 /spl mu/m, 0.9 V complementary metal-oxide-semiconductor (CMOS) technology generation with 1.3-64 billion transistors on a chip in 2010. While these deviations can be reduced to some degree by selecting optimal values of channel width, the associated penalties in dynamic and static power, and in packing density demand improved MOSFET structures aimed at minimizing parameter deviations.		Xinghai Tang;Vivek De;James D. Meindl	1997	IEEE Trans. VLSI Syst.	10.1109/92.645063	electronic engineering;subthreshold slope;computer science;electrical engineering;control theory;doping;threshold voltage;cmos;subthreshold conduction;statistics;monte carlo method	EDA	20.543491362832015	58.61962543340068	190675
e07bb44ed9d4a775732b21a99ad4a024027dbf5c	variability aware low-power delay optimal buffer insertion for global interconnects	optimal solution;global interconnects;signal line delays;low power electronics buffer circuits circuit optimisation integrated circuit interconnections integrated circuit modelling;delay variation;clock networks;system performance;robust optimization;design space;buffer circuits;size 65 nm variability aware low power delay optimal buffer insertion global interconnects clock skew signal line delays power overheads power delay optimal buffer insertion closed form expressions power robustness trade off analysis clock networks;low power;variability delay variation interconnect delay low power optimal buffer insertion;integrated circuit modelling;propagation delay;integrated circuit interconnections;low power electronics;delay variatioh;integrated circuit interconnections propagation delay capacitance robustness;closed form expressions;robustness;capacitance;variability;optimal buffer insertion;circuit optimisation;variational models;clock skew;size 65 nm;power robustness trade off analysis;buffer insertion;interconnect delay;variability aware low power delay optimal buffer insertion;power overheads;power delay optimal buffer insertion	Global interconnect delay variations may cause clock skew, unpredictable signal line delays, and degraded system performance. Conventional variation mitigation techniques incur large delay and power overheads, as variability increases in sub-65 nm technologies. This paper presents a methodology to include robustness optimization in power-delay optimal buffer insertion. Closed form expressions are derived for the delay variation model used in the optimization and its accuracy is verified against simulation results. Using the power, delay, and delay variation models, a design space is constructed for the interconnect. Through power-robustness trade-off analysis of the design space, the optimal buffering solution for the interconnect is computed. Comparison with simulation results verifies the accuracy of the optimal solution computed using this method. The application of this methodology in enhancing robustness of clock networks during buffer insertion phase is demonstrated and simulation results presented.	clock skew;electrical connection;heart rate variability;mathematical optimization;propagation delay;simulation;software propagation	Ashok Narasimhan;Ramalingam Sridhar	2010	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2010.2073790	embedded system;propagation delay;electronic engineering;robust optimization;real-time computing;clock skew;computer science;capacitance;low-power electronics;robustness	EDA	21.25120241671102	58.099087442877185	191046
0a7ed2238489d818e31d82c4d89e85fb83e4092d	a fast statistical soft error rate estimation method for nano-scale combinational circuits	soft error rate;process variations;statistical analysis;transient faults;soft error	Nano-scale digital integrated circuits are getting increasingly vulnerable to soft errors due to aggressive technology scaling. On the other hand, the impacts of process variations on characteristics of the circuits in nano era make statistical approaches as an unavoidable option for soft error rate estimation procedure. In this paper, we present a novel statistical Soft Error Rate estimation framework. The vulnerability of the circuits to soft errors is analyzed using a newly defined concept called Statistical Vulnerability Window (SVW). SVW is an inference of the necessary conditions for a Single Event Transient (SET) to cause observable errors in the given circuit. The SER is calculated using a probabilistic formulation based on the parameters of SVWs. Experimental results show that the proposed method provides considerable speedup (about 5 orders of magnitude) with less than 5 % accuracy loss when compared to Monte-Carlo SPICE simulations. In addition, the proposed framework, keeps its efficiency when considering a full spectrum charge collections (more than 36X speedups compared to the most recently published similar work).	combinational logic;gnu nano;soft error	Mohsen Raji;Behnam Ghavami	2016	J. Electronic Testing	10.1007/s10836-016-5583-3	econometrics;electronic engineering;real-time computing;soft error;computer science;statistics	EDA	22.080149074866405	58.6706292473431	191285
61b62041b9a47d72867e21ddd352a0a0418a3ed8	analysis of retention time distribution of embedded dram - a new method to characterize across-chip threshold voltage variation	across chip threshold voltage variation;retention time distribution;technology development;subthreshold current;standard deviation;edram array retention time distribution embedded dram across chip threshold voltage variation ibm subthreshold current data retention time;random access memory threshold voltage circuit testing fets subthreshold current monitoring scattering manufacturing statistics gain measurement;data retention time;data mining;retention time;chip;arrays;logic gates;threshold voltage;capacitors;embedded dram;edram array;gaussian distribution;dram chips;ibm	In this paper, we investigate the retention time distribution of IBM's 65nm node embedded DRAM. We demonstrate that subthreshold current is the dominant leakage mechanism that determines data retention time, and the retention distribution can be attributed to array Vt variation. Based on this study, we present a new technique for characterization of across-chip Vt variation. The Vt median value and standard deviation of transfer devices within an eDRAM array are estimated by analyzing the retention characteristics. The evaluation results are confirmed by the parametric test data. The proposed method is fast and can be used to monitor Vt variation in both technology development and manufacture. The impact of array Vt spread on the retention and performance of eDRAM is discussed.	dynamic random-access memory;edram;embedded system;spectral leakage;test data	Wei Kong;Paul C. Parries;Guohua Wang;Subramanian S. Iyer	2008	2008 IEEE International Test Conference	10.1109/TEST.2008.4700556	normal distribution;chip;electronic engineering;real-time computing;capacitor;logic gate;computer hardware;engineering;threshold voltage;standard deviation;ibm;statistics	EDA	22.533663247341103	55.76392714638979	191667
82ea4e5bc734c996f9a784ab95d331e1b164b847	delay defect diagnosis methodology using path delay measurements	delay estimation;integrated circuit testing;least squares approximations;timing;average first hit rank;delay defect diagnosis methodology;estimated segment delay;fast diagnosis method;inequality constrained least squares method;multiple delay defect;path delay measurements;segment delays;single delay defect;timing failure;timing information	With aggressive device scaling, timing failures have become more prevalent due to manufacturing defects and process variations. When timing failure occurs, it is important to take corrective actions immediately. Therefore, an efficient and fast diagnosis method is essential. In this paper, we propose a new diagnostic method using timing information. Our method approximately estimates all the segment delays of measured paths in a design using inequality-constrained least squares methods. Then, the proposed method ranks the possible locations of delay defects based on the difference between estimated segment delays and the expected values of segment delays. The method works well for multiple delay defects as well as single delay defects. Experiment results show that our method yields good diagnostic resolution. With the proposed method, the average first hit rank (FHR), was within 7 for single delay defect and within 8 for multiple delay defects.	failure analysis;image scaling;information retrieval;linear least squares (mathematics);seven-segment display;social inequality;software bug;timing failure	Eun Jung Jang;Jaeyong Chung;Jacob A. Abraham	2011	2011 International Symposium on Integrated Circuits		mathematical optimization;electronic engineering;real-time computing;delay calculation;elmore delay;mathematics;contamination delay	EDA	21.729809433248125	54.26886472935857	191765
a831ffbf8f5fd85c6cce6cea6c2537165c38b8ea	ultra low power asynchronous charge sharing logic	domino differential cascode voltage switch;summing circuits;clock generator;8 bit processor;circuito aritmetico;evaluation performance;ultra low power;procede de transfert;cascode connection;performance evaluation;generateur electrique;multiplying circuits;adiabatic logic;integrated circuit;procesador 8 bits;dissipation energie;montage cascode;implementation;evaluacion prestacion;electronic generator;rendement energetique;montaje cascode;circuito integrado;energy dissipation;circuito logico;transfer processing;tecnologia mos complementario;processeur 8 bits;tecnologia doble rail;logica adiabatica;circuit additionneur;circuit logique;procesamiento de transferencia;robustesse;rendimiento energetico;electric generator;generador de reloj;low power electronics;generateur electronique;circuit multiplicateur;robustness;asynchronous logic;disipacion energia;temps retard;delay time;logique adiabatique;implementacion;technologie mos complementaire;generador electronico;logic circuit;energetic efficiency;electronique faible puissance;dual rail technology;tiempo retardo;generador electrico;technologie double rail;circuit arithmetique;circuit integre;complementary mos technology;generateur horloge;arithmetic circuit;charge sharing;robustez	Asynchronous logic enables significant power reduction and high robustness in digital design. In this paper, a novel Asynchronous Charge Sharing Logic (ACSL) is proposed to achieve ultra-low dynamic and static power with little trade-off in performance. ACSL combines adiabatic logic with charge sharing technology so that the penalty of power clock generator in adiabatic circuit is eliminated while nearly 50% energy transferring efficiency is obtained. Also, by discharging all internal nodes to ground in idle mode, a saving of 75% of static power of a one-bit full adder is achieved while compared to the popular Domino Differential Cascode Voltage Switch Logic (DDCVSL) adder. Some 8-bit multipliers are built based on ACSL, PFAL (Positive Feedback Adiabatic Logic), DDCVSL and dual-rail Domino logic. All our implementations results are reported for the 45 nm CMOS process. At least 30% dynamic power reduction and more than 24% improvement of the Power-Delay Product are achieved compared to other three types of logic. Significant leakage power reductions of more than 30% can be also achieved.	charge sharing	Jiaoyan Chen;Dilip P. Vasudevan;Michel P. Schellekens;Emanuel M. Popovici	2012	J. Low Power Electronics	10.1166/jolpe.2012.1213	electronic engineering;adiabatic circuit;logic level;asynchronous circuit;logic gate;logic family;telecommunications;computer science;engineering;electrical engineering;dissipation;integrated circuit;pass transistor logic;sequential logic;pull-up resistor;implementation;8-bit;electric generator;low-power electronics;robustness	EDA	18.314990622009176	55.620265767350595	191820
f9a951f12bd2033a2bdf305c5a68949c0d1f0e2b	an efficient nbti sensor and compensation circuit for stable and reliable sram cells		Abstract Aggressive technology scaling causes unavoidable reliability issues in modern high-performance integrated circuits. The major reliability factors in nanoscale VLSI design is the negative bias temperature instability (NBTI) degradation and soft-errors in the space and terrestrial environment. In this paper, an on-chip analog adaptive body bias (OA-ABB) circuit to compensate the degradation due to NBTI aging is presented. The OA-ABB is used to compensate the parameter variations and improves the SRAM circuit yield regarding read current, hold SNM, read SNM, write margin and word line write margin (WLWM). The OA-ABB consists of standby leakage current sensor circuit, decision circuit and body bias control circuit. Circuit level simulation for SRAM cell is performed for pre- and post-stress of 10 years NBTI aging. The proposed OA-ABB reduces the effect of NBTI on the stability of SRAM cell. The simulation results show the hold SNM, read SNM and WLWM decreases by 10.55%, 8.55%, and 3.25% respectively in the absence of OA-ABB whereas hold SNM, read SNM and WLWM decreases by only 0.61%, 1.48%, and 0.72% respectively by using OA-ABB to compensate the degradation. The figure of merit of 6T SRAM cell also improved by 17.24% with the use of OA-ABB.	negative-bias temperature instability;static random-access memory	Ambika Prasad Shah;Nandakishor Yadav;Ankur Beohar;Santosh Kumar Vishvakarma	2018	Microelectronics Reliability	10.1016/j.microrel.2018.05.015	very-large-scale integration;electronic engineering;engineering;static random-access memory;negative-bias temperature instability;leakage (electronics);integrated circuit;figure of merit	EDA	19.18686476414148	59.44070697830081	192591
889b285daf6269c25ce643a209938f8a007ebf88	on-line error detection in digital microfluidic biochips	reservoirs;detectors;target bioassay protocol online error detection digital microfluidic biochips digital microfluidic technology lab on a chip life critical application biochips testing disposable biochips online test technique;circuit faults;on line testing;on line testing biochips diagnosis digital microfluidics error detection;biochips;testing;reservoirs testing circuit faults electrodes arrays transportation detectors;arrays;electrodes;microfluidics;transportation;digital microfluidics;lab on a chip;microfluidics lab on a chip;error detection;diagnosis	Digital microfluidic technology is being increasingly used for implementing a lab-on-a-chip with many life-critical applications. Testing of these biochips is thus indispensable not only after manufacture, but also during in-field operation. To keep the product cost (including both design and test) low for disposable biochips, efficient on-line test techniques are desirable. All previous on-line test mechanisms interleave testing and the target bioassay protocol, but they involve overhead such as use of separate test droplet(s) and increased completion time. In this paper, we propose a simple on-line error-detection methodology that can be performed concurrently with the normal operation of the system with no or little extra effort. The proposed procedure does not require any test droplet. In the case of incorrect operation, the error is detected on or before the completion of the bioassay. The main objective of the proposed strategy is to ensure the correctness of the executed assay on-chip and not to guarantee the absence of a defect in the chip. The given assay protocol is assumed to be executed correctly if the on-line procedure finishes with success. The assay is aborted as soon as an error is detected, thereby saving costly sample/reagents. Moreover, the scheme can be easily adopted to enhance diagnosis.	correctness (computer science);error detection and correction;interleaved memory;online and offline;overhead (computing);software bug;turing test	Debasis Mitra;Sarmishtha Ghoshal;Hafizur Rahaman;Krishnendu Chakrabarty;Bhargab B. Bhattacharya	2012	2012 IEEE 21st Asian Test Symposium	10.1109/ATS.2012.56	embedded system;transport;detector;electronic engineering;microfluidics;error detection and correction;biochip;lab-on-a-chip;engineering;electrode;digital microfluidics;software testing;reservoir	Embedded	24.225622591990636	53.62883023772341	192652
a9fbfbbab9bd4ddc670b9283ebc43305c0331108	exploiting combinatorial redundancy for offset calibration in flash adcs	flash adc;cmos integrated circuits;process variation;statistical element selection adc analog to digital converter calibration combinatorial redundancy comparator flash adc;systematics;low frequency;satisfiability;combinatorial redundancy;chip;integrated circuit design;built in self test;word length 8 bit flash adc offset calibration cmos node process variation analog design scaling random intra die variation circuit design statistical element selection methodology ses methodology post manufacturing calibration test chip digitally calibrated comparator array built in combinatorial redundancy pelgrom type sizing analog to digital converter size 65 nm;total power;redundancy analogue digital conversion built in self test calibration cmos analogue integrated circuits cmos logic circuits combinational circuits comparators circuits integrated circuit design integrated circuit testing;redundancy;comparators circuits;logic gates;comparator;redundancy calibration transistors ash cmos integrated circuits systematics logic gates;cmos analogue integrated circuits;cmos logic circuits;analogue digital conversion;transistors;adc;ash;integrated circuit testing;statistical element selection;analog to digital converter;logic gate;calibration;combinational circuits	Process variations in advanced CMOS nodes limit the benefits of scaling for analog designs. In the presence of increasing random intra-die variations, mismatch becomes a significant design challenge for circuits such as comparators. In this paper we describe and demonstrate the details of a statistical element selection (SES) methodology that relies on the combinatorial growth of subsets of selectable circuit elements (e.g., input transistors in a comparator) to provide redundancy for post-manufacturing calibration of specifications (e.g., offset). A test chip consisting of an array of digitally calibrated comparators with built-in combinatorial redundancy was manufactured in 65 nm bulk CMOS. Over 99.5% of the comparators satisfy the given offset specification compared to 15% for Pelgrom-type sizing. A second test chip in the same process consists of an 8-bit, 1.5 GS/s flash ADC and achieves 37 db SNDR at low frequencies. The total power is 35 mW, 20 mW in the S&H and 15 mW in the ADC core. The figure of merit is 0.42 pJ/conv.	8-bit;accessible surface area;analog-to-digital converter;built-in self-test;cmos;comparator;flash adc;image scaling;least significant bit;monte carlo method;performance per watt;randomness;roland gs;scsi enclosure services;sinadr;simulation;transistor	Gökçe Keskin;Jonathan E. Proesel;Jean-Olivier Plouchart;Lawrence T. Pileggi	2011	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2011.2157255	electronic engineering;real-time computing;logic gate;computer science;engineering;electrical engineering	EDA	23.908742040563617	54.61170489429377	192911
52b3daadf14cf508799410296e453c02d53d567e	adaptive techniques for overcoming performance degradation due to aging in digital circuits	adaptive technique;overcoming performance degradation;dual effect;adaptive supply voltage;nmos transistor;present-day digital circuit design;pmos transistor;positive bias temperature instability;aged circuit;adaptive body bias;hf-based high-k dielectric;negative bias temperature instability;degradation;ageing;threshold voltage;digital circuits;indexation;aging;stress;adaptive control;critical path;optimization problem;3d;logic gates;chip;circuit design;look up table	Negative Bias Temperature Instability (NBTI) in PMOS transistors has become a major reliability concern in present-day digital circuit design. Further, with the recent usage of Hf-based high-k dielectrics for gate leakage reduction, Positive Bias Temperature Instability (PBTI), the dual effect in NMOS transistors has also reached significant levels. Consequently, designers are required to build in substantial guard-bands into their designs, leading to large area and power overheads, in order to guarantee reliable operation over the lifetime of a chip. We propose a guard-banding technique based on adaptive body bias (ABB) and adaptive supply voltage (ASV), to recover the performance of an aged circuit, and compare its merits over previous approaches.	colour banding;digital electronics;elegant degradation;high-κ dielectric;integrated circuit design;nmos logic;negative-bias temperature instability;pmos logic;spectral leakage;transistor	Sanjay V. Kumar;Chris H. Kim;Sachin S. Sapatnekar	2009	2009 Asia and South Pacific Design Automation Conference		ageing;electronic engineering;real-time computing;adaptive control;computer science;engineering;electrical engineering	EDA	19.61255583294142	59.07916213897964	193892
ad9eff4c819c1e1ae09c2dda25d1d5768c711605	"""""""dfy and dfr are more important than dft"""""""	silicon;design for testability;leakage current;circuit faults;leak detection;design optimization;process design;manufacturing;circuit testing;gate leakage	The purpose of testing a circuit is to verify that it functionally matches its ideal behavior. If the behavior is not what is anticipated, it is considered failing. If the failing behavior is a result of a defective design, e.g. a “bug”, a serious re-design effort would be undertaken. This re-design is based upon knowledge of typical circuit behavior and re-design encompasses using a broad spectrum of electrical modeling. However, this only looks into one aspect of a circuit: the electrical representation. This effort does not necessarily explore the physical manifestation of the components used to create the circuit and their susceptibility to failure.	divergence-from-randomness model;failure	David M. Wu	1999		10.1109/TEST.1999.805878	reliability engineering;process design;electronic engineering;multidisciplinary design optimization;engineering;electrical engineering;design for testing;leakage;manufacturing;silicon	EDA	23.313991654248223	54.182808689424434	194045
5ebb8bb3a769397b16d90ef9cb0551659c67c7ab	statistical lifetime analysis of memristive crossbar matrix	memristor;rram;statistical analysis cmos memory circuits integrated circuit reliability matrix algebra memristors;uncertainty;lifetime distribution statistical lifetime analysis memristive crossbar matrix memristors memory technologies cmos fabrication process reliability concerns analytic approach;emerging device memristor uncertainty crossbar endurance process variability rram;crossbar;memristors resistance probability distribution monte carlo methods reliability switches gaussian distribution;conference report;process variability;emerging device;endurance	Memristors are considered one of the most favorable emerging device alternatives for future memory technologies. They are attracting great attention recently, due to their high scalability and compatibility with CMOS fabrication process. Alongside their benefits, they also face reliability concerns (e.g. manufacturing variability). In this sense our work analyzes key sources of uncertainties in the operation of the memristive memory and we present an analytic approach to predict the expected lifetime distribution of a memristive crossbar.	cmos;crossbar switch;distributed manufacturing;heart rate variability;memristor;scalability;semiconductor device fabrication	Peyman Pouyan;Esteve Amat;Antonio Rubio	2015	2015 10th International Conference on Design & Technology of Integrated Systems in Nanoscale Era (DTIS)	10.1109/DTIS.2015.7127378	electronic engineering;engineering;electrical engineering;theoretical computer science	EDA	21.985861405160236	59.10381146762365	196281
af2f191db60fe70bd1d48e235e2666ce82abdeeb	hierarchical analog/mixed-signal circuit optimization under process variations and tuning	phased locked loop analog circuit optimization mixed signal circuit optimization process variations hierarchical optimization integrated circuit design building circuit block pareto models yield aware system optimization chip manufacturability post silicon tuning self tuning function blocks;process variation;pareto optimisation;integrated circuit yield;phased locked loop;analog;mixed signal;bismuth;yield analog mixed signal optimization postsilicon tuning;phase locked loops circuit optimisation circuit tuning integrated circuit design integrated circuit yield mixed analogue digital integrated circuits pareto optimisation;analog circuit optimization;chip manufacturability;mixed signal circuit optimization;post silicon tuning;yield;phase locked loops;self tuning function blocks;integrated circuit design;process variations;tuning;building circuit block pareto models;integrated circuit modeling;charge pumps;hierarchical optimization;mixed analogue digital integrated circuits;circuit tuning;postsilicon tuning;optimization;jitter;circuit optimisation;optimization tuning bismuth phase locked loops jitter integrated circuit modeling charge pumps;yield aware system optimization;circuit optimization	A hierarchical optimization methodology is presented to achieve robust analog/mixed-signal circuit design with consideration of process variations. Hierarchical optimization using building circuit block Pareto models is an efficient approach for optimizing nominal performances of large analog circuits. However, yield-aware system optimization, as dictated by the need for safeguarding chip manufacturability in scaled technologies, is completely nontrivial. Two fundamental difficulties are addressed for achieving such a methodology: yield-aware Pareto performance characterization at the building block level and yield-aware optimization problem formulation at the system level. In addition, postsilicon tuning in complex mixed-signal system designs is investigated and the proposed optimization framework is extended for such systems. The presented methodology is demonstrated by hierarchical optimization of a phased-locked loop consisting of multiple building blocks and self-tuning function blocks.	analogue electronics;circuit design;design for manufacturability;mathematical optimization;mixed-signal integrated circuit;optimization problem;pareto efficiency;performance tuning;profiling (computer programming);program optimization;self-tuning	Guo Yu;Peng Li	2011	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2010.2071250	mixed-signal integrated circuit;control engineering;probabilistic-based design optimization;yield;electronic engineering;phase-locked loop;jitter;engineering;bismuth;control theory;process variation;integrated circuit design	EDA	24.487353110434622	57.76665183699423	196301
29814bd96f9e4b9497def9d8b7c1ca7b6572ed8f	an asynchronous circuit design with fast forwarding technique at advanced technology node	monte carlo simulation;delay variation;fabrication functionality;monte carlo simulations;size 65 nm;proposed asynchronous circuit;fast forwarding;cycle time analysis;new asynchronous circuit design;single track;asynchronous circuit;fast forwarding technique;asynchronous circuit design;cycle time;logic design;domino logic circuit;dual-rail;monte carlo methods;asynchronous circuits;advanced technology node;handshaking process;forward transition;functional success rate;temperature;fabrication;logic circuits	In this paper, a new asynchronous circuit design is presented. A special technique that enables fast forwarding is applied to the circuits, and the forward transition improves to less than 2. The handshaking process and cycle time of the asynchronous circuits are analyzed, and its performance and functionality under fabrication and temperature variations are evaluated through Monte Carlo simulations in 65 nm technology. The proposed asynchronous circuits are compared to the static and domino logic circuits to assess their delay variations and functional success rates.	asynchronous circuit;circuit design;domino logic;fast forward;handshaking;logic gate;monte carlo method;simulation	Chin-Khai Tang;Chun-Yen Lin;Yi-Chang Lu	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.117	asynchronous system;embedded system;electronic engineering;real-time computing;asynchronous circuit;computer science;statistics;monte carlo method	EDA	21.567200479427605	55.2370296228912	196609
1e1831eb0ec5940b4766799c2c49451917a4dd96	statistical static timing analysis: a new approach to deal with increased process variability in advanced nanometer technologies	process parameters;process control;statistical static timing analysis	As process parameter dimensions continue to scale down, the gap between the designed layout and what is really manufactured on silicon is increasing. Due to the difficulty in process control in nanometer technologies, manufacturing-induced variations are growing both in number and as a percent of feature size and electrical parameters. Therefore, characterization and modeling of the underlying sources of variability, along with their correlations, is becoming more and more difficult and costly.	heart rate variability;statistical static timing analysis	Davide Pandini	2007		10.1007/978-3-540-74442-9_57	real-time computing;simulation;engineering;process control	EDA	21.99690285684038	57.81125271826143	196896
ccbf179c436c275d8dfebd3b7e8835522883f14c	an experimental evaluation of the differential bics for i/sub ddq/ testing	cmos test chips;cmos integrated circuits;opens;i sub ddq testing;built in current sensor;intra layer shorts;inter layer shorts;chip;electric current measurement;fault diagnosis integrated circuit testing cmos integrated circuits electric sensing devices electric current measurement;differential bics;integrated circuit testing;fault coverage;experimental evaluation;ic testing differential bics i sub ddq testing built in current sensor cmos test chips inter layer shorts intra layer shorts opens fault coverage;ic testing;fault diagnosis;circuit testing circuit faults electrical fault detection fault detection power supplies voltage switching circuits current supplies monitoring sampling methods;electric sensing devices	"""In this paper we present an experimental study on the effectiveness of I/sub DDQ/ testing using the differential built-in current sensor (BICS) circuit. Two new test chips were designed and fabricated implementing a CMOS version of the 74181 ALU chip. In copies of this circuit we included the capability of activating 45 different """"realistic"""" CMOS faults: inter- and intra-layer shorts and opens. We examine the fault coverage of the differential BICS for these realistic faults. A significant finding of our study is that I/sub DDQ/ testing has the potential to detect several classes of """"opens"""". Moreover, these include precisely those open faults for which two pattern voltage tests can get invalidated because of transient switching states."""		Walter W. Weber;Adit D. Singh	1995		10.1109/VTEST.1995.512677	chip;embedded system;electronic engineering;fault coverage;telecommunications;computer science;engineering;electrical engineering;cmos	NLP	22.813388661574507	53.75700660850237	196931
de3e9218509037ad4d5a8de32ed29b5bc4ae15f4	24% power reduction by post-fabrication dual supply voltage control of 64 voltage domains in vddmin limited ultra low voltage logic circuits	voltage control;cmos integrated circuits;dual supply voltage;cmos technology;fine grain power supply voltage;codecs;ultra low voltage;multiple voltage domain;fine grain power supply voltage low voltage logic circuit low power dual supply voltage;frequency 300 khz post fabrication dual supply voltage control pdvc multiple voltage domain minimum operating voltage limited ultra low voltage logic circuit power consumption reduction des codec s circuit fabrication cmos technology multiple built in self testing maximum power reduction measurement size 65 nm voltage 437 mv voltage 397 mv;logic circuits;layout;frequency 300 khz;power supply;voltage 437 mv;voltage 397 mv;low voltage;maximum power reduction measurement;built in self test;low power;low voltage logic circuit;logic gates;post fabrication dual supply voltage control;des codec s circuit fabrication;cmos logic circuits;low power electronics;integrated circuit testing;multiple built in self testing;low power electronics built in self test cmos logic circuits integrated circuit testing;power reduction;power consumption;logic circuits codecs logic gates layout voltage control power measurement cmos integrated circuits;size 65 nm;power consumption reduction;pdvc;minimum operating voltage limited ultra low voltage logic circuit;power measurement	A post-fabrication dual supply voltage (V<sub>DD</sub>) control (PDVC) of multiple voltage domains is proposed for a minimum operating voltage (V<sub>DDmin</sub>)-limited ultra low voltage logic circuits. PDVC effectively reduces an average V<sub>DD</sub> below V<sub>DDmin</sub>, thereby reducing the power consumption of logic circuits. PDVC is applied to a DES CODEC'S circuit fabricated in 65nm CMOS. The layout of DES CODEC'S is divided into 64 V<sub>DD</sub> domains and each domain size is 54μm × 63.2μm. High V<sub>DD</sub> (V<sub>DDH</sub>) or low V<sub>DD</sub> (V<sub>DDL</sub>) is applied to each domain and the selection of V<sub>DD</sub>'s is performed based on multiple built-in self tests. V<sub>DDH</sub> is selected in V<sub>DDmin</sub>-critical domains, while V<sub>DDL</sub> is selected in V<sub>DDmin</sub>-non-critical domains. A maximum 24% power reduction was measured with the proposed PDVC at 300kHz, V<sub>DDH</sub> = 3D437mV, and V<sub>DDL</sub> = 3D397mV.	cmos;canonical account;codec;iteration;logic gate;ultra-low-voltage processor;value-driven design	Tadashi Yasufuku;Koji Hirairi;Yu Pu;Yun Fei Zheng;Ryo Takahashi;Masato Sasaki;Hiroshi Fuketa;Atsushi Muramatsu;Masahiro Nomura;Hirofumi Shinohara;Makoto Takamiya;Takayasu Sakurai	2012	Thirteenth International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2012.6187553	electronic engineering;real-time computing;logic gate;computer science;engineering;electrical engineering;cmos	Arch	17.41026345810294	55.913513873589544	196976
f9f826a7157ef626b32ee306b67c5657c1024682	the value of statistical testing for quality, yield and test cost improvement	test cost improvement;adaptive testing;integrated circuit yield;cost reduction;quality improvement;statistical test;yield improvement;yield estimation;manufacturing testing;statistical analysis cost reduction integrated circuit testing integrated circuit yield production testing;burn in reduction;statistical analysis;integrated circuit testing;statistics;cost effectiveness;ic manufacturing;statistical testing;statistical analysis costs logic testing integrated circuit testing large scale integration semiconductor device testing silicon cmos technology logic design materials testing;adaptive test statistical testing quality improvement yield improvement test cost improvement semiconductor test data ic manufacturing burn in reduction;deep sub micron;production testing;adaptive test;semiconductor test data	This paper is the third in a lecture series on statistical analysis of semiconductor test data. Statistical test methods offer increased value in IC manufacturing and test over traditional maverick silicon screening methods by optimizing the trade-off between yield and reliability in deep sub-micron CMOS technologies. Effective use of the vast volume of data generated by test can lead to cost-effective screening of subtle defects, burn-in reduction or elimination and test cost reduction through adaptive test	burn-in;cmos;common criteria;iddq testing;mathematical optimization;maverick framework;overhead (computing);rejection sampling;semiconductor;software bug;statistical model;test data;throughput;variance reduction	Robert Madge;Brady Benware;Mark Ward;W. Robert Daasch	2005	IEEE International Conference on Test, 2005.	10.1109/TEST.2005.1583990	reliability engineering;statistical hypothesis testing;quality management;electronic engineering;engineering;test compression;computerized adaptive testing;statistics	EDA	22.912718926047933	54.84815874949193	197329
48d8976365401be283fc1ab81074da835749844e	metastability challenges for 65nm and beyond; simulation and measurements		Recent synchronizer metastability measurements indicate degradation of MTBF with technology scaling, calling for measurement and calibration circuits in 65nm and below. Degradation of parameters can be even worse if the system is operated at extreme supply voltages and temperature conditions. In this work we study the behavior of synchronizers in a broad range of supply voltage and temperature corners. A digital on-chip measurement system is presented that helps to characterize synchronizers in future technologies and a new calibrating system is shown that accounts for changes in delay values due to supply voltage and temperature changes. We present a detailed comparison of measurements and simulations for a fabricated 65nm bulk CMOS circuit and discuss implications of the measurements for synchronization systems in 65nm and beyond. We propose an adaptive self-calibrating synchronizer to account for supply voltage, temperature, global process variations and DVFS.	cmos;dynamic voltage scaling;elegant degradation;image scaling;john d. wiley;mean time between failures;simulation;synchronization (computer science);synchronizer (algorithm);system of measurement	Salomon Beer;Ran Ginosar;Jerome Cox;Tom Chaney;David M. Zar	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)			EDA	20.255580488043673	59.38788184286775	197540
83a4993666a8635565cafbc20b0f6f026ac06f02	independent n and p process monitors for body bias based process corner correction	size 55 nm process monitors on die pmos nmos digital outputs integrated blocks soc designs logic circuits die as fabricated process corners corner skew lots power correction digital circuits parametric yield correction logic ring oscillators embedded microprocessors sram read current variability body bias based process corner correction;sram chips logic circuits mosfet oscillators process monitoring semiconductor device testing;monitoring delays mos devices random access memory current measurement mirrors system on chip	Process monitors that independently sense on-die PMOS and NMOS as-fabricated performance are presented. The monitors provide digital outputs, making them easily integrated blocks on SOC designs. We present two monitor approaches, one primarily analog and one primarily digital, applied to both logic circuits and SRAM, which may require different optimal body biases for best operation. The monitors correctly sense the die as-fabricated process corners on 55-nm test die, demonstrated on FF, FS, TT, SF SS corner skew lots. Experimentally measured performance and power correction is demonstrated for digital circuits, as well as parametric yield correction for SRAMs. Logic ring oscillators demonstrate 74% reduction in standard deviation for delay and leakage with monitor specified body biases. Similar improvement is demonstrated on embedded microprocessors. Finally, 55% and 72% reduction in SRAM read current variability and leakage, respectively, is also shown.	brute-force search;die (integrated circuit);digital electronics;embedded system;experiment;heart rate variability;logic gate;microprocessor;nmos logic;pmos logic;process corners;spectral leakage;static random-access memory;system on a chip	Lawrence T. Clark;David Kidd;Vineet Agrawal;Samuel Leshner;Gokul Krishnan	2014	Proceedings of the IEEE 2014 Custom Integrated Circuits Conference	10.1109/CICC.2014.6946092	embedded system;electronic engineering;engineering;electrical engineering	EDA	18.921326285328647	58.39436845260678	198072
6a565ebe52152d3227447e1cb7c9322b18653748	advanced timing analysis based on post-opc extraction of critical dimensions	optical distortion;critical dimension;silicon;analytical models;opc;embedded design flow;timing characterization advanced timing analysis post opc extraction critical dimensions optical proximity correction on silicon chip performance on silicon performance residual opc errors full chip layouts speed path characterization critical gates matching transistors embedded design flow multilayer extraction;proximity effect lithography;integrated circuit layout;timing characterization;logic design;matching transistors;critical gates;post opc extraction;optical proximity correction;timing performance analysis analytical models permission circuits optical distortion nanoscale devices optical design optical devices silicon;logic design integrated circuit layout proximity effect lithography;design flow;full chip layouts;speed path characterization;process cd;layout;chip;permission;nanoscale devices;residual opc errors;performance analysis;timing analysis;place and route;circuits;multilayer extraction;on silicon performance;optical design;advanced timing analysis;critical dimensions;on silicon chip performance;optical devices;timing	While performance specifications are verified before sign-off for a modern nanometer scale design, extensive application of optical proximity correction substantially alters the layout introducing systematic variations to the simulated and verified performance. As a result, actual on-silicon chip performance is quite different from sign-off expectations. This paper presents a new methodology to provide better estimates of on-silicon performance. The technique relies on the extraction of residual OPC errors from placed and routed full chip layouts to derive actual (i.e., calibrated to silicon) CD values that are then used in timing analysis and speed path characterization. This approach is applied to a state-of-the-art microprocessor and contrasted with traditional design flow practices where ideal (i.e., drawn) Lgate values are employed, leading to a subsequent lack of predictive power. We present a platform for diagnosing and improving OPC quality on gates with specific functionality such as critical gates or matching transistors. Furthermore, with more accurate timing analysis we highlight the necessity of a post-OPC verification embedded design flow, by showing substantial differences in the Si-based timing simulations in terms of significant reordering of speed path criticality and a 36.4% increase in worst-case slack. Extensions of this methodology to multi-layer extraction and timing characterization are also proposed.	best, worst and average case;design flow (eda);embedded system;integrated circuit;layer (electronics);microprocessor;open platform communications;routing;self-organized criticality;simulation;slack variable;static timing analysis;transistor	Jie Yang;Luigi Capodieci;Dennis Sylvester	2005	Proceedings. 42nd Design Automation Conference, 2005.	10.1145/1065579.1065676	chip;layout;electronic circuit;electronic engineering;ole for process control;logic synthesis;real-time computing;computer science;engineering;design flow;electrical engineering;place and route;integrated circuit layout;silicon;engineering drawing;static timing analysis;critical dimension;optical proximity correction	EDA	21.632978623808764	57.19827426667109	198249
adf460b3de7f3d418346c50e267d37a187746ba6	neutron radiation test of graphic processing units	barium testing helium dh hemts;single event latchup;logic circuits;gpu;failure analysis;life testing;graphics processing units;single event transient;integrated circuit testing;storage management chips;single event latchup neutron radiation testing gpu single event upset single event transient;storage management chips failure analysis graphics processing units integrated circuit reliability integrated circuit testing life testing logic circuits neutron effects;single event upset;integrated circuit reliability;neutron effects;logic resources commercial off the shelf graphic processing unit gpu neutron radiation testing campaigns accelerated radiation experiments neutrons sensitivity failure in time gpu memory fit sea level;neutron radiation testing	This paper reports and analyzes the results of neutrons radiation testing campaigns on a modern commercial-off-the-shelf Graphic Processing Unit (GPU). A set of guidelines for accelerated radiation experiments on CPUs is presented, emphasizing the shrewdness necessary to ease the test and gain meaningful data. Radiation test results are presented and discussed, highlighting the neutrons sensitivities of the different GPU memory and logic resources in terms of Failure In Time (FIT) due to neutrons at sea level.	central processing unit;experiment;graphics processing unit	Paolo Rech;Caroline Aguiar;Ronaldo Rodrigues Ferreira;Christopher Frost;Luigi Carro	2012	2012 IEEE 18th International On-Line Testing Symposium (IOLTS)	10.1109/IOLTS.2012.6313841	reliability engineering;embedded system;failure analysis;electronic engineering;real-time computing;logic gate;engineering	Embedded	20.487924952593396	56.93681229825364	198339
42b2bac9086995078ba9a12626a12abacef2ee9f	experimental study of the adaptive body bias on-chip (abboc) for bias temperature instability (bti) and process variations (pv) compensation		Sub-micron CMOS Technology scaling imposes several challenges on the designers, especially for high speed applications, such as reliability and variability. Inserting an Adaptive Body Bias on-Chip (ABBoC) to monitor the variations of the threshold voltage due to the Bias Temperature Instability (BTI) and Process Variations (PV) and adapt the body bias voltage to compensate the threshold voltage change is experimentally studied in this work. In this work, 23 dies, emulating a microprocessor critical path, are fabricated using TSMC 65nm CMOS technology. Each die has an ABBoC that can be enabled to compensate for the threshold voltage variations. These dies are fabricated in July 2011 and the clock frequency and total power consumption are measured every year for 6 consecutive years till July 2017. The measured clock frequency and total power consumption are measured when the ABBoC is disabled, and when it is enabled. From this experimental study, it has been shown that the ABBoC improves the total dies yield from 65.2% to 100% after 2 years aging time and from 0% to 82.6% after 6 years aging time. Also, this study shows that the total dies yield without the ABBoC is degraded from 82.6% to 0% after 6 years aging.	biasing;btrieve;cmos;clock rate;critical path method;die (integrated circuit);emulator;experiment;image scaling;microprocessor;negative-bias temperature instability;spatial variability	Hassan Mostafa	2018	2018 7th International Conference on Modern Circuits and Systems Technologies (MOCAST)	10.1109/MOCAST.2018.8376627	microprocessor;chip;threshold voltage;clock rate;mosfet;electrical engineering;biasing;logic gate;materials science;cmos	EDA	19.685194827134204	59.2192192004378	198752
257de9c8b123e815834ec65cd66622c6f9771962	predicting shot-level sram read/write margin based on measured transistor characteristics	test time reduction;time measurement;size 28 nm static random access memory cell shot level sram read write margin transistor characteristics sram array test read write metrics model fitting framework sram cells lithography united microelectronics corporation process technology read static noise margin;training;transistors integrated circuit measurement integrated circuit modelling integrated circuit noise integrated circuit testing sram chips;sram cells transistors training radio frequency voltage measurement time measurement;sram characterization;test time reduction array test structure model fitting process monitor sram characterization;sram cells;radio frequency;transistors;process monitor;model fitting;article;array test structure;voltage measurement	An SRAM-array test structure provides the capability of directly measuring the characteristics of each transistor and the read/write metrics for each static random access memory (SRAM) cell in the array. However, the total test time of measuring the read/write metrics takes longer than that of measuring each transistor's characteristics. This paper presents a model-fitting framework to predict the average read/write metrics of the SRAM cells in a lithography shot using only the measured transistor characteristics. The proposed framework is validated through the measurement result of 4750 samples of a 128-bit SRAM-array test structure implemented in a United Microelectronics Corporation 28-nm process technology. The experimental results show that the learned models can achieve at least 97.77% R-square on fitting the shot-level read static noise margin, write margin, and read current based on 2375-sample testing data.	128-bit;bit cell;cell (microprocessor);curve fitting;die shrink;experiment;metal gate;noise margin;random access;semiconductor device fabrication;software release life cycle;static random-access memory;transistor;vii	Shu-Yung Bin;Shih-Feng Lin;Ya Ching Cheng;Wen-Rong Liau;Alex Hou;Mango C.-T. Chao	2016	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2015.2418998	electronic engineering;real-time computing;engineering;electrical engineering;radio frequency;transistor;quantum mechanics;time	EDA	22.350929268137147	55.6873463867689	198793
6ac86bbb2502b6ae117b8eaa1bc634fc5703eb03	design of high-performance cmos level converters considering pvt variations		CMOS SoCs can reduce power consumption while maintaining performance by adopting voltage scaling (VS) technologies. The operating speed of the level converter (LC) strongly affects the effectiveness of VS technologies. However, PVT variations can cause serious problems to the LC, because the state-of-the-art LC designs do not give enough attention to this issue. In this work, we proposed to analyze the impact of PVT variations on the performance of the LC using a previously developed heuristic sizing methodology. Based on the evaluation results from different operating corners with different offset voltages and temperatures, we proposed a variation-tolerant LC that achieves both high performance and low energy with a high tolerability for PVT variations.	cmos	Jinn-Shyan Wang;Yu-Juey Chang;Chingwei Yeh	2011	IEICE Transactions		system on a chip;electronic engineering;input offset voltage;telecommunications;computer science;engineering;electrical engineering;integrated circuit;low voltage;dimensioning;low-power electronics	HPC	19.67992032566062	58.502475951071034	199505
ef47afd347048df81a14e7c98339a303993a6f2f	improving the power-delay product in scl circuits using source follower output stage	source coupled logic scl;ultra low power;subthreshold;cmos technology;subthreshold scl stscl;current 10 na;size 0 18 mum;buffer circuits;voltage 0 6 v;source coupled logic;source follower buffers;cmos logic circuits tail energy consumption logic circuits cmos technology threshold voltage capacitance coupling circuits voltage control mos devices;integrated circuit modelling;low power electronics buffer circuits cmos logic circuits integrated circuit modelling;cmos logic circuits;low power electronics;power delay product;voltage 0 6 v power delay product source follower buffers source coupled logic circuits low power electronics cmos technology size 0 18 mum current 10 na;current mode logic cml;source coupled logic circuits	This article explores the effect of using source follower buffers (SFB) at the output of source coupled logic (SCL) circuits. This technique can help to improve the power-delay product (PDP) of an SCL gate approximately by a factor of two. The proposed approach has been applied to improve the PDP in sub-threshold SCL circuits that have been developed for ultra- low power applications. Designed in conventional digital 0.18mum CMOS technology, the proposed SCL gate utilizing SFB at the output achieves a PDP of 0.5fJ/fF/gate while the gate draws 10nA from a 0.6V supply voltage.	blue (queue management algorithm);cmos;power–delay product;simulation;structured text	Armin Tajalli;Frank K. Gürkaynak;Yusuf Leblebici;Massimo Alioto;Elizabeth J. Brauer	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541375	electronic engineering;real-time computing;computer science;engineering;electrical engineering;operating system;cmos;subthreshold conduction;low-power electronics	Arch	18.025632099983024	57.830616633551024	199569
