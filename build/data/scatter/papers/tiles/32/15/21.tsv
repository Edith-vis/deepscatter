id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
86018b864ab7fcda50809486d85a6005ee92f592	keynote address ii: how's the parallel computing revolution going?	biomedical monitoring;software;computer languages;technological innovation;software developer;parallel computing revolution;performance improvement;scalable virtual machine services;virtual machines;awards activities;parallel computing revolution hardware vendors chip multiprocessors software developer performance improvement software innovation parallel performance virtuous cycle scalable virtual machine services;chip multiprocessors;virtual machines microprocessor chips multiprocessing systems parallel processing;multiprocessing systems;parallel processing;parallel performance virtuous cycle;microprocessor chips;software innovation;hardware vendors;awards activities software computer languages technological innovation parallel processing biomedical monitoring	Two trends changed the computing landscape over the past decade: (1) hardware vendors started delivering chip multiprocessors (CMPs) instead of uniprocessors, and (2) software developers increasingly chose managed languages instead of native languages. Unfortunately, the former change is disrupting the virtuous-cycle between performance improvements and software innovation. Establishing a new parallel performance virtuous cycle for managed languages will require scalable applications executing on scalable Virtual Machine (VM) services, since the VM schedules, monitors, compiles, optimizes, garbage collects, and executes together with the application. This talk describes current progress, opportunities, and challenges for scalable VM services. The parallel computing revolution urgently needs more innovations.	aquaporin 1;choose (action);computation (action);digital revolution;parallel computing;programming languages;scalability;schedule (document type);software developer;uniprocessor system;virtual machine;executing - querystatuscode	Kathryn S. McKinley	2011	2011 IEEE 17th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2011.5749730	parallel processing;parallel computing;real-time computing;computer science;virtual machine;operating system;programming language	Arch	-8.051954379408896	46.32855803302572	145169
2b4bfac9b6f3931ee9f5af3c940c6e162ea2daa2	jacev: a programming and execution environment for asynchronous iterative computations on volatile nodes	asynchronous iterative algorithms;library design;desktop grid computing;execution environment;computational science problems;volatile nodes	In this paper we present JaceV, a multi-threaded Java based library designed to build asynchronous parallel iterative applications (with direct communications between computation nodes) and execute them in a volatile environment. We describe the components of the system and evaluate the performance of JaceV with the implementation and execution of an iterative application with volatile nodes.	computation	Jacques M. Bahi;Raphaël Couturier;Philippe Vuillemin	2006		10.1007/978-3-540-71351-7_7	parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-9.530600820803748	40.79448408872766	145402
dcfb21c447d72dea1bc91d00b2fcd9b003b83962	hybrid address spaces	parallel programming models;runtime systems;many core processors	HighlightsHybrid address spaces, as a new approach for software on non-coherent many-cores.Application-specific software-controlled coherence for non-coherent many-cores.Scalable, staged MapReduce runtime system on Intel SCC using hybrid address spaces.Scalable sorting and all-to-all exchange algorithms using on-chip communication. This paper introduces hybrid address spaces as a fundamental design methodology for implementing scalable runtime systems on many-core architectures without hardware support for cache coherence. We use hybrid address spaces for an implementation of MapReduce, a programming model for large-scale data processing, and the implementation of a remote memory access (RMA) model. Both implementations are available on the Intel SCC and are portable to similar architectures. We present the design and implementation of HyMR, a MapReduce runtime system whereby different stages and the synchronization operations between them alternate between a distributed memory address space and a shared memory address space, to improve performance and scalability. We compare HyMR to a reference implementation and we find that HyMR improves performance by a factor of 1.71i? over a set of representative MapReduce benchmarks. We also compare HyMR with Phoenix++, a state-of-art implementation for systems with hardware-managed cache coherence in terms of scalability and sustained to peak data processing bandwidth, where HyMR demonstrates improvements of a factor of 3.1i? and 3.2i? respectively. We further evaluate our hybrid remote memory access (HyRMA) programming model and assess its performance to be superior of that of message passing.		Anastasios Papagiannis;Dimitrios S. Nikolopoulos	2014	Journal of Systems and Software	10.1016/j.jss.2014.06.058	computer architecture;parallel computing;real-time computing;computer science;physical address;operating system;programming language	OS	-8.618124151765159	46.14565012252819	146505
b279560a8556a66e49360845c0d773d07b0f9795	accelerating code on multi-cores with fastflow	paper;lock free synchronization;performance;offload;programming techniques;package;patterns;computer science;c;multi core;memory	FastFlow is a programming framework specifically targeting cache-coherent shared-memory multicores. It is implemented as a stack of C++ template libraries built on top of lock-free (and memory fence free) synchronization mechanisms. Its philosophy is to combine programmability with performance. In this paper a new FastFlow programming methodology aimed at supporting parallelization of existing sequential code via offloading onto a dynamically created software accelerator is presented. The new methodology has been validated using a set of simple micro-benchmarks and some real applications. keywords: offload, patterns, multi-core, lock-free synchronization, C++.	c++;cache coherence;coherence (physics);library (computing);memory barrier;multi-core processor;non-blocking algorithm;parallel computing;shared memory;software development process	Marco Aldinucci;Marco Danelutto;Peter Kilpatrick;Massimiliano Meneghin;Massimo Torquati	2011		10.1007/978-3-642-23397-5_17	multi-core processor;computer architecture;parallel computing;real-time computing;performance;computer science;operating system;database;distributed computing;pattern;memory;programming language;package	Arch	-6.871949335019676	45.025618638496525	146633
c2ae36a5f7d947ccd8992cde7f60ea53e3531f53	design and implementation of mapreduce using the pgas programming model with upc	distributed memory;programmability;performance evaluation;software performance evaluation c language distributed shared memory systems parallel languages parallel programming;software performance evaluation;large data sets;parallel programming;unified parallel c;collective primitives;partitioned global address space;upc;programming model;instruction sets electronics packaging libraries multicore processing programming java;c language;hpc;low latency;design and implementation;distributed environment;distributed shared memory systems;high performance computer;load distribution;mapreduce;parallel programs;parallel languages;parallel applications;collective primitives upc mapreduce hpc programmability;shared memory environments mapreduce distributed environments high performance computing partitioned global address space programming model unified parallel c language parallel programming performance evaluation distributed memory environments customized load distribution	MapReduce is a powerful tool for processing large data sets used by many applications running in distributed environments. However, despite the increasing number of computationally intensive problems that require low-latency communications, the adoption of MapReduce in High Performance Computing (HPC) is still emerging. Here languages based on the Partitioned Global Address Space (PGAS) programming model have shown to be a good choice for implementing parallel applications, in order to take advantage of the increasing number of cores per node and the programmability benefits achieved by their global memory view, such as the transparent access to remote data. This paper presents the first PGAS-based MapReduce implementation that uses the Unified Parallel C (UPC) language, which (1) obtains programmability benefits in parallel programming, (2) offers advanced configuration options to define a customized load distribution for different codes, and (3) overcomes performance penalties and bottlenecks that have traditionally prevented the deployment of MapReduce applications in HPC. The performance evaluation of representative applications on shared and distributed memory environments assesses the scalability of the presented MapReduce framework, confirming its suitability.	code;collective operation;distributed memory;library (computing);load balancing (computing);locality of reference;mapreduce;parallel computing;partitioned global address space;performance evaluation;programming model;scalability;shared memory;software deployment;unified parallel c (upc);usability	Carlos Teijeiro;Guillermo L. Taboada;Juan Touriño;Ramón Doallo	2011	2011 IEEE 17th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2011.162	computer architecture;supercomputer;parallel computing;distributed memory;computer science;weight distribution;partitioned global address space;operating system;database;distributed computing;universal product code;programming paradigm;programming language;distributed computing environment;low latency	HPC	-11.158861522273304	44.7443147689217	146817
a96cd7e23bd578c21e242e89d3c496ca9a9ac18c	dynamic translation of structured loads/stores and register mapping for architectures with simd extensions		More and more modern processors have been supporting non-contiguous SIMD data accesses. However, translating such instructions has been overlooked in the Dynamic Binary Translation (DBT) area. For example, in the popular QEMU dynamic binary translator, guest memory instructions with strides are emulated by a sequence of scalar instructions, leaving a significant room for performance improvement when the host machines have SIMD instructions available. Structured loads/stores, such as VLDn/VSTn in ARM NEON, are one type of strided SIMD data access instructions. They are widely used in signal processing, multimedia, mathematical and 2D matrix transposition applications. Efficient translation of such structured loads/stores is a critical issue when migrating ARM executables to other ISAs. However, it is quite challenging since not only the translation of structured loads/stores is not trivial, but also the difference between guest and host register configurations must be taken into consideration. In this work, we present the design and implementation of translating structured loads/stores in DBT, including target code generation as well as efficient SIMD register mapping. Our proposed register mapping mechanisms are not limited to handling structured loads/stores, they can be extended to deal with normal SIMD instructions. On a set of OpenCV benchmarks, our QEMU-based system has achieved a maximum speedup of 5.41x, with an average improvement of 2.93x. On a set of BLAS benchmarks, our system has also obtained a maximum speedup of 2.19x and an average improvement of 1.63x.	arm architecture;blas;binary translation;cpu cache;central processing unit;code generation (compiler);data access;executable;just-in-time compilation;microprocessor;opencv;overhead (computing);register file;simd;signal processing;skylake (microarchitecture);speedup;x86	Sheng-Yu Fu;Ding-Yong Hong;Yu-Ping Liu;Jan-Jan Wu;Wei-Chung Hsu	2017		10.1145/3078633.3081029	programming language;parallel computing;real-time computing;code generation;signal processing;speedup;computer science;binary number;executable;data access;binary translation;simd	Arch	-5.67419075945388	46.20021396873595	146826
b2fd651e6e602b538f3c2a3534e2b15cefbc004a	a domain-specific high-level programming model	digital signal processing;auto tuning;heterogeneous cluster;unfolding techniques;programming model;data flow graph	Nowadays, computing hardware continues to move toward more parallelism and more heterogeneity, to obtain more computing power. From personal computers to supercomputers, we can find several levels of parallelism expressed by the interconnections of multi-core and many-core accelerators. On the other hand, computing software needs to adapt to this trend, and programmers can use parallel programming models PPM to fulfil this difficult task. There are different PPMs available that are based on tasks, directives, or low-level languages or library. These offer higher or lower abstraction levels from the architecture by handling their own syntax. However, to offer an efficient PPM with a greater additional high-level abstraction level while saving on performance, one idea is to restrict this to a specific domain and to adapt it to a family of applications. In the present study, we propose a high-level PPM specific to digital signal-processing applications. It is based on data-flow graph models of computation, and a dynamic run-time model of execution StarPU. We show how the user can easily express this digital signal-processing application and can take advantage of task, data, and graph parallelism in the implementation, to enhance the performances of targeted heterogeneous clusters composed of CPUs and different accelerators e.g., GPU and Xeon Phi. Copyright © 2015 John Wiley & Sons, Ltd.	domain-specific language;high- and low-level;high-level programming language;programming model	Farouk Mansouri;Sylvain Huet;Dominique Houzet	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3622	parallel computing;real-time computing;computer science;theoretical computer science;operating system;digital signal processing;data-flow analysis;database;distributed computing;data parallelism;programming paradigm;programming language;task parallelism	PL	-5.899688899814474	44.29960827649607	146967
55896d996e2842d18d14ab46795dc32fadfcd614	load balancing for a class of irregular and dynamic problems: region growing image segmentation algorithms	electrical capacitance tomography;image segmentation;data compression;identity based encryption;gas insulated transmission lines;parallel implementations;data mining;programming model;split and merge approac;data dependence;segmentation algorithm;load management;load balancing;merging;unpredictable load fluctuations parallel implementations segmentation algorithm split and merge approac image segmentation load balancing region growing;unpredictable load fluctuations;load distribution;image analysis;load management image segmentation gas insulated transmission lines merging identity based encryption electrical capacitance tomography data compression image analysis data mining tin;load balance;parallel implementation;tin;region growing;parallel algorithms image segmentation;single program multiple data;parallel algorithms	This paper discusses and evaluates a parallel implementations of a segmentation algorithm based on the Split-andMerge approach. The solution has been conceived for a multiprocessor using the SPMD (Single Program Multiple Data) programming model and executions have been carried out on a Cray-T3E system. Our main goal is intended to describe our experiences in solving the region growing problem, which is representative of a class of non-uniform problems, characterized by a behavior that is data dependent. Since this problem exhibits unpredictable load fluctuations, it requires the use of load balancing schemes to achieve efficient parallel solutions. We also propose and analize several strategies for the selection of the region identifiers and its influence on the execution time and the load distribution.	algorithm;cray t3e;identifier;image segmentation;load balancing (computing);multiprocessing;programming model;region growing;run time (program lifecycle phase);spmd	Maria Dolores Gil Montoya;Consolación Gil;Inmaculada García	1999		10.1109/EMPDP.1999.746660	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-7.314800440850386	40.964849033005386	147511
21d7928c52def00e4a2d8035ebb755e4c146d376	ttl-compatible multiport bus.				Chris S. Wallace;David Koch	1985	Comput. Syst. Sci. Eng.		parallel computing;distributed computing;computer science	DB	-9.873263888130873	43.16036506432006	148083
1092ffeb20bba8ede39e13a7129f8556951b6f32	tempera: template massively parallel library for efficient n-dimensional signal processing	richardson lucy deconvolution algorithm tempera library template massively parallel library n dimensional signal processing c library parallel signal processing image deconvolution point wise algebraic operations generic algorithms linear operators cpu gpu cuda graphics processing unit compute unified device architecture policy based template design;parallel programming algebra c language deconvolution genetic algorithms graphics processing units image processing;libraries graphics processing units kernel transforms signal processing algorithms instruction sets transfer functions;image deconvolution cuda template metaprogramming signal restoration	We present TEMPeRA: a C++ library for efficient parallel signal processing, with a focus on image deconvolution. TEMPeRA makes porting new algorithms from MATLAB to C++ easier than with the conventional methods. The library provides a class to describe a signal, with main point-wise algebraic operations, that is compatible with standard library generic algorithms. Interface for linear operators is also provided, suitable for modeling the blurring effect introduced by acquisition devices, such as telescopes or microscopes. Both classes defined in the library can exploit either CPU or GPU, using CUDA: this allows the end user to write device independent code. Moreover, thanks to policy-based template design, support for different architectures is introduced. The library is then exploited for the implementation of Richardson-Lucy deconvolution algorithm. Benchmark results shows remarkable speedup when comparing serial (CPU) code and parallel (CUDA) implementation.	algorithm;blas;benchmark (computing);c++;cuda;central processing unit;deblurring;design rationale;device independence;directive (programming);discrete fourier transform;fftw;fast fourier transform;generic function;graphics processing unit;linear algebra;matlab;map (parallel pattern);opencl api;openmp;parallel computing;programmer;richardson number;richardson–lucy deconvolution;shared memory;signal processing;speedup;standard library;template metaprogramming	Riccardo Zanella;Francesco Ceccon	2014	2014 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2014.6903748	multidimensional signal processing;parallel computing;computer science;theoretical computer science;digital image processing;general-purpose computing on graphics processing units;computer graphics (images)	HPC	-5.650617342302309	42.54133144210126	148161
22a5c5b366e56339b34a66ce2a4a106592656e40	a model of dynamic compilation for heterogeneous compute platforms	nvidia geforce gtx 280;paper;heterogeneous systems;nvidia geforce 9800 gx2;cuda;ptx;thesis;nvidia;computer science;opencl		compiler;dynamic compilation	Andrew Kerr	2012			computational science;computer architecture;parallel computing;computer science	PL	-7.536373321376239	42.59854869748637	148365
6365578f56228762eebe6da2f62a382d6e277c80	load balancing strategies for symbolic vision computations	high performance computing platforms load balancing strategies symbolic vision computations high level vision algorithms geometric constraints symbolic search operations distributed memory machines ibm sp2 cray t3d;load balancing strategies;distributed memory systems;symbolic vision computations;resource allocation;distributed memory machine;satisfiability;computer vision;high performance computing platforms;high performance computer;load management computer vision distributed computing concurrent computing runtime library partitioning algorithms portable computers monitoring scholarships image matching;distributed memory machines;symbolic search operations;cray t3d;load balance;high level vision algorithms;ibm sp2;geometric constraints;distributed memory systems computer vision resource allocation	Most intermediate and high-level vision algorithms manipulate symbolic features. A key operation in these vision algorithms is to search symbolic features satisfying certain geometric constraints. Parallelizing this symbolic search needs a non-trivial algorithmic technique due to the unpredictable workload. In this paper, we propose load balancing strategies for parallelizing symbolic search operations on distributed memory machines. By using an initial workload estimate, we rst partition the computations such that the workload is distributed evenly across the processors. In addition, we perform task migrations dynamically to adapt to the evolving workload. To demonstrate the usefulness of our load balancing strategies, experiments were conducted on an IBM SP2 and a Cray T3D. Our results show that our task migration strategy can balance the unpredictable workload with little overhead. Our code using C and MPI is portable onto other high performance computing platforms.	algorithm;automatic parallelization;central processing unit;computation;cray t3d;distributed memory;experiment;high- and low-level;load balancing (computing);message passing interface;overhead (computing);parallel computing;supercomputer	Yongwha Chung;Jongwook Woo;Ramakant Nevatia;Viktor K. Prasanna	1996		10.1109/HIPC.1996.565833	parallel computing;resource allocation;computer science;load balancing;theoretical computer science;operating system;distributed computing;programming language;satisfiability	HPC	-7.448213430325443	45.02165685636898	148496
759f71604058fc891607f54dd122bc994197b768	xenix and the motorola 68000 family		Abstract   The main features of the Xenix time-sharing system are discussed, paying particular attention to those aspects of the operating system which relate most closely to the hardware on which the system is running. In particular the architecture of the CPU, the memory management hardware, and the disc system are discussed.	motorola 68000	Bill Bateson	1984	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(84)90250-3	embedded system;parallel computing;real-time computing;computer hardware;operating system	EDA	-9.027214832415401	45.09305612413545	148544
0342642615d33cfef16b957a79e1de07ca0a517d	a heterogeneous gasnet implementation for fpga-accelerated computing		This paper introduces an effort to incorporate reconfigurable logic (FPGA) components into the Partitioned Global Address Space model. For this purpose, we have implemented a heterogeneous implementation of GASNet that supports distributed applications with software and hardware components and easy migration of kernels from software to hardware. We present a use case and preliminary performance numbers.	distributed computing;field-programmable gate array;partitioned global address space;reconfigurable computing	Ruediger Willenberg;Paul Chow	2014		10.1145/2676870.2676885	computer architecture;parallel computing;real-time computing;computer science	HPC	-7.937206177385571	44.21703424545477	148809
3dcd04c85c77ec18d89cc62c9388fe42ca6dad6f	efficient parallelization for amr mhd multiphysics calculations; implementation in astrobear	cluster computing;memory management;high performance computing;adaptive mesh refinement;tree structure;parallel;threading;load balance;distributed tree	Current AMR simulations require algorithms that are highly parallelized and manage memory efficiently. As compute engines grow larger, AMR simulations will require algorithms that achieve new levels of efficient parallelization and memory management. We have attempted to employ new techniques to achieve both of these goals. Patch or grid based AMR often employs ghost cells to decouple the hyperbolic advances of each grid on a given refinement level. This decoupling allows each grid to be advanced independently. In AstroBEAR we utilize this independence by threading the grid advances on each level with preference going to the finer level grids. This allows for global load balancing instead of level by level load balancing and allows for greater parallelization across both physical space and AMR level. Threading of level advances can also improve performance by interleaving communication with computation, especially in deep simulations with many levels of refinement. To improve memory management we have employed a distributed tree algorithm that requires processors to only store and communicate local sections of the AMR tree structure with neighboring processors.	adaptive multi-rate audio codec;algorithm;automatic parallelization;central processing unit;computation;coupling (computer programming);forward error correction;grid computing;list of algorithms;load balancing (computing);memory management;multiphysics;parallel computing;parallel programming model;refinement (computing);simulation;thread (computing);tree structure	Jonathan Carroll-Nellenback;Brandon Shroyer;Adam Frank;Chen Ding	2013	J. Comput. Physics	10.1016/j.jcp.2012.10.004	supercomputer;parallel computing;real-time computing;adaptive mesh refinement;threading;computer cluster;computer science;load balancing;theoretical computer science;parallel;tree structure;memory management	HPC	-6.266783146351992	40.840403594814376	149060
088429afa48de98a5175f2a6d971055573ec07d5	understanding the performance of streaming applications deployed on hybrid systems	coarse grained dataflow computation;application development;programming environments;application software;pipelined computation;hybrid computing architecture;acceleration;lan interconnection;computer architecture;streaming media;memory architecture;hybrid system;performance gain;data flow computing;streaming media field programmable gate arrays application software computer architecture graphics lan interconnection computer science performance gain acceleration memory architecture;streaming application;computer science;field programmable gate arrays;coarse grained dataflow computation performance gain streaming application hybrid computing architecture auto pipe application development environment pipelined computation;auto pipe application development environment;graphics;pipeline processing;programming environments data flow computing pipeline processing	Significant performance gains have been reported by exploiting the specialized characteristics of hybrid computing architectures for a number of streaming applications. While it is straightforward to physically construct these hybrid systems, application development is often quite difficult. We have built an application development environment, Auto-Pipe, that targets streaming applications deployed on hybrid architectures. Here, we describe some of the current and future characteristics of the Auto-Pipe environment that facilitate an understanding of the performance of an application that is deployed on a hybrid system.	hybrid system;software development kit;tracing (software)	Joseph M. Lancaster;Ron Cytron;Roger D. Chamberlain	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536381	acceleration;computer architecture;application software;parallel computing;real-time computing;computer science;graphics;operating system;distributed computing;rapid application development;field-programmable gate array;hybrid system	Arch	-8.021639814043226	45.31390463890433	149364
5b13f467b66e7f6cdd4c54506b8370452e49c341	pesa i-a parallel architecture for production systems	production system			Franz Schreiner;Gerhard Zimmermann	1987			parallel computing;distributed computing;architecture;computer science	HPC	-9.830008462290692	42.6409034726278	149458
6eb028956cc0c88aeb7b769b2f825ff38b85df90	design of a recursively structured parallel computer	distributed system;multiprocessor systems;parallel computer;liveness;deadlock;minimal;petri net;strongly connected;high performance;high speed;parallel processing;trap;complete	There is several types of high performance parallel processing systems, which are developed in order to allow a fast solution for the high complexity problems. Some of these systems, such as: high speed local networks and distributed systems, are implemented in spread environments. Other systems, such as: pipeline computers, array processors, multiprocessor systems, dataflow computers and VLSI computer structures, are implemented in confined environments [1]. This work shows the design and the development of a parallel computer, which can be used in spread or confined environments.	central processing unit;computer;dataflow;distributed computing;multiprocessing;parallel computing;recursion	Claudio Kirner	1989		10.1145/75427.1030239	complete;parallel processing;parallel computing;real-time computing;computer science;deadlock;distributed computing;programming language;trap;petri net;strongly connected component;liveness	HPC	-10.748680730971708	42.271561683705826	149553
ef48ab1f8befdf2757fb2537d30c567e0f03bb12	a case study of optimistic computing on the grid: parallel mesh generation	mesh generation optimistic computing parallel mesh generation grid portable runtime environment optimistic control grid performance monitoring optimistic grid computing;performance evaluation;performance evaluation grid computing mesh generation;grid applications;computer aided software engineering concurrent computing grid computing mesh generation computational modeling optimization methods scientific computing delay discrete event simulation runtime environment;world wide grid;mesh generation;grid computing;mobile application	This paper describes our progress in creating a case study on optimistic computing for the Grid using parallel mesh generation. For the implementation of both methods we will be using a Portable Runtime Environment for Mobile Applications (PREMA) which is extended to provide support for optimistic control using grid performance monitoring and prediction. Based on the observed performance of a world-wide grid testbed, we will use this case study to develop a methodology for estimating target operating regions for grid applications. The goal of this project is to generalize the experience and knowledge of optimistic grid computing gained through mesh generation into a tool that can be applied to tightly coupled computations in other application domains.	computation;grid computing;middleware;parallel mesh generation;programming paradigm;testbed	Nikos Chrisochoides;Andriy Fedorov;Bruce Lowekamp;Marcia Zangrilli;Craig A. Lee	2003		10.1109/IPDPS.2003.1213374	mesh generation;parallel computing;computer science;theoretical computer science;distributed computing;grid computing	HPC	-10.0805079043252	39.48170874620507	149815
1a8c2efa32e80036194a8f913d479ca80e7ba053	next generation system software for future high-end computing systems	system software application software parallel processing delay program processors earth power system modeling computer science laboratories high performance computing;system software;application software;high performance computing;earth;performance improvement;design and implementation;next generation;parallel computer;runtime system;computer science;memory hierarchy;power system modeling;high end computing;program processors;parallel processing	Future high-end computers will offer great performance improvements over today’s machines, enabling applications of far greater complexity. However, designers must solve the challenge of exploiting massive parallelism efficiency in the face of very high latencies across the memory hierarchy. We believe the key to meeting this challenge is the design and implementation of new models and languages which address the problems of parallelism and latency on such machines. This paper presents an overview of our ongoing research toward this goal. Specifically, we will develop a suitable program execution model, a high-level programming notation, and a compiler and runtime system based on the underlying models. These are based on our previous work in parallel multithreaded systems, but are suitably enhanced to meet the needs of future high-end computers.	compiler;computer;high- and low-level;high-level programming language;memory hierarchy;parallel computing;runtime system;thread (computing)	Guang R. Gao;Kevin B. Theobald;Ziang Hu;Haiping Wu;Jizhu Lu;Keshav Pingali;Paul Stodghill;Thomas L. Sterling;Rick L. Stevens;Mark Hereld	2002		10.1109/IPDPS.2002.1016578	parallel processing;computer architecture;application software;parallel computing;computer science;operating system;distributed computing;earth;programming language	PL	-8.291617330472981	45.4005804904758	150138
3fb20b2e7378c927c4d9c66e9adf337c550ab049	stingray: cone tracing using a software dsm for sci clusters	kernel;shared memory;personal communication networks;computer graphics;prototypes;scalable coherent interface;cluster of workstations;image generation;prototypes clustering algorithms linux supercomputers kernel image generation hardware laboratories personal communication networks computer graphics;ray tracing;clustering algorithms;linux;supercomputers;hardware	In this paper we consider the use of a supercomputer with a hardware shared memory versus a cluster of workstations using a software Distributed Shared Memory (DSM). We focus on ray tracing applications to compare both architectures. We have ported Stingray, a parallel cone tracer developed on a SGI Origin 2000 supercomputer, on a cluster using a Scalable Coherent Interface (SCI) network and a software DSM called SciFS. We present concepts of cone tracing with Stingray, concepts of SCI cluster with a DSM and the implementation issues. We compare the results obtained with the two architectures and we discuss the trade-off – price/performance/programming ease – of both architectures. 12 We show with Stingray that a modest 12 nodes SCI cluster with an efficient software DSM is 5 times cheaper and can perform up to 2.3 times better than a SGI Origin 2000 with 6 processors. We think that a software DSM is well suited for this kind of applications and provides both ease of programming and scalable performance.	central processing unit;coherent;computer cluster;cone tracing;distributed shared memory;ray tracing (graphics);scalability;supercomputer;workstation	Alexandre Meyer;Emmanuel Cecchet	2001	Proceedings 42nd IEEE Symposium on Foundations of Computer Science	10.1109/CLUSTR.2001.959957	shared memory;ray tracing;computer architecture;parallel computing;kernel;computer science;operating system;prototype;cluster analysis;programming language;computer graphics;linux kernel	HPC	-11.146405460856833	44.763247902974214	150154
c501b57f9cd690d32edfdae54cc6a22ebc0f1b0e	application performance on a cluster-booster system		The DEEP projects have developed a variety of hardware and software technologies aiming at improving the efficiency and usability of next generation high-performance computers. They evolve around an innovative concept for heterogeneous systems: the Cluster-Booster architecture. In it, a general purpose cluster is tightly coupled to a manycore system (the Booster). This modular way of integrating heterogeneous components enables applications to freely choose the kind of computing resources on which it runs most efficiently. Codes might even be partitioned to map specific requirements of code-parts onto the best suited hardware. This paper presents for the first time measurements done by a real world scientific application demonstrating the performance gain achieved with this kind of code-partition approach.	booster (electric power);computer hardware;manycore processor;multi-core processor;next-generation network;requirement;supercomputer;usability	Anke Kreuzer;Norbert Eicker;Jorge Amaya;Estela Suarez	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00019	distributed computing;computer science;booster (rocketry);architecture;computer architecture;usability;modular design;non-volatile memory;software;bandwidth (signal processing)	HPC	-7.875206841891328	44.50984798126995	150530
81d66ecbedd4f3bb3cf9c770805ec0603f2a1d48	systematic detection of memory related performance bottlenecks in gpgpu programs		Graphics processing units (GPUs) pose an attractive choice for designing high-performance and energy-efficient software systems. This is because GPUs are capable of executing massively parallel applications. However, the performance of GPUs is limited by the contention in memory subsystems, often resulting in substantial delays and effectively reducing the parallelism. In this paper, we propose GRAB, an automated debugger to aid the development of efficient GPU kernels. GRAB systematically detects, classifies and discovers the root causes of memory-performance bottlenecks in GPUs. We have implemented GRAB and evaluated it with several open-source GPU kernels, including two real-life case studies. We show the usage of GRAB through improvement of GPU kernels on a  real NVIDIA Tegra K1 hardware  – a widely used GPU for mobile and handheld devices. The guidance obtained from GRAB leads to an overall improvement of up to 64%.	general-purpose computing on graphics processing units	Adrian Horga;Sudipta Chattopadhyay;Petru Eles;Zebo Peng	2016	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2016.08.002	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;distributed computing	Embedded	-7.106531060615019	45.76923906059374	150568
83ea542d68e3a70165d1a642f50aaf61f8fee5d7	experimental analysis of a mixed-mode parallel architecture performing sequence sorting.	experimental analysis;mixed mode;parallel architecture		sorting	Samuel A. Fineberg;Thomas L. Casavant;Howard Jay Siegel	1990			distributed computing;experimental analysis of behavior	HPC	-9.539872129995883	41.95840165593949	150881
4b0b1fd123ec9c43e82bc60d7fa0b9254d60a28d	task-based fmm for heterogeneous architectures	paper;heterogeneous systems;fast multipole method;cuda;heterogeneous architectures;scheduling;nvidia;tesla m2070;runtime system;computer science;fast multipole methods;tesla m2090;graphics processing unit;pipeline;numerical simulation	High performance fast multipole method is crucial for the numerical simulation of many physical problems. In a previous study, we have shown that task-based fast multipole method provides the flexibility required to process a wide spectrum of particle distributions efficiently on multicore architectures. In this paper, we now show how such an approach can be extended to fully exploit heterogeneous platforms. For that, we design highly tuned graphics processing unit (GPU) versions of the two dominant operators P2P and M2L) as well as a scheduling strategy that dynamically decides which proportion of subsequent tasks is processed on regular CPU cores and on GPU accelerators. We assess our method with the StarPU runtime system for executing the resulting task flow on an Intel X5650 Nehalem multicore processor possibly enhanced with one, two, or three Nvidia Fermi M2070 or M2090 GPUs (Santa Clara, CA, USA). A detailed experimental study on two 30 million particle distributions (a cube and an ellipsoid) shows that the resulting software consistently achieves high performance across architectures.	fast multipole method	Emmanuel Agullo;Bérenger Bramas;Olivier Coulaud;Eric F Darve;Matthias Messner;Toru Takahashi	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3723	computer simulation;parallel computing;computer hardware;fast multipole method;computer science;operating system;scheduling;pipeline	PL	-5.1802381838946445	39.88681993873378	151072
b7dbf21d0cdc5797bdf31f9219e79a7e073034cc	a fine-grain approach to distributed embedded systems design			embedded system;systems design	Zdravko Karakehayov	1995			distributed computing;parallel computing;computer science;distributed design patterns	EDA	-9.874736405243588	43.42303809291462	151116
56bc3cf02015ba506425b8e08b2a239a3dbc6293	star-mpi: self tuned adaptive routines for mpi collective operations	collective communication;low power architecture;complexity effective architecture;message passing interface;issue logic;wakeup logic;system architecture;high performance;parallel applications	Message Passing Interface (MPI) collective communication routines are widely used in parallel applications. In order for a collective communication routine to achieve high performance for different applications on different platforms, it must be adaptable to both the system architecture and the application workload. Current MPI implementations do not support such software adaptability and are not able to achieve high performance on many platforms. In this paper, we present STAR-MPI (Self Tuned Adaptive Routines for MPI collective operations), a set of MPI collective communication routines that are capable of adapting to system architecture and application workload. For each operation, STAR-MPI maintains a set of communication algorithms that can potentially be efficient at different situations. As an application executes, a STAR-MPI routine applies the Automatic Empirical Optimization of Software (AEOS) technique at run time to dynamically select the best performing algorithm for the application on the platform. We describe the techniques used in STAR-MPI, analyze STAR-MPI overheads, and evaluate the performance of STAR-MPI with applications and benchmarks. The results of our study indicate that STAR-MPI is robust and efficient. It is able to and efficient algorithms with reasonable overheads, and it out-performs traditional MPI implementations to a large degree in many cases.	algorithm;direction finding;kleene star;library (computing);message passing interface;run time (program lifecycle phase);star height;systems architecture;xerox star	Ahmad Faraj;Xin Yuan;David K. Lowenthal	2006		10.1145/1183401.1183431	parallel computing;real-time computing;computer science;message passing interface;operating system;distributed computing;systems architecture	HPC	-7.749434900793304	42.20138481416802	152647
3faa0bfffcd3ee88d0c3800c9407848197fb194f	heterogeneous multi-cluster networking with the madeleine iii communication library	libraries;virtual networks;communication system;application software;high performance computing;power system interconnection;physics computing;computer networks;programming profession;workstations;parallel computer;information system;metacomputing;distributed algorithm;configuration management;libraries computer networks application software workstations high performance computing hardware programming profession physics computing metacomputing power system interconnection;hardware	This paper introduces the new version of the Madeleine portable multi-protocol communication library. Madeleine version III now includes full, flexible multi-cluster support associated to a redesigned version of the transparent multi-network message forwarding mechanism. Madeleine III works together with a new configuration management module to handle a wide panel of network-heterogeneous multi-cluster configurations. The integration of a new topology information system allows programmers of parallel computing applications to build highly optimized distributed algorithms on top of the transparent multi-network communication system provided by Madeleine III’s virtual networks. The preliminary experiments we conducted regarding the new virtual network capabilities of Madeleine III showed interesting results with an asymptotic bandwidth of 43 MB/s over a virtual link made of a SISCI/SCI and a BIP/Myrinet physical link.	computer cluster;computer science;configuration management;data structure;device driver;directory (computing);distributed algorithm;distributed computing;experiment;information system;integrated development environment;les trophées du libre;library (computing);linear programming;megabyte;message passing interface;olivier fourdan;overhead (computing);parallel computing;programmer;routing table;session (computer science);thread (computing);throughput	Olivier Aumage	2002		10.1109/IPDPS.2002.1015658	distributed algorithm;application software;parallel computing;workstation;computer science;operating system;distributed computing;configuration management;programming language;information system;communications system;computer network	HPC	-11.429954172588747	45.266660747158824	153007
546b7d1d06ca8efe52baea4bafc3199114995942	exploiting task-based parallelism for parallel discrete event simulation		Today large-scale simulation applications are becoming common in research and industry. A significant fraction of them run on multi-core clusters. Current parallel simulation kernels use multi-process and multi-thread to exploit inter-node parallelism and intra-node parallelism on multi-core clusters. We exploit task-base parallelism in parallel discrete event simulation (PDES) kernels, which is more fine-grained than thread-level and process-level parallelism. In our system, every simulation event is wrapped to a task. Work-stealing task scheduling scheme is applied to achieve dynamic load balancing among the multi-cores, and a graph partitioning approach is applied in partitioning simulation entities among the cluster nodes. Experimental results show that our PDES kernel outperforms existing PDES kernels by fully exploiting task parallelism.	clustered file system;entity;graph partition;kernel (operating system);load balancing (computing);multi-core processor;parallel computing;scheduling (computing);simulation;task parallelism;terabyte;work stealing	Yizhuo Wang;Zhiwei Gao;Weixing Ji;Han Zhang;Duzheng Qing	2018	2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)	10.1109/PDP2018.2018.00095	discrete event simulation;parallel computing;kernel (linear algebra);distributed computing;task parallelism;work stealing;scheduling (computing);exploit;computer science;graph partition	HPC	-5.208122770168102	42.228968549928844	153035
5e811749da092fb55fa3e9283b87484e390531ab	on the synchronizing tools for parallel programs				Tomasz Müldner	1981	Fundam. Inform.		discrete mathematics;synchronizing;mathematics	HPC	-10.586958214457663	42.16866789139112	153068
1c87366488e2b85e32e0e87eaca9779174071a12	a parallel adaptive version of the block-based gauss-jordan algorithm	workstation clusters parallel algorithms;gauss jordan algorithm;network of workstations now;sun sparc4 workstations parallel adaptive version block based gauss jordan algorithm numerical analysis adaptive folding mars cluster of dec alpha processors gigaswitch network ethernet network;numerical analysis;network of workstation;gaussian processes mars workstations ice parallel programming programming environments numerical analysis lan interconnection network topology fault tolerance;meta systems;workstation clusters;adaptive paral lelism;parallel algorithms	This paper presents a parallel adaptive version of the block-based Gauss-Jordan algorithm used in numerical analysis to invert matrices. This version includes a characterization of the workload of processors and a mechanism of its adaptive folding/unfolding. The application is implemented and experimented with MARS in dedicated and non-dedicated environments. The results show that an absolute efficiency of 92% is possible on a cluster of DEC/ALPHA processors interconnected by a Gigaswitch network and an absolute efficiency of 67% can be obtained on an Ethernet network of SUN-Sparc4 workstations. Moreover, the adaptability of the algorithm is experimeted on a non-dedicated meta-system including both the two parks of machines.	central processing unit;dec alpha;fault tolerance;gauss–newton algorithm;heterogeneous computing;meta-system;numerical analysis;out-of-core algorithm;performance evaluation;scheduling (computing);software development process;unfolding (dsp implementation);workstation	Nouredine Melab;El-Ghazali Talbi;Serge G. Petiton	1999		10.1109/IPPS.1999.760499	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-8.671322196056037	41.51211139013475	153154
72c3439b712f0172ff147ba33e80ca1a591afeae	system level accelerator with blue gene: a heterogeneous computing model for grand challenge problems	heterogeneous computing		blue gene;heterogeneous computing	Tim David	2008		10.3233/978-1-60750-073-5-365	symmetric multiprocessor system;parallel computing;computer science;distributed computing;theoretical computer science	HPC	-9.13974334469831	42.56576299994267	153373
3643e80cd8761fbab05e4d427ae7744582a2993f	a comparative study of programming environments exploiting heterogeneous systems		This paper compares programming environments that exploit heterogeneous systems to process a large amount of data efficiently. Our motivation is to investigate the feasibility of the adaptive, transparent migration of intensive computation for a large amount of data across heterogeneous programming languages and processors for high performance and programmability. We compare a variety of programming environments composed of programming languages, such as Java and C, memory space models, such as distinct and shared memory, and parallel processors, such as general-purpose CPUs and graphics processing units (GPUs) to examine their performance-programmability tradeoffs. In addition, we introduce a software-based shared virtual memory that creates a view of the host memory inside GPU kernels to enable seamless computation offloading from the host to the device. This paper reveals a programmability-performance hierarchy in which programs increase their performance at the cost of decreasing programmability. The experimental results suggest the desirability of a well-balanced system.	algorithm;central processing unit;computation offloading;computer graphics;dspace;graphics processing unit;java;processor register;programming language;seamless3d;shared memory	Bongsuk Ko;Seunghun Han;Yongjun Park;Moongu Jeon;Byeongcheol Lee	2017	IEEE Access	10.1109/ACCESS.2017.2708738	real-time computing;data science;data mining	HPC	-5.909437650130079	44.345511809437795	154560
35cd28e19698815e5d36cf4a4fb0fee48dca7e4d	the elaps framework: experimental linear algebra performance studies		Optimal use of computing resources requires extensive coding, tuning and benchmarking. To boost developer productivity in these time consuming tasks, we introduce the Experimental Linear Algebra Performance Studies framework (ELAPS), a multi-platform open source environment for fast yet powerful performance experimentation with dense linear algebra kernels, algorithms, and libraries. ELAPS allows users to construct experiments to investigate how performance and efficiency vary depending on factors such as caching, algorithmic parameters, problem size, and parallelism. Experiments are designed either through Python scripts or a specialized GUI, and run on the whole spectrum of architectures, ranging from laptops to clusters, accelerators, and supercomputers. The resulting experiment reports provide various metrics and statistics that can be analyzed both numerically and visually. We demonstrate the use of ELAPS in four concrete application scenarios and in as many computing environments, illustrating its practical value in supporting critical performance decisions. General Terms Performance, Experimentation	algorithm;analysis of algorithms;computer cluster;computer programming;experiment;graphical user interface;laptop;library (computing);linear algebra;numerical analysis;open-source software;parallel computing;performance tuning;python;supercomputer	Elmar Peise;Paolo Bientinesi	2015	CoRR		simulation;computer science;theoretical computer science;operating system;mathematics;programming language	DB	-5.3940957357651955	44.8379987232829	154595
7f0d49614cae78b52fa8e45e7de9054536b8978e	parallel and distributed algorithms: laboratory assignments in joyce/linda	parallel computing;parallel algorithm;computer network;visualization;operating system;parallel and distributed algorithms;parallel computer;laboratory;data structure;distributed architecture	A NSF ILI grant funded development of parallel and distributed laboratories for an undergraduate course in parallel algorithms. The laboratory assignments explored various parallel and distributed architectures, and paradigms. These assignments were implemented using the Joyce/Linda[McDonald92] language. The Joyce/Linda software was also utilized to develop parallel and distributed laboratory assignments for courses in data structures, operating systems and computer networks. The parallel and distributed algorithms course examined both theoretical and practical areas of study.	data structure;distributed algorithm;ibm notes;joyce currie little;linda (coordination language);operating system;parallel algorithm;parallel computing	Bruce S. Elenborgen	1996		10.1145/236452.236478	distributed algorithm;visualization;data structure;computer science;theoretical computer science;distributed computing;parallel algorithm;laboratory	Theory	-11.330183432280252	39.75411049731074	154613
111eb24c93996b7eaee6242461ace11a909b6008	code generation for gpu accelerators from a domain-specific language for medical imaging			domain-specific language;graphics processing unit;medical imaging	Richard Membarth	2013				PL	-7.046966940369024	42.20480149602157	154667
7fd62e9f84839bcc506874c0a4776cdd6b8f1f0c	parallel programming by r.h. perrott addison-wesley, workingham, uk, 1987, 252 pages (incl. index) (£14.95)	indexation;parallel programs		parallel computing	Iain D. Craig	1989	Robotica	10.1017/S0263574700005270	computer science;artificial intelligence;software engineering	DB	-10.562845269570555	41.54462378005007	154906
4317b1bf2d9788865a14a09245f83bae46539826	performance and granularity control in the spades parallel simulation system	distributed memory;distributed memory systems parallel programming discrete event simulation;distributed memory systems;parallel programming;parallel programming details performance granularity control spades parallel simulation system parallel discrete event simulation mapping process structured parallel discrete event simulation parallel simulation framework spades c distributed memory fujitsu ap3000 parallel computer underlying complex parallel simulation synchronization;parallel discrete event simulation;computational modeling concurrent computing read only memory parallel programming discrete event simulation programming environments distributed computing independent component analysis liver computer simulation;parallel computer;parallel programs;simulation model;discrete event;parallel simulation;discrete event simulation	Parallel simulation has the potential to accelerate the execution of simulation applications. However, developing a parallel discrete-event simulation from scratch requires an in-depth knowledge of the mapping process from the physical model to the simulation model, and a substantial effort in optimisingperformance. This paper presents an overview of the SPaDES (Structured Parallel Discrete-Event Simulation) parallel simulation framework. We focus on the performance analysis of SPaDES/C ++, an implementation of SPaDES on a distributed-memory Fujitsu AP3000 parallel computer. SPaDES/C ++ hides the underlying complex parallel simulation synchronization and parallel programming details from the simulationist. Our empirical results show that the SPaDES framework can deliver good speedup if the process granularity is properly optimised.	application checkpointing;c++;central processing unit;computation;distributed memory;experiment;message passing;parallel computing;spades;scalability;simulation;software framework;speedup;workbench	Yong Meng Teo;Seng Chuan Tay	1999		10.1109/ISPAN.1999.778923	parallel computing;real-time computing;computer science;distributed computing	HPC	-10.459354800498437	40.00909211152421	154939
c5f885257e5a1db3d25e9ef362b5b32c6ff444fc	the load balancing algorithm based on the parallel implementation of ipo and fmm	large scale;parallel computer;load balance;parallel implementation	The large scale arbitrary cavity scattering problems can be tackled by parallel computing approach. This paper proposes a new method to distribute cavity scattering computing tasks on multiprocessors computer based on the IPO and FMM. The samples of IPO and FMM pattern can be divided into eight blocks, and this paper just analyzes the load balancing of the block in the lower-left corner using the invariability of angle. So this problem can be transformed into how to equally distribute an N*N matrix on P processors. Two matrix partitioning algorithms are introduced, and experimental results show that the optimal sub-structure algorithm is able to balance the load among multiprocessors effectively, thereby, improving the performance of the entire system.	algorithm;fast multipole method;load balancing (computing)	Ting Wang;Yue Hu;Yanbao Cui;Weiqin Tong;Xiaoli Zhi	2009		10.1007/978-3-642-11842-5_57	parallel computing;real-time computing;computer science;load balancing;distributed computing	HPC	-5.647737395001255	40.39886409222117	155512
b68efd620d014e2101f0aa57b7f0591a598446e4	efficient parallel i/o in seismic imaging	image coding;three dimensional;large scale;high performance computer;parallel i o	Although high performance computers tend to be mea sured by their processor and communication speeds, the bottleneck for many large-scale applications is the I/O performance rather than the computational or communi cation performance. One such application is the process ing of three-dimensional seismic data. Seismic data sets, consisting of recorded pressure waves, can be very large, sometimes more than a terabyte in size. Even if the computations can be performed in core, the time required to read the initial seismic data and velocity model and write images is substantial. In this paper, the authors discuss an approach in handling the massive I/O requirements of seismic processing and show the performance of their imaging code Salvo on the Intel Paragonâ¢ computer.	input/output;parallel i/o	Ron A. Oldfield;David E. Womble;Curtis C. Ober	1998	IJHPCA	10.1177/109434209801200303	three-dimensional space;parallel computing;computer hardware;computer science;theoretical computer science;operating system	HPC	-4.985960531598777	41.154640886338214	156244
afb304a4b8b3a601548d9838432c2c5aad01de8a	an execution environment for flexible task-oriented software on multicore systems	coordination language;numerical analysis;scheduling;execution environment;software development;cluster system;task based programming;parallel computer;multicore processors;mapping;memory hierarchy;parallel architecture;parallel programs;parallel execution;dynamic scheduling	The article addresses the challenges of software development for current and future parallel computers, which are expected to be dominated by multicore and many-core architectures. Using these multicore processors for cluster systems will create systems with thousands of cores and deep memory hierarchies. To efficiently exploit the tremendous parallelism of these hardware platforms, a new generation of programming methodologies is needed. This article proposes a parallel programming methodology exploiting a task-based representation of application software. For the specification of task-based programs, a coordination language is presented, which uses external variables to express the cooperation between tasks. For the actual execution of a task-based program on a specific parallel architecture, different dynamic scheduling algorithms embedded into an execution environment are introduced. Runtime experiments for complex methods from a numerical analysis are performed on different parallel execution platforms.	multi-core processor	Thomas Rauber;Gudula Rünger	2012	Concurrent Engineering: R&A	10.1177/1063293X12446664	multi-core processor;computer architecture;parallel computing;real-time computing;dynamic priority scheduling;numerical analysis;computer science;software development;scheduling;parallel programming model	SE	-7.307693646096777	43.84336121474251	156454
69e5635f1a10d7ca54091a834dd29dd8e0499e52	piecewise cubic interpolation on distributed memory parallel computers and clusters of workstations	linear algebra;distributed memory;interpolation;distributed memory systems;mathematics computing;linear algebra routines;cluster of workstations;workstation clusters distributed memory systems interpolation linear algebra mathematics computing message passing parallel algorithms parallel machines;interpolation concurrent computing distributed computing workstations polynomials linear algebra algorithms equations portable computers high performance computing;message passing piecewise cubic interpolation distributed memory parallel computers workstation clusters lapack routines scalapack linear algebra routines;lapack routines;parallel computer;message passing;parallel machines;distributed memory parallel computers;workstation clusters;high performance;piecewise cubic interpolation;parallel algorithms;scalapack	The aim of this paper is to present two new portable and high performance implementations of routines that can be used for piecewise cubic interpolation. The first one (sequential) is based on LAPACK routines, while the next, based on ScaLAPACK is designed for distributed memory parallel computers and clusters. The results of experiments performed on a cluster of twenty Itanium 2 processors and on Cray XI are also presented and shortly discussed	central processing unit;computer;const (computer programming);cubic hermite spline;cubic function;distributed memory;experiment;interpolation;itanium;lapack;numerical analysis;parallel computing;scalapack;workstation	Przemyslaw Stpiczynski;Joanna Potiopa	2006	International Symposium on Parallel Computing in Electrical Engineering (PARELEC'06)	10.1109/PARELEC.2006.68	parallel computing;message passing;distributed memory;interpolation;computer science;theoretical computer science;linear algebra;distributed computing;parallel algorithm	HPC	-7.335463703584744	39.37140573569344	156994
e8ce80ad7eaece20abfbae331d0d2f141f298e2a	distribution of parallel discrete-event simulations in ges: core design and optimizations	performance analysis;parallel;optimization;scalability;distributed;discrete event simulation	Computer simulations have become an indispensable tool for the empirical study of large-scale systems. The timely simulation of these systems however, is not without its challenges. Simulators have to be able to harness the full computational power of modern architectures through parallel execution and overcome the memory limitations of a single computer. In this paper we investigate techniques for distributed and parallel execution of the Grid Economics Simulator. We present the design of a parallel and distributed simulation core that uses a conservative time synchronization protocol and describe the optimizations we performed to improve the performance of the simulator. We analyze the performance of the distributed simulation setup through two different application scenarios. Our results demonstrate how the presented techniques contribute to attain significant speedups on a distributed system consisting of multi-core machines and commodity networking hardware.		Silas De Munck;Kurt Vanmechelen;Jan Broeckhove	2011		10.4108/icst.simutools.2011.245500	parallel computing;real-time computing;scalability;simulation;computer science;discrete event simulation;operating system;parallel;distributed computing	HPC	-10.964755189729203	40.77666507427772	157737
e9b7c6ffe59e88a63b48fb99bd27764004650896	introduction to the special issue on the 11th international conference on field-programmable technology (fpt'12)	parallel computing;multiprocessor;network on chip;fpga;mpi			Jason Helge Anderson;Kiyoung Choi	2014	TRETS	10.1145/2655712	computer architecture;parallel computing;multiprocessing;computer science;message passing interface;distributed computing;network on a chip;field-programmable gate array	Robotics	-9.30942933389473	41.972172139248464	157810
ff1641e35d77d107a4ef60064b8cc4f4b5bc5978	highly efficient spatial data filtering in parallel using the opensource library cpppo	closure models;multi scale;parallel filtering;mpi;post processing	CPPPO is a compilation of parallel data processing routines developed with the aim to create a library for ‘‘scale bridging’’ (i.e. connecting different scales by mean of closure models) in a multi-scale approach. CPPPO features a number of parallel filtering algorithms designed for use with structured and unstructured Eulerian meshes, as well as Lagrangian data sets. In addition, data can be processed on the fly, allowing the collection of relevant statistics without saving individual snapshots of the simulation state. Our library is provided with an interface to the widely-used CFD solver OpenFOAM R , and can be easily connected to any other software package via interface modules. Also, we introduce a novel, extremely efficient approach to parallel data filtering, and show that our algorithms scale super-linearly on multi-core clusters. Furthermore, we provide a guideline for choosing the optimal Eulerian cell selection algorithm depending on the number of CPU cores used. Finally, we demonstrate the accuracy and the parallel scalability of CPPPO in a showcase focusing on heat and mass transfer from a dense bed of particles.	bridging (networking);c++;cartesian perceptual compression;central processing unit;compiler;computation;computational fluid dynamics;documentation;download;lagrangian (field theory);lagrangian and eulerian specification of the flow field;library (computing);multi-core processor;numerical analysis;on the fly;open-source software;openfoam;particle system;run time (program lifecycle phase);scalability;selection algorithm;simulation;solver;speedup;time complexity	Federico Municchi;Christoph Goniva;Stefan Radl	2016	Computer Physics Communications	10.1016/j.cpc.2016.05.026	parallel computing;computer hardware;computer science;message passing interface;theoretical computer science;video post-processing	HPC	-5.906989586838459	39.54179400841979	157880
1398d00547f1a0f13f60754fc320bc4de3a8d61a	hierarchical task mapping of cell-based amr cosmology simulations	topology-aware task mapping technique;art;cosmology simulation;supercomputers;cell-based amr cosmology simulations;cell-based amr cosmology simulation;astronomy computing;cosmology;topology-aware task mapping techniques;application communication cost reduction;intra-node mapping;application communication cost;network traffic minimization;maximum message size minimization;adaptive mesh refinement;multiprocessing systems;intra-socket communication;intra-node communication;art application;hierarchical task mapping scheme;application process;performance optimization;fat-tree topologies;adaptive refinement tree;minimisation;inter-node mapping;multiprocessor cluster architectural properties;parallel machines;3d torus;cost reduction;cpu sockets;multithreading;critical path;multicore;critical section	Cosmology simulations are highly communication-intensive, thus it is critical to exploit topology-aware task mapping techniques for performance optimization. To exploit the architectural properties of multiprocessor clusters (the performance gap between inter-node and intra-node communication as well as the gap between inter-socket and intra-socket communication), we design and develop a hierarchical task mapping scheme for cell-based AMR (Adaptive Mesh Refinement) cosmology simulations, in particular, the ART application. Our scheme consists of two parts: (1) an inter-node mapping to map application processes onto nodes with the objective of minimizing network traffic among nodes and (2) an intra-node mapping within each node to minimize the maximum size of messages transmitted between CPU sockets. Experiments on production supercomputers with 3D torus and fat-tree topologies show that our scheme can significantly reduce application communication cost by up to 50%. More importantly, our scheme is generic and can be extended to many other applications.	adaptive multi-rate audio codec;adaptive mesh refinement;cpu socket;fat tree;mathematical optimization;multiprocessing;network traffic control;performance tuning;simulation;supercomputer	Jingjin Wu;Zhiling Lan;Xuanxing Xiong;Nickolay Y. Gnedin;Andrey V. Kravtsov	2012	2012 International Conference for High Performance Computing, Networking, Storage and Analysis		multi-core processor;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;cosmology	HPC	-9.049335209231423	44.263594942160495	158021
029920d4fe0dcd2677f9944b5ed62314a21ca4ec	optimizing communication for charm++ applications by reducing network contention	mapping;mesh topology;performance optimization	Optimal network performance is critical for efficient parallel scaling of communicationbound applications on large machines. No-load latencies do not increase significantly with number of hops traveled when wormhole routing is deployed. Yet, we and others have recently shown that in presence of contention, message latencies can grow substantially large. Hence task mapping strategies should take the topology of the machine into account on large machines. In this paper, we present topology aware mapping as a technique to optimize communication on three-dimensional mesh interconnects and hence improve performance. Our methodology is facilitated by the idea of object-based decomposition used in Charm++ which separates the processes of decomposition from mapping of computation to processors and allows a more flexible mapping based on communication patterns between objects. Exploiting this and the topology of the allocated job partition, we present mapping strategies for a production code, OpenAtom to improve overall performance and scaling. OpenAtom presents complex communication scenarios of interaction involving multiple groups of objects and makes the mapping task a challenge. Results are presented for OpenAtom on up to 16,384 processors of Blue Gene/L, 8,192 processors of Blue Gene/P and 2,048 processors of Cray XT3.	blue gene;central processing unit;computation;cray xt3;electrical connection;image scaling;network performance;object-based language;openatom;optimizing compiler;routing;wormhole switching	Abhinav Bhatele;Eric J. Bohm;Laxmikant V. Kalé	2011	Concurrency and Computation: Practice and Experience	10.1002/cpe.1637	parallel computing;computer science;theoretical computer science;mesh networking;operating system;database;distributed computing	HPC	-8.776116936774732	44.459502919611765	158251
0647391355016a20c70fe41839a3ff501f7c70eb	savina - an actor benchmark suite: enabling empirical evaluation of actor libraries	performance comparison;benchmark suite;java actor libraries;actor model	This paper introduces the Savina benchmark suite for actor-oriented programs. Our goal is to provide a standard benchmark suite that enables researchers and application developers to compare different actor implementations and identify those that deliver the best performance for a given use-case. The benchmarks in Savina are diverse, realistic, and represent compute (rather than I/O) intensive applications. They range from popular micro-benchmarks to classical concurrency problems to applications that demonstrate various styles of parallelism. Implementations of the benchmarks on various actor libraries are made publicly available through an open source release. This will allow other developers and researchers to compare the performance of their actor libraries on these common set of benchmarks.	benchmark (computing);concurrency (computer science);input/output;library (computing);open-source software;parallel computing	Shams Mahmood Imam;Vivek Sarkar	2014		10.1145/2687357.2687368	real-time computing;simulation;computer science;data mining	PL	-6.300213771817916	45.141720312469914	158414
6bb35da9ac7ab06ac93a60e3fd1a5f1b987190f0	instruction-level distributed microarchitecture based on data decoupling			coupling (computer programming);microarchitecture	Gyungho Lee;Akhilesh Tyagi	2000			computer science;parallel computing;decoupling (cosmology);microarchitecture	EDA	-9.074295328376442	43.370526922327656	158722
b791dec135f32abe386f8a434b3f650506ac0b33	applications of many-core technologies to on-line event reconstruction in high energy physics experiments	detectors;microwave integrated circuits;graphics processing units microwave integrated circuits data transfer physics computer architecture detectors peer to peer computing;physics;computer architecture;graphics processing units;peer to peer computing;data transfer	Interest in many-core architectures applied to real time selections is growing in High Energy Physics (HEP) experiments. In this paper we describe performance measurements of many-core devices when applied to a typical HEP online task: the selection of events based on the trajectories of charged particles. We use as benchmark a scaled-up version of the algorithm used at CDF experiment at Tevatron for online track reconstruction - the SVT algorithm - as a realistic test-case for low-latency trigger systems using new computing architectures for LHC experiment. We examine the complexity/performance trade-off in porting existing serial algorithms to many-core devices. We measure performance of different architectures (Intel Xeon Phi and AMD GPUs, in addition to NVidia GPUs) and different software environments (OpenCL, in addition to NVidia CUDA). Measurements of both data processing and data transfer latency are shown, considering different I/O strategies to/from the many-core devices.	algorithm;benchmark (computing);cuda;central processing unit;computation;experiment;heterogeneous element processor;input/output;large hadron collider;manycore processor;multi-core processor;network packet;online and offline;parallel computing;programming tool;xeon phi	A. Gianelle;Silvia Amerio;Denis Bastieri;M. Corvo;W. Ketchum;Tong Liu;Alessandro Lonardo;D. Lucchesi;S. Poprocki;Ryan Rivera;Laura Tosoratto;Piero Vicini;P. Wittich	2013	2013 IEEE Nuclear Science Symposium and Medical Imaging Conference (2013 NSS/MIC)	10.1109/NSSMIC.2013.6829552	detector;general-purpose computing on graphics processing units	HPC	-5.872186737458784	46.16423371090476	159149
224ce5eff3e2325318d7f9cd3fdfcaf90494adfc	fast collective operations using shared and remote memory access protocols on clusters	data transmission;general and miscellaneous mathematics computing and information science;memory protocols;shared memory;mpich implementation collective communication operations symmetric multiprocessor nodes shared remote memory collectives point to point message passing remote memory access protocols shared memory access protocols cluster computing semantics communication graphs;collective communication;point to point;implementation;multiprocessing systems message passing memory protocols interrupts workstation clusters;access protocols broadcasting message passing testing concurrent computing distributed computing iterative algorithms performance gain hardware scalability;performance improvement;array processors;message passing;interrupts;multiprocessing systems;workstation clusters;remote memory access;management;open source	This paper describes a novel methodology for implementing a common set of collective communication operations on clusters based on symmetric multiprocessor (SMP) nodes. Called Shared-Remote-Memory collectives, or SRM, our approach replaces the point-to-point message passing, traditionally used in implementation of collective message-passing operations, with a combination of shared and remote memory access (RMA) protocols that are used to implement semantics of the collective operations directly. Appropriate embedding of the communication graphs in a cluster maximizes the use of shared memory and reduces network communication. Substantial performance improvements are achieved over the highly optimized commercial IBM implementation and the open-source MPICH implementation of MPI across a wide range of message sizes on the IBM SP. For example, depending on the message size and number of processors, SRM implementation of broadcast, reduce, and barrier outperforms IBM MPI_Bcast by 27-84%, MPI_Reduce by 24-79%, and MPI_Barrier by 73% on 256 processors, respectively.	central processing unit;fibre channel point-to-point;file spanning;ibm personal computer;mpich;memory bandwidth;message passing;open-source software;revolution in military affairs;shared memory;symmetric multiprocessing;system reference manual	Vinod Tipparaju;Jarek Nieplocha;Dhabaleswar K. Panda	2003		10.1109/IPDPS.2003.1213188	shared memory;parallel computing;message passing;real-time computing;point-to-point;computer science;operating system;interrupt;distributed computing;programming language;implementation;data transmission	HPC	-11.050475440723607	46.31921042302426	159209
c174be0a9599c9fe7f47786f649362bf966492f2	realities associated with parallel processing	data mining;scientific production;gallium arsenide;fortran;electronic structure;density functional;parallel processing	At the T. J. Watson Research Center, there is a very active Condensed Matter Physics Group engaged in the study of semiconductors such as silicon (Si) and gallium-arsenide (Ga-As)1. One of the most important computer codes developed at Watson is a Density Functional Program which is used to study the electronic structure of semiconductors. This program also consumes the most CPU time of all other production applications at Watson. Thus, it was decided to undertake the parallelization of this code not only with the hope of reducing elapsed time, improving turnaround on the IBM 30902, and conserving other system resources such as memory and disk space, but also in an attempt to test the IBM Parallel FORTRAN Compiler3 on a production program while developing an understanding of the impact of parallel jobs from a system standpoint. A speedup of 4.4 using 6 processors was achieved for this important scientific production code.	central processing unit;code;disk space;electronic structure;fortran;job stream;parallel computing;semiconductor;speedup;thomas j. watson research center	Daniel E. Platt;Ryan A. Rossi;James W Wells;J. Becker	1989	Proceedings of the 1989 ACM/IEEE Conference on Supercomputing (Supercomputing '89)	10.1145/76263.76281	parallel processing;gallium arsenide;parallel computing;computer science;theoretical computer science;operating system;data mining;programming language;electronic structure	HPC	-7.850965097628027	39.837622919512846	159520
63f094535d28f8625c8610ac58df01cbed8f41c6	information processing and oppertunities for hpcc use in industry	linear algebra;new technology;image processing;national information infrastructure;high speed networks;agile manufacturing;information processing;scientific computing;parallel architecture;multimedia database;high performance	"""The U.S. National HPCC effort, represented by a set of approximately fifty Grand Challenge problems, is largely focused on issues of scientific simulation [1]. In addition to advancing the mission of the federal agencies sponsoring this program, and the state of science in the field of physics, biology, and chemistry, and related fields such as atmospheric science, one can view the HPCC initiative as an effort to speed the process of technological innovation. Hardware systems are now relatively mature, and the HPCC program has successfully accelerated the development of new software technologies such as parallel compilers, scientific libraries, system software, and programming tools. HPCC has matured into a proven technology [2], but its success is mainly limited to applications in the research laboratory. Industry, and society more generally, have not yet adopted HPCC as a mainstream technology. Both technical and social obstacles limit industries' use of HPCC. The scientific community has focused on high performance levels, while industry places much more importance on reliability and ease of use. Perceived risk by industry executives of adopting """"exotic"""" HPCC technologies, a general recommendation not to put """"mission-critical"""" industry applications on HPCC systems, and pressure to maximize short term or end of the quarter earnings [3] are examples of the obstacles to HPCC application development in industry."""	compiler;hpcc systems;information processing;library (computing);mission critical;programming tool;simulation;usability	Geoffrey C. Fox;Kim Mills	1994		10.1007/BFb0020341	information processing;image processing;computer science;theoretical computer science;linear algebra;database;algebra	HPC	-8.195222112987317	39.394401502654304	159748
9245b3a1be86af626fa4768115c3e5dd1fc9f197	a parallel tree n-body code for heterogeneous clusters	heterogeneous cluster	Without Abstract		Vincenzo Antonuccio-Delogu;Ugo Becciani	1994		10.1007/BFb0030132	computer science	HPC	-9.557294941693424	42.148101321898906	159947
dcf1b6960ae08fd77a451e025c196c4544335d57	versatility of bulk synchronous parallel computing: from the heterogeneous cluster to the system on chip			bulk synchronous parallel;parallel computing;system on a chip	Olaf Bonorden	2008			bulk synchronous parallel;system on a chip;parallel computing;heterogeneous cluster;computer science	HPC	-9.4769659816397	42.93269719935553	160259
dd3ddc35465a5027e618f58b3f4cc035ecbdd533	new data structures for matrices and specialized inner kernels: low overhead for high performance	symmetric positive definite;dense linear algebra;low overhead;cholesky factorization;new data structures;specialized inner kernels;high performance;data structure	Dense linear algebra codes are often expressed and coded in terms of BLAS calls. This approach, however, achieves suboptimal performance due to the overheads associated to such calls. Taking as an example the dense Cholesky factorization of a symmetric positive definite matrix we show that the potential of non-canonical data structures for dense linear algebra can be better exploited with the use of specialized inner kernels. The use of non-canonical data structures together with specialized inner kernels has low overhead and can produce excellent performance.	assembly language;blas;cholesky decomposition;code;data structure;goto;linear algebra;mathematical optimization;optimizing compiler;overhead (computing);partial template specialization;register file	José R. Herrero	2007		10.1007/978-3-540-68111-3_69	parallel computing;data structure;computer science;theoretical computer science;distributed computing;programming language;cholesky decomposition	Arch	-4.68770101424621	41.78974636092598	161338
7d9f040171e26df7b073b684053c3f28b92334bd	coarse mesh partitioning for tree based amr		In tree based adaptive mesh refinement, elements are partitioned between processes using a space filling curve. The curve establishes an ordering between all elements that derive from the same root element, the tree. When representing more complex geometries by patching together several trees, the roots of these trees form an unstructured coarse mesh. We present an algorithm to partition the elements of the coarse mesh such that (a) the fine mesh can be load-balanced to equal element counts per process regardless of the element-to-tree map and (b) each process that holds fine mesh elements has access to the meta data of all relevant trees. As an additional feature, the algorithm partitions the meta data of relevant ghost (halo) trees as well. We develop in detail how each process computes the communication pattern for the partition routine without handshaking and with minimal data movement. We demonstrate the scalability of this approach on up to 917e3 MPI ranks and .37e12 coarse mesh elements, measuring run times of one second or less.	adaptive multi-rate audio codec;adaptive mesh refinement;algorithm;emoticon;handshaking;load balancing (computing);out of memory;out-of-core algorithm;patch (computing);refinement (computing);root element;run time (program lifecycle phase);scalability;simulation;space-filling curve;tree-meta;treemapping	Carsten Burstedde;Johannes Holke	2017	SIAM J. Scientific Computing	10.1137/16M1103518	computer science;theoretical computer science;t-vertices	HPC	-5.7504603546189825	40.9296095857238	161726
b28d32c4d136561a841f948de170e9b36c91407e	asynchronous parallel molecular dynamics simulations	parallel algorithm;molecular dynamic simulation;computer architecture;object oriented;parallel computer;molecular dynamic;scalable coding	Dynamic development of parallel computers makes them standard tool for large simulations. The technology achievements are not followed by the progress in scalable code design. The molecular dynamics is a good example. In this paper we present novel approach to the molecular dynamics which is based on the new asynchronous parallel algorithm inspired by the novel computer architectures. We present also implementation of the algorithm written in Java. Presented code is object-oriented, multithread and distributed. The performance data is also available.	computer simulation;molecular dynamics	Jaroslaw Mederski;Lukasz Mikulski;Piotr Bala	2007		10.1007/978-3-540-68111-3_46	molecular dynamics;parallel computing;computer science;theoretical computer science;distributed computing;parallel algorithm;object-oriented programming	HPC	-10.228881914699942	39.38694253016358	162577
aa921441e890402908f447b47a41e210673885b5	trace-driven simulation of decoupled architectures	computer architecture;discrete event simulation;performance evaluation;linpack;perfect club benchmarks;compiler-assisted program;decoupled architectures;high-performance computer systems;profiler-assisted program;trace-driven simulation	The development of accurate trace-driven simulat ion models has become a key act iv i ty i n the design of new high-performance computer sys tems. Tracedriven s imulat ion is fas t ) enabling analysis of the behaviour of large application and benchmark programs on a new computer sys tem. W e describe a trace-driven s imulat ion engine for a decoupled processor architecture. W e report on t w o ways of generating ef ic ient execution traces of real programs for this engine: profiler-assisted and compiler-assisted, and also on the use of synthet ic traces. Results are reported f o r ezecution on the s imulation engine of traces f r o m Linpack and one of the Perfect Club benchmarks, as wel l as synthesized traces.	benchmark (computing);compiler;feedback arc set;lunpack;microarchitecture;nonlinear system;simulation;supercomputer;synthetic intelligence;tracing (software)	Sathiamoorthy Manoharan;Nigel P. Topham;A. W. R. Crawford	1994			gold;computer simulation;process design;benchmark;computer architecture;application software;parallel computing;real-time computing;microarchitecture;computer science;discrete event simulation;operating system;computational model	Arch	-10.13259764015706	41.00718941551244	163099
29d61226250099fe59f1b321b9989f5251fc33b8	high throughput grid computing with an ibm blue gene/l	databases;individual processors schedule;processor scheduling;startup latency;resource management;standard grid middleware;coprocessors;coupled climate model high throughput grid computing ibm blue gene l massively parallel mpi single processor jobs individual processors schedule grid enabled interface supporting single processor tasks globus gram standard grid middleware daymet partition execution system scheduler startup latency;monitoring;ibm blue gene l;system scheduler;single processor jobs;processor scheduling grid computing middleware;middleware;scalability;massively parallel mpi;cobalt;high throughput;daymet;grid computing;cobalt databases resource management coprocessors monitoring throughput scalability;partition execution;globus gram;throughput;grid enabled interface supporting;high throughput grid computing;single processor tasks;coupled climate model	While much high-performance computing is performed using massively parallel MPI applications, many workflows execute jobs with a mix of processor counts. At the extreme end of the scale, some workloads consist of large quantities of single-processor jobs. These types of workflows lead to inefficient usage of massively parallel architectures such as the IBM Blue Gene/L (BG/L) because of allocation constraints forced by its unique system design. Recently, IBM introduced the ability to schedule individual processors on BG/L - a feature named high throughput computing (HTC) - creating an opportunity to exploit the systempsilas power efficiency for other classes of computing. In this paper, we present a Grid-enabled interface supporting HTC on BG/L. This interface accepts single-processor tasks using Globus GRAM, aggregates HTC tasks into BG/L partitions, and requests partition execution using the underlying system scheduler. By separating HTC task aggregation from scheduling, we provide the ability for workflows constructed using standard Grid middleware to run both parallel and serial jobs on the BG/L. We examine the startup latency and performance of running large quantities of HTC jobs. Finally, we deploy Daymet, a component of a coupled climate model, on a BG/L system using our HTC interface.	blue gene;central processing unit;climate model;commodity computing;grid computing;high-throughput computing;job control (unix);job scheduler;job stream;middleware;performance per watt;scheduling (computing);supercomputer;systems design;taito l system;throughput	Jason Cope;Michael Oberg;Henry M. Tufo;Theron Voran;Matthew Woitaszek	2007	2007 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2007.4629250	high-throughput screening;throughput;parallel computing;real-time computing;scalability;cobalt;computer science;resource management;operating system;middleware;coprocessor;grid computing	HPC	-7.6987840749510745	44.169272688032905	163481
9f669898189a4410f2f6f4901d7dd8d94c59dae8	a scalable infiniband network topology-aware performance analysis tool for mpi	high performance computing;performance analysis tool;mvapich2 mpi library;hpc cluster;hpc application;ib network;fat tree;communication pattern;scalable infiniband network topology-aware;performance overhead;ib network topology;network topology-aware manner	Over the last decade, InfiniBand (IB) has become an increasingly popular interconnect for deploying modern supercomputing systems. As supercomputing systems grow in size and scale, the impact of IB network topology on the performance of high performance computing (HPC) applications also increase. Depending on the kind of network (FAT Tree, Tori, or Mesh), the number of network hops involved in data transfer varies. No tool currently exists that allows users of such large-scale clusters to analyze and visualize the communication pattern of HPC applications in a network topology-aware manner. In this paper, we take up this challenge and design a scalable, low-overhead InfiniBand Network Topology-Aware Performance Analysis Tool for MPI INTAP-MPI. INTAP-MPI allows users to analyze and visualize the communication pattern of HPC applications on any IB network (FAT Tree, Tori, or Mesh). We integrate INTAP-MPI into the MVAPICH2 MPI library, allowing users of HPC clusters to seamlessly use it for analyzing their applications. Our experimental analysis shows that the INTAP-MPI is able to profile and visualize the communication pattern of applications with very low memory and performance overhead at scale.	fat tree;infiniband;library (computing);message passing interface;network topology;overhead (computing);profiling (computer programming);scalability;supercomputer	Hari Subramoni;Jérôme Vienne;Dhabaleswar K. Panda	2012		10.1007/978-3-642-36949-0_49	parallel computing;computer science;operating system;distributed computing	HPC	-9.319224328318963	46.02529776500383	164571
78b5a12c3144e2a5d9d95eacea0a7f42ea2fbae7	parallelizing the lm osem image reconstruction on multi-core clusters		In this paper we present four different parallel implementations of the popular LM OSEM medical image reconstruction algorithm. While two of them use libraries such as MPI, OpenMP, or Threading Building Blocks (TBB) directly, the other two implementations use algorithmic skeletons of the Münster Skeleton Library Muesli to hide the parallelism. We compare the implementations w.r.t. runtime, efficiency, and programming style and show the resulting benchmarks which have been conducted on a multi-processor, multi-core cluster computer.	algorithm;algorithmic skeleton;automatic parallelization;computer cluster;iterative reconstruction;library (computing);message passing interface;multi-core processor;multiprocessing;openmp;parallel computing;programming style;threading building blocks	Philipp Ciechanowicz;Philipp Kegel;Maraike Schellmann;Sergei Gorlatch;Herbert Kuchen	2009		10.3233/978-1-60750-530-3-169	iterative reconstruction;multi-core processor;algorithmic skeleton;skeleton (computer programming);theoretical computer science;implementation;parallel computing;cluster (physics);computer cluster;computer science;programming style	HPC	-6.253811400156435	42.90004060757687	164648
c0750b408ccce3ccca96ff051bcbc5d3bece1454	adaptive loop tiling for a multi-cluster cmp	virtual machine;java programming;data locality;chip multiprocessor;feedback directed optimization;loop tiling;lattice boltzmann;automatic parallelization;multi cluster cmp	Loop tiling is a fundamental optimization for improving data locality. Selecting the right tile size combined with the parallelization of loops can provide additional performance increases in the modern of Chip MultiProcessor (CMP) architectures. This paper presents a runtime optimization system which automatically parallelizes loops and searches empirically for the best tile sizes on a scalable multi-cluster CMP. The system is built on top of a virtual machine and targets the runtime parallelization and optimization of Java programs. Experimental results show that runtime parallelization and tile size searching are capable of improving performance for two BLAS kernels and one Lattice-Boltzmann simulation, despite overheads.	blas;java;locality of reference;mathematical optimization;parallel computing;scalability;separation kernel;simulation;tiling window manager;virtual machine	Jisheng Zhao;Matthew Horsnell;Mikel Luján;Ian Rogers;Chris C. Kirkham;Ian Watson	2008		10.1007/978-3-540-69501-1_23	loop tiling;computer architecture;parallel computing;real-time computing;computer science;virtual machine;operating system;lattice boltzmann methods;automatic parallelization	HPC	-6.697050423213122	45.76682345159703	165094
7964be136784ab3bd01f0bab5140800e27c83b58	improving the performance of grid-enabled mpi by intelligent message compression			message passing interface	Hwang-Jik Lee;Kyung-Lang Park;Kwang-Won Koh;Oh-Young Kwon;Hyoung-Woo Park;Shin-Dug Kim	2003			grid;compression (physics);computer science;distributed computing	HPC	-8.993581686730057	41.89496459216177	165217
10446e36578c2952d4f9b90b3eaa2ae552e9295e	embree: a kernel framework for efficient cpu ray tracing	simd;coprocessor;spmd;ray tracing;cpu	We describe Embree, an open source ray tracing framework for x86 CPUs. Embree is explicitly designed to achieve high performance in professional rendering environments in which complex geometry and incoherent ray distributions are common. Embree consists of a set of low-level kernels that maximize utilization of modern CPU architectures, and an API which enables these kernels to be used in existing renderers with minimal programmer effort. In this paper, we describe the design goals and software architecture of Embree, and show that for secondary rays in particular, the performance of Embree is competitive with (and often higher than) existing state-of-the-art methods on CPUs and GPUs.	application programming interface;central processing unit;graphics processing unit;high- and low-level;kernel (operating system);open-source software;programmer;ray tracing (graphics);software architecture;x86	Ingo Wald;Sven Woop;Carsten Benthin;Gregory S. Johnson;Manfred Ernst	2014	ACM Trans. Graph.	10.1145/2601097.2601199	ray tracing;computer architecture;parallel computing;simd;computer science;operating system;central processing unit;spmd;coprocessor	Graphics	-6.23264766723901	44.17188553953224	165598
8659f3bdd70ff0ae31104e581e63c898aa75680f	automatic opencl code generation for multi-device heterogeneous architectures	nvidia quadro fx 5800;kernel;specification languages graphics processing units parallel processing program compilers resource allocation;paper;performance evaluation;heterogeneous systems;accelerators;code generation;accelerators heterogeneous architectures opencl code generation;domain specific language automatic opencl code generation multidevice heterogeneous architectures gpu xeon phis data parallel applications interaccelerator data movements dynamic load balancing programming tool stepocl;arrays;heterogeneous architectures;synchronization;nvidia;writing;kernel performance evaluation programming hardware arrays writing synchronization;computer science;opencl;intel xeon phi;programming;hardware	Using multiple accelerators, such as GPUs or Xeon Phis, is attractive to improve the performance of large data parallel applications and to increase the size of their workloads. However, writing an application for multiple accelerators remains today challenging because going from a single accelerator to multiple ones indeed requires to deal with potentially non-uniform domain decomposition, inter-accelerator data movements, and dynamic load balancing. Writing such code manually is time consuming and error-prone. In this paper, we propose a new programming tool called STEPOCL along with a new domain specific language designed to simplify the development of an application for multiple accelerators. We evaluate both the performance and the usefulness of STEPOCL with three applications and show that: (i) the performance of an application written with STEPOCL scales linearly with the number of accelerators, (ii) the performance of an application written using STEPOCL competes with a handwritten version, (iii) larger workloads run on multiple devices that do not fit in the memory of a single device, (iv) thanks to STEPOCL, the number of lines of code required to write an application for multiple accelerators is roughly divided by ten.	cognitive dimensions of notations;compute kernel;data parallelism;domain decomposition methods;domain-specific language;graphics processing unit;heuristic (computer science);kernel (operating system);load balancing (computing);mathematical optimization;matrix multiplication;online and offline;opencl api;profiling (computer programming);programming tool;rewrite (programming);source lines of code	Pei Li;Elisabeth Brunet;François Trahay;Christian Parrot;Gaël Thomas;Raymond Namyst	2015	2015 44th International Conference on Parallel Processing	10.1109/ICPP.2015.105	synchronization;programming;computer architecture;parallel computing;kernel;telecommunications;computer science;operating system;xeon phi;programming language;writing;code generation	HPC	-6.265914563568314	43.624481296997054	165823
0d1544a0cee22d7796858893cfd7258d830758d6	predicting parallelization of sequential programs using supervised learning	support vector machines;program control structures;parallel programming;decision trees support vector machines training feature extraction benchmark testing accuracy;feature extraction;application program interfaces;pattern classification;support vector machines application program interfaces decision trees feature extraction learning artificial intelligence parallel programming pattern classification program control structures;learning artificial intelligence;decision trees;dependency graph features sequential program parallelization prediction dynamic features supervised learning algorithm training nas parallel benchmark code npb code openmp parallelization directives support vector machines classification problem adaboost program features within loop instruction counts decision tree error reduction	We investigate an automatic method for classifying which regions of sequential programs could be parallelized, using dynamic features of the code collected at runtime. We train a supervised learning algorithm on versions of the NAS Parallel Benchmark (NPB) code hand-annotated with OpenMP parallelization directives in order to approximate the parallelization that might be produced by a human expert. A model comparison shows that support vector machines and decision trees have comparable performance on this classification problem, but boosting using AdaBoost is able to increase the performance of the decision trees. We further analyze the relative importance of the collected program features and demonstrate that within-loop instruction counts provide the greatest contribution to decision tree error reduction, with dependency graph features of secondary importance.	adaboost;approximation algorithm;automatic parallelization;benchmark (computing);decision tree;model selection;openmp;parallel computing;run time (program lifecycle phase);supervised learning;support vector machine	Daniel Fried;Zhen Li;Ali Jannesari;Felix Wolf	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.108	support vector machine;feature extraction;computer science;theoretical computer science;machine learning;decision tree;pattern recognition;automatic parallelization	ML	-4.5935995704935	44.801321497447404	166280
c76dc2dc724714f04b5be01684c7292478ce6edf	active pixel merging on hypercube multicomputers	load balance	1 I n t r o d u c t i o n There are two approaches for parallel polygon rendering in multicomputers; image-space parallelism [1, 2, 3] and object-space parallelism [4, 5, 6]. In objectspace parallel rendering, input polygons are parti t ioned among the processors. Each processor, then, runs a sequential rendering algorithm for its local polygons. Each generated pixel is locally z-buffered to eliminate local hidden pixels. After local z-buffering, pixels generated in each processor should be globally merged, because more than one processor may produce a pixel for the same screen coordinate. The global z-buffering operations during the pixel merging phase can be considered as an overhead to the sequential rendering. Furthermore, each global z-buffering operation necessitates interprocessor communication. Efficient implementat ion of the pixel merging phase is thus a crucial factor for the performance of object-space parallel rendering. In its simplest form, pixel merging phase can be performed by exchanging pixel information for all pixel locations between processors. We will call this scheme full z-buffer merging. This scheme may introduce large communication overhead in pixel merging phase because pixel information for inactive pixel locations are also exchanged. This overhead can be reduced * This work is partially supported by Intel Supercomputer Systems Division grant no. SSD100791-2 and The Scientific and Technical Research Council of Turkey (T/~IBiTAK) grant no. EEEAG-5.	algorithm;central processing unit;distributed computing;inter-process communication;overhead (computing);parallel computing;parallel rendering;pixel;supercomputer;z-buffering	Tahsin M. Kurç;Cevdet Aykanat;Bülent Özgüç	1996		10.1007/3-540-61142-8_565	computer architecture;parallel computing;computer science;load balancing;distributed computing	Visualization	-6.69496377668851	40.97365945705365	167074
63b0697ea7561029d648f5fc3cf940c29f2701b1	optimizing graph algorithms in asymmetric multicore processors		Asymmetric multicore processors (AMP) fall under a special subcategory of modern-day heterogeneous multicore architectures with different participating core types executing a common instruction set architecture. The innate asymmetry in the performance of different cores in AMPs poses interesting challenges. Irregular workloads, such as graph algorithms, intensify these challenges as the parallel workloads in these algorithms cannot be precisely characterized at compile time. In this paper, we propose a framework named scheduler for irregular AMPs, which optimizes the efficiency of the given AMP system for a given algorithm-graph pair by optimizing the graph representation and using a predictor to find the optimal configurations to run the algorithm-graph pair. The optimization is performed in two stages: 1) finding an optimal graph representation and 2) finding an optimal hardware configuration to run the input algorithm-graph pair. We have tested the efficiency of our system on five different graph algorithms over eight real-world and synthetic graphs. On an average, we see 42.82% improvement in energy delay product over the base case.	central processing unit;compile time;electronic data processing;graph (abstract data type);graph theory;kerrison predictor;list of algorithms;loss function;mathematical optimization;multi-core processor;nonlinear system;optimizing compiler;parallel algorithm;recursion;scheduling (computing);synthetic data	Michaela Trilck-Winkler;Rupesh Nasre	2018	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2018.2858366	computer science;graph (abstract data type);dynamic priority scheduling;algorithm;instruction set;multi-core processor;subcategory;graph;compile time	Arch	-4.643023156810159	44.77016577245087	167584
0ff09bd48ad14062d75680c6b8897edd925e44d1	a parallel loop self-scheduling on extremely heterogeneous pc clusters	heterogeneous cluster;heterogeneous environment;loop scheduling;cluster system;pc cluster;message passing;load balance	Cluster computers are a viable and less expensive alternative to symmetric multiprocessor systems. However, a serious difficulty in concurrent programming of a cluster computer system is how to deal with scheduling and load balancing of such a system which may consist of heterogeneous computers. Self-scheduling schemes suitable for parallel loops with independent iterations on heterogeneous computer clusters have been designed in the past. These schemes, such as FSS, GSS and TSS, can achieve load balancing in SMP, even in a moderate heterogeneous environment, but are not suitable in extremely heterogeneous environments. In this paper, we propose a heuristic approach to solve parallel loop scheduling problems on an extremely heterogeneous PC cluster environment.	analysis of algorithms;central processing unit;clock rate;computer cluster;concurrent computing;data parallelism;flying-spot scanner;heuristic;iteration;load balancing (computing);loop scheduling;matrix multiplication;scheduling (computing);symmetric multiprocessing	Chao-Tung Yang;Shun-Chyi Chang	2003	J. Inf. Sci. Eng.	10.1007/3-540-44864-0_112	parallel computing;message passing;real-time computing;computer science;load balancing;distributed computing;programming language	HPC	-7.957941377776325	41.97368158798652	167773
ab69dd990461e9f3ac3cc81934265cd90e9c3ed3	divide and conquer parallelisation methods for digital image processing algorithms			algorithm;digital image processing;parallel computing	Andreas Bieniek	2000				Graphics	-9.066379578034663	41.5420185994305	167917
d9ebf8426f0a0d6d719ce2b76a870e5eace0e63a	memonet: network interface plugged into a memory slot	programming environments;personal computer;software prototyping;programming environments network interfaces optical links workstation clusters;prototypes;network interfaces delay estimation bandwidth computer architecture software prototyping prototypes optical fiber communication central processing unit hardware protection;high performance distributed computing;computer architecture;protection;network interfaces;optical links;on the fly;bandwidth;workstation clusters;network interface;optical fiber communication;distributed computing environment memonet network interface memory slot communication architecture full duplex optical link hardware delay atomic on the fly sending;delay estimation;central processing unit;hardware	The communication architecture of the DIMMnet-I network interface, based on MEMOnet, is described. MEMOnet is an architecture consisting of a network integace plugged into a memory slot. The DIMMnet-I prototype will have two banks of PC133 based SO-DIMM slots and an 8Gbps full duplex optical link or two 448M%/s full duplex LVDS channel links. The sofrware overhead incurred to generate a message is only 1 CPU cycle and the estimated hardware delay is less than IOOns using the atomic on-the$y sending with header TL%. The estimated achievable communication bandwidth with block on-the& sending with protection stampable window memory is 44OMB/s which was observed in our experiments writing to the DIMM area with a write combining attribute. This is 3.3 times higher than the maximum bandwidth of PCI. This high performance distributed computing environment is available using economical personal computers with DIMM slots.	dimm;distributed computing environment;duplex (telecommunications);experiment;instruction cycle;low-voltage differential signaling;network interface;overhead (computing);pc133;personal computer;prototype;so-dimm;write combining	Noboru Tanabe;Junji Yamamoto;Hiroaki Nishi;Tomohiro Kudoh;Yoshihiro Hamada;Hironori Nakajo;Hideharu Amano	2000		10.1109/CLUSTR.2000.888988	embedded system;parallel computing;real-time computing;computer hardware;computer science;network interface;operating system;computer network	HPC	-11.824097892691706	46.36002863484078	168066
2fe48a3ebbc3f907d6310609f724ed4a280e51d5	using surface effect measures to model parallel performance	parallel performance;domain decomposition;modeling and simulation;three dimensional;waiting time;communication cost;parallel implementation;parallel programs	Many factors affect the performance of parallel programs including idle time, wait time, and communication costs. The latter often constitute a significant source of overhead. We consider in this paper the impact of such costs on the performance of parallel implementations of three domain decompositions of a three-dimensional model for tissue growth on a cluster. These are regular domain decompositions that comprise a slab decomposition, a rod decomposition, and a block decomposition. Using a set of measures that qualify each decomposition, we explore their capability to predict the parallel performance of each implementation. The results of our experiments confirm the applicability of using these measures as predictors of performance for our application, and potentially for other similar ones.	3d modeling;experiment;overhead (computing);slab allocation	Belgacem Ben Youssef	2010		10.1145/1774088.1774587	three-dimensional space;real-time computing;simulation;computer science;modeling and simulation;distributed computing;domain decomposition methods	HPC	-9.297116105763948	39.73978742800552	168559
4d51ca669400e53c7639f89ae8653e2bff691acc	model-driven simd code generation for a multi-resolution tensor kernel	tensile stress registers kernel arrays generators indexes;kernel;generators;tensile stress;code generation;arrays;indexes;tensors parallel processing;registers;indexation;vector code synthesizer model driven simd code generation multiresolution tensor kernel model driven compile time code generator tensor contraction expression short vector simd code madness quantum chemistry application c based implementation model driven code generator optimal loop permutation splat operation short vector simd architecture;multi resolution;parallel processing;tensors	In this paper, we describe a model-driven compile-time code generator that transforms a class of tensor contraction expressions into highly optimized short-vector SIMD code. We use as a case study a multi-resolution tensor kernel from the MADNESS quantum chemistry application. Performance of a C-based implementation is low, and because the dimensions of the tensors are small, performance using vendor optimized BLAS libraries is also sub optimal. We develop a model-driven code generator that determines the optimal loop permutation and placement of vector load/store, transpose, and splat operations in the generated code, enabling portable performance on short-vector SIMD architectures. Experimental results on an SSE-based platform demonstrate the efficiency of the vector-code synthesizer.	blas;code generation (compiler);compile time;compiler;computation;contraction mapping;intrinsic function;kernel (operating system);library (computing);madness;model-driven architecture;model-driven integration;software portability;streaming simd extensions;texture splatting	Kevin Stock;Thomas Henretty;Iyyappa Murugandi;P. Sadayappan;Robert J. Harrison	2011	2011 IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2011.101	systematic code;database index;parallel processing;parallel computing;kernel;tensor;computer hardware;computer science;theoretical computer science;operating system;distributed computing;processor register;stress;dual code;programming language;algorithm;code generation	HPC	-5.735507224919453	43.10748434452291	168990
a6cec2d78fc1c53758e618c36b1db56a4916db8b	performance evaluation of the sun fire link smp clusters	remote shared memory;memory based interconnect;performance evaluation;high performance computing;sun fire link;multiprocessor clusters;openmp;mpi;clusters of multiprocessors;rsm;smp cluster;system area networks	The interconnection networks and the communication system software are critical in achieving high performance in clusters. Sun Fire Link interconnect, is a memory-based interconnect, where the Sun MPI uses the Remote Shared Memory (RSM) model for its user-level inter-node messaging. This paper presents the overall architecture of the Sun Fire Link and its messaging layer. We provide an in-depth performance evaluation of a Sun Fire Link cluster at the RSM, MPI, and application layers. The MPI ping-pong latency and bandwidth are five microseconds and 660 MB/s, respectively. In general, the Sun Fire Link cluster performs relatively well in most cases.	computer cluster;interconnection;megabyte;message passing interface;performance evaluation;response surface methodology;shared memory;user space	Ying Qian;Ahmad Afsahi;Nathan R. Fredrickson;Reza Zamani	2004	IJHPCN	10.1504/IJHPCN.2006.013476	embedded system;supercomputer;parallel computing;real-time computing;computer science;message passing interface;operating system	HPC	-10.332966422336348	46.28790423350309	169082
9fe682e6761fffbe8d1961ce373a8ed29db94bd8	cluster programming using the openmp accelerator model		Computation offloading is a programming model in which program fragments (e.g., hot loops) are annotated so that their execution is performed in dedicated hardware or accelerator devices. Although offloading has been extensively used to move computation to GPUs, through directive-based annotation standards like OpenMP, offloading computation to very large computer clusters can become a complex and cumbersome task. It typically requires mixing programming models (e.g., OpenMP and MPI) and languages (e.g., C/C++ and Scala), dealing with various access control mechanisms from different cloud providers (e.g., AWS and Azure), and integrating all this into a single application. This article introduces computer cluster nodes as simple OpenMP offloading devices that can be used either from a local computer or from the cluster head-node. It proposes a methodology that transforms OpenMP directives to Spark runtime calls with fully integrated communication management, in a way that a cluster appears to the programmer as yet another accelerator device. Experiments using LLVM 3.8, OpenMP 4.5 on well known cloud infrastructures (Microsoft Azure and Amazon EC2) show the viability of the proposed approach, enable a thorough analysis of its performance, and make a comparison with an MPI implementation. The results show that although data transfers can impose overheads, cloud offloading from a local machine can still achieve promising speedups for larger granularity: up to 115× in 256 cores for the 2MM benchmark using 1GB sparse matrices. In addition, the parallel implementation of a complex and relevant scientific application reveals a 80× speedup on a 320 core machine when executed directly from the headnode of the cluster.	access control;amazon elastic compute cloud (ec2);amazon web services;benchmark (computing);c++;cloud computing;cloud storage;computation offloading;computer cluster;control system;data compression;directive (programming);distributed computing;experiment;gigabyte;graphics processing unit;llvm;mathematical optimization;message passing interface;microsoft azure;openmp;overhead (computing);parallel computing;pipeline (computing);polyhedron;programmer;programming model;programming paradigm;regular expression;scala;sparse matrix;speedup;yet another	Hervé Yviquel;Esther K. Kemper;Guido Araujo	2018	TACO	10.1145/3226112	parallel computing;cloud computing;scala;programming paradigm;granularity;computer science;speedup;directive;computation offloading;computer cluster	HPC	-6.4928337066313535	43.952516101550884	169450
2c41396323b7e80c0112087177df922829a7c466	flux, sorting, and supercomputer organization for ai applications	computers;digital computers;general and miscellaneous mathematics computing and information science;sorting;point to point;information retrieval;performance;mathematical logic;equipment interfaces;artificial intelligent;computer architecture;massively parallel computer;sorting network;array processors;artificial intelligence;algorithms;design;logic programs;programming 990210 supercomputers 1987 1989;supercomputers;parallel processing	A central issue for the design of massively parallel computers is their ability to sort. We consider organizations that are suitable for fast sorting, both those that use point-to-point connections and those that connect processors with multipoint nets. We show that for fast sorting and minimal area, nets must connect at least G nodes each, if the network has n nodes. We then discuss some of the ways that fast-sorting networks can be used to speed up other processes, such as combinatorial search.	central processing unit;combinatorial search;computer;multipoint ground;parallel computing;point-to-point protocol;sorting network;supercomputer	Jeffrey D. Ullman	1984	J. Parallel Distrib. Comput.	10.1016/0743-7315(84)90002-9	parallel processing;design;mathematical logic;parallel computing;performance;sorting network;computer science;sorting;theoretical computer science;external sorting;distributed computing;algorithm	Theory	-9.19144351033192	40.16839701162963	169615
5076c97e746283b17f2161b5164f6dbc487b2dfa	asynchronous parallel computation of self-organizing maps	parallel computer		computation;organizing (structure);parallel computing	Maurice W. Benson;Jie Hu	2003			asynchronous communication;theoretical computer science;self-organizing map;computer science;distributed computing	Logic	-9.969664832376852	42.068713027036225	169847
148c618a871d6017f4017908ef027a0391b57cc6	an approach towards automation firmware modeling for an exploration and evaluation of efficient parallelization alternatives	graph theory;eeepa tool chain;automation firmware;instruments;automation firmware modeling;software modeling;parallelization alternative;multi core architectures;dynamic event logs;firmware;runtime;software engineering;runtime synchronization schedules multicore processing adaptation model writing instruments;dynamic event logs automation firmware modeling parallelization alternative multicore architecture corporate software development parallel firmware design model based exploration eeepa tool chain graph based firmware modeling;software engineering firmware graph theory parallel processing;lines of code;adaptation model;synchronization;model based exploration;multicore processing;software development;schedules;graph based firmware modeling;writing;multi core architectures automation firmware software modeling static parallelization;multicore architecture;parallel firmware design;parallel processing;corporate software development;static parallelization	Due to stagnating CPU cycles, future performance gains in automation firmware are unlikely to be achieved without parallelization for multi-core architectures. However, for a sophisticated system comprising millions of lines of code, this process induces significant effort, especially when having to keep real time and safety conditions. As efficiency matters in corporate software development, obtaining maximum speedup by spending no more implementation effort than necessary is intended. Thus, the design of a parallel firmware is recommended to base on the results of a model-based exploration and evaluation of efficient parallelization alternatives. For this purpose, we developed the EEEPA tool chain, that starts with graph-based firmware modeling on basis of dynamic event logs.	automatic parallelization;automation;central processing unit;computer multitasking;firmware;genetic algorithm;heuristic;multi-core processor;multi-objective optimization;np-completeness;parallel computing;pareto efficiency;software development;source lines of code;speedup;toolchain	Jürgen Bregenzer;Julian Hartmann	2011	2011 Sixth International Symposium on Parallel Computing in Electrical Engineering	10.1109/PARELEC.2011.35	multi-core processor;firmware;parallel processing;synchronization;computer architecture;parallel computing;real-time computing;schedule;computer science;graph theory;software development;operating system;modeling language;programming language;writing;source lines of code;automatic parallelization	Arch	-5.9343750801977	45.630132006670344	170192
96ba7256cf6677fb0efb222b9c0ce982a3de2fab	dynamic scheduling monte-carlo framework for multi-accelerator heterogeneous clusters	exotic option;partial differential equation;kernel;paper;scheduling autoregressive processes computer graphic equipment coprocessors field programmable gate arrays monte carlo methods pricing;heterogeneous systems;heterogeneous cluster;energy efficient;processor scheduling;pricing;simulation framework;computer graphic equipment;distributed computing;16 amd phenom 9650 quad core 2 4ghz cpu;generalized autoregressive conditional heteroskedasticity model dynamic scheduling monte carlo simulation framework multiaccelerator heterogeneous clusters asset simulation option pricing 8 virtex 5 xc5vlx330t fpga 8 tesla c1060 gpu 16 amd phenom 9650 quad core 2 4ghz cpu garch asset simulation efficient allocation line;hardware accelerator;option pricing;fpga;coprocessors;tesla c1060;cuda;computational modeling;autoregressive processes;energy consumption;scheduling;generalized autoregressive conditional heteroskedasticity model;mathematical model;nvidia;8 tesla c1060 gpu;field programmable gate arrays computational modeling kernel processor scheduling hardware graphics processing unit mathematical model;garch asset simulation;asset simulation;computer science;field programmable gate arrays;monte carlo;graphics processing unit;monte carlo simulation;multiaccelerator heterogeneous clusters;efficient allocation line;dynamic scheduling monte carlo simulation framework;monte carlo methods;8 virtex 5 xc5vlx330t fpga;dynamic scheduling;hardware	Monte-Carlo (MC) simulation is an effective tool for solving complex problems such as many-body simulation, exotic option pricing and partial differential equation solving. The huge amount of computation in MC makes it a good candidate for acceleration using hardware and distributed computing platforms. We propose a novel MC simulation framework suitable for a wide range of problems. This framework enables different hardware accelerators in a multi-accelerator heterogeneous cluster to work collaboratively on a single application. It also provides scheduling interfaces to adaptively balance the workload according to the cluster status. Two financial applications, involving asset simulation and option pricing, are built using this framework to demonstrate its capability and flexibility. A cluster with 8 Virtex-5 xc5vlx330t FPGAs and 8 Tesla C1060 GPUs using the proposed framework provides 44 times speedup and 19.6 times improved energy efficiency over a cluster with 16 AMD Phenom 9650 quad-core 2.4GHz CPUs for the GARCH asset simulation application. The Efficient Allocation Line (EAL) is proposed for determining the most efficient allocation of accelerators for either performance or energy consumption.	central processing unit;computation;computer hardware;data dependency;distributed computing;equation solving;evaluation assurance level;field-programmable gate array;graphics processing unit;hardware acceleration;load balancing (computing);many-body problem;monte carlo method;multi-core processor;scalability;scheduling (computing);simulation;speedup	Anson H. T. Tse;David B. Thomas;Kuen Hung Tsoi;Wayne Luk	2010	2010 International Conference on Field-Programmable Technology	10.1109/FPT.2010.5681495	embedded system;parallel computing;real-time computing;simulation;computer science;operating system;monte carlo method	HPC	-4.854522621419253	41.563785313519155	170209
de9129dda4a919dc687158f071aef1d71f76c0b1	meta-vipios: harness distributed i/o resources with vipios	distributed 1 0;cluster computing;paralled 1 0;mpi io	Introduction 1 Two factor8 8trongly inftuenced the re8earch in high performance computing in the la8t few year8, the 1/0 bottleneck and clu8ter 8y8tem8. Fir8tly, for many 8upercomputing application8 the limiting factor i8 not the number of available CPU8 anymore, but the bandwidth of the di8k 1/0 8y8tem. Secondly, a 8hift from the cla88ical, co8tly 8upercomputer 8y8tem8 to affordable clu8ter8 of WOrk8tation8 i8 apparent, which allow8 problem 8olution8 to a much lower price. A8 a re8ult we pre8ent in thi8 paper the Vienna Parallel Input Output Sy8tem (ViP1OS), which harne88e8 1/0 re8ource8 available in clu8ter type 8y8tem8 for high performance (parallel and/o'f' di8tributed) application8. ViP1OS i8 a client8erver ba8ed 8y8te11i to increa8e the bandwidth of di8k acce88e8 by (re)di8tributing the data among available 1/0 re8ource8 and parallelizing the execution 8cheme. 1t follow8 a data engineering. approach by combining characteri8tic8 of parallel 1/0 runtime librarie8 and parallel file 8y8tem8 with a 8mart admini8tration module.	division by zero;information engineering;input/output;parallel computing;supercomputer;test harness	Thomas Fuerle;Oliver Jorns;Erich Schikuta;Helmut Wanek	2000	Computación y Sistemas		parallel computing;real-time computing;computer science;distributed computing	DB	-8.474155883760453	40.2902842167567	170412
b1f31bb4b24c694a72340d44798bdcec49fcaf50	actorx10: an actor library for x10	high performance computing;parallel programming;actor based programming;stream processing	The APGAS programming model is a powerful computing paradigm for multi-core and massively parallel computer architectures. It allows for the dynamic creation and distribution of thousands of threads amongst hundreds of nodes in a cluster computer within a single application. For programs of such a complexity, appropriate higher level abstractions on computation and communication are necessary for performance analysis and optimization. In this work, we present actorX10, an X10 library of a formally specified actor model based on the APGAS principles. The realized actor model explicitly exposes communication paths and decouples these from the control flow of the concurrently executed application components. Our approach provides the right abstraction for a wide range of applications. Its capabilities and advantages are introduced and demonstrated for two applications from the embedded system and HPC domain, i.e., an object detection chain and a proxy application for the simulation of tsunami events.	actor model;component-based software engineering;computation;computer architecture;computer cluster;control flow;embedded system;mathematical optimization;multi-core processor;object detection;parallel computing;profiling (computer programming);programming model;simulation;x10	Sascha Roloff;Alexander Pöppl;Tobias Schwarzer;Stefan Wildermann;Michael Bader;Michael Glaß;Frank Hannig;Jürgen Teich	2016		10.1145/2931028.2931033	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-7.179690244375014	43.661645636031295	170745
35137f5cda9458876ce57cb0501acf5b08f20b43	implementing parareal - openmp or mpi?		The paper presents a comparison between MPI and OpenMP implementations of the parallel-in-time integration method Parareal. A special-purpose, lightweight FORTRAN code is described, which serves as a benchmark. To allow for a fair comparison, an OpenMP implementation of Parareal with pipelining is introduced, which relies on manual control of locks for synchronisation. Performance is compared in terms of runtime, speedup, memory footprint and energy-to-solution. The pipelined shared memory implementation is found to be the most efficient, particularly with respect to memory footprint. Its higher implementation complexity, however, might make it difficult to use in legacy codes.	benchmark (computing);code;fortran;legacy system;lock (computer science);memory footprint;message passing interface;numerical methods for ordinary differential equations;openmp;parareal;pipeline (computing);run time (program lifecycle phase);shared memory;speedup	Daniel Ruprecht	2015	CoRR		parallel computing;parareal;computer science	HPC	-6.452999856931401	42.767618897954335	171138
f40d8fca3921e9dee0349f4225c063b6ad3b88ab	epilog: re-interpreting and extending prolog for a multiprocessor environment			multiprocessing;prolog	Michael J. Wise	1984			programming language;computer architecture;multiprocessing;computer science;prolog	HPC	-10.688563610975443	42.05158304827454	171312
09eeacc9f95ff89b06eb4134204c5001a082d350	simd and msimd variants of the non-von supercomputer			simd;supercomputer	David Elliot Shaw	1984			computer architecture;parallel computing;simd;computer science;supercomputer	HPC	-9.379593710034818	42.18192146136235	171460
03028a78daf97a01a26975a72c59c8d97cb18810	portable mapping of data parallel programs to opencl for heterogeneous systems	paper;nvidia geforce gtx 580;learning mapping;heterogeneous systems;graphics processing units kernel benchmark testing predictive models indexes arrays feature extraction;ati radeon hd 7970;learning mapping gpu opencl machine;ati;parallel programming;gpu;machine;program compilers application program interfaces graphics processing units multiprocessing systems parallel programming;compilers;machine learning;data parallel program mapping core i7 amd radeon 7970 core i7 nvidia geforce gtx 580 nas parallel benchmark suite openmp code opencl code predictive modeling data transformations heterogeneous multicores high level language data parallel openmp programs compiler general purpose gpu based systems heterogeneous systems;graphics processing units;application program interfaces;nvidia;multiprocessing systems;computer science;program compilers;opencl;programming languages	General purpose GPU based systems are highly attractive as they give potentially massive performance at little cost. Re-alizing such potential is challenging due to the complexity of programming. This paper presents a compiler based approach to automatically generate optimized OpenCL code from data-parallel OpenMP programs for GPUs. Such an approach brings together the benefits of a clear high levellanguage (OpenMP) and an emerging standard (OpenCL) for heterogeneous multi-cores. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architectures and uses predictive modeling to automatically determine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on the multi-core host. We applied our approach to the entire NAS parallel benchmark suite and evaluated it on two distinct GPU based systems: Core i7/NVIDIA GeForce GTX 580 and Core 17/AMD Radeon 7970. We achieved average (up to) speedups of 4.51x and 4.20x (143x and 67x) respectively over a sequential baseline. This is, on average, a factor 1.63 and 1.56 times faster than a hand-coded, GPU-specific OpenCL implementation developed by independent expert programmers.	baseline (configuration management);benchmark (computing);compiler;data parallelism;geforce 500 series;geforce 900 series;general-purpose computing on graphics processing units;graphics processing unit;multi-core processor;network-attached storage;opencl api;openmp;predictive modelling;programmer;radeon hd 7000 series	Dominik Grewe;Zheng Wang;Michael F. P. O'Boyle	2013	Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)	10.1109/CGO.2013.6494993	computer architecture;compiler;machine;parallel computing;computer science;operating system;programming language	Arch	-5.072725473445633	44.897165837117676	171951
1f51c3e1dc098ac412c1c736aaf16f3f3c9a29cb	linux cluster in theory and practice: a novel approach in teaching cluster computing based on the intel atom platform	system engineering;cluster computing;theory and practice;linux cluster;computer architecture;large scale;background knowledge;parallel computer;student learning;teaching practice	Current trends and studies on future architectures show, that the complexity of parallel computer systems is increasing steadily. Hence, the industry requires skilled employees, who have in addition to the theoretical fundamentals, practical experiences in the design and administration of such systems. However, investigations have shown, that practical approaches are still missing in current curricula, especially in these areas. For this reason, the chair of Computer Architecture at the faculty of Computer Science at the Technische Universität Dresden, developed and introduced the course ”Linux Cluster in Theory and Practice” (LCTP). The main objectives of this course are to provide background knowledge about the design and administration of large-scale parallel computer systems and the practical implementation on the available hardware. In addition, students learn how to solve problems in a structured approach and as part of a team. This paper analyzes the current variety of courses in the area of parallel computing systems, describes the structure and implementation of LCTP and provides first conclusions and an outlook on possible further developments.	atom;computer architecture;computer cluster;computer science;linux;microsoft outlook for mac;parallel computing	Andy Georgi;Stefan Hohlig;Robin Geyer;Wolfgang E. Nagel	2011		10.1016/j.procs.2011.04.209	simulation;computer cluster;computer science;artificial intelligence;operating system;data mining;algorithm	DB	-11.646325907770217	39.46483945947013	172067
1812fb662c1dad94a33c9e74a0326203aaec7cc9	exploring manycore multinode systems for irregular applications with fpga prototyping	prototypes;computer architecture;synchronization;field programmable gate arrays;instruction sets;hardware	We propose an intermediate approach between full custom hardware systems and full-software tools. Figure 1 shows the overview of the proposed architecture. We start from an off-the-shelf architecture composed of simple, in-order cores and an on-chip interconnection. The onchip interconnection interfaces the processing core with the memory controller for the external memory (DDR3) and the shared I/O peripherals. We add three custom components: the Global Memory Access Scheduler (GMAS), the Global Network Interface (GNI) and the Global SYNChronization module (GSYNC). The GMAS enables support for the scrambled address space. It also implements part of the support latency tolerance, storing remote memory operations, and acts as a scheduler for lightweight software multithreading.	address space;fpga prototyping;full custom;global network;input/output;interconnection;manycore processor;memory controller;multi-core processor;multithreading (computer architecture);network interface;peripheral;scheduling (computing);tcp global synchronization;thread (computing)	Marco Ceriani;Simone Secchi;Antonino Tumeo;Oreste Villa;Gianluca Palermo	2013	2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/HOTCHIPS.2013.7478329	embedded system;computer architecture;parallel computing;fpga prototype	Arch	-10.93683465181635	46.038589920708205	172677
03ddf3511c3e69af2369a949508a2fc71f74f4e1	compilers and parallel computing systems	keynote talk abstract;programming language;chip;parallel computer;high performance	Increasing the delivered performance of computers by running programs in parallel is an old idea with a new urgency. Multi cores (multi processors) on chips have emerged as a way to increase performance wherever chips are used. The talk will focus on the role programming languages and compilers must play in delivering parallel performance to users and applications. The speaker's personal experiences with languages and compilers for high performance systems will provide the basis for her observations. The talk is intended to encourage the exploration of new approaches.	central processing unit;compiler;computer;multi-core processor;parallel computing;programming language	Frances E. Allen	2008	2008 IEEE 14th International Symposium on High Performance Computer Architecture	10.1145/1345206.1345208	chip;computer architecture;parallel computing;computer science;operating system;programming language	Arch	-9.868325361154417	41.20371557922037	172782
4b2b072810d9147a79cfa72ed58b2210d6d2da78	a flexible patch-based lattice boltzmann parallelization approach for heterogeneous gpu-cpu clusters	paper;lattice boltzmann model;heterogeneous computing;tesla c1060;cuda;heterogeneous computations;tesla s1070;nvidia;lattice boltzmann method;lattice boltzmann;mpi;computer science	Sustaining a large fraction of single GPU performance in par allel computations is considered to be the major problem of GPU-based clusters. In this article, this topic is addressed in the context of a lattice Boltzmann flow solver that is integrated in the Wa LBerla software framework. We propose a multi-GPU implementation using a block-structur ed MPI parallelization, suitable for load balancing and heterogeneous computations on CPUs and G PUs. The overhead required for multi-GPU simulations is discussed in detail and it is demon strated that the kernel performance can be sustained to a large extent. With our GPU implementati o , we achieve nearly perfect weak scalability on InfiniBand clusters. However, in strong scaling scenarios multi-GPUs make less efficient use of the hardware than IBM BG/P and x86 cluste rs. Hence, a cost analysis must determine the best course of action for a particular simulat ion task. Additionally, weak scaling results of heterogeneous simulations conducted on CPUs and GPUs simultaneously are presented using clusters equipped with varying node configurations.	automatic parallelization;central processing unit;computation;computer cluster;graphics processing unit;image scaling;infiniband;job control (unix);lattice boltzmann methods;line printer daemon protocol;load balancing (computing);message passing interface;multiphysics;openmp;overhead (computing);parallel computing;patch (computing);scalability;simulation;software framework;solver;wa-tor;x86	Christian Feichtinger;Johannes Habich;Harald Köstler;Georg Hager;Ulrich Rüde;Gerhard Wellein	2011	Parallel Computing	10.1016/j.parco.2011.03.005	computational science;parallel computing;computer science;theoretical computer science;lattice boltzmann methods	HPC	-5.426708246789149	39.64887985176523	173096
a6c741e017fd4d30bedd945112e02b2c21735391	vcluster: a thread-based java middleware for smp and heterogeneous clusters with thread migration support	parallel programming library;smp machine;heterogeneous machine;fortran programming language;java programming language;new parallel programming model;parallel virtual machine;pure java;message-passing library;ever-increasing performance;heterogeneous cluster;thread migration support;thread-based java middleware	Clusters, composed of symmetric multiprocessor (SMP) machines and heterogeneous machines, have become increasingly popular for high-performance computing. Message-passing libraries, such as message-passing interface (MPI) and parallel virtual machine (PVM), are de facto parallel programming libraries for clusters that usually consist of homogeneous and uni-processor machines. For SMP machines, MPI is combined with multithreading libraries like POSIX Thread and OpenMP to take advantage of the architecture. In addition to existing parallel programming libraries that are in C-C++ and FORTRAN programming languages, the Java programming language presents itself as another alternative with its object-oriented framework, platform neutral byte code, and ever-increasing performance. This paper presents a new parallel programming model and a library, VCluster, which implements this model. VCluster is based on migrating virtual threads instead of processes to support clusters of SMP machines more efficiently. The implementation uses thread migration, which can be used in dynamic load balancing. VCluster was developed in pure Java, utilizing the portability of Java to support clusters of heterogeneous machines. Several applications are developed to illustrate the use of this library and compare the usability and performance of VCluster with other approaches. Copyright © 2007 John Wiley & Sons, Ltd.	java;middleware;process migration;symmetric multiprocessing	Hua Zhang;Joohan Lee;Ratan K. Guha	2008	Softw., Pract. Exper.	10.1002/spe.862	computer architecture;parallel computing;message passing;multithreading;java concurrency;computer science;message passing interface;operating system;middleware;programming language;java	HPC	-11.64235626884884	43.61145822166031	173542
3fc558833ed2a2fa84a9ec4f392359dbfdfd7bda	a layered approach to parallel software performance prediction: a case study	separate characterisation;modular nature;parallelisation technique;case study;layered approach;software performance engineering;parallel software performance prediction;structured layered methodology;parallel system;parallel machine;accurate performance prediction;parallelisation strategy;software performance;parallel systems	An approach to the characterisation of parallel systems using a structured layered methodology is described here. The aim of this is to produce accurate performance predictions which maybe used to influence the choice of machines and investigate implementation trade-offs. The methodology described enables the separate characterisation of both application, and parallel machine to be developed independently but integrated though an intermediary layer encompassing mapping and parallelisation techniques. The layered approach enables characterisations which are modular, re-usable, and can be evaluated using analytical techniques. The approach is based upon methods introduced in Software Performance Engineering (SPE) and structural model decomposition but due to its modular nature, takes less time for development. A case study in image synthesis is considered in which factors from both the application and parallel system are investigated, including the accuracy of predictions, the parallelisation strategy, and scaling behaviour.	defense in depth (computing);image scaling;parallel computing;performance engineering;performance prediction;rendering (computer graphics);software performance testing	Efstathios Papaefstathiou;Darren J. Kerbyson;Graham R. Nudd	1994			computational science;computing;computer science;theoretical computer science;software system;computer engineering	SE	-9.621925703432712	39.797616758059924	173727
57f777c6ae38f79fa250e01c1d4606e387565823	floating point processor for the vax 8600	floating point		floating-point unit;vax 8000	Tryggve Fossum;William J. Grundmann;Virginia Blaha	1985			computer hardware;floating point;floating-point unit;computer science	Theory	-9.56671781497181	42.50317925235653	175814
0da75c9421852051758803ab85d25e8eed04b0bc	automatic routine tuning to represent landform attributes on multicore and multi-gpu systems	parallel computing;auto tuning;landform representation;performance estimation;articulo;multi gpu;multicore	Auto-tuning techniques have been used in the design of routines in recent years. The goal is to develop routines which automatically adapt to the conditions of the computational system in such a way that efficient executions are obtained independently of the end-user experience. This paper aims to explore programming routines that can be automatically adapted to the computational system conditions, making possible to use auto-tuning to represent landform attributes on multicores and multi-GPU systems using high- performance computing techniques for efficient solution of two-dimensional polynomial regression models that allow large problem instances to be addressed.	algorithm;auto-tune;computer simulation;graphics processing unit;mathematical optimization;multi-core processor;polynomial;scalability;self-tuning;the times;user experience	Murilo Boratto;Pedro Alonso;Domingo Giménez;Marcos Barreto	2014	The Journal of Supercomputing	10.1007/s11227-014-1191-0	multi-core processor;parallel computing;real-time computing;simulation;computer science;theoretical computer science;distributed computing	HPC	-5.791450076546361	45.08459536641602	176343
de6d64e212a3e76d2e2f2da94c069f3b3dd77237	automatic calibration of performance models on heterogeneous multicore architectures	linear algebra;task performance;heterogeneous computing;performance estimation;high performance computer;performance model;performance prediction;prediction model;runtime system;hash function	Multicore architectures featuring specialized accelerators are getting an increasing amount of attention, and this success will probably influence the design of future High Performance Computing hardware. Unfortunately, programmers are actually having a hard time trying to exploit all these heterogeneous computing units efficiently, and most existing efforts simply focus on providing tools to offload some computations on available accelerators. Recently, some runtime systems have been designed that exploit the idea of scheduling - as opposed to offloading - parallel tasks over the whole set of heterogeneous computing units. Scheduling tasks over heterogeneous platforms makes it necessary to use accurate prediction models in order to assign each task to its most adequate computing unit [2]. A deep knowledge of the application is usually required to model per-task performance models, based on the algorithmic complexity of the underlying numeric kernel.#R##N##R##N#We present an alternate, auto-tuning performance prediction approach based on performance history tables dynamically built during the application run. This approach does not require that the programmer provides some specific information. We show that, thanks to the use of a carefully chosen hash-function, our approach quickly achieves accurate performance estimations automatically. Our approach even outperforms regular algorithmic performance models with several linear algebra numerical kernels.	multi-core processor	Cédric Augonnet;Samuel Thibault;Raymond Namyst	2009		10.1007/978-3-642-14122-5_9	parallel computing;real-time computing;hash function;computer science;theoretical computer science;linear algebra;operating system;database;distributed computing;predictive modelling;programming language;symmetric multiprocessor system	HPC	-6.764050998294566	46.18448423787864	176402
76a51a050abd611b174e59b94edd6b67d3596a12	partitioning and mapping for parallel nested dissection on distributed memory architectures	distributed memory architecture;cholesky factorization;message passing;load balance;parallel implementation	In this paper, we consider the parallel implementation of a block Cholesky factorization based on a nested dissection ordering for unstructured problems. We focus on loosely coupled networks of many processors with local memory and message passing mechanism. More precisely, we study a parallel block solver associated with refined partitions from the separator partition; the aim is to find the partition corresponding to the correct granularity leading to a high quality mapping (in terms of load balancing for the processors, of average length for the routing paths, and of average edge contention on the network). Then, we propose a refinement algorithm leading to this good granularity, and we provide some numerical measurements using the mapping tool included in the ADAM environment.	distributed memory;nested dissection	Pierre Charrier;Jean Roman	1992		10.1007/3-540-55895-0_424	distributed shared memory;computer architecture;parallel computing;computer science;distributed computing	HPC	-5.843053374711759	40.9526284165552	176414
f870171a29df25427c63e32b55b902b43154c7d6	the effectiveness of combining in reducing hot-spot contention in hypercube multicomputers	hot spot			Sivarama P. Dandamudi;Derek L. Eager	1990			distributed computing;parallel computing;hypercube;computer science;hot spot (veterinary medicine)	NLP	-10.365010133765278	43.469693397897636	177205
1c57bccdb479c668962ca42e5f70f00c4c2b14a8	minimizing communication overhead using pipelining for multi-dimensional fft on distributed memory machines	programming paradigm	 this paper we have presented different algorithms to compute the bi-dimensional FFT. Thesemethods allow the overlapping of the communications by the computations and to reduce thenumber of start-up costs.We have shown that the overlap is total using coarse grain pipelining. The experiments corroboratenicely this theoretical analysis.Some other methods, using the SPMD-like programming paradigm, and other experiments arediscussed in [4].References 	distributed memory;fast fourier transform;pipeline (computing)	Christophe Calvin;Frédéric Desprez	1993			fast fourier transform;parallel computing;distributed memory;programming paradigm;computer science;pipeline (computing);distributed computing	HPC	-4.917290305319443	40.307269038758065	177491
914994d596a4dd74ac50adbe45ca36b6e5e595b6	dynamic load balancing of lattice boltzmann free-surface fluid animations	free surface;dynamic load balancing;lucas kanade;adaptive grid;image stabilisation;lattice boltzmann method;lattice boltzmann;fluid simulation;graphics processing unit	We investigate the use of dynamic load balancing for more efficient parallel Lattice Boltzmann Method (LBM) Free Surface simulations. Our aim is to produce highly detailed fluid simulations with large grid sizes and without the use of optimisation techniques, such as adaptive grids, which may impact on simulation quality. We divide the problem into separate simulation chunks, which can then be distributed over multiple parallel processors. Due to the purely local grid interaction of the LBM, the algorithm parallelises well. However, the highly dynamic nature of typical scenes means that there is an unbalanced distribution of the fluid across the processors. Our proposed Dynamic Load Balancing strategy seeks to improve the efficiency of the simulation by measuring computation and communication times and adjusting the fluid distribution accordingly.	algorithm;central processing unit;computation;lattice boltzmann methods;load balancing (computing);mathematical optimization;parallel computing;simulation;unbalanced circuit	Ashley Reid;James E. Gain;Michelle Kuttel	2010		10.1145/1811158.1811174	mathematical optimization;simulation;computer science;theoretical computer science;lattice boltzmann methods	HPC	-6.428016540609161	40.75126687454947	177728
31c0e24927357c0e007ae33c423091eda3fdd38e	operating system support for general purpose single system image clusters	operating system		operating system;single system image	José M. Bernabéu-Aubán;Yousef Y. A. Khalidi	1997			single system image;computer science;parallel computing;operating system;cluster (physics);distributed computing	Robotics	-10.805956140990114	43.75379478057473	177769
7c976363005a155642ffa8e50a2c585a64357638	evaluating the interconnection latency costs on the performance of a cmp with multisliced l2			interconnection;interrupt latency	Mario Marino	2006			latency (engineering);computer science;parallel computing;interconnection;distributed computing	Arch	-9.945400218104686	43.89669735335098	177891
2e44b8d06e16bfbda94a9727c6337213039a37ea	ibm z13 firmware innovations for simultaneous multithreading and i/o virtualization	instruction sets virtual machine monitors interrupters multithreading virtualization	The IBM z13™ delivers significant new capabilities in terms of overall system capacity provided by simultaneous multithreading (SMT), and new I/O virtualization functionality for PCI Express™ (PCIe™) adapters. In this paper, we describe how the host firmware stack was enhanced to enable these new functions. For SMT, new support was added in the firmware running on the Central Processor Complex (CPC) to represent and manage the operation of more processor cores compared with the previous z Systems™ generation, with multiple threads each. The Processor Resource/Systems Manager™ (PR/SM™) hypervisor design was modified to allow exploitation of SMT under the control of the operating system running in a logical partition. The z13 supports Single Root I/O Virtualization (SR-IOV) for PCIe adapters to allow sharing of adapters among operating system images running in separate logical partitions. The virtualization intermediary firmware runs in a new firmware execution environment, the Firmware Platform Container (FPC). The IBM internal Central Electronics Complex (CEC) SIMulation (CECSIM) environment was extended to reflect the new system structure, scale, and functions, to allow early verification of the z13 firmware designs, and to ensure excellent firmware quality for an efficient hardware bring-up and system test.	firmware;input/output;multithreading (computer architecture);simultaneous multithreading	Christine Axnix;G. Bayer;H. Bohm;Joachim von Buttlar;Mark S. Farrell;Lisa Cranton Heller;Jeffrey P. Kubala;S. E. Lederer;Raymond Mansell;Angel Nuñez Mencias;Stefan Usenbinz	2015	IBM Journal of Research and Development	10.1147/JRD.2015.2435494	computer architecture;full virtualization;parallel computing;virtualization;multithreading;computer science;virtual machine;operating system;simultaneous multithreading	HPC	-10.23747922746987	45.64922402767264	178320
6b7c74e9d7d334613bf22032ad1184a92c66b643	argo nodeos: toward unified resource management for exascale		Exascale systems are expected to feature hundreds of thousands of compute nodes with hundreds of hardware threads and complex memory hierarchies with a mix of on-package and persistent memory modules. In this context, the Argo project is developing a new operating system for exascale machines. Targeting production workloads using workflows or coupled codes, we improve the Linux kernel on several fronts. We extendthe memory management of Linux to be able to subdivide NUMA memory nodes, allowing better resource partitioning among processes running on the same node. We also add support for memory-mapped access tonode-local, PCIe-attached NVRAM devices and introduce a new scheduling class targeted at parallel runtimes supporting user-level load balancing. These features are unified into compute containers, a containerization approach focused on providing modern HPC applications with dynamic control over a wide range of kernel interfaces. To keep our approach compatible with industrial containerization products, we also identifycontentions points for the adoption of containers in HPC settings. Each NodeOS feature is evaluated by using a set of parallel benchmarks, miniapps, and coupled applications consisting of simulation and data analysis components, running on a modern NUMA platform. We observe out-of-the-box performance improvements easily matching, and often exceeding, those observed with expert-optimized configurations on standard OS kernels. Our lightweight approach to resource management retains the many benefits of a full OS kernel that application programmers have learned to depend on, at the same time providing a set of extensions that can be freely mixed and matched to best benefit particular application components.	adobe streamline;application programming interface;baseline (configuration management);benchmark (computing);central processing unit;code;component-based software engineering;dimm;data visualization;experiment;fits;gromacs;industrial pc;input/output;job stream;linux;linux;load balancing (computing);memory hierarchy;memory management;middleware;nas parallel benchmarks;non-volatile random-access memory;operating system;out of the box (feature);pci express;persistent memory;programmer;resource management (computing);scheduling (computing);simulation;user space	Swann Perarnau;Judicael A. Zounmevo;Matthieu Dreher;Brian C. Van Essen;Roberto Gioiosa;Kamil Iskra;Maya Gokhale;Kazutomo Yoshii;Peter H. Beckman	2017	2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2017.25	parallel computing;computer science;distributed computing;memory management;thread (computing);instruction set;scheduling (computing);load balancing (computing);persistence (computer science);non-volatile random-access memory;linux kernel	OS	-6.7315677487219725	44.6625409726623	178826
8e2cc3a9f9ac273fe9ad32742c202f6633159fe9	an invitation to the world of pax	microprocessors;cmos technology;concurrent computing;application software;computer architecture concurrent computing cmos technology parallel processing application software space technology large scale integration research and development inductors microprocessors;computer architecture;research and development;large scale integration;inductors;space technology;parallel processing	"""Using contemporary LSI and simple architecture, we can construct a multiplepurpose machine PAX with a 1 Gflops speed. Our goal for the 1990's: a 1 Tflops PAX. O ne of the most popular research and development topics in computing is the exploitation of parallelism in processing computationally intensive applications and the creation ofthe necessary super-high-speed computer. I PAX computer development, while similar to other attempts to create a superhigh-speed computer, is somewhat different in approach. The PAX computer project, sponsored by the Japanese Ministry of Education, was started in 1977 by me and Prof. T. Kawai, Keio University. It aimed at the realization of a high-speed multi-microprocessor system that could calculate the power distribution in the Boiling Water Reactor (BWR) by solving the coupled neutronic-thermo-hydraulic equations that describe the three-dimensional reactor core. This goal was attained when PACS-32, an array of 32 microprocessors connected in a two-dimensional, nearestneighbor-mesh,2 performed a successful load-follow analysis of the BWR. The method of parallel processing used was very straightforward and traditional in that it took advantage of the locality of the physical process and the proximity of the physical actions: The physical space was directly mapped onto the processor array, and each processor executed its own mapped local task with proper exchange of data between the adjacent processors. During the development of PACS-32, there were indications that PACS architecture could be effective for a multipurpose, parallel computer covering the greater part of all scientific and engineering applications. The name """"PACS"""" (Processor Array for Continuum Simulation) was later changed to """"PAX,"""" which stands for Processor Array eXperiment; four hardware PAX prototypes (with 9, 32, 64, and 128 processors, respectively) have been constructed so far. The newest PAX prototype, the PAX-64J, is an industry-made, commercial version with 64 DCJ1 1 microprocessors. Nowadays, the PAX prototypes are used intensively in studies aimed at developing algorithms to be used with PAX-type architecture in solving scientific-application problems. As evidenced by the June 1985 issue of Computer, I the scope of parallel-processing research and commercial development is expanding; it spans from commercial supercomputers with vector-processing capability to highly parallel multi-microprnocessor systems for limited application. It is Nvious that all the growth is strongly dependent on the progress of LSI technology. The recent, innovative CMOS technology will definitely influence multiprocessing architecture in the sense that device technology is fundamental and is sometimes the most crucial economic factor. CMOS technology favors the highly parallel construct; it is shifting the R&D spectrum toward higher parallelism in both commercial supercomputers and specialized multiprocessor systems."""	algorithm;cmos;central processing unit;flops;locality of reference;microprocessor;multiprocessing;pax;parallel computing;physics and astronomy classification scheme;picture archiving and communication system;processor array;prototype;reactor (software);simulation;supercomputer;triune continuum paradigm;vector processor;water model	Tsutomu Hoshino	1986	Computer	10.1109/MC.1986.1663229	parallel processing;computer architecture;application software;parallel computing;concurrent computing;computer science;operating system;space technology;programming language;cmos;inductor	Arch	-8.044625789326723	39.36800588757075	179038
50ce2ac9fb3514c7a0ec7fc1248e60200ad7a91f	an experimental model to analyze openmp applications for system utilization	parallel computing;high performance computing;parallel speed up;performance analysis;openmp;parallel efficiency;automatic parallelization;dtrace	Data centers are increasingly focused on optimal use of resources. For technical computing environments, with compute-dominated workloads, we can increase data center efficiencies by increasing multicore processor utilization. OpenMP programmers need assistance in better understanding efficiencies and scaling for both dedicated and throughput environments. An experimental OpenMP performance analysis model has been developed to give insight into many application scalability bottlenecks. A tool has been developed to implement the model. Compared to other performance analysis tools, this tool takes into account how the operating system scheduler affects OpenMP threaded application performance. Poor parallel scalability can result in reduced system utilization. A case study shows how the tool helped diagnose performance loss caused by OpenMP work distribution schedule strategies. Changing the work distribution schedule substantially improved application performance and system utilization. This tool is specific to Solaris and Studio compilers, although the performance model is applicable to other OpenMP compilers, Linux and UNIX systems.	openmp	Mark Woodyard	2011		10.1007/978-3-642-21487-5_3	computer architecture;supercomputer;parallel computing;computer science;operating system;automatic parallelization	HPC	-7.381490035432858	46.03999225903115	179307
6530b511845d8cb9d58ad211fddd4c5203241095	declarative tuning for locality in parallel programs	locality;runtime;computational modeling;tuning;scheduling;declarative;distribution functions;tasks;programming;parallel processing;hardware	Optimized placement of data and computation for locality is critical for improving performance and reducing energy consumption on modern computing systems. However, for most programming models, modifying data and computation placements typically requires rewriting large portions of the application, thereby posing a huge performance portability challenge in today's rapidly evolving architecture landscape. In this paper we present TunedCnC, a novel, declarative and flexible CnC tuning framework for controlling the spatial and temporal placement of data and computation by specifying hierarchical affinity groups and distribution functions. TunedCnC emphasizes a separation of concerns: the domain expert specifies a parallel application by defining data and control dependences, while the tuning expert specifies how the application should be executed on a given architecture - defining when and where for data and computation placement. The application remains unchanged when tuned for a different platform or towards different performance goals. We evaluate the utility of TunedCnC on several applications, and demonstrate that varying the tuning specification can have a significant impact on an application's performance. Our evaluation is performed using an implementation of the Concurrent Collections (CnC) declarative parallel programming model, but our results should be applicable to tuning of other data-flow task-parallel programming models as well.	admissible numbering;affinity analysis;algorithm;compiler;computation;concurrent collections;dataflow;declarative programming;distributed memory;executable;locality of reference;parallel computing;parallel programming model;processor affinity;rewriting;separation of concerns;subject-matter expert	Sanjay Chatterjee;Nick Vrvilo;Zoran Budimlic;Kathleen Knobe;Vivek Sarkar	2016	2016 45th International Conference on Parallel Processing (ICPP)	10.1109/ICPP.2016.58	parallel processing;programming;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distribution function;programming language;computational model;scheduling	HPC	-7.511873447253282	44.42194881372067	179621
59807f4d1ef5d04a6039b29d5ba1832c74f505bb	parallel instance discrete-event simulation using a vector uniprocessor	discrete event simulation;parallel processing;stochastic processes;block selection policies;block selection policy;parallel instance discrete event simulation;simulated system;stochastic flow-graph representation;vector uniprocessor	This paper examines the possibility of running N simulations in parallel on a vector processor. In such a system each instance of execution runs identical code but with a different input data set. The main problem which is addressed is the choice of block selection policy, that is, the choice of which indivisible block of code to execute next. This paper investigates four block selection policies by simulating the execution of such a system. A stochastic flow-graph representation was chosen to model the execution of a simulation. A two-level block selection policy was found to have the best potential speedup of the four block selection policies. The speedup levels achieved were not large, and decreased when there were a large number of unique event types (and therefore handlers) in the simulated system. keywords parallel simulation, vector processors, SIMD processors, stochastic modelling.	central processing unit;graph (abstract data type);indivisible;simd;simulation;speedup;stochastic modelling (insurance);uniprocessor system;vector processor	James F. Ohi;Bruno R. Preiss	1991			stochastic process;parallel processing;vector processor;parallel computing;computer science;technical report;stochastic modelling;theoretical computer science;discrete event simulation;process control;distributed computing;parallel algorithm;graph	Arch	-11.614774944296661	41.361178922698905	179692
05d6e23cfc591d66c366ed9f75c5d91d7f78c059	data-centric multi-level blocking	data locality;current control;control flow;numerical linear algebra;memory hierarchy;high performance;block codes	We present a simple and novel framework for generating blocked codes for high-performance machines with a memory hierarchy. Unlike traditional compiler techniques like tiling, which are based on reasoning about the control flow of programs, our techniques are based on reasoning directly about the flow of data through the memory hierarchy. Our data-centric transformations permit a more direct solution to the problem of enhancing data locality than current control-centric techniques do, and generalize easily to multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard benchmarks from the problem domain of dense numerical linear algebra. The simplicity and intuitive appeal of our approach should make it attractive to compiler writers as well as to library writers.	code;compiler;control flow;dataflow;locality of reference;memory hierarchy;numerical linear algebra;problem domain;tiling window manager	Induprakas Kodukula;Nawaaz Ahmed;Keshav Pingali	1997		10.1145/258915.258946	block code;parallel computing;computer science;theoretical computer science;numerical linear algebra;programming language;control flow;algorithm	PL	-5.264113836331411	42.98576939765518	179732
268f96cae688a513da45a9b9b858214fbba31f6a	a hierarchical grid algorithm for accelerating high-performance conjugate gradient benchmark on sunway many-core processor		This paper presents analysis and optimizations for High Performance Conjugate Gradient benchmark (HPCG) on the Sunway many-core processor. For modern multi-core and many-core processors, HPCG always presents a poor performance and under-utilizes computation resource because of its low arithmetic intensity and fine-grain parallelism. We apply two conventional methods to parallel Gauss-Seidel smoother the most time consumer kernel in HPCG, including Level-Scheduling (LS) and Multi-Coloring (MC). These strategies are effective and achieve 1.54x and 5.52x performance improvement. For overcoming the poor locality for MC and limited parallelism for LS, we propose a novel Hierarchical Grid (HG) algorithm and our algorithmic and architecture-aware optimizations achieve an aggregated performance of 3.54 Gflops, which is around 0.475% of the peak performance and 15.4x higher than reference on the single core-group of SW26010 processor. With MPI parallelize, we balance the parallelism, pre-processing, convergence rate and communication overheads, we achieved 192 TFlops (70% parallelization efficiency) when scaling to 81920 CGs (5,324,800 cores) on Sunway Taihulight System. Moreover, we analyze the adaptability of our parallel method and optimization strategies and summarize several key points when refactoring and optimizing HPC applications on the Sunway heterogeneous many-core processor.	algorithm;benchmark (computing);central processing unit;code refactoring;computation;conjugate gradient method;convex conjugate;flops;gauss–seidel method;hpcg benchmark;image scaling;least squares;locality of reference;manycore processor;mathematical optimization;multi-core processor;parallel computing;preprocessor;rate of convergence;sw26010;sunway	Chenzhi Liao;Junshi Chen;Wenting Han;Huanqi Cao;Zhichao Su;Wanwang Yin;Hong An	2017		10.1145/3162957.3163049	sw26010;rate of convergence;flops;code refactoring;adaptability;conjugate gradient method;multi-core processor;algorithm;sunway taihulight;computer science	HPC	-5.53125596837792	41.831632198484705	180197
1df7b4f1b81d2857c42e7227b2ada006a479a18f	on the reflective nature of the spring kernel	main idea;reflective nature;integrated approach;different emphasis;previous paper;real-time application;real-time operating system;spring kernel;next generation;real-time system	real{time operating system for complex, next generation, real-time applications. The Spring Kernel is being implemented in stages on a network of 68020 and 68030 based multiprocessors called SpringNet. Version 1 of the Kernel is now operational. While much has already been written on the Spring Kernel [10, 11, 12, 4], the purpose of this invited paper is to combine ideas found in separate papers and presents them with a	cache (computing);fault tolerance;kernel (operating system);memory refresh;next-generation network;operating system;pipeline (computing);programming language;real-time clock;real-time computing;requirement;run time (program lifecycle phase);static timing analysis;systems design;wait state	John A. Stankovic	1991		10.1007/978-3-642-76501-8_1	kernel (linear algebra);categorization;theoretical computer science;predictability;computer science	OS	-9.577553810692343	44.21041716409464	180216
4f7fd143943a5f997a66cfaeb4792af1792cb807	rethinking caches for throughput processors: technical perspective	design styles;nonnumerical algorithms and problems;performance analysis and design aids	人的研究采取了不同的方法,通过 注重多线程处理器的线程调度来改 善缓存行为并提高计算机系统性 能。基本理念非常简单:当线程拥 有数据局部性时,增加它的调度频 次以提高其数据保留在缓存中的可 能性。这种方法具有减少一个时间 窗口中执行的线程数量的效果,可 以提高吞吐量和总体性能(这或许 在一定程度上违反直觉)。因此, 在某些情形中“少即是多”。 论文中所述的体系结构具有诸 多优点。首先,它不需要程序员的 输入,也不需要提示工作集合中 保留多少线程数。其次,它自动适 应程序的局部性行为,最终在线程 级数据局部性不足时返回到基准的 “最多线程数”方法。除了研究中 所呈现的性能益处外,这种改进局 部性的方法也降低了带宽需求,以 及跨芯片数据传输所耗的功率。 本论文展示了当代计算机体系 结构研究的两个方面。其一,重新 思索了当新的体系结构模型、技术 或应用领域涌现时所需的传统智 慧。其二,本论文展示了在传统计 算机体系结构界限之间进行联合优 化时可以利用的机会。尤其而言, 线程调度不仅有改善 L1 缓存的潜 能(本论文中所述),而且也能改 进二级缓存、DRAM 和互连体系 结构。	central processing unit;throughput	Stephen W. Keckler	2014	Commun. ACM	10.1145/2682585	theoretical computer science;throughput;computer science	Theory	-9.128653279716975	44.32221621247303	180570
74f5a03e5c7b619414a650cd48f7cdc544adc136	discovery of potential parallelism in sequential programs	code locations multicore cpu desktops servers writing programs hardware parallelism sequential programs computational units read compute write pattern concurrent scheduling code sections arbitrary granularity parallelization;computer languages;instruments;parallel processing instruments program processors computer languages explosions runtime buildings;parallel programming;runtime;parallel programming concurrency control multiprocessing systems;concurrency control;explosions;multiprocessing systems;program processors;parallel processing;buildings	Although multicore CPUs are dominating the market of desktops and servers, writing programs that utilize the available hardware parallelism on these architectures still remains a challenge. In this paper, we present a dynamic approach for automatically identifying potential parallelism in sequential programs. Our method is based on the notion of computational units, which are small sections of code following the read-compute-write pattern that can form the atoms of concurrent scheduling. In contrast to earlier approaches, our method can identify parallelism between code sections of arbitrary granularity and does not rely on a predefined notion of language constructs subject to parallelization. Experimental results show that reasonable speedups can be achieved by parallelizing sequential programs manually according to our findings. By comparing our findings to known parallel implementations of sequential programs, we demonstrate that we are able to detect the most important code locations to be parallelized.	automatic parallelization;central processing unit;code;computation;dynamic logic (digital electronics);experiment;memory footprint;microsoft outlook for mac;multi-core processor;overhead (computing);parallel computing;programmer;recommender system;scheduling (computing);server (computing);shadow memory;speedup	Zhen Li;Ali Jannesari;Felix Wolf	2013	2013 42nd International Conference on Parallel Processing	10.1109/ICPP.2013.119	parallel processing;computer architecture;parallel computing;computer science;operating system;concurrency control;data parallelism;programming language;instruction-level parallelism;implicit parallelism;task parallelism;parallel programming model	HPC	-6.990243961005273	46.28793481014113	181911
f6f8dc8b2ab329c4130625999c196c4418f1ca15	performance evaluation of hybrid programming patterns for large cpu/gpu heterogeneous clusters	performance evaluation;gpu cluster;cuda;openmp;mpi;npb	The CPU/GPU heterogeneous clusters are important platforms for high performance computing applications. However, there are many challenges for efficiently performing the scientific and engineering legacy code on these heterogeneous systems. In this paper, we endeavor to address the programming-model issue by combining the existing models (i.e., MPI, OpenMP and CUDA). First, two hybrid programming patterns are presented, namely the   MPI+CUDA     MPI  +  CUDA        and   MPI+OpenMP/CUDA     MPI  +  OpenMP/CUDA       . Second, three kernels (i.e., EP, CG and MG) of the NAS parallel benchmarks (NPBs), which are abstracted from many legacy computational fluid dynamics applications, are implemented with the above two patterns. Third, these hybrid implementations are executed on the TianHe-1A supercomputer, and the corresponding experimental results show that significant performance improvement can be achieved with the above patterns. Finally, a detailed performance analysis about the two hybrid patterns is performed and some guidelines for porting the legacy code onto large-scale heterogeneous CPU/GPU clusters are also given.	central processing unit;computer cluster;graphics processing unit;performance evaluation	Fengshun Lu;Junqiang Song;Fukang Yin;Xiaoqian Zhu	2012	Computer Physics Communications	10.1016/j.cpc.2012.01.019	computational science;computer architecture;parallel computing;computer science;message passing interface	HPC	-6.193687190560869	43.70315920471091	181981
d7d28e261bc96f1b07b2369b5acc4b1503566f3c	special issue on instruction level parallelism (ilp)	instruction level parallel		instruction-level parallelism;parallel computing	Daniel Tabak	1998	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(98)00085-4	computer science;theoretical computer science;minimal instruction set computer;data parallelism;instruction scheduling;instruction-level parallelism;explicitly parallel instruction computing;implicit parallelism;task parallelism	EDA	-9.356021826747503	42.67794978408149	182186
aafe01c2933b6e2f97296edcaeefd77d3052b698	towards portable visualization and virtual environment applications on distributed memory architectures	distributed memory architecture;virtual environment	Without Abstract	distributed memory;virtual reality	Alexander del Pino	1996		10.1007/3-540-61142-8_680	uniform memory access;memory footprint;computer architecture;parallel computing;computer science;virtual machine;virtual memory;operating system;memory protection;data diffusion machine	HPC	-10.11159832511854	43.19750235563466	182402
ecd4b6a92c50b6c34fb50f1ca6a0e2bb231a4594	out-of-core and pipeline techniques for wavefront algorithms	linear algebra;out of core;concurrent computing;memory management;software libraries;storage management;distributed computing;data distribution;pipeline technique;linear algebra out of core computation pipeline technique wavefront algorithm parallel algorithms storage management;wavefront algorithm;numerical computation;out of core computation;storage management pipeline processing parallel algorithms;parallel processing;pipeline;pipeline processing concurrent computing delay distributed computing memory management costs parallel processing linear algebra hardware software libraries;pipeline processing;hardware;parallel algorithms;out of core algorithms	Several numerical computation algorithms exhibit dependences that lead to a wavefront of the computation. Depending on the data distribution chosen, pipelining communication and computation can be the only way to avoid a sequential execution of the parallel code. The computation grain has to be wisely chosen to obtain at the same time a maximum parallelism and a small communication overhead. On the other hand, when the size of data exceeds the memory capacity of the target platform, data have to be stored on disk. The concept of out-of-core computation aims at minimizing the impact of the I/O needed to compute on such data. It has been applied successfully on several linear algebra applications. In this paper we apply out-of-core techniques to wavefront algorithms. The originality of our approach is to overlap computation, communication, and I/O. An original strategy is proposed using several memory blocks accessed in a cyclic way. The resulting pipeline algorithm achieves a saturation of the disk resource, which is the bottleneck in out-of-core algorithms.	32-bit;address space;alias systems corporation;australian informatics olympiad;computation;experiment;generic programming;image processing;input/output;international parallel and distributed processing symposium;job control (unix);library (computing);linear algebra;memory management;message passing interface;numerical analysis;out-of-core algorithm;overhead (computing);paging;parallel computing;pipeline (computing);requirement	Eddy Caron;Frédéric Desprez;Frédéric Suter	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.318	model of computation;parallel processing;parallel computing;out-of-core algorithm;concurrent computing;computer science;theoretical computer science;linear algebra;operating system;distributed computing;parallel algorithm;pipeline;memory management	HPC	-9.222990242508583	43.70402348030035	182518
84c095404f31b4724c9e9fe4c49e5dbc468bb652	lambda machine overview	next generation;high performance	LMI will be introducing its next generation LISP Machine, the Lambda, for market in the spring. The major design philosophy of the Lambda machine is the concept of modularity. The system is designed so that it can exist in a large number of configurations centered around a 32 bit high performance bus.	32-bit;circa;computation;computer science;harvester;linear matrix inequality;lisp machine;megabyte;next-generation network;unix;vax	Mache Creeger	1982	SIGART Newsletter	10.1145/1056176.1056179	simulation;computer science;algorithm	AI	-8.36908650241096	41.23557109308696	182526
3ed963405458aa06c7af6f2edddccf48981887d4	automated tuning of parallel i/o systems: an approach to portable i/o performance for scientific applications	scientific application;software portability;program diagnostics;system configuration;software libraries;rule based;input output programs;bibliographies;parallel programming;automatic performance optimization;simulated annealing;community networks;software libraries bibliographies parallel programming software portability natural sciences computing program diagnostics input output programs matrix multiplication;total optimization overhead automated tuning parallel i o systems portable i o performance scientific applications usability goals supercomputer platform parallel i o system user applications collective i o requests multidimensional arrays optimization engine high quality i o plans human intervention system configuration randomized search based algorithms parameter tuning parameter settings panda parallel i o library ibm sp out of core matrix multiplication application system configurations;application software humans computer society communication networks multidimensional systems analytical models cost function database systems resource management usability;parallel i o;matrix multiplication;natural sciences computing;high performance;performance modeling;random search	ÐParallel I/O systems typically consist of individual processors, communication networks, and a large number of disks. Managing and utilizing these resources to meet performance, portability, and usability goals of high-performance scientific applications has become a significant challenge. For scientists, the problem is exacerbated by the need to retune the I/O portion of their code for each supercomputer platform where they obtain access. We believe that a parallel I/O system that automatically selects efficient I/O plans for user applications is a solution to this problem. In this paper, we present such an approach for scientific applications performing collective I/O requests on multidimensional arrays. Under our approach, an optimization engine in a parallel I/O system selects high-quality I/O plans without human intervention, based on a description of the application I/O requests and the system configuration. To validate our hypothesis, we have built an optimizer that uses rule-based and randomized search-based algorithms to tune parameter settings in Panda, a parallel I/O library for multidimensional arrays. Our performance results obtained from an IBM SP using an out-of-core matrix multiplication application show that the Panda optimizer is able to select high-quality I/O plans and deliver high performance under a variety of system configurations with a small total optimization overhead. Index TermsÐParallel I/O, performance modeling, automatic performance optimization, simulated annealing.	analysis of algorithms;asynchronous i/o;cpu cache;central processing unit;compiler;contingency plan;fits;genetic algorithm;high-level programming language;input/output;library (computing);logic programming;mathematical optimization;matrix multiplication;message passing interface;nonlinear system;out-of-core algorithm;overhead (computing);parallel i/o;performance prediction;randomized algorithm;resource contention;run time (program lifecycle phase);runtime system;simulated annealing;software portability;supercomputer;synthetic intelligence;system configuration;telecommunications network;testbed;time complexity;usability goals	Ying Chen;Marianne Winslett	2000	IEEE Trans. Software Eng.	10.1109/32.844494	rule-based system;software portability;simulation;random search;simulated annealing;matrix multiplication;computer science;engineering;theoretical computer science;operating system;software engineering;programming language;i/o scheduling	HPC	-7.253819906550587	41.99789474660233	182852
8c9038b1411da189d8d444597c2018415e4a5c74	fast epistasis detection in large-scale gwas for intel xeon phi clusters	parallel computing;xeon phi coprocessor;epistasis;gwas;openmp;mpi	epiSNP is a program for identifying pairwise single nucleotide polymorphism (SNP) interactions (epistasis) that affect quantitative traits in genome-wide association studies (GWAS). A parallel MPI version (EPISNPmpi) was created in 2008 to address this computationally-expensive analysis on data sets with many quantitative traits and markers. However, the explosion in genome sequencing will lead to the creation of large-scale data sets that will overwhelm EPISNPmpi's ability to compute results in a reasonable amount of time. Thus, epiSNP was rewritten to efficiently handle these large data sets. This was accomplished by performing serial optimizations, improving MPI load balancing, and introducing parallel OpenMP directives to further enhance load balancing and allow execution on the Intel Xeon Phi coprocessor (MIC). These additions resulted in new scalable versions of epiSNP using MPI, MPI+OpenMP, and MPI+OpenMP with one or two MICs. For a large 774,660 SNP data set with 1,634 individuals, the runtime on 126 nodes of TACC's Stampede Supercomputer was 10.61 minutes without MICs, and 5.13 minutes with 2 MICs. This translated to speedups over EPISNPmpi of 17X without MICs, and 36X with 2 MICs.	automatic vectorization;compiler;coprocessor;dead code elimination;domain decomposition methods;fortran;interaction;intrinsic function;load balancing (computing);message passing interface;openmp;preprocessor;scalability;speedup;supercomputer;whole genome sequencing;xeon phi	Glenn R. Luecke;Nathan T. Weeks;Brandon M. Groth;Marina Kraeva;Li Ma;Luke M. Kramer;James E. Koltes;James M. Reecy	2015	2015 IEEE Trustcom/BigDataSE/ISPA	10.1109/Trustcom.2015.637	computer architecture;parallel computing;computer science;operating system	HPC	-4.912848207618072	42.749492385699675	183009
605762b71f59be66a11ab46b06cbb1cdb7bb34c0	design, configuration, implementation, and performance of a simple 32 core raspberry pi cluster		In this report, I describe the design and implementation of an inexpensive, eight node, 32 core, cluster of raspberry pi single board computers, as well as the performance of this cluster on two computational tasks, one that requires significant data transfer relative to computational time requirements, and one that does not. We have two use-cases for the cluster: (a) as an educational tool for classroom usage, such as covering parallel algorithms in an algorithms course; and (b) as a test system for use during the development of parallel metaheuristics, essentially serving as a personal desktop parallel computing cluster. Our preliminary results show that the slow 100 Mbps networking of the raspberry pi significantly limits such clusters to parallel computational tasks that are either long running relative to data communications requirements, or that which requires very little internode communications. Additionally, although the raspberry pi 3 has a quad-core processor, parallel speedup degrades during attempts to utilize all four cores of all cluster nodes for a parallel computation, likely due to resource contention with operating system level processes. However, distributing a task across three cores of each cluster node does enable linear (or near linear) speedup.	computation;computer cluster;data rate units;desktop computer;emoticon;multi-core processor;operating system;parallel algorithm;parallel computing;parallel metaheuristic;raspberry pi 3 model b (latest version);requirement;resource contention;speedup;time complexity	Vincent A. Cicirello	2017	CoRR		parallel computing;distributed computing;real-time computing;cluster (physics);metaheuristic;parallel algorithm;computer science;data transmission;cluster node;speedup	HPC	-7.954370942039855	41.977047042310126	183203
7cd41ec0088a38df195a78c66f09883db86bf14e	tera-scale 1d fft with low-communication algorithm and intel® xeon phi™ coprocessors	data movement optimization tera scale 1d fft low communication algorithm intel xeon phi coprocessors tera scale performance disciplined performance programming methodology many core wide vector processors tflops bandwidth bound fft computation low inter node communication cost segment of interest fft node local computations low communication algorithm parallel architecture hpc systems;wide vector many core processors;fft;communication avoiding algorithms;bandwidth optimizations;xeon phi;demodulation abstracts program processors optimization;parallel architectures coprocessors fast fourier transforms multiprocessing systems;xeon phi bandwidth optimizations communication avoiding algorithms fft wide vector many core processors	This paper demonstrates the first tera-scale performance of Intel® Xeon Phi#8482; coprocessors on 1D FFT computations. Applying a disciplined performance programming methodology of sound algorithm choice, valid performance model, and well-executed optimizations, we break the tera-flop mark on a mere 64 nodes of Xeon Phi and reach 6.7 TFLOPS with 512 nodes, which is 1.5x than achievable on a same number of Intel® Xeon® nodes. It is a challenge to fully utilize the compute capability presented by many-core wide-vector processors for bandwidth-bound FFT computation. We leverage a new algorithm, Segment-of-Interest FFT, with low inter-node communication cost, and aggressively optimize data movements in node-local computations, exploiting caches. Our coordination of low communication algorithm and massively parallel architecture for scalable performance is not limited to running FFT on Xeon Phi; it can serve as a reference for other bandwidth-bound computations and for emerging HPC systems that are increasingly communication limited.	algorithm;central processing unit;computation;coprocessor;flops;fast fourier transform;manycore processor;multi-core processor;parallel computing;scalability;software development process;vector processor;xeon phi	Jongsoo Park;Ganesh Bikshandi;Karthikeyan Vaidyanathan;Ping Tak Peter Tang;Pradeep Dubey;Daehyun Kim	2013	2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)	10.1145/2503210.2503242	fast fourier transform;computer architecture;parallel computing;computer hardware;computer science;operating system;xeon phi	HPC	-4.842215325335419	44.11847397275685	183262
90a5d3d5dd87279b5cf10fbb5c5987fd437e9e46	bridge-connectivity and biconnectivity algorithms for parallel computer models	parallel computer;chip;matrix multiplication		algorithm;biconnected graph;computer simulation;parallel computing	Yung H. Tsin	1983			parallel computing;distributed computing;theoretical computer science;chip;matrix multiplication;computer science	Theory	-9.794851795365595	42.18861317084685	183305
5da6261149587b33242199d5061c6ce4abae8a5e	parallel computing: accelerating computational science and engineering - cse	paper;heterogeneous systems;cuda;nvidia;algorithms;computer science;book	Parallel computing has been the enabling technology of high-end machines for many years. Now, it has finally become the ubiquitous key to the efficient use of any kind of multi-processor computer architecture, from smart phones, tablets, embedded systems and cloud computing up to exascale computers. This book presents the proceedings of ParCo2013 the latest edition of the biennial International Conference on Parallel Computing held from 10 to 13 September 2013, in Garching, Germany. The conference focused on several key parallel computing areas. Themes included parallel programming models for multi- and manycore CPUs, GPUs, FPGAs and heterogeneous platforms, the performance engineering processes that must be adapted to efficiently use these new and innovative platforms, novel numerical algorithms and approaches to large-scale simulations of problems in science and engineering.The conference programme also included twelve mini-symposia (including an industry session and a special PhD Symposium), which comprehensively represented and intensified the discussion of current hot topics in high performance and parallel computing. These special sessions covered large-scale supercomputing, novel challenges arising from parallel architectures (multi-/manycore, heterogeneous platforms, FPGAs), multi-level algorithms as well as multi-scale, multi-physics and multi-dimensional problems.It is clear that parallel computing including the processing of large data sets (Big Data) will remain a persistent driver of research in all fields of innovative computing, which makes this book relevant to all those with an interest in this field.		Michael Bader;Arndt Bode;Hans-Joachim Bungartz;Michael Gerndt;G. R. Joubert;F. Peters	2014			computational science;parallel computing;computer science;theoretical computer science;operating system;data-intensive computing;algorithm;unconventional computing	NLP	-7.8317161866676335	39.90374906437633	183589
26ea2edb81baab6008ad41a6d128b82f54f7ddbe	a hybrid computer performance modelling system	performance modelling		computer performance;hybrid computer	Eric Foxley	1978	Comput. J.	10.1093/comjnl/21.3.205	computer science	Theory	-10.030506228761386	43.936796540821376	183899
2e2eed37a65b9acd5c1775f5820f7289f4e35238	new massively parallel scheme for incompressible smoothed particle hydrodynamics (isph) for highly nonlinear and distorted flow		Abstract A new massively parallel scheme is developed to simulate free-surface flows with the meshless method incompressible smoothed particle hydrodynamics (ISPH) for simulations involving more than 100 million particles. As a pressure-projection method, ISPH requires the solution of a sparse matrix for the pressure Poisson equation (PPE) which is non trivial for large problems where the particles are moving with continuously evolving sparsity. The new scheme uses a Hilbert space filling curve with a cell-linked list to map the entire domain so that domain decomposition and load balancing can be achieved easily to take advantage of geometric locality in order to reduce latency in memory cache access. The computational domain can be subdivided into more than 12,000 partitions using the message passing interface (MPI) for communication between partitions. Load balancing is achieved using the open-source Zoltan library using a new particle weighting system. To solve the PPE for large problems using tens of thousands of partitions, the open-source PETSc library is used which requires the HYPRE BoomerAMG preconditioner to ensure rapid convergence for ISPH. The performance of the code is benchmarked on the U.K. National Supercomputer ARCHER. The results show that domain decomposition with a space filling curve can efficiently treat irregularly distributed particles creating a well-balanced scheme demonstrating that the approach is well matched to the highly irregular subdomains and non-uniform distribution of ISPH free-surface simulations. The benchmark results show that massively parallel ISPH code can achieve over 90% efficiency for the solution of the PPE, but the efficiency of computing matrix coefficients decreases when using more than 12000 partitions giving overall efficiencies in excess of 43% up to 6144 MPI partitions, highlighting future improvements required. The work demonstrates that the Zoltan and PETSc libraries can be effectively combined with ISPH to offer the capability of developing a massively parallel ISPH toolkit.	nonlinear system;smoothed-particle hydrodynamics;smoothing	Xiaohu Guo;Benedict D. Rogers;Steven J. Lind;Peter Stansby	2018	Computer Physics Communications	10.1016/j.cpc.2018.06.006	massively parallel;mathematical optimization;domain decomposition methods;mathematics;sparse matrix;preconditioner;load balancing (computing);smoothed-particle hydrodynamics;message passing interface;supercomputer	HPC	-4.899534232365236	39.40135109291193	184284
067872241af4a1c138e3a35c29c1ee9d6f5da836	a layout-aware optimization strategy for collective i/o	experimental tests;system performance;collective i o;file system;parallel file system;petascale file system;parallel i o;data layout;mpi i o	In this study, we propose an optimization strategy to promote a better integration of the parallel I/O middleware and parallel file systems. We illustrate that a layout-aware optimization strategy can improve the performance of current collective I/O in parallel I/O system. We present the motivation, prototype design and initial verification of the proposed layout-aware optimization strategy. The analytical and initial experimental testing results demonstrate that the proposed strategy has a potential in improving the parallel I/O system performance.	input/output;mathematical optimization;middleware;parallel i/o;prototype	Yong P Chen;Huaiming Song;Rajeev Thakur;Xian-He Sun	2010		10.1145/1851476.1851530	computer architecture;parallel computing;computer science;theoretical computer science;computer performance	HPC	-7.566806657533837	43.20502854916475	184312
f86f4135ff559d6eed6bf324c96719713b4f51f8	performance analysis of cooley-tukey fft algorithms for a many-core architecture	performance analysis of cooley tukey fft algorithms for a many core architecture;parallel algorithm;articulo;fft;relative error;high performance computer;performance analysis;performance model;many core;performance tuning	Given that many-core architectures are becoming the mainstream framework for high performance computing, it is important to develop a performance model for many-core architectures to assist parallel algorithms design and applications performance tuning. In this paper, we propose a performance modeling technique for parallel Cooley-Tukey FFT algorithms, for an abstract many-core architecture that captures generic features and parameters of a class of real many-core architectures. We have verified our performance model on the IBM Cyclops-64 (C64) many-core architecture. The experimental results demonstrate that our model can predict the performance trend accurately, with an average relative error of 16%, when running on up to 16 cores. The average relative error rate gradually increases to 29%, when running on up to 64 cores. The experimental results also reveal that key to performance for this class of many-core architectures is using the local memory and higher radix algorithms to reduce the memory traffic requirements.	approximation error;cooley–tukey fft algorithm;fast fourier transform;intel core (microarchitecture);manycore processor;parallel algorithm;performance prediction;performance tuning;profiling (computer programming);requirement;supercomputer	Long Chen;Guang R. Gao	2010		10.1145/1878537.1878622	fast fourier transform;approximation error;computer architecture;parallel computing;real-time computing;computer science;operating system;parallel algorithm	HPC	-5.07362224359164	44.92352062321278	184669
37105527f02ab95dd6dd03690044696c2d15e3f0	a performance estimator for parallel programs	programa paralelo;parallel algorithm;performance estimation;analyse performance;performance analysis;fortran;alternating direction implicit;parallel programs;parallel program;programme parallele;analisis eficacia	In this paper we describe a Parallel Performance Estimator suitable for the comparative evaluation of parallel algorithms. The Esti-mator is designed for SPMD programs written in either C or FORTRAN. Simulation is used to produce estimates of execution times for varying numbers of processors and to analyse the communication overheads. Results from the estimator are compared with actual results (obtained on a 16 processor IBM SP2 machine) for an Alternating Direction Implicit (ADI) solver of linear equations and for a Parallel Sort by Regular Sampling (PSRS) sorting program. In both cases the plots of Execution Time versus Number of Processors are accurate to 20% and show all of the features of the equivalent plots of the measured data.	alternating direction implicit method;central processing unit;fortran;linear equation;parallel algorithm;spmd;simulation;solver;sorting	Jeffrey S. Reeve	1999		10.1007/3-540-48311-X_24	parallel computing;real-time computing;alternating direction implicit method;computer science;theoretical computer science;operating system;parallel algorithm	HPC	-7.710621727125618	39.78443619167008	184796
c5025ed8a0b87855e01db6868a33b08c3ed49266	a fully gpu-implemented rigid body simulator	rigid body	In this paper we study how to implement a fully GPU-based rigid body simulator by programming shaders for every phase of the simulation. We analyze the pros and cons of different approaches, and point out the bottlenecks we have detected. We also apply the developed techniques to two case studies, comparing them with the analogous versions running on CPU.	bottleneck (software);central processing unit;convex optimization;graphics processing unit;shader;simulation	Álvaro del Monte;Roberto Torres;Pedro J. Martín;Antonio Gavilanes	2008			rigid body;computer science	Robotics	-5.777415384601669	42.236460069930494	184822
1f1948b9b8243cd1d08d616fb29b1aca0a11a026	2d-fmfi sar application on hpc architectures with ompss parallel programming model	hpc architectures;graphics processing unit computer architecture synthetic aperture radar image reconstruction runtime interpolation programming;symmetric multiprocessor architectures;interpolation;spacecraft on board platforms;aerospace engineering;high performance computing;gpu ompss openmp cuda space applications sar smp;parallel programming;gpu;space applications;smp gpu environments;runtime;cuda;computer architecture;gpgpu;hpc;sar;heterogeneous architectures;image reconstruction;graphics processing units;openmp;multicore cpu;smp;multiprocessing systems;openmp 2d fmfi sar application hpc architectures ompss parallel programming model spacecraft on board platforms high performance computing hpc heterogeneous architectures multicore cpu gpgpu many core accelerators synthetic aperture radar symmetric multiprocessor architectures smp gpu environments;many core accelerators;graphics processing unit;programming;aerospace instrumentation;ompss parallel programming model;2d fmfi sar application;software packages;synthetic aperture radar aerospace engineering aerospace instrumentation graphics processing units multiprocessing systems parallel programming software packages space vehicles;space vehicles;ompss;synthetic aperture radar	Spacecraft on-board platforms will soon contain high performance off-the-shelf computing components, as the only way to deal with applications which require High Performance Computing (HPC) capabilities. Heterogeneous architectures, give the possibility to profit from specific features of such components, by combining them on the same computing platform. Such architectures integrate different multicore CPUs with many-core accelerators (GPGPUs) so that parts of the same application can efficiently execute on the CPU while other parts can profit more from the GPU features. The problem stands in the way how to program these heterogeneous architectures. In this paper we introduce the OmpSs Programming Model as a framework to program heterogeneous architectures. For benchmarking purposes, we use a Synthetic Aperture Radar (SAR) application, which we parallelized with OmpSs for Symmetric Multi-Processor (SMP) architectures and for hybrid SMP/GPU environments. We also compare the results obtained with OmpSs with the ones obtained with OpenMP in SMP environments and it turns out that OmpSs outperforms OpenMP. When applying OmpSs to the SAR code for hybrid architectures, OmpSs does not offer better performance than CUDA, but it offers support for heterogeneous architectures and it also increases programmer's productivity.	cuda;central processing unit;computer architecture;general-purpose computing on graphics processing units;graphics processing unit;manycore processor;multi-core processor;on-board data handling;openmp;parallel computing;parallel programming model;programmer;symmetric multiprocessing;systems design	Fisnik Kraja;Arndt Bode;Xavier Martorell	2012	2012 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2012.6268638	iterative reconstruction;embedded system;programming;computer architecture;supercomputer;parallel computing;synthetic aperture radar;interpolation;computer science;specific absorption rate;operating system;symmetric multiprocessing;general-purpose computing on graphics processing units	HPC	-5.245324382822385	43.46257449673343	185140
2466112573dac5cb2eb9f1dbd4b93ac1767b74d8	early experience with the barrelfish os and the single-chip cloud computer		Traditional OS architectures based on a single, shared-memory kernel face significant challenges from hardware trends, in particular the increasing cost of system-wide cachecoherence as core counts increase, and the emergence of heterogeneous architectures – both on a single die, and also between CPUs, co-processors like GPUs, and programmable peripherals within a platform. The multikernel is an alternative OS model that employs message passing instead of data sharing and enables architectureagnostic inter-core communication, including across non-coherent shared memory and PCIe peripheral buses. This allows a single OS instance to manage the complete collection of heterogeneous, non-cache-coherent processors as a single, unified platform. We report on our experience running the Barrelfish research multikernel OS on the Intel Single-Chip Cloud Computer (SCC). We describe the minimal changes required to bring the OS up on the SCC, and present early performance results from an SCC system running standalone, and also a single Barrelfish instance running across a heterogeneous machine consisting of an SCC and its host PC.	autostereogram;barrelfish;cache coherence;central processing unit;cloud computing;coherence (physics);die (integrated circuit);emergence;graphics processing unit;inter-process communication;message passing;multi-core processor;multikernel;multiplexing;operating system;pci express;peripheral;scheduling (computing);shared memory;single-chip cloud computer;speedup;working set	Simon Peter;Adrian Schüpbach;Dominik Menzi;Timothy Roscoe	2011			operating system;barrelfish;single-chip cloud computer;computer science	Arch	-8.144065424156054	45.57590067685742	185160
1318207254101caf48df5827ce276d37ac20de40	exploiting geometric partitioning in task mapping for parallel computers	parallel machines application program interfaces graph theory message passing;partitioning algorithms program processors resource management measurement libraries network topology three dimensional displays;libtopomap library task mapping parallel computers mpi tasks sparse node allocation parallel machine geometric partitioning algorithm structured finite difference mini app minighost cray xe6 molecular dynamics mini app minimd communication time reduction graph based mappings	"""We present a new method for mapping applications' MPI tasks to cores of a parallel computer such that communication and execution time are reduced. We consider the case of sparse node allocation within a parallel machine, where the nodes assigned to a job are not necessarily located within a contiguous block nor within close proximity to each other in the network. The goal is to assign tasks to cores so that interdependent tasks are performed by """"nearby"""" cores, thus lowering the distance messages must travel, the amount of congestion in the network, and the overall cost of communication. Our new method applies a geometric partitioning algorithm to both the tasks and the processors, and assigns task parts to the corresponding processor parts. We show that, for the structured finite difference mini-app Mini Ghost, our mapping method reduced execution time 34% on average on 65,536 cores of a Cray XE6. In a molecular dynamics mini-app, Mini MD, our mapping method reduced communication time by 26% on average on 6144 cores. We also compare our mapping with graph-based mappings from the LibTopoMap library and show that our mappings reduced the communication time on average by 15% in MiniGhost and 10% in MiniMD."""	algorithm;baseline (configuration management);blue gene;central processing unit;cluster analysis;cray xe6;finite difference;geometric median;interdependence;k-means clustering;linear algebra;molecular dynamics;multigrid method;network congestion;parallel computing;run time (program lifecycle phase);sparse matrix	Mehmet Deveci;Sivasankaran Rajamanickam;Vitus J. Leung;Kevin T. Pedretti;Stephen Olivier;David P. Bunde;Ümit V. Çatalyürek;Karen D. Devine	2014	2014 IEEE 28th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2014.15	parallel computing;computer science;theoretical computer science;operating system;distributed computing;programming language	HPC	-5.642285065800972	41.09091784885547	185173
859973d50d28d19034d53847fc9bd70562e55967	reducing data distribution bottlenecks by employing data visualization filters	simulation computational processes;data visualization computational modeling computer simulation workstations scalability concurrent computing distributed computing research initiatives computer architecture computer interfaces;cache storage;cache storage data visualisation workstations distributed databases;multiple networked workstations;distributed database;concurrent computing;data visualization distribution;parallel computation distribution;distributed computing;central data cache data distribution bottleneck reduction data visualization filters parallel computation distribution data visualization distribution advanced visualization technology project hierarchical data cache architecture modeling computational processes simulation computational processes simultaneous data access multiple visualization client visual analysis computer simulation scalability multiple networked workstations distributed database data consumers;simultaneous data access;hierarchical data;data consumers;multiple visualization client;data distribution;modeling computational processes;data visualisation;computer architecture;data storage;computational modeling;data cache;hierarchical data cache architecture;workstations;visual analysis;data visualization;data access;distributed databases;distributed parallel computing;research initiatives;scalability;data visualization filters;data distribution bottleneck reduction;advanced visualization technology project;central data cache;computer interfaces;computer simulation	Between 1994 and 1997, researchers at Southwest Research Institute (SwRI) investigated methods for distributing parallel computation and data visualization under the support of an internally funded Research Initiative Program entitled the Advanced Visualization Technology Project (AVTP). A hierarchical data cache architecture was developed to provide a flexible interface between the modeling or simulation computational processes and data visualization programs. Compared to conventional post facto data visualization approaches, this data cache structure provides many advantages including simultaneous data access by multiple visualization clients, comparison of experimental and simulated data, and visual analysis of computer simulation as computation proceeds. However, since the data cache was resident on a single workstation, this approach did not address the issue of scalability of methods for avoiding the data storage bottleneck by distributing the data across multiple networked workstations. Scalability through distributed database approaches is being investigated as part of the Applied Visualization using Advanced Network Technology Infrastructure (AVANTI) project. This paper describes a methodology currently under development that is intended to avoid bottlenecks that typically arise as the result of data consumers (e.g. visualization applications) that must access and process large amounts of data that has been generated and resides on other hosts, and which must pass through a central data cache prior to being used by the data consumer.	data visualization	Ernest Franke;Michael J. Magee	1999		10.1109/HPDC.1999.805305	data access;parallel computing;scalability;workstation;computer science;theoretical computer science;operating system;computer data storage;database;distributed computing;computational model;distributed database;data visualization;hierarchical database model	Visualization	-9.494730600921915	39.53148580385563	185346
870c48a2c2fb067d12138bc0620b1a11a40a6a97	design, implementation, and performance evaluation of mpi 3.0 on portals 4.0	portals;point to point;one sided;synchronization;mpi	The latest version of the Portals interconnect programming interface contains several improvements intended for MPI point-to-point and one-sided communication operations, including new functionality defined in MPI-3. This paper discusses the rationale for these improvements to Portals and describes how they can be used in an MPI implementation. We provide preliminary micro-benchmark performance results using a reference implementation of Portals over InfiniBand Verbs and the Open MPI implementation.	application programming interface;benchmark (computing);design rationale;infiniband;message passing interface;open mpi;performance evaluation;point-to-point construction;portals;reference implementation	Amin Hassani;Anthony Skjellum;Ron Brightwell;Brian W. Barrett	2013		10.1145/2488551.2488563	parallel computing;real-time computing;computer science;distributed computing	HPC	-11.264979444821353	45.59622462723657	185387
d6beea674d62e76a79518c2ea8c6a27c75cb426a	using pvm in the simulation of a hybrid dataflow architecture	multiprocessor systems;optical interconnect;message passing	The aim of this work is to propose the migration of a hybrid dataflow architecture simulator developed on an uniprocessor system (sequential execution) to a multiprocessor system (parallel execution), using a message passing environment. To evaluate the communication between two process, we have been simulating an optical interconnection network based on WDM technics.	dataflow architecture;parallel virtual machine;simulation	Marcos Antonio Cavenaghi;Roberta Spolon Ulson;J. E. M. Perea-Martins;S. G. Domingues;Álvaro Garcia Neto	1996		10.1007/3540617795_48	dataflow architecture;computer architecture;parallel computing;distributed computing	Arch	-10.641413298132658	42.44267596047353	185460
3862b228226a053aef03b58f322adb5f93af43c1	adaptive migratory scheme for distributed shared memory	generalized multiway braching;conditional execution;tree vliw architecture;memory access;software distributed shared memory;dis tributed shared memory;speculative code motion;branch code motion;distributed shared memory	Software distributed shared memory (DSM) systems have many advantages over message passing systems. Since DSM provides a user a simple shared memory abstraction, the user does not have to be concerned with data movement between hosts. Many applications programmed for a multiprocessor system with shared memory can be executed on a software DSM system without significant modifications. This paper summarizes our research on DSM.	distributed shared memory;message passing;multiprocessing	Jai-Hoon Kim;Nitin H. Vaidya	1997		10.1145/263580.263659	distributed shared memory;shared memory;parallel computing;real-time computing;page fault;distributed memory;computer science;operating system;distributed computing	Arch	-10.952166742096358	43.773200052366406	186018
72151378b78a27d633c68d31dba3187a143a6ce0	intel paragon xp/s - architecture and software enviroment	intel paragon xp;software enviroment	The paper describes the hardware and software components of the Intel Paragon XP/S system, a distributed memory scalable multicomputer. The Paragon processing nodes, which are based on the Intel i860 XP RISC processor, are connected by a two-dimensional mesh with high bandwidth. This new interconnection network and the new operating system are the main differences between the Paragon and its predecessor, the iPSC/860 with its hypercube topology. The paper first gives an overview of the Paragon system architecture, the node architecture, the interconnection network, I/O interfaces, and peripherals. The second part outlines the Paragon OSF/1 operating system and the program development environment including programming models, compilers, application libraries, and tools for parallelization, debugging, and performance analysis.	intel paragon	Rüdiger Esser;Renate Knecht	1993		10.1007/978-3-642-78348-7_13	computer architecture;parallel computing;intel high definition audio;computer science;operating system;intel ipsc	Arch	-10.813548865816545	42.936604879594725	186325
a12abf6ef0503d4288d64611ca221a7a8fc043ed	practical performance portability in the parallel ocean program (pop)	parallel ocean program;performance;portability;computer architecture;cluster analysis;ocean models;vector;scalar	The design of the Parallel Ocean Program (POP) is described with an emphasis on portability. Performance of POP is presented on a wide variety of computational architectures, including vector architectures and commodity clusters. Analysis of POP performance across machines is used to characterize performance and identify improvements while maintaining portability. A new design of the POP model, including a cache blocking and land point elimination scheme is described with some preliminary performance results.	blocking (computing);computer cluster;loop nest optimization;vector processor	Philip W. Jones;Patrick H. Worley;Yoshikatsu Yoshida;James B. White;John M. Levesque	2005	Concurrency - Practice and Experience	10.1002/cpe.894	parallel computing;simulation;scalar;performance;vector;computer science;cluster analysis;computer graphics (images)	Metrics	-6.842088176046377	42.06810523005253	186990
572a25d816af0c0faf59d98941d476338330d735	optimization of fusion kernels on accelerators with indirect or strided memory access patterns		This paper describes optimization for high-dimensional stencil computations on accelerators involving complex memory access patterns, which appear in five dimensional fusion plasma turbulence codes, GYSELA and GT5D. They include different types of memory access patterns, the indirect memory access in GYSELA with a Semi-Lagrangian scheme and the strided memory access in GT5D with a Finite-Difference scheme. We focus on the affinity of the memory access patterns to accelerators such as GPGPUs and Xeon Phi coprocessors. On both devices, the Array of Structure of Array (AoSoA) data layout is preferable for contiguous memory accesses. It is shown that the effective local cache usage by improving spatial and temporal data locality is critical on Xeon Phi. On GPGPU, the texture memory usage improves the performance of the indirect memory accesses in the Semi-Lagrangian scheme. The reuse of registers by taking account of the physical symmetry of the Finite-Difference scheme reduces the amount of memory accesses. Through these optimizations, we achieve acceleration of 3.9 (8.1) on Xeon Phi (GPGPU) for the Semi-Lagrangian scheme and of 1.4 (3.9) on Xeon Phi (GPGPU) for the Finite-Different scheme with respect to the fully optimized codes on Sandy Bridge.	code;computation;coprocessor;finite difference method;general-purpose computing on graphics processing units;locality of reference;mathematical optimization;plasma active;processor affinity;sandy bridge;semi-lagrangian scheme;semiconductor industry;texture memory;turbulence;xeon phi	Yuuichi Asahi;Guillaume Latu;Takuya Ina;Yasuhiro Idomura;Virginie Grandgirard;Xavier Garbet	2017	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2016.2633349	kernel (linear algebra);parallel computing;computer science;xeon phi;texture memory;instruction set;cache;stencil;general-purpose computing on graphics processing units;coprocessor	HPC	-4.854171479004827	39.91617646376819	187091
a1963befdce51db635646223a9982c5aa1550de6	openmp tasks: asynchronous programming made easy	hierarchical programming asynchronous programming hpc architecture coral initiative fpu performance single node performance node architecture intranode parallelism thread synchronization reduction data locality memory hierarchy node parallelization shyfem code software parallelization openmp task technology openmp programming;algorithms parallel processing image color analysis computer architecture instruction sets synchronization benchmark testing;multi threading message passing	The task technology is one of most promising to exploit node parallelism for next generation of HPC architectures, like those under development within the CORAL initiative in the US. In general, single FPU performance is not going to increase any longer in contrast to single node performance. As a consequence, nodes architecture will feature more and more FPUs (proportional to the peak power of the node itself) and determine the need for an efficient technology able to take advantage of intra-node parallelism, reducing synchronization among threads and improving data locality to meet the memory hierarchy. In this paper we present a case study of node parallelization using SHYFEM code, a software for coastal area studies. To the best of our knowledge, this is one of the first work that shows how to fully parallelize a software with OpenMP task technology. We present the comparison between tasks and threads OpenMP programming paradigms, showing the advantages of using the hierarchical and asynchronous programming paradigm made available by OpenMP tasks.	floating-point unit;locality of reference;memory hierarchy;next-generation network;openmp;parallel computing;programming paradigm	Eric Pascolo;S. Salon;Donata Melaku Canu;Cosimo Solidoro;Carlo Cavazzoni;Georg Umgiesser	2016	2016 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2016.7568430	computer architecture;parallel computing;real-time computing;computer science;operating system;distributed computing;programming language	HPC	-6.4785761170333735	45.00832858835874	187238
807d004041c1dc990470de779b6dd3012f0a9785	hierarchical matrix-matrix multiplication based on multiprocessor tasks	distributed memory;matrix multiplication;memory hierarchy	We consider the realization of matrix-matrix multiplication and propose a hierarchical algorithm implemented in a task-parallel way using multiprocessor tasks on distributed memory. The algorithm has been designed to minimize the communication overhead while showing large locality of memory references. The task-parallel realization makes the algorithm especially suited for cluster of SMPs since tasks can then be mapped to the different cluster nodes in order to efficiently exploit the cluster architecture. Experiments on current cluster machines show that the resulting execution times are competitive with state-of-the-art methods like PDGEMM .	algorithm;computer cluster;distributed memory;locality of reference;matrix multiplication;multiprocessing;overhead (computing)	Sascha Hunold;Thomas Rauber;Gudula Rünger	2004		10.1007/978-3-540-24687-9_1	distributed shared memory;computer architecture;parallel computing;distributed memory;matrix multiplication;computer science;theoretical computer science;memory map	HPC	-4.612052166734207	41.63503440442813	187390
bc75e39a00730c3e0019d81f01e045cc2ac101d1	a simple bridging model for high-performance computing	bulk synchronous parallel programming;multi core processor;cluster computing;grid and cluster computing;parallel algorithm;message passing languages scalable computing libraries and programming environments grid and cluster computing;programming environment;high performance computing;resource allocation;computer model;prediction algorithms;semantics;parallel programming;libraries and programming environments;hierarchical routing network;scalable computing;graphics accelerator;computer architecture;simple bridging model;computational modeling;scatter gather parallel programming;load balancing scatter gather parallel programming parallel execution model high performance computing simple bridging model sgl language bulk synchronous parallel programming parallel architecture multicore processor graphics accelerator hierarchical routing network message passing programming;bulk synchronous parallel;high performance computer;load balancing;message passing;computational modeling program processors programming computer architecture prediction algorithms semantics parallel algorithms;multicore processor;load balance;resource allocation message passing parallel programming programming languages;parallel architecture;message passing programming;hierarchical routing;programming;program processors;languages;parallel execution model;sgl language;programming languages;parallel algorithms	This paper introduces the Scatter-Gather parallel-programming and parallel execution model in the form of a simple imperative language named SGL. Its design is based on experience with Bulk-synchronous parallel (BSP) programming and BSP languages. SGL's novel features are motivated by the last decade's move to multi-level and heterogeneous parallel architectures invoving multi-core processors, graphics accelerators and hierarchical routing networks. The design of SGL is coherent with Valiant's Multi-BSP while offering a programming interface even simpler than the primitives of Bulk-Synchronous parallel ML (BSML). SGL appears to cover a large subset of all BSP algorithms while avoiding complex message-passing programming. It allows automatic load balancing and like all BSP-inspired systems, predictable, portable and scalable performance.	algorithm;application programming interface;bridging (networking);bridging model;bulk synchronous parallel;central processing unit;coherence (physics);graphics processing unit;imperative programming;load balancing (computing);message passing;multi-core processor;routing;scalability;supercomputer	Chong Li;Gaétan Hains	2011	2011 International Conference on High Performance Computing & Simulation	10.1109/HPCSim.2011.5999831	multi-core processor;computer architecture;parallel computing;computer science;load balancing;operating system;distributed computing;semantics;parallel algorithm;programming language	HPC	-11.372232375612786	39.623645368715295	188459
465f25c04622812c8917f04953e511ad340e8a50	application of computational gpu opencl kernels for near-realtime audio processing		The possibilities to use OpenCL on GPU for low-latency audio processing are considered.	graphics processing unit;opencl api;streaming media	Petr F. Kartsev	2018		10.1145/3204919.3204943	parallel computing;audio signal processing;latency (engineering);computer science;supercomputer	Graphics	-7.1531057114415635	42.994672717702336	188698
6c50b304c4583de639b221e8b0c7dc0ab0b71b2c	energy estimation tool fpga-based approach for petroleum industry	oil drilling;petroleum industry energy consumption field programmable gate arrays oil drilling;fpga;computer architecture;energy measurement;energy consumption;field programmable gate arrays mathematical model equations energy consumption energy measurement computer architecture parallel processing;petroleum industry;mathematical model;seismic applications and petroleum industry;marmousi model energy estimation tool petroleum industry energy consumption high performance processing seismic migration application petroleum wells drilling fpga architecture brazilian industry;field programmable gate arrays;seismic applications and petroleum industry fpga energy consumption;parallel processing	Energy consumption is one of the great villains in high-performance processing when applied to large clusters that continuously run certain applications. Seismic migration applications are targets of this type of processing, since this feature denotes a need to apply complex models that are continuously run to evaluate drilling of petroleum wells. This work describes an analysis tool of energy consumption of a seismic application applied to FPGA architecture for a real Brazilian industry. A comparative study with the traditional multi-core and with GPGPU architectures is performed and results indicate an increase in efficiency/Joule of about 23 and 1.5 times higher respectively. Experiments performed with the Marmousi model revels an error about 3.7% when compared with measured values.	experiment;field-programmable gate array;general-purpose computing on graphics processing units;joule;multi-core processor	Gilliano Ginno Silva de Menezes;Abel G. Silva-Filho;Viviane Lucy Santos de Souza;Victor Wanderley Costa de Medeiros;Manoel Eusebio de Lima;Rodrigo Gandra;Ricardo Bragança	2012	2012 41st International Conference on Parallel Processing Workshops	10.1109/ICPPW.2012.88	parallel processing;computer science;field-programmable gate array	HPC	-4.754610078688231	40.98175904790732	188887
1a9cb4b9927df8678e65f7c2157e3df80081b3b3	meterpu: a generic measurement abstraction api enabling energy-tuned skeleton backend selection	energy;skeleton programming;optimization	We present MeterPU, an easy-to-use, generic and low-overhead abstraction API for taking measurements of various metrics (time, energy) on different hardware components (e.g. CPU, DRAM, GPU), using pluggable platform-specific measurement implementations behind a common interface in C++. We show that with MeterPU, not only legacy (time) optimization frameworks, such as autotuned skeleton back-end selection, can be easily retargeted for energy optimization, but also switching different optimization goals for arbitrary code sections now becomes trivial. We apply MeterPU to implement the first energy-tunable skeleton programming framework, based on the SkePU skeleton programming library.	application programming interface	Lu Li;Christoph W. Kessler	2015		10.1109/Trustcom.2015.625	embedded system;parallel computing;real-time computing;computer science;skeleton	Networks	-6.15142540111194	44.3917980843945	189088
24decda6638e7f4710bc18756bea84e9e5b3b164	offloading collective operations to programmable logic on a zynq cluster	fpga;collective operations;soc;mpi;zynq	This paper describes our architecture and implementation for offloading collective operations to programmable logic in the communication substrate. Collective operations – operations that involve communication between groups of co-operating processes – are widely used in parallel processing. The design and implementation strategies of collective operations plays a significant role in their performance and thus affects the performance of many high performance computing applications that utilize them. Collectives are central to the widely used Message Passing Interface (MPI) programming model. The programmable logic provided by FPGAs is a powerful option for creating task-specific logic to aid applications. While our work is evaluated on the Xilinx Zynq SoC, it is generally applicable in scenarios where there is programmable logic in the communication pipeline, including FPGAs on network interface cards like the NetFPGA or new systems like Intel's Xeon with on-die Altera FPGA resources. In this paper we have adapted and generalized our previous work in offloading collective operations to the NetFPGA. Here we present a general collective offloading framework for use in applications using the Message Passing Interface (MPI). The implementation is realized on the Xilinx Zynq reference platform, the Zedboard, using an Ethernet daughter card called EthernetFMC. Results from microbenchmarks are presented as well as from some scientific applications using MPI.	field-programmable gate array;message passing interface;netfpga;network interface controller;parallel computing;programmable logic device;programming model;supercomputer	Omer Arap;D. Martin Swany	2016	2016 IEEE 24th Annual Symposium on High-Performance Interconnects (HOTI)	10.1109/HOTI.2016.024	system on a chip;computer architecture;parallel computing;real-time computing;computer science;message passing interface;operating system;field-programmable gate array	HPC	-10.451970950786642	45.800704197423016	189330
2b2cd8237ca1ecd7171485961f64bd447d5d62bb	topology mapping of irregular parallel applications on torus-connected supercomputers	high-performance computing;topology mapping;communication optimization;torus network;analytical algorithm	Supercomputers with ever increasing computing power are being built for scientific applications. As the system size scales up, so does the size of interconnect network. As a result, communication in supercomputers becomes increasingly expensive due to the long distance between nodes and network contention. Topology mapping, which maps parallel application processes onto compute nodes by considering network topology and application communication pattern, is an essential technique for communication optimization. In this paper, we study the topology mapping problem for torus-connected supercomputers, and present an analytical topology mapping algorithm for parallel applications with irregular communication patterns. We consider our problem as a discrete optimization problem in the geometric domain of a torus topology, and design an analytical mapping algorithm, which uses numerical solvers to compute the mapping. Experimental results show that our algorithm provides high-quality mappings on 3-dimensional torus, which significantly reduce the communication time by up to 72%.	algorithm;discrete optimization;map;mathematical optimization;network topology;numerical analysis;optimization problem;supercomputer	Jingjin Wu;Xuanxing Xiong;Eduardo Berrocal;Jia Wang;Zhiling Lan	2016	The Journal of Supercomputing	10.1007/s11227-016-1876-7	parallel computing;computer science;theoretical computer science;distributed computing;extension topology;logical topology	HPC	-5.595318711541945	40.94135160814797	189803
a9470f8f65bc8e67c708c8256f128b91be8b0fce	task mapping stencil computations for non-contiguous allocations	stencil communication pattern;non contiguous allocation;improved scalability;task mapping	We examine task mapping algorithms for systems that allocate jobs non-contiguously. Several studies have shown that task placement affects job running time. We focus on jobs with a stencil communication pattern and use experiments on a Cray XE to evaluate novel task mapping algorithms as well as some adapted to this setting. This is done with the miniGhost miniApp which mimics the performance of CTH, a shock physics application. Our strategies improve average and single-run times by as much as 28% and 36% over a baseline strategy, respectively.	algorithm;baseline (configuration management);computation;experiment;job stream;time complexity	Vitus J. Leung;David P. Bunde;Jonathan Ebbers;Stefan P. Feer;Nickolas W. Price;Zachary D. Rhodes;Matthew Swank	2014		10.1145/2555243.2555277	parallel computing;real-time computing;computer science;distributed computing	HPC	-8.6596919245795	44.30041714014753	190058
5a6ac82975aebb614a88a89fe79213f0d4d8ab00	grouping in nested loops for parallel execution on multicomputers	multiprocessing programs;nested loops;computer systems digital;computer systems programming;parallel processing			Chung-Ta King;Lionel M. Ni	1989			parallel processing;computer architecture;parallel computing;nested loop join;computer science;distributed computing	HPC	-10.139352655933028	41.9363709128574	190758
0247ffee0cb55f24ac8dcfa747a3c600f322d340	conference on experiences in applying parallel processors to scientific computation	scientific computing	There is general agreement that uniprocessor speeds are nearing fundamental limits and that the only way that more speed can be attained is to exploit computational parallelism of one sort or another. The concept is simple but the implementations are not. Clearly, the way to the future must be evolutionary, and it will require the skills of the computer industry, universities, and the national laboratories. The first production mutiprocessor supercomputer systems are now being delivered. Indications are that within the next five years supercomputer manufacturers will offer systems with up to 16 processors. Successful exploitation of their potential performance will require algorithms, software, and hardware that, when combined as a single system, will achieve high-average processor utilization and introduce little additional work relative to a single-processor implementation. The questions of how best to do these things are occupying the attention of researchers all over the world. This work is considered the key to expansion of computer performance. Within the past two years, a significant amount of experimentation on the parallel processing of scientific computation has been done. Most of this work as related to Supercomputers has not been previously reported. Consequently, on March 13-15, Lawrence Livermore National Laboratory and Los Alamos National Laboratory hosted a meeting at Gleneden Beach, Oregon, at which many of these experiments were discussed. The general thrust of these presentations suggests several important results such as 1. A broad spectrum of scientific computation is amenable to parallel processing. The presentations revealed that parallel formulations have been achieved for meaningful computational kernels from such areas as Plasma simulation, Lagrangian fluid flow simulation, Reactor safety simulation, Automated reasoning,	algorithm;automated reasoning;central processing unit;computation;computational science;computer performance;experiment;parallel computing;reactor (software);simulation;supercomputer;thrust;uniprocessor system	Bill Buzbee;George Michael	1984	Parallel Computing	10.1016/S0167-8191(84)90108-X	computational science;computer science;theoretical computer science	HPC	-7.835525546204842	39.69439908604025	190972
8ea24f84de4bed02780f67d3dec5ec15f598ade6	an execution trace verification method on linearizability		As multicore processors have become the standard architecture for generalpurpose machines, programmers are required to write software optimized for parallelism. Verifying parallel code is very complex, and new practical methods for verification are therefore very important. Tools that provide verification for complex parallel code are still missing. This dissertation introduces a method for testing the execution of code and shows that this method already provides valuable results for the application programmer. The verification method is used to prove whether a program trace is linearizable. In addition, some useful properties of algorithms are highlighted by which a practical use of the method can be optimized. Furthermore, this work provides a case study on priority queues of how this method can be applied. Our case study shows that practicable results can be obtained with this method.	algorithm;central processing unit;linearizability;multi-core processor;parallel computing;priority queue;programmer;tracing (software)	Kristijan Dragicevic	2011			priority queue;linearizability;parallel computing;software;multi-core processor;architecture;programmer;runtime verification;computer science	SE	-11.685260131483084	40.45829135086814	191494
4e09cf45900f5cb0bfcbd3043e3c921245b8e801	topic 9 parallel and distributed programming	distributed application;design tool;design and development;distributed programs;experimental evaluation;parallel programs;high performance	Developing parallel or distributed applications is a hard task and it requires advanced algorithms, realistic modeling, efficient design tools, high performance languages and libraries, and experimental evaluation. This topic provides a forum for presentation of new results and practical experience in this domain. It emphasizes research that facilitates the design and development of correct, high-performance, portable, and scalable parallels program. We received 24 papers and accepted 7. “Delayed Side-Effects Ease Multi-Core Programming” and “MCSTL: The Multi-Core Standard Template Library” deal with multi-core programming wich is one of the hot topic in parallel programming. The first paper is focused on language while the other is focused on library. The library approach is also used in the paper “Library Support for Parallel Sorting in Scientific Computations” while the compilation approach is applied in the paper “Nested Parallelism in the OMPi OpenMP/C Compiler”. Other approaches such as skeleton have been proposed for parallel programming. This approach serves as the basis of two papers in this session: “DomainSpecific Optimization Strategy for Skeleton Programs” and“ Management in Distributed Systems: a Semi-Formal Approach”. Finally, such a topic would not be complete without an “application” paper. “Efficient Parallel Simulation of Large-Scale Neuronal Networks on Clusters of Multiprocessor Computers” presents an innovative way to parallelize biological neural network.	algorithm;artificial neural network;compiler;distributed computing;library (computing);multi-core processor;multiprocessing;openmp;parallel computing;parallels desktop for mac;scalability;semiconductor industry;simulation;skeleton (computer programming);sorting;standard template library	Luc Moreau;Emmanuel Jeannot;George Bosilca;Antonio J. Plaza	2007		10.1007/978-3-540-74466-5_67	parallel computing;computer science;theoretical computer science;distributed computing;distributed design patterns;programming language	HPC	-10.942359724545302	39.74928971931787	191596
e97ec7cbdef13039d557890607b81dbcb24bc5d6	heterogeneous multi-core parallel sgemm performance testing and analysis on cell/b.e processor	microprocessors;kernel;performance evaluation;performance test;multi core matrix multiplication cell optimization heterogeneous parallel;cell b e processor;heterogeneous multiprocessor;cell;cell processor;single precision general matrix multiplication;memory access;computer architecture;performance improvement;heterogeneous;computer architecture microprocessors blades registers kernel instruction sets broadband communication;heterogeneous multicore parallel sgemm performance testing;registers;numerical algorithm;parallel;scientific computing;blades;sti cell processor;level 3 blas;optimization;floating point;matrix multiplication;multiprocessing systems;performance evaluation matrix multiplication multiprocessing systems parallel processing;memory allocation;interleaved memory allocation heterogeneous multicore parallel sgemm performance testing cell b e processor level 3 blas sti cell processor heterogeneous multiprocessor single precision general matrix multiplication;broadband communication;parallel processing;multi core;instruction sets;interleaved memory allocation	Matrix multiplication is one of the most common numerical operations in the field of scientific computing, which is the kernel routine of Level 3 BLAS. The STI CELL processor is a heterogeneous multiprocessor with a unique design to achieve high peak floating point performance. As matrix multiplication operation is essential for a wide range of numerical algorithms, so performance improvements to the GEMM routine immediately can benefit the entire algorithm. In this paper, we provide a new way to utilize the hardware features of Cell to achieve better performance on the Single Precision General Matrix Multiplication (SGEMM), through both heterogeneous PPEs and SPEs parallelization, our method gains speedup over the Cell SDK (2.5%). An extra speedup about 30% of performance is achieved via interleaved memory allocation, which improves memory access.	algorithm;blas;cell (microprocessor);computational science;interleaved memory;matrix multiplication;multi-core processor;multiprocessing;numerical analysis;parallel computing;single-precision floating-point format;software development kit;speedup	Yan Li;Yunquan Zhang;Ke Wang;Wenhua Guan	2010	2010 IEEE Fifth International Conference on Networking, Architecture, and Storage	10.1109/NAS.2010.48	multi-core processor;cell;parallel processing;computer architecture;parallel computing;kernel;real-time computing;matrix multiplication;computer science;floating point;operating system;instruction set;parallel;processor register;memory management	HPC	-5.4524770102677795	44.173250225624315	192400
2c7006d20b396869a36bf6d8094a45ca8239e028	a comparison of mpi, shmem and cache-coherent shared address space programming models on the sgi origin2000	data parallel languages;barrier synchronization;parallel algorithm;data locality;data parallelism;run time systems;cache reuse;application program interface;programming model;loop scheduling;macro dataflow;object parallelism;object oriented;community structure;high performance computer;cache coherence;programming models;scientific computation;communication pattern;dependence driven execution	We compare the performance of three major programming models— a load-store cache-coherent shared address space (CC-SAS), message passing (MP) and the segmented SHMEM model—on a modern, 64-processor hardware cache-coherent machine, one of the two major types of platforms upon which high-performance computing is converging. We focus on applications that are either regular and predictable or at least do not require fine-grained dynamic replication of irregularly accessed data. Within this class, we use programs with a range of important communication patterns. We examine whether the basic parallel algorithm and communication structuring approaches needed for best performance are similar or different among the models, whether some models have substantial performance advantages over others as problem size and number of processors change, what the sources of these performance differences are, where the programs spend their time, and whether substantial improvements can be obtained by modifying either the application programming interfaces or the implementations of the programming models on this type of platform.	address space;analysis of algorithms;application programming interface;cache (computing);cache coherence;central processing unit;coherence (physics);message passing interface;parallel algorithm;sas;shmem;supercomputer	Hongzhang Shan;Jaswinder Pal Singh	1999		10.1145/305138.305210	parallel computing;real-time computing;computer science;operating system;distributed computing;programming paradigm;programming language	Arch	-7.255339797344065	43.03753237477904	192534
2d8832063f037a3dde6df3eb4dbaaf32ff1ea2cb	on the clumps model of parallel computation	computer storage;parallel computation;algorithme parallele;multi level memory hierarchy;calculo paralelo;parallel architectures;architecture parallele;parallel computer;memory hierarchy;memoria ordinador;unified architecture;calcul parallele;parallel processing;memoire ordinateur;parallel algorithms	The unification of four basic was attempted to form a single class. is, unifying classes which simulated on each other and run in asymptotically equivalent time. aim being for any of four parallel be applicable to single, unified class the properties of four classes, so forming the basis a universal model of in the parallel machine in the parallel programming the programming paradigm applicable over the unified of architectures.	computation;parallel computing;programming paradigm;unification (computer science)	Duncan K. G. Campbell	1998	Inf. Process. Lett.	10.1016/S0020-0190(98)00059-3	parallel processing;parallel computing;computer science;theoretical computer science;computer data storage;parallel algorithm;algorithm	Arch	-11.15439219610334	41.86684593205821	192608
22fa1237ed6e3832087266bca43cd107b6b8fb47	evaluation of the feasibility of making large-scale x-ray tomography reconstructions on clouds	computed tomography;elastic compute cloud;reconstruction;elastic compute cloud cloud computing hpc computed tomography reconstruction amazon ec2;x ray microscopy cloud computing computerised tomography image reconstruction medical image processing;hpc;slabs;three dimensional displays;image reconstruction;slabs cloud computing image reconstruction instruction sets computed tomography hardware three dimensional displays;amazon ec2;hpc based hardware large scale x ray tomography reconstructions mangoose medical image application 3d volume reconstruction cloud computing panel detectors computed tomography execution times clusters accelerators hardware renewal hardware maintenance amazon ec2 platform;cloud computing;instruction sets;hardware	This work focuses on the evaluation of the suitability of Mangoose++, a medical image application for reconstruction of 3D volumes, by means of Cloud Computing. Due to the increasing resolution of panel detectors in computed tomography and the need of lower execution times, the use of parallel implementations for clusters and accelerators have been generalized. Anyhow, the renewal and maintenance of hardware is expensive which makes Cloud Computing a valuable alternative. In our evaluation, we analyze and discuss the costs and efficiency of the Mangosee++ application over Amazon EC2 platform, demonstrating that lower times can be achieved in a reasonable price compared with owned HPC-based hardware. We also provide a comparison between distinct hardware configurations so that we can infer the advantages and disadvantages of each one.	aggregate data;algorithm;amazon elastic compute cloud (ec2);cpu cache;ct scan;central processing unit;cloud computing;experiment;graphics processing unit;hdmi;hardware performance counter;mapreduce;multi-core processor;programming model;run time (program lifecycle phase);sensor;tomography	Estefania Serrano;Guzman Bermejo;Francisco Javier García Blas;Jesús Carretero	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.106	iterative reconstruction;supercomputer;simulation;cloud computing;computer science;theoretical computer science;operating system;instruction set;database;distributed computing;computed tomography;computer graphics (images)	Arch	-5.060308368476464	44.181693771422935	193079
6385ecf574bc1dea8148ed3983b2ca4bcbeacd83	dynamic task parallelism with a gpu work-stealing runtime system	paper;tesla c2050;data parallelism;cuda;nvidia;computer science;task scheduling	NVIDIA’s Compute Unified Device Architecture (CUDA) enabled GPUs become accessible to mainstream programming. Abundance of simple computational cores and high memory bandwidth make GPUs ideal candidates for data parallel applications. However, its potential for executing applications that combine task and data parallelism has not been explored in detail. CUDA does not provide a viable interface for creating dynamic tasks and handling load balancing issues. Any support for such has to be orchestrated entirely by the CUDA programmer today. In this work, we introduce a finish-async style API to GPU device programming as first step towards task parallelism. We present the design and implementation details of our new intra-device inter-SM workstealing runtime system. We compare performance results using our runtime to direct execution on the device as well as past work on GPU runtimes. Finally, we show how this runtime can be targeted by extensions to the high-level CnC-CUDA programming model.	application programming interface;benchmark (computing);cuda;central processing unit;data parallelism;dijkstra's algorithm;double-ended queue;execution unit;graphics processing unit;high memory;high- and low-level;load balancing (computing);mathematical optimization;memory bandwidth;memory management;overhead (computing);parallel computing;parallel programming model;programmer;programming language;programming model;run time (program lifecycle phase);runtime system;scheduling (computing);task parallelism;thread block;warp (information security);work stealing;x10	Sanjay Chatterjee;Max Grossman;Alina Simion Sbîrlea;Vivek Sarkar	2011		10.1007/978-3-642-36036-7_14	computer architecture;parallel computing;computer science;operating system;data parallelism;instruction-level parallelism;implicit parallelism;task parallelism	HPC	-6.4652296795414745	44.06373193262184	194528
ca0d82b4e0733949bdac38ce71e1bb66ec5ba302	folded fat h-tree: an interconnection topology for dynamically reconfigurable processor array	distributed system;systeme reparti;calculateur embarque;reconfigurable architectures;pervasive computing;metodo arborescente;procesador panel;array processor;informatica difusa;network topology;processeur tableau;sistema repartido;toro;informatique diffuse;torus;tore;boarded computer;tree structured method;methode arborescente;topologie circuit;dynamically reconfigurable processor;architecture reconfigurable;calculador embarque	Fat H-Tree is a novel on-chip network topology for a dynamic reconfigurable processor array. It includes both fat tree and torus structure, and suitable to map tasks in a stream processing. For on-chip implementation, folding layout is also proposed. Evaluation results show that Fat H-Tree reduces the distance of H-Tree from 13% to 55%, and stretches the throughput almost three times.	array data structure;fat tree;h tree;interconnection;network on a chip;network topology;processor array;reconfigurable computing;stream processing;throughput	Yutaka Yamada;Hideharu Amano;Michihiro Koibuchi;Akiya Jouraku;Kenichiro Anjo;Katsunobu Nishimura	2004		10.1007/978-3-540-30121-9_29	embedded system;parallel computing;real-time computing;computer science;torus;operating system;distributed computing;ubiquitous computing;network topology;computer network	Arch	-9.30036054779808	44.01073844860427	194842
47e01fa85da697fba64a6682e921e5520c2653d2	benchmarking mixed-mode petsc performance on high-performance architectures	parallel	The trend towards highly parallel multi-processing is ubiquitous in all modern computer architectures, ranging from handheld devices to large-scale HPC systems; yet many applications are struggling to fully utilise the multiple levels of parallelism exposed in modern high-performance platforms. In order to realise the full potential of recent hardware advances, a mixed-mode between shared-memory programming techniques and inter-node message passing can be adopted which provides high-levels of parallelism with minimal overheads. For scientific applications this entails that not only the simulation code itself, but the whole software stack needs to evolve. In this paper, we evaluate the mixed-mode performance of PETSc, a widely used scientific library for the scalable solution of partial differential equations. We describe the addition of OpenMP threaded functionality to the library, focusing on sparse matrix-vector multiplication. We highlight key challenges in achieving good parallel performance, such as explicit communication overlap using task-based parallelism, and show how to further improve performance by explicitly load balancing threads within MPI processes. Using a set of matrices extracted from Fluidity, a CFD application code which uses the library as its linear solver engine, we then benchmark the parallel performance of mixed-mode PETSc across multiple nodes on several modern HPC architectures. We evaluate the parallel scalability on Uniform Memory Access (UMA) systems, such as the Fujitsu PRIMEHPC FX10 and IBM BlueGene/Q, as well as a Non-Uniform Memory Access (NUMA) Cray XE6 platform. A detailed comparison is performed which highlights the characteristics of each particular architecture, before demonstrating efficient strong scalability of sparse matrix-vector multiplication with significant speedups over the pure-MPI mode.	algorithm;benchmark (computing);blue gene;cas latency;computer architecture;cray xe6;cyberinfrastructure;hector;hpcx;image scaling;load balancing (computing);load/store architecture;matrix multiplication;memory bandwidth;message passing interface;mixed-signal integrated circuit;mobile device;multiprocessing;next-generation network;non-uniform memory access;numerical linear algebra;open-source software;openmp;petsc;primehpc fx10;parallel computing;proposed directive on the patentability of computer-implemented inventions;scalability;shared memory;simulation;solver;sparse matrix;speedup;supercomputer;the matrix;thread (computing);thread safety;uniform memory access	Michael Lange;Gerard Gorman;Michèle Weiland;Lawrence Mitchell;Xiaohu Guo;James Southern	2013	CoRR		computer architecture;parallel computing;real-time computing;computer science;operating system;distributed computing;programming language	HPC	-5.851891094477006	40.16517824772419	195053
187707bac36afba07befec1e6fc497e79857a49f	a framework for integrating data alignment, distribution, and redistribution in distributed memory multiprocessors	distribution;distributed memory;distributed memory systems;0 1 integer programming distributed memory multiprocessors data alignment parallel architectures scalability data mapping data layout strategy automatic data mapping;multiprocessor systems;loop parallelization;indexing terms;distributed memory multiprocessor;large scale;distributed memory systems parallel architectures;parallel architectures;graph representation;redistribution;cost effectiveness;performance prediction;data layout;parallel architecture;integer program;article;alignment;linear 0 1 integer programming;parallel processing availability linear programming concurrent computing distributed computing parallel programming computer architecture application software scalability large scale integration;automatic data mapping	ÐParallel architectures with physically distributed memory provide a cost-effective scalability to solve many large scale scientific problems. However, these systems are very difficult to program and tune. In these systems, the choice of a good data mapping and parallelization strategy can dramatically improve the efficiency of the resulting program. In this paper, we present a framework for automatic data mapping in the context of distributed memory multiprocessor systems. The framework is based on a new approach that allows the alignment, distribution, and redistribution problems to be solved together using a single graph representation. The Communication Parallelism Graph (CPG) is the structure that holds symbolic information about the potential data movement and parallelism inherent to the whole program. The CPG is then particularized for a given problem size and target system and used to find a minimal cost path through the graph using a general purpose linear 0-1 integer programming solver. The data layout strategy generated is optimal according to our current cost and compilation models. Index TermsÐAutomatic data mapping, alignment, distribution, redistribution, performance prediction, distributed-memory multiprocessor, loop parallelization, linear 0-1 integer programming.	analysis of algorithms;central pattern generator;compiler;data structure alignment;distributed memory;graph (abstract data type);integer programming;linear programming;multiprocessing;parallel computing;performance prediction;scalability;solver	Jordi Garcia;Eduard Ayguadé;Jesús Labarta	2001	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.920590	distribution;computer architecture;parallel computing;cost-effectiveness analysis;index term;distributed memory;redistribution;computer science;theoretical computer science;distributed computing;graph;programming language	HPC	-7.120199315763607	41.966973715862096	195092
a100a0f73c950670f0046a4fb1f90138df326d19	cached dasd evaluations for paging and non paging data (abstract).	data abstraction			George W. Dodson	1982			parallel computing;real-time computing;page replacement algorithm;database	Theory	-11.006492633453197	44.036714169875644	195344
a668b177693f71adc1ec816be92f6769c0b0dae5	an empirical study on the potential of search parallelism on mimd architecture.	empirical study		mimd;parallel computing	Javed I. Khan;David Y. Y. Yun	1992			computer architecture;parallel computing;computer science;theoretical computer science;data parallelism	Arch	-9.398141712114866	41.85741175629868	195379
c70aa4e1e4cb09472691530173545bdd2da040ad	load-balancing methods for parallel and distributed constraint solving	computers;resource allocation;benchmark problem;execution time scalability load balancing method constraint programming parallel constraint solving distributed constraint solving;load balancing method;distributed constraint solving;execution time scalability;parallel constraint solving;shape;declarative languages;resource allocation constraint handling parallel processing;constraint programming;computers program processors programming parallel processing scalability search problems shape;datavetenskap datalogi;distributed search;constraint handling;constraint solving;load balance;search problems;scalability;programming;program processors;parallel processing	Program parallelization and distribution becomes increasingly important when new multi-core architectures and cheaper cluster technology provide ways to improve performance. Using declarative languages, such as constraint programming, can make the transition to parallelism easier for the programmer. In this paper, we address parallel and distributed search in constraint programming (CP) by proposing several load-balancing methods. We show how these methods improve the execution-time scalability of constraint programs. Scalability is the greatest challenge of parallelism and it is particularly an issue in constraint programming, where load-balancing is difficult. We address this problem by proposing CP-specific load-balancing methods and evaluating them on a cluster by using benchmark problems. Our experimental results show that the methods behave differently well depending on the type of problem and the type of search. This gives the programmer the opportunity to optimize the performance for a particular problem.	benchmark (computing);constraint programming;constraint satisfaction problem;declaration (computer programming);distributed web crawling;load balancing (computing);mathematical optimization;multi-core processor;optimization problem;parallel computing;programmer;scalability;search tree;solver	Carl Christian Rolf;Krzysztof Kuchcinski	2008	2008 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2008.4663786	concurrent constraint logic programming;parallel processing;programming;constraint programming;parallel computing;scalability;constraint satisfaction;shape;resource allocation;computer science;load balancing;theoretical computer science;distributed computing	HPC	-7.351278355771483	45.21262287031337	195424
f1ab00210e5d675d3e7cc520b4248540459634c0	ph: a language for implicit parallel programming (a review of implicit parallel programming in ph by rishiyur s. nikhil & arvind)			parallel computing		2003	IEEE Distributed Systems Online			HPC	-10.13336644717903	40.69128762495453	196428
7ee23bd4979aff5374ffa58800d4204e1f4714a2	omb-gpu: a micro-benchmark suite for evaluating mpi libraries on gpu clusters	clusters;micro benchmarks;gpgpu;mpi	General-Purpose Graphics Processing Units (GPGPUs) are becoming a common component of modern supercomputing systems. Many MPI applications are being modified to take advantage of the superior compute potential offered by GPUs. To facilitate this process, many MPI libraries are being extended to support MPI communication from GPU device memory. However, there is lack of a standardized benchmark suite that helps users evaluate common communication models on GPU clusters and do a fair comparison for different MPI libraries. In this paper, we extend the widely used OSU Micro-Benchmarks (OMB) suite with benchmarks that evaluate performance of point-point, multi-pair and collective MPI communication for different GPU cluster configurations. Benefits of the proposed benchmarks for MVAPICH2 and OpenMPI libraries are illustrated.	benchmark (computing);graphics processing unit;message passing interface	Devendar Bureddy;Hao Wang;Akshay Venkatesh;Sreeram Potluri;Dhabaleswar K. Panda	2012		10.1007/978-3-642-33518-1_16	computational science;parallel computing;computer science;operating system	HPC	-5.822374637857347	44.023120467440926	196613
dbcf5b29764005c12472914c74217d67ffe01e63	efficient java communication libraries over infiniband	libraries;protocols;object oriented languages java communication libraries network communications high speed networks java fast sockets java direct infiniband parallel java applications programming productivity;java fast sockets;software libraries;high speed networks;parallel java applications;sockets;data mining;software libraries java;arrays;java libraries sockets high speed networks delay throughput computer architecture bandwidth design optimization quality of service;java communication libraries;java direct infiniband;programming productivity;high performance;network communications;object oriented languages;throughput;java	This paper presents our current research efforts on efficient Java communication libraries over InfiniBand. The use of Java for network communications still delivers insufficient performance and does not exploit the performance and other special capabilities (RDMA and QoS) of high-speed networks, especially for this interconnect. In order to increase its Java communication performance, InfiniBand has been supported in our high performance sockets implementation, Java Fast Sockets (JFS), and it has been greatly improved the efficiency of Java Direct InfiniBand (Jdib), our low-level communication layer, enabling zero-copy RDMA capability in Java. According to our experimental results, Java communication performance has been improved significantly, reducing start-up latencies from 34μs down to 12 and 7μs for JFS and Jdib, respectively, whereas peak bandwidth has been increased from 0.78 Gbps sending serialized data up to 6.7 and 11.2 Gbps for JFS and Jdib, respectively. Finally, it has been analyzed the impact of these communication improvements on parallel Java applications, obtaining significant speedup increases of up to one order of magnitude on 128 cores.	data rate units;fast fourier transform;high- and low-level;infiniband;interoperability;java;java performance;library (computing);overhead (computing);performance evaluation;remote direct memory access;software portability;speedup;user space;zero-copy	Guillermo L. Taboada;Juan Touriño;Ramón Doallo;Yao Lin;Jizhong Han	2009	2009 11th IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2009.87	communications protocol;throughput;computer architecture;parallel computing;computer science;operating system;sockets direct protocol;programming language;object-oriented programming;java	HPC	-10.276414541319117	46.35207002031476	196777
df21917dbe0205d8107acff0499fa2ee799c6812	efficient high performance computing on heterogeneous platforms	parallel programming	Heterogeneous platforms are mixes of different processing units in a compute node (e.g., CPUs+GPUs, CPU+MICs) or a chip package (e.g., APUs). This type of platforms keeps gaining popularity in various computer systems ranging from supercomputers to mobile devices. In this context, improving their efficiency and usability has become increasingly important. In this thesis, we develop systematic methods for a large variety of data parallel applications to efficiently utilize heterogeneous platforms. Specifically, (1) we evaluate the suitability of OpenCL as a unified programming model for heterogeneous computing and improve OpenCLu0027s efficiency for programming heterogenous platforms; (2) we develop a workload partitioning framework to accelerate imbalanced applications on heterogeneous platforms, where we match the heterogeneity of the platform with the imbalance of the workload; (3) we propose a model-based prediction method to correctly and quickly predict the optimal workload partitioning, maximizing the performance gain while speeding up the partitioning process; (4) we generalize a systematic workload partitioning approach which improves performance for both balanced and imbalanced applications, for applications with different datasets and execution scenarios, and for platforms with different hardware mixes; (5) we design an application analyzer that analyzes application kernel structures and enables different partitioning strategies accordingly to obtain both high performance and wide applicability for workload partitioning on heterogeneous platforms. To summarize, this thesis demonstrates that heterogeneous platforms are the right solution, performance-wise, for many classes of data parallel applications, and shows how high performance can be achieved systematically.		Jie Shen	2015			parallel computing;real-time computing;computer science;operating system	HPC	-4.777398694082333	44.5691855827894	196831
892b7a56dad27028ba2cc68a900b8ee220792e85	exploiting concurrent kernel execution on graphic processing units	sar sensor processing concurrent kernel execution graphic processing units coprocessor solution high performance computing domain multicore platform cpu threads parallel application computing resource sharing nvidia fermi architecture multithreaded application environment context funneling cuda v4 0 synthetic microbenchmark tests compact application benchmark ssca 3;manuals;kernel;multi threading;tesla c2070;paper;concurrent kernel execution;gpu computing;resource allocation;multi threaded programming;performance;computer graphic equipment;graphics processing unit kernel context instruction sets switches programming manuals;coprocessors;resource allocation computer graphic equipment concurrency control coprocessors multiprocessing systems multi threading parallel processing;cuda;programming techniques;concurrency control;nvidia;graphic processing unit;multiprocessing systems;computer science;switches;graphics processing unit;programming;context;parallel applications;parallel processing;concurrent kernel execution gpu computing multi threaded programming;instruction sets	Graphics processing units (GPUs) have been accepted as a powerful and viable coprocessor solution in high-performance computing domain. In order to maximize the benefit of GPUs for a multicore platform, a mechanism is needed for CPU threads in a parallel application to share this computing resource for efficient execution. NVIDIA's Fermi architecture pioneers the feature of concurrent kernel execution; however, only kernels of the same thread context can execute in parallel. In order to get the best use of a GPU device in a multi-threaded application environment, this paper explores the techniques to effectively share a context, i.e., context funneling, which could be done either manually at application level, or automatically at the GPU runtime starting from CUDA v4.0. For synthetic microbenchmark tests, we find that both funneling mechanisms are more capable of exploring the benefit of concurrent kernel execution than traditional context switching, therefore improving the overall application performance. We also find that the manual funneling mechanism provides the highest performance and more explicit control, while CUDA v4.0 provides better productivity with good performance. Finally, we assess the impact of such techniques on a compact application benchmark, SSCA#3 — SAR sensor processing.	benchmark (computing);cuda;central processing unit;computer multitasking;concurrent computing;context switch;coprocessor;dhrystone;fermi (microarchitecture);graphics processing unit;kernel (operating system);multi-core processor;programmer;scheduling (computing);supercomputer;synthetic intelligence;thread (computing)	Lingyuan Wang;Miaoqing Huang;Tarek A. El-Ghazawi	2011	2011 International Conference on High Performance Computing & Simulation	10.1109/HPCSim.2011.5999803	parallel processing;programming;computer architecture;parallel computing;kernel;real-time computing;multithreading;performance;network switch;resource allocation;computer science;operating system;concurrency control;instruction set;distributed computing;programming language;general-purpose computing on graphics processing units;coprocessor	HPC	-6.115997850350766	45.58976280063952	196852
4b46206d47c28f1cf8cf5d1fb125ce87d067fae5	parallelme: a parallel mobile engine to explore heterogeneity in mobile computing architectures		Following the evolution of desktops, mobile architectures are currently witnessing growth in processing power and complexity with the addition of different processing units like multi-core CPUs and GPUs. To facilitate programming and coordinating resource usage in these heterogeneous architectures, we present ParallelME, a ParallelMobile Engine designed to explore heterogeneity in mobile computing architectures. ParallelME provides a high-level library with a friendly programming language abstraction for developers, facilitating the programming of operations that can be translated into low-level parallel tasks. Additionally, these tasks are coordinated by a runtime framework, which is responsible for scheduling and controlling the execution on the low-level platform. ParallelME's purpose is to explore parallelism with the benefit of not changing the programming model, through a simple programming language abstraction that is similar to sequential programming. We performed a comparative analysis of execution time, memory and power consumption between ParallelME, OpenCL and RenderScript using an image processing application. ParallelME greatly increases application performance with reasonable memory and energy consumption.	mobile computing	Guilherme Andrade;Wilson de Carvalho;Renato Utsch;Pedro Caldeira;Mohammad-Reza Yousefi;Fabricio Ferracioli;Leonardo C. da Rocha;Michael Frank;Dorgival O. Guedes;Renato A. C. Ferreira	2016		10.1007/978-3-319-43659-3_33	parallel computing;real-time computing;reactive programming;computer science;theoretical computer science;operating system;database;distributed computing;programming language;parallel programming model	HCI	-6.151065440607414	44.854923090449844	196908
1310e423302e0067c3c181546cd00f02af2c770e	efficient parallel simulation of a sliding window protocol	parallel algorithm;tcp;parallel prefix;eprints newcastle university;open access;communication protocol;emeritus professor isi mitrani;dr stephen mcgough;sliding window;parallel simulation	Two parallel algorithms for simulating a sliding window communication protocol are described. The first exploits the parallel generation of interarrival, service and transfer times, whilst the second is based on parallel prefix computations. Both algorithms allow the processors to work largely asynchronously with each other. The efficiency of the two approaches is evaluated experimentally, using an implementation on a cluster of 12 processors connected by fast Ethernet. © 2002 Elsevier Science B.V. All rights reserved.	buffer overflow;central processing unit;communications protocol;computation;degree of parallelism;experiment;initial condition;network packet;parallel algorithm;parallel computing;performance evaluation;rare events;simulation;while	A. Stephen McGough;Isi Mitrani	2002	Perform. Eval.	10.1016/S0166-5316(02)00039-1	sliding window protocol;communications protocol;parallel computing;real-time computing;computer science;transmission control protocol;distributed computing;parallel algorithm;computer network	HPC	-10.384652102290929	45.5725517668584	197395
f4c9b9de5ce78c68b7f6aed63807e1d8614d8aff	thread-safe shmem extensions		This paper is intended to serve as a proposal for thread safety extensions to the OpenSHMEM specification and at the same time describes planned support for thread-safety for Cray SHMEM on Cray XE and XC systems.	shmem;thread safety	Monika ten Bruggencate;Duncan Roweth;Steve Oyanagi	2014		10.1007/978-3-319-05215-1_13	parallel computing;computer hardware;computer science;operating system	OS	-10.63120491711375	40.76209039529071	198265
9c264e5d5921b92623fdc8aebb5533f69ab5ac57	parallel discrete event simulation of space shuttle operations	parallel computer;object oriented design;discrete event simulation;process capability;object oriented;structure function;real time;object oriented programming	This paper describes the application of parallel simulation techniques to represent structured functional parallelism present within the Space Shuttle Operations Flow, utilizing the Synchronous Parallel Environment for Emulation and Discrete-Event Simulation (SPEEDES), an object-oriented multicomputing architecture. SPEEDES is a unified parallel simulation environment, which allocates events over multiple processors to get simulation speed up. Its optimistic processing capability minimizes simulation lag time behind wall clock time, or multiples of real-time. SPEEDES accommodates increases in processes complexity with additional parallel computing nodes to allow sharing of processing loads. This papers focuses on the whole process of translating a model of Space Shuttle Operations Flow represented in a process-driven approach to object oriented design, verification, validation, and implementation.	best practice;central processing unit;component-based software engineering;emulator;legacy system;open-source software;parallel computing;real-time clock;simulation;unified modeling language	José A. Sepúlveda;Luis Rabelo;Mario Marin;Amith Paruchuri;Amit Wasadikar;Karthik Nayaranan	2004	Proceedings of the 2004 Winter Simulation Conference, 2004.		real-time computing;simulation;computer science;distributed computing;programming language;object-oriented programming	HPC	-10.772590618223527	40.27256758289602	198510
2df3b2ba0d2ea7249b3da7852c02e8f1b9ab99ab	m08 - programming using rapidmind on the cell be	single system image;data parallel;dynamic compilation;high performance computer;scientific computing;scientific applications;overlapping communication and computation;parallel programs;performance modeling;memory model	The multi-core Cell BE architecture has the potential to deliver high performance computation to many applications. However, it requires parallel programming on several levels and uses a memory model that requires explicit scheduling of memory transfers. The RapidMind development platform enables access to the power of the Cell BE by providing a simple data-parallel model of execution. It combines a dynamic compiler with a runtime streaming execution manager, and provides a single system image for computations running on multiple cores. The interface to this system is embedded inside ISO standard C++, where it can capture computations specified through an innovative metaprogrammed interface. This tutorial will introduce and demonstrate the use of the RapidMind platform on the Cell BE through a number of examples drawn from visualization and scientific computation.	c++;cell (microprocessor);compiler;computation;computational science;dynamic compilation;embedded system;multi-core processor;parallel computing;scheduling (computing);single system image	Michael D. McCool;Bruce D'Amora	2006		10.1145/1188455.1188686	memory model;computer architecture;parallel computing;dynamic compilation;computer science;theoretical computer science;operating system;programming language	HPC	-10.914176085152926	40.67458392789087	198559
a6de556cedf5ced6880096660af6f864f5ecc08d	self-scaling stream processing: a bio-inspired approach to resource allocation through dynamic task replication	load balancing and task assignment;kernel;36 processor system;multiprocessor systems;resource allocation;bioinspired dynamic task replication algorithm;data stream;resource management;cellular architecture;adaptable architectures multiprocessor systems multiple data stream architectures multiprocessors load balancing and task assignment cellular architecture reconfigurable hardware;fpga;data mining;fpga self scaling stream processing resource allocation bioinspired dynamic task replication algorithm 36 processor system embedded multiprocessors systems;self scaling stream processing;cryptography;pipelines;embedded multiprocessors systems;load balance;task assignment;multiprocessing systems;field programmable gate arrays;stream processing;adaptable architectures;adaptive architecture;multiple data stream architectures multiprocessors;program processors;reconfigurable hardware;parallel processing;resource allocation field programmable gate arrays multiprocessing systems;throughput	In this article, we show how the use of a bio-inspired dynamic task replication algorithm, in the context of stream processing, can be used to significantly improve the performance of embedded programs. We also show that this programming methodology, which is not tied to a particular implementation, can also be used as an heuristic for task mapping in the context of embedded multiprocessors systems. The technique was applied to a 36-processor system implemented on a scalable mesh of FPGAS for two different case studies: for AES encryption, it resulted in a ten-fold speedup compared to a static implementation, while for MJPEG compression a throughput multiplication of 11 was obtained.	algorithm;british informatics olympiad;central processing unit;computer architecture;embedded system;encryption;heuristic;interrupt;load balancing (computing);multiprocessing;real-time operating system;real-time transcription;replication (computing);scalability;shortest path problem;software development process;speedup;stream processing;throughput;world-system	Pierre-André Mudry;Gianluca Tempesti	2009	2009 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2009.25	parallel processing;computer architecture;parallel computing;real-time computing;computer science;resource management;field-programmable gate array	Embedded	-9.034212409614202	43.934261988842856	199132
cf0fea57af625fc9c477bf8943561b0614278c66	a d&t roundtable: are single-chip multiprocessors in reach?	single-chip multiprocessors;first page	First Page of the Article			2001	IEEE Design & Test of Computers			Embedded	-9.475786185052257	42.03960683297168	199606
17a0ee6fc1e7230691864abe6d1dec49b36fabfd	implementation of stereo matching using a high level compiler for parallel computing acceleration	paper;heterogeneous systems;cuda;stereo matching;nvidia quadro fx 3700;nvidia;computer science;hmpp;opencl;application;nvidia geforce gtx 550 ti	Heterogeneous computing systems increase the performance of parallel computing in many domains of general purpose computing with CPU, GPU and other accelerators. With Hardware developments, the software developments like Compute Unified Device Architecture (CUDA) and Open Computing Language (OpenCL) try to offer a simple and visual framework for parallel computing. But it turns out to be more difficult than programming on CPU platform for optimization of performance. For one kind of parallel computing application, there are different configurations and parameters for various hardware platforms.  In this paper, we apply the Hybrid Multi-cores Parallel Programming (HMPP) to automatically generate tunable code for GPU platform and show the results of implementation of Stereo Matching with detailed comparison with C code version and manual CUDA version. The experimental results show that default and optimized HMPP have approximately the same performance and the better quality of disparity map compared with CUDA implementation.	binocular disparity;cuda;central processing unit;compiler;computer stereo vision;graphics processing unit;heterogeneous computing;high-level programming language;mathematical optimization;parallel computing	Jinglin Zhang;Jean-François Nezan;Jean-Gabriel Cousin;Erwan Raffin	2012		10.1145/2425836.2425892	computer architecture;application software;parallel computing;computer science;operating system;general-purpose computing on graphics processing units	HPC	-5.584767529104147	43.812597436373146	199619
52ac2f1620687a9070c6c3354c30343c3de80671	an improvement of openmp pipeline parallelism with the batchqueue algorithm	pipeline processing message passing multiprocessing programs;multiprocessing programs;openmp pipeline parallelism inter core communication multi core systems cache coherency;cache coherency;multi core systems;pipeline parallelism;message passing;single producer single consumer communication algorithm openmp pipeline parallelism batchqueue algorithm multicore programming sequential program openmp stream computing extension multiple producer multiple consumer queues;openmp;inter core communication;parallel processing pipelines throughput benchmark testing synchronization multicore processing scalability;pipeline processing	In the context of multicore programming, pipeline parallelism is a solution to easily transform a sequential program into a parallel one without requiring a whole rewriting of the code. The OpenMP stream-computing extension presented by Pop and Cohen proposes an extension of OpenMP to handle pipeline parallelism. However, their communication algorithm relies on Multiple-producer-Multiple-Consumer queues, while pipelined applications mostly deal with linear chains of communication, i.e., with only a single producer and a single consumer. To improve the performance of the OpenMP stream-extension, we propose to add a more specialized Single-Producer-Single-Consumer communication algorithm called Batch Queue and to select it for one-to-one communication. Our evaluation shows that Batch Queue is then able to improve the throughput up to a factor 2 on an 8-core machine both for example application and real applications. Our study shows therefore that using specialized and efficient communication algorithms can have a significant impact on the overall performance of pipelined applications.	algorithm;cache coherence;false sharing;job queue;multi-core processor;multiprocessing;one-to-one (data model);openmp;parallel computing;pipeline (computing);rewriting;stream processing;throughput	Thomas Preud'homme;Julien Sopena;Gaël Thomas;Bertil Folliot	2012	2012 IEEE 18th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2012.55	cache coherence;computer architecture;parallel computing;message passing;computer science;operating system;distributed computing;programming language	HPC	-7.214691517488966	44.98533774122799	199803
