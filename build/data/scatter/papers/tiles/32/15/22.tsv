id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
ac4343dba195f0b6953967baa1545e19d8f5a868	register renaming and scheduling for dynamic execution of predicated code	instruction level parallel;performance evaluation;processor scheduling;dynamic scheduling microarchitecture registers processor scheduling hardware modems process design availability runtime microprocessors;performance improvement;parallel architectures;program compilers;intel itanium processor pipeline processor performance compiler techniques predicated execution instruction level parallelism dynamic microarchitecture hardware optimizations register renaming scheduling;dynamic scheduling;program compilers processor scheduling parallel architectures performance evaluation	To achieve higher processor pel3rormance requires greater synergy between advanced hardware features and innovative compiler techniques. Recent advancement in compilation techniques for predicated execution has provided signifcant opportunily in exploiting instruction level parallelism. Howevel; little research has been done on how to eficiently execute predicated code in a dynamic microarchitecture. In this papel; we evaluate hardware optimizations for executing predicated code on a dynamically scheduled microarchitecture. We provide two novel ideas to improve the eficiency of executing predicated code. On a generic Intel Itaniirm processor pipeline model, we demonstrate that, with some microarchitecture enhancements, a dynamic execution processor can achieve about 16% performance improvement over an equivalent static execution processor:	compiler;instruction-level parallelism;microarchitecture;out-of-order execution;parallel computing;register renaming;schedule (computer science);synergy	Perry H. Wang;Hong Wang;Ralph-Michael Kling;Kalpana Ramakrishnan;John Paul Shen	2001		10.1109/HPCA.2001.903248	computer architecture;parallel computing;real-time computing;dynamic priority scheduling;computer science;very long instruction word;out-of-order execution;operating system;processor register	Arch	-6.570083359482019	50.28157513101463	136671
d680807467d5d3d7461909be59494bef5f76798e	poor scalability of parallel shared memory model: myth or reality?	distributed memory;shared memory;programming paradigm;coarse grained	Large CFD models require memory sizes larger than can be supported by today's single 'node' computers. Using the memory of more than one node can greatly complicate the creation of a well-performing program. We believe that preserving globally addressable memory beyond the boundary of a single node enables diversity of programming methods and provides flexibility essential for optimum algorithm development. We isolate the effects of algorithm and implementation by porting the same parallel CFD algorithm in three styles: fine-grain shared memory, coarse-grain shared memory, and coarse-grain distributed memory.#R##N##R##N#Despite some fundamental differences in programming implementation, bothdistributed memory and coarse-grain shared memory code provide very close parallel performance due to algorithmic similarities. At the same time fine-grain shared memory code, despite use of the same programming paradigm as coarse-grain shared memory program, falls far behind due to unavoidable parallel performance penalties caused by Amdahl's law and some other limitations.	scalability;shared memory	Mark Kremenetsky;Arthur Raefsky;Steven P. Reinhardt	2003		10.1007/3-540-44864-0_68	cuda pinned memory;uniform memory access;distributed shared memory;shared memory;memory model;interleaved memory;parallel computing;distributed memory;computer science;theoretical computer science;operating system;distributed computing;overlay;programming paradigm;extended memory;flat memory model;data diffusion machine;cache-only memory architecture;memory map;memory management	HPC	-10.829633868161455	48.19592662967732	137428
29865b96e2a155846ad2e0f58392bf7f7806203b	a dynamically reconfigurable operating system for manycore systems	manycore;os;resource management	In this paper, we suggest a partitioning OS, a novel locality-aware resource management scheme for manycore systems. It manages manycore resources as a hierarchical manner and allocates cores and memory as close each other as possible while dynamically considering both multiple applications' needs and ever-changing system status. Also we can dynamically (re)configure OS features for each partition according to the needs of the application on it. We show the effectiveness of the proposed method through a real-life application, ray tracing, and achieves a scalable speedup, 13.46 times at 16 cores.	locality of reference;manycore processor;multi-core processor;operating system;ray tracing (graphics);real life;reconfigurability;scalability;speedup	Chaeseok Im;Minkyu Jeong;Jae Don Lee;Seungwon Lee	2013		10.1145/2480362.2480665	computer architecture;parallel computing;real-time computing;computer science;resource management;operating system	HPC	-7.11692383536785	49.20634969719608	137569
6d100fc9aee1923c1988e747d3acffe8438a67e5	using scratchpad to exploit object locality in java	cache storage;storage management;cache size java object locality generational garbage collection cache memory scratchpad memory memory traffic reduction cache only configuration;garbage collection;object localization;java read write memory degradation bandwidth delay traffic control;cache storage storage management java;java	Performance of modern computers is tied closely to the effective use of cache because of the continually increasing speed discrepancy between processors and main memory. We demonstrate that generational garbage collection employed by a system with cache and scratchpad memory can take advantage of the locality of small short-lived objects in Java and reduce memory traffic by as much as 20% when compared to a cache-only configuration. Converting half of the cache to scratchpad can be more effective at reducing memory traffic than doubling or even quadrupling the size of the cache for several of the applications in SPECjvm98.	cpu cache;central processing unit;computer data storage;discrepancy function;garbage collection (computer science);inbound marketing;java;locality of reference;memory hierarchy;period-doubling bifurcation;scratchpad memory;systems design	Carl S. Lebsack;J. Morris Chang	2005	2005 International Conference on Computer Design	10.1109/ICCD.2005.111	bus sniffing;uniform memory access;manual memory management;shared memory;least frequently used;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;operating system;smart cache;garbage collection;programming language;mesi protocol;java;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	Arch	-11.790281222231183	50.65411388890337	137929
306eed124f3513e98f78ac294518a9167f52dd3b	architectural support for block transfers in a shared-memory multiprocessor	multiprocessor interconnection networks;architectural support;performance evaluation;clocks;operating systems computers shared memory systems memory architecture performance evaluation;buffer storage;operating systems hardware computer architecture buffer storage registers multiprocessor interconnection networks access protocols clocks process control;base architecture;computer architecture;performance improvement;shared memory systems;operating system;hardware support;initialization code;registers;memory architecture;process control;access protocols;block transfers;hector multiprocessor;memory access behavior;operating system performance improvement architectural support block transfers shared memory multiprocessor hardware support hector multiprocessor base architecture initialization code memory access behavior;operating systems computers;operating systems;hardware;shared memory multiprocessor	This paper examines how the performance of a shared-memory multiprocessor can be improved by including hardware support for block transfers. A system similar to the Hector multiprocessor developed at the University of Toronto is used as a base architecture. It is shown that such hardware support can improve the performance of initialization code by as much as 50%, but that the amount of improvement depends on the memory access behavior of the program and the way in which the operating system issues block transfer re-	hector;multiprocessing;operating system;shared memory	Steven J. E. Wilton;Zvonko G. Vranesic	1993		10.1109/SPDP.1993.395551	computer architecture;parallel computing;real-time computing;computer science;operating system;process control;processor register;multiprocessor scheduling	Arch	-10.905554882126546	48.830840849312075	137936
f218d2aa67a14608651b91171a30ea69d1cf72b5	capping speculative traces to improve performance in simultaneous multi-threading cpus	smt pipeline capping speculative traces simultaneous multithreading cpu data path components instruction level parallelism smt environment multithreading system clock cycle level issue queue iq;multi threading;performance evaluation;superscalar;pipelines instruction sets clocks message systems throughput load modeling;simultaneous multi threading superscalar speculative execution;simultaneous multi threading;performance evaluation microprocessor chips multi threading;speculative execution;microprocessor chips	Simultaneous Multi-Threading (SMT) improves the overall performance of superscalar CPUs by allowing concurrent execution of multiple independent threads with sharing of key data path components in order to better utilize the resources. Speculative executions help modern processors to exploit more Instruction-Level Parallelism. However, the performance penalty from a miss speculation is much more prominent in an SMT environment than a traditional multi-threading system due to the resulted waste of shared resources at clock-cycle level, versus thread level. In this paper, we show that instructions fetched due to incorrect prediction can be more than 30% of all instructions, which results in a huge waste of resources that could have been better used by other non-speculative threads. To minimize this waste of resources, a technique is proposed in this paper to control the amount of speculative instructions dispatched into Issue Queue (IQ), the most critically shared resource in the SMT pipeline. Simulation result shows the proposed technique can reduce the waste of resource due to miss-speculated traces by 38% and improve overall throughput by up to 17% in IPC.	central processing unit;clock rate;digital footprint;frequency capping;instruction-level parallelism;inter-process communication;parallel computing;pipeline (computing);simulation;simultaneous multithreading;speculative execution;superscalar processor;thread (computing);throughput;tracing (software)	Yilin Zhang;Wei-Ming Lin	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.27	computer architecture;parallel computing;real-time computing;multithreading;computer science;superscalar;operating system;speculative multithreading;speculative execution	Arch	-6.614481310824649	52.29856239214338	138124
3a2ff791d4cfee09da98fbfff8283929fb0f5733	storage class memory and databases: opportunities and challenges		Storage Class Memory (SCM) is emerging as a viable solution to lift DRAM’s scalability limits, both in capacity and energy consumption. Indeed, SCM combines the economic characteristics, non-volatility, anddensity of traditional storage media with the low latency and byteaddressability of DRAM. In this paper we survey research works on how SCM can be leveraged in databases and explore different solutions ranging from using SCM as disk replacement, to single-level storage architectures, where SCM is used as universal memory (i.e., as memory and storage at the same time), together with the challenges that stem from these opportunities. Finally, we synthesize our findings into recommendations on how to exploit the full potential of SCM in next-generation database architectures.	database;dynamic random-access memory;multi-level cell;non-volatile memory;read-only memory;scalability;universal memory;volatility	Ismail Oukid;Robert Kettler;Thomas Willhalm	2017	it - Information Technology	10.1515/itit-2016-0052	computer science;memory management;parallel computing;database;data architecture;uniform memory access;non-volatile random-access memory;virtual memory;shared memory;memory protection	DB	-11.673392605238808	52.13507684948274	138919
47eac1b840c634dc00623cd69381ad3f310ad2fd	a template library to integrate thread scheduling and locality management for numa multiprocessors	costly remote memory access;irregular memory access pattern;locality management;data space;automatic data placement;shared data structure;data layout;numa multiprocessors;thread mapping;non-uniform memory architecture;template library;thread scheduling	Many multicore multiprocessors have a non-uniform memory architecture (NUMA), and for good performance, data and computations must be partitioned so that (ideally) all threads execute on the processor that holds their data. However, many multithreaded applications show heavy use of shared data structures that are accessed by all threads of the application. Automatic data placement and thread scheduling for these applications is (still) difficult.  We present a template library for shared data structures that allows a programmer to express both the data layout (how the data space is partitioned) as well as thread mapping and scheduling (when and where a thread is executed). The template library supports programmers in dividing computations and data for reducing the percentage of costly remote memory accesses in NUMA multicore multiprocessors. Initial experience with ferret, a program with irregular memory access patterns from the PARSEC benchmark suite, shows that this approach can reduce the number of remote accesses from 42% to 10% and results in a performance improvement of 3% without overwhelming the programmer.	benchmark (computing);computation;data structure;dataspaces;locality of reference;multi-core processor;multithreading (computer architecture);non-uniform memory access;parsec benchmark suite;programmer;scheduling (computing);thread (computing)	Zoltan Majo;Thomas R. Gross	2012			parallel computing;real-time computing;computer science;operating system;cache-only memory architecture	Arch	-8.296125843552778	49.17607343307892	139441
19dd0283e3e20d799d294b542ebe842f7842d09e	dynamic binary code translation for data prefetch optimization	spec2000 benchmarks dynamic binary code translation data prefetch optimization program execution compilers machine dependent code optimization software vendors software production dynamic code modification;software vendors;software;optimisation;history;code optimization;machine dependent code optimization;software production;storage management;dp industry;prefetching;program interpreters;binary codes;dynamic binary code translation;spec2000 benchmarks;dynamic code modification;program execution;compilers;memory access;registers;data prefetching;data prefetch optimization;storage management binary codes dp industry optimisation program interpreters;optimization;prefetching optimization binary codes software history hardware registers;dynamic optimization;hardware	Recently, CPUs with an identical ISA tend to have different microarchitectures, different computation resources, and special instructions. To achieve efficient program execution on such hardware, compilers have machine-dependent code optimization. However, software vendors cannot adopt this optimization for software production, since the software would be widely distributed and therefore it must be executable on any machine with the same ISA. On the other hand, there is a significant gap between processorpsilas operational speed and memory access speed, and currently the gap is increasing. In this paper, we introduce several special prefetch instructions that are suited for memory access patterns that frequently appear in program execution. However, such special instructions are utilized only by compilerpsilas machine-dependent code optimization, and therefore software vendors do not utilize such instructions. To increase opportunities for effectively exploiting the instructions for optimization, we propose dynamic optimization techniques that consist of dynamic code modification and analysis methods of memory references. We evaluate the techniques by using SPEC2000 benchmarks.	benchmark (computing);binary code;central processing unit;compiler;computation;dynamic programming;executable;hdos;machine-dependent software;mathematical optimization;microarchitecture;program optimization;simulation	Tomoaki Ukezono;Kiyofumi Tanaka	2008	2008 13th Asia-Pacific Computer Systems Architecture Conference	10.1109/APCSAC.2008.4625474	binary code;computer architecture;compiler;parallel computing;real-time computing;computer science;operating system;program optimization;processor register;programming language	HPC	-6.213673910353891	50.11958609481687	139575
18423ebc96cb3878dc684730e25ac7f63124b75e	dycache: dynamic multi-grain cache management for irregular memory accesses on gpu		GPU utilizes the wide cache-line (128B) on-chip cache to provide high bandwidth and efficient memory accesses for applications with regularly-organized data structures. However, emerging applications exhibit a lot of irregular control flows and memory access patterns. Irregular memory accesses generate many fine-grain memory accesses to L1 data cache. This mismatching between fine-grain data accesses and the coarse-grain cache design makes the on-chip memory space more constrained and as a result, the frequency of cache line replacement increases and L1 data cache is utilized inefficiently. Fine-grain cache management is proposed to provide efficient cache management to improve the efficiency of data array utilization. Unlike other static fine-grain cache managements, we propose a dynamic multi-grain cache management, called DyCache, to resolve the inefficient use of L1 data cache. Through monitoring the memory access pattern of applications, DyCache can dynamically alter the cache management granularity in order to improve the performance of GPU for applications with irregular memory accesses while not impact the performance for regular applications. Our experiment demonstrates that DyCache can achieve a 40% geometric mean improvement on IPC for applications with irregular memory accesses against the baseline cache (128B), while for applications with regular memory accesses, DyCache does not degrade the performance.	baseline (configuration management);cpu cache;dspace;data structure;graphics processing unit;memory access pattern	Hui Guo;Libo Huang;Yashuai L&#x00FC;;Sheng Ma;Zhiying Wang	2018	IEEE Access	10.1109/ACCESS.2018.2818193	throughput;memory management;parallel computing;distributed computing;cpu cache;cache;granularity;instruction set;data structure;computer science;array data type	HPC	-10.819680832915957	52.90695369414871	139751
bddccb6adf356e9487022beec17cb98abafa09b9	an efficient cache design for scalable glueless shared-memory multiprocessors	cache;point to point;memory wall;chip;large scale;low latency;cache coherence protocol;critical path;cache coherence;directory structure;shared memory multiprocessor;glueless shared memory multiprocessors	Traditionally, cache coherence in large-scale shared-memory multiprocessors has been ensured by means of a distributed directory structure stored in main memory. In this way, the access to main memory to recover the sharing status of the block is generally put in the critical path of every cache miss, increasing its latency. Considering the ever-increasing distance to memory, these cache coherence protocols are far from being optimal from the perspective of performance. On the other hand, shared-memory multiprocessors formed by connecting chips that integrate the processor, caches, coherence logic, switch and memory controller through a low-cost, low-latency point-to-point network (glueless shared-memory multiprocessors) are a reality.In this work, we propose a novel design for the L2 cache level, at which coherence has to be maintained, aimed at being used in glueless shared-memory multiprocessors. Our proposal splits the cache structure into two different parts: one for storing data and directory information for the blocks requested by the local processor, and another one for storing only directory information for blocks accessed by remote processors. Using this cache scheme we remove the directory from main memory. Besides saving memory space, our proposal brings very significant reductions in terms of latency of the cache misses (speed-ups of 3.0 on average), which translate into reductions in applications' execution time of 31% on average.	cpu cache;cache coherence;central processing unit;computer data storage;critical path method;dspace;directory (computing);memory controller;point-to-point protocol;run time (program lifecycle phase);scalability;shared memory	Alberto Ros;Manuel E. Acacio;José M. García	2006		10.1145/1128022.1128065	bus sniffing;uniform memory access;shared memory;pipeline burst cache;cache coherence;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	Arch	-10.048585680100109	52.41380523803148	140357
e68d81716168c599d2c170f27f8fd6fe4dd96e19	analyzing data locality in gpu kernels using memory footprint analysis		Abstract Memory footprint is a metric for quantifying data reuse in memory trace. It can also be used to approximate cache performance, especially in shared cache systems. Memory footprint is acquired through memory footprint analysis (FPA). However, its main limitation is that, for a memory trace of n accesses, the all-window FPA algorithm requires O ( n 3 ) time. Therefore, in this paper, we propose an analytical algorithm for FPA, whereby the average footprints are calculated in O ( n 2 ). The proposed algorithm can also be employed for window distribution analysis. Moreover, we propose a framework to enable the application of FPA to GPU kernels and model the performance of L1 cache memories. The results of experimental evaluations indicate that our proposed framework functions 1.55X slower than the Xiang’s formula, as a fast average FPA method, while it can also be utilized for window distribution analysis. In the context of FPA-based cache performance estimation, the experimental results indicate a fair correlation between the estimated L1 miss rates and those of the native GPU executions. On average, the proposed framework has 23.8% error in the estimation of L1 cache miss rates. Further, our algorithm runs 125X slower than the reuse distance analysis (RDA) when analyzing a single kernel. However, the proposed method outperforms RDA in modeling shared caches and multiple kernel executions in GPUs.		Mohsen Kiani;Amir Rajabzadeh	2019	Simulation Modelling Practice and Theory	10.1016/j.simpat.2018.12.003	computer science;memory footprint;real-time computing;parallel computing;kernel (linear algebra);cache;locality;cpu cache;reuse;shared memory	SE	-6.810371525147517	49.79560298351195	140422
6517752e486a57b17ce27ce2269045b5c995e5b5	revisiting clustered microarchitecture for future superscalar cores: a case for wide issue clusters	superscalar core;steering policy;clustered microarchitecture;instruction level parallelism	During the past 10 years, the clock frequency of high-end superscalar processors has not increased. Performance keeps growing mainly by integrating more cores on the same chip and by introducing new instruction set extensions. However, this benefits only some applications and requires rewriting and/or recompiling these applications. A more general way to accelerate applications is to increase the IPC, the number of instructions executed per cycle. Although the focus of academic microarchitecture research moved away from IPC techniques, the IPC of commercial processors was continuously improved during these years.  We argue that some of the benefits of technology scaling should be used to raise the IPC of future superscalar cores further. Starting from microarchitecture parameters similar to recent commercial high-end cores, we show that an effective way to increase the IPC is to allow the out-of-order engine to issue more micro-ops per cycle. But this must be done without impacting the clock cycle. We propose combining two techniques: clustering and register write specialization. Past research on clustered microarchitectures focused on narrow issue clusters, as the emphasis at that time was on allowing high clock frequencies.  Instead, in this study, we consider wide issue clusters, with the goal of increasing the IPC under a constant clock frequency. We show that on a wide issue dual cluster, a very simple steering policy that sends 64 consecutive instructions to the same cluster, the next 64 instructions to the other cluster, and so forth, permits tolerating an intercluster delay of three cycles. We also propose a method for decreasing the energy cost of sending results from one cluster to the other cluster.	central processing unit;clock rate;clock signal;cluster analysis;image scaling;inter-process communication;micro-operation;microarchitecture;nehalem (microarchitecture);partial template specialization;rewriting;superscalar processor	Pierre Michaud;Andrea Mondelli;André Seznec	2015	TACO	10.1145/2800787	computer architecture;parallel computing;real-time computing;computer science;operating system;instruction-level parallelism	Arch	-7.620100963482108	52.82475587571305	140529
a01ff4dabe28dbb479d1241162e994db31543445	on the power of (even a little) flexibility in dynamic resource allocation		We study the role of partial flexibility in large-scale dynamic resource allocation problems, in which multiple types of processing resources are used to serve multiple types of incoming demands that arrive stochastically over time. Partial flexibility refers to scenarios where (a) only a small fraction of the total processing resources is flexible, or (b) each resource is capable of serving only a small number of demand types. Two main running themes are architecture and information: the former asks how a flexible system should be structured to fully harness the benefits of flexibility, and the latter looks into how information, across the system or from the future, may critically influence performance. Overall, our results suggest that, with the right architecture, information, and decision policies, large-scale systems with partial flexibility can often vastly outperform their inflexible counterparts in terms of delay and capacity, and sometimes be almost as good as fully flexible systems. Our main findings are: 1. Flexible architectures. We show that, just like in fully flexible systems, a large capacity region and a small delay can be achieved even with very limited flexibility, where each resource is capable of serving only a vanishingly small fraction of all demand types. However, the system architecture and scheduling policy need to be chosen more carefully compared to the case of a fully flexible system. (Chapters 3 and 4.) 2. Future information in flexible systems. We show that delay performance in a partially flexible system can be significantly improved by having access to predictive information about future inputs. When future information is sufficient, we provide an optimal scheduling policy under which delay stays bounded in heavy-traffic. Conversely, we show that as soon as future information becomes insufficient, delay diverges to infinity under any policy. (Chapters 5 and 6.) 3. Decentralized partial pooling. For the family of Partial Pooling flexible architectures, first proposed and analyzed by [84], we demonstrate that a decentralized scheduling policy can achieve the same heavy-traffic delay scaling as an optimal centralized longest-queue-first policy used in prior work. This demonstrates that asymptotically optimal performance can be achieved in a partially flexible system with little information sharing. Our finding, which makes use of a new technical result concerning the limiting distribution of an M/M/1 queue fed by a superposition of input processes, strengthens the result of [84], and provides a simpler line of analysis. (Chapter 7.) Thesis Supervisor: John N. Tsitsiklis Title: Clarence J. Lebel Professor of Electrical Engineering	asymptotically optimal algorithm;centralized computing;computer architecture;dual total correlation;electrical engineering;emoticon;image scaling;optimal design;quantum superposition;scheduling (computing);systems architecture;theme (computing)	Kuang Xu	2014				HPC	-9.549508654911316	53.172239501212054	140956
32c0e9fd4723591562a140dac80be6567073b357	configurable range memory for effective data reuse on programmable accelerators	on chip memory architectures and management;scratchpads;array mapping;compiler controlled memories;coarse grained reconfigurable architecture	While programmable accelerators such as application-specific processors and reconfigurable architectures can dramatically speed up compute-intensive kernels of an application, application performance can still be severely limited by the communication between processors. To minimize the communication overhead, a shared memory such as a scratchpad memory may be employed between the main processor and the accelerator coprocessor. However, this setup poses a significant challenge to the main processor, which now must manage data on the scratchpad explicitly, resulting in superfluous data copying due to the inflexibility of a scratchpad. In this article, we present an enhancement of a scratchpad, Configurable Range Memory (CRM), whose address range can be reprogrammed to minimize unnecessary data copying between processors and therefore promote data reuse on the accelerator, and also present a software management algorithm for the CRM. Our experimental results involving detailed simulation of full multimedia applications demonstrate that our CRM architecture can reduce the communication overhead quite effectively, reducing the kernel execution time by up to 28% and the application runtime by up to 12.8%, in addition to considerable system energy reduction, compared to the conventional architecture based on a scratchpad.	algorithm;central processing unit;coprocessor;overhead (computing);reconfigurable computing;run time (program lifecycle phase);scratchpad memory;shared memory;simulation;software project management;speedup	Jongeun Lee;Seongseok Seo;Jong Kyung Paek;Kiyoung Choi	2014	ACM Trans. Design Autom. Electr. Syst.	10.1145/2566662	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	-5.057375494759789	50.14114637104177	141095
757a5426c52dc4f8fe8e8121d5909f0763a80819	rrb vector: a practical general purpose immutable sequence	sequences;immutable;arrays;trees;vectors;data structures;relaxed radix balanced;radix balanced	State-of-the-art immutable collections have wildly differing performance characteristics across their operations, often forcing programmers to choose different collection implementations for each task. Thus, changes to the program can invalidate the choice of collections, making code evolution costly. It would be desirable to have a collection that performs well for a broad range of operations. To this end, we present the RRB-Vector, an immutable sequence collection that offers good performance across a large number of sequential and parallel operations. The underlying innovations are: (1) the Relaxed-Radix-Balanced (RRB) tree structure, which allows efficient structural reorganization, and (2) an optimization that exploits spatio-temporal locality on the RRB data structure in order to offset the cost of traversing the tree. In our benchmarks, the RRB-Vector speedup for parallel operations is lower bounded by 7x when executing on 4 CPUs of 8 cores each. The performance for discrete operations, such as appending on either end, or updating and removing elements, is consistently good and compares favorably to the most important immutable sequence collections in the literature and in use today. The memory footprint of RRB-Vector is on par with arrays and an order of magnitude less than competing collections.	amortized analysis;benchmark (computing);central processing unit;data structure;image scaling;immutable object;international conference on functional programming;locality of reference;mathematical optimization;memory footprint;programmer;random access;scala;speedup;standard library;time complexity;tree structure	Nicolas Stucki;Tiark Rompf;Vlad Ureche;Phil Bagwell	2015		10.1145/2784731.2784739	parallel computing;data structure;computer science;theoretical computer science;sequence;programming language;algorithm	PL	-7.396812393751159	47.6326626534425	141168
05f1bcc0067eeb6a4cc2c4f1696a69ad21f49f7f	architectural implications for simd processors in the wireless communication domain	simd;protocols;kernel;performance evaluation;lte;mrf;telecommunication computing;long term evolution;matrix algebra;computer architecture kernel mimo registers performance gain hardware vectors;computer architecture;shuffle;parallel architectures;vectors;registers;performance gain;multiprocessing systems;mimo;telecommunication computing file organisation long term evolution matrix algebra multiprocessing systems parallel architectures performance evaluation protocols;matrix register file wireless communication domain simd processor architecture performance improvement single instruction multiple data architectures long term evolution protocol lte protocol cycle accurate simulator concurrent execution scalar processing parallel processing bidirectional shuffle unit performance gain;shuffle simd lte mrf;hardware;file organisation	To further improve the performance of SIMD (Single Instruction Multiple Data) architectures, which are widely used in the wireless communication domain. The main components of Long Term Evolution (LTE) protocol are analyzed. Performance investigation is taken on a cycle-accurate simulator, featuring the main characteristics of existing SIMD architectures. Based on the investigation, three insightful architectural implications, including the concurrent execution of scalar and parallel processing, multiple sub-matrixes accessible matrix register file, and bidirectional shuffle unit are proposed. The experiment result shows that an average of 30% performance gain can be achieved by the SIMD architecture enhanced with the proposed implications. The hardware cost of these implications is also discussed.	compaq lte;computer architecture simulator;ibm notes;parallel computing;register file;simd;simulation	Yaohua Wang;Kai Zhang;Jianghua Wan;Sheng Liu;Xi Ning;Shuming Chen	2012	2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems	10.1109/HPCC.2012.176	communications protocol;computer architecture;parallel computing;kernel;real-time computing;materials recovery facility;simd;computer science;lte advanced;operating system;distributed computing;processor register;computer network;mimo	HPC	-7.3176250567633705	47.664344687496715	141245
7222272c05fe7d4694a9d3ca9cced82080e7dda1	vcache: visualization applet for processor caches	size structure;computer organization;memory access;visualization;difference set	Caches can be one of the more difficult subjects to teach in computer organization introductory courses. To aid this, we introduce an applet, VCache, which can visualize cache interactions on various caches with different sizes, structures and policies. The visualization is achieved by animating and color-highlighting the activity and showing the contents of the cache. Using this, students can learn the principles of the cache and the performance implications, by experimenting different sets of memory accesses on different caches.	applet;cpu cache;experiment;interaction;microarchitecture	Berkin Ilbeyi;John A. Nestor	2010		10.1145/1822090.1822180	cache coherence;parallel computing;real-time computing;visualization;false sharing;microarchitecture;computer science;theoretical computer science;operating system;cache algorithms;difference set	Visualization	-11.610736376873207	48.075153205820335	141524
763cdd0d3b575dbc7b4fecced165bf36d1384a1f	cpu2006 working set size	data replication;cmp caches;performance modeling and projection;stack simulation	SPEC CPU2000 had a target memory footprint of 200 MB for the benchmarks [1], to enable the suite to run on machines with 256 MB of memory. Six years have elapsed since the release of that suite, and in that time memory sizes have increased significantly, so the memory requirements for the recently released CPU2006 reflect this. CPU2006 has been targeted to have a benchmark memory footprint of about 900MB, allowing the suite to run on machines with 1GB of memory.	benchmark (computing);gigabyte;memory footprint;requirement;spec#;working set size	Darryl Gove	2007	SIGARCH Computer Architecture News	10.1145/1241601.1241619	uniform memory access;shared memory;interleaved memory;computer architecture;parallel computing;real-time computing;computer science;operating system;conventional memory;flat memory model;computing with memory;replication	Arch	-11.07585074459251	51.089462930174	141645
fa829792d6b3864b7254d196550ab411dc0a67ce	a measurement study of memory transaction characteristics on a powerpc-based macintosh	storage allocation;cache storage;apple computers;power measurement timing power cables time measurement system buses logic random access memory buffer storage current measurement read write memory;memory access;cache storage apple computers memory architecture storage allocation;address trace;memory architecture;memory transactions;memory hierarchy;workloads measurement study memory transaction characteristics powerpc based macintosh cache organizations memory hierarchy designs time stamps memory transaction behavior time stamped address traces	Address traces are acquired typically in order to evaluate different cache organizations and memory hierarchy designs. Address traces have other uses, however. For example, traces can provide information about patterns of different types of memory accesses (e.g. reads versus writes). If the trace data includes time stamps, bursts of accesses can be detected and analyzed. This paper describes a series of studies pe$onned on a PowerPC based Macintosh computer to examine memory transaction behavior using time stamped address traces. The measurement setups and workloads are described, along with a description of the results obtained.	burst error;memory hierarchy;powerpc;software transactional memory;tracing (software)	Thomas L. Adams	1996		10.1109/CMPCON.1996.501755	memory address;uniform memory access;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;memory refresh;computer hardware;computer science;physical address;virtual memory;computer data storage;computer memory;conventional memory;extended memory;flat memory model;registered memory;cache pollution;cache-only memory architecture;memory map;non-uniform memory access;memory management	Metrics	-7.960323859364977	52.186874336678926	141962
7a78ef5ac1d29201643d47d472cb12b963a2cb9c	adaptive and application dependent runtime guided hardware prefetcher reconfiguration on the ibm power7	bm power7 microprocessor;high performance computing;hardware data prefetcher engines;hpc;conference report	Hardware data prefetcher engines have been extensively used to reduce the impact of memory latency. However, microprocessors’ hardware prefetcher engines do not include any automatic hardware control able to dynamically tune their operation. This lacking architectural feature causes systems to operate with prefetchers in a fixed configuration, which in many cases harms performance and energy consumption. In this paper, a piece of software that solves the discussed problem in the context of the IBM POWER7 microprocessor is presented. The proposed solution involves using the runtime software as a bridge that is able to characterize user applications’ workload and dynamically reconfigure the prefetcher engine. The proposed mechanisms has been deployed over OmpSs, a state-of-the-art task-based programming model. The paper shows significant performance improvements over a representative set of microbenchmarks and High Performance Computing (HPC) applications.	cas latency;microprocessor;prefetcher;programming model	David Prat;Cristobal Ortega;Marc Casas;Miquel Moretó;Mateo Valero	2015	CoRR		supercomputer;parallel computing;computer science;operating system;distributed computing;operations research;algorithm	HPC	-8.404354145908794	49.52668200709791	142030
52ab3590beab798d76641ef2c36bc6dba4739858	disjoint out-of-order execution processor	speculative multithreading;latency tolerant processors;checkpoint processors;continual flow pipelines	High-performance superscalar architectures used to exploit instruction level parallelism in single-thread applications have become too complex and power hungry for the multicore processors era. We propose a new architecture that uses multiple small latency-tolerant out-of-order cores to improve single-thread performance. Improving single-thread performance with multiple small out-of-order cores allows designers to place more of these cores on the same die. Consequently, emerging highly parallel applications can take full advantage of the multicore parallel hardware without sacrificing performance of inherently serial and hard to parallelize applications. Our architecture combines speculative multithreading (SpMT) with checkpoint recovery and continual flow pipeline architectures. It splits single-thread program execution into disjoint control and data threads that execute concurrently on multiple cooperating small and latency-tolerant out-of-order cores. Hence we call this style of execution Disjoint Out-of-Order Execution (DOE). DOE uses latency tolerance to overcome performance issues of SpMT caused by interthread data dependences. To evaluate this architecture, we have developed a microarchitecture performance model of DOE based on PTLSim, a simulation infrastructure of the x86 instruction set architecture. We evaluate the potential performance of DOE processor architecture using a simple heuristic to fork control independent threads in hardware at the target addresses of future procedure return instructions. Using applications from SpecInt 2000, we study DOE under ideal as well as realistic architectural constraints. We discuss the performance impact of key DOE architecture and application variables such as number of cores, interthread data dependences, intercore data communication delay, buffers capacity, and branch mispredictions. Without any DOE specific compiler optimizations, our results show that DOE outperforms conventional SpMT architectures by 15%, on average. We also show that DOE with four small cores can perform on average equally well to a large superscalar core, consuming about the same power. Most importantly, DOE improves throughput performance by a significant amount over a large superscalar core, up to 2.5 times, when running multitasking applications.	branch predictor;central processing unit;computer multitasking;die (integrated circuit);heuristic;instruction-level parallelism;microarchitecture;multi-core processor;multithreading (computer architecture);optimizing compiler;out-of-order execution;parallel computing;return statement;specint;simulation;speculative execution;speculative multithreading;superscalar processor;thread (computing);throughput;transaction processing system;x86	Mageda Sharafeddine;Komal Jothi;Haitham Akkary	2012	TACO	10.1145/2355585.2355592	computer architecture;parallel computing;real-time computing;computer science;operating system;speculative multithreading	Arch	-8.196956492214294	51.16284848270172	142147
a96eff9127e34c2baf39c0cba9ff9e617226b572	a run-time task migration scheme for an adjustable issue-slots multi-core processor	vliw processor;softcore;task migration;interrupts;multi core	In this paper, we present a run-time task migration scheme for an adjustable/reconfigurable issue-slots very long instruction word (VLIW) multi-core processor. The processor has four 2-issue ρ-VEX VLIW cores that can be merged together to form larger issue-width cores. With a task migration scheme, a code running on a core can be shifted to a larger or a smaller issue-width core for increasing the performance or reducing the power consumption of the whole system, respectively. All the cores can be utilized in an efficient manner, as a core needed for a specific job can be freed at run-time by shifting its running code to another core. The task migration scheme is realized with the implementation of interrupts on the ρ-VEX cores. The design is implemented in a Xilinx Virtex-6 FPGA. With different benchmarks, we demonstrate that migrating a task running on a smaller issue-width core to a larger issuewidth core at run-time results in a considerable performance gain (up to 3.6x). Similarly, gating off one, two, three, or four cores can reduce the dynamic power consumption of the whole system by 24%, 42%, 61%, or 81%, respectively.	benchmark (computing);field-programmable gate array;interrupt;marginal model;multi-core processor;opcode;scheduling (computing)	Fakhar Anjam;Quan Kong;Roel Seedorf;Stephan Wong	2012		10.1007/978-3-642-28365-9_9	multi-core processor;embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;interrupt	HPC	-5.471478160564051	52.83200073837501	142313
0cbd5abb6f784c1198d24164a92d11cc14382b63	a multithreaded communication engine for multicore architectures	libraries;message passing multithreaded communication engine multicore architectures multithreading;microprocessors;engines multicore processing multithreading yarn libraries message passing software performance computer architecture microprocessors network interfaces;multi threading;yarn;multicore architectures;newmadeleine;overlap;software performance;computer architecture;network interfaces;engines;critical path;multicore processing;message passing;multithreaded communication engine;multicore architecture;thread;multi threading message passing;communication;pioman;parallel applications;multithreading	The current trend in clusters leads towards an increase of the number of cores per node. As a result, an increasing number of parallel applications is mixing message passing and multithreading as an attempt to better match the underlying architecture's structure. This naturally raises the problem of designing efficient, multithreaded implementations of MPL In this paper, we present the design of a multithreaded communication engine able to exploit idle cores to speed up communications in two ways: it can move CPU- intensive operations out of the critical path (e.g. PIO transfers off load), and is able to let rendezvous transfers progress asynchronously. We have implemented these methods in the PM2 software suite, evaluated their behavior in typical cases, and we have observed good performance results in overlapping communication and computation.	algorithm;ambiguous name resolution;benchmark (computing);central processing unit;communication engine;computation;critical path method;event-driven programming;idle scan;mpich;mathematical optimization;message passing;multi-core processor;multiphoton lithography;multithreading (computer architecture);network packet;pm2;programmed input/output;scheduling (computing);semiconductor intellectual property core;software suite;thread (computing)	François Trahay;Elisabeth Brunet;Alexandre Denis;Raymond Namyst	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536139	computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;distributed computing	Arch	-8.204442174996029	50.7706054283181	142447
b8048c62a985dcb0f7827db470f148e4db58535f	mamba: a scalable communication centric multi-threaded processor architecture	cache storage;multi threading;queueing theory;synchronisation;computer architecture;registers;message systems;scalable communication multithreaded processor architecture mamba multicore system on chip communication optimization lightweight communication interthread communication memory access cache storage fine grained synchronization primitive fpga fifo bit based implementation;instruction sets message systems registers benchmark testing computer architecture field programmable gate arrays;multiprocessing systems;field programmable gate arrays;synchronisation cache storage field programmable gate arrays multiprocessing systems multi threading parallel memories queueing theory;parallel memories;benchmark testing;instruction sets	In this paper we describe Mamba, an architecture designed for multi-core systems. Mamba has two major aims: (i) make on-chip communication explicit to the programmer so they can optimize for it and (ii) support many threads and supply very lightweight communication and synchronization primitives for them. These aims are based on the observations that: (i) as feature sizes shrink, on-chip communication becomes relatively more expensive than computation and (ii) as we go increasingly multi-core we need highly scalable approaches to inter-thread communication and synchronization. We employ a network of processors where a given memory access will always go to the same cache, removing the need for a coherence protocol and allowing the program explicit control over all communication. A presence bit associated with each word provides a very lightweight, finegrained synchronization primitive. We demonstrate an FPGA implementation with micro-benchmarks of standard spinlock and FIFO implementations and show that presence bit based implementations provide more efficient locking, and lower latency FIFO communications compared to a conventional shared memory implementation whilst also requiring fewer memory accesses. We also show that Mamba performance is insensitive to total thread count, allowing the use of as many threads as desired.	cache coherence;central processing unit;computation;fifo (computing and electronics);field-programmable gate array;goto;inter-process communication;lock (computer science);microarchitecture;multi-core processor;programmer;scalability;shared memory;spinlock;synchronization (computer science);thread (computing);while	Gregory A. Chadwick;Simon W. Moore	2012	2012 IEEE 30th International Conference on Computer Design (ICCD)	10.1109/ICCD.2012.6378652	embedded system;synchronization;benchmark;parallel computing;real-time computing;multithreading;computer hardware;computer science;operating system;instruction set;processor register;queueing theory;field-programmable gate array	Arch	-7.540281447150799	51.56195271700805	142484
106418740c1f939ba53326cf0dcd04b0d31bb06d	heterogeneous mini-rank: adaptive, power-efficient memory architecture	quadcore system;random access memory;minirank architecture;power saving;memory management;performance evaluation;two channel ddr3 1066 memory;power efficiency;performance;power memory system mini rank heterogeneous performance;heterogeneous minirank design;memory access;memory management bandwidth instruction sets power demand random access memory performance evaluation operating systems;power aware computing;mini rank;heterogeneous;memory architecture;power aware computing dram chips memory architecture;dram rank;memory systems;bandwidth;memory power consumption;energy delay product memory architecture memory power consumption server platforms minirank architecture dram rank heterogeneous minirank design quadcore system two channel ddr3 1066 memory;server platforms;power consumption;energy delay product;memory system;power demand;memory bandwidth;power;dram chips;operating systems;instruction sets	Memory power consumption has become a big concern in server platforms. A recently proposed mini-rank architecture reduces the memory power consumption by breaking each DRAM rank into multiple narrow mini-ranks and activating fewer devices for each request. However, its fixed and uniform configuration may degrade performance significantly or lose power saving opportunities on some workloads. We propose a heterogeneous mini-rank design that sets the near-optimal configuration for each workload based on its memory access behavior and its memory bandwidth requirement. Compared with the original, homogeneous mini-rank design, the heterogeneous mini-rank design can balance between the performance and power saving and avoid large performance loss. For instance, for multiprogramming workloads with SPEC2000 application running on a quad-core system with two-channel DDR3-1066 memory, on average, the heterogeneous mini-rank can reduce the memory power by 53.1% (up to 60.8%) with the performance loss of 4.6% (up to 11.1%), compared with a conventional memory system. In comparison, the x32 homogeneous mini-rank can only save memory power by up to 29.8%; and the x8 homogeneous mini-rank will cause performance loss by up to 22.8%. Compared with x16 homogeneous mini-rank configuration, it can further reduce the EDP (energy-delay product) by up to 15.5% (10.0% on average).	computer multitasking;dynamic random-access memory;electronic data processing;memory architecture;memory bandwidth;mini-itx;multi-core processor;server (computing)	Kun Fang;Hongzhong Zheng;Zhichun Zhu	2010	2010 39th International Conference on Parallel Processing	10.1109/ICPP.2010.11	uniform memory access;interleaved memory;parallel computing;real-time computing;distributed memory;electrical efficiency;computer hardware;performance;computer science;operating system;instruction set;power;registered memory;memory bandwidth;bandwidth;computing with memory;memory management	HPC	-6.266072061222529	53.28511487586324	142795
1c8001fe6494c6ce4daa7470e3b089443a805775	a study of tree-based control flow prediction schemes	software performance evaluation flow graphs program control structures trees mathematics programming theory;program control structures;software performance evaluation;trees mathematics;flow graphs;tree graphs accuracy microprocessors flow graphs educational institutions computer aided instruction frequency clocks tree data structures;programming theory;control flow;prediction accuracy;superscalar processor;mips i instruction set tree based control flow prediction schemes instruction fetching sequential program wide issue superscalar processors multiple branch outcome prediction noncontiguous code portions conditional branches tree like subgraph control low graph superscalar fetch mechanism correlation extent prediction accuracy tree depth fetch size spec 92 integer benchmarks;instructions per cycle	I n order to fetch a large number of instructions per cycle f rom a sequential program, wide-issue superscalar processors have to predict the outcome o f multiple branches in a cycle, and fetch instructions f rom non-contiguous portions of code. Past research has developed schemes that predict the outcome of multiple branches b y performing a single prediction. That is, instead of predicting the outcome of every conditional branch, a tree-like subgraph of the control flow graph o f the executed program is considered as a single prediction unit, and a path is predicted through the tree, thereby allowing the superscalar fetch mechanism to go past multiple branches per cycle. I n this paper , we investigate the potential of using different extents of correlation to improve the prediction accuracy of control B o w prediction. W e also investigate the potential of increasing the tree depth to increase the fetch size. We measure the prediction accuracy of these schemes using the SPEC '92 integer benchmarks and the MIPSI instruction set.	benchmark (computing);branch (computer science);central processing unit;control flow graph;instructions per cycle;read-only memory;superscalar processor;wide-issue	Bunith Cyril;Bunith Franklin	1997		10.1109/HIPC.1997.634465	parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;control flow;instructions per cycle	Arch	-7.076522954866643	50.96303694770501	142808
dce7e4a338d08444e0847517648f6394975b8e0b	impact of gpus parallelism management on safety-critical and hpc applications reliability	graphics processing units instruction sets reliability neutrons parallel processing strain error analysis;reliability;storage management graphics processing units multi threading parallel algorithms processor scheduling safety critical software;neutrons;error analysis;graphics processing units;parallel algorithms gpgpus reliability radiation;gpgpus;strain;parallel processing;radiation;instruction sets;workload evaluation gpu parallelism management safety critical applications hpc application reliability graphic processing units computational power scheduling strain parallel process management gpu cross section neutron radiation experiments nvidia gpu application degree of parallelism gpu parallelism management memory latency thread registers number processors occupancy parallel application gpu radiation sensitivity dependence code dop code execution time mean workload failure metrics;parallel algorithms	Graphics Processing Units (GPUs) offer high computational power but require high scheduling strain to manage parallel processes, which increases the GPU cross section. The results of extensive neutron radiation experiments performed on NVIDIA GPUs confirm this hypothesis. Reducing the application Degree Of Parallelism (DOP) reduces the scheduling strain but also modifies the GPU parallelism management, including memory latency, thread registers number, and the processors occupancy, which influence the sensitivity of the parallel application. An analysis on the overall GPU radiation sensitivity dependence on the code DOP is provided and the most reliable configuration is experimentally detected. Finally, modifying the parallel management affects the GPU cross section but also the code execution time and, thus, the exposure to radiation required to complete computation. The Mean Workload and Executions Between Failures metrics are introduced to evaluate the workload or the number of executions computed correctly by the GPU on a realistic application.	cas latency;cuda;central processing unit;computation;cross section (geometry);degree of parallelism;dynamic dispatch;experiment;graphics processing unit;parallel computing;run time (program lifecycle phase);scheduling (computing)	Paolo Rech;Laércio Lima Pilla;Philippe Olivier Alexandre Navaux;Luigi Carro	2014	2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks	10.1109/DSN.2014.49	radiation;parallel processing;computer architecture;parallel computing;real-time computing;computer science;operating system;instruction set;reliability;parallel algorithm;data parallelism;strain;neutron;task parallelism	HPC	-8.557843200741857	48.76097603219985	142939
add350d0c5605c98d285b87493fc77c1d68281df	architectural support for server-side php processing		PHP is the dominant server-side scripting language used to implement dynamic web content. Just-in-time compilation, as implemented in Facebook's state-of-the-art HipHopVM, helps mitigate the poor performance of PHP, but substantial overheads remain, especially for realistic, large-scale PHP applications. This paper analyzes such applications and shows that there is little opportunity for conventional microarchitectural enhancements. Furthermore, prior approaches for function-level hardware acceleration present many challenges due to the extremely flat distribution of execution time across a large number of functions in these complex applications. In-depth analysis reveals a more promising alternative: targeted acceleration of four fine-grained PHP activities: hash table accesses, heap management, string manipulation, and regular expression handling. We highlight a set of guiding principles and then propose and evaluate inexpensive hardware accelerators for these activities that accrue substantial performance and energy gains across dozens of functions. Our results reflect an average 17.93% improvement in performance and 21.01% reduction in energy while executing these complex PHP workloads on a state-of-the-art software and hardware platform.	compiler;dynamic web page;function-level programming;graphic art software;hardware acceleration;hash table;hiphop for php;just-in-time compilation;laravel;memory management;microarchitecture;phalcon;regular expression;run time (program lifecycle phase);scripting language;server (computing);server-side scripting;string (computer science);symfony;web content;web development;yii	Dibakar Gope;David J. Schlais;Mikko H. Lipasti	2017	2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)	10.1145/3079856.3080234	hash table;parallel computing;server-side;real-time computing;operating system;computer science;dynamic web page;embedded system;scripting language;benchmark (computing);microarchitecture;server;hardware acceleration	Arch	-7.25275034735434	46.54997104679553	143590
5dab06919498a1d3c7780604074e8c6ffcc2a288	automatic detection of nondeterminacy in parallel programs		Many of today’s computers, for example the Cray X-MP, Gray 2, ETA”, Alliant FX/8, Sequent Symmetry, and Encore Multimax, are multiprocessors. Multiprocessing is a very appealing approach to parallelism since it can be used not only to accelerate the execution of a single program but also to increase throughput and reliability. Furthermore, the presence of independent control units makes possible the execution of parallel loops with branching, subroutine calls and random memory activity in a more effective way than is possible in single processor vector machines. However, writing and debugging a parallel program for a multiprocessor is, in general, more difficult, than writing sequential vector programs.	alliant computer systems;cray x-mp;debugging;encore computer;multiprocessing;parallel computing;subroutine;throughput	Perry A. Emrath;David A. Padua	1988		10.1145/68210.69224	computer science	HPC	-10.092013464871762	48.45422523568096	144692
43b292c5d0b9091b42dff112d0662fa18bb17a3d	adaptive partitioning strategies for loop parallelism in heterogeneous architectures	processor scheduling graphics processing units multiprocessing systems parallel programming;graphics processing units throughput instruction sets multicore processing pipelines engines kernel;parallel_for template adaptive partitioning strategies loop parallelism heterogeneous architectures multicores gpu accelerators graphics processing units dynamic scheduling strategy adaptive partitioning heuristic;dynamic scheduling heterogeneous computing adaptive partitioning task parallelism	This paper explores the possibility of efficiently using multicores in conjunction with multiple GPU accelerators under a parallel task programming paradigm. In particular, we address the challenge of extending a parallel_for template to allow its exploitation on heterogeneous systems. The extension is based on a two-stages pipeline engine which is responsible for partitioning and scheduling the chunks into the computational resources. Under this engine, we propose a dynamic scheduling strategy coupled with an adaptive partitioning heuristic that resizes chunks to prevent underutilization and load unbalance of CPUs and GPUs. In this paper we introduce the adaptive partitioning heuristic which is derived from an analytical model that minimizes the load unbalance while maximizes the throughput in the system. Using two benchmarks we evaluate the overhead introduced by our template extensions finding that it is negligible. We also evaluate the efficiency of our adaptive partitioning strategies and compared them with related work.	adaptive filter;benchmark (computing);central processing unit;computation;computational resource;for loop;graphics processing unit;heterogeneous system architecture;heuristic;multi-core processor;overhead (computing);parallel computing;programming paradigm;scheduling (computing);throughput	Angeles G. Navarro;Antonio Vilches;Rafael Asenjo;Francisco Corbera	2014	2014 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2014.6903677	computer architecture;parallel computing;real-time computing;computer science;data parallelism;instruction-level parallelism;general-purpose computing on graphics processing units;task parallelism	HPC	-6.741260362388114	49.3148542828831	144772
5dc4cf1560c3a74a6591dd806c351d723709d154	analysis of the tradeoffs between energy and run time for multilevel checkpointing		In high-performance computing, there is a perpetual hunt for performance and scalability. Supercomputers grow larger offering improved computational science throughput. Nevertheless, with an increase in the number of systems’ components and their interactions, the number of failures and the power consumption will increase rapidly. Energy and reliability are among the most challenging issues that need to be addressed for extreme scale computing. We develop analytical models for run time and energy usage for multilevel fault-tolerance schemes. We use these models to study the tradeoff between run time and energy in FTI, a recently developed multilevel checkpoint library, on an IBM Blue Gene/Q. Our results show that energy consumed by FTI is low and the tradeoff between the run time and energy is small. Using the analytical models, we explore the impact of various system-level parameters on run time and energy tradeoffs.	application checkpointing;blue gene;computational science;fault tolerance;ibm websphere extreme scale;interaction;run time (program lifecycle phase);scalability;supercomputer;throughput;transaction processing system	Prasanna Balaprakash;Leonardo Arturo Bautista-Gomez;Mohamed-Slim Bouguerra;Stefan M. Wild;Franck Cappello;Paul D. Hovland	2014		10.1007/978-3-319-17248-4_13	throughput;parallel computing;scalability;multi-objective optimization;computer science	HPC	-4.893172091087489	48.50053041962243	144874
3d046420c52a61fd1a669ad6d2a2a9861bc5e61d	computer architectures and programming models for scalable parallel computing	programming model;computer architecture;parallel computer	Parallel computing technology offers the opportunity for one basic computer architecture that achieves a wide performance range, at cost nearly linear in performance: A family of products, ranging from a workstation to a massively parallel processor with thousands of nodes, built from the same basic hardware components; the same languages, the same user interfaces used over all the range; the same application codes running efficiently on all these machines. ‘Ilk is the vision of scalable parallel computing. Is this dream, marketing hyperbole, or can it be reality? We examine, in our presentation, some of the underpinnings of this technology, We shall focus on parallel machines built of standard RISC microprocessor nodes, and consider the likely evolution of such systems in coming 3-5 years. The effective performance of a parallel machine is a function of many parameters: number of nodes, node compute rate, memory, internode communication bandwidth and latency, external 1/0 bandwidth and latency, and more. An intuitive requirement for scalable performance is that memory per node, internal and external bandwidth per node, and internal and external latency, stay nearly constant, as the number of nodes increase. Available technology can achieve these goals, over the range of interest. An examination of algorithmic performance show that these requirements for scalable performance have no better theoretical justification than the usual rules of thumb for uniprocessors (one byte of memory and one bit of 1/0 per flop/s). However, like these rules of thumb, they can provide a convention on what is a balanced parallel architecture. Such convention, even if arbitrary, simplifies the design of scalable software, since it leaves us with only two free performance parameters: number of nodes, and node performance. Internode communication latency is the performance parameter where current parallel architectures show the widest spread. Currently, shared memory machines achieve a much lower latency than distributed memory machines. An examination of the likely evolution of both architectures indicates that the gap will narrow, with increasingly similar communication mechanisms used on both. We shall examine the pros and cons of communication baaed on memory to cache transfers as compared to communication based on memory to memory tranafers. Parallel codes are written with various levels of explicit parallelism. At the most basic level, user explicitly control both data and control partition – this is the case for message passing. The compiler can still play an important role in optimizing communication by hiding la. tency. At the next level, languages are designed either to express control partition (control parallelism) or to express data partition (data parallelism). In the first case, the compiler derives an efficient data layout and communication pattern, from the partition of control; in the second case, it derives an efhcient assignment of computation to processors and an efficient communication pattern, from the partition of data. Both approaches give larger scope for compiler optimization. Finally there is the Holy Grail of implicit parallelism, all dealt by the compiler. Both distribution of computation and minimization of communication are, oftentimes, essential parts of parallel algorithm design. Therefore, I expect languages with explicit parallelism to be essential for the success of scalable parallel computing. Languages, like High Performance Fortran, that offer a choice of levels of control (explicit data and control parallelism, explicit data parallelism, “no explicit parallelism), may be the right approach: the user specify data or control partition only when the compiler fails to derive an efficient partition on its own. Such strategy can work only if the compiler ceases to be a black box, and provides to the user a clear model of the control and data partition derived from the source code,	algorithm design;black box;byte;central processing unit;code;computation;computer architecture;data parallelism;distributed memory;division by zero;erewhon;explicit parallelism;flops;high performance fortran;implicit parallelism;linear algebra;mathematical optimization;message passing;microprocessor;optimizing compiler;parallel algorithm;parallel computing;requirement;scalability;shared memory;task parallelism;uniprocessor system;user interface;workstation	Marc Snir	1993		10.1145/158511.158513	computer architecture;computing;parallel computing;reactive programming;functional reactive programming;computer science;cellular architecture;theoretical computer science;extensible programming;fifth generation computer;computer programming;abstract machine;programming paradigm;inductive programming;programming language;computer network programming;parallel programming model	HPC	-7.545224343683095	46.6091477329639	145663
0e3abdf2d87245c81ae1eb3501cca61d0d1c1c33	on the impact of os and linker effects on level-2 cache performance	analytical models;level 2;microprocessors;information systems;dynamic linking;process design;operating systems microprocessors joining processes information systems process design costs performance analysis hardware computational modeling analytical models;computational modeling;technology and engineering;operating system;levels of abstraction;indexation;performance analysis;cache performance;joining processes;data layout;operating systems;hardware	The design of microprocessors depends strongly on architectural simulation. As simulation can be very slow, it is necessary to reduce simulation time by simplifying the simulator and increasing its level of abstraction. A very common abstraction is to ignore operating system effects. As a result of this, there is no information available during simulation about the relationship between virtual addresses and physical addresses This information is important for lower-level caches and main memory as these memories are indexed using the physical address. Another simplification relates to simulating only statically linked programs, instead of the commonly used dynamic linking. This results in different data layouts and, as we show in this paper, it effects the miss rate of physically indexed caches such as the level-2 cache. This paper investigates the error associated to these simplifications in the modeling of level-2 caches and shows that performance can be underestimated or overestimated with errors up to 24%.	benchmark (computing);cpu cache;computer data storage;device driver;dynamic linker;exclusive or;experiment;hans moravec;hash function;heart rate variability;jonas;level of detail;linker (computing);microprocessor;national fund for scientific research;operating system;physical address;simulation;spatial variability;stani michiels;static build;static library	Hans Vandierendonck;Koen De Bosschere	2006	14th IEEE International Symposium on Modeling, Analysis, and Simulation	10.1109/MASCOTS.2006.36	process design;parallel computing;real-time computing;computer science;theoretical computer science;operating system;computational model;information system	Arch	-9.466757776660234	49.78698715461796	145713
bdb4068c8f62b669f40273a3cc01ff85238881ea	characterizing and mitigating work time inflation in task parallel programs	multi-threading;performance evaluation;processor scheduling;shared memory systems;intel openmp task scheduler;numa systems;qthreads library;data access latency;locality-aware scheduling;multithreaded computation;parallel openmp applications;parallel applications;scheduling overheads;sequential computation;shared memory parallel programming;task parallel openmp programs;thread idleness;work time inflation characterization;work time inflation mitigation;affinity;numa	Task parallelism raises the level of abstraction in shared memory parallel programming to simplify the development of complex applications. However, task parallel applications can exhibit poor performance due to thread idleness, scheduling overheads, and work time inflation -- additional time spent by threads in a multithreaded computation beyond the time required to perform the same work in a sequential computation. We identify the contributions of each factor to lost efficiency in various task parallel OpenMP applications and diagnose the causes of work time inflation in those applications.  Increased data access latency can cause significant work time inflation in NUMA systems. Our locality framework for task parallel OpenMP programs mitigates this cause of work time inflation. Our extensions to the Qthreads library demonstrate that locality-aware scheduling can improve performance up to 3X compared to the Intel OpenMP task scheduler.	computation;data access;locality of reference;openmp;parallel computing;scheduling (computing);shared memory;task parallelism;thread (computing);windows task scheduler	Stephen Olivier;Bronis R. de Supinski;Martin Schulz;Jan Prins	2012	2012 International Conference for High Performance Computing, Networking, Storage and Analysis	10.3233/SPR-130369	computer architecture;parallel computing;real-time computing;computer science;operating system;disjoint-set	HPC	-10.001470517800206	49.684237465286664	145881
5c32171b24fce7f0e8f395c7665756d47e85446d	the mips r10000 superscalar microprocessor	microprocessors;logic arrays;instruction execution;write back caches mips r10000 superscalar microprocessor sequential memory consistency exception handling speculative execution memory addresses cache refills memory latency;logic design;superscalar;microprocessors delay logic arrays cmos logic circuits out of order logic design adaptive arrays pipelines registers design optimization;cache memory;write back caches;instruction set architecture;design optimization;mips r10000 superscalar microprocessor;memory consistency;out of order;real world application;low latency;memory addresses;registers;cmos logic circuits;cache refills;pipelines;adaptive arrays;sequential memory consistency;speculative execution;memory systems;exception handling;high performance;memory latency;microprocessor chips;instructions per cycle;mips r10000	cache refills early. he Mips RlOOOO is a dynamic, superscalar microprocessor that implements T the 64-bit Mips 4 instruction set architecture. It fetches and decodes four instructions per cycle and dynamically issues them to five fully-pipelined, low-latency execution units. Instructions can be fetched and executed speculatively beyond branches. Instructions graduate in order upon completion. Although execution is out of order, the processor still provides sequential memory consistency and precise exception handling. The RlOOOO is designed for high performance, even in large, real-world applications with poor memory locality. With speculative execution, it calculates memory addresses and initiates cache refills early. Its hierarchical, nonblocking memory system helps hide memory latency with two levels of set-associative, write-back caches. Figure 1 shows the RlOOOO system configuration, and the RlOOOO box lists its principal features. Out-of-order superscalar processors are inherently complex. To cope with this complexity, the RlOOOO uses a modular design that locates much of the control logic within regular structures, including the active list, register map tables, and instruction queues.	64-bit computing;cas latency;cache (computing);central processing unit;consistency model;exception handling;execution unit;instructions per cycle;locality of reference;microprocessor;modular design;parsing;r10000;speculative execution;superscalar processor;system configuration	Kenneth C. Yeager	1996	IEEE Micro	10.1109/40.491460	memory address;exception handling;interleaved memory;computer architecture;parallel computing;logic synthesis;real-time computing;multidisciplinary design optimization;cpu cache;cas latency;computer science;out-of-order execution;superscalar;operating system;instruction set;pipeline transport;processor register;memory map;instructions per cycle;speculative execution;low latency	Arch	-8.001891856806274	50.96143218328375	145971
a66c90cb2b427a41b3c530864ae70b32abd6916c	a framework for automatic dynamic data mapping	directed graphs;distributed memory;optimal solution;interprocess communication;distributed memory systems;local data access;remote data access;parallelizing compiler;loop parallelization;program control structures;performance;parallelizing compilers;distributed computing;software performance evaluation;phased arrays costs distribution strategy data structures distributed computing computer architecture chromium programming profession tail ear;data distribution;computer architecture;dynamic data;directed graphs data structures distributed memory systems program control structures parallelising compilers software performance evaluation linear programming integer programming;communication parallelism graph;ear;integer programming;parallelising compilers;quality;data structures;programming profession;chromium;distributed memory multiprocessors;high performance fortran;linear 0 1 integer programming solver;compilation time;linear programming;one dimensional array distributions;integer program;automatic dynamic data mapping;distribution strategy;data structure;static data distribution;dynamic data distribution;static and dynamic data mapping;tail;linear 0 1 integer programming;automatic data distribution;quality automatic dynamic data mapping distributed memory multiprocessors data distribution loop parallelization parallelizing compiler remote data access local data access performance interprocess communication dynamic data distribution static data distribution data structure communication parallelism graph linear 0 1 integer programming solver one dimensional array distributions compilation time;phased arrays	Data distribution is one of the key aspects that a parallelizing compiler for a distributed memory architecture should consider, to get eeciency from the system. The cost of accessing local and remote data can be one or several orders of magnitude diierent, and this can dramatically aaect the performance of the system. In this paper, we present an approach to automatically derive static or dynamic data distribution strategies for the arrays used in a program. All the information required about data movement and parallelism is contained in a single data structure, called the Communication-Parallelism Graph (CPG). The problem is modeled and solved using a general purpose linear 0-1 integer programming solver. This allows us to nd the optimal solution for the problem for one-dimensional array distributions. The solution found is dynamic, in the sense that the layout of the arrays can change during the execution of the program, if this leads to better performance. We also show the feasibility of using this approach in terms of compilation time and quality of the solutions generated.	array data structure;central pattern generator;compiler;distributed memory;dynamic data;integer programming;linear programming;parallel computing;solver	Jordi Garcia;Eduard Ayguadé;Jesús Labarta	1996		10.1109/SPDP.1996.570321	computer architecture;chromium;parallel computing;dynamic data;distributed memory;data structure;performance;computer science;theoretical computer science;operating system;distributed computing;programming language;tail	HPC	-6.446408513862468	46.463343820011936	146930
c77332269f50490a0c23d9652eef8aa8ac6ff6f3	branch history matching: branch predictor warmup for sampled simulation	accurate branch predictor warmup;warmup budget;branch history matching;fixed-length warmup;sampled simulation;cache state warmup;sampling unit;warmup phase;total warmup length budget;warmup length;branch predictor warmup	Computer architects and designers rely heavily on simulation. The downside of simulation is that it is very time-consuming — simulating an industry-standard benchmark on today’s fastest machines and simulators takes several weeks. A practical solution to the simulation problem is sampling. Sampled simulation selects a number of sampling units out of a complete program execution and only simulates those sampling units in detail. An important problem with sampling however is the microarchitecture state at the beginning of each sampling unit. Large hardware structures such as caches and branch predictors suffer most from unknown hardware state. Although a great body of work exists on cache state warmup, very little work has been done on branch predictor warmup. This paper proposes Branch History Matching (BHM) for accurate branch predictor warmup during sampled simulation. The idea is to build a distribution for each sampling unit of how far one needs to go in the pre-sampling unit in order to find the same static branch with a similar global and local history as the branch instance appearing in the sampling unit. Those distributions are then used to determine where to start the warmup phase for each sampling unit for a given total warmup length budget. Using SPEC CPU2000 integer benchmarks, we show that BHM is substantially more efficient than fixed-length warmup in terms of warmup length for the same accuracy. Or reverse, BHM is substantially more accurate than fixed-length warmup for the same warmup budget.	benchmark (computing);branch predictor;cpu cache;cold start;fastest;locality of reference;microarchitecture;national fund for scientific research;sampling (signal processing);simulation	Simon Kluyskens;Lieven Eeckhout	2007		10.1007/978-3-540-69338-3_11	parallel computing;real-time computing;simulation;computer science;operating system	Arch	-6.242716175899647	51.03386998477646	147462
fd2cb6b839ba5872efb90d32a2f4bcd5adc026f3	an efficient data reuse strategy for multi-pattern data access		Memory partitioning has been widely adopted to increase the memory bandwidth. Data reuse is a hardware-efficient way to improve data access throughput by exploiting locality in memory access patterns. We found that for many applications in image and video processing, a global data reuse scheme can be shared by multiple patterns. In this paper, we propose an efficient data reuse strategy for multi-pattern data access. Firstly, a heuristic algorithm is proposed to extract the reuse information as well as find the non-reusable data elements of each pattern. Then the non-reusable elements are partitioned into several memory banks by an efficient memory partitioning algorithm. Moreover, the reuse information is utilized to generate the global data reuse logic shared by the multi-pattern. We design a novel algorithm to minimize the number of registers required by the data reuse logic. Experimental results show that compared with the state-of-the-art approach, our proposed method can reduce the number of required BRAMs by 62.2% on average, with the average reduction of 82.1% in SLICE, 87.1% in LUTs, 71.6% in Flip-Flops, 73.1% in DSP48Es, 83.8% in SRLs, 46.7% in storage overhead, 79.1% in dynamic power consumption, and 82.6% in execution time of memory partitioning. Besides, the performance is improved by 14.4%.		Wensong Li;Fan Yang;Hengliang Zhu;Xuan Zeng;Dian Zhou	2018	2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1145/3240765.3240778	throughput;memory bank;memory management;computer science;real-time computing;high-level synthesis;heuristic (computer science);reuse;data access;memory bandwidth;distributed computing	EDA	-4.664107515771731	50.84179197822199	148104
da0c8c9c5d7904a82174fe74b9c9fc1cb01c50eb	inferno: a functional simulation infrastructure for modeling microarchitectural data speculations	logic simulation;multi threading;multi threading application program interfaces logic simulation;microarchitecture;microarchitecture context modeling yarn pipelines process design frequency processor scheduling modular construction productivity clocks;yarn;functional model convergence apis;clocks;building block;processor scheduling;instruction semantics;large scale system;inferno;relational database;process design;pipelines;application program interfaces;microarchitectural data speculations modelling;speculative execution;context management;performance model;functional model;processor designs;design rationale;functional simulation construction framework;modular construction;productivity;frequency;relational database inferno functional simulation infrastructure microarchitectural data speculations modelling design rationales functional simulation construction framework processor designs instruction semantics functional model convergence apis;context modeling;high speed;functional simulation infrastructure;design rationales	This paper presents key insights and design rationales behind Inferno, a functional simulation construction framework developed at Intel to support execution-driven cycle-accurate performance modeling and simulation of advanced microarchitectural data speculation techniques for future processor designs and explorations. Inferno divides the task of functional simulation into three essential components, namely: (1) context manager of in-flight speculatively executed instructions, (2) stateless emulator of instruction semantics, and (3) a high speed functional simulator capable of booting OS and running large-scale system workloads. These building block components work together in concert via a set of well-architected functional model convergence APIs. With a novel abstraction called speculative domain, the context manager serves effectively as a relational database about the in-flight instructions and on-going speculations. Through a set of functional model usage APIs, the context manager enables performance models to express arbitrary microarchitectural data speculation scenarios. The contribution of this paper is to demonstrate the importance of providing a functional model with modular construction, proper abstraction and expressive APIs for speculative state management. Inferno is such a functional model construction framework and has significantly improved productivity in modeling a variety of sophisticated data speculation microarchitecture techniques.	inferno;logic simulation;microarchitecture	Hong Wang;Shiri Manor;Dave LaFollette;Nadav Nesher;Ku-jei King;Perry H. Wang;Shay Levy;Shai Satt;Gal Carmeli;Arjun Kapur;Ioannis Schoinas;Ed Rubinstein;Rahul Bhatt	2003		10.1109/ISPASS.2003.1190228	process design;computer architecture;productivity;parallel computing;real-time computing;multithreading;microarchitecture;relational database;computer science;function model;operating system;logic simulation;frequency;pipeline transport;context model;programming language;design rationale;speculative execution	EDA	-8.93442428988858	48.22900290117432	148231
06d4de9ff989867f3bf464516d158df0ee3bcae6	enabling intra-plane parallel block erase in nand flash to alleviate the impact of garbage collection		Garbage collection (GC) in NAND flash can significantly decrease I/O performance in SSDs by copying valid data to other locations, thus blocking incoming I/O requests. To help improve performance, NAND flash utilizes various advanced commands to increase internal parallelism. Currently, these commands only parallelize operations across channels, chips, dies, and planes, neglecting the block level due to risk of disturbances that can compromise valid data by inducing errors. However, due to the triple-well structure of the NAND flash plane architecture, it is possible to erase multiple blocks within a plane, in parallel, without diminishing the integrity of the valid data. The number of page movements due to multiple block erases can be restrained so as to bound the overhead per GC. Moreover, more capacity can be reclaimed per GC which delays future GCs and effectively reduces their frequency. Such an Intra-Plane Parallel Block Erase (IPPBE) in turn diminishes the impact of GC on incoming requests, improving their response times. Experimental results show that IPPBE can reduce the time spent performing GC by up to 50.7% and 33.6% on average, read/write response time by up to 47.0%/45.4% and 16.5%/14.8% on average respectively, page movements by up to 52.2% and 26.6% on average, and blocks erased by up to 14.2% and 3.6% on average. An energy analysis conducted indicates that by reducing the number of page copies and the number of block erases, the energy cost of garbage collection can be reduced up to 44.1% and 19.3% on average.	blocking (computing);cluster analysis;flash memory;garbage collection (computer science);inductive reasoning;input/output;overhead (computing);parallel computing;response time (technology);whole earth 'lectronic link	Tyler Garrett;Jun Yang;Youtao Zhang	2018		10.1145/3218603.3218627	architecture;real-time computing;computer science;nand gate;garbage collection;response time;copying;compromise;communication channel	HPC	-9.874412051814385	53.31704789727544	148372
954e1cff0668108b42126fc812c2963c7ece16bb	optimizing the management of reference prediction table for prefetching and prepromotion	cache;prefetching;prepromotion;reference prediction table;scalar filter policy;bimodal insert policy;memory	Prefetching and prepromotion are two important techniques for hiding the memory access latency. Reference prediction tables (RPT) plays a significant role in the process of prefetching or prepromoting data with linear memory access patterns. The traditional RPT management, LRU replacement algorithm, can not manage RPT efficiently. This leads to that large RPT has to be used for the considerable performance. The cost brought from the large capacity limits the usage of RPT in real processors. This paper uses bimodal insert policy (BIP) and proposed scalar filter policy (SFP) in the RPT management. Owing to matching the using characteristics of RPT, BIP can reduce the RPT thrashing and SFP can filter the useless scalar instructions in it. After testing 8 NPB benchmarks on a fullsystem simulator, we find that our approaches improve the RPT hit rate by 53.81% averagely, and increases prefetching and prepromotion operations by 18.85% and 53.55% averagely over the traditional LRU management.	bip-8;cpu cache;central processing unit;computer architecture simulator;linear programming;link prefetching;optimizing compiler;page replacement algorithm;reference implementation;simulated fluorescence process algorithm;simulation;thrashing (computer science)	Junjie Wu;Xuejun Yang	2010	JCP	10.4304/jcp.5.2.242-249	parallel computing;real-time computing;cache;computer science;operating system;database;memory	HPC	-10.719300692909666	52.8996266831725	148421
0fc0910aba6a5690059843fd72e99c871a16a577	virtual machine aware communication libraries for high performance computing	libraries;virtual machine;inter vm communication;kernel;shared memory;performance evaluation;application management;high performance computing;perforation;virtual machining;machinery production industries;voice mail;technology management;virtual machine monitors;large scale;servers;virtual machines vm;xen;high performance computer;process control;driver circuits;mpi;virtual machines vm mpi vmm bypass xen inter vm communication;productivity;peer to peer computing;vmm bypass;nas parallel benchmarks;environmental management;high performance;system management;virtual machining libraries high performance computing virtual manufacturing technology management voice mail productivity machinery production industries environmental management large scale systems;virtual manufacturing;large scale systems	As the size and complexity of modern computing systems keep increasing to meet the demanding requirements of High Performance Computing (HPC) applications, manageability is becoming a critical concern to achieve both high performance and high productivity computing. Meanwhile, virtual machine (VM) technologies have become popular in both industry and academia due to various features designed to ease system management and administration. While a VM-based environment can greatly help manageability on large-scale computing systems, concerns over performance have largely blocked the HPC community from embracing VM technologies. In this paper, we follow three steps to demonstrate the ability to achieve near-native performance in a VM-based environment for HPC. First, we propose Inter-VM Communication (IVC), a VM-aware communication library to support efficient shared memory communication among computing processes on the same physical host, even though they may be in different VMs. This is critical for multi-core systems, especially when individual computing processes are hosted on different VMs to achieve fine-grained control. Second, we design a VM-aware MPI library based on MVAPICH2 (a popular MPI library), called MVAPICH2-ivc, which allows HPC MPI applications to transparently benefit from IVC. Finally, we evaluate MVAPICH2-ivc on clusters featuring multi-core systems and high performance InfiniBand interconnects. Our evaluation demonstrates that MVAPICH2-ivc can improve NAS Parallel Benchmark performance by up to 11% in VM-based environment on eight-core Intel Clover-town systems, where each compute process is in a separate VM. A detailed performance evaluation for up to 128 processes (64 node dual-socket single-core systems) shows only a marginal performance overhead of MVAPICH2-ivc as compared with MVAPICH2 running in a native environment. This study indicates that performance should no longer be a barrier preventing HPC environments from taking advantage of the various features available through VM technologies.	benchmark (computing);electrical connection;infiniband;inter-process communication;library (computing);marginal model;message passing interface;multi-core processor;multiprocessing;openvms;overhead (computing);performance evaluation;requirement;shared memory;single-core;supercomputer;systems management;virtual machine	Wei Huang;Matthew J. Koop;Qi Gao;Dhabaleswar K. Panda	2007	Proceedings of the 2007 ACM/IEEE Conference on Supercomputing (SC '07)	10.1145/1362622.1362635	shared memory;embedded system;productivity;parallel computing;kernel;real-time computing;systems management;computer science;virtual machine;technology management;message passing interface;operating system;process control;application lifecycle management;server;computer network	HPC	-10.999682094292393	47.913290500668765	148611
b25d7dfd6b1fa9fe5d46fa9f46bd6eb31b78b915	past and future directions for concurrent task scheduling		A wave of parallel processing research in the 1970s and 1980s developed various techniques for concurrent task scheduling, including work-stealing scheduling and lazy task creation, and various ideas for supporting speculative computing, including the sponsor model, but these ideas did not see large-scale use as long as uniprocessor clock speeds continued to increase rapidly from year to year. Now that the increase in clock speeds has slowed dramatically and multicore processors have become the answer for increasing the computing throughput of processor chips, increasing the performance of everyday applications on multicore processors by using parallelism has taken on greater importance, so concurrent task scheduling techniques are getting a second look.		Robert H. Halstead	2014		10.1007/978-3-662-44471-9_8	throughput;parallel computing;scheduling (computing);uniprocessor system;multi-core processor;parallel processing;computer science	Robotics	-8.423865659670945	51.255070753787386	149490
9775fcded79fadd8994096ce9dbac96dbbe16d38	computer forensics using graphics processing unit for file searching	new technology;computer forensics;new technology file system ntfs;search method;master file table mft;cuda;file system;graphic processing unit;central processing unit;graphics processing unit gpu	In this project, I conduct a research that strives to make a huge impact on the area of computer forensics. When performing various investigations the process of finding evidence can be a lengthy one in terms of time and resources. We felt it would be cool if we could somehow use a GPU to aid in computer forensic investigations, particularly searching for files. The objective of this research is to test the time efficiency of a standard search method, and compare the results with GPU multi-threading search method. My hypothesis is that the GPU multiprocessors will significantly reduce the time and resources it takes to go through with an investigation of searching for files. My hypothesis is that the GPU multiprocessors will significantly reduce the time and resources it takes to go through with an investigation of searching for files, rendering the time it takes obsolete so that the original, reliable evidence may be used for prosecutions, validation of misconduct, etc. The structure of the graphics processing unit is designed specifically for heavily, complex computations, compared to central processing units that are designed for memory and storage purposes.	central processing unit;computation;computer forensics;computer graphics;graphics processing unit;thread (computing)	Andrea Fails	2012		10.1145/2184512.2184620	computer hardware;computer science;operating system;real-time computer graphics;general-purpose computing on graphics processing units;computer graphics (images)	OS	-7.722302161710157	47.25185715980608	149497
8fdceae3c7a1fe14bedee2a77fdb3a0a6674f652	future execution: a hardware prefetching technique for chip multiprocessors	storage management multiprocessing systems;hardware prefetching yarn delay iron laboratories microprocessors instruments computer aided instruction engines;storage management;chip multiprocessor;speccpu2000 benchmark suite hardware prefetching chip multiprocessors data prefetching thread based prefetching adjustable lookahead distance;speculative execution;multiprocessing systems	This paper proposes a new hardware technique for using one core of a CMP to prefetch data for a thread running on another core. Our approach simply executes a copy of all non-control instructions in the prefetching core after they have executed in the primary core. On the way to the second core, each instruction's output is replaced by a prediction of the likely output that the nth future instance of this instruction will produce. Speculatively executing the resulting instruction stream on the second core issues load requests that the main program will probably a reference in the future. Unlike previously proposed thread-based prefetching approaches, our technique does not need any thread spawning points, features an adjustable lookahead distance, does not require complicated analyzers to extract prefetching threads, is recovery-free, and necessitates no storage for the prefetching threads. We demonstrate that for the SPECcpu2000 benchmark suite, our mechanism significantly increases the prefetching coverage and improves the primary core's performance by 10% on average over a baseline that already includes an aggressive hardware stream prefetcher. We further show that our approach works well in combination with runahead execution.	baseline (configuration management);benchmark (computing);cpu cache;compiler;entry point;kerrison predictor;legacy code;operating system;parsing;prefetch input queue;prefetcher;programmer;runahead;speedup	Ilya Ganusov;Martin Burtscher	2005	14th International Conference on Parallel Architectures and Compilation Techniques (PACT'05)	10.1109/PACT.2005.23	parallel computing;real-time computing;computer science;operating system;speculative execution	Arch	-7.432347817487247	51.077932566585304	149845
1ec4079e08cc72c88a4000a0240171ef62ca77af	a discussion on non-blocking/lockup-free caches	performance evaluation;mpp;cache memory;system performance;data distribution;input output;parallelism;concurrency;data cache;multilevel caches;trace driven simulation;file systems	In our previous contribution in the June 1996 issue of CAN, we presented a discussion on lockup-free caches. The article raised a couple of issues with the audience, which we will attempt to address in this addendum.	addendum;blocking (computing);non-blocking algorithm	Samson Belayneh;David R. Kaeli	1996	SIGARCH Computer Architecture News	10.1145/235688.235691	bus sniffing;input/output;least frequently used;pipeline burst cache;cache coherence;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;false sharing;concurrency;cpu cache;tag ram;computer hardware;cache;computer science;write-once;cache invalidation;operating system;direct memory access;computer performance;smart cache;mesi protocol;cache algorithms;cache pollution;non-uniform memory access	Arch	-10.761186912175068	48.24771540593248	149941
2b1559599b0f0b729f50c07c832a6ec28fbf250b	the sgi origin software environment and application performance	virtual memory;ccnuma;silicon graphics inc;multiprocessing programs;multiprocessing;processor scheduling;user level facilities sgi origin software environment application performance cellular irix operating system origin 200 2000 systems silicon graphics inc scalability modularity nonuniform memory access subsystem redesign i o subsystem scheduler virtual memory system kernel features;non uniform memory access;input output programs;performance;software performance evaluation;software performance evaluation multiprocessing systems multiprocessing programs virtual storage processor scheduling input output programs operating system kernels;system performance;operating system;scheduling;software performance application software hardware silicon operating systems graphics processor scheduling kernel scalability system software;dsm;numa;multiprocessing systems;operating system kernels;unix;virtual storage	Cellular Irix is the name of the new operating system for the Origin 200/2000 systems as well as the new Onyx/sup 2/, all from Silicon Graphics Inc. (SGI). The scalability, modularity and non-uniform memory access (NUMA) properties of the Origin system require significant redesign of several operating system subsystems, including the I/O subsystem, the scheduler and the virtual memory system. This paper focuses on the kernel features incorporated into Cellular Irix, the user-level facilities providing access to those features and the system performance achieved.	graphics;irix;input/output;non-uniform memory access;openvms;operating system;scalability;scheduling (computing);uniform memory access;user space	Steve Whitney;John McCalpin;Nawaf Bitar;John L. Richardson;Luis Stevens	1997	Proceedings IEEE COMPCON 97. Digest of Papers	10.1109/CMPCON.1997.584691	parallel computing;real-time computing;computer science;operating system	OS	-11.413252761239523	48.30838165441044	149977
f4a7ae3c06453c24270a9ac4673a7db8bb607a3f	efficient utilization of gpgpu cache hierarchy	cache bypassing;gpgpu;warp throttling;conflict avoiding;cache management	Recent GPUs are equipped with general-purpose L1 and L2 caches in an attempt to reduce memory bandwidth demand and improve the performance of some irregular GPGPU applications. However, due to the massive multithreading, GPGPU caches suffer from severe resource contention and low data-sharing which may degrade the performance instead. In this work, we propose three techniques to efficiently utilize and improve the performance of GPGPU caches. The first technique aims to dynamically detect and bypass memory accesses that show streaming behavior. In the second technique, we propose dynamic warp throttling via cores sampling (DWT-CS) to alleviate cache thrashing by throttling the number of active warps per core. DWT-CS monitors the MPKI at L1, when it exceeds a specific threshold, all GPU cores are sampled with different number of active warps to find the optimal number of warps that mitigates thrashing and achieves the highest performance. Our proposed third technique addresses the problem of GPU cache associativity since many GPGPU applications suffer from severe associativity stalls and conflict misses. Prior work proposed cache bypassing on associativity stalls. In this work, instead of bypassing, we employ a better cache indexing function, Pseudo Random Interleaving Cache (PRIC), that is based on polynomial modulus mapping, in order to fairly and evenly distribute memory accesses over cache sets. The proposed techniques improve the average performance of streaming and contention applications by 1.2X and 2.3X respectively. Compared to prior work, it achieves 1.7X and 1.5X performance improvement over Cache-Conscious Wavefront Scheduler and Memory Request Prioritization Buffer respectively.	best, worst and average case;cpu cache;discrete wavelet transform;forward error correction;general-purpose computing on graphics processing units;general-purpose markup language;graphics processing unit;memory bandwidth;modulus robot;multithreading (computer architecture);polynomial;resource contention;sampling (signal processing);thrashing (computer science);thread (computing)	Mahmoud Khairy;Mohamed Zahran;Amr G. Wassal	2015		10.1145/2716282.2716291	bus sniffing;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;computer hardware;cache;computer science;cache invalidation;smart cache;cache algorithms;cache pollution	HPC	-8.705928691234359	52.96578961283978	150365
8de8b04532a3e0a010ed44811022cff952de710e	balancing thread partition for efficiently exploiting speculative thread-level parallelism	thread level speculation;thread level parallelism;chip multiprocessor;profiles methods;chip;data dependence;speculative execution;parallel architecture;value prediction	General-purpose computing is taking an irreversible step toward on-chip parallel architectures. One way to enhance the performance of chip multiprocessors is the use of thread-level speculation (TLS). Identifying the points where the speculative threads will be spawned becomes one of the critical issues of this kind of architectures. In this paper, a criterion for selecting the region to be speculatively executed is presented to identify potential sources of speculative parallelism in general-purpose programs. A dynamic profiling method has been provided to search a large space of TLS parallelization schemes and where parallelism was located within the application. We analyze key factors impacting speculative thread-level parallelism of SPEC CPU2000, evaluate whether a given application or parts of it are suitable for TLS technology, and study how to balance thread partition for efficiently exploiting speculative thread-level parallelism. It shows that the inter-thread data dependences are ubiquitous and the synchronization mechanism is necessary; Return value prediction and loop unrolling are important to improve performance. The information we got can be used to guide the thread partition of TLS.	parallel computing;speculative execution;task parallelism	Yaobin Wang;Hong An;Bo Liang;Li Wang;Ming Cong;Yongqing Ren	2007		10.1007/978-3-540-76837-1_8	chip;computer architecture;parallel computing;real-time computing;computer science;operating system;distributed computing;data parallelism;instruction-level parallelism;speculative multithreading;task parallelism;speculative execution	Arch	-6.610772893827405	49.7578959291211	151148
3000c714a7d5afad6d3b498f3505685890fade46	processing mpi derived datatypes on noncontiguous gpu-resident data	benchmarking;paper;tesla c2050;concurrent programming;concurrent programming structures;graphics processing units data models computer graphics;cuda;datatype mpi graphics processing unit cuda;nvidia;algorithms;mpi;computer science;graphics processing unit;datatype;shoc stencil benchmark mpi derived datatypes processing message passing interface noncontiguous gpu resident data graphics processing unit noncontiguous data layouts gpu memory parallel noncontiguous data processing methodology fine grained data point parallelism level tree based datatype encoding gpu threads cuda compute unified device architecture dma based processing kernel based packing strategies hacc mesh analysis;parallel architectures application program interfaces data handling graphics processing units message passing	Driven by the goals of efficient and generic communication of noncontiguous data layouts in GPU memory, for which solutions do not currently exist, we present a parallel, noncontiguous data-processing methodology through the MPI datatypes specification. Our processing algorithm utilizes a kernel on the GPU to pack arbitrary noncontiguous GPU data by enriching the datatypes encoding to expose a fine-grained, data-point level of parallelism. Additionally, the typically tree-based datatype encoding is preprocessed to enable efficient, cached access across GPU threads. Using CUDA, we show that the computational method outperforms DMA-based alternatives for several common data layouts as well as more complex data layouts for which reasonable DMA-based processing does not exist. Our method incurs low overhead for data layouts that closely match best-case DMA usage or that can be processed by layout-specific implementations. We additionally investigate usage scenarios for data packing that incur resource contention, identifying potential pitfalls for various packing strategies. We also demonstrate the efficacy of kernel-based packing in various communication scenarios, showing multifold improvement in point-to-point communication and evaluating packing within the context of the SHOC stencil benchmark and HACC mesh analysis.	algorithm;benchmark (computing);cuda;direct memory access;global variable;graphics processing unit;kernel (operating system);mesh analysis;message passing interface;overhead (computing);parallel computing;point-to-point protocol;point-to-point (telecommunications);resource contention;set packing	John Jenkins;James Dinan;Pavan Balaji;Tom Peterka;Nagiza F. Samatova;Rajeev Thakur	2014	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2013.234	computer architecture;parallel computing;concurrent computing;data type;computer hardware;computer science;message passing interface;operating system;programming language;benchmarking	HPC	-6.3474156372191	46.965853340484	151330
08410786b78c5a99e8362b74c6fabac93552536b	incorporating selective victim cache into gpgpu for high-performance computing	selective caching;shared memory;gpgpu;victim cache;register file		cpu cache;general-purpose computing on graphics processing units;supercomputer	Jianfei Wang;Fengfeng Fan;Li Jiang;Xiaoyao Liang;Naifeng Jing	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.4104	distributed computing;register file;parallel computing;computer science;smart cache;cache;cache algorithms;general-purpose computing on graphics processing units;cache pollution;supercomputer;shared memory	HPC	-10.86957040307554	48.932971883116295	151334
1d6776549be221634decaf8b5dceb0ba88b12d8b	a study of switch models for the scalable coherent interface	switch models;scalable coherent interface	The Scalable Coherent Interface (SCI) specifies a topology-independent communication protocol with the possibility of connecting up to 64 K nodes. SCI switches are the key components in building large SCI systems effectively. This paper presents four alternative architectures for a possible SCI switch implementation. Of these four models, two could be seen as crossbar-based switches. The main difference between the two crossbar switch models is the different structure of queues: either a parallel structure, called SwitchLink or serial, called CrossSwitch. The other two models are a ring-based switch model, which is actually connecting four ports into an internal SCI ring and a bus-based switch model which uses an internal bidirectional bus. We will describe each model in detail and compare them by simulation.	coherent	Bin Wu;Andreas Johannes Bogaerts;B. Skaali	1995			embedded system	Vision	-11.505093253574339	46.69102002407437	152325
8963e366e778920cad5338358de45f24ffa42dae	an application-driven study of parallel system overheads and network bandwidth requirements	shared memory;performance evaluation;design space;interconnection network;virtual machines parallel architectures shared memory systems performance evaluation;shared memory systems;parallel architectures;virtual machines;parallel systems;execution driven simulation;application driven study;parallel architecture;parallel systems overheads;bandwidth parallel architectures multiprocessor interconnection networks network topology network synthesis delay concurrent computing parallel machines computer architecture hardware;parallel applications;network overheads application driven study parallel system overheads network bandwidth requirements parallel application parallel systems research design space parallel applications shared memory parallel architectures simulated architectures bandwidth requirements network topologies execution driven simulation tool spasm shared memory machines private caches network connection network bandwidth problem sizes network technologies	Evaluating and analyzing the performance of a parallel application on an architecture to explain the disparity between projected and delivered performance is an important aspect of parallel systems research. However, conducting such a study is hard due to the vast design space of these systems. In this paper, we study two important aspects related to the performance of parallel applications on shared memory parallel architectures. First, we quantify the overheads observed during the execution of these applications on three di erent simulated architectures. We next use these results to synthesize the bandwidth requirements for the applications with respect to di erent network topologies. This study is performed using an execution-driven simulation tool called SPASM, which provides a way of isolating and quantifying the di erent parallel system overheads in a non-intrusive manner. The rst exercise shows that in shared memory machines with private caches, as long as the applications are well-structured to exploit locality, the key determinant that impacts performance is network contention. The second exercise quanti es the network bandwidth needed to minimize the e ect of network contention. Speci cally, it is shown that for the applications considered, as long as the problem sizes are increased commensurate with the system size, current network technologies supporting 200300 MBytes/sec link bandwidth are su cient to keep the network overheads (such as latency and contention) within acceptable bounds.	binocular disparity;locality of reference;network topology;requirement;shared memory;simulation;systems theory	Anand Sivasubramaniam;Aman Singla;Umakishore Ramachandran;H. Venkateswaran	1999	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.755819	shared memory;parallel computing;network traffic control;real-time computing;computer science;virtual machine;operating system;distributed computing;computer network	Arch	-10.369429327024108	48.791427577276	152361
69be7031de28f9ae6e846a2da96508ebeb784e24	supporting concurrent memory access and multioperations in moving threads cmps	memory access			Martti Forsell;Ville Leppänen	2010			uniform memory access;shared memory;interleaved memory;real-time computing;computer hardware;computer science;non-uniform memory access	HPC	-10.91662618271614	48.92549874608389	152476
bbea37a83cadc1680905c36ce59628383991704e	architectural support for enhanced smt job scheduling	new thread scheduling algorithm;thread behavior;expected resource utilization;smt scheduling;smt machine;job scheduling;architectural support;group thread;enhanced smt job scheduling;fine-grain memory system activity;cache activity vector;scheduling technique;resource utilization;instruction sets;linux;multi threading;resource allocation;simultaneous multithreading;operating system;scheduling algorithm;thread level parallelism	By converting thread-level parallelism to instruction level parallelism, Simultaneous Multithreaded (SMT) processors are emerging as effective ways to utilize the resources of modern superscalar architectures. However, the full potential of SMT has not yet been reached as most modern operating systems use existing single-thread or multiprocessor algorithms to schedule threads, neglecting contention for resources between threads. To date, even the best SMT scheduling algorithms simply try to group threads for co-residency based on each thread's expected resource utilization but do not take into account variance in thread behavior. As such, we introduce architectural support that enables new thread scheduling algorithms to group threads for co-residency based on fine-grain memory system activity information. The proposed memory monitoring framework centers on the concept of a cache activity vector, which exposes runtime cache resource information to the operating system to improve job scheduling. Using this scheduling technique, we experimentally evaluate the overall performance improvement of workloads on an SMT machine compared against the most recent Linux job scheduler. This work is first motivated with experiments in a simulated environment, then validated on a Hyperthreading-enabled Intel Pentium-4 Xeon microprocessor running a modified version of the latest Linux Kernel.	job scheduler;job shop scheduling;scheduling (computing)	Alex Settle;Joshua L. Kihm;Andrew Janiszewski;Daniel A. Connors	2004		10.1109/PACT.2004.10024	computer architecture;in situ resource utilization;parallel computing;real-time computing;multithreading;gang scheduling;resource allocation;computer science;job scheduler;operating system;instruction set;green threads;scheduling;simultaneous multithreading;linux kernel;task parallelism	HPC	-8.785820827831147	49.95912696085752	152965
6c4f4c0ffbec55d1f0e70737b812c0f589991d06	application performance impact on trimming of a full fat tree infiniband fabric	topology;kernel;parallel processing computer network performance evaluation;fat tree;fabrics switches atmospheric modeling computational modeling meteorology numerical models kernel;infiniband;computational modeling;trimmed fat tree options full fat tree infiniband fabric infiniband traffic measurement traffic pattern analysis application performance impact hpc systems high performance computing systems;fat tree infiniband topology;fabrics;atmospheric modeling;numerical models;switches;meteorology	We measured InfiniBand traffic in our full fat tree fabric and measured performance impact of trimming the fabric on our major application kernels. Based on traffic pattern analysis and application performance impact we infer that a 2:1 trimmed fat tree is a cost effective alternative to a full fat tree for this specific set of applications. The methodology we used may be useful for others who are performing design trade-offs for HPC systems. We also propose that switch hardware vendors design director class switches with trimmed fat tree options that optimize per port costs.	british undergraduate degree classification;fat tree;infiniband;network switch;pattern recognition	Siddhartha Sankar Ghosh;Davide Del Vento;Rory C. Kelly;Irfan Elahi;Nathan Rini;Benjamin Matthews;Storm Knights;Thomas Engel;Ben Jamroz;Shawn Strande	2016	2016 2nd IEEE International Workshop on High-Performance Interconnection Networks in the Exascale and Big-Data Era (HiPINEB)	10.1109/HIPINEB.2016.14	embedded system;parallel computing;real-time computing;computer science	HPC	-9.238411718472861	47.913605750206166	153060
67cf1189c859d66bac309f9438df434fb651f97a	cache coherence protocol and memory performance of the intel haswell-ep architecture	peer to peer computing protocols coherence sockets benchmark testing bidirectional control bridges;protocols;bridges;memory performance;sockets;patents cache coherence protocol memory performance intel haswell ep architecture contemporary microprocessor design memory subsystem memory hierarchy on chip communication cache coherence mechanisms coherence state control memory location architectural properties memory latency bandwidth characteristics core to core transfers;protocols cache storage integrated circuit design microprocessor chips multiprocessing systems;bidirectional control;cache coherence;coherence;numa;numa cache coherence memory performance;peer to peer computing;benchmark testing	A major challenge in the design of contemporary microprocessors is the increasing number of cores in conjunction with the persevering need for cache coherence. To achieve this, the memory subsystem steadily gains complexity that has evolved to levels beyond comprehension of most application performance analysts. The Intel Has well-EP architecture is such an example. It includes considerable advancements regarding memory hierarchy, on-chip communication, and cache coherence mechanisms compared to the previous generation. We have developed sophisticated benchmarks that allow us to perform in-depth investigations with full memory location and coherence state control. Using these benchmarks we investigate performance data and architectural properties of the Has well-EP micro-architecture, including important memory latency and bandwidth characteristics as well as the cost of core-to-core transfers. This allows us to further the understanding of such complex designs by documenting implementation details the are either not publicly available at all, or only indirectly documented through patents.	benchmark (computing);best, worst and average case;cas latency;cpu cache;cache coherence;central processing unit;control system;data dredging;directory (computing);elegant degradation;expectation propagation;haswell (microarchitecture);mathematical optimization;memory address;memory hierarchy;microarchitecture;microprocessor;non-uniform memory access;overhead (computing);performance prediction;performance tuning;scalability;shared memory;software documentation;snoop	Daniel Molka;Daniel Hackenberg;Robert Schöne;Wolfgang E. Nagel	2015	2015 44th International Conference on Parallel Processing	10.1109/ICPP.2015.83	bus sniffing;uniform memory access;distributed shared memory;shared memory;communications protocol;benchmark;cache coherence;interleaved memory;computer architecture;snoopy cache;parallel computing;real-time computing;cache coloring;coherence;cpu cache;computer science;write-once;cache invalidation;operating system;smart cache;memory coherence;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;memory map;non-uniform memory access	Arch	-9.449083044524418	49.616319682949666	153274
6b8408178ac4aa1ca8412def1f6c88791a5d00dc	the need for fast communication in hardware-based speculative chip multiprocessors	speculative multithreading;hardware yarn registers costs;chip multiprocessor;chip;synchronisation;parallel architectures;data dependence;speculative execution;data dependence speculation;cost effectiveness;multiprocessing systems;parallel architectures multiprocessing systems synchronisation;high performance;register communication;workloads fast communication hardware based speculative chip multiprocessors chip multiprocessor architectures speculative execution data dependent threads cross thread dependences compiler synchronization register level memory level wide issue dynamic processors high performance hardware mechanism on chip processors	Chip-multiprocessor (CMP) architectures are a promising design alternative to exploit the ever-increasing number of transistors that can be put on a die. To deliver high performance on applications that cannot be easily parallelized, CMPs can use additional support for speculatively executing the possibly data-dependent threads of an application. While some of the cross-thread dependences in applications must be handled dynamically, others can be fully determined by the compiler. For the latter dependences, the threads can be made to synchronize and communicate either at the register level or at the memory level. In the past, it has been unclear whether the higher hardware cost of register-level communication is cost-effective. In this paper, we show that the wide-issue dynamic processors that will soon populate CMPs, make fast communication a requirement for high performance. Consequently, we propose an effective hardware mechanism to support communication and synchronization of registers between on-chip processors. Our scheme adds enough support to enable register-level communication without specializing the architecture so much toward speculation that it leads to much unutilized hardware under workloads that do not need speculative parallelization. Finally, the scheme allows the system to achieve near ideal performance.	speculative execution	Venkata Krishnan;Josep Torrellas	1999		10.1109/PACT.1999.807402	chip;synchronization;computer architecture;parallel computing;real-time computing;cost-effectiveness analysis;computer science;operating system;speculative multithreading;speculative execution	Arch	-8.0276608340228	51.579858546949076	156416
45becf9db6725dbd7b24426e324d89b06ff31564	adaptive in-cache streaming for efficient data management		The design of adaptive architectures is frequently focused on the sole adaptation of the processing blocks, often neglecting the power/performance impact of data transfers and data indexing in the memory subsystem. In particular, conventional address-based models, supported on cache structures to mitigate the memory wall problem, often struggle when dealing with memory-bound applications or arbitrarily complex data patterns that can be hardly captured by prefetching mechanisms. Stream-based techniques have proven to efficiently tackle such limitations, although not well-suited to handle all types of applications. To mitigate the limitations of both communication paradigms, an efficient unification is herein proposed, by means of a novel in-cache stream paradigm, capable of seamlessly adapting the communication between the address-based and stream-based models. The proposed morphable infrastructure relies on a new dynamic descriptor graph specification, capable of handling regular arbitrarily complex data patterns, which is able to improve the main memory bandwidth utilization through data reutilization and reorganization techniques. When compared with state-of-the-art solutions, the proposed structure offers higher address generation efficiency and achievable memory throughputs, and a significant reduction of the amount of data transfers and main memory accesses, resulting on average in 13 times system performance speedup and in 245 times energy-delay product improvement, when compared with the previous implementations.	cpu cache;clock signal;computer data storage;manycore processor;memory bandwidth;memory bound function;memory controller;multi-core processor;prefetcher;programme delivery control;programming paradigm;random-access memory;re-order buffer;speedup;steady state;stride scheduling;unification (computer science)	Nuno Neves;Pedro Tom&#x00E1;s;Nuno Roma	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2017.2671405	flat memory model;real-time computing;memory management;computer science;cache;uniform memory access;physical address;interleaved memory;memory bandwidth;shared memory	Arch	-10.125080488587749	51.72975667430258	156652
f51c76a564a9d0a5fccd4f07d4fa698f7e1b542d	run-time versus compile-time instruction scheduling in superscalar (risc) processors: performance and tradeoffs	instruction level parallel;target processor;processor scheduling;reduced instruction set computing;compile time instruction scheduling;spectrum;runtime;design optimization;process design;basic block scheduling;computer architecture;scheduling algorithm;runtime processor scheduling hardware reduced instruction set computing dynamic scheduling performance gain design optimization optimizing compilers process design;lookahead hardware;computer architecture processor scheduling reduced instruction set computing;superscalar processors;performance gain;program trace;target processor compile time instruction scheduling run time instruction scheduling instruction scheduling superscalar processors risc processors program trace basic block scheduling lookahead hardware;risc processors;run time instruction scheduling;instruction scheduling;optimizing compilers;dynamic scheduling;hardware	The RISC revolution has spurred the development of processors with increasing degrees of instruction level parallelism (ILP). In order to realize the full potential of these processors, multiple instructions must continuously be issued and executed in a single cycle. Consequently, instruction scheduling plays a crucial role as an optimization in this context. While early attempts at instruction scheduling were limited to compile-time approaches, the current trends are aimed at providing dynamic support in hardware. In this paper, we present the results of a detailed comparative study of the performance advantages to be derived by the spectrum of instruction scheduling approaches: from limited basic-block schedulers in the compiler, to novel and aggressive schedulers in hardware. A significant portion of our experimental study via simulations, is devoted to understanding the performance advantages of run-time scheduling. Our results indicate it to be effective in extracting the ILP inherent to the program trace being scheduled, over a wide range of machine and program parameters. Furthermore, we also show that this effectiveness can be further enhanced by a simple basic-block scheduler in the compiler, which optimizes for the presence of the run-time scheduler in the target; current basic-block schedulers are not designed to take advantage of this feature. We demonstrate this fact by presenting a novel basic-block scheduling algorithm that is sensitive to the lookahead hardware in the target processor.	central processing unit;compile time;compiler;instruction scheduling;scheduling (computing);superscalar processor	Allen Leung;Krishna V. Palem;Cristian Ungureanu	1996		10.1109/HIPC.1996.565826	fair-share scheduling;fixed-priority pre-emptive scheduling;reduced instruction set computing;computer architecture;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;instruction scheduling;round-robin scheduling	Arch	-5.62281727952317	51.39702094706505	156931
3da4f5d052b3e04346507cf16471984e74fe63cb	a practical method for estimating performance degradation on multicore processors, and its application to hpc workloads	computer centres;human factors;learning (artificial intelligence);multi-threading;multiprocessing systems;parallel processing;resource allocation;workstation clusters;hpc cluster operators;hpc workloads;data center consolidation;distributed applications;energy saving;execution perturbation;machine learning model;multicore cpu;multicore processors;multiple threads;online degradation;performance degradation estimation;resource sharing;workload consolidation	When multiple threads or processes run on a multi-core CPU they compete for shared resources, such as caches and memory controllers, and can suffer performance degradation as high as 200%. We design and evaluate a new machine learning model that estimates this degradation online, on previously unseen workloads, and without perturbing the execution.  Our motivation is to help data center and HPC cluster operators effectively use workload consolidation. Data center consolidation is about placing many applications on the same server to maximize hardware utilization. In HPC clusters, processes of the same distributed applications run on the same machine. Consolidation improves hardware utilization, but may sacrifice performance as processes compete for resources. Our model helps determine when consolidation is overly harmful to performance. Our work is the first to apply machine learning to this problem domain, and we report on our experience reaping the advantages of machine learning while navigating around its limitations. We demonstrate how the model can be used to improve performance fidelity and save energy for HPC workloads.	cpu cache;central processing unit;computer cluster;computer performance;data center;distributed computing;elegant degradation;kerrison predictor;machine learning;mean squared error;microarchitecture;multi-core processor;problem domain;semiconductor consolidation;server (computing);test set	Tyler Dwyer;Alexandra Fedorova;Sergey Blagodurov;Mark Roth;Fabien Gaud;Jian Pei	2012	2012 International Conference for High Performance Computing, Networking, Storage and Analysis		embedded system;parallel processing;fault tolerance;parallel computing;real-time computing;multithreading;resource allocation;computer science;operating system;distributed computing;fault detection and isolation	HPC	-8.171435370039307	49.614242972712894	157171
411fb90fa4dd6db983bf3627fa15c500a39b151b	experimental evaluation of gpus radiation sensitivity and algorithm-based fault tolerance efficiency	radiation effects;error correction codes;gpu internal resource sensitivity gpu radiation sensitivity algorithm based fault tolerance efficiency experimental evaluation graphic processing units error rate parallel algorithms output error patterns radiation responses design optimized algorithm based fault tolerance strategies code reliability computational overhead isis uk lansce los alamos nm usa;performance evaluation;instruction sets graphics processing units reliability error correction codes neutrons parallel processing sensitivity;radiation effects computational complexity error correction codes fault tolerant computing graphics processing units parallel algorithms performance evaluation;fault tolerant computing;computational complexity;graphics processing units;software based hardening gpu neutron sensitivity multiple errors;parallel algorithms	Experimental results demonstrate that Graphic Processing Units are very prone to be corrupted by neutrons. We have performed several experimental campaigns at ISIS, UK and at LANSCE, Los Alamos, NM, USA accessing the sensitivity of the GPU internal resources as well as the error rate of common parallel algorithms. Experiments highlight output error patterns and radiation responses that can be fruitfully used to design optimized Algorithm-Based Fault Tolerance strategies and provide pragmatic programming guidelines to increase the code reliability with low computational overhead.	fault tolerance;graphics processing unit;isis;overhead (computing);parallel algorithm	Paolo Rech;Luigi Carro	2013	2013 IEEE 19th International On-Line Testing Symposium (IOLTS)	10.1109/IOLTS.2013.6604091	embedded system;electronic engineering;parallel computing;real-time computing;computer science;theoretical computer science;operating system;parallel algorithm;programming language;computational complexity theory;algorithm	Embedded	-8.841421168945947	48.623684867014354	157450
51e43a2784d6aeed3e9d0d64bb57943e948d620b	improving memory traffic by assembly-level exploitation of reuses for vector registers	shared memory;vector compilers;data locality;supercomputer;vectorization;data dependence;vector register;partial reuse;reuse distance	In this paper, we propose a compilation scheme to analyze and exploit the implicit reuses of vector register data. According to the reuse analysis, we present a translation strategy that translates the vectorized loops into assembly vector codes with exploitation of vector reuses. Experimental results show that our compilation technique can improve the execution time and traffic between shared memory and vector registers. Techniques discussed here are simple, systematic, and easy to be implemented in the conventional vector compilers or translators to enhance the data locality of vector registers.	algorithm;assembly language;code;compiler;data dependency;entity–relationship model;locality of reference;processor register;run time (program lifecycle phase);shared memory	Chih-Yung Chang;Tzung-Shi Chen;Jang-Ping Sheu	2000	The Journal of Supercomputing	10.1023/A:1008134522009	shared memory;supercomputer;parallel computing;real-time computing;computer science;theoretical computer science;operating system;vectorization	HPC	-6.286764706947612	49.54265993558264	157852
3c8199949448adb38ed115bfeb19f8babdc849ae	data layout transformation for structure vectorization on simd architectures	optimizing compiler open64 structure vectorization simd architectures data layout transformation technology static analysis technique;layout;arrays;indexes;program diagnostics parallel processing program compilers;simd structure reference data layout transformation vectorization;decision support systems;transforms;switches;benchmark testing;decision support systems arrays layout switches indexes transforms benchmark testing	Structure references are commonly-used at the core of applications in a multitude of domains such as image processing, signal processing, especially the scientific and engineering applications. SIMD instruction sets, as SSE, AVX, AltiVec and 3DNow, provide a promising and widely available avenue for enhancing performance on modern processors. However existing memory accessing shackles limit the achieved performance for structure reference on modern SIMD architectures. In this paper, we propose a novel data layout transformation technology that addresses the accessing obstacles, along with a static analysis technique for detecting the legal loops in where this transformation is suitable. And this approach is implemented in the Optimizing Compiler Open64. The experimental results show that the proposed method can translate application with structure access into vectorizable codes, thereby advancing the execution efficiency adequately.	3dnow!;advanced vector extensions;altivec;automatic vectorization;central processing unit;code;image processing;open64;optimizing compiler;sensor;signal processing;static program analysis;streaming simd extensions	Peng-yuan Li;Qing-hua Zhang;Rong-cai Zhao;Hai-ning Yu	2015	2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2015.7176233	layout;database index;benchmark;computer architecture;parallel computing;decision support system;network switch;computer science;theoretical computer science;operating system;database;programming language	SE	-4.717869066659274	47.42966713597602	158201
e163821b12cd529b9c11a714d3beaacaa0acc63a	supporting concurrent memory access in tcf-aware processor architectures		The Thick Control Flow (TCF) model packs together self-similar computations to simplify parallel programming and to eliminate redundant usage of associated software and hardware resources. While there are processor architectures supporting native execution ofprograms written for the model, none of them support concurrent memory access that can speed up execution of many algorithms by a logarithmic factor. In this paper, we propose an architectural solution implementing concurrent memory access for TCF-aware processors. The solution is based on bounded size step caches and two-phase structure of the TCF-aware processors. Step caches capture and hold the references made during the on-going step of an execution that are independent by the definition of TCF execution and therefore avoid coherence problems. The 2-phase structure reduces some concurrent accesses to a frontend operation followed by broadcast in the spreading network. According to our evaluation, a concurrent memory access-aware B-backend unit TCF processor executes certain algorithms up to B times faster than the baseline TCF processor.	algorithm;baseline (configuration management);central processing unit;computation;control flow;machine code;parallel computing;self-similarity;tor carding forum;two-phase locking	Martti Forsell;Jussi Roivainen;Ville Leppänen;Jesper Larsson Träff	2017	2017 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)	10.1109/NORCHIP.2017.8124962	parallel computing;real-time computing;control flow;programming paradigm;software;computer science;computation;speedup;bounded function;microarchitecture	Arch	-7.41665991495135	48.242156986042296	158454
45422329d6275195c4fda90eedf757ffbcd90eb4	a filtering mechanism to reduce network bandwidth utilization of transaction execution	energy efficiency;network traffic;communication cost modeling;on chip network;transactional memory	Hardware Transactional Memory (HTM) relies heavily on the on-chip network for intertransaction communication. However, the network bandwidth utilization of transactions has been largely neglected in HTM designs. In this work, we propose a cost model to analyze network bandwidth in transaction execution. The cost model identifies a set of key factors that can be optimized through system design to reduce the communication cost of HTM. Based on the model and network traffic characterization of a representative HTM design, we identify a huge source of superfluous traffic due to failed requests in transaction conflicts. As observed in a spectrum of workloads, 39% of the transactional requests fail due to conflicts, which renders 58% of the transactional network traffic futile. To combat this pathology, a novel in-network filtering mechanism is proposed. The on-chip router is augmented to predict conflicts among transactions and proactively filter out those requests that have a high probability to fail. Experimental results show the proposed mechanism reduces total network traffic by 24% on average for a set of high-contention TM applications, thereby reducing energy consumption by an average of 24%. Meanwhile, the contention in the coherence directory is reduced by 68%, on average. These improvements are achieved with only 5% area added to a conventional on-chip router design.	analysis of algorithms;computer architecture simulator;directory (computing);html;network on a chip;network packet;network traffic control;overhead (computing);rendering (computer graphics);requests;router (computing);simulation;systems design;transactional memory	Lihang Zhao;Lizhong Chen;Woojin Choi;Jeffrey T. Draper	2016	TACO	10.1145/2837028	transactional memory;parallel computing;network traffic control;real-time computing;computer science;operating system;distributed computing;efficient energy use;programming language;computer security	Arch	-8.627325979815124	53.037911395327896	158483
f00988e07f1b0f3fc87ec8ab88b1dc1f9e885371	evaluating schedulers in a reconfigurable multicore heterogeneous system	reconfigurable architectures;multicore;scheduling;heterogeneity	The use of heterogeneous multicore processors is getting extremely common, and those that comprise reconfigurable logic are becoming an attractive alternative. However, to leverage them as much as possible to speed up applications, an effective scheduler, which can adequately distribute threads to cores with different processing capabilities, is required. Therefore, this work evaluates the implementation of three distinct scheduling approaches aiming at a reconfigurable heterogeneous architecture: a static algorithm that allocates the threads on the first free core, where they will run during the entire execution; an Instruction Count IC Driven scheduler, which reallocates threads during synchronization points accordingly to their instruction count; and an Oracle scheduler, which is capable of deciding the best thread allocation possible. The goal is to determine the necessity of scheduling algorithms and the potential of reconfigurable heterogeneous architectures. We show that a static scheduler might be used with minimal loss 0.01i¾?% in performance when applications have high load balancing between threads. For unbalanced applications, the static scheduler can reach only 40i¾?% of the Oracle's performance. On the other hand, the dynamic scheduler varies from 97i¾?% to 35i¾?% of the Oracle's performance in similar conditions.	multi-core processor	Jeckson Dellagostin Souza;João Victor Gomes Cachola;Luigi Carro;Mateus B. Rutzig;Antonio Carlos Schneider Beck	2016		10.1007/978-3-319-30481-6_21	multi-core processor;embedded system;scheduler activations;parallel computing;real-time computing;computer science;heterogeneity;operating system;distributed computing;scheduling	EDA	-5.88573597537835	52.786377182893624	158948
839cd5729bd4286902d9117d73362ba5223e3027	design of a bus-based shared-memory multiprocessor dice	memory replacement;chip;texas instruments;large scale;coherence protocol;shared bus;cache coherence;distributed shared memory;high performance;cache only memory architecture;shared memory multiprocessor	DICE is a shared-bus multiprocessor based on a distributed shared-memory architecture, known as cache-only memory architecture (COMA). Unlike previous COMA proposals for large-scale multiprocessing, DICE utilizes COMA to effectively decrease the speed gap between modem high-performance microprocessors and the bus. DICE tries to optimize COMA for a shared-bus medium, in particular to reduce detrimental effects of the cache coherence and the ‘last memory block’ problem on replacement. In this paper, we present a global bus design of DICE based on the IEEE futurebus 1 backplane bus and the Texas Instruments chip-set. Our design demonstrates that necessary bus transactions for DICE can be done efficiently with existing standard bus signals. Considering the benefits of COMA and the moderate design complexity it adds to the conventional shared-bus multiprocessor design, a bus-based COMA multiprocessor, such as DICE, can become a viable candidate for future shared-bus multiprocessor designs. q 1999 Elsevier Science B.V. All rights reserved.	backplane;bus (computing);cache coherence;cache-only memory architecture;data dredging;futurebus;microprocessor;modem;multiprocessing;network traffic control;overhead (computing);page replacement algorithm;processor technology;relocation (computing);shared memory	Gyungho Lee;Bland Quattlebaum;Sangyeun Cho;Larry L. Kinney	1999	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(98)00097-0	chip;bus sniffing;distributed shared memory;embedded system;cache coherence;parallel computing;real-time computing;distributed memory;computer science;local bus;operating system;system bus;control bus;back-side bus;address bus;cache-only memory architecture	Arch	-8.837237816117781	52.06212089960502	159771
8bd18ceaca0542851c0863a0bd6990c4f7a5c95f	application-based early performance evaluation of sgi altix 4700 systems for sgi systems	benchmarking;single system image;new technology;performance evaluation;climate model;computation fluid dynamics;network performance;memory performance;system performance;input output;real world application;high performance computer;performance analysis;next generation;ames research center;sgi altix 4700;nas parallel benchmarks;supercomputing	The suitability of the next generation of high performance computing systems for petascale simulations will depend on a balance between various factors such as processor performance, memory performance, local and global network performance, and Input/Output (I/O) performance. As the supercomputing industry develops new technologies for these subsystems, achieving system balance becomes challenging. In this paper we evaluate the performance of newly introduced dual-core based SGI Altix 4700 systems (both Bandwidth and Density models) and we compare their performance with that of a single-core based SGI Altix 3700 Bx2 system. The SGI Altix 4700 Density system installed in October 2007 at NASA Ames Research Center is the largest 2048-processor single system image (SSI) system in the world. We used the High Performance Computing Challenge (HPCC) benchmark, NAS Parallel benchmarks (NPB) and five real-world applications, three from computational fluid dynamics, one from climate modeling and one from nanotechnology. Our study shows that the SGI Altix 4700 Bandwidth system performs slightly better and SGI Altix 4700 Density system performs slightly worse than the SGI Altix 3700 Bx2 up to 128 processors, while the performance of the systems is almost the same beyond 128 processors, when the communication time dominates the compute time.	benchmark (computing);central processing unit;climate model;computation;computational fluid dynamics;global network;hpcc;input/output;multi-core processor;nas parallel benchmarks;network performance;performance evaluation;petascale computing;simulation;single system image;single-core;supercomputer	Subhash Saini;Dennis C. Jespersen;Dale Talcott;M. Jahed Djomehri;Timothy Sandstrom	2008		10.1145/1366230.1366251	computer architecture;parallel computing;real-time computing;computer science	HPC	-9.669949246504205	46.39313376636266	160172
bdde4392bb6be04c04f37ff2ce84e387910156d6	quantitative application data flow characterization for heterogeneous multicore architectures		r ecent trends show a steady increase in the utilization of heterogeneous multicore architectures in order to address the ever-growing need for computing performance. ese emerging architectures pose specific challenges with regard to their programmability. In addition, they require efficient application mapping schemes to fully harness their processing power and avoid bolenecks. In this respect, it is of critical importance to analyse application behaviour, and the data communication between tasks, in particular. In this dissertation, we present a profiling framework that helps developers to gain an insight into the behaviour of an application. e presented profiling framework is generic and not restricted to a particular platform, application, or purpose. We utilize this framework with the primary goal of mapping applications onto a heterogeneous multicore architecture. e framework includes amemory access profiling toolset, called Quad, that provides quantitative information regarding the memory accesses in an application. Quad utilizes Dynamic Binary Instrumentation (DBI) to detect the actual data dependencies that occur between the tasks of an application at runtime. Additionally, it also provides accurate memory access measurements, such as the amount of data transferred between tasks and the memory size required for their communication. Such information can be utilized to identify critical parts of an application, to highlight coarsegrained parallelism opportunities, and to guide code optimizations. As a proof of concept to substantiate the usefulness of the extracted profiling information, we utilize the main output of Quad, the antitative Data Usage (QDU) graph, as the input model to formulate a general application partitioning problem. e formulation of this intractable problem is flexible and accommodates different design objectives and constraints. Subsequently, we propose a heuristic algorithm to find high quality partitions of an application in a reasonable amount of time. In addition to the complexity analysis of the proposed algorithm, we present a thorough theoretical analysis of the application partitioning problem. In order to evaluate the quality of the solutions, we developed a test bench for generating synthetic QDU graphs and compared the results against the optimal partitions obtained using an exhaustive search. e comparison results show that the proposed heuristic algorithm is able to provide optimal or near-optimal solutions. To further prove the applicability of the profiling framework, we investigate in detail the utilization of the framework in practice, by mapping two real applications onto	algorithm;analysis of algorithms;brute-force search;computational complexity theory;data dependency;dataflow architecture;display resolution;heuristic (computer science);multi-core processor;parallel computing;partition problem;perl dbi;profiling (computer programming);run time (program lifecycle phase);synthetic intelligence;task parallelism;test bench	Sayyed Arash Ostadzadeh	2012			parallel computing;real-time computing;computer science;theoretical computer science	HPC	-5.573425403753859	47.71178648228333	160526
b7ab3786a117a900cdda9e819bbf43cb540bce3e	fast, accurate and flexible data locality analysis	simulator flexible data locality analysis data memory references static locality analysis data structures memory instructions;compile based tools;data locality;memory architecture data structures;memory architecture;data analysis interference information analysis computer architecture electronic mail performance analysis dynamic compiler pattern analysis indexing sun;data structures;conference report;data locality analysis;data structure	This paper presents a tool based on a new approach for analyzing the locality exhibited by data memory references. The tool is very fast because it is based on a static locality analysis enhanced with very simple profiling information, which results in a negligible slowdown. This feature allows the tool to be used for highly time-consuming applications and to include it as a step in a typical iterative analysis-optimization process. The tool can provide a detailed evaluation of the reuse exhibited by a program, quantifying and qualifying the different types of misses either globally or detailed by program sections, data structures, memory instructions, etc. The accuracy of the tool is validated by comparing its results with those provided by a simulator.	locality of reference	F. Jesús Sánchez;Antonio González	1998		10.1109/PACT.1998.727182	locality of reference;parallel computing;data structure;computer science;theoretical computer science;operating system;database;programming language	HPC	-9.239659749426504	49.070316145465384	160762
7d6df0afc5121c77a759e69196fb4282b1d9c98b	logic design for a high performance mainframe computer, the hitac m-880 processor	online transaction processing hitac m 880 scalar processor mainframe computer packaging optimal pipeline stage evaluation cache access merge access processor performance;logic design;storage management;storage management buffer storage logic design microprocessor chips pipeline processing;buffer storage;pipeline processing;microprocessor chips;logic design high performance computing pipelines electric breakdown packaging logic circuits throughput cache storage laboratories transaction databases	Logic design and its effects on the HITAC M 8 8 0 basic scalar processor are described. The M-880 is a high end mainframe computer which employs both current high speed circuits and packaging technologies, as well as logic methods to improve performance. This paper especially focuses on and proposes an optimal pipeline stage evaluation method, together with a new cache access method termed merge access. The combined effect of the logic methoak is a 10% improvement in processor performance with online transaction processing.	hitac;logic synthesis;mainframe computer;online transaction processing;scalar processor	Yooichi Shintani;Kiyoshi Inoue;Toru Shonai;Keiji Wada;S. Abe;Katsuro Wakai	1991		10.1109/ICCD.1991.139833	embedded system;pipeline burst cache;computer architecture;parallel computing;logic synthesis;logic family;computer hardware;computer science;operating system	Arch	-7.834033610945032	52.6235886429468	161039
2800c99326d25abd034a2b255cacaa1384837cec	using kernel couplings to predict parallel application performance	analytical models;software performance evaluation parallel programming;coupling parameter;processor architecture;kernel;mathematics;application software;kernel analytical models predictive models application software performance analysis algebra mathematics computer science laboratories mathematical model;bt dataset;lu dataset parallel application performance coupling parameter performance predictions kernel couplings nas parallel benchmarks processor architecture bt dataset sp dataset;software performance evaluation;sp dataset;parallel application performance;parallel programming;performance predictions;relative error;lu dataset;algebra;performance analysis;performance model;mathematical model;kernel couplings;performance prediction;predictive models;computer science;nas parallel benchmarks;parallel applications	Performance models provide significant insight into the performance relationships between an application and the system used for execution. The major obstacle to developing performance models is the lack of knowledge about the performance relationships between the different functions that compose an application. This paper addresses the issue by using a coupling parameter, which quantifies the interaction between kernels, to develop performance predictions. The results, using three NAS Parallel Application Benchmarks, indicate that the predictions using the coupling parameter were greatly improved over a traditional technique of summing the execution times of the individual kernels in an application. In one case the coupling predictor had less than 1% relative error in contrast the summation methodology that had over 20% relative error. Further, as the problem size and number of processors scale, the coupling values go through a finite number of major value changes that is dependent on the memory subsystem of the processor architecture.	analysis of algorithms;approximation error;benchmark (computing);central processing unit;communications satellite;computer performance;coupling (computer programming);distributed computing;experiment;ibm notes;kernel (operating system);kerrison predictor	Valerie E. Taylor;Xingfu Wu;Jonathan Geisler;Rick L. Stevens	2002		10.1109/HPDC.2002.1029910	approximation error;application software;parallel computing;kernel;microarchitecture;computer science;theoretical computer science;operating system;machine learning;mathematical model;predictive modelling	HPC	-9.101654959248119	47.95840900770728	161407
f0d71498042174b953ca59d92d501f0a18e71c8c	transparent memory: a hardware solution to the memory conflict problem		Abstract#R##N##R##N#The paper proposes a memory organization aiming at reduction of memory conflict produced by the multiple access. The proposed memory organization has the effect of suppressing not only the memory conflicts but also the write memory conflicts. The conflict of read access is suppressed by using more than one memory module of the same content, and the conflict of write access is suppressed by restricting the write area and using the distributed communication mechanism. It is shown by the performance evaluation simulation that the memory conflict is sufficiently small compared with the number of multiple read memory and approaches a constant with the increase of the number of processors n. The hardware complexity is relatively large. The physical memory is n times compared with the logical address space and the number of memory control circuits is n2. There is one connection between the memory control circuit and the connection network. By specifying the area of applications and simplifying the connection configuration, the hardware complexity can be reduced.		Takumi Hisano	1986	Systems and Computers in Japan	10.1002/scj.4690171110	auxiliary memory;uniform memory access;distributed shared memory;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;sense amplifier;memory refresh;computer science;physical address;theoretical computer science;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;memory map;memory management	EDA	-11.68385216585365	48.54436309415412	161501
33f151380269bb334994d3928f011a97020e895b	design and analysis of 3d-maps (3d massively parallel processor with stacked memory)	physical structures;rtl implementation and simulation;computer aided design;3d integrated circuits;clocks;layout;placement and routing;power 4 w 3d maps 3d massively parallel processor with stacked memory two tier 3d stacking technology face to face bond pads parallel benchmarks frequency 277 mhz;3d multiprocessor memory stacked systems;microprocessors and microcomputers;three dimensional displays;multicore processing;storage management parallel processing;rtl implementation and simulation 3d multiprocessor memory stacked systems 3d integrated circuits computer aided design;bandwidth;three dimensional displays bandwidth through silicon vias multicore processing;tiles;article;parallel processors;through silicon vias	This paper describes the architecture, design, analysis, and simulation and measurement results of the 3D-MAPS (3D massively parallel processor with stacked memory) chip built with a 1.5 V, 130 nm process technology and a two-tier 3D stacking technology using 1.2 \microm-diameter, 6 \micro m-height through-silicon vias (TSVs) and 3.4\nbsp\microm-diameter face-to-face bond pads. 3D-MAPS consists of a core tier containing 64 cores and a memory tier containing 64 memory blocks. Each core communicates with its dedicated 4KB SRAM block using face-to-face bond pads, which provide negligible data transfer delay between the core and the memory tiers. The maximum operating frequency is 277 MHz and the maximum memory bandwidth is 70.9 GB/s at 277 MHz. The peak measured memory bandwidth usage is 63.8 GB/s and the peak measured power is approximately 4 W based on eight parallel benchmarks.	bond;clock rate;gigabyte;goodyear mpp;kilobyte;memory bandwidth;multitier architecture;parallel computing;simulation;stacking;static random-access memory;three-dimensional integrated circuit	Daehyun Kim;Krit Athikulwongse;Michael B. Healy;Mohammad M. Hossain;Moongon Jung;Ilya Khorosh;Gokul Kumar;Young-Joon Lee;Dean L. Lewis;Tzu-Wei Lin;Chang Liu;Shreepad Panth;Mohit Pathak;Minzhen Ren;Guanhao Shen;Taigon Song;Dong Hyuk Woo;Xin Zhao;Joungho Kim;Ho Choi;Gabriel H. Loh;Hsien-Hsin S. Lee	2015	IEEE Transactions on Computers	10.1109/TC.2013.192	multi-core processor;layout;embedded system;computer architecture;semiconductor memory;parallel computing;real-time computing;memory bank;computer science;operating system;computer aided design;memory controller;conventional memory;programming language;registered memory;memory bandwidth;bandwidth;computing with memory;statistics;non-uniform memory access	Arch	-6.798142237670864	52.563794858257076	161565
e80ccbd999f8f20ef8f06619e99073c6fca995b8	elastic-cache: gpu cache architecture for efficient fine- and coarse-grained cache-line management		GPUs provide high-bandwidth/low-latency on-chip shared memory and L1 cache to efficiently service a large number of concurrent memory requests (to contiguous memory space). To support warp-wide accesses to L1 cache, GPU L1 cache lines are very wide. However, such L1 cache architecture cannot always be efficiently utilized when applications generate many memory requests with irregular access patterns especially due to branch and memory divergences. In this paper, we propose Elastic-Cache that can efficiently support both fine- and coarse-grained L1 cache-line management for applications with both regular and irregular memory access patterns. Specifically, it can store 32- or 64-byte words in non-contiguous memory space to a single 128-byte cache line. Furthermore, it neither requires an extra tag storage structure nor reduces the capacity of L1 cache since it stores auxiliary tags for fine-grained L1 cache-line managements in sharedmemory space that is not fully used in many applications. Our experiment shows that Elastic-Cache improves the geo-mean performance of applications with irregular memory access patterns by 58% without degrading performance of applications with regular memory access patterns.	baseline (configuration management);cpu cache;cache (computing);dspace;graphics processing unit;ibm notes;locality of reference;principle of locality;shared memory	Bingchao Li;Jizhou Sun;Murali Annavaram;Nam Sung Kim	2017	2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2017.81	cache coloring;parallel computing;cache;page cache;computer science;cache-only memory architecture;real-time computing;uniform memory access;mesif protocol;cache pollution;cache algorithms	Arch	-10.22964820973196	51.77685702598826	161968
3441fdee4b7a3aec2e0c41b72ac51ed54f9351f1	prediction outcome history-based confidence estimation for load value prediction	memory access;prediction accuracy;value prediction	Load instructions occasionally incur very long latencies that can significantly affect system performance. Load value prediction alleviates this problem by allowing the CPU to speculatively continue processing without having to wait for the slow memory access to complete. Current load value predictors can only correctly predict about forty to seventy percent of the fetched load values. To avoid the cycle-penalty for mispredictions in the remaining cases, confidence estimators are employed. They inhibit all predictions that are not likely to be correct. In this paper we present a novel confidence estimator that is based on prediction outcome histories. Profiles are used to identify the high-confidence history patterns. Our confidence estimator is able to trade off coverage for accuracy and vice-versa with great flexibility and reaches an average prediction accuracy over SPECint95 of as high as 99.3%. Cycle-accurate pipeline-level simulations show that a simple last value predictor combined with our confidence estimator outperforms other predictors, sometimes by over 100%. Furthermore, this predictor is one of two predictors that yield a genuine speedup for all eight SPECint95 programs.	branch misprediction;branch predictor;central processing unit;digital history;heart rate variability;kaplan–meier estimator;kerrison predictor;microprocessor;simulation;speedup;value (ethics)	Martin Burtscher;Benjamin G. Zorn	1999	J. Instruction-Level Parallelism		computer science;data mining;statistics	Arch	-7.073077721275556	51.493640573768175	161983
90e81bf1051d84bfe3fb000b57d59b3b5ccb6100	measuring memory access latency for software objects in a numa system-on-chip architecture	soclib memory access latency software objects numa system on chip architecture on chip memory multi processor system on chip mpsoc nonuniform memory access packet processing video streaming i o traffic;video streaming;video streaming multiprocessing systems storage management chips system on chip;software hardware system on chip space exploration data structures data mining;system on chip;storage management chips;multiprocessing systems	We consider streaming applications modeled as a set of tasks communicating via channels. These channels are mapped to on-chip memory of a multi-processor system on chip (MPSoC) with non-uniform memory access. In complex applications like advanced packet processing and video streaming, often only part of the data transits through the channels. Tasks also communicate via shared memory; synchronization mechanisms like locks and barriers might be required. Effects of I/O on the traffic on the interconnect also have to be taken into account, all together increasing traffic to and from memory. Our clustered MPSoC architecture is modeled with SoCLib. SocLib's design space exploration tool proposes, among others, communication channels and shared memory for inter-task communication. Each consists of one of several software objects which are mapped to on-chip memory. The difficulty when measuring latency is to find out which (co-)processor issued a request for a particular software object. We intervene early in the design process by monitoring the transfers on the interconnection network caused by the access to these software objects. We identify the software objects by name and trace the corresponding memory accesses. In spite of the cycle accurate bit accurate level of simulation, our method has little overhead and avoids distorting the performance results.	design space exploration;distortion;inter-process communication;interconnection;lock (computer science);mpsoc;multiprocessing;network packet;non-uniform memory access;overhead (computing);shared memory;simulation;streaming media;system on a chip;uniform memory access	Daniela Genius	2013	2013 8th International Workshop on Reconfigurable and Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2013.6581525	memory address;system on a chip;uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;computer science;physical address;operating system;computer memory;overlay;extended memory;registered memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-7.752285788779104	52.2815554572143	162103
14bf7ea288638aed16d4ca86ec268ab4f4e8f51a	reducing the performance gap of remote gpu virtualization with infiniband connect-ib	computers;graphics processing units virtualization bandwidth servers computers fabrics;virtualization;virtualisation graphics processing units parallel processing;servers;graphics processing units;fabrics;bandwidth;network bandwidth performance gap reduction remote gpu virtualization graphics processing unit infiniband connect ib parallel application high performance computing hpc cluster	GPU accelerators may provide great performance improvements in the context of parallel applications. However, their use in HPC clusters may also present some disadvantages such as their high cost and high power consumption. In addition, this kind of accelerators are generally underutilized. Remote GPU virtualization could be a solution to overcome these drawbacks, but its performance is usually impaired because the network bandwidth is lower than the PCIe one. In this paper we analyze how the InfiniBand Connect-IB network adapters (with performance similar to that of PCIe 3.0) reduce the overhead of remote GPU virtualization. We show that this overhead is decreased to 1.5% in terms of bandwidth, and to 0.51% in the tested application.	bandwidth (signal processing);computer cluster;data center;data rate units;graphics processing unit;infiniband;input/output;network interface controller;overhead (computing);pci express;parallel computing;x86 virtualization	Carlos Reaño;Federico Silla	2016	2016 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2016.7543854	embedded system;parallel computing;virtualization;computer science;operating system;bandwidth;server	HPC	-9.947272826892636	46.899589046414285	162924
6bd3b274a2117f706db87be25cf029c5960433ff	tornado warning: the perils of selective replay in multithreaded processors	branch misprediction;recovery;circuit complexity;processor state;checkpoint;simultaneous multithreading;coarse grained;instruction scheduling	"""As future technologies push towards higher clock rates, traditional scheduling techniques that are based on wake-up and select from an instruction window fail to scale due to their circuit complexities. Speculative instruction schedulers can significantly reduce logic on the critical scheduling path, but can suffer from instruction misscheduling that can result in wasted issue opportunities.Misscheduled instructions can spawn other misscheduled instructions, only to be replayed over again and again until correctly scheduled. These """"tornadoes"""" in the speculative scheduler are characterized by extremely low useful scheduling throughput and a high volume of wasted issue opportunities. The impact of tornadoes becomes even more severe when using Simultaneous Multithreading. Misschedulings from one thread can occupy a significant portion of the processor issue bandwidth, effectively starving other threads.In this paper, we propose Zephyr, an architecture that inhibits the formation of tornadoes. Zephyr makes use of existing load latency prediction techniques as well as coarse-grain FIFO queues to buffer instructions before entering scheduling queues. On average, we observe a 23% improvement in IPC performance, 60% reduction in hazards, 41% reduction in occupancy, and 48% reduction in the number of replays compared with a baseline scheduler."""	baseline (configuration management);central processing unit;clock rate;fifo (computing and electronics);hazard (computer architecture);instruction window;multithreading (computer architecture);scheduling (computing);simultaneous multithreading;spawn (computing);speculative execution;thread (computing);throughput;tornado;zephyr	Yongxiang Liu;Anahita Shayesteh;Gokhan Memik;Glenn Reinman	2005		10.1145/1088149.1088157	circuit complexity;embedded system;parallel computing;real-time computing;recovery;computer science;operating system;instruction scheduling;branch misprediction;simultaneous multithreading	HPC	-8.244382501589746	51.55193016489618	163205
241cd1617bbec9980e81a546f79327fdb80e9167	achieving a balanced low-cost architecture for mass storage management through multiple fast ethernet channels on the beowulf parallel workstation	multiple concurrent file transfer data rates;magnetic disc storage;parallel disk throughput;concurrent computing;workstation network;memory architecture parallel architectures parallel machines storage management local area networks telecommunication channels workstations magnetic disc storage;storage management;workstations costs computer architecture computer networks concurrent computing space technology bandwidth parallel processing throughput ethernet networks;price performance design space;computer networks;computer architecture;popc approach;parallel system architecture;parallel architectures;single user environment;parallel systems;memory architecture;mass storage management;workstations;network of workstation;message passing;high performance computing systems;beowulf;pile of pcs;bandwidth;internal network bandwidth;pc commodity mass market;parallel machines;linux;parallel i o;balanced low cost architecture;disk capacity;space technology;networks of workstations;100 mbit s balanced low cost architecture mass storage management multiple fast ethernet channels beowulf parallel workstation workstation network high performance computing systems pc commodity mass market price performance design space parallel system architecture pile of pcs popc approach disk capacity internal network bandwidth single user environment parallel disk throughput multiple concurrent file transfer data rates;distributed systems;telecommunication channels;multiple fast ethernet channels;ethernet networks;parallel processing;local area networks;100 mbit s;throughput;beowulf parallel workstation	"""Network-of-Workstations (NOW) seek to leverage commercial workstation technology to produce high performance computing systems at costs appreciably lower than parallel computers speciically designed for that purpose. The capabilities of technologies emerging from the PC commodity mass market are rapidly evolving to converge with those of workstations while at signiicantly lower cost. A new operating point in the price-performance design space of parallel system architecture may be derived through parallelism of PC subsystems. The Pile-of-PCs, PopC (pronounced \pop-see""""), approach is being explored through the Beowulf Parallel Workstation developed to provide order-of-magnitude increases in disk capacity and band-width for a single user environment at costs commensurate with conventional high-end workstations. This paper explores a critical aspect of the architecture trade-oo space for Beowulf associated with the balance of parallel disk throughput and internal network bandwidth. The ndings presented demonstrate that parallel channels of commodity 100 Mbps Ethernet are both necessary and suucient to support the data rates of multiple concurrent le transfers on a sixteen processor Beowulf parallel workstation."""	computer cluster;computer data storage;converge;data rate units;intranet;mass storage;operating point;parallel computing;supercomputer;systems architecture;throughput;user interface;workstation	Thomas L. Sterling;Donald J. Becker;Chance Reschke;Daniel Savarese;Michael R. Berry	1996		10.1109/IPPS.1996.508045	local area network;embedded system;parallel processing;throughput;parallel computing;message passing;workstation;concurrent computing;computer science;operating system;distributed computing;space technology;programming language;linux kernel;bandwidth;computer network	HPC	-10.889675652647837	47.175022187240096	163568
e1783f67febfa818f69b97458371f27b00765173	exploring multi-grained parallelism in compute-intensive devs simulations	massive fine grained event level parallelism;ibm cell processor;discrete event system specification multigrained parallelism compute intensive devs parallel simulation parallelization strategies massive fine grained event level parallelism optimization strategies 3d environmental model multicore architectures shared memory architectures ibm cell processor;optimisation;optimization strategies;yarn;parallelization strategies;concurrent computing;shared memory;multicore architectures;parallel architectures discrete event simulation multiprocessing systems optimisation;computational techniques;application software;3d environmental model;cell processor;acceleration;shared memory architectures;computer architecture;computational modeling;devs formalism;parallel architectures;multicore processing;discrete event system specification;multigrained parallelism;multiprocessing systems;compute intensive devs parallel simulation;computer simulation;logical process;parallel processing;parallel simulation;environmental modeling;parallel processing concurrent computing computational modeling discrete event simulation multicore processing application software computer simulation acceleration computer architecture;discrete event simulation	We propose a computing technique for efficient parallel simulation of compute-intensive DEVS models on the IBM Cell processor, combining multi-grained parallelism and various optimizations to speed up the event execution. Unlike most existing parallelization strategies, our approach explicitly exploits the massive fine-grained event-level parallelism inherent in the simulation process, while most of the logical processes are virtualized, making the achievable parallelism more deterministic and predictable. Together, the parallelization and optimization strategies produced promising experimental results, accelerating the simulation of a 3D environmental model by a factor of up to 33.06. The proposed methods can also be applied to other multicore and shared-memory architectures.	cell (microprocessor);devs;mathematical optimization;multi-core processor;parallel computing;shared memory;simulation	Qi Liu;Gabriel A. Wainer	2010	2010 IEEE Workshop on Principles of Advanced and Distributed Simulation	10.1109/PADS.2010.5471652	computer simulation;acceleration;multi-core processor;shared memory;parallel processing;computer architecture;application software;parallel computing;real-time computing;simulation;concurrent computing;computer science;discrete event simulation;operating system;data parallelism;computational model;instruction-level parallelism;scalable parallelism;implicit parallelism;task parallelism	HPC	-5.9284629972451715	47.84457077723304	164048
553fd30d1e0a3eabf8290c09018ad6ffa88c5419	data path issues in a highly concurrent machine	processing element;data cache	With the advent of highly concurrent machines, the new architectural problems are the same as the old, just highly exacerbated. It is desired to achieve high read bandwidths for the processing elements of an SISD computer, while keeping the register and memory write bandwidths low and keeping the interconnection network simple. In this paper a particular concurrent machine is examined, and solutions to these problems are proposed, simulated, verified, and measured. New techniques called write elimination and write liberation are presented that reduce the number of write ports necessary to the data cache, and reduce the internal complexity of the cache. The elimination of addressable hardware registers is also achieved.	cpu cache;hardware register;interconnection;killzone: liberation;sisd	Augustus K. Uht;Darin B. Johnson	1992		10.1145/146628.140497	cache-oblivious algorithm;chemistry;cache;cache invalidation;cache algorithms;cache pollution	Arch	-10.900484316789694	48.66986159059583	164165
8e25c5c9a7ad3650edc89ad4f0fdcd3140909be2	transactional programming in a multi-core environment	thread level parallelism;multi core processor;memory management;software systems;parallel programming;garbage collection;hardware architecture;chip;atomicity;software transactional memory;compiler optimization;transactional memory;parallel programs	With single thread performance starting to plateau, HW architects have turned to chip level multiprocessing (CMP) to increase processing power. All major microprocessor companies are aggressively shipping multi-core products in the mainstream computing market. Moore's law will largely be used to increase HW thread-level parallelism through higher core counts in a CMP environment. CMPs bring new challenges into the design of the software system stack.  In this tutorial, we talk about the shift to multi-core processors and the programming implications. In particular, we focus on transactional programming. Transactions have emerged as a promising alternative to lock-based synchronization that eliminates many of the problems associated with lock-based synchronization. We discuss the design of both hardware and software transactional memory and quantify the tradeoffs between the different design points. We show how to extend the Java and C languages with transactional constructs, and how to integrate transactions with compiler optimizations and the language runtime (e.g., memory manager and garbage collection).	central processing unit;computer performance;garbage collection (computer science);java;memory management;microprocessor;moore's law;multi-core processor;multiprocessing;optimizing compiler;parallel computing;software system;software transactional memory;task parallelism	Ali-Reza Adl-Tabatabai;Christoforos E. Kozyrakis;Bratin Saha	2007		10.1145/1229428.1229484	chip;multi-core processor;computer architecture;transactional memory;parallel computing;real-time computing;computer science;operating system;software transactional memory;hardware architecture;optimizing compiler;garbage collection;programming language;atomicity;task parallelism;software system;memory management	Arch	-8.023837443096912	47.031143984777096	164186
a2f9c4c33bead9e6037164fd7c5f42d158be60c4	the network adapter: the missing link between mpi applications and network performance	nic;fat trees;networks;cost consumption network adapter mpi applications network performance concerning topology switch technology link technology communication library communication libraries onchip memory memory bandwidth host memory network bandwidth hardware complexity energy consumption;near neighbor;message passing application program interfaces computer interfaces computer networks;computer networks;hpc;application program interfaces;message passing;adapter;collectives;near neighbor networks fat trees nic adapter hpc collectives all to all;all to all;computer interfaces;switches libraries adaptation models delay computational modeling protocols receivers	Network design aspects that influence cost and performance can be classified according to their distance from the applications, into issues concerning topology, switch technology, link technology, network adapter, and communication library. The network adapter has a privileged position to take decisions with more global information than any other component in the network. It receives feedback from the switches and requests from the communication libraries and applications. Also, compared to a network switch, an adapter has access to significantly more memory (host memory and on-chip memory) and memory bandwidth (which typically exceeds network bandwidth). The potential of the adapter to improve global network performance has not yet been fully exploited. In this work we show a series of noticeable performance improvements (of at least 10% to 15%) for medium-sized message exchanges in typical HPC communication patterns by optimizing message segmentation and packet injection policies, that can be implemented in an adapter's firmware inexpensively. We also show that implementing equivalent solutions in the switch (as opposed to the adapter) leads to only marginal performance improvements as the ones obtained by controlling the segmentation and injection policy at the adapter, while involving significantly more cost. In addition, enhancing the adapter will lead to less hardware complexity in the switches, thus reducing cost and energy consumption.	complexity;critical path method;enhanced graphics adapter;feedback;firmware;global network;library (computing);mpich;mvapich;marginal model;memory bandwidth;memory segmentation;network interface controller;network packet;network performance;network switch;open mpi;overhead (computing);quality of service;real-time computing;real-time transcription;run time (program lifecycle phase);spatial variability	Germán Rodríguez;Cyriel Minkenberg;Ronald P. Luijten;Ramón Beivide;Patrick Geoffray;Jesús Labarta;Mateo Valero;Steve Poole	2012	2012 IEEE 24th International Symposium on Computer Architecture and High Performance Computing	10.1109/SBAC-PAD.2012.17	embedded system;adapter;supercomputer;parallel computing;message passing;real-time computing;computer science;operating system;network interface controller	Arch	-10.296542835106056	47.276044534775316	164572
53e25cbb87a6b23c29bc1a8ed42f833a1e9279ee	a complexity-effective approach to alu bandwidth enhancement for instruction-level temporal redundancy	buffer storage;instruction sets;logic design;microprocessor chips;multiprocessing systems;parallel architectures;redundancy;alu bandwidth enhancement;alu bandwidth requirements;ipc loss;complexity-effective design;dual instruction stream superscalar core;instruction reuse buffer;instruction-level temporal redundancy;load handling;microarchitectural extensions;processor components;temporally redundant instruction stream	Previous proposals for implementing instruction-level temporalredundancy in out-of-order cores have reported a performancedegradation of upto 45% in certain applications compared to anexecution which does not have any temporal redundancy. An importantcontributor to this problem is the insufficient number ofALUs for handling the amplified load injected into the core. At thesame time, increasing the number of ALUs can increase the complexityof the issue logic, which has been pointed out to be oneof the most timing critical components of the processor. This paperproposes a novel extension of a prior idea on instruction reuseto ease ALU bandwidth requirements in a complexity-effective wayby exploiting certain interesting properties of a dual (temporallyredundant) instruction stream. We present microarchitectural extensionsnecessary for implementing an instruction reuse buffer(IRB) and integrating this with the issue logic of a dual instructionstream superscalar core, and conduct extensive evaluationsto demonstrate how well it can alleviate the ALU bandwidth problem.We show that on the average we can gain back nearly 50%of the IPC loss that occurred due to ALU bandwidth limitationsfor an instruction-level temporally redundant superscalar execution,and 23% of the overall IPC loss.	arithmetic logic unit;graph bandwidth;microarchitecture;requirement;superscalar processor	Angshuman Parashar;Sudhanva Gurumurthi;Anand Sivasubramaniam	2004	Proceedings. 31st Annual International Symposium on Computer Architecture, 2004.		computer architecture;application software;parallel computing;real-time computing;degradation;computer science;operating system;redundancy;logic;bandwidth	Arch	-7.418496076829164	52.06622378638378	164974
ba48945dae956355f1f9004d9d50063dcda4f301	on the effectiveness of set associative page mapping and its application to main memory management	lru;virtual memory;memory management;placement algorithm;cache memory;linear paging model;electron beam;paging;efficient implementation;linear model;geometric model;set associative page mapping;charged couple device;trace driven simulation;buffer memory	Set associative page mapping algorithms have become widespread for the operation of cache memories for reasons of cost and efficiency. In this paper we show how to calculate analytically the effectiveness of set associative paging relative to full associative (unconstrained mapping) paging. For two miss ratio models, Saltzer's linear model and a mixed geometric model, we are able to obtain simple, closed form expressions for the relative LRU fault rates. Trace driven simulations are used to verify the accuracy of our results. We suggest that as electronically accessed third level memories, such as electron beam memories, magnetic bubbles or charge coupled devices become available, algorithms currently used only for cache paging will be applied to main memory, for the same reasons of efficiency, implementation ease and cost.	algorithm;cpu cache;charge-coupled device;computer data storage;electron;geometric modeling;linear model;memory management;paging;simulation;time complexity;tracing (software)	Alan Jay Smith	1976			demand paging;parallel computing;real-time computing;thrashing;page cache;cathode ray;cpu cache;page replacement algorithm;computer science;virtual memory;theoretical computer science;geometric modeling;operating system;linear model;flat memory model;charge-coupled device;cache-only memory architecture;memory map;paging;memory management	HPC	-11.160216330737985	50.36594840866489	165371
18c663e6f03f4f01b6983d18763db3d34c732014	towards extremely fast context switching in a block-multithreaded processor	cycle time;switches yarn workstations bridges delay random access memory pipelines multithreading microprocessors fault tolerance;chip;memory access;synchronisation;profitability;memory cycle time extremely fast context switching block multithreaded processor fast context switch latencies memory accesses synchronization operations rhamma functional unit off chip cache workstation environment;functional unit	Multithreaded processors use a fast context switch to bridge latencies caused by memory accesses or by synchronization operations. In the block-multithreaded processor Ð called Rhamma Ð load/store, synchronization and execution operations of different threads of control are executed simultaneously by appropriate functional units. A fast context switch is performed, whenever a functional unit comes across an operation destined for another unit. Switching contexts on each load/store instruction sequence allows a much faster context switch in the execution unit than previously published designs do. The results show the potential of multithreading to spare expensive off-chip cache in a workstation environment. The load/store unit proves as the principal bottleneck. In particular the memory cycle time is performance critical. We show that multithreaded processors profit more than conventional RISC processors by a shorter memory cycle time.	central processing unit;context switch;execution unit;multithreading (computer architecture);thread (computing);workstation	Winfried Grünewald;Theo Ungerer	1996		10.1109/EURMIC.1996.546486	uniform memory access;shared memory;parallel computing;real-time computing;computer hardware;computer science	Arch	-8.352740253362107	51.77516542706453	165703
eb8711b614c67427a9526dfabcdb685d054a3e0b	pica: an ultra-light processor for high-througput applications	operand addressed context cache;silicon;cache storage;architectural design;optoelectronic devices;application software;36 bit overhead minimization fixed size activation contexts word tag synchronization bits node implementation pica ultra light processor fine grain message passing architecture high throughput parallel applications operand addressed context cache round robin task manager single cycle task swaps storage management local memory multi node chip prototype chip i o requirements high bandwidth 3d optical network epitaxial liftoff optoelectronic devices through chip transmission 3 2 gbit s;systolic arrays;storage management;optical fiber networks;optical computing;multi node chip prototype;epitaxial liftoff;parallel processing computer architecture silicon optical fiber networks bandwidth systolic arrays application software optical arrays optical design vliw;cache storage message passing parallel architectures storage management microprocessor chips optical computing;word tag synchronization bits;local memory;vliw;computer architecture;node implementation;round robin;parallel architectures;optical arrays;through chip transmission;overhead minimization;ultra light processor;single cycle task swaps;high throughput parallel applications;fine grain message passing architecture;message passing;pica;fixed size activation contexts;bandwidth;3 2 gbit s;high bandwidth 3d optical network;high throughput;round robin task manager;optical design;36 bit;chip i o requirements;parallel applications;parallel processing;microprocessor chips	This paper introduces Pica, a ne-grain, message passing architecture designed to eeciently support high-throughput parallel applications. The architecture minimizes overhead for basic parallel operations. An operand-addressed context cache and round-robin task manager allow single cycle task swaps. Fixed-sized activation contexts simplify storage management. Word-tag synchronization bits provide low-cost synchronization. The focus on high-throughput applications allows a small local memory (1024 36-bit words). A complete node (including memory) can be implemented using a fraction of a chip. A multi-node chip prototype (four nodes/chip) is being designed. In order to meet chip I/O requirements, a high-bandwidth, three-dimensional optical network is also being designed. Using recent developments in epitaxial liftoo of op-toelectronic devices and through-chip transmission, a network is presented that provides 3.2 Gbits/sec oo-chip bandwidth.	36-bit;acer pica;data rate units;epitaxy;high-throughput computing;input/output;message passing;operand;overhead (computing);prototype;requirement;round-robin scheduling;task manager;throughput	D. Scott Wills;W. Stephen Lacy;Huy Cat;Michael A. Hopper;Ashutosh Razdan;Sek M. Chai	1993		10.1109/ICCD.1993.393342	high-throughput screening;embedded system;parallel processing;electronic engineering;application software;parallel computing;message passing;real-time computing;computer hardware;telecommunications;computer science;very long instruction word;operating system;optical computing;silicon;bandwidth;computer network	Arch	-6.765374631161486	52.40596442445519	165813
c2f2a945beb9154061432e7a532c2f0100c57fd7	workload partitioning strategy for improved parallelism on fpga-cpu heterogeneous chips		In heterogeneous computing, efficient parallelism can be obtained if every device runs the same task on a different portion of the data set. This requires designing a scheduler which assigns data chunks to compute units proportional to their throughputs. For FPGA-CPU heterogeneous devices, to provide the best possible overall throughput, a scheduler should accurately evaluate the different performance behaviour of the compute devices. In this article, we propose a scheduler which initially detects the highest throughput each device can obtain for a specific application with negligible overhead and then partitions the dataset for improved performance. To demonstrate the efficiency of this method, we choose a Zynq UltraScale+ ZCU102 device as the hardware target and parallelise four applications showing that the developed scheduler can provide up to 94.06% of the throughput achievable at an ideal condition, with comparable power and energy consumption.		Sam Amiri;Mohammad Hosseinabady;Andres Rodriguez;Rafael Asenjo;Angeles G. Navarro;Jose Nunez-Yanez	2018	2018 28th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2018.00071	field-programmable gate array;throughput;real-time computing;parallel computing;workload;task analysis;symmetric multiprocessor system;computer science;energy consumption;benchmark (computing)	HPC	-5.733728480795566	52.591728396620034	165871
9382d8125a01dd24590d68fa05a16f333d3f7b85	accurate per-flow measurement with bloom sketch		Sketch is a probabilistic data structure, and is widely used for per-flow measurement in network. The most common sketches are the CM sketch and its several variants. However, given a limited memory size, these sketches always significantly overestimate some flows, exhibiting poor accuracy. To address this issue, we proposed a novel sketch named the Bloom sketch, combining the sketch with the Bloom filter, another well-known probabilistic data structure widely used for membership queries. Extensive experiments based on real IP traces show that our Bloom sketch achieves up to 14.47× higher accuracy compared with the CM sketch, while exhibiting comparable insertion and query speed. Our source code is available at Github [1].	bloom (shader effect);bloom filter;count–min sketch;data structure;experiment;tracing (software)	Yang Zhou;Hao Jin;Peng Liu;Haowei Zhang;Tong Yang;Xiaoming Li	2018	IEEE INFOCOM 2018 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOMW.2018.8406964	throughput;theoretical computer science;probabilistic logic;flow measurement;computer science;data structure;bloom;sketch;bloom filter;source code	Vision	-11.609060715342446	52.98467700913352	166215
23d7da91f129c28bb78df775badfce8bc480c9bd	optimized pre-copy live migration for memory intensive applications	scientific application;virtual machine;state migration;random access memory;measurement;fault tolerant;virtual machining bandwidth instruction sets monitoring benchmark testing random access memory measurement;rate of change;checkpoint restart virtual machines state migration;resource allocation;virtual machining;software performance evaluation;software fault tolerance;checkpoint restart;checkpointing;end to end application performance optimized pre copy live migration memory intensive applications resource consolidation fault tolerance kvm rate control xen mpi application openmp scientific application performance analysis migration control virtualization technologies virtual machines checkpoint restart kernel based virtual machine;rate control;monitoring;virtual machines;application program interfaces;virtualisation application program interfaces checkpointing message passing resource allocation software fault tolerance software performance evaluation virtual machines;performance analysis;message passing;bandwidth;rate limiting;on line algorithm;benchmark testing;virtualisation;instruction sets	Live migration is a widely used technique for resource consolidation and fault tolerance. KVM and Xen use iterative pre-copy approaches which work well in practice for commercial applications. In this paper, we study pre-copy live migration of MPI and OpenMP scientific applications running on KVM and present a detailed performance analysis of the migration process. We show that due to a high rate of memory changes, the current KVM rate control and target downtime heuristics do not cope well with HPC applications: statically choosing rate limits and downtimes is infeasible and current mechanisms sometimes provide suboptimal performance. We present a novel on-line algorithm able to provide minimal downtime and minimal impact on end-to-end application performance. At the core of this algorithm is controlling migration based on the application memory rate of change.	downtime;end-to-end principle;fault tolerance;heuristic (computer science);iterative method;online algorithm;online and offline;openmp;semiconductor consolidation	Khaled Z. Ibrahim;Steven A. Hofmeyr;Costin Iancu;Eric Roman	2011	2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC)	10.1145/2063384.2063437	parallel computing;real-time computing;computer science;virtual machine;operating system;distributed computing	HPC	-11.688871376178398	49.32240556100428	167115
0bbe09a03d8fa6c254d3e3f6342fe40bfd63c05e	cache coherence on a slotted ring	cache coherence;point to point;shared memory	1 Abstract-The Express Ring is a new architecture under investigation at the University of Southern California. Its main goal is to demonstrate that a slotted unidirectional ring with very fast point-to-point interconnections can be at least ten times faster than a shared bus, using the same technology, and may be the topology of choice for future shared-memory multiprocessors. In this paper we introduce the Express Ring architecture and present a snooping cache coherence protocol for this machine. This protocol shows how consistency of shared memory accesses can be efficiently maintained in a ring-connected multiprocessor. We analyze the proposed protocol and compare it to other more usual alternatives for point-to-point connected machines, such as the SCI cache coherence protocol and directory based protocols.	cache coherence;data dredging;directory (computing);multiprocessing;point-to-point protocol;shared memory	Luiz André Barroso;Michel Dubois	1991			distributed computing;parallel computing;cache coloring;cache invalidation;smart cache;mesi protocol;bus sniffing;mesif protocol;snoopy cache;computer science;write-once	Arch	-11.509011797068741	46.766804143635866	168005
e967dbdb1236627b440e7fa2256c5ed27f5e0bb2	efficient and fair multi-programming in gpus via effective bandwidth management		Managing the thread-level parallelism (TLP) of GPGPU applications by limiting it to a certain degree is known to be effective in improving the overall performance. However, we find that such prior techniques can lead to sub-optimal system throughput and fairness when two or more applications are co-scheduled on the same GPU. It is because they attempt to maximize the performance of individual applications in isolation, ultimately allowing each application to take a disproportionate amount of shared resources. This leads to high contention in shared cache and memory. To address this problem, we propose new application-aware TLP management techniques for a multi-application execution environment such that all co-scheduled applications can make good and judicious use of all the shared resources. For measuring such use, we propose an application-level utility metric, called effective bandwidth, which accounts for two runtime metrics: attained DRAM bandwidth and cache miss rates. We find that maximizing the total effective bandwidth and doing so in a balanced fashion across all co-located applications can significantly improve the system throughput and fairness. Instead of exhaustively searching across all the different combinations of TLP configurations that achieve these goals, we find that a significant amount of overhead can be reduced by taking advantage of the trends, which we call patterns, in the way application's effective bandwidth changes with different TLP combinations. Our proposed pattern-based TLP management mechanisms improve the system throughput and fairness by 20% and 2x, respectively, over a baseline where each application executes with a TLP configuration that provides the best performance when it executes alone.	bandwidth management;baseline (configuration management);cpu cache;central processing unit;dynamic random-access memory;fairness measure;general-purpose computing on graphics processing units;graphics processing unit;overhead (computing);parallel computing;resource contention;server (computing);system on a chip;task parallelism;throughput	Haonan Wang;Fan Luo;Mohamed Ibrahim;Onur Kayiran;Adwait Jog	2018	2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2018.00030	parallel computing;throughput;cache;computer science;dram;distributed computing;interference (wave propagation);general-purpose computing on graphics processing units;bandwidth (signal processing);bandwidth management;shared memory	Arch	-8.993526952693736	51.413936590639224	168089
9283340ed35953a41473ada43133e69579899aac	the performance potential of multiple functional unit processors	performance evaluation;pipeline processing;cray-like processor model;benchmark programs;dataflow limits;multiple functional unit processors;multiple-functional units;nonvector machines;performance potential;pipelining	In this paper, we look at the interaction of pipelining and multiple functional units in single processor machines. When implementing a high performance machine, a number of hardware techniques maybe used to improve the performance of the final system. Our goal is to gain an understanding of how each of these techniques contribute to performance improvement. As a basis for our studies we use a CRAY-like processor model and the issue rate (instructions per clock cycle) as the performance measure. We then systematically augment this base, non-pipelined, machine with more and more hardware features and evaluate the performance impact of each feature. We find, for example, that in non-vector machines, pipelining multiple function units does not provide significant performance improvements. Dataflow limits are then derived for our benchmark programs to determine the performance potential of each benchmark. In addition, other limits are computed which apply more realistic constraints on a computation. Based on these more realistic limits, we determine it is worthwhile to investigate the performance improvements that can be achieved from issuing multiple instructions each clock cycle. Several hardware approaches are evaluated for issuing multiple instructions each clock cycle.	benchmark (computing);clock signal;computation;dataflow;execution unit;instructions per cycle;pipeline (computing)	Andrew R. Pleszkun;Gurindar S. Sohi	1988			computer architecture;parallel computing;real-time computing;computer science;operating system;instructions per cycle	Arch	-7.3860827553615644	51.31057672597126	168208
268020456aa8c7965a5bf7d75d24f9c3c846a72d	a staged memory resource management method for cmp systems		Memory interference is a critical impediment to system performance in CMP systems. To address this problem, we first propose a Dynamically Proportional Bandwidth Throttling policy (DPBT), which dynamically throttles back memory-intensive applications based on their memory access behavior. DPBT achieves a more balance memory bandwidth partitioning. Moreover, we improve the previous memory channel partitioning scheme by integrating it with a bank partitioning. We further integrate DPBT with the improved memory channel partitioning scheme and a memory scheduling policy to leverage the architecture advantages, and present a Stage Memory Resource Management Method (SRM). Experimental results show that DPBT improves system throughput/fairness by 13.5%/31.1%. SRM provides 27.1% better system throughput and 34.8% better system fairness.	best-effort delivery;binary space partitioning;digital back-propagation;dynamic random-access memory;fairness measure;interference (communication);memory bandwidth;memory management;parallel computing;scheduling (computing);throughput	Yangguo Liu;Junlin Lu;Dong Tong;Xu Cheng	2017	2017 IEEE 28th International Conference on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2017.7995264	computer science;parallel computing;throughput;real-time computing;memory management;channel allocation schemes;architecture;memory bandwidth;uniform memory access;multi-channel memory architecture;interleaved memory	HPC	-9.335619104373503	52.5004755919448	169076
f6751f3349b5831ec64dec9d389ce8e9e7be0539	on the performance of interleaved memories with non-uniform access probabilities	memory bandwidth;exact solution;parallel processing	System structure and program behaviour are two major factors that influence the performance of a tightly-coupled multiprocessor. The latter has been usually ignored in most of the previous studies. The authors study the performance of a tightly-coupled multiprocessor in which a crossbar is employed to interconnect p processors to m memory modules. A set of non-uniformly distributed probabilities is also employed to illustrate the program behaviour, but no distinction is made between the processors. An inverse relation between the average request completion time and the effective memory bandwidth is obtained and three approximation methods are proposed. Their solutions are compared with the exact solution. Among them the repetitive augmenting method which based on the idea of aggregation generates the best result. 18 references.		Hung-Chang Du;Jean-Loup Baer	1983			distributed computing;parallel computing;multiprocessing;interconnection;distributed memory;interleaved memory;crossbar switch;memory bandwidth;computer science;uniform memory access;parallel processing	Theory	-10.063631512207845	49.51096373519135	169386
6358a95d5141b2d0167cd56eaa4cec989da254dc	scratchpad sharing in gpus	thread level parallelism;control flow graph;scratchpad sharing;code motion	General-Purpose Graphics Processing Unit (GPGPU) applications exploit on-chip scratchpad memory available in the Graphics Processing Units (GPUs) to improve performance. The amount of thread level parallelism (TLP) present in the GPU is limited by the number of resident threads, which in turn depends on the availability of scratchpad memory in its streaming multiprocessor (SM). Since the scratchpad memory is allocated at thread block granularity, part of the memory may remain unutilized. In this article, we propose architectural and compiler optimizations to improve the scratchpad memory utilization. Our approach, called Scratchpad Sharing, addresses scratchpad under-utilization by launching additional thread blocks in each SM. These thread blocks use unutilized scratchpad memory and also share scratchpad memory with other resident blocks. To improve the performance of scratchpad sharing, we propose Owner Warp First (OWF) scheduling that schedules warps from the additional thread blocks effectively. The performance of this approach, however, is limited by the availability of the part of scratchpad memory that is shared among thread blocks.  We propose compiler optimizations to improve the availability of shared scratchpad memory. We describe an allocation scheme that helps in allocating scratchpad variables such that shared scratchpad is accessed for short duration. We introduce a new hardware instruction, relssp, that when executed releases the shared scratchpad memory. Finally, we describe an analysis for optimal placement of relssp instructions, such that shared scratchpad memory is released as early as possible, but only after its last use, along every execution path.  We implemented the hardware changes required for scratchpad sharing and the relssp instruction using the GPGPU-Sim simulator and implemented the compiler optimizations in Ocelot framework. We evaluated the effectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK, GPGPU-Sim, and Rodinia. The kernels that under-utilize scratchpad memory show an average improvement of 19% and maximum improvement of 92.17% in terms of the number of instruction executed per cycle when compared to the baseline approach, without affecting the performance of the kernels that are not limited by scratchpad memory.	baseline (configuration management);buffer overflow;cuda;general-purpose computing on graphics processing units;graphics processing unit;multiprocessing;one-way function;optimizing compiler;parallel computing;processor register;scheduling (computing);scratchpad memory;sensor;software development kit;task parallelism;thread block;value range analysis	Vishwesh Jatala;Jayvant Anantpur;Amey Karkare	2017	TACO	10.1145/3075619	scheduling (computing);task parallelism;parallel computing;computer science;graphics processing unit;real-time computing;scratchpad memory;multiprocessing;general-purpose computing on graphics processing units;thread (computing);optimizing compiler	Arch	-7.377350643913881	50.02558289708003	170012
5aa289bbcd7e8a385b7fb6483edebd935e11844f	freeze'nsense: estimation of performance isolation in cloud environments	performance measure;resource management;computation interference;application profiling;hardware performance counters;data center management;cloud services;performance isolation	Modern computing hardware has a very good task parallelism, but resource contention between tasks remains high. This renders large fractions of CPU time wasted and leads to application interference. Even tasks running on dedicated CPU cores can still incur interference from other tasks, most notably because of the caches and other hardware components shared by more than one core. The level of interference depends on the nature of executed tasks and is difficult to predict. A customer who has been granted that his task will run as if it were alone (e.g., a CPU core dedicated to a virtual machine), indeed suffers from significant performance degradation due to the time spent waiting for resources occupied by other tasks. Measuring actual performance of a task or a virtual machine can be difficult. However, even more challenging is estimating what the performance of the task should be if it were running completely in isolation. In this paper, we present a measurement technique Freeze’nSense. It is based on the hardware performance counters and allows measuring actual performance of a task and estimating performance as if the task were in isolation, all during runtime. To estimate performance in isolation, the proposed technique performs a short-time freezing of the potentially interfering tasks. Freeze’nSense introduces lower than 1% overhead and is confirmed to provide accurate and reliable measurements. In practice, Freeze’nSense becomes a valuable tool helping to automatically identify tasks that suffer the most in a shared environment and move them to a distant core. The observed performance improvement can be as large as 80–100% for individual tasks, and scale up to 15–20% for the computing node. Copyright © 2016 John Wiley & Sons, Ltd.	cpu cache;central processing unit;computer hardware;elegant degradation;hardware performance counter;interference (communication);isolation (database systems);john d. wiley;overhead (computing);parallel computing;rendering (computer graphics);resource contention;run time (program lifecycle phase);task parallelism;virtual machine	Alexandre Kandalintsev;Dzmitry Kliazovich;Renato Lo Cigno	2017	Softw., Pract. Exper.	10.1002/spe.2456	real-time computing;simulation;cloud computing;computer science;engineering;resource management;operating system	HPC	-9.122580718290383	51.60904270214241	170118
7c0bd7ce51c62671d5ffc1506786b0b7861ce00a	utility-based acceleration of multithreaded applications on asymmetric cmps	asymmetric cmps;multithreaded applications;barriers;multicore;critical sections;heterogeneous cmps	Asymmetric Chip Multiprocessors (ACMPs) are becoming a reality. ACMPs can speed up parallel applications if they can identify and accelerate code segments that are critical for performance. Proposals already exist for using coarse-grained thread scheduling and fine-grained bottleneck acceleration. Unfortunately, there have been no proposals offered thus far to decide which code segments to accelerate in cases where both coarse-grained thread scheduling and fine-grained bottleneck acceleration could have value. This paper proposes Utility-Based Acceleration of Multithreaded Applications on Asymmetric CMPs (UBA), a cooperative software/hardware mechanism for identifying and accelerating the most likely critical code segments from a set of multithreaded applications running on an ACMP. The key idea is a new Utility of Acceleration metric that quantifies the performance benefit of accelerating a bottleneck or a thread by taking into account both the criticality and the expected speedup. UBA outperforms the best of two state-of-the-art mechanisms by 11% for single application workloads and by 7% for two-application workloads on an ACMP with 52 small cores and 3 large cores.	code segment;criticality matrix;hardware acceleration;multithreading (computer architecture);programmer;scheduling (computing);speedup;thread (computing)	José A. Joao;M. Aater Suleman;Onur Mutlu;Yale N. Patt	2013		10.1145/2485922.2485936	multi-core processor;computer architecture;parallel computing;real-time computing;computer science;operating system;distributed computing;critical section	Arch	-8.923844631195522	49.97647953214759	170526
71fe6d9a45030901b04c115fa16bb058eb575915	an out-of-order superscalar processor with speculative execution and fast, precise interrupts	out-of-order superscalar processor;speculative execution;precise interrupts;dynamic scheduling;out of order;virtual memory;cycle time;registers;page coloring;branch prediction;operating systems;throughput;out of order execution;system performance;cache	The achievement of fast, precise interrupts and the implementation of multiple levels of branch predictions are two of the problems associated with the dynamic scheduling of instructions for superscalar processors. Their solution is especially difficult if short cycle time operation is desired. We present solutions to these problems through the development of the Fast Dispatch Stack (FDS) system. We show that the FDS is capable of scheduling storage, branch, and register-toregister instructions for concurrent and out-oforder executions; the FDS implements fast and precise interrupts in a natural, efficient way; and it facilitates speculative execution -Instructions preceding and following one or more predicted conditional branch instructions may issue. When necessary, their effects are undone in one machine cycle. We evaluated the FDS system with extensive simulations.	application checkpointing;benchmark (computing);branch (computer science);branch predictor;central processing unit;compiler;computer architecture;electrical engineering;execution unit;family computer disk system;ieee transactions on computers;instruction cycle;interrupt;mainframe computer;microprocessor;out-of-order execution;scalar processor;scheduling (computing);simulation;speculative execution;superscalar processor;undo;vax;wen-mei hwu;x86	Harry Dwyer;Hwa C. Torng	1992		10.1145/144953.145834	parallel computing;speculative multithreading	Arch	-9.825238524172654	49.44497526943143	171240
9f1816833fb3633189bf6288cbcbfb821d459748	exploiting thread-data affinity in openmp with data access patterns	average speedup;architecture-agnostic programmer hint;task scheduling;parallel loop;nas parallel benchmark suite;thread-data affinity;dynamic loop iteration scheduling;architectural information;openmp programming model;loop iteration;data access pattern;data access locality	In modern NUMA architectures, preserving data access locality is a key issue to guarantee performance. We define, for the OpenMP programming model, a type of architecture-agnostic programmer hint to describe the behaviour of parallel loops. These hints are only related to features of the program, in particular to the data accessed by each loop iteration. The runtime will then combine this information with architectural information gathered during its initialization, to guide task scheduling, in case of dynamic loop iteration scheduling. We prove the effectiveness of the proposed technique on the NAS parallel benchmark suite, achieving an average speedup of 1.21x.	data access;openmp;processor affinity	Andrea Di Biagio;Ettore Speziale;Giovanni Agosta	2011		10.1007/978-3-642-23400-2_22	parallel computing;real-time computing;computer science;operating system;database;distributed computing;programming language	HPC	-7.340870466167201	47.882952048272166	171255
3f340e4b51de03cb16eaecda146528d591b8b4f1	paging tradeoffs in distributed-shared-memory multiprocessors	scheduling;virtual memory;synchronization.;shared-memory multiprocessors	"""Massively parallel processors have begun using commodity operating systems that support demand-paged virtual memory. To evaluate the utility of virtual memory, we measured the behavior of seven shared-memory parallel application programs on a simulated distributed-shared-memory machine. Our results (i) confirm the importance of gang CPU scheduling, (ii) show that a page-faulting processor should spin rather than invoke a parallel context switch, (iii) show that our parallel programs frequently touch most of their data, and (iv) indicate that memory, not just CPUs, must be """"gang scheduled"""". Overall, our experiments demonstrate that demand paging has limited value on current parallel machines because of the applications' synchronization and memory reference patterns and the machines' high page-fault and parallel-context-switch overheads."""	central processing unit;context switch;distributed shared memory;experiment;operating system;page fault;paging;scheduling (computing)	Doug Burger;Rahmat S. Hyder;Barton P. Miller;David A. Wood	1994	The Journal of Supercomputing	10.1007/BF00128100	computer architecture;parallel computing;real-time computing	HPC	-8.843828411702995	49.48457689355056	172156
6ecd282ddfbcb7db870e78602f3021ceac44f931	nv-tree: a consistent and workload-adaptive tree structure for non-volatile memory	phase change materials;nonvolatile memory central processing unit random access memory data structures phase change materials ash throughput;random access memory;tree;data structures;nonvolatile memory;non volatile memory;ash;workload adaptive;workload adaptive non volatile memory data consistency tree;data consistency;central processing unit;throughput	"""The non-volatile memory (NVM) which can provide DRAM-like performance and disk-like persistency has the potential to build single-level systems by replacing both DRAM and disk. Keeping data consistency in such systems is non-trivial because memory writes may be reordered by CPU. Although ordered memory writes for achieving data consistency can be implemented using the memory fence and the CPU cache line flush instructions, they introduce a significant overhead (more than 10X slower in performance). In this paper, we focus on an important and common data structure, B <inline-formula><tex-math notation=""""LaTeX"""">$^+$</tex-math><alternatives> <inline-graphic xlink:type=""""simple"""" xlink:href=""""yang-ieq1-2479621.gif""""/></alternatives></inline-formula>Tree. Based on our quantitative analysis for consistent tree structures, we propose NV-Tree, a consistent, cache-optimized and workload-adaptive B <inline-formula><tex-math notation=""""LaTeX"""">$^+$</tex-math><alternatives> <inline-graphic xlink:type=""""simple"""" xlink:href=""""yang-ieq2-2479621.gif""""/></alternatives></inline-formula>Tree variant with significantly reduced consistency cost (up to 96 percent reduction in CPU cache line flush). To further optimize NV-Tree under various workloads, we propose a workload-adaptive scheme in which the sizes of individual nodes can be dynamically adjusted to improve the performance over time. We implement and evaluate NV-Tree and NV-Store, a key-value store based on NV-Tree, on an NVDIMM server. NV-Tree outperforms the state-of-art consistent tree structures by up to 12X under write-intensive workloads. NV-Store increases the throughput by up to 7.3X under YCSB workloads compared to Redis."""	cpu cache;central processing unit;data structure;dynamic random-access memory;global variable;key-value database;memory barrier;multi-level cell;nvdimm;non-volatile memory;nv network;overhead (computing);redis;server (computing);throughput;tree structure;volatile memory;xlink;ycsb	Jun Yang;Qingsong Wei;Chundong Wang;Cheng Chen;Khai Leong Yong;Beixin Julie He	2016	IEEE Transactions on Computers	10.1109/TC.2015.2479621	uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;non-volatile memory;static random-access memory;data structure;computer hardware;computer science;operating system;memory scrubbing;overlay;extended memory;flat memory model;registered memory;cache pollution;cache-only memory architecture;memory map;non-uniform memory access;memory management	OS	-10.862729519084127	53.155979996809876	172499
c0394b974768ac0c87f325b589750531e95cd05e	static locality analysis for cache management	cache storage;storage management;proposals pollution bandwidth microprocessors interference computer architecture electronic mail pattern analysis information analysis tagging;selective cache locality analysis cache management array references static locality analysis data caches compile time interference analysis dual data cache temporal locality;selective cache;memory architecture cache storage storage management;locality analysis;data cache;memory architecture;dual data cache;cache management	Most memory references in numerical codes correspond to array references whose indices are affine functions of surrounding loop indices. These array references follow a regular predictable memory pattern that can be analyzed at compile time. This analysis can provide valuable information like the locality exhibited by the program, which can be used to implement a more intelligent caching strategy. In this paper we propose a static locality analysis oriented to the management of data caches. We show that previous proposals on locality analysis are not appropriate when the programs have a high conflict miss ratio. This paper extends those proposals by introducing a compile time interference analysis that significantly improve the performance of them. We first show how this analysis can be used to characterize the dynamic locality properties of numerical codes. This evaluation show for instance that a large percentage of references exhibit only temporal locality and another significant percentage does not exhibit any type of locality. This motivates the use of a dual data cache, which has a module specialized to exploit temporal locality, and a selective cache respectively. Then, the performance provided by these two cache organizations is evaluated. In both organizations, the static locality analysis is responsible for tagging each memory instruction accordingly to the particular type(s) of locality that it exhibits.	cpu cache;code;compile time;compiler;interference (communication);locality of reference;numerical analysis	F. Jesús Sánchez;Antonio González;Mateo Valero	1997		10.1109/PACT.1997.644022	locality of reference;bus sniffing;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;loop interchange;cache;computer science;write-once;cache invalidation;database;write buffer;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;non-uniform memory access	Arch	-9.89710905764883	50.42387484691049	172954
b57cb77917991771a2c6a40d08bec4e01d1da14a	exploiting execution locality with a decoupled kilo-instruction processor	off-chip memory access;decoupled kilo-instruction processor;memory latency;exploiting execution locality;execution locality;independent instruction;instruction issue time;instruction window;in-flight instruction;l2 cache;out-of-order execution	Overcoming increasing memory latency is one of the main problems that microprocessor designers have faced over the years. The two basic techniques introduced to mitigate latencies are caches and out-of-order execution. However, neither of these solutions is adequatefor hiding off-chip memory accesses in the order of 200 cycles or more. Theoretically, increasing the size of the instruction window would allow much longer latencies to be hidden. But scaling the structures to support thousands of in-flight instructions would be prohibitively expensive. However, the distribution of instruction issue times under the presence of L2 cache misses is highly correlated. This paper describes this phenomenon of Execution Locality and shows how it can be exploited with an inexpensive microarchitecture consisting of two linked cores. This Decoupled Kilo-Instruction Processor (D-KIP) is very effective in recovering lost potential performance. Extensive simulations show that speed-ups of up to 379% are possible for numerical benchmarks thanks to the exploitation of impressive degrees of Memory-Level Parallelism (MLP) and the execution of independent instructions in the shadow of L2 misses.	cas latency;cpu cache;central processing unit;code;computer memory;coupling (computer programming);image scaling;instruction window;inter-process communication;locality of reference;memory-level parallelism;microarchitecture;microprocessor;numerical analysis;out-of-order execution;simulation;speedup;window function	Miquel Pericàs;Adrián Cristal;Rubén González;Daniel A. Jiménez;Mateo Valero	2005		10.1007/978-3-540-77704-5_5	parallel computing;real-time computing;computer hardware;computer science;operating system;distributed computing	Arch	-8.4270117081921	51.205425451399925	174031
bff6c3624694d5d9711c765bc496faadaf2b3767	slow memory: the rising cost of optimism	rising cost;slow memory;cost function;computer science;hardware;protocols;discrete event simulation;chip;reactive power;parallel programming	Rapid progress in the design of fast CPU chips has outstripped progress in memory and cache performance. Optimistic algorithms would seem to be more vulnerable to poor memory performance because they require extra memory for state saving and anti-messages. We examine the performance of both optimistic and conservative protocols in controlled experiments to evaluate the effects of memory speed and cache size, using a variety of applications.	algorithm;cpu cache;central processing unit;experiment	Richard A. Meyer;Jay Martin;Rajive L. Bagrodia	2000			chip;parallel computing;real-time computing;simulation;telecommunications;computer science;discrete event simulation;operating system;distributed computing	Arch	-10.62212166776974	51.021466356234825	174385
06c65513c1eec4831cca68af612f17aee0f142d4	peach2: an fpga-based pcie network device for tightly coupled accelerators		In recent years, heterogeneous clusters using accelerators are often used for high performance computing systems. In such clusters, inter-node communication between accelerators requires several memory copies via CPU memory, and the communication latency incurred severely reduces performance. To solve this problem, we have been proposing a Tightly Coupled Accelerators (TCA) architecture intended to reduce the communication latency between accelerators over different nodes. In the TCA architecture, PCI Express packets are used for communication among GPUs over nodes. We developed a communication chip that we call the named PEACH2 chip, to help implement the TCA architecture. In this paper, we describe the details of the design and implementation of the PEACH2 chip, with respect to its routing mechanism and its DMA controller using FPGA. We evaluated the PEACH2 on a new platform that uses the latest Xeon CPU, IvyBridge, and achieved 2.3 GBytes/sec between GPUs over nodes, while the performance was only 880 MBytes/sec on the previous platform with SandyBridge.	advanced telecommunications computing architecture;central processing unit;channel i/o;computer cluster;direct memory access;field-programmable gate array;gigabyte;graphics processing unit;networking hardware;pci express;random-access memory;routing;sandy bridge;supercomputer	Yuetsu Kodama;Toshihiro Hanawa;Taisuke Boku;Mitsuhisa Sato	2014	SIGARCH Computer Architecture News	10.1145/2693714.2693716	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-10.17673457950562	46.46957587408378	174436
d796cfead9ff6e961fdf13da8a73bc36312fc1a5	realization of prams: processor design	processor architecture;shared memory;hot spot;memory access;operating system	We present a processor architecture for SB-PRAM, a parallel machine with shared address space and uniform memory access time. The processor uses a reduced instruction set and provides in hardware mechanisms for the emulation of shared memory: random hashing to avoid hot spots, multiple contexts with regular scheduling to hide network latency and fast context switch to minimize overhead. Furthermore it provides hardware support for parallel operating systems and for the eecient compilation of parallel high level languages. We give technical data for a prototype VLSI implementation with a oating point unit.	access time;address space;cas latency;compiler;context switch;emulator;high-level programming language;operating system;overhead (computing);parallel computing;processor design;prototype;sandy bridge;scheduling (computing);shared memory;uniform memory access	Jörg Keller;Wolfgang J. Paul;Dieter Scheerer	1994		10.1007/BFb0020421	uniform memory access;distributed shared memory;shared memory;interleaved memory;computer architecture;semiconductor memory;parallel computing;real-time computing;distributed memory;base and bounds;computer science;memory protection;computer memory;conventional memory;extended memory;processor register;memory map;non-uniform memory access;memory management	Arch	-10.04978279730608	48.47652036274979	175278
a0df38f727d3f56e1622d03805b2e9e05fdd2988	a practical data classification framework for scalable and high performance chip-multiprocessors	resource management;multi threaded parallel;multi core single chip multiprocessors;cache coherence practically private data classification pipelined parallel multi threaded parallel openmp compilers;runtime;compilers;instruction sets coherence runtime benchmark testing resource management dynamic scheduling optimization;chip multiprocessor nuca based caching performance improvement coherence traffic compiler assisted mechanism cache coherence design interconnect coherence directory tlb microarchitectural constructs data classification scheme many core architectures coherence overhead mitigation data access latency mitigation parallel applications practically private scalable high performance parallel systems data access behavior application specific characteristics;pipelined parallel;cache coherence;openmp;ubiquitous computing cache storage parallel architectures pattern classification performance evaluation program compilers;coherence;optimization;data classification;practically private;benchmark testing;dynamic scheduling;instruction sets	State-of-the-art chip multiprocessor (CMP) proposals emphasize general optimizations designed to deliver computing power for many types of applications. Potentially, significant performance improvements that leverage application-specific characteristics such as data access behavior are missed by this approach. In this paper, we demonstrate how scalable and high-performance parallel systems can be built by classifying data accesses into different categories and treating them differently. We develop a novel compiler-based approach to speculatively detect a data classification termed practically private, which we demonstrate is ubiquitous in a wide range of parallel applications. Leveraging this classification provides efficient solutions to mitigate data access latency and coherence overhead in today's many-core architectures. While the proposed data classification scheme can be applied to many micro-architectural constructs including the TLB, coherence directory, and interconnect, we demonstrate its potential through an efficient cache coherence design. Specifically, we show that the compiler-assisted mechanism reduces an average of 46% coherence traffic and achieves up to 12%, 8%, and 5% performance improvement over shared, private, and state-of-the-art NUCA-based caching, respectively, depending on scenarios.	c dynamic memory allocation;cache (computing);cache coherence;comparison and contrast of classification schemes in linguistics and metadata;compile time;data access;information privacy;manycore processor;multi-core processor;multiprocessing;optimizing compiler;overhead (computing);scalability;statistical classification;translation lookaside buffer	Yong Li;Rami G. Melhem;Alex K. Jones	2014	IEEE Transactions on Computers	10.1109/TC.2013.161	embedded system;benchmark;cache coherence;computer architecture;compiler;parallel computing;real-time computing;coherence;dynamic priority scheduling;computer science;resource management;operating system;instruction set	HPC	-8.617409686234438	50.750888705528176	175457
141663ac5bcb8f783d2ecd1d1938a9a4a69a3158	on the performance potential of different types of speculative thread-level parallelism: the dl vers	performance evaluation;data dependence;doall loops;speculative execution;value dependence;control dependence	Recent research in thread-level speculation (TLS) has proposed several mechanisms for optimistic execution of difficult-to-analyze serial codes in parallel. Though it has been shown that TLS helps to achieve higher levels of parallelism, evaluation of the unique performance potential of TLS, i.e., performance gain that be achieved only through speculation, has not received much attention. In this paper, we evaluate this aspect, by separating the speedup achievable via true TLP (thread-level parallelism) and TLS, for the SPEC CPU2000 benchmark. Further, we dissect the performance potential of each type of speculation --- control speculation, data dependence speculation and data value speculation. To the best of our knowledge, this is the first dissection study of its kind. Assuming an oracle TLS mechanism --- which corresponds to perfect speculation and zero threading overhead --- whereby the execution time of a candidate program region (for speculative execution) can be reduced to zero, our study shows that, at the loop-level, the upper bound on the arithmetic mean and geometric mean speedup achievable via TLS across SPEC CPU2000 is 39.16% (standard deviation = 31.23) and 18.18% respectively.	benchmark (computing);code;data dependency;overhead (computing);parallel computing;run time (program lifecycle phase);spec#;speculative execution;speculative multithreading;speedup;task parallelism;thread (computing);transport layer security	Arun Kejariwal;Xinmin Tian;Wei Li;Milind Girkar;Sergey Kozhukhov;Hideki Saito;Utpal Banerjee;Alexandru Nicolau;Alexander V. Veidenbaum;Constantine D. Polychronopoulos	2006		10.1145/1183401.1183407	parallel computing;real-time computing;computer science;operating system;distributed computing;speculative multithreading;speculative execution	Arch	-8.074771216268907	49.91046360910388	175469
5d506c0fcf050ddfe773bfe16a63551c5bc56032	retrospective: lockup-free instruction fetch/prefetch cache organization	lockup-free instruction;prefetch cache organization	In the past decade, there has been much literature describing various cache organizations that exploit general programming idiosyncrasies to obtain maximum hit rate (the probability that a requested datum is now resident in the cache). Little, if any, has been presented to exploit: (1) the inherent dual input nature of the cache and (2) the many-datum reference type central processor instructions. No matter how high the cache hit rate is, a cache miss may impose a penalty on subsequent cache references. This penalty is the necessity of waiting until the missed requested datum is received from central memory and, possibly, for cache update. For the two cases above, the cache references following a miss do not require the information of the datum not resident in the cache, and are therefore penalized in this fashion. In this paper, a cache organization is presented that essentially eliminates this penalty. This cache organizational feature has been incorporated in a cache/memory interface subsystem design, and the design has been implemented and prototyped. An existing simple instruction set machine has verified the advantage of this feature; future, more extensive and sophisticated instruction set machines may obviously take more advantage. Prior to prototyping, simulations verified the advantage.	cpu cache;cache (computing);central processing unit;geodetic datum;reference type;simulation	David Kroft	1998		10.1145/285930.285939	instruction prefetch;computer architecture;parallel computing;computer hardware	Arch	-9.943073735841093	52.68968900688187	175566
ef12112e116da7475b6c772ac2fbd9598af4d811	a neurocomputing approach to congestion control in an atm multiplexer	neurocomputing approach;congestion control;atm multiplexer	Abstract Communication in cache-coherent distributed shared memory(DSM) often requires invalidating (or writing back) cached copiesof a memory block, incurring high overheads. This paper proposes Last-Touch Predictors (LTPs) that learn and predict the OlasttouchO to a memory block by one processor before the block isaccessed and subsequently invalidated by another. By predicting alast-touch and (self-)invalidating the block in advance, an LTPhides the invalidation time, signiÞcantly reducing the coherenceoverhead. The key behind accurate last-touch prediction is trace-based correlation, associating a last-touch with the sequence ofinstructions (i.e., a trace) touching the block from a coherencemiss until the block is invalidated. Correlating instructions enablesan LTP to identify a last-touch to a memory block uniquelythroughout an applicationOs execution.In this paper, we use results from running shared-memory applica-tions on a simulated DSM to evaluate LTPs. The results indicatethat: (1) our base case LTP design, maintaining trace signatureson a per-block basis, substantially improves prediction accuracyover previous self-invalidation schemes to an average of 79%; (2)our alternative LTP design, maintaining a global trace signaturetable, reduces storage overhead but only achieves an averageaccuracy of 58%; (3) last-touch prediction based on a singleinstruction only achieves an average accuracy of 41% due toinstruction reuse within and across computation; and (4) LTPenables selective, accurate, and timely self-invalidation in DSM,speeding up program execution on average by 11%.	atm turbo;computational neuroscience;multiplexer;network congestion	Ahmed A. Tarraf;Ibrahim W. Habib;Tarek N. Saadawi	1996	J. High Speed Networks	10.3233/JHS-1996-5402	parallel computing;real-time computing;telecommunications;computer science;theoretical computer science;computer security	HPC	-7.556701386154937	51.860094526196804	175617
ad7809470890f7a9221449e2a649f75146f80e7d	implementation and evaluation of fast parallel packet filters on a cell processor	multi core processor;packet filtering;code optimization;cell processor;information network;cell broadband engine;cost effectiveness;software pipelining	Packet filters are essential for most areas of recent informa- tion network technologies. While high-end expensive routers and firewalls are implemented in hardware-based, flexible and cost-effective ones are usually in software-based solutions using general-purpose CPUs but have less performance. The authors have studied the methods of applying code optimization techniques to the packet filters executing on a single core processor. In this paper, by utilizing the multi-core processor Cell Broadband Engine with software pipelining, we construct a parallelized and SIMDed packet filter 40 times faster than the naive C program filter executed on a single core.		Yoshiyuki Yamashita;Masato Tsuru	2010		10.1007/978-3-642-14292-5_22	multi-core processor;software pipelining;embedded system;parallel computing;real-time computing;cost-effectiveness analysis;computer science;operating system;program optimization;programming language;packet switch;network processor	HPC	-4.682828352382278	46.893384937836665	175640
93adfe4813168fcdceaec3f1a86987076837ed11	profiling heterogeneous multi-gpu systems to accelerate cortically inspired learning algorithms	cortically inspired learning algorithm;nvidia geforce gtx 280;image recognition;cuda framework heterogeneous multigpu system cortically inspired learning algorithm parallel computing device human neocortex gpgpu accelerated extension intelligent learning model mammalian neocortex hardware accelerator multiple kernel launch overhead software work queue structure gpu architecture memory system global thread scheduler runtime profiling tool parallel learning algorithm;parallel learning algorithm;learning algorithm;paper;tesla c2050;fault tolerant;neural networks;heterogeneous systems;energy efficient;learning model;computer model;human neocortex;biological system modeling;computer graphic equipment;brain models;brain models graphics processing unit computational modeling biological system modeling neurons;hardware accelerator;nvidia geforce 9800 gx2;coprocessors;cuda;computational modeling;structure and function;runtime profiling tool;gpgpu accelerated extension;parallel processing computer graphic equipment coprocessors;heterogeneous multigpu system;mammalian neocortex;parallel computer;nvidia;memory systems;graphic processing unit;algorithms;optimization;neurons;computer science;software work queue structure;global thread scheduler;memory system;graphics processing unit;gpu architecture;multiple kernel launch overhead;parallel processing;intelligent learning model;parallel computing device;reverse engineering;cuda framework;device modeling	Recent advances in neuroscientific understanding make parallel computing devices modeled after the human neocortex a plausible, attractive, fault-tolerant, and energy-efficient possibility. Such attributes have once again sparked an interest in creating learning algorithms that aspire to reverse-engineer many of the abilities of the brain. In this paper we describe a GPGPU-accelerated extension to an intelligent learning model inspired by the structural and functional properties of the mammalian neocortex. Our cortical network, like the brain, exhibits massive amounts of processing parallelism, making today's GPGPUs a highly attractive and readily-available hardware accelerator for such a model. Furthermore, we consider two inefficiencies inherent to our initial design: multiple kernel-launch overhead and poor utilization of GPGPU resources. We propose optimizations such as a software work-queue structure and pipelining the hierarchical layers of the cortical network to mitigate such problems. Our analysis provides important insight into the GPU architecture details including the number of cores, the memory system, and the global thread scheduler. Additionally, we create a runtime profiling tool for our parallel learning algorithm which proportionally distributes the cortical network across the host CPU as well as multiple GPUs, whether homogeneous or heterogeneous, that may be available to the system. Using the profiling tool with these optimizations on Nvidia's CUDA framework, we achieve up to 60x speedup over a single-threaded CPU implementation of the model.	algorithm;apevia;artificial intelligence;bottleneck (software);cuda;central processing unit;fault tolerance;fermi (microarchitecture);geforce 200 series;geforce 8 series;general-purpose computing on graphics processing units;graphics processing unit;hardware acceleration;machine learning;mathematical optimization;multiple buffering;overhead (computing);parallel computing;parallel programming model;pipeline (computing);powerpc 600;reverse engineering;scheduling (computing);speedup;thread (computing)	Andrew Nere;Atif Hashmi;Mikko H. Lipasti	2011	2011 IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2011.88	parallel processing;fault tolerance;parallel computing;real-time computing;hardware acceleration;computer science;theoretical computer science;operating system;distributed computing;efficient energy use;programming language;computational model;coprocessor;reverse engineering	Arch	-4.7398121925900805	47.039111891605216	175749
4451dc968599c7c13b21b7f17370b809d6a8a4d5	difficult-path branch prediction using subordinate microthreads	microcomputers;microarchitecture;branch prediction;accuracy;computer architecture;hardware	Branch misprediction penalties continue to increase as microprocessor cores become wider and deeper. Thus, improving branch prediction accuracy remains an important challenge. Simultaneous Subordinate Microthreading (SSMT) provides a means to improve branch prediction accuracy. SSMT machines run multiple, concurrent microthreads in support of the primary thread. We propose to dynamically construct microthreads that can speculatively and accurately pre-compute branch outcomes along frequently mispredicted paths. The mechanism is intended to be implemented entirely in hardware. We present the details for doing so. We show how to select the right paths, how to generate accurate predictions, and how to get this information in a timely way. We achieve an average gain of 8.4% (42% maximum) over a very aggressive baseline machine on the SPECint95 and SPECint2000 benchmark suites.	baseline (configuration management);benchmark (computing);branch misprediction;branch predictor;microprocessor;microthread	Robert S. Chappell;Francis Tseng;Yale N. Patt;Adi Yoaz	2002			computer architecture;parallel computing;real-time computing;microarchitecture;computer science;operating system;microcomputer;accuracy and precision;branch predictor	Arch	-7.324558973023893	51.44972501403581	175838
999e7b4f4e4af4b6bb114fd9e27894f28c94f88f	hppac introduction and committees		Workshop Description: Power and energy are now recognized as first-order constraints in high-performance computing. Optimizing performance under power and energy bounds requires coordination across not only the software stack (compilers, operating and runtime systems, job schedulers) but also coordination with cooling systems and outwards to electrical suppliers. As we continue to move towards exascale and extreme scale computing, understanding how power translates to performance becomes an increasingly critical problem. The purpose of this workshop is to provide a forum where cutting-edge research in the above topic can be shared with others in the community.	computer cooling;first-order predicate;ibm websphere extreme scale;job scheduler;optimizing compiler;supercomputer	Barry Rountree;Shuaiwen Song	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.240	embedded system;real-time computing;simulation;operating system;distributed computing;efficient energy use	HPC	-4.68404391555124	49.13578921418781	176173
353236cf123b765b99f9bc4a6b1f795335ec9652	power-efficient breadth-first search with dram row buffer locality-aware address mapping		Graph analysis applications have been widely used in real services such as road-traffic analysis and social network services. Breadth-first search (BFS) is one of the most representative algorithms for such applications; therefore, many researchers have tuned it to maximize performance. On the other hand, owing to the strict power constraints of modern HPC systems, it is necessary to improve power efficiency (i.e., performance per watt) when executing BFS. In this work, we focus on the power efficiency of DRAM and investigate the memory access pattern of a state-of-the-art BFS implementation using a cycle-accurate processor simulator. The results reveal that the conventional address mapping schemes of modern memory controllers do not efficiently exploit row buffers in DRAM. Thus, we propose a new scheme called per-row channel interleaving and improve the DRAM power efficiency by 30.3% compared to a conventional scheme for a certain simulator setting. Moreover, we demonstrate that this proposed scheme is effective for various configurations of memory controllers.	algorithm;breadth-first search;data structure;dynamic random-access memory;forward error correction;locality of reference;memory access pattern;memory controller;parallel computing;performance per watt;sequential access;simulation;social network;traffic analysis	Satoshi Imamura;Yuichiro Yasui;Koji Inoue;Takatsugu Ono;Hiroshi Sasaki;Katsuki Fujisawa	2016	2016 High Performance Graph Data Management and Processing Workshop (HPGDMP)		parallel computing;real-time computing;memory rank;computer hardware;computer science	HPC	-5.36105891633057	49.10304446041978	176478
9489cae112deebefdae57379524d48ecefce04e9	code generation in the polytope model	functional programming languages;processor scheduling mathematical model code standards scheduling algorithm merging;code generation;automatic programming;parallelizing compilers code generation polytope model automatic parallelization nested loops mathematical model high quality parallelism;heap storage management;parallelising compilers;stream processing;automatic programming parallelising compilers;nondeterminate merge	Automatic parallelization of nested loops, based on a mathematical model, thepolytope model , has been improved significantly over the last decade: state-of-theart methods allow flexible distributions of computations in space and time, which lead to high-quality parallelism. However, these methods have not found their way into practical parallelizing compilers due to the lack of code generation schemes which are able to deal with the new-found flexibility. To close this gap is the purpose of this paper.	automatic parallelization;code generation (compiler);compiler;computation;http 404;mathematical model;parallel computing;polytope model	Martin Griebl;Christian Lengauer;Sabine Wetzel	1998		10.1109/PACT.1998.727179	parallel computing;stream processing;computer science;theoretical computer science;operating system;programming language;functional programming;code generation;automatic parallelization	PL	-6.238369762315709	46.5056457134299	176596
dfd0059eeb9625cbf93caf55a10409516ba419f0	scalable memory hierarchies for embedded manycore systems	manycore architecture;embedded system;distributed memory hierarchy	As the size of FPGA devices grows following Moore’s law, it becomes possible to put a complete manycore system onto a single FPGA chip. The centralized memory hierarchy on typical embedded systems in which both data and instructions are stored in the off-chip global memory will introduce the bus contention problem as the number of processing cores increases. In this work, we present our exploration into how distributed multi-tiered memory hierarchies can effect the scalability of manycore systems. We use the Xilinx Virtex FPGA devices as the testing platforms and the buses as the interconnect. Several variances of the centralized memory hierarchy and the distributed memory hierarchy are compared by running various benchmarks, including matrix multiplication, IDEA encryption and 3D FFT. The results demonstrate the good scalability of the distributed memory hierarchy for systems up to 32 MicroBlaze processors, which is constrained by the FPGA resources on the Virtex-6LX240T device.	bus contention;central processing unit;centralized computing;distributed memory;embedded system;encryption;fast fourier transform;field-programmable gate array;manycore processor;matrix multiplication;memory hierarchy;moore's law;multi-core processor;real life;scalability;virtex (fpga)	Sen Ma;Miaoqing Huang;Eugene Cartwright;David L. Andrews	2012		10.1007/978-3-642-28365-9_13	distributed shared memory;shared memory;embedded system;interleaved memory;computer architecture;parallel computing;real-time computing;distributed memory;computer science;operating system;overlay;conventional memory;flat memory model;registered memory;memory map;memory management	EDA	-9.614310772535108	48.31909394301909	177320
393b28963a311226524c5650c417834de152ab1a	performance impact of operating systems' caching parameters on parallel file systems	parallel file systems;performance analysis;performance model;operating systems caching parameters	This research work investigates the peformance impact of operating systems' (OS) caching parameters on a parallel file system (PFS). Through an extensive experimental analysis, an analytical performance model is proposed in order to reflect caching effects on performance of file write operations. A qualitative and quantitative evaluation of 855 test cases over 3 different platforms was performed. Results indicate that the proposed model is effective in representing caching effects, identifying both situation and intensity of performance degradation. Observed mean absolute percentage error (MAPE) of predicted values is less than 36%.	approximation error;cache (computing);clustered file system;elegant degradation;forward secrecy;operating system;test case	Eduardo Camilo Inacio;Mario A. R. Dantas;Francieli Zanon Boito;Philippe Olivier Alexandre Navaux;Douglas Dyllon Jeronimo de Macedo	2015		10.1145/2695664.2695986	parallel computing;real-time computing;computer science;operating system;database	Metrics	-8.964589970095423	49.11862052168109	177379
1ca4b5b4bfa98182f045231061785c630196e5af	an improvement of trace scheduling for global microcode compaction	execution time;special loop compaction algorithm;global microcode compaction;global compaction algorithm;improved trace scheduling compaction;extra space;hand compaction;trace scheduling procedure;global compaction;improved trace scheduling algorithm;improved algorithm;vlsi;control store;scheduling algorithm;rom;microarchitecture;microcode	Fisher's trace scheduling procedure for global compaction has proven to be able to produce significant reduction in execution time of compacted microcode, however extra space may be sometimes required during bookkeeping, and the efficacy of compaction of microprogram loop is lower than that of hand compaction.  This paper introduces an improved trace scheduling compaction algorithm to mitigate the drawbacks mentioned above. The improved algorithm is based on a modified menu of moving microoperations, an improved trace scheduling algorithm, and a special loop compaction algorithm. Preliminary tests indicate that this global compaction algorithm gives shorter execution time and less space requirement in comparison with Fisher's algorithm.	algorithm;data compaction;fisher information;micro-operation;microcode;run time (program lifecycle phase);scheduling (computing);trace scheduling	Bogong Su;Shiyuan Ding;Lan Jin	1984		10.1145/800016.808217	computer architecture;parallel computing;real-time computing;microarchitecture;computer science;operating system;microcode;very-large-scale integration;control store;scheduling;read-only memory	Arch	-5.014671422015912	52.22133751208085	177544
36dd09a097dbc68d93460021dbf270837c9985bd	optimizing the performance of virtual machine synchronization for fault tolerance	virtual machine;virtual machine monitor;fault tolerance virtualization hypervisor checkpoint recovery;optimisation;virtualization;fault tolerant;virtualisation checkpointing fault tolerant computing optimisation synchronisation virtual machines virtual storage;hypervisor;recovery;journal;checkpointing;virtual machine monitors fault tolerance fault tolerant systems computer architecture synchronization;memory access;synchronisation;virtual machine monitors;computer architecture;fault tolerant system;fault tolerant computing;virtual machines;fault tolerant systems;synchronization;期刊论文;fault tolerance;memory tracking mechanism optimization virtual machine synchronization fault tolerance hypervisor based fault tolerance virtualization technology hbft memory access checkpointing epochs read fault reduction write fault prediction;checkpoint;high frequency;virtual storage;virtualisation	Hypervisor-based fault tolerance (HBFT), which synchronizes the state between the primary VM and the backup VM at a high frequency of tens to hundreds of milliseconds, is an emerging approach to sustaining mission-critical applications. Based on virtualization technology, HBFT provides an economic and transparent fault tolerant solution. However, the advantages currently come at the cost of substantial performance overhead during failure-free, especially for memory intensive applications. This paper presents an in-depth examination of HBFT and options to improve its performance. Based on the behavior of memory accesses among checkpointing epochs, we introduce two optimizations, read-fault reduction and write-fault prediction, for the memory tracking mechanism. These two optimizations improve the performance by 31 percent and 21 percent, respectively, for some applications. Then, we present software superpage which efficiently maps large memory regions between virtual machines (VM). Our optimization improves the performance of HBFT by a factor of 1.4 to 2.2 and achieves about 60 percent of that of the native VM.	application checkpointing;backup;epoch (reference date);fault tolerance;hypervisor;mathematical optimization;mission critical;optimizing compiler;overhead (computing);virtual machine;x86 virtualization	Jun Zhu;Zhefu Jiang;Zhen Xiao;Xiaoming Li	2011	IEEE Transactions on Computers	10.1109/TC.2010.224	embedded system;fault tolerance;parallel computing;real-time computing;computer science;operating system	HPC	-11.115929614887385	50.99856365339987	177867
102df10591f98830cc3357b47729d6f9e9af3eca	translation lookaside buffer consistency: a software approach	performance evaluation;translation lookaside buffer;hardware implementation	We discuss the translation lookaside buffer (TLB) consistency problem for multiprocessors, and introduce the Mach shootdown algorithm for maintaining TLB consistency in software. This algorithm has been implemented on several multiprocessors, and is in regular production use. Performance evaluations establish the basic costs of the algorithm and show that it has minimal impact on application performance. As a result, TLB consistency does not pose an insurmountable obstacle to multiprocessors with several hundred processors. We also discuss hardware support options for TLB consistency ranging from a minor interrupt structure modification to complete hardware implementations. Features are identified in current hardware that compound the TLB consistency problem; removal or correction of these features can simplify and/or reduce the overhead of maintaining TLB consistency in software.	algorithm;central processing unit;overhead (computing);translation lookaside buffer	David L. Black;Richard F. Rashid;David B. Golub;Charles R. Hill;Robert V. Baron	1989		10.1145/70082.68193	computer architecture;parallel computing;real-time computing;computer science;operating system;translation lookaside buffer	OS	-11.52661390842436	49.47889067171094	178361
566e1d40da15c6369770807b522666d4a638799e	performance evaluation of a non-blocking multithreaded architecture for embedded, real-time and dsp applications	decoupled architectures.;multithreaded architectures;vliw;superscalars	This paper presents the evaluation of a non-blocking, decoupled memory/execution, multithreaded architecture known as the Scheduled Dataflow (SDF). The major recent trend in digital signal processor (DSP) architecture is to use complex organizations to exploit instruction level parallelism (ILP). The two most common approaches for exploiting the ILP are Superscalars and Very Long Instruction Word (VLIW) architectures. On the other hand, our research explores a simple, yet powerful execution paradigm that is based on non-blocking threads, and decoupling of memory accesses from execution pipeline. This paper compares the execution cycles required for programs on SDF with the execution cycles required by programs on Superscalar and VLIW architectures.	blocking (computing);coupling (computer programming);dataflow;digital signal processor;embedded system;instruction-level parallelism;non-blocking algorithm;parallel computing;performance evaluation;programming paradigm;real-time transcription;signal processing;superscalar processor;thread (computing);very long instruction word	Krishna M. Kavi;Joseph Arul;Roberto Giorgi	2001			computer architecture;digital signal processing;very long instruction word;thread (computing);dataflow;architecture;superscalar;computer science	Arch	-6.478329613490407	47.544319161309105	178800
20b235701e232fc6ab10bd0ead8a19a17a13b378	signet: network-on-chip filtering for coarse vector directories	multiprocessor interconnection networks;network-on-chip;signet;coarse vector directories;directory protocols;network-on-chip filtering;scalable cache coherence;storage overheads	Scalable cache coherence is imperative as systems move into the many-core era with cores counts numbering in the hundreds. Directory protocols are often favored as more scalable in terms of bandwidth requirements than broadcast protocols; however, directories incur storage overheads that can become prohibitive with large systems. In this paper, we explore the impact that reducing directory overheads has on the network-on-chip and propose SigNet to mitigate these issues. SigNet utilizes signatures within the network fabric to filter out extraneous requests prior to reaching their destination. Overall, we demonstrate average reductions in interconnect activity of 21% and latency improvements of 20% over a coarse vector directory while utilizing as little as 25% of the area of a full-map directory.	antivirus software;broadcast domain;cache coherence;directory (computing);imperative programming;manycore processor;network on a chip;requirement;scalability	Natalie D. Enright Jerger	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		filter;embedded system;communications protocol;cache coherence;electronic engineering;parallel computing;real-time computing;coherence;telecommunications;computer science;operating system;pipeline transport;moore's law;network on a chip;particle detector;broadcasting;bandwidth;power network design;computer network	EDA	-8.464686128699336	52.37042986647812	178936
aa9f8b472e1828d2548524857217ab06745fe1f0	the effect of temperature on amdahl law in 3d multicore era	analytical models;thermal simulations;random access memory;chip multiprocessor;3dintegrated circuits;three dimensional displays multicore processing random access memory analytical models scalability temperature measurement parallel processing;multicore;three dimensional displays;multicore processing;amdahl s law;in memory computing architectures amdahl law 3d multicore era 3d chip multiprocessors thermal limit 3d cmp scalability;scalability;temperature measurement;3dintegrated circuits chip multiprocessor multicore thermal simulations amdahl s law;parallel processing;microprocessor chips	This work studies the influence of temperature on performance and scalability of 3D Chip Multiprocessors (CMP) from Amdahl's law perspective. We find that 3D CMP may reach its thermal limit before reaching its maximum power. We show that a high level of parallelism may lead to high peak temperatures even in small scale 3D CMPs, thus limiting 3D CMP scalability and calling for different, in-memory computing architectures.	amdahl's law;dynamic random-access memory;fm towns marty;high-level programming language;image scaling;in-memory processing;maximum power transfer theorem;merge sort;multi-core processor;parallel computing;scalability;task parallelism	Leonid Yavits;Amir Morad;Ran Ginosar	2016	IEEE Transactions on Computers	10.1109/TC.2015.2458865	multi-core processor;parallel processing;computer architecture;parallel computing;real-time computing;speedup;computer science	HPC	-6.2601360266823605	53.216859462048085	179051
117967a1bf6d500db05005e745593960e4555fce	an integrated framework for compiler-directed cache coherence and data prefetching	integrated approach;programacion paralela;processor scheduling;parallel programming;optimizacion compiladora;performance programme;performance improvement;shared memory systems;compiler optimization;data prefetching;execution driven simulation;cache coherence;eficacia programa;program performance;ordonnancement processeur;parallel programs;distributed shared memory;optimisation compilateur;systeme memoire partagee;memory latency;programmation parallele;shared memory multiprocessor	Cache coherence enforcement and memory latency reduction and hiding are very important and challenging problems in the design of large-scale distributed shared-memory (DSM) multiprocessors. We propose an integrated approach to solve these problems through a compiler-directed cache coherence scheme called the Cache Coherence with Data Prefetching (CCDP) scheme. The CCDP scheme enforces cache coherence by prefetching the potentially-stale references in a parallel program. It also prefetches the non-stale references to hide their memory latencies. To optimize the performance of the CCDP scheme, some prefetch hard-ware support is provided to efficiently handle these two forms of data prefetching operations. We also developed the compiler techniques utilized by the CCDP scheme for stale reference detection, prefetch target analysis and prefetch scheduling. We evaluated the performance of the CCDP scheme via execution-driven simulations of several applications from the SPEC CFP95 and CFP92 benchmark suites. The simulation results show that the CCDP scheme provides significant performance improvements for the benchmark programs studied.	cache coherence;compiler;link prefetching	Hock-Beng Lim;Pen-Chung Yew	1998		10.1007/3-540-48319-5_4	distributed shared memory;cache coherence;parallel computing;real-time computing;cas latency;computer science;operating system;optimizing compiler;database;distributed computing	Arch	-10.745837480629758	50.80864767323778	179072
6ac43f486d48f280296b102685d9ab6709f31c06	generalized just-in-time trace compilation using a parallel task farm in a dynamic binary translator	task farm;just in time compilation;instruction set architecture;just in time compiler;dynamic work scheduling;parallelization;dynamic binary translation;just in time	Dynamic Binary Translation (DBT) is the key technology behind cross-platform virtualization and allows software compiled for one Instruction Set Architecture (ISA) to be executed on a processor supporting a different ISA. Under the hood, DBT is typically implemented using Just-In-Time (JIT) compilation of frequently executed program regions, also called traces. The main challenge is translating frequently executed program regions as fast as possible into highly efficient native code. As time for JIT compilation adds to the overall execution time, the JIT compiler is often decoupled and operates in a separate thread independent from the main simulation loop to reduce the overhead of JIT compilation. In this paper we present two innovative contributions. The first contribution is a generalized trace compilation approach that considers all frequently executed paths in a program for JIT compilation, as opposed to previous approaches where trace compilation is restricted to paths through loops. The second contribution reduces JIT compilation cost by compiling several hot traces in a concurrent task farm. Altogether we combine generalized light-weight tracing, large translation units, parallel JIT compilation and dynamic work scheduling to ensure timely and efficient processing of hot traces. We have evaluated our industry-strength, LLVM-based parallel DBT implementing the ARCompact ISA against three benchmark suites (EEMBC, BioPerf and SPEC CPU2006) and demonstrate speedups of up to 2.08 on a standard quad-core Intel Xeon machine. Across short- and long-running benchmarks our scheme is robust and never results in a slowdown. In fact, using four processors total execution time can be reduced by on average 11.5% over state-of-the-art decoupled, parallel (or asynchronous) JIT compilation.	benchmark (computing);binary translation;central processing unit;compiler;cross-platform virtualization;eembc;hood method;hardware virtualization;just-in-time compilation;llvm;machine code;multi-core processor;overhead (computing);run time (program lifecycle phase);scheduling (computing);simulation;tracing (software)	Igor Böhm;Tobias J. K. Edler von Koch;Stephen C. Kyle;Björn Franke;Nigel P. Topham	2011		10.1145/1993498.1993508	single compilation unit;computer architecture;parallel computing;real-time computing;dynamic compilation;native image generator;jit spraying;computer science;just-in-time compilation;compilation error;programming language	PL	-7.503675195852124	48.414661619308916	179315
2ab20423fdcb571c3ef3f19fb12c1a50cef880ed	parallel shared-memory workloads performance on asymmetric multi-core architectures	cache storage;shared memory;performance evaluation;parallel shared memory workloads performance;memory hierarchy multi core heterogeneous multi processor performance evaluation;diverse parallelization schemes;computer architecture;performance asymmetric cores;energy consumption;multicore processing informatics throughput computer architecture power dissipation parallel processing proposals instruction sets energy consumption;multicore processing;power dissipation;heterogeneous multi processor;informatics;multiprocessing systems;memory hierarchy;proposals;high performance;private caches;parallel applications;address space;parallel processing;multi core;performance evaluation cache storage computer architecture multiprocessing systems parallel processing;asymmetric multi core architectures;instruction sets;throughput;performance evaluation parallel shared memory workloads performance asymmetric multi core architectures performance asymmetric cores diverse parallelization schemes address space private caches	Putting performance asymmetric cores inside the same processor can be a good alternative to obtain high performance per area, throughput and single-threaded performance. However, the impact of running parallel applications on this type of machine is not clear, since most of previous work focused on multi-programmed and server workloads where there is low or no dependence between threads. In this work, we analyze the impact of running parallel shared-memory programs on heterogeneous multi-core setups using six parallel applications with diverse parallelization schemes. Moreover, we show that, in some cases, with a high number of cores, it is better to put one complex core than several simple ones. The impact of sharing the address space between asymmetric cores with private caches was also investigated and the number of invalidations per write access was not greater than a comparable homogeneous configuration.	address space;file system permissions;multi-core processor;parallel computing;server (computing);shared memory;thread (computing);throughput	Felipe Lopes Madruga;Henrique Cota de Freitas;Philippe Olivier Alexandre Navaux	2010	2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2010.35	multi-core processor;parallel processing;computer architecture;parallel computing;real-time computing;computer science;operating system	HPC	-9.014177739772492	49.94352879425193	179319
de26790c27c617e3adcbbb9ea8c33c84683f35e8	data flow analysis of mpi program using dynamic analysis technique with partial execution		Message Passing Interface (MPI) is a dominant parallel programming paradigm. MPI processes communicate with each other by sending or receiving messages through communication functions. The applicationu0027s communication latency will be less if processes are scheduled on nearest cores or nodes and communication latency will be more if processes are scheduled on farthest cores or nodes.The communication latency can be reduced by using topology-aware process placement technique. In this technique, MPI processes are placed on the nearest cores if they have more communication between them. To find the communication pattern between processes, analysis of MPI program is required. Various techniques like static, symbolic and dynamic analysis are available for finding communication pattern of MPI program. These techniques are either taking more time for analysis or fail to find correct communication pattern. In this paper, we have proposed DAPE (Dynamic Analysis with Partial Execution) technique for analysis of MPI program, which finds correct communication pattern in less time as compared to existing techniques. The experimental results show that the proposed technique outperforms over the existing techniques.	data-flow analysis;dataflow architecture	Karveer B. Manwade;Dinesh B. Kulkarni	2017	Scalable Computing: Practice and Experience		latency (engineering);parallel computing;message passing interface;computer science;data-flow analysis	PL	-8.659742985680476	47.83012935349622	179340
00e34271e04743916ef004866045124e135ee858	keynote speech: avoiding the memory bottleneck through structured arrays	processing element;multiplication operator;null;parallel programs;memory bandwidth	Basic to parallel program speedup is dealing with memory bandwidth requirements. One solution is an architectural arrangement to stream data across multiple processing elements before storing the result in memory. This MISD type of configuration provides multiple operations per data item fetched from memory. One realization of this streamed approach uses FPGAs. We'll discuss both the general memory problem and some results based on work at Maxeler using FPGAs for acceleration.	data item;field-programmable gate array;misd;memory bandwidth;parallel computing;requirement;speedup;streaming media;von neumann architecture	Michael J. Flynn	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370204	memory address;cuda pinned memory;uniform memory access;shared memory;multiplication operator;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;memory refresh;computer science;theoretical computer science;operating system;distributed computing;computer memory;overlay;conventional memory;extended memory;flat memory model;programming language;registered memory;memory bandwidth;algorithm;computing with memory;memory map;memory management	Arch	-9.907348109678834	48.20498317042337	180157
9ec186524691da1acafa68b5bcb650c8d945cafd	accurate modelling of interconnection networks in vector supercomputers	interconnection network	Multistage interconnection networks are studied via a detailed simulation model. The model contains several assumptions that improve accuracy over previous studies. Performance resutts for both throughput and latencies are given, and previously-used simplifying assumptions are evaluated to determine their effect on final results. We show that accurate modeling of memory reference streams (srnde one vectors versus random references) has a relatively small effect on performance, although it is not completely insignificant. On the other hand, we show that accurate modeling of network fanout is important. That is, vector supereomputers will have more memory banks than processors, and therefore need to fan out from the processors to the memory. The characteristics of this fanout are an important performance feature. We also re-enforce previously published results which show that for vector reference patterns, switches with internal buffering perform much better than switches with input and output buffering, only. Perhaps the most interesting results involve the feedback loop from processor registers, through the memory, and back to the registers. This feedback can lead to instabilities and oscillations in the network which cause reduced network efficiency and unexpected network behavior. For exampIe, using more vector registers can, under some circumstances, reduce overall performance.	central processing unit;computer memory;fan-out;feedback;input/output;memory bank;multistage interconnection networks;network switch;processor register;simulation;supercomputer;throughput;vector processor	J. E. Smith;W. R. Taylor	1991		10.1145/109025.109091	computer science	Arch	-9.635670416661721	49.876752769580825	180354
0ae4e6351a44c2119ff62db5c74f44d812a6f709	hardware transactional memory supporting i/o operations within transactions	software;cache storage;transactional buffer overflow hardware transactional memory system architecture i o operation multicore processor cache coherent mechanism transactional thread;multi core processor;multi threading;programmability;hardware transactional memory;i o;buffer overflow;registers;memory architecture;cache coherence;hardware software programming registers buffer overflow coherence memory architecture;coherence;multi threading cache storage memory architecture;transactional memory;system architecture;programming;multi core;multi core transactional memory i o programmability;hardware	I/O operation within transactions is one of the challenges for hardware transactional memory. This paper analyses the problem of I/O operations within transactions, and proposes a hardware transactional memory system architecture based on multi-core processor and current cache coherent mechanisms. The system supports execution of transactions by adding transactional buffer and related hardware and software. I/O operations within transactions are implemented by partial commit based on commit-lock, and blocking / waking-up of transactional threads. The solution solves or avoids the problems that I/O operations within transactions faced, including rollback, transaction migration and transactional buffer overflow. The system has been implemented by simulation. Its performance is evaluated by five benchmark applications. Simulation results show that the transactional programs executed in our system outperformed traditional lock-based programs.	benchmark (computing);blocking (computing);buffer overflow;cache coherence;central processing unit;coherence (physics);component-based software engineering;input/output;multi-core processor;rollback (data management);simulation;software transactional memory;systems architecture;transaction processing	Yijing Liu;Xin Zhang;He Li;Mingxiu Li;Depei Qian	2008	2008 10th IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2008.71	multi-core processor;computer architecture;transactional memory;parallel computing;real-time computing;computer science;operating system;software transactional memory;compensating transaction;systems architecture	HPC	-11.009638813769111	49.35659288147812	180461
ae9f5601bfcc5a5e0221e47c5c77cd42641e52f1	organization and performance of a two-level virtual-real cache hierarchy	virtual memory;cache;cache coherence;mutiprocessors;memory hierarchy	We propose and analyze a two-level cache organization that provides high memory bandwidth. The first-level cache is ac- cessed directly by virtual addresses. It is small, fast, and, with- out the burden of address translation, can easily be optimized to match the processor speed. The virtually-addressed cache is backed up by a large physically-addressed cache; this second- level cache provides a high hit ratio and greatly reduces mem- ory traffic. We show how the second-level cache can be easily extended to solve the synonym problem resulting from the use of a virtually-addressed cache at the first level. Moreover, the second-level cache can be used to shield the virtually-addressed first-level cache from irrelevant cache coherence interference. Finally, simulation results show that this organization has a performance advantage over a hierarchy of physically-addressed caches in a multiprocessor environment.	cpu cache	Wen-Hann Wang;Jean-Loup Baer;Henry M. Levy	1989		10.1109/ISCA.1989.714548	bus sniffing;pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;page cache;cpu cache;cache;write-once;cache invalidation;smart cache;memory organisation;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;memory map;non-uniform memory access	Arch	-9.258008239427955	52.971543151941034	180794
7d04c59cb9bfde7c4f2c1b50c544327fe3315dac	loop selection to boost thread level speculation parallelism in chip multiprocessor systems	thread level speculation parallelism;chip multiprocessor systems;software;thread level speculation;thread level parallelism;yarn;integer application;program control structures;chip multiprocessor;floating point application loop selection thread level speculation parallelism chip multiprocessor systems spec cpu2000 integer application;spec cpu2000;data mining;automatic parallelization loop selection thread level speculation chip multiprocessor thread level parallelism;yarn multiprocessing systems parallel processing concurrent computing costs information technology performance loss performance gain system recovery writing;loop selection;floating point application;program control structures floating point arithmetic multiprocessing systems parallel algorithms;optimization;floating point;multiprocessing systems;floating point arithmetic;benchmark testing;automatic parallelization;pipeline processing;parallel algorithms	A novel loop selection framework with cost evaluation to boost thread level speculation (TLS) parallelism in chip multiprocessor (CMP) systems is proposed in this paper. In order to improve the performance, a loop selection framework with the aid of TLS and profiling is added to reduce unnecessary loops parallelization for their low gains even losses under parallelization. A number of techniques for this parallelization based TLS are presented and results provided indicate the performance contribution on six SPEC CPU2000 benchmark applications. This TLS parallelization yielded an average 175% speedup on our three integer applications and an average 198% on our three floating point applications.	benchmark (computing);brute-force search;computer architecture simulator;greedy algorithm;multi-core processor;multiprocessing;parallel computing;profiling (computer programming);simulation;speculative multithreading;speedup;thread (computing)	Yue Wu;Lei Xu;Hongbin Yang	2009	2009 Ninth IEEE International Conference on Computer and Information Technology	10.1109/CIT.2009.42	computer architecture;parallel computing;real-time computing;computer science;floating point;operating system;automatic parallelization	HPC	-6.9974788635383085	50.250308874974536	181163
5061a4931a37f372830c5098887c57dbee2ec128	comparative performance evaluation of hot spot contention between min-based and ring-based shared-memory architectures	synchronization lock performance evaluation hot spot contention min based shared memory architectures ring based shared memory architectures multistage interconnection network hierarchical ring structures cache memory systems bbn tc2000 ksr1;multiprocessor interconnection networks;hierarchical rings hr;analytical models;modelizacion;evaluation performance;degradation;hot spot contention;architecture systeme;analytical models telecommunication traffic traffic control memory architecture multiprocessor interconnection networks switches large scale systems degradation computer architecture high performance computing;shared memory;performance evaluation;the bbn tc2000;hierarchical rings;multiprocessor;reseau interconnecte;gollete estrangulamiento;high performance computing;memoria compartida;evaluacion prestacion;slotted rings;multistage interconnection networks;traffic control;cache memory;bbn tc2000;multistage interconnection network;multistage interconnection network min;cache memory systems;hot spot;antememoria;synchronisation;modelisation;computer architecture;synchronisation performance evaluation shared memory systems multistage interconnection networks;antememoire;goulot etranglement;telecommunication traffic;multistage interconnexion networks;shared memory systems;ring based shared memory architectures;memory architecture;min based shared memory architectures;the ksr1;performance modeling and measurements;arquitectura sistema;ksr1;synchronization lock;hierarchical ring structures;multiprocesador;system architecture;switches;red interconectada;modeling;interconnected power system;bottleneck;memoire partagee;large scale systems;multiprocesseur	Hot spot content ion on a network-based sharedmemory architecture occurs when a large number of processors try to access a globally shared variable across the network. While Multistage Interconnection Network (MIN) and Hierarchical Ring (HR) structures are two important bases on which to build large scale shared-memory multiprocessors, the different interconnection networks and cache/memory systems of the two archltectures respond very differently to network bottleneck situations. In this paper, we present a comparative performance evaluation of hot spot effects on the MIN-based and HR-based sharedmemory architectures. Both nonblocking MIN-based and HRbased architectures are classified, and analytical models are described for understanding network differences and for evaluating hot spot performance on both architectures. The analytical comparisons indicate that HR-based architectures have the potential to handle various contentions caused by hot spots more efficiently than MIN-based architectures. Intensive performance measurements on hot spots have been conducted on the BBN TC2000 (MIN-based) and the KSRl (HR-based) machines. Performance experiments were also conducted on the practical experience of hot spots with respect to synchronization lock algorithms. The experimental results are consistent with the analytical models, and present practical observations and an evaluation of hot spots on the two types of architectures.	algorithm;analytical engine;cpu cache;central processing unit;experiment;hot spare;hotspot (wi-fi);interconnection;multistage amplifier;network congestion;performance evaluation;shared variables;shared memory	Xiaodong Zhang;Yong Yan;Robert Castañeda	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.406963	shared memory;embedded system;synchronization;parallel computing;real-time computing;multiprocessing;systems modeling;degradation;cpu cache;network switch;computer science;operating system;distributed computing;hot spot;computer network;systems architecture	HPC	-10.368174043620083	47.27683237391271	181717
adf99f6bd420cb45b210bf7a91889859893848f2	augmenting cache partitioning with thread-aware insertion/promotion policies to manage shared caches	replacement;cache partitioning;shared caches	In this paper, we augment traditional cache partitioning with thread-aware adaptive insertion and promotion policies to manage shared L2 caches. The proposed mechanism can mitigate destructive inter-thread interference, and meanwhile retain some fraction of the working set in the cache, therefore results in better performance.	cpu cache;interference (communication);working set	Xiufeng Sui;Junmin Wu;Guoliang Chen;Yixuan Tang;Xiaodong Zhu	2010		10.1145/1787275.1787292	bus sniffing;cache coherence;computer architecture;parallel computing;real-time computing;cache coloring;cache;computer science;cache invalidation;smart cache;cache algorithms;cache pollution	Arch	-9.91493944338115	52.30083956413755	182868
386cfe7eddabfb13c481dd5ebfdada1ad77c26cf	automatically selecting profitable thread block sizes for accelerated kernels		Graphics processing units (GPUs) provide high performance at low power consumption as long as resources are well utilized. Thread block size is one factor in determining a kernel's occupancy, which is a metric for measuring GPU utilization. A general guideline is to find the block size that leads to the highest occupancy. However, many combinations of block and grid sizes can provide highest occupancy, but performance can vary significantly between different configurations. This is because variation in thread structure yields different utilization of hardware resources. Thus, optimizing for occupancy alone is insufficient and thread structure must also be considered. It is the programmer's responsibility to set block size, but selecting the right size is not always intuitive. In this paper, we propose using machine learning to automatically select profitable block sizes. Additionally, we show that machine learning techniques coupled with performance counters can provide insight into the underlying reasons for performance variance between different configurations.	block size (cryptography);graphics processing unit;hardware acceleration;kernel (operating system);machine learning;programmer;thread (computing);thread block	Tiffany A. Connors;Apan Qasem	2017	2017 IEEE 19th International Conference on High Performance Computing and Communications; IEEE 15th International Conference on Smart City; IEEE 3rd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2017.58	grid;real-time computing;occupancy;kernel (linear algebra);block size;programmer;computer science;guideline;thread (computing)	HPC	-5.633321499530312	52.42894730257583	183221
bb3bb4636bd8f464f863fb799dc46b08af31a87b	improving first level cache efficiency for gpus using dynamic line protection		A modern Graphics Processing Unit (GPU) utilizes L1 Data (L1D) caches to reduce memory bandwidth requirements and latencies. However, the L1D cache can easily be overwhelmed by many memory requests from GPU function units, which can bottleneck GPU performance. It has been shown that the performance of L1D caches is greatly reduced for many GPU applications as a large amount of L1D cache lines are replaced before they are re-referenced. By examining the cache access patterns of these applications, we observe L1D caches with low associativity have difficulty capturing data locality for GPU applications with diverse reuse patterns. These patterns result in frequent line replacements and low data re-usage.  To improve the efficiency of L1D caches, we design a Dynamic Line Protection scheme (DLP) that can both preserve valuable cache lines and increase cache line utilization. DLP collects data reuse information from the L1D cache. This information is used to predict protection distances for each memory instruction at runtime, which helps maintain a balance between exploitation of data locality and over-protection of cache lines with long reuse distances. When all cache lines are protected in a set, redundant cache misses are bypassed to reduce the contention for the set. The evaluation result shows that our proposed solution improves cache hits while reducing cache traffic for cache-insufficient applications, achieving up to 137% and an average of 43% IPC improvement over the baseline.	baseline (configuration management);cpu cache;cache (computing);digital light processing;graphics processing unit;locality of reference;memory bandwidth;requirement;run time (program lifecycle phase)	Xian Zhu;Robert Wernsman;Joseph Zambreno	2018		10.1145/3225058.3225104	parallel computing;graphics processing unit;reuse;cpu cache;cache;memory bandwidth;computer science;bottleneck	Arch	-10.385749676155683	52.83765700396701	183697
97031ecd529876ce6393d9de338f8fee75c8ca51	tmt - a tlb tag management framework for virtualized platforms	architectural design;performance evaluation;virtualized platforms;tlb tag management framework;multi core architectures;translation lookaside buffer;data mining;chip;cr3 tagging;workload characteristics;computer architecture;servers;low latency;statistical analysis;virtual machines;bare metal;registers;sensitivity analysis;low latency management architecture;micro architecture design;cr3 tagging tlb tag management framework virtualized platforms multi core architectures micro architecture design 86 bare metal translation lookaside buffer tag manager table tmt low latency management architecture process specific identifiers system simulation workload characteristics sensitivity analysis;process specific identifiers;platform virtualization voice mail computer architecture hardware tagging high performance computing resource virtualization resource management management information systems laboratories;86;virtual machines multiprocessing systems performance evaluation statistical analysis;multiprocessing systems;virtual environment;switches;tag manager table;system simulation;context;tmt;tagging	The rise in multi-core architectures has led to the abundance of computing resources on a chip. Virtualization has emerged as a way to efficiently partition and share these resources. Thus, the emphasis in micro-architecture design, especially in x86, has shifted towards providing hardware support for better performance of VMs on bare metal. One of the areas of focus for these efforts is the Translation Lookaside Buffer (TLB). Recent modifications in the TLB include the addition of tags as a part of the TLB entry and the incorporation of hardware primitives to perform tag comparison during TLB lookup. In this paper we present the Tag Manager Table (TMT), a low-latency management architecture for tagging the TLB entries using process-specific identifiers (based on the CR3 register in x86), and thereby reducing the number of flushes and the miss rate in the TLB. Using a full system simulation approach, we investigate the performance benefit of these tags and explore how it varies with the size of the TMT, the TLB architecture and the workload characteristics. We also perform a sensitivity analysis and quantify the relative importance of all these factors in determining the benefit from CR3 tagging. While our focus is on virtualized platforms, this approach is equally applicable for non virtualized environments.	translation lookaside buffer	Girish Venkatasubramanian;Renato J. O. Figueiredo;Ramesh Illikkal;Donald Newell	2009		10.1109/SBAC-PAD.2009.21	embedded system;parallel computing;real-time computing;computer science;virtual machine;operating system	Metrics	-9.240701434877046	50.14337743976785	183830
4e3982d5a4513a7902507fc8a039bace5c74e74f	a study of architectural optimization methods in bioinformatics applications	instruction level parallel;optimal method;cache memory;i o overlapping;system performance;sequence alignment;instruction level parallelism;bioinformatics	Studies in the optimization of sequence alignment have been carried out in bioinformatics. In this paper, we have focused on two aspects: memory usage and execution time. Our study suggests that cache memory does not have a significant effect on system performance. Our attention then turns to optimize Smith–Waterman’s algorithm. Two instruction level methods have been proposed and 2–8 fold speed improvements have been observed after the optimization has been implemented. Further improvements on system performance have been achieved by overlapping computation with system I/O usage.	bioinformatics;cpu cache;computation;ibm system i;input/output;mathematical optimization;program optimization;run time (program lifecycle phase);sequence alignment;smith–waterman algorithm	Gamaliel Tan;L. Xu;Z. Dai;Shouhua Feng;Ninghui Sun	2007	IJHPCA	10.1177/1094342007078175	computer architecture;parallel computing;cpu cache;computer science;theoretical computer science;operating system;sequence alignment;computer performance;instruction-level parallelism	HPC	-6.699425594859876	48.88648298095185	184295
b4dbe952ef34b77549d25b6e7ff4b10fafb721f9	performance modeling based on multidimensional surface learning for performance predictions of parallel applications in non-dedicated environments	eigenvalues and eigenfunctions;prediction error;rational polynomials;predictive models multidimensional systems curve fitting kernel polynomials analytical models system testing concurrent computing supercomputers degradation;software performance evaluation curve fitting eigenvalues and eigenfunctions parallel processing polynomials;software performance evaluation;supercomputer education research centre;rational polynomials performance modeling multidimensional surface learning performance predictions parallel applications nondedicated environments parallel eigenvalue problem multidimensional curve fitting model;polynomials;multi dimensional;performance predictions;parallel eigenvalue problem;performance model;multidimensional curve fitting model;multidimensional surface learning;performance prediction;curve fitting;performance modeling;parallel applications;parallel processing;nondedicated environments	Modeling the performance behavior of parallel applications to predict the execution times of the applications for larger problem sizes and number of processors has been an active area of research for several years. The existing curve fitting strategies for performance modeling utilize data from experiments that are conducted under uniform loading conditions. Hence the accuracy of these models degrade when the load conditions on the machines and network change. In this paper, we analyze a curve fitting model that attempts to predict execution times for any load conditions that may exist on the systems during application execution. Based on the experiments conducted with the model for a parallel eigenvalue problem, we propose a multi-dimensional curve-fitting model based on rational polynomials for performance predictions of parallel applications in non-dedicated environments. We used the rational polynomial based model to predict execution times for 2 other parallel applications on systems with large load dynamics. In all the cases, the model gave good predictions of execution times with average percentage prediction errors of less than 20%	analysis of algorithms;approximation algorithm;central processing unit;computation;curve fitting;execution pattern;expect;experiment;input/output;load (computing);multiprocessing;parallel computing;performance prediction;polynomial;preprocessor;time complexity	Jay Yagnik;H. A. Sanjay;Sathish S. Vadhiyar	2006	2006 International Conference on Parallel Processing (ICPP'06)	10.1109/ICPP.2006.60	parallel processing;mathematical optimization;parallel computing;computer science;theoretical computer science;machine learning;mean squared prediction error;statistics;polynomial;curve fitting	HPC	-8.931691422116728	48.12464166870438	184380
03cea526b70e33039e07beb5d811f506c0a758a8	experimental implementation of dynamic access ordering	buffer storage;performance evaluation;storage management;stream memory controller;caching;decoupling;dynamic access ordering;high-performance microprocessor;memory access patterns;memory bandwidth;memory subsystem;performance bottleneck;vector-like algorithms	As microprocessor speeds increase, memory bandwidth is rapidly becoming the performance bottleneck in the execution of vector- like algorithms. Although caching provides adequate performance for many problems, caching alone is an insufficient solution for vector applications with poor temporal and spatial locality. More- over, the nature of memories themselves has changed. Current DRAM components should not be treated as uniform access-time RAM: achieving greater bandwidth requires exploiting the charactistics of components at every level of the memory hierarchy. This paper describes hardware-assisted access ordering and our hardware development effort to build a Stream Memory Controller (SMC) that implements the technique for a commercially available high-performance microprocessor, the Intel i860. Our strategy augments caching by combining compile-time detection of memory access patterns with a memory subsystem that decouples the order of requests generated by the processor from that issued to the memory system. This decoupling permits requests to be issued in an order that optimizes use of the memory system.	algorithm;binocular disparity;cpu cache;compile time;compiler;computation;dynamic random-access memory;graph bandwidth;locality of reference;memory bandwidth;memory controller;memory hierarchy;microprocessor;principle of locality;random access;simulation	Sally A. McKee;Robert H. Klenke;Andrew J. Schwab;William A. Wulf;Steven A. Moyer;James H. Aylor;Charles Y. Hitchcock	1994			uniform memory access;distributed shared memory;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;sliding mode control;memory refresh;computer hardware;computer science;operating system;decoupling;sequence;overlay;conventional memory;extended memory;flat memory model;computer applications;registered memory;memory bandwidth;bandwidth;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management;memory ordering	Arch	-10.845401297854165	50.354436987772324	184400
f3687e020feff3ebbb7191c634a822a4202ccdee	a profiling tool for detecting cache-critical data structures	performance monitoring;cache optimization;data structure;parallel applications	A poor cache behavior can significantly prohibit achieving high speedup and scalability of parallel applications. This means optimizing a program with respect to cache locality can potentially introduce considerable performance gain. As a consequence, programmers usually perform cache locality optimization for acquiring the expected performance of their applications. Within this work, we developed a data profiling tool dprof with the goal of supporting the users in this task by allowing them to detect the optimization targets in their programs. In contrast to similar tools which mostly focus on code regions, we address data structures because they are the direct objects that programmers have to work with. Based on the Performance Monitoring Unit (PMU) provided by modern processors, dprof is capable of finding cache-critical variables, arrays, or even a segment of an array. It can also locate theses access hotspots to the most concrete position such as individual functions and code lines. This feature allows the user to apply dprof for efficient cache optimization.	cpu cache;central processing unit;data structure;locality of reference;mathematical optimization;power management unit;profiling (computer programming);programmer;scalability;sensor;speedup	Jie Tao;Tobias Gaugler;Wolfgang Karl	2007		10.1007/978-3-540-74466-5_7	parallel computing;real-time computing;data structure;cache;computer science;cache invalidation;operating system;database;distributed computing;programming language;cache algorithms	HPC	-6.790425453662528	48.89109627401147	184605
09a5136de811f4abdf8a92086c0e4c14b6e5b0cb	simulation points for spec cpu 2006	analytical models;dynamic instruction count spec cpu 2006 benchmark statistical techniques simpoint methodology;computer architecture;accuracy;computational modeling;statistical analysis;clustering algorithms;predictive models;floating point;statistical techniques;predictive models computational modeling computer simulation instruction sets acceleration hardware aggregates runtime;statistical analysis computer architecture;benchmark testing;data models	Increasing sizes of benchmarks make detailed simulation an extremely time consuming process. Statistical techniques such as the SimPoint methodology have been proposed in order to address this problem during the initial design phase. The SimPoint methodology attempts to identify repetitive, long, large-grain phases in programs and predict the performance of the architecture based on its aggregate performance on the individual phases. This study attempts to compare accuracy of the SimPoint methodology for the SPEC CPU 2006 benchmark suite with that of SPEC CPU 2000 and to study the large-grain phases in the two benchmark suites using the SimPoint methodology. We find that there has not been a significant increase in the number of simulation points required to accurately predict the behavior of the programs in SPEC CPU 2006, despite its significantly larger data footprint and dynamic instruction count. We also find that the programs in both benchmark suites have similar characteristics in terms of the number of phases that contribute significantly towards overall behavior, further emphasizing the similarity between the two benchmark suites with respect to the number of simulation points required for similar accuracies.	aggregate data;benchmark (computing);cpu cache;central processing unit;computer architecture;feedback;ibm notes;mean squared error;memory footprint;specfp;simulation;thread (computing);vii;working set size	Arun Asokan Nair;Lizy Kurian John	2008	2008 IEEE International Conference on Computer Design	10.1109/ICCD.2008.4751891	embedded system;data modeling;benchmark;sdet;computer architecture;real-time computing;simulation;computer science;floating point;operating system;machine learning;accuracy and precision;predictive modelling;cluster analysis;computational model;statistics	Robotics	-6.598908373166657	50.91219652875575	184606
408bc5a9ab2e361d9349ff0e8fd447fd8374c5b9	unified compiler algorithm for optimizing locality, parallelism and communication in out-of-core computations		This paper presents compiler algorithms to optimize outof-core programs. These algorithms consider loop and data layout transformations in a tied framework. The performance of an out-of-core loop nest containing many references can be improved by a combination of restructuring the loops and file layouts. This approach considers array references one-by-one and attempts to optimize each reference for parallelism and locality. When there are references for which parallelism optimizations do not work, communication is vectorized so that data transfer can be performed before the innermost tiling loop. Preliminary re.suIts from handcompiles on IBM SP-2 and Intel Paragon show that this approach reduces the execution time, improves the bandwidth speedup and overall speedup. In addition, we extend the base algorithm to work with file layout constraints and show how it can be used for optimizing programs consisting of multiple loop nests.	amdahl's law;computation;intel paragon;locality of reference;optimizing compiler;out-of-core algorithm;parallel computing;run time (program lifecycle phase);speedup;tiling window manager	Mahmut T. Kandemir;Alok N. Choudhary;J. Ramanujam;Meenakshi A. Kandaswamy	1997		10.1145/266220.266228	computer architecture;parallel computing;compiler correctness;computer science;theoretical computer science;data parallelism;instruction-level parallelism;implicit parallelism;task parallelism	PL	-6.994582114836678	48.431971572154374	185281
d1e7215a1b8011b963d8abc368fee9f2265428ed	shared-cache simulation for multi-core system with lru2-mru collaborative cache replacement algorithm	cache storage;replacement algorithm;collaboration;prediction algorithms;mru shared cache miss ratio stack distance replacement algorithm mpki lru2;shared cache;testing;stack distance;mpki;multiprocessing systems cache storage;computational modeling;miss ratio;collaboration algorithm design and analysis prediction algorithms testing multicore processing computational modeling data models;multicore processing;lru2;cache replacement algorithm optimization l2 shared cache simulation multicore system lru2 mru collaborative cache replacement algorithm stack distance mpki misses per thousand instruction miss ratio;multiprocessing systems;algorithm design and analysis;mru;data models	The L2 shared cache is an important resource for multi-core system. The cache replacement algorithm of L2 shared cache is one of the key factors in judging whether the L2 shared cache of multi-core system is efficient. In this paper, we study shared-cache simulation for multi-core with the LRU2-MRU collaborative cache replacement algorithm. We propose a theoretical foundation for LRU2-MRU to show the property, test the stack distance of the LRU2-MRU algorithm. In addition, the simulation results demonstrate that the MPKI (misses per thousand instructions) of LRU2-MRU is lower than other cache replacement algorithm, and the miss ratio for shared-cache can be reduce through cache replacement algorithm optimization.	cpu cache;call stack;mathematical optimization;multi-core processor;page replacement algorithm;simulation	Shan Ding;Shiya Lui;Yuanyuan Li	2012	2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2012.112	bus sniffing;multi-core processor;shared memory;data modeling;least frequently used;pipeline burst cache;algorithm design;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;prediction;cache;computer science;write-once;cache invalidation;operating system;adaptive replacement cache;software testing;smart cache;mesi protocol;computational model;cache algorithms;cache pollution;mesif protocol;collaboration	EDA	-10.898087481877381	51.62269685296235	185480
45cd6063b2d8879c49111f64c727da19c70a80e2	decomposability, instabilities, and saturation in multiprogramming systems	hierarchy;performance evaluation;networks of queues;multiprogramming;aggregation;paging;system levels;thrashing;instabilities;saturation;dynamic behavior	A step-by-step approach to model the dynamic behavior and evaluate the performance of computing systems is proposed. It is based on a technique of variable aggregation and the concept of nearly decomposable systems, both borrowed from Econometrics. This approach is taken in order to identify in multiprogramming paging systems (i) unstable regimes of operations and (ii) critical computing loads which bring the system into states of saturation. This analysis leads to a more complete definition of the circumstances in which “thrashing” can set in.	computer multitasking;control theory;paging;thrashing (computer science)	Pierre-Jacques Courtois	1975	Commun. ACM	10.1145/360881.360887	real-time computing;thrashing;computer multitasking;computer science;theoretical computer science;operating system;distributed computing;saturation;hierarchy;paging	HPC	-11.698251567749155	49.18304077496709	185521
33bcecd9f81667c329d4fd1076ee071ff2dbe6d2	retrospective: a low-overhead coherence solution for multiprocessors with private cache memories	cache memory	The cache coherence solution proposed in this paper, now referred to as “Illinois Protocol”, had its origin in my work on performance modeling and analysis of multiprocessors. Having completed the work on performance of multiprocessor interconnection networks in 1978 and first published in ISCA-1979 [1], I embarked upon extending the analysis to multiprocessors with cache memories. The analytical method used in [l] was readily extendible to more complex situations involving transactions between processors, caches, interconnection networks and memories. The interconnection network used was either crossbar or multi-stage Delta Network [l]. This analytical work I completed in Fall of 1980, and was later published in the Transactions on Compufers [2]. The focus of these two papers was interference in the interconnection network with or without cache and therefore cache coherence did not get much attention, neither did bus based systems. However, things changed in 1981 when I came across a research project that Ed Davidson and his students were conducting at Illinois. Davidson had built a multiprocessor, called AMP-1 using the microprocessor Motorola 6800 and a synchronous bus. This system was designed around 1977-78 time frame by Bob Horst and Roy Kravitz and is described in an ISCA-1980 paper [3]. Several others were involved in performance modeling and measurement of the AMP-l, notably Joel Emer and David Yen. This is when I thought I could use my multiprocessor analytical method of [2] for single bus multiprocessor systems. While the AMP-1 did not have private cache, it still raised my interest in analyzing a bus based system with private caches. In 1982, I was familiar with then prevalent microprocessor buses, namely Intel Multibus and Motorola VME Bus. I thought I should model a hypothetical multiprocessor system with caches and a bus like VME or Multibus. I was teaching a hardware lab that designed an interface for Multibus and as a result I was very cognizant of the lowest level details of its bus protocol. To model such a bus I needed to know various events that caused bus activities. A survey of literature found no bus based system with private caches. Most cache papers were directory based protocols and in addition the interconnection either was a crossbar or not mentioned. So I decided to just assume some arbitrary cache protocol. After all my goal was to provide a model and analytical method for such a system, not invent new cache protocols. As it happens with many innovations, they are often unplanned! So in Fall 1982, to make my analysis more realistic, I decided to define a cache protocol that had some practicality and low cost in relation to bus interfaces for Multibus. It was not very difficult to come up with a reasonable cache protocol. In that Fall of 1982, Marc Papamarcos started his M.S. Thesis under my supervision. He was very familiar with VME bus and Motorola microprocessors. So I asked him to work on a hardware implementation of this cache protocol for the Motorola VME bus. It so happens that the protocol is not directly implementable on either Multibus or VME bus without extending the capabilities of the bus in some way. However, we made sure that any modification to the bus were simple, practical and of low cost. Mark carried out a very detailed hardware design for the cache controller [4]. Mark also worked out details for implementing an indivisible read-modify-write for the Motorola 68K.	bus (computing);cpu cache;cache coherence;central processing unit;crossbar switch;directory (computing);extensibility;indivisible;interconnection;interference (communication);marc (archive);mesi protocol;marc blank;microprocessor;motorola 68000;multibus;multiprocessing;operational amplifier;profiling (computer programming);read-modify-write;vmebus	Janak H. Patel	1998		10.1145/285930.285947	bus sniffing;cache coherence;computer architecture;snoopy cache;parallel computing;cache coloring;cpu cache;computer hardware;cache;computer science;write-once;cache invalidation;operating system;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	Arch	-11.352885017302098	47.951826591614726	185544
1621ba8bce09b38111752f3f5e3f094d6d77a5a2	data parallel address architecture	data parallel memory systems;data parallel;data parallel address architecture;write access;multimedia application;random access memory streaming media bandwidth delay memory management switches parallel processing scheduling computer architecture;dram bandwidth;read access;parallel architectures;memory systems;write access data parallel address architecture data parallel memory systems dram bandwidth read access;parallel memories;dram chips;parallel memories dram chips parallel architectures;reading and writing	Data parallel memory systems must maintain a large number of outstanding memory references to fully use increasing DRAM bandwidth in the presence of increasing latency. At the same time, the throughput of modern DRAMs is very sensitive to access pattern's due to the time required to precharge and activate banks and to switch between read and write access. To achieve memory reference parallelism a system may simultaneously issue references from multiple reference threads. Alternatively multiple references from a single thread can be issued in parallel. In this paper, we examine this tradeoff and show that allowing only a single thread to access DRAM at any given time significantly improves performance by increasing the locality of the reference stream and hence reducing precharge/activate operations and read/write turnaround. Simulations of scientific and multimedia applications show that generating multiple references from a single thread gives, on average, 17% better performance than generating references from two parallel threads	benchmark (computing);computer simulation;data parallelism;dynamic random-access memory;elegant degradation;file system permissions;interference (communication);locality of reference;parallel computing;principle of locality;scheduling (computing);thread (computing);throughput	Jung Ho Ahn;William J. Dally	2006	IEEE Computer Architecture Letters	10.1109/L-CA.2006.4	uniform memory access;computer architecture;parallel computing;memory rank;sense amplifier;cas latency;computer hardware;computer science	Arch	-9.300761479309555	52.4979251811675	186331
a711f11c37e8023b36e040340ebadce03e6ffaea	the accuracy of trace-driven simulations of multiprocessors	processing element;time dependent;first come first serve;performance metric;memory systems;performance prediction;trace driven simulation	In trace-driven simulation, traces generated for one set of system characteristics are used to simulate a system with different characteristics. However, the execution path of a multiprocessor workload may depend on the order of events occurring on different processing elements. The event order, in turn, depends on system charcteristics such as memory-system latencies and buffer-sizes. Trace-driven simulations of multiprocessor workloads are inaccurate unless the dependencies are eliminated from the traces.We have measured the effects of these inaccuracies by comparing trace-driven simulations to direct simulations of the same workloads. The simulators predicted identical performance only for workloads whose traces were timing-independent. Workloads that used first-come first-served scheduling and/or non-deterministic algorithms produced timing-dependent traces, and simulation of these traces produced inaccurate performance predictions. Two types of performance metrics were particularly affected: those related to synchronization latency and those derived from relatively small numbers of events. To accurately predict such performance metrics, timing-independent traces or direct simulation should be used.	algorithm;computer simulation;multiprocessing;scheduling (computing);synchronization (computer science);tracing (software)	Stephen R. Goldschmidt;John L. Hennessy	1993		10.1145/166955.167001	parallel computing;real-time computing;computer hardware;computer science;operating system	Metrics	-9.664619039922469	49.778508128625134	186834
2e7751e68a02537aa06c3abb13f6379d3042053e	instruction issue logic for high-performance, interruptable pipelined processors	interruptable pipelined processor;instruction issue logic	The performance of pipelined processors is severely limited by data dependencies. In order to achieve high performance, a mechanism to alleviate the effects of data dependencies must exist. If a pipelined CPU with multiple functional units is to be used in the presence of a virtual memory hierarchy, a mechanism must also exist for determining the state of the machine precisely. In this paper, we combine the issues of dependency-resolution and preciseness of state. We present a design for instruction issue logic that resolves dependencies dynamically and. at the same time, guarantees a precise state of the machine, without a significant hardware overhead. Detailed simulation studies for the proposed mechanism, using the Lawrence Livermore loops as a benchmark, are presented.	benchmark (computing);central processing unit;data dependency;livermore loops;memory hierarchy;overhead (computing);pipeline (computing);simulation	Gurindar S. Sohi;Sriram Vajapeyam	1998		10.1145/285930.285992	computer architecture;parallel computing;real-time computing;computer science;virtual memory;operating system;programming language	Arch	-8.165548308606501	51.050417054879276	186842
1604516f4afe7038c32e189ae85c6c3ad44f23ed	estimating the overlap between dependent computations for automatic parallelization	cluster computing;programming language;data dependence;automatic parallelization	Researchers working on the automatic parallelization of programs have long known that too much parallelism can be even worse for performance than too little, because spawning a task to be run on another CPU incurs overheads. Autoparallelizing compilers have therefore long tried to use granularity analysis to ensure that they only spawn off computations whose cost will probably exceed the spawn-off cost by a comfortable margin. However, this is not enough to yield good results, because data dependencies may also limit the usefulness of running computations in parallel. If one computation blocks almost immediately and can resume only after another has completed its work, then the cost of parallelization again exceeds the benefit. We present a set of algorithms for recognizing places in a program where it is worthwhile to execute two or more computations in parallel that pay attention to the second of these issues as well as the first. Our system uses profiling information to compute the times at which a procedure call consumes the values of its input arguments and the times at which it produces the values of its output arguments. Given two calls that may be executed in parallel, our system uses the times of production and consumption of the variables they share to determine how much their executions would overlap if they were run in parallel, and therefore whether executing them in parallel is a good idea or not. We have implemented this technique for Mercury in the form of a tool that uses profiling data to generate recommendations about what to parallelize, for the Mercury compiler to apply on the next compilation of the program. We present preliminary results that show that this technique can yield useful parallelization speedups, while requiring nothing more from the programmer than representative input data for the profiling run.	algorithm;automatic parallelization;central processing unit;compiler;computation;data dependency;mercury;parallel computing;profiling (computer programming);programmer;spawn (computing);subroutine;the times	Paul Bone;Zoltan Somogyi;Peter Schachte	2011	TPLP	10.1017/S1471068411000184	parallel computing;real-time computing;computer cluster;computer science;theoretical computer science;programming language;automatic parallelization	PL	-7.7222149208614	48.41002965655858	186921
07508d419d9abfb99a5df0413ae8ef0f76bc2e0f	a technique for obtaining kernel mode address traces on a pentium-based linux system		Simulation and performance analysis of memory hierarchies require traces of memory address references. There are several software-based techniques to obtain the address references of user application programs, and cache and TLB simulations are usually driven by these sorts of traces. However, kernel mode address references are typically not captured by user tracing methods and are thus absent from the simulations. For some workloads, operating system activity has a significant impact on cache and TLB performance and therefore should be represented in the traces. This paper examines a single-step interrupt technique to obtain kernel-mode address references made by a Linux operating system running on an Intel x86 processor. Kernel address traces also allow a systems programmer to identify the frequent paths within the operating system and to guide optimizations.	algorithm;cpu cache;kernel (operating system);linux;memory address;memory hierarchy;operating system;programmer;protection ring;simulation;system programming;tracing (software);translation lookaside buffer;x86	Sachin Shirhatti;Mark Smotherman	1997		10.1145/2817460.2817474	parallel computing;computer hardware;computer science;kernel virtual address space;operating system;logical address	OS	-10.171494339537196	48.84406849202564	187054
0394d0ea1bedcd8a133b6aa90c24cb37f7e7abbf	design and implementation of a hardware checkpoint/restart core	software;performance evaluation checkpointing cmos logic circuits field programmable gate arrays linux multiprocessing systems;performance evaluation;checkpointing;manycore processors hardware checkpoint restart core design hardware checkpoint restart core implementation fpga circuit performance analysis circuit size analysis solid state drive sata2 linux server pcie sata2 host bus adaptor cmos chip;cmos logic circuits;nonvolatile memory;checkpointing hardware bandwidth software field programmable gate arrays nonvolatile memory solids;bandwidth;linux;multiprocessing systems;field programmable gate arrays;solids;hardware	A fast hardware-based checkpoint-restart mechanism is proposed in this paper. A circuit was developed and implemented on an FPGA as a proof-of-concept. Further the size and performance of this circuit was analyzed by instrumenting the cores and taking measurements with a commercial solid state (SATA2) drive. The same tests were measured using a modern Linux server with a conventional PCIe SATA2 host bus adaptor. The results suggest that the circuit would be a tiny fraction of a modern CMOS chip (less than 2%) while providing a significant performance advantage over a software-only solution.	algorithm;application checkpointing;cmos;central processing unit;field-programmable gate array;general-purpose markup language;host adapter;instrumentation (computer programming);linux;manycore processor;non-volatile memory;overhead (computing);pci express;serial ata;server (computing);solid-state drive;system image;transaction processing system;transistor;volatile memory	Ashwin A. Mendon;Ron Sass;Zachary K. Baker;Justin L. Tripp	2012	IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN 2012)	10.1109/DSNW.2012.6264676	computer architecture;parallel computing;real-time computing;non-volatile memory;computer science;operating system;solid;linux kernel;bandwidth;field-programmable gate array	EDA	-10.05898084681986	48.19580006300518	187914
4f80f4a8a1a98f3267ec0279e712990ffad020a3	yasmin: efficient intra-node communication using generic sockets		Nowadays, virtual machines are becoming widely used and their range of applications include a large number of scientific fields. From HPC to IaaS, communication between co-located VMs is a critical factor of efficiency. In our paper, we examine communication methods between VMs located in the same physical node, optimizing communication cost without sacrificing upper-layer API compatibility. We present YASMIN (Yet Another Shared Memory Implementation for Intra-Node), a generic socket-compliant framework for intra-node communication in the Xen hypervisor. We build on the concept of Vchan, a Xen library for intra-node communication between different VMs and we use Xen granting and signaling mechanisms to provide an efficient communication framework. The key of our design is the transport layer which runs underneath the AF VSOCK protocol family, implemented as a dynamically inserted module. We are able to achieve 4.4x higher bandwidth rate and 65% lower latency without the need of application binary recompilation.	anisotropic filtering;application programming interface;binary file;clock rate;cloud computing;computer performance;daemon (computing);firewall (computing);hypervisor;image scaling;kernel (operating system);mathematical optimization;netfront;network function virtualization;network packet;open-source software;openvms;protocol stack;response time (technology);router (computing);shared memory;throughput;user space;virtual machine;yet another	Michalis Rozis;Stefanos Gerangelos;Nectarios Koziris	2017		10.1007/978-3-319-67630-2_43	virtualization;computer science;parallel computing;yet another;latency (engineering);distributed computing;hypervisor;transport layer;virtual machine;shared memory;bandwidth (signal processing)	HPC	-9.954402048308511	46.75407965983585	188135
a11f1c8620ad0043ef716023aedb71939e3a44f0	scalability analysis of the spec openmp benchmarks on large-scale shared memory multiprocessors	large scale;shared memory multiprocessors;spec;shared memory multiprocessor	We present a detailed investigation of the scalability characteristics of the SPEC OpenMP benchmarks on large-scale shared memory multiprocessor machines. Our study is based on a tool that quantifies four well-defined overhead classes that can limit scalability – for each parallel region separately and for the application as a whole.	multiprocessing;openmp;overhead (computing);scalability;shared memory	Karl Fürlinger;Michael Gerndt;Jack J. Dongarra	2007		10.1007/978-3-540-72586-2_115	shared memory;computer architecture;parallel computing;distributed memory;computer science;operating system;cache-only memory architecture	HPC	-8.25581586280688	47.83247636296879	188324
01f54ee33f0002b50e9d4986dd5c4209f3a3a351	predicting performance of parallel computations	directed graphs;directed acyclic graph;forecasting;queueing network;general and miscellaneous mathematics computing and information science;convergence;performance evaluation;queueing theory directed graphs parallel processing performance evaluation;technology utilization;queueing theory;standard deviation;performance;simulation;index termsperformance prediction;indexing terms;commercial multiprocessor;accuracy;concurrent systems;service centers;array processors;laplace transformation;commercial multiprocessor performance prediction parallel computations concurrent systems task system series parallel directed acyclic graph service centers queuing network model simulation;performance model;parallel computer;parallel processing computers;programming 990200 mathematics computers;algorithms;design;performance prediction;queuing network model;concurrent processing;queuing networks;task system;parallel computations;series parallel directed acyclic graph;mimd computers;series parallel;parallel processing;concurrent computing predictive models computational modeling delay effects prediction methods testing computer errors statistical analysis error analysis added delay	An accurate and computationally efficient method for predicting the performance of a class of parallel computations running on concurrent systems is described. A parallel computation is modeled as a task system with precedence relationships expressed as a series-parallel directed acyclic graph. Resources in a concurrent system are modeled as service centers in a queuing network model. Using these two models as inputs, the method outputs predictions of expected execution time of the parallel computation and the concurrent system utilization. The method is validated against both detailed simulation and actual execution on a commercial multiprocessor. Using 100 test cases, the average error of the prediction when compared to simulation statistics is 1.7%, with a standard deviation of 1.5%; the maximum error is about 10%. >	computation	Victor Wing-Kit Mak;Stephen F. Lundstrom	1990	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.80155	parallel processing;design;parallel computing;convergence;performance;forecasting;computer science;theoretical computer science;operating system;database;distributed computing;standard deviation;queueing theory;directed acyclic graph;statistics	HPC	-9.463257950505094	48.15698622393131	188485
ec244c356910696ff2637c8f4cc5bdd5b1103902	an efficient address mapping algorithm for multichannel structure ssd	random access memory;flash translation layer solid state drive address mapping parallelism multi channel architecture;arrays;parallelism;time factors;solid state drive;flash translation layer;software algorithms;multi channel architecture;paged storage flash memories;algorithm design and analysis;parallel processing;flash memories;memory usage reduction multichannel structure ssd simulator flash translation layer solid state drive four level parallelism region based page level address mapping algorithm continuous logical page assignment logical address region level block level page level logical page mapping physical page three level mapping algorithm performance analysis;address mapping;random access memory parallel processing algorithm design and analysis arrays software algorithms time factors flash memories	In allusion to the problem that traditional flash translation layer cannot meet the requirement of the multichannel structure Solid-State Drive, considering the four-level parallelism existed in it, a region-based page level address mapping algorithm is proposed. The algorithm assigns continuous logical pages to different channels and divides the logical address into three different levels including region, block and page. The logical page can be mapped to the physical page using this three-level mapping algorithm, which can guarantee performance and reduce memory usage. To evaluate the proposed algorithm, we use the SSD simulator, and experimental and analytical results show that the algorithm can release memory space of SSD, lessen the page waste, prolong the life-span and improve the performance of the SSD.	algorithm;associative entity;computer data storage;dspace;embedded system;ftl: faster than light;flash file system;flash memory controller;garbage collection (computer science);parallel computing;region-based memory management;simulation;solid-state drive	Xiaole Cai;Yahui Li;Yadi Zhang;Peng Li	2015	2015 11th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2015.37	flash file system;parallel processing;algorithm design;parallel computing;real-time computing;page fault;page replacement algorithm;computer science;theoretical computer science;operating system;algorithm	EDA	-11.447834478459862	51.28327949728419	188486
32cf05b9827462f658c7c18886f6cb3c3bd104af	energy efficient comparators for superscalar datapaths	instruction level parallel;cache storage;low power datapath;issue queue;energy efficient;logic circuits;energy dissipation;65;parallel architectures content addressable storage benchmark testing logic circuits comparators circuits cache storage;out of order execution;low power;parallel architectures;comparators circuits;low power datapath energy efficient comparators;associative memories logic circuits comparators cache memories parallel architectures;energy efficient comparators superscalar datapaths instruction level parallelism content addressable logic memory hierarchy dissipate on match comparator designs microarchitectural level statistics realistic bit patterns spec 2000 benchmarks load store queues dependency checking logic 32 bit comparator;memory hierarchy;energy efficient comparators;content addressable storage;benchmark testing	Modern superscalar datapaths use aggressive execution reordering to exploit instruction-level parallelism. Comparators, either explicit or embedded into content-addressable logic, are used extensively throughout such designs to implement several key out-of-order execution mechanisms and support the memory hierarchy. The traditional comparator designs dissipate energy on a mismatch in any bit position. As mismatches occur with a much higher frequency than matches in many situations, considerable improvements in energy dissipation are to be gained by using comparators that dissipate energy predominantly on a full match and little or no energy on partial or complete mismatches. We make two contributions. First, we introduce a series of dissipate-on-match comparator designs, including designs for comparing long arguments. Second, we show how comparators, used in modern datapaths, can be chosen and organized judiciously based on the microarchitectural-level statistics to minimize the energy dissipation. We use the actual layout data and the realistic bit patterns of the comparands (obtained from the simulated execution of SPEC 2000 benchmarks) to show the energy impact from the use of the new comparator designs. For the same delay, the proposed 8-bit comparators dissipate 70 percent less energy than the traditional designs if used within issue queues and 73 percent less energy if used within load-store queues. The use of the proposed 6-bit comparators within the dependency checking logic is shown to increase the energy dissipation by 65 percent on the average compared to the traditional designs. We also find that the use of a hybrid 32-bit comparator, comprised of three traditional 8-bit blocks and one proposed 8-bit block, is the most energy-efficient solution for the use in the load-store queue, resulting in 19 percent energy reduction compared to the use of four traditional 8-bit blocks used to implement a 32-bit comparator.	32-bit;8-bit;bit numbering;comparator;datapath;embedded system;instruction-level parallelism;memory hierarchy;microarchitecture;out-of-order execution;parallel computing;superscalar processor	Dmitry V. Ponomarev;Gürhan Küçük;Oguz Ergin;Kanad Ghose	2004	IEEE Transactions on Computers	10.1109/TC.2004.29	embedded system;benchmark;computer architecture;parallel computing;real-time computing;logic gate;computer science;out-of-order execution;dissipation;operating system;efficient energy use	Arch	-6.843524158287636	53.31328544052258	188586
708dfea923a73fe774ee1046adb25d83a9d5aae2	scheduling of conditional branches using ssa form for superscalar/vliw processors	vliw processor;optimization technique;processor scheduling;vliw processors;instruction set;processor scheduling vliw optimal scheduling computer science parallel processing delay scheduling algorithm runtime dynamic scheduling hardware;compensation code;complexity;runtime;conditional branches scheduling;instruction set conditional branches scheduling ssa vliw processors superscalar processors optimization very long instruction word processors global scheduling conditional branches code motion compensation code complexity;vliw;scheduling algorithm;parallel architectures;computational complexity processor scheduling instruction sets parallel architectures;computational complexity;optimal scheduling;superscalar processors;speculative execution;global scheduling;very long instruction word;optimization;code motion;computer science;control dependence;parallel processing;very long instruction word processors;dynamic scheduling;conditional branches;instruction sets;hardware;ssa	Global scheduling and optimization techniques are proposed to get more enough speedup for superscalar and VLIW (Very Long Instruction Word) processors. When we consider global scheduling and optimization, one of the most important issue is how to schedule conditional branches. Control dependences are caused by conditional branches and limit the scope of scheduling. Most of previous scheduling schemes are based on speculative or predicated execution techniques to overcome conditional branches. However, speculative execution requires computation for code motion and insertion of compensation code to preserve semantics. The complexity of scheduler is largely due to computation for code motion. In addition, performance is dependent on branch outcomes. Predicated execution makes it possible to schedule more simpler. But the difficulties in the design of the instruction set are a serious problem. This paper proposes scheduling method using SSA form. The scheduling algorithm can be more simpler by utilizing /spl phi/-functions aggressively because computations for code motion are not required. We don't need complex hardware support. Our scheme also makes the performance independent on the result of branch outcomes.	central processing unit;static single assignment form;superscalar processor	Seong Uk Choi;Sung-Soon Park;Myong-Soon Park	1996		10.1109/ICPADS.1996.517581	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel processing;computer architecture;parallel computing;real-time computing;dynamic priority scheduling;computer science;very long instruction word;rate-monotonic scheduling;operating system;two-level scheduling;instruction set	Logic	-6.417438936588406	50.49177266022346	188781
e454ac36ab555f1b618e4936e36dcc572f9b151e	compiler techniques for scalable performance of stream programs on multicore architectures	electrical engineering and computer science;thesis	Given the ubiquity of multicore processors, there is an acute need to enable the development of scalable parallel applications without unduly burdening programmers. Currently, programmers are asked not only to explicitly expose parallelism but also concern themselves with issues of granularity, load-balancing, synchronization, and communication. This thesis demonstrates that when algorithmic parallelism is expressed in the form of a stream program, a compiler can effectively and automatically manage the parallelism. Our compiler assumes responsibility for low-level architectural details, transforming implicit algorithmic parallelism into a mapping that achieves scalable parallel performance for a given multicore target. Stream programming is characterized by regular processing of sequences of data, and it is a natural expression of algorithms in the areas of audio, video, digital signal processing, networking, and encryption. Streaming computation is represented as a graph of independent computation nodes that communicate explicitly over data channels. Our techniques operate on contiguous regions of the stream graph where the input and output rates of the nodes are statically determinable. Within a static region, the compiler first automatically adjusts the granularity and then exploits data, task, and pipeline parallelism in a holistic fashion. We introduce techniques that data-parallelize nodes that operate on overlapping sliding windows of their input, translating serializing state into minimal and parametrized inter-core communication. Finally, for nodes that cannot be data-parallelized due to state, we are the first to automatically apply software-pipelining techniques at a coarse granularity to exploit pipeline parallelism between stateful nodes. Our framework is evaluated in the context of the StreamIt programming language. StreamIt is a high-level stream programming language that has been shown to improve programmer productivity in implementing streaming algorithms. We employ the StreamIt Core benchmark suite of 12 real-world applications to demonstrate the effectiveness of our techniques for varying multicore architectures. For a 16-core distributed memory multicore, we achieve a 14.9x mean speedup. For benchmarks that include sliding-window computation, our sliding-window data-parallelization techniques are required to enable scalable performance for a 16-core SMP multicore (14x mean speedup) and a 64-core distributed shared memory multicore (52x mean speedup). Thesis Supervisor: Saman Amarasinghe Title: Professor	benchmark (computing);central processing unit;compiler;computation;digital signal processing;distributed memory;distributed shared memory;encryption;graph (discrete mathematics);high- and low-level;holism;input/output;inter-process communication;microsoft windows;multi-core processor;parallel computing;pipeline (computing);programmer;programming language;programming productivity;scalability;serialization;software pipelining;speedup;state (computer science);stream processing;streaming algorithm;symmetric multiprocessing	Michael I. Gordon	2010			parallel computing;real-time computing;computer science;theoretical computer science;data parallelism;instruction-level parallelism;implicit parallelism	Arch	-7.264691211358464	48.12918328588863	189006
56ddd9f187a4895945ce8f4a42a2e85a1742c181	characterizing energy efficiency of i/o intensive parallel applications on power-aware clusters	energy efficiency;concurrent computing;performance evaluation;measurement;parallel i o access patterns;application software;energy efficient;high performance computing;impact factor;i o intensive parallel applications;energy efficiency application software frequency energy consumption supercomputers computer science high performance computing cloud computing concurrent computing power engineering computing;energy performance;power engineering computing;access pattern;energy consumption;high performance computer;bandwidth;parallel i o energy efficiency;power aware clusters;parallel i o;computer science;performance evaluation parallel processing;magnetic cores;frequency;access pattern energy efficiency parallel i o;parallel i o energy efficiency i o intensive parallel applications power aware clusters high performance computing parallel i o access patterns;supercomputers;parallel applications;parallel processing;benchmark testing;cloud computing	Energy efficiency and parallel I/O performance have become two critical measures in high performance computing (HPC). However, there is little empirical data that characterize the energy-performance behaviors of parallel I/O workload. In this paper, we present a methodology to profile the performance, energy, and energy efficiency of parallel I/O access patterns and report our findings on the impacting factors of parallel I/O energy efficiency. Our study shows that choosing the right buffer size can change the energy-performance efficiency by up to 30 times. High spatial and temporal spacing can also lead to significant improvement in energy-performance efficiency (about 2X). We observe CPU frequency has a more complex impact, depending on the IO operations, spatial and temporal, and memory buffer size. The presented methodology and findings are useful for evaluating the energy efficiency of I/O intensive applications and for providing a guideline to develop energy efficient parallel I/O technology.	central processing unit;data access;data buffer;input/output;interaction;mathematical optimization;parallel i/o;scalability;supercomputer	Rong Ge;Xizhou Feng;Sindhu Subramanya;Xian-He Sun	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470904	parallel computing;real-time computing;computer science;distributed computing	HPC	-6.407972234549034	49.16443751556601	189079
b0cca31e27f621c556cc39ec790f2eeeeba7de21	studying the impact of multicore processor scaling on cache coherence directories via reuse distance analysis	dissertation	Title of dissertation: Studying the Impact of Multicore Processor Scaling on Cache Coherence Directories via Reuse Distance Analysis Minshu Zhao, Doctor of Philosophy, 2015 Dissertation directed by: Professor Donald Yeung Department of Electrical and Computer Engineering Directories are one key part of a processor’s cache coherence hardware, and constitute one of the main bottlenecks in multicore processor scaling, e.g. core count and cache size scaling. Many research effects have tried to improve the scalability of the directory, but most of them only simulate a few architecture configurations. It is important to study the directory’s architecture dependency, as the CPUs continue to scale. This is because besides applications, directory behaviors are also highly sensitive to architecture. Varying core count directly affect s the amount of sharing in the directory, and varying the data cache hierarchy affects the directory access stream. But unfortunately, exploring the huge design space of multiple core counts and cache configurations is challenging using traditional architectural simulation due to the slow speed of simulations. This thesis studies the directory using multicore reuse distance analysis. It extends existing multicore reuse distance techniques, developing a method to extract directory access information from the parallel LRU stacks used to acquire privatestack reuse distance profiles. This thesis implements this method in a PIN-based profiler to study the directory behavior, including the directory access pattern and directory content, and to analyze current directory techniques. The profile results show that the directory accesses are highly dependent on cache size, exhibiting a 3.5x drop when scaling the data cache size from 16KB to 1MB; the sharing causes the ratio of directory entry to cache blocks to drop below 50%; and the majority of the accesses are to a small percentage of the directory entries. Cache simulations are performed to validate the profiling results, showing the profiled results are within 14.5% of simulation on average. This thesis also analyzes different directory techniques using the insights from the profiler. The case studies on the Cuckoo, DGD, SCD techniques and multi-level directories show that required directory size varies significantly with CPU scaling, the opportunity of compressing private data decreases with cache scaling, reducing the sharer list size is an effective technique and a small L1 directory is sufficient to capture most of the latency critical accesses respectively. Studying the Impact of Multicore Processor Scaling on Cache Coherence Directories via Reuse Distance Analysis	cpu cache;cache coherence;central processing unit;computer engineering;directory (computing);directory service;dworkin's game driver;image scaling;information privacy;multi-core processor;personal identification number;scalability;simulation;working directory	Minshu Zhao	2015			computer architecture;parallel computing;computer science;theoretical computer science	Arch	-9.024200119701108	52.29440629363314	189109
f14daf49c33e00e2e9abcd12ae5378152f18ff7c	race-free interconnection networks and multiprocessor consistency	computer networks;shared memory;bandwidth;distributed computing;network topology;sequential consistency;system performance	Modern shared-memory multiprocmors require complex interconnection networks to provide sufficient communication bandwidth between processors. They also rely on advanced memory systems that allow multiple memory operations to be made in parallel. It is expensive to maintain a high consistency level in a machine based on a general network, but for special interconnection topologies, some of these costs can he reduced. We define and study one class of interconnection networks, race-free networks. New conditions for sequential consistency are presented which show that sequential consistency can be maintained if all accesses in a multiprocessor can be ordered in an acyclic graph. We show that this can be done in racefree networks without the need for a transaction to be globally performed before the next transaction can be issued: We also investigate what is required to maintain processor consistency in race-free networks. In a race-free network which maintains processor consistency, writes may be pipelined, and reads may bypass writes. - The proposed methods reduce the latencies associated with processor write-misses to shared data.	interconnection;multiprocessing	Anders Landin;Erik Hagersten;Seif Haridi	1991		10.1109/ISCA.1991.1021604	distributed shared memory;shared memory;cache coherence;computer architecture;parallel computing;real-time computing;distributed memory;computer science;consistency model;operating system;distributed computing;computer performance;sequential consistency;network topology;bandwidth;computer network	Arch	-11.429768188150975	49.039552088180194	189188
fa61807ef2716312fdc7e25fa17aa5d522afbefc	overcoming the memory bottleneck in suffix tree construction	tree data structures algorithm design and analysis buildings concurrent computing computational modeling programming profession sorting data warehouses software libraries data mining;models of computation;tree data structures;suffix tree;sorting memory bottleneck suffix tree construction data structure external memory model random page accesses;data structures;fast algorithm;parallel computer;external memory;string matching;optimal algorithm;data structure;external memory computation	The su x tree of a string is the fundamental data structure of string processing. Recent focus on massive data sets has sparked interest in overcoming the memory bottlenecks of known algorithms for building su x trees. Our main contribution is a new algorithm for su x tree construction in which we choreograph almost all disk accesses to be via the sort and scan primitives. This algorithm achieves optimal results in a variety of sequential and parallel computational models. Two of our results are: In the traditional external memory model, in which only the number of disk accesses is counted, we achieve an optimal algorithm, both for single and multiple disk cases. This is the rst optimal algorithm known for either model. Traditional disk page access counting does not differentiate between random page accesses and block transfers involving several consecutive pages. This difference is routinely exploited by expert programmers to get fast algorithms on real machines. We adopt a simple accounting scheme and show that our algorithm achieves the same optimal tradeo for block versus random page accesses as the one we establish for	algorithm;analysis of algorithms;comparison of programming languages (string functions);computation;computational model;computer data storage;data structure;input/output;page cache;programmer;sorting;string (computer science);suffix tree;time complexity;von neumann architecture	Martin Farach-Colton;Paolo Ferragina;S. Muthukrishnan	1998		10.1109/SFCS.1998.743441	model of computation;combinatorics;parallel computing;data structure;computer science;theoretical computer science;database;compressed suffix array;tree;k-d-b-tree;programming language;algorithm;string searching algorithm	Theory	-11.645513797350075	50.12296879248414	189234
ea7da9a3ce0af76c3dcbcc51c6da2986eef95b6a	improving the demand paging performance with nand-type flash memory	nand flash memory;flash memory;paged storage;kernel;memory management;mobile device;performance evaluation;onenand flash;virtual i o segment;general purpose operating systems;virtual i o segment demand paging performance nand type flash memory onenand flash mobile devices general purpose operating systems;logic gates;operating system;nand type flash memory;virtual i o segment onenand flash;paged storage flash memories logic gates operating systems computers;driver circuits;linux;flash memory delay memory architecture costs operating systems optimization methods production random access memory high performance computing application software;high performance;operating systems computers;mobile devices;flash memories;optimization methods;demand paging performance	Because of its superior read performance and other favorable features over standard NAND flash memory, OneNAND flash has become the most promising alternative for implementing high-performance mobile devices. However, the superior read performance of OneNAND flash can not be fully utilized under the I/O architecture of existing general purpose operating systems. This paper investigates the problem in the context of demand paging, and introduces a new scheme called virtual I/O segment for solving that problem. Experiments by implementation show that the read latency of OneNAND flash based block device can be reduced as much as 54% by the use of virtual I/O segments. Also, page fault handling time of real-world embedded applications is reduced as much as 42%.	embedded system;flash memory;input/output;mobile device;nand gate;operating system;page fault;paging	Seunghwan Hyun;Sehwan Lee;Sungyong Ahn;Kern Koh	2008	2008 International Conference on Computational Sciences and Its Applications	10.1109/ICCSA.2008.69	flash file system;demand paging;parallel computing;real-time computing;computer hardware;computer science;operating system;flash memory emulator;mobile device	Embedded	-11.166578110086665	53.2709378489906	190386
e54dfca6135618dcaa22a15ea32f49506850326b	near data computation for message-passing chip-multiprocessors	memory management;computational modeling;system on chip;message systems;pipelines;message passing;algorithm design and analysis	As the CMP (Chip-Multiprocessor) scale increases' moving data to computation becomes more and more expensive in terms of latency and energy consumption. Conversely, the scheme of moving computation to data has potential to improve efficiency, especially for the irregular applications that contain graph computing, hash map or matrix multiplication. This paper proposed a near-data processing scheme for the large scale CMPs with hardware message passing support, which we called in-place computation scheme. In the in-place computation scheme, an application's critical irregular data is partitioned into on-chip memory-slices and each slice is managed by an adjacent core, as any other core intends to operate such data, it sends a message to the data owner to process it, rather than fetching the data back for computation by itself. In the paper, we described the programming model, hardware/software requirements and optimization strategies of the in-place computation scheme in details. Simulations show that, compared with conventional implementations, it can improve the performance and energy-efficiency significantly in most cases, due to following factors: 1) It greatly reduces data movements and synchronizations, 2) It decreases core-pipeline stalls and improves efficiency of memory-accesses.	be file system;cpu cache;cs-cipher;comparison sort;computation;computer simulation;degree of parallelism;hash table;in-place algorithm;instruction pipelining;manycore processor;mathematical optimization;matrix multiplication;message passing;multiprocessing;original net animation;parallel computing;programming model;requirement;software requirements;xfig	Yanhua Li;Youhui Zhang;Kunpeng Song;Haibin Wang;Weiming Zheng	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0103	system on a chip;algorithm design;parallel computing;message passing;computer science;theoretical computer science;operating system;distributed computing;pipeline transport;programming language;computational model;memory management	HPC	-7.3820112441861765	49.62691241709444	190587
3bcdf2f53f153e7b56716c387acb0b6e5ba9109e	multitasking real-time embedded gpu computing tasks	gpu computing;real time embedded tasks;physical sciences and mathematics;multitasking	In this study, we consider the specific characteristics of workloads that involve multiple real-time embedded GPU computing tasks and design several schedulers that use alternative approaches. Then, we compare the performance of schedulers and determine which scheduling approach is more effective for a given workload and why. The major conclusions of this study include: (a) Small kernels benefit from running kernels concurrently. (b) The combination of small kernels, high-priority kernels with longer runtimes, and lower-priority kernels with shorter runtimes benefits from a CPU scheduler that dynamically changes kernel order on the Fermi architecture. (c) Due to limitations of existing GPU architectures, currently CPU schedulers outperform their GPU counterparts. We also highlight the shortcomings of current GPU architectures with regard to running multiple real-time tasks, and recommend new features that would improve scheduling, including hardware priorities, preemption, programmable scheduling, and a common time concept and atomics across the CPU and GPU.	central processing unit;computer multitasking;embedded system;emoticon;fermi (microarchitecture);general-purpose computing on graphics processing units;graphics processing unit;preemption (computing);real-time clock;real-time transcription;runtime system;scheduling (computing)	Pinar Muyan-Özçelik;John D. Owens	2016		10.1145/2883404.2883408	parallel computing;real-time computing;computer science;operating system	Embedded	-5.631133865513658	51.78095766289666	190930
ae0ab5ca7885b6df4f21cd842665bb96298aae16	performance evaluation of noc architectures for parallel workloads	focusing;analytical models;parallel programming network on chip;topology;parallel programs noc architectures parallel workloads;performance evaluation;network on chip;parallel programming;system on a chip;general purpose processor;visualization;network on a chip performance analysis analytical models visualization pattern analysis topology informatics next generation networking costs character generation;character generation;performance analysis;next generation;informatics;pattern analysis;magnetic cores;parallel programs;network on a chip;parallel workloads;high performance;program processors;noc architectures;next generation networking	Network-on-Chip is the state-of-the-art approach to interconnect many processing cores in the next generation of general-purpose processors. In this context, the problem is to choose NoC architectures capable of achieving high performance for parallel programs. Therefore, the main goal of this paper is to evaluate the performance of three NoC architectures using well-known parallel workloads.	central processing unit;computer architecture;general-purpose markup language;network on a chip;performance evaluation	Henrique C. Freitas;Marco Antonio Zanata Alves;Lucas Mello Schnorr;Philippe Olivier Alexandre Navaux	2009	2009 3rd ACM/IEEE International Symposium on Networks-on-Chip	10.1109/NOCS.2009.5071450	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;visualization;computer science;network on a chip;informatics;computer network	Arch	-8.702874668248942	47.374989543667496	191714
b9986abdbe164a1ff9cfa322eb9fa2a66fbfab79	reducing false sharing and improving spatial locality in a unified compilation framework	large granularity parallelism large shared memory multiprocessors coherent caches data sharing granularity coherence unit size spatial locality parallelism unified compilation framework false sharing reduction mathematical framework multiple writer false sharing optimization techniques data transformations sgi cray origin 2000 multiprocessor loop transformations synchronization cost reduction memory subsystem performance;data transformations;cache storage;data sharing;false sharing reduction;cache locality;computer society;coherent caches;parallel programming shared memory systems synchronisation program compilers cache storage program control structures;large shared memory multiprocessors;application software;optimization technique;program control structures;data reuse;parallel programming;synchronisation;mathematical framework;parallelism;shared memory systems;loop transformations;shared memory multiprocessors;data structures;false sharing;sgi cray origin 2000 multiprocessor;optimization techniques;data transformation;application software optimizing compilers parallel machines computer society parallel processing costs multiprocessing systems program processors data structures multidimensional systems;unified compilation framework;parallel machines;memory subsystem performance;multiple writer false sharing;multiprocessing systems;spatial locality;program compilers;optimizing compilers;data sharing granularity;coherence unit size;program processors;synchronization cost reduction;parallel processing;multidimensional systems;large granularity parallelism;shared memory multiprocessor;loop and memory layout transformations	The performance of applications on large shared-memory multiprocessors with coherent caches depends on the interaction between the granularity of data sharing, the size of the coherence unit, and the spatial locality exhibited by the applications, in addition to the amount of parallelism in the applications. Large coherence units are helpful in exploiting spatial locality, but worsen the effects of false sharing. A mathematical framework that allows a clean description of the relationship between spatial locality and false sharing is derived in this paper. First, a technique to identify a severe form of multiple-writer false sharing is presented. The importance of the interaction between optimization techniques aimed at enhancing locality and the techniques oriented toward reducing false sharing is then demonstrated. Given the conflicting requirements, a compiler-based approach to this problem holds promise. This paper investigates the use of data transformations in addressing spatial locality and false sharing, and derives an approach that balances the impact of the two. Experimental results demonstrate that such a balanced approach outperforms those approaches that consider only one of these two issues. On an eight-processor SGI/Cray Origin 2000 multiprocessor, our approach brings an additional 9 percent improvement over a powerful locality optimization technique that uses both loop and data transformations. Also, the presented approach obtains an additional 19 percent improvement over an optimization technique that is oriented specifically toward reducing false sharing. This study also reveals that, in addition to reducing synchronization costs and improving the memory subsystem performance, obtaining large granularity parallelism is helpful in balancing the effects of enhancing locality and reducing false sharing, rendering them compatible.	cache coherence;coherence (physics);compiler;false sharing;locality of reference;loop invariant;mathematical optimization;multiprocessing;parallel computing;principle of locality;requirement;shared memory	Mahmut T. Kandemir;Alok N. Choudhary;J. Ramanujam;Prithviraj Banerjee	2003	IEEE Trans. Parallel Distrib. Syst.	10.1109/TPDS.2003.1195407	parallel processing;parallel computing;real-time computing;computer science;operating system;database;distributed computing;programming language;data transformation	PL	-10.634921286217862	50.03856345030477	191947
3370ef4c1a68bba20a2dd1f5c6f67d422afe7a11	using locality and interleaving information to improve shared cache performance	college park donald yeung liu;electrical engineering using locality and interleaving information to improve shared cache performance university of maryland;dissertation;wanli	Title of dissertation: Using Locality and Interleaving Information to Improve Shared Cache Performance Wanli Liu, Doctor of Philosophy, 2009 Dissertation directed by: Professor Donald Yeung Department of Electrical and Computer Engineering The cache interference is found to play a critical role in optimizing cache allocation among concurrent threads for shared cache. Conventional LRU policy usually works well for low interference workloads, while high cache interference among threads demands explicit allocation regulation, such as cache partitioning. Cache interference is shown to be tied to inter-thread memory reference interleaving granularity: high interference is caused by fine-grain interleaving while low interference is caused coarse-grain interleaving. Profiling of real multi-program workloads shows that cache set mapping and temporal phase result in the variation of interleaving granularity. When memory references from different threads map to disjoint cache sets, or they occur in distinct time windows, they tend to cause little interference due to coarse-grain interleaving. The interleaving granularity measured by runlength in workloads is found to correlate with the preference of cache management policy: fine-grain interleaving workloads perform better with cache partitioning, and coarse-grain interleaving workloads perform better with LRU. Most existing shared cache management techniques are based on working set locality analysis. This dissertation studies the shared cache performance by taking both locality and interleaving information into consideration. Oracle algorithm which provides theoretical best performance is investigated to provide insight into how to design a better practical policy. Profiling and analysis of Oracle algorithm lead to the proposal of probabilistic replacement (PR), a novel cache allocation policy. With aggressor threads information learned on-line, PR evicts the bad locality blocks of aggressor threads probabilistically while preserving good locality blocks of non-aggressor threads. PR is shown to be able to adapt to the different interleaving granularities in different sets over time. Its flexibility in tuning eviction probability also improves fairness among thread performance. Evaluation indicates that PR outperforms LRU, UCP, and ideal cache partitioning at moderate hardware cost. For single program cache management, this dissertation also proposes a novel technique: reuse distance last touch predictor (RD-LTP). RD-LTP is able to capture reuse distance information, which represents the intrinsic memory reference pattern. Based on this improved LT predictor, an MRU LT eviction policy is developed to select the right victim at the presence of incorrect LT prediction. In addition to LT predictor, another predictor: reuse distance predictors (RDPs) is proposed, which is able to predict actual reuse distance values. Compared to various existing cache management techniques, these two novel predictors deliver higher cache performance with higher prediction coverage and accuracy at moderate hardware cost. Using Locality and Interleaving Information to Improve Shared Cache Performance	algorithm;cpu cache;computer engineering;computer multitasking;emi (protocol);fairness measure;forward error correction;interference (communication);kerrison predictor;linux test project (ltp);locality of reference;microsoft windows;multiple granularity locking;online and offline;profiling (computer programming);ruby document format;working set	Wanli Liu	2009			parallel computing;computer science;database;world wide web	Arch	-9.329725031261384	52.011058965610225	193823
cce7256f569eeec1b959fbadd410550bf18e14a7	complexity effective memory access scheduling for many-core accelerator architectures	ring networks;paper;fifo dram scheduler;perforation;ring network;processor scheduling;row access locality maximisation;accelerator architectures random access memory out of order multiprocessor interconnection networks control systems bandwidth graphics logic performance loss system performance;interconnection network arbitration scheme;memory controllers;coprocessors;out of order;interconnection network;complexity effective effective memory access scheduling many core accelerator architectures dram systems memory controllers graphics processing unit architecture in order scheduling approach dram request scheduling in order first in first out dram scheduler fifo dram scheduler interconnection network arbitration scheme out of order memory request scheduling ring networks mesh networks crossbar networks bank level parallelism row access locality maximisation;chip;cuda;dram systems;memory access;many core accelerator architectures;bank level parallelism;in order scheduling approach;memory architecture;crossbar networks;scheduling;memory controller;graphics processors;on chip interconnection networks;graphics processors memory controller on chip interconnection networks;nvidia;memory systems;graphic processing unit;dram request scheduling;mesh networks;complexity effective effective memory access scheduling;computer science;in order first in first out dram scheduler;magnetic cores;out of order memory request scheduling;architecture;scheduling coprocessors dram chips memory architecture;dram chips;memory;graphics processing unit architecture;multiprocessor interconnection;throughput;hardware	"""Modern DRAM systems rely on memory controllers that employ out-of-order scheduling to maximize row access locality and bank-level parallelism, which in turn maximizes DRAM bandwidth. This is especially important in graphics processing unit (GPU) architectures, where the large quantity of parallelism places a heavy demand on the memory system. The logic needed for out-of-order scheduling can be expensive in terms of area, especially when compared to an in-order scheduling approach. In this paper, we propose a complexity-effective solution to DRAM request scheduling which recovers most of the performance loss incurred by a naive in-order first-in first-out (FIFO) DRAM scheduler compared to an aggressive out-of-order DRAM scheduler. We observe that the memory request stream from individual GPU """"shader cores"""" tends to have sufficient row access locality to maximize DRAM efficiency in most applications without significant reordering. However, the interconnection network across which memory requests are sent from the shader cores to the DRAM controller tends to finely interleave the numerous memory request streams in a way that destroys the row access locality of the resultant stream seen at the DRAM controller. To address this, we employ an interconnection network arbitration scheme that preserves the row access locality of individual memory request streams and, in doing so, achieves DRAM efficiency and system performance close to that achievable by using out-of-order memory request scheduling while doing so with a simpler design. We evaluate our interconnection network arbitration scheme using crossbar, mesh, and ring networks for a baseline architecture of 8 memory channels, each controlled by its own DRAM controller and 28 shader cores (224 ALUs), supporting up to 1,792 in-flight memory requests. Our results show that our interconnect arbitration scheme coupled with a banked FIFO in-order scheduler obtains up to 91% of the performance obtainable with an out-of-order memory scheduler for a crossbar network with eight-entry DRAM controller queues."""	baseline (configuration management);computer graphics;crossbar switch;dynamic random-access memory;fifo (computing and electronics);graphics processing unit;interconnection;interleaved memory;locality of reference;manycore processor;memory controller;microarchitecture;network topology;parallel computing;relay;resultant;router (computing);scheduling (computing);shader;throughput	George L. Yuan;Ali Bakhoda;Tor M. Aamodt	2009	2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1145/1669112.1669119	chip;embedded system;ring network;interleaved memory;throughput;parallel computing;real-time computing;memory rank;static random-access memory;cas latency;computer science;architecture;operating system;memory controller;universal memory;memory;registered memory;scheduling	Arch	-8.856845491671146	51.952313232892	194283
6f93e0325e577f49f4bed46a2adcfee4a649dc83	data reorganization in memory using 3d-stacked dram	silicon;random access memory;random access memory silicon hardware legged locomotion semantics filling;legged locomotion;semantics;filling;reorganization operation optimization memory data reorganization 3d stacked dram dram aware reshape accelerator;storage management dram chips optimisation;hardware	In this paper we focus on common data reorganization operations such as shuffle, pack/unpack, swap, transpose, and layout transformations. Although these operations simply relocate the data in the memory, they are costly on conventional systems mainly due to inefficient access patterns, limited data reuse and roundtrip data traversal throughout the memory hierarchy. This paper presents a two pronged approach for efficient data reorganization, which combines (i) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM, and (ii) a mathematical framework that is used to represent and optimize the reorganization operations.  We evaluate our proposed system through two major use cases. First, we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads. Then, we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package. We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs. For the various test cases, in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware.	central processing unit;dynamic random-access memory;global variable;graphics processing unit;in-memory database;locality of reference;math kernel library;memory hierarchy;overhead (computing);paging;parallel computing;physical address;test case;tree traversal	Berkin Akin;Franz Franchetti;James C. Hoe	2015	2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)	10.1145/2749469.2750397	computer architecture;parallel computing;real-time computing;computer hardware;computer science;operating system;semantics;silicon	Arch	-4.742180412854951	47.97476484918495	194415
b87f47877158a50dac7e06aa3dab96e7722d6a54	performance modeling of a parallel i/o system: an application driven approach		The broadening disparity between the performance of I/O devices and the performance of processors and communication links on parallel platforms is a major obstacle to achieving high performance in many parallel application domains. We believe that understanding the interactions among application I/O access patterns, parallel le systems , and I/O hardware conngurations is a prerequisite to identifying levels of I/O parallelism (i.e., the number of disks across which les should be distributed) that maximize application performance. To validate this conjecture, we constructed a series of I/O benchmarks that encapsulate the behavior of a class of I/O intensive access patterns. Performance measurements on the Intel Paragon XP/S demonstrated that the ideal distribution of data across storage devices is a strong function of the I/O access pattern. Based on this experience, we propose a simple, product form queuing network model that eeectively predicts the performance of both I/O benchmarks and I/O intensive scientiic applications as a function of I/O hardware connguration.	benchmark (computing);binocular disparity;central processing unit;input/output;intel paragon;interaction;network model;parallel i/o;parallel computing	Evgenia Smirni;Christopher L. Elford;Daniel A. Reed;Andrew A. Chien	1997			parallel computing;parallel i/o;computer science	HPC	-9.261717160633744	46.64341247700284	194742
25c6ad6e86f3402ab437418e6472e6c551f5d139	compiler-assisted data distribution for chip multiprocessors	limiting factor;chip multiprocessor;partitioning;data distribution;memory access;compiler assisted caching;non uniform cache architecture;data access;data placement	"""Data access latency, a limiting factor in the performance of chip multiprocessors, grows significantly with the number of cores in non-uniform cache architectures with distributed cache banks. To mitigate this effect, it is necessary to leverage the data access locality and choose an optimum data placement. Achieving this is especially challenging when other constraints such as cache capacity, coherence messages and runtime overhead need to be considered. This paper presents a compiler-based approach used for analyzing data access behavior in multi-threaded applications. The proposed experimental compiler framework employs novel compilation techniques to discover and represent multi-threaded memory access patterns (MMAPs). At run time, symbolic MMAPs are resolved and used by a partitioning algorithm to choose a partition of allocated memory blocks among the forked threads in the analyzed application. This partition is used to enforce data ownership by associating the data with the core that executes the thread owning the data. We demonstrate how this information can be used in an experimental architecture to accelerate applications. In particular, our compiler assisted approach shows a 20% speedup over shared caching and 5% speedup over the closest runtime approximation, """"first touch""""."""	algorithm;approximation;cpu cache;compiler;data access;distributed cache;locality of reference;overhead (computing);run time (program lifecycle phase);speedup;thread (computing)	Yong Li;Ahmed Abousamra;Rami G. Melhem;Alex K. Jones	2010	2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)	10.1145/1854273.1854335	uniform memory access;data access;parallel computing;real-time computing;limiting factor;cache;computer science;operating system;distributed computing	HPC	-9.724224188351997	51.52277383482129	194767
e10cb1a682247ece754c7e17037ffc9842cf8b18	exploiting the inter-cluster record reuse for stream processors	kernel;streaming media program processors arrays clustering algorithms kernel optimization;arrays;streaming media;clustering algorithms;storage management microprocessor chips multiprocessing systems program compilers;optimization;memory access intercluster record reuse stream processor alu cluster stream transpose approach intercluster communication;program processors	Memory accesses limit the performance of stream processors. The stream compiler exploits the reuse of records distributed on different ALU clusters by introducing inter-cluster communications, which decreases the program performance. The paper presents the Stream Transpose (ST) approach to exploit such reuse. The approach, by reorganizing the data, puts data that have been distributed on neighboring ALU clusters on the same ALU cluster, hence exploiting the reuse without any inter-Cluster communications. The experimental results show the approach can exploit the reuse of records distributed among ALU clusters without any inter-cluster communications or any decrease of accessing streams, and gains at most 1.46 speedup over the approach with inter-cluster communication.	arithmetic logic unit;central processing unit;compiler;exploit (computer security);speedup;stream processing	Gen Li;Caixia Sun;Hongwei Zhou;Fayuan Wang	2014	2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)	10.1109/HPCC.2014.159	parallel computing;kernel;real-time computing;computer science;operating system;distributed computing;cluster analysis	HPC	-5.836934683097971	50.50597581844147	194886
015b89151d03dc0aa64452e978496c389af2687a	quantifying and optimizing data access parallelism on manycores		Data access parallelism (DAP) indicates how well available hardware resources are utilized by data accesses. This paper investigates four complementary components of data access parallelism in detail: cache-level parallelism (CLP), bank-level parallelism (BLP), network-level parallelism (NLP), and memory controller-level parallelism (MLP). Specifically, we first quantify these four components for a set of 20 multi-threaded benchmark programs, and show that, when executed on a state-of-the-art manycore platform, their original values are quite low compared to the maximum possible values they could take. We next perform a limit study, which indicates that significant performance improvements are possible if the values of these four components of DAP could be maximized. Building upon our observations from this limit study, we then present two practical computation and network access scheduling schemes. Both these schemes make use of profile data, but, while the compiler-based strategy uses fixed priorities of CLP, BLP, NLP, and MLP, the machine learning-based one employs a predictive machine learning model. Our experiments indicate 30.8% and 36.9% performance improvements with the compiler-based and learning-based schemes, respectively. Our results also show that the proposed schemes consistently achieve significant improvements under different values of the major experimental parameters.	access network;benchmark (computing);computation;data access;experiment;fubini–study metric;icl distributed array processor;machine learning;manycore processor;memory controller;memory-level parallelism;multi-core processor;natural language processing;optimizing compiler;parallel computing;scheduling (computing);thread (computing)	Jihyun Ryoo;Orhan Kislal;Xulong Tang;Mahmut T. Kandemir	2018	2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)	10.1109/MASCOTS.2018.00022	parallel computing;data parallelism;compiler;real-time computing;scheduling (computing);computation;computer science;network on a chip;data access;access network	Arch	-9.653616283518943	51.12250006769473	195198
eb293296eb4bb9c8581e1f58dcddacf38c445117	stable memory for a disk write cache	tolerancia falta;gestion memoire;haute performance;fault tolerant;limiting factor;multiprocessor;storage management;raid;stable memory;controller;cache memory;checkpointing;log structured filesystem;subsystem;antememoria;gestion memoria;antememoire;fault tolerant system;supervisor;sous systeme;fault tolerance;controleur;sistema tolerando faltas;alto rendimiento;vram;systeme tolerant les pannes;multiprocesador;high performance;subsistema;tolerance faute;multiprocesseur	Lack of I/O performance is fast becoming a limiting factor in many computing systems. The yearly doubling of CPU speeds is not being matched by corresponding gains in I/O performance. This paper explores one aspect of the architecture of a high performance fault-tolerant cached RAID subsystem for a multiprocessor. The disk write cache is implemented as a memory-mapped stable memory. The features of a VRAM-based stable memory and its associated RAID controller are discussed	cpu cache	Brian A. Coghlan;Jeremy A Jones	1995	Microprocessing and Microprogramming	10.1016/0165-6074(95)90627-O	embedded system;fault tolerance;parallel computing;real-time computing;cache coloring;page cache;it8212;computer science;disk array controller;operating system;non-standard raid levels;write buffer;cache pollution;standard raid levels;raid processing unit	Arch	-11.67458141926944	49.200417247540955	195372
450fc2a4e1ed2c050efecf63377ddbfb1f351a78	an empirical study of decentralized ilp execution models	processing element;instruction level parallel;decentralization;empirical study;execution unit dependence;data dependence;speculative execution;instruction level parallelism;control dependence;hardware window;dynamic scheduling	Recent fascination for dynamic scheduling as a means for exploiting instruction-level parallelism has introduced significant interest in the scalability aspects of dynamic scheduling hardware. In order to overcome the scalability problems of centralized hardware schedulers, many decentralized execution models are being proposed and investigated recently. The crux of all these models is to split the instruction window across multiple processing elements (PEs) that do independent, scheduling of instructions. The decentralized execution models proposed so far can be grouped under 3 categories, based on the criterion used for assigning an instruction to a particular PE. They are: (i) execution unit dependence based decentralization (EDD), (ii) control dependence based decentralization (CDD), and (iii) data dependence based decentralization (DDD). This paper investigates the performance aspects of these three decentralization approaches. Using a suite of important benchmarks and realistic system parameters, we examine performance differences resulting from the type of partitioning as well as from specific implementation issues such as the type of PE interconnect.We found that with a ring-type PE interconnect, the DDD approach performs the best when the number of PEs is moderate, and that the CDD approach performs best when the number of PEs is large. The currently used approach---EDD---does not perform well for any configuration. With a realistic crossbar, performance does not increase with the number of PEs for any of the partitioning approaches. The results give insight into the best way to use the transistor budget available for implementing the instruction window.	cdd;centralized computing;crossbar switch;data dependency;execution unit;fascination;instruction window;instruction-level parallelism;microprocessor;parallel computing;scalability;scheduling (computing);transistor	Narayan Ranganathan;Manoj Franklin	1998		10.1145/291069.291061	parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;programming language;empirical research;decentralization;instruction-level parallelism;speculative execution	Arch	-7.891633860892503	51.50848944439425	196130
5cdba218d49d0e7977c370e4e667fdb0925acc85	optimizing general-purpose cpus for energy-efficient mobile web computing	software managed cache;specialization;customization;accelerator;web browsing	Mobile applications are increasingly being built using web technologies as a common substrate to achieve portability and to improve developer productivity. Unfortunately, web applications often incur large performance overhead, directly affecting the user quality-of-service (QoS) experience. Traditional techniques in improving mobile processor performance have mostly been adopting desktop-like design techniques such as increasing single-core microarchitecture complexity and aggressively integrating more cores. However, such a desktop-oriented strategy is likely coming to an end due to the stringent energy and thermal constraints that mobile devices impose. Therefore, we must pivot away from traditional mobile processor design techniques in order to provide sustainable performance improvement while maintaining energy efficiency.  In this article, we propose to combine hardware customization and specialization techniques to improve the performance and energy efficiency of mobile web applications. We first perform design-space exploration (DSE) and identify opportunities in customizing existing general-purpose mobile processors, that is, tuning microarchitecture parameters. The thorough DSE also lets us discover sources of energy inefficiency in customized general-purpose architectures. To mitigate these inefficiencies, we propose, synthesize, and evaluate two new domain-specific specializations, called the Style Resolution Unit and the Browser Engine Cache. Our optimizations boost performance and energy efficiency at the same time while maintaining general-purpose programmability. As emerging mobile workloads increasingly rely more on web technologies, the type of optimizations we propose will become important in the future and are likely to have a long-lasting and widespread impact.	acm transactions on computer systems;algorithm;application domain;baseline (configuration management);c++;cascading style sheets;central processing unit;chromium (web browser);computation;computational science;computer data storage;dark silicon;data structure;datapath;dennard scaling;desktop computer;domain-specific language;emergence;fork (software development);gecko;general-purpose markup language;general-purpose modeling;google chrome;google native client;html;html5;hardware description language;image scaling;intel core (microarchitecture);javascript;library (computing);load balancing (computing);microarchitecture;mobile app;mobile device;mobile processor;multi-core processor;next-generation network;optimizing compiler;overhead (computing);partial template specialization;physics engine;processor design;quality of service;real-time transcription;retrospect (software);scheduling (computing);search/retrieve via url;signal processing;single-core;software ecosystem;software system;tegra;unreal development kit;video processing;wurfl;web application;web browser engine;webkit;webplatform;webrtc;write once, run anywhere	Yuhao Zhu;Vijay Janapa Reddi	2017	ACM Trans. Comput. Syst.	10.1145/3041024	web application;mobile processor;real-time computing;computer science;distributed computing;cache;software portability;web navigation;mobile web;mobile computing;microarchitecture	Arch	-7.553988993933426	47.04853432183144	196236
081c32609be4adcf16fe6f3bd6ae35ce2622edaf	performance analysis and evaluation of pcie 2.0 and quad-data rate infiniband	pcie 2 0;high performance systems;bit rate 40 gbit s performance evaluation pcie 2 0 quad data rate infiniband high performance systems commodity multicore systems pci express fabric mellanox infiniband host channel adapter double data rate interface nas parallel benchmark is performance;bandwidth benchmark testing throughput computer architecture performance evaluation switches physical layer;quad data rate infiniband;shared memory;double data rate interface;double data rate;performance evaluation;peripheral interfaces;pci express fabric;perforation;physical layer;nas parallel benchmark is performance;host channel adapter;bit rate 40 gbit s;qdr;infiniband;computer architecture;pci;high performance computer;performance analysis;message passing;bandwidth;pci qdr infiniband;switches;high performance;mellanox infiniband;benchmark testing;commodity multicore systems;throughput	High-performance systems are undergoing a major shift as commodity multi-core systems become increasingly prevalent. As the number of processes per compute node increase, the other parts of the system must also scale appropriately to maintain a balanced system. In the area of high-performance computing, one very important element of the overall system is the network interconnect that connects compute nodes in the system. InfiniBand is a popular interconnect for high- performance clusters. Unfortunately, due to limited bandwidth of the PCI-Express fabric, InfiniBand performance has remained limited. PCI-Express (PCIe) 2.0 has become available and has doubled the transfer rates available. This additional I/O bandwidth balances the system and makes higher data rates for external interconnects such as InfiniBand feasible. As a result, InfiniBand quad-data rate (QDR) mode has become available on the Mellanox InfiniBand host channel adapter (HCA) with a 40 Gb/sec signaling rate. In this paper we perform an in-depth performance analysis of PCIe 2.0 and the effect of increased InfiniBand signaling rates. We show that even using the double data rate (DDR) interface, PCIe 2.0 enables a 25% improvement in NAS parallel benchmark IS performance. Furthermore, we show that when using QDR on PCIe 2.0, network loopback can outperform a shared memory message passing implementation. We show that increased interconnection bandwidth significantly improves the overall system balance by lowering latency and increasing bandwidth.	benchmark (computing);byte;data rate units;double data rate;etsi satellite digital radio;emoticon;infiniband;input/output;interconnection;loopback;message passing;multi-core processor;norm (social);one-way function;pci express;profiling (computer programming);quad data rate sram;shared memory;speaker wire;supercomputer;uncompressed video	Matthew J. Koop;Wei Huang;Karthik Gopalakrishnan;Dhabaleswar K. Panda	2008	2008 16th IEEE Symposium on High Performance Interconnects	10.1109/HOTI.2008.26	shared memory;embedded system;benchmark;throughput;parallel computing;message passing;real-time computing;network switch;computer science;operating system;conventional pci;double data rate;bandwidth;physical layer;computer network	HPC	-10.35918164078827	47.01202083919279	196596
8ec9c356ae978d39ff313189a585a7c2a575898c	model-driven adaptation of double-precision matrix multiplication to the cell processor architecture	multicore architectures;cell processor;performance models;adaptation;dual precision matrix multiplication;parallel algorithms	The main delivery of this paper is a model-driven approach to adaptation of the double-precision matrix multiplication to architectures of blade systems based on two types of Cell processors. A hierarchical algorithm used for adaptation consists of four levels. The first level provides sharing computation among all the 16 SPE cores of the IBM BladeCenter QS21 or QS22. The second level corresponds to a macro-kernel, and is responsible for the data management in the main memory, as well as communication between the main memory and local stores of SPE cores. Each macro-kernel operation is implemented within the local store of an SPE core. The third level corresponds to a kernel of the algorithm; each kernel operation is implemented on a single SPE within its local store as a sequence of micro-kernel operations. The fourth level is a micro-kernel implemented within the register file of an SPE core. The proposed approach is based on two performance models. The purpose of the first model is optimization of communication across all 16 SPE cores of the IBM BladeCenter, including the main memory and local stores of SPEs. It is constructed as a function of size of matrix blocks. This model allows for selecting ''the best'' size of the macro-kernel. The second performance model is aiming at optimization of computations within a single SPE core, taking into account constraints on traffic between the local store and register file of SPE. The model accounts for such factors as size of local store, number of registers, properties of double-precision operations, balance between pipelines, etc. This model allows for selecting ''the best'' size of kernel and micro-kernel operations. The model-driven adaptation is followed by a series of systematic optimization steps. They include loop unrolling, double buffering on register and memory levels, as well as using NUMA library. The proposed adaptation and optimization steps are fully implemented in C language, without optimizing code manually. For the IBM QS21 system, which uses two Cell processors of the first generation, this implementation allows for achieving 27.24Gflop/s, which is 93.1% of the peak performance. This result is obtained for matrices of size 4096 by 4096. For the IBM QS22 system, based on PowerXCell 8i processors, the performance of double-precision arithmetic is extremely higher, so 184.4Gflop/s is achieved, as 90.0% of the peak performance. This result is reported for the matrix multiplication of size 15,872 by 15,872. The overall performance could be slightly improved by substituting the macro-kernel developed in this work with the highly optimized Cell BLAS dgemm_64x64 kernel.	cell (microprocessor);double-precision floating-point format;matrix multiplication;microarchitecture;model-driven integration	Roman Wyrzykowski;Krzysztof Rojek;Lukasz Szustak	2012	Parallel Computing	10.1016/j.parco.2011.08.006	parallel computing;real-time computing;computer hardware;computer science;operating system;parallel algorithm;adaptation	HPC	-5.69359398932585	48.83030251631776	196979
226a7b7162b998dd5f8f3563a228510c686700d8	a task-centric memory model for scalable accelerator architectures	data sharing;paper;1024 core mimd accelerator;1000 core compute accelerator;software management;accelerator architectures hardware collaborative software software maintenance coherence memory management prefetching computer applications application software protocols;1000 core processors;hardware architecture;chip;computer architecture;computational modeling;software protocol;scalable accelerator architecture;compute accelerator cache coherence memory model computer architecture;memory architecture;hardware cache;synchronization;shared caches;compute accelerator;parallel processing dram chips memory architecture multiprocessing systems;dynamic random access memory task centric memory model scalable accelerator architecture 1000 core compute accelerator visual computing application 1000 core processors data sharing memory system software protocol hardware cache 1024 core mimd accelerator runtime system software management hardware prefetching dram shared caches;hardware prefetching;cache coherence;memory systems;task centric memory model;coherence;algorithms;dynamic random access memory;runtime system;multiprocessing systems;computer science;memory system;dram;programming;use case;visual computing application;program processors;communication pattern;parallel processing;dram chips;management policy;hardware;memory model	This article presents a memory model for parallel compute accelerators with task-based programming models that uses a software protocol, working in collaboration with hardware caches, to maintain a coherent, single address space view of memory without requiring hardware cache coherence. The memory model supports visual computing applications, which are becoming an important class of workloads capable of exploiting 1,000-core processors.	address space;cpu cache;cache (computing);cache coherence;central processing unit;coherence (physics);memory model (programming);visual computing	John H. Kelm;Daniel R. Johnson;Steven S. Lumetta;Matthew I. Frank;Sanjay J. Patel	2009	2009 18th International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2009.16	chip;use case;uniform memory access;distributed shared memory;shared memory;memory model;parallel processing;synchronization;programming;cache coherence;interleaved memory;computer architecture;parallel computing;real-time computing;dynamic random-access memory;distributed memory;coherence;computer science;operating system;hardware architecture;overlay;memory coherence;extended memory;flat memory model;computational model;dram;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management;memory ordering	HPC	-8.452299094809915	47.8083635017975	196998
545922e410bf760f6adc7225fb0d416e09287837	energy-efficient query processing on embedded cpu-gpu architectures	memory mapped io;multi socket cpus;numa	Energy efficiency is a major design and optimization factor for query co-processing of databases in embedded devices. Recently, GPUs of new-generation embedded devices have evolved with the programmability and computational capability for general-purpose applications. Such CPU-GPU architectures offer us opportunities to revisit GPU query co-processing in embedded environments for energy efficiency. In this paper, we experimentally evaluate and analyze the performance and energy consumption of a GPU query co-processor on such hybrid embedded architectures. Specifically, we study four major database operators as micro-benchmarks and evaluate TPC-H queries on CARMA, which has a quad-core ARM Cortex-A9 CPU and a NVIDIA Quadro 1000M GPU. We observe that the CPU delivers both better performance and lower energy consumption than the GPU for simple operators such as selection and aggregation. However, the GPU outperforms the CPU for sort and hash join in terms of both performance and energy consumption. We further show that CPU-GPU query co-processing can be an effective means of energy-efficient query co-processing in embedded systems with proper tuning and optimizations.	arm cortex-a9;arm architecture;central processing unit;coprocessor;database;embedded system;experiment;general-purpose markup language;graphics processing unit;hash join;ibm tivoli storage productivity center;mathematical optimization;multi-core processor;nvidia quadro	Xuntao Cheng;Beixin Julie He;Chiew Tong Lau	2015		10.1145/2771937.2771939	parallel computing;real-time computing;computer science;memory-mapped i/o;operating system;database;non-uniform memory access	DB	-5.626219043097814	48.1206708634003	197759
47a3a30a1a1401d1e82b5a6c8022cf67c5911cd0	fault-tolerance of distributed genetic algorithms on many-core processors	fault tolerance;many core processors;genetic algorithms	This paper reports on fault-tolerant technology for use with high-speed parallel evolutionary computation on many-core processors. In particular, for distributed GA models which communicate between islands, we propose a method where an island's ID number is added to the header of data transferred by this island for use in fault detection, and we evaluate this method using Deceptive functions and Sudoku puzzles. As a result, we show that it is possible to detect single stuck-at faults with practically negligible overheads in applications where the time spent performing genetic operations is large compared with the data transfer speed between islands. We also show that it is still possible to obtain an optimal solution when a single stuck-at fault is assumed to have occurred, and that increasing the number of parallel threads has the effect of making the system less susceptible to faults and more sustainable.	central processing unit;evolutionary computation;fault detection and isolation;fault tolerance;genetic algorithm;identification (information);list of device bit rates;manycore processor;software release life cycle;stuck-at fault;sudoku	Yuji Sato;Mikiko Sato	2013		10.1145/2464576.2480778	parallel computing;real-time computing;computer science;distributed computing	HPC	-4.737688663331465	48.16504793454614	197802
a5aa3530d97a4c6ce9e5ef73bd37e5fffc76e643	h-parafac: hierarchical parallel factor analysis of multidimensional big data	gpgpu multi dimensional data processing factorization big data parallel factor analysis;tensile stress;technological innovation;acceleration;computational modeling;tensile stress computational modeling graphics processing units parallel processing mathematical model technological innovation acceleration;graphics processing units;mathematical model;parallel processing	"""It has long been an important issue in various disciplines to examine massive multidimensional data superimposed by a high level of noises and interferences by extracting the embedded multi-way factors. With the quick increases of data scales and dimensions in the big data era, research challenges arise in order to (1) reflect the dynamics of large tensors while introducing no significant distortions in the factorization procedure and (2) handle influences of the noises in sophisticated applications. A hierarchical parallel processing framework over a GPU cluster, namely H-PARAFAC, has been developed to enable scalable factorization of large tensors upon a “divide-and-conquer” theory for Parallel Factor Analysis (PARAFAC). The H-PARAFAC framework incorporates a coarse-grained model for coordinating the processing of sub-tensors and a fine-grained parallel model for computing each sub-tensor and fusing sub-factors. Experimental results indicate that (1) the proposed method breaks the limitation on the scale of multidimensional data to be factorized and dramatically outperforms the traditional counterparts in terms of both scalability and efficiency, e.g., the runtime increases in the order of <inline-formula> <tex-math notation=""""LaTeX"""">$n^2$</tex-math><alternatives><inline-graphic xlink:href=""""wang-ieq1-2613054.gif""""/> </alternatives></inline-formula> when the data volume increases in the order of <inline-formula> <tex-math notation=""""LaTeX"""">$n^3$</tex-math><alternatives><inline-graphic xlink:href=""""wang-ieq2-2613054.gif""""/> </alternatives></inline-formula>, (2) H-PARAFAC has potentials in refraining the influences of significant noises, and (3) H-PARAFAC is far superior to the conventional window-based counterparts in preserving the features of multiple modes of large tensors."""	big data;distortion;embedded system;factor analysis;gpu cluster;high-level programming language;parallel computing;scalability;xlink	Dan Chen;Yangyang Hu;Lizhe Wang;Albert Y. Zomaya;Xiaoli Li	2017	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2016.2613054	acceleration;parallel processing;parallel computing;simulation;computer science;data science;theoretical computer science;operating system;mathematical model;database;distributed computing;stress;computational model;algorithm;statistics	Visualization	-4.903014238584184	46.71352181069915	198573
0344b1b6ae33e11f9bd86db0d1edd5348fd00d6c	loose loops sink chips	multi threading pipeline processing microprocessor chips;pipelines delay registers hazards feedback loop computer science performance loss frequency computational modeling process design;multi threading;chip;architectural simulator micro architectural loops processor pipelines loose loops load resolution loop distributed register algorithm load mis speculations issue to execute latency;pipeline processing;microprocessor chips	This paper explores the concept of micro-architectural loops and discusses their impact on processor pipelines. In particular, we establish the relationship between loose loopsand pipeline length and configuration, and show their impact on performance. We then evaluate the load resolution loop in detail and propose thedistributed register algorithm (DRA) as a way of reducing this loop. It decreases the performance loss due to load mis-speculations by reducing the issue-to-execute latency in the pipeline. A new loose loop is introduced into the pipeline by the DRA, but the frequency of mis-speculations is very low. The reduction in latency from issue to execute, along with a low mis-speculation rate in the DRA result in up to a 4% to 15% improvement in performance using a detailed architectural	algorithm;benchmark (computing);computer architecture simulator;cyclic redundancy check;dynamic resolution adaptation;hazard (computer architecture);langton's loops;operand;pipeline (computing);register file	Eric Borch;Eric Tune;Srilatha Manne;Joel S. Emer	2002		10.1109/HPCA.2002.995719	chip;embedded system;parallel computing;real-time computing;multithreading;telecommunications;computer science;operating system	Arch	-6.351028587267568	52.88060107971423	198744
d9d96b2e8f4a38adea741cb20e4f86980bf3c221	vismi: software distributed shared memory for infiniband clusters	protocols;software distributed shared memory;distributed shared memory systems;cluster system;protocols computer architecture hardware parallel programming space technology performance gain software performance parallel architectures lan interconnection local area networks;home based lazy release consistency;workstation clusters;workstation clusters distributed shared memory systems protocols;synchronization overhead reduction vismi software distributed shared memory infiniband clusters cluster systems home based lazy release consistency protocol multiple writer coherence scheme false sharing performance gain optimized page invalidation mechanisms	This work describes ViSMI, a software distributed shared memory system for cluster systems connected via InfiniBand. ViSMI implements a kind of home-based lazy release consistency protocol, which uses a multiple-writer coherence scheme to alleviate the traffic introduced by false sharing. For further performance gain, InfiniBand features and optimized page invalidation mechanisms are applied in order to reduce synchronization overhead. First experimental results show that ViSMI introduces good performance comparable to similar software DSMs.	64-bit computing;address space;clustered file system;computer cluster;distributed shared memory;false sharing;infiniband;lazy evaluation;locality of reference;openmp;overhead (computing);release consistency	Christian Osendorfer;Jie Tao;Carsten Trinitis;Martin Mairandres	2004	Third IEEE International Symposium on Network Computing and Applications, 2004. (NCA 2004). Proceedings.	10.1109/NCA.2004.1347776	distributed shared memory;shared memory;communications protocol;cache coherence;parallel computing;real-time computing;distributed memory;computer science;operating system;distributed computing;computer network;supercomputer architecture	HPC	-11.369260865940626	46.606937112690225	199293
