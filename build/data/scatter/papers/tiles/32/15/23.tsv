id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e79aa0338ed6130b84b20866e72d3d74644d2e9e	energy-aware scheduling for sensor node platforms	energy aware scheduling;sensor node;dpm energy aware scheduling sensor node dvs;energy consumption energy aware real time scheduling battery powered sensor node platform energy saving dynamic voltage scaling based scheduling approach dvs based scheduling approach;wireless sensor networks energy conservation telecommunication power management telecommunication scheduling;dpm;real time systems processor scheduling time frequency analysis time factors timing dynamic scheduling;dvs	This paper primarily presents a technique to integrate energy-aware real-time scheduling into battery-powered sensor node platforms. The proposed scheduling technique attempts to achieve energy savings while meeting timing requirements of real-time tasks as well as improving the response time of non-real-time tasks. The proposed energy-aware real-time scheduling technique is addressed from two perspectives: a dynamic voltage scaling (DVS)-based scheduling approach and a combined real-time and non-real-time scheduling approach. From the perspective of DVS-based scheduling approach, it reduces the processor frequency and still ensures that no tasks miss their timing constraints. From the perspective of a combined real-time and non-real-time scheduling scheme, tasks are allowed to have timing constraints. Consequently, real-time tasks need to be processed in a timely fashion and delivered to meet their timing constraints, and non-real-time tasks need to be quickly processed to deliver a fast response. An experimental evaluation shows that the proposed scheduling technique yields efficient performance in terms of low energy consumption and a fast average response time for non-real-time tasks, whilst meeting the timing constraints of real-time tasks.	dynamic voltage scaling;image scaling;real-time clock;real-time transcription;requirement;response time (technology);scheduling (computing);sensor node;while	Sungwoo Tak;Hangeul Kim;Donglyul Kim;Yougyung Kim	2014	2014 15th International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2014.18	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;earliest deadline first scheduling;sensor node;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;two-level scheduling;deadline-monotonic scheduling;stride scheduling;scheduling;gain scheduling;least slack time scheduling;round-robin scheduling;proportionally fair	Embedded	-5.392010547420858	59.23970122611689	136618
56e2223ef6feed712a0765dd451f363fe7554a95	wele-raid: a ssd-based raid for system endurance and performance	reliability;raid;performance;wear leveling;ssd;endurance	Due to the limited erasure/program cycles of flash memory, flashbased SSDs need to prolong their life time using wear-leveling mechanism to meet their advertised capacity all the time. However, there is no wear-leveling mechanism among SSDs in RAID system, which makes some SSDs wear out faster than others. Once any one of SSDs fails, reconstruction must be triggered immediately. But, the cost of this process is so high that the reliability and availability is affected seriously. We propose WeLe-RAID which introduces Wear-Leveling mechanism among flash SSDs to enhance the endurance of entire SSD-based RAID system. As we know that under the workload of random access pattern, parity stripes suffer from much more updates because every update to the data stripe would cause the modification to the related parity stripe. Based on this principle, we introduce age-driven parity distribution scheme to guarantee the wear-leveling among flash SSDs. At the same time, because of age-driven parity distribution, it brings into the performance benefit with better load balance. Compared with conventional RAID mechanism, it significantly improves the life span and performance with ignorable overhead.	algorithm;bsd;data structure;digital history;flash memory;load balancing (computing);overhead (computing);parity bit;raid;random access;response time (technology);solid-state drive;standard raid levels;stripes;wear leveling	Yimo Du;Fang Liu;Zhiguang Chen;Xin Ma	2011		10.1007/978-3-642-24403-2_20	embedded system;real-time computing;computer hardware;performance;computer science;operating system;reliability;raid	OS	-11.44379586477897	55.15401620120239	136627
990c868bb86677d483e510c09c1c835c6737cad4	maximizing io performance via conflict reduction for flash memory storage systems	data transfer;programming;decoding;public key cryptography;architecture;sensors	Flash memory has been widely deployed during the recent years with the improvement of bit density and technology scaling. However, a significant performance degradation is also introduced with the development trend. The latency of IO requests on flash memory storage systems is composed of access conflict latency, data transfer latency, flash chip access latency and ECC encoding/decoding latency. Studies show that the access conflict latency, which is mainly induced by the slow transfer latency and access latency, has become the dominate part of the IO latency, especially for IO intensive applications. This paper proposes to reduce the flash access conflict latency through the reduction of the transfer and flash access latencies. A latency model is built to construct the relationship among the transfer latency and access latency based on the reliability characteristics of flash memory. Simulation experiments show that the proposed approach achieves significant performance improvement.	areal density (computer storage);elegant degradation;experiment;flash memory;image scaling;simulation;socket.io	Qiao Li;Liang Shi;Congming Gao;Kaijie Wu;Chun Jason Xue;Qingfeng Zhuge;Edwin Hsing-Mean Sha	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		latency;parallel computing;real-time computing;computer hardware;computer science;architecture	EDA	-9.510863432968387	54.43632834418953	136791
52326a8d13cdcaed942cfd2df92d1b3747067cdb	energy-aware dynamic task scheduling applied to a real-time multimedia application on an xscale board	real time;multimedia application;task scheduling		real-time transcription;scheduling (computing);xscale	Chantal Ykman-Couvreur;Francky Catthoor;Johan Vounckx;Andy Folens;Filip Louagie	2005	J. Low Power Electronics	10.1166/jolpe.2005.037	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;real-time computing;dynamic priority scheduling;computer science;operating system;two-level scheduling	Embedded	-6.151165323052332	59.64967124301931	137047
75f9c9d129e1a53453af73651ecd4b4cc250d37a	optimal configuration for multiversion real-time systems using slack based schedulability	optimization;real time systems	In an embedded control system, control performances of each job depend on its latency and a control algorithm implemented in it. In order to adapt a job set to optimize control performances subject to schedulability, we design several types of control software for each job, which will be called versions, and select one version from them when the job is released. A real-time system where each job has several versions is called a multiversion real-time system. A benefit and a CPU utilization of a job depend on the versions. So, it is an important problem to select a version of each job so as to maximize the total benefit of the system subject to a schedulability condition. Such a problem will be called an optimal configuration problem. In this paper, we assume that each version is specified by the relative deadline, the execution time, and the benefit. We show that the optimal configuration problem is transformed to a maximum path length problem. We propose an optimal algorithm based on the forward dynamic programming. Moreover, we propose sub-optimal algorithms to reduce computation times. The efficiencies of the proposed algorithms are illustrated by simulations.	real-time operating system;real-time transcription;scheduling (computing);slack variable	Sayuri Terada;Toshimitsu Ushio	2010	IEICE Transactions		real-time computing;computer science;distributed computing	Embedded	-10.132439151153195	60.32166032238307	137804
e3778e46a9f3370ba5cca9551827891eb1584050	real-time i/o management system with cots peripherals	real time systems servers processor scheduling bridges scheduling hardware delay;system buses buffer storage embedded systems peripheral interfaces scheduling;peripheral interfaces;processor scheduling;real time;buffer storage;bridges;system buses;input output;embedded systems;servers;scheduling;bus;linux;cots real time linux input output peripheral bus scheduling;cots;peripheral;i o traffic cots peripherals real time embedded system commercial off the shelf component cots component mass produced peripheral buses cost reduction time to market cots interconnect system timing degradation high bandwidth i o peripheral real time i o management system real time bridges i o virtualization capabilities peripheral scheduler i o subsystem cots based embedded system real time scheduling timing unpredictability maximum delay buffered i o data transaction buffer size data loss virtual device physical device;hardware;real time systems	Real-time embedded systems are increasingly being built using commercial-off-the-shelf (COTS) components such as mass-produced peripherals and buses to reduce costs, time-to-market, and increase performance. Unfortunately, COTS-interconnect systems do not usually guarantee timeliness, and might experience severe timing degradation in the presence of high-bandwidth I/O peripherals. Moreover, peripherals do not implement any internal priority-based scheduling mechanism, hence, sharing a device can result in data of high priority tasks being delayed by data of low priority tasks. To address these problems, we designed a real-time I/O management system comprised of 1) real-time bridges with I/O virtualization capabilities, and 2) a peripheral scheduler. The proposed framework is used to transparently put the I/O subsystem of a COTS-based embedded system under the discipline of real-time scheduling, minimizing the timing unpredictability due to the peripherals sharing the bus. We also discuss computing the maximum delay due to buffered I/O data transactions as well as determining the buffer size needed to avoid data loss. Finally, we demonstrate experimentally that our prototype real-time I/O management system successfully exports multiple virtual devices for a single physical device and prioritizes I/O traffic, guaranteeing its timeliness.	bus (computing);central processing unit;computer data storage;electronic hardware;elegant degradation;embedded system;experiment;input/output;interference (communication);management system;memory bandwidth;peripheral bus;preprocessor;prototype;real-time clock;real-time transcription;scheduling (computing);uniprocessor system	Emiliano Betti;Stanley Bak;Rodolfo Pellizzoni;Marco Caccamo;Lui Sha	2013	IEEE Transactions on Computers	10.1109/TC.2011.202	input/output;bus;embedded system;parallel computing;real-time computing;computer science;operating system;scheduling;linux kernel;peripheral;server;computer network	Embedded	-9.305058681918426	57.50918697804308	137938
e4f76e34e5dec3c919ec7411c673aa3afb0a01a3	an architectural support for reduction of in-rush current in systems with instruction controlled power gating		The present work introduces a hardware based technique for reduction of in-rush current in processors with power gating (PG) facility. A PG instruction has been introduced which is responsible in turning on multiple components from sleep to active mode at overlapped time intervals. The supporting hardware for the proposed PG instruction allows overlapped wake-up as long as the resultant in-rush current is tolerable by the system. The efficacy of the proposed method is evaluated on MiBench and MediaBench benchmark programs. The proposed method reduces in-rush current by an average of 35% with average performance loss of 5%.		Sumanta Pyne	2018		10.1145/3194554.3194645	real-time computing;computer science;power gating;inrush current	Arch	-5.873389185479626	55.64517502032588	138001
09c596ab28872b9c94e52ffc23299fc6986851c5	the power of isolation	power aware scheduling;memory management;power aware scheduling compositionality real time;real time;real time systems multiprocessing systems power aware computing power consumption;upper bound;power aware computing;computational modeling;energy consumption;compositionality;energy consumption power demand jitter upper bound computational modeling equations memory management;multiprocessing systems;power consumption;jitter;power demand;power consumption overhead cpu memory utilization noncompositional power consumption individual system component isolation isolation quality spatial isolation temporal isolation nonlinear property edf scheduled hard real time systems periodic software tasks power isolation upper bounds lower bounds frequency scaling;real time systems	Non-functional system properties such as CPU and memory utilization as well as power consumption are usually non-compositional. However, such properties can be made compositional by isolating individual system components through over-provisioning. The challenge is to relate the involved isolation cost and the resulting isolation quality properly. Temporal and spatial isolation have been studied extensively. Here we study the compositionality of power consumption as another and in this regard unexplored example of a non-linear property with many important applications. In particular, we introduce the concept of power isolation for EDF-scheduled hard real-time systems running periodic software tasks. A task is power isolated if there exist lower and upper bounds on its power consumption independent of any other concurrently running tasks. The main challenges in providing power isolation are to find a relationship between the power consumption of a system and the contribution of a single task to this power consumption as well as understanding the trade-off between quality and cost of power isolation. We present lower and upper bounds on the power consumption of a task as functions of task utilization, frequency scaling, and power model. Furthermore, we discuss the variance between lower and upper bounds (quality) and the power consumption overhead (cost) of power isolation.	central processing unit;earliest deadline first scheduling;existential quantification;frequency scaling;image scaling;nonlinear system;overhead (computing);provisioning;real-time clock;real-time computing	Silviu S. Craciunas;Christoph M. Kirsch	2012	2012 IEEE 15th International Conference on Computational Science and Engineering	10.1109/ICCSE.2012.59	embedded system;parallel computing;real-time computing;jitter;computer science;operating system;upper and lower bounds;computational model;principle of compositionality;memory management	Embedded	-5.232009937280906	58.432272159905764	138418
ac5dbaa76233e471b4361fafef23ae468088269e	the effects of cpu: i/o overlap on computer system configurations	performance measure;reconfiguration;error recovery;capability machine;system configuration;multiprocessor systems;computer system;input output;domain;protection;fault tolerant system;operating system;synchronization;parametric analysis;error confinement;high speed;monitor;rollback	Recent trends in computer technology and design favor the development of computer systems with low levels of multiprogramming. Performance measures such as throughput can be improved under these circumstances by overlapping computation and input/output requests (CPU: I/O overlap). A computationally efficient model is presented which determines accurate bounds on the expected improvement which CPU: I/O overlap may introduce. This model can represent a wide range of multiprogrammed multiprocessor systems. The model is applied to a parametric analysis over a range of multiprocessor systems. CPU: I/O overlap appears to be promising for high speed computers with low levels of multiprogramming and for multiprocessor systems with a large number of processors.	algorithmic efficiency;central processing unit;computation;computer multitasking;input/output;multiprocessing;throughput	Donald F. Towsley	1978		10.1145/800094.803055	input/output;embedded system;synchronization;monitor;fault tolerance;computer architecture;parallel computing;real-time computing;rollback;domain;computer science;control reconfiguration;operating system;parametric statistics;cpu shielding	OS	-9.984940011212723	57.45650004266614	138479
f9e7ed55ba153b3cfd1e7b32a22ffb16e8de9008	an open-system framework for flash-memory storage system	flash memory;mtd translation layer;mlc flash memory;storage system;compatibility;open system;flexibility	In recent years, MLC flash memory, which stores 2 or more bits per cell, has gradually replaced SLC flash memory due to its lower cost and higher density. However, MLC flash memory also brings new constraints to the management. As a result, management schemes designed for SLC flash memory could not be directly applied to MLC flash memory, or a severe performance degradation would be suffered. This paper proposes a translation layer above flash-memory medium to make it transparent to management schemes. With our translation layer, management schemes designed for SLC flash memory or MLC flash memory could be directly applied without considering the type of underlying flash memory medium. Our experiment results show that only a minor overhead would be suffered while applying our translation layer in flash-memory storage systems.	computer data storage;elegant degradation;flash memory;mp3;mtd-f;mobile phone;monoidal t-norm logic;overhead (computing);wear leveling	Jen-Wei Hsieh;Shang-Yang Chang	2011		10.1145/1982185.1982314	flash file system;auxiliary memory;embedded system;interleaved memory;semiconductor memory;parallel computing;computer hardware;computer science;computer memory;universal memory;extended memory;open system;compatibility;registered memory;memory management	OS	-11.010594063396598	54.71609959675045	138499
0456fc19f159bca46a135bd2ef41eab59b92735a	a combined sensor placement and convex optimization approach for thermal management in 3d-mpsoc with liquid cooling	3d;placement;liquid;thermal;liquid cooling;management;thermal management;mpsoc;cooling	Modern high-performance processors employ thermal management systems, which rely on accurate readings of on-die thermal sensors. Systematic tools for analysis and determination of best allocation and placement of thermal sensors is therefore a highly relevant problem. Moreover liquid cooling has emerged as a promising solution for addressing the elevated temperatures in 3D Multi-Processor Systems-on-Chips (MPSoCs). In this work, we present a combined sensor placement and convex optimization approach for thermal management in 3D-MPSoC with liquid cooling. This approach first finds the best locations inside the 3D-MPSoC where thermal sensors can be placed using a greedy approach. Then, the temperature sensing information is subsequently used by our convex-based thermal management policy to optimize the performance of the MPSoC while guaranteeing a reliable working condition. We perform experiments on a 3D multicore architecture case-study using benchmarks ranging from web-accessing to playing multimedia. Our results show a reduction up to 10 in the number of required sensors. Moreover our policy satisfies performance requirements, while reducing cooling energy by up to 72% compared with traditional state of the art liquid cooling techniques. The proposed policy also keeps the thermal profile up to 18 1C lower compared with state of the art 3D thermal management techniques using variable-flow liquid cooling. & 2011 Elsevier B.V. All rights reserved.	benchmark (computing);best practice;central processing unit;computer cooling;convex optimization;dynamic voltage scaling;experiment;greedy algorithm;mpsoc;mathematical optimization;multi-core processor;multitier architecture;requirement;sensor;state space;system on a chip;thermal management (electronics);thermal management of high-power leds;thermal profiling;water cooling	Francesco Zanini;David Atienza;Giovanni De Micheli	2013	Integration	10.1016/j.vlsi.2011.12.003	thermal;electronic engineering;real-time computing;thermal management of electronic devices and systems;simulation;computer science;computer cooling;3d computer graphics;placement	EDA	-4.826824510336811	57.32225967922148	138999
4769530f5087d70b4763b453905056c98c68ae38	osek-like kernel support for engine control applications under edf scheduling	shafts application program interfaces automobile industry engines operating system kernels;kernel engines job shop scheduling standards dynamic scheduling real time systems;hardware and architecture;osek standard edf scheduling crankshaft osek compliant real time kernel autosar standard automotive industries fixed priority scheduling dynamic scheduling policy erika enterprise operating system api	Engine control applications typically include computational activities consisting of periodic tasks, activated by timers, and engine-triggered tasks, activated at specific angular positions of the crankshaft. Such tasks are typically managed by a OSEK-compliant real-time kernel using a fixed-priority scheduler, as specified in the AUTOSAR standard adopted by most automotive industries. Recent theoretical results, however, have highlighted significant limitations of fixed-priority scheduling in managing engine-triggered tasks that could be solved by a dynamic scheduling policy. To address this issue, this paper proposes a new kernel implementation within the ERIKA Enterprise operating system, providing EDF scheduling for both periodic and engine-triggered tasks. The proposed kernel has been conceived to have an API similar to the AUTOSAR/OSEK standard one, limiting the effort needed to use the new kernel with an existing legacy application. The proposed kernel implementation is discussed and evaluated in terms of run-time overhead and footprint. In addition, a simulation framework is presented, showing a powerful environment for studying the execution of tasks under the proposed kernel.	autosar;angularjs;application programming interface;erika enterprise;earliest deadline first scheduling;fixed-priority pre-emptive scheduling;kernel (operating system);legacy system;osek;operating system;overhead (computing);real-time transcription;scheduling (computing);simulation;timer	Vincenzo Apuzzo;Alessandro Biondi;Giorgio C. Buttazzo	2016	2016 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2016.7461345	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;real-time computing;dynamic priority scheduling;rate-monotonic scheduling;operating system;two-level scheduling;kernel preemption	Embedded	-8.901438873745684	57.912803354057054	139066
280ff8d2d036708587f926d402eb9c72b839b5dd	value-based scheduling in real-time database systems	database system;real time;transaction values and deadlines;priority and concurrency algorithms;priority mapping;resource and data contention;simulation model	In a real-time database system, an application may assign avalue to a transaction to reflect the return it expects to receive if the transaction commits before its deadline. Most research on real-time database systems has focused on systems where all transactions are assigned the same value, the performance goal being to minimize the number of missed deadlines. When transactions are assigned different values, the goal of the system shifts to maximizing the sum of the values of those transactions that commit by their deadlines. Minimizing the number of missed deadlines becomes a secondary concern. In this article, we address the problem of establishing a priority ordering among transactions characterized by both values and deadlines that results in maximizing the realized value. Of particular interest is the tradeoff established between these values and deadlines in constructing the priority ordering. Using a detailed simulation model, we evaluate the performance of several priority mappings that make this tradeoff in different, but fixed, ways. In addition, a “bucket” priority mechanism that allows the relative importannce of values and deadlines to be controlled is introduced and studied. The notion of associating a penalty with transactions whose deadlines are not met is also briefly considered.	database;real-time clock;real-time operating system;real-time transcription;scheduling (computing);simulation	Jayant R. Haritsa;Michael J. Carey;Miron Livny	1993	The VLDB Journal	10.1007/BF01232184	real-time computing;computer science;simulation modeling;database;distributed computing	Embedded	-11.408524729435953	58.692717519562045	139109
08ef37c98fc96e273ea6759a25b98f636bde0b75	composite abortable locks	scott algorithm;multi threading;threading composite abortable locks abortable queue based lock mechanism test and set based backoff lock mechanism nonblocking abortable ql scott algorithm;memory management;abortable queue based lock mechanism;queueing theory;size structure;nonblocking abortable ql;soft real time;yarn memory management sun laboratories computer science databases application software testing system recovery content management;composite abortable locks;test and set based backoff lock mechanism;threading;queueing theory multi threading	The need to allow threads to abort an attempt to acquire a lock (sometimes called a timeout) is an interesting new requirement driven by state-of-the-art database applications with soft real-time constraints. This paper presents a new composite abortable lock (CAL), a combination of abortable queue-based (QL) and test-and-set based backoff (BL) lock mechanisms, which provides non-blocking aborts while ensuring low space requirements without need for a memory reclamation scheme. The key observation motivating our approach is that the fast lock hand-off achieved by QLs only requires the first few threads to be queued (not all waiting threads), and that the remaining threads can run as in a BL. We developed an algorithm that uses only a short fixed size structure for queueing, allowing most threads to back-off. This reduces worst-case space overhead dramatically, and improves performance by eliminating the need for expensive and complicated memory management mechanisms. Experimental results show that our new CAL algorithm not only saves on space, it actually outperforms Scott's state-of-the-art nonblocking abortable QL under contention, and even more so when there are more threads than processors. Moreover, as the rate of lock aborts increases, the CAL continues to perform well, while Scott's algorithm deteriorates rapidly	bl (logic);backoff;best, worst and average case;blocking (computing);cal;central processing unit;lock (computer science);memory management;non-blocking algorithm;overhead (computing);real-time clock;real-time computing;requirement;test-and-set	Virendra J. Marathe;Mark Moir;Nir Shavit	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639367	parallel computing;real-time computing;multithreading;threading;computer science;operating system;distributed computing;queueing theory;algorithm;computer network;memory management	OS	-11.154027185464857	57.85266014402828	139204
39091a2f3affc6b41b84fdefeffbc407928a4937	real-time hierarchical systems with arbitrary scheduling at global level	real time systems scheduling;embedded systems;real time algorithms;partitioned systems;real time systems	Partitioned architectures isolate software components into independent partitions whose execution will not interfere with other partitions, preserving temporal and spatial isolation. Hierarchical scheduling can effectively be used to schedule these systems. Schedulability analysis of hierarchical real-time systems is based on prior knowledge of the local and the global scheduling algorithms. In a partitioned system with safety and security issues and certification assurance levels, global scheduling is usually generated using a static table. Therefore, each partition must allocate task jobs only in the temporal windows reserved for that partition. Even if the static table can come originally from a periodic server or other scheduling policy, the final plan may be modified due to changes in the system requirements. As a consequence, the CPU assignment to a partition does not have to correspond to any known policy. In this case, it is not possible to use existing scheduling analysis for hierarchical systems. This paper studies a new scheduling problem: a hierarchical system in which global policy is not known but provided as a set of arbitrary time windows. © 2016 Elsevier Inc. All rights reserved. f c e c	algorithm;central processing unit;component-based software engineering;job (computing);microsoft windows;real-time clock;real-time computing;requirement;scheduling (computing);scheduling analysis real-time systems;server (computing);system requirements	Ana Guasque;Patricia Balbastre Betoret;Alfons Crespo	2016	Journal of Systems and Software	10.1016/j.jss.2016.05.040	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;deadline-monotonic scheduling;stride scheduling;distributed computing;gain scheduling;least slack time scheduling;lottery scheduling;round-robin scheduling	Embedded	-10.026895685214209	59.923574289625904	139468
c111caed42f318d59e3d30bfd875bcee8581652b	power capping of cpu-gpu heterogeneous systems through coordinating dvfs and task mapping	resource allocation;power aware computing;data parallel application cpu gpu heterogeneous systems dvfs task mapping coordination computer system power budget power delivery cooling system power management technique system power consumption power capping technique single computing node device frequency frequency scaling load imbalance power violation near optimal settings application execution;graphics processing units;task mapping gpgpu power capping dvfs;graphics processing units power demand mathematical model computational modeling kernel equations performance evaluation;multiprocessing systems;resource allocation graphics processing units multiprocessing systems power aware computing power consumption;power consumption	Future computer systems are built under much stringent power budget due to the limitation of power delivery and cooling systems. To this end, sophisticated power management techniques are required. Power capping is a technique to limit the power consumption of a system to the predetermined level, and has been extensively studied in homogeneous systems. However, few studies about the power capping of CPU-GPU heterogeneous systems have been done yet. In this paper, we propose an efficient power capping technique through coordinating DVFS and task mapping in a single computing node equipped with GPUs. In CPU-GPU heterogeneous systems, settings of the device frequencies have to be considered with task mapping between the CPUs and the GPUs because the frequency scaling can incurs load imbalance between them. To guide the settings of DVFS and task mapping for avoiding power violation and the load imbalance, we develop new empirical models of the performance and the maximum power consumption of a CPU-GPU heterogeneous system. The models enable us to set near-optimal settings of the device frequencies and the task mapping in advance of the application execution. We evaluate the proposed technique with five data-parallel applications on a machine equipped with a single CPU and a single GPU. The experimental result shows that the performance achieved by the proposed power capping technique is comparable to the ideal one.	central processing unit;computer cooling;dynamic voltage scaling;frequency capping;frequency scaling;graphics processing unit;heterogeneous computing;image scaling;maximum power transfer theorem;power glove;power management;run time (program lifecycle phase);uninterruptible power supply	Toshiya Komoda;Shingo Hayashi;Takashi Nakada;Shinobu Miwa;Hiroshi Nakamura	2013	2013 IEEE 31st International Conference on Computer Design (ICCD)	10.1109/ICCD.2013.6657064	embedded system;parallel computing;real-time computing;resource allocation;computer science;operating system	HPC	-4.802282202976675	56.33818345789409	139893
ddc63fc3f3ee053dc6ff68aa0e1d8c2142a707f9	boosting disk performance by compressing on-board disk cache		Over the last two decades, the performance gap between RAM and disk drives has been widened by about 50% per year. This is because the disk access time was improved only about 8% per year due to the mechanical delays. In order to improve the disk performance, each disk is equipped with an on-board disk cache to bridge the performance gap between the high-speed I/O bus and the slow magnetic media. The on-board disk cache is normally made of SRAM. This is because the SRAM is faster and relatively insensitive to disturbances such as electrical noise in contrast to DRAM. However, it is challenging to increase the capacity of a single-chip SRAM. Furthermore, SRAM is much more expensive and power consuming than DRAM. Therefore, the manufacture normally integrates a relatively small-size on-board disk cache to the disk drives. This paper proposes to compress the on-board disk cache to improve the effective cache size, thus improving the disk performance. Because the prefetching of on-board disk cache has a significant impact on the disk performance, this paper only compresses the prefetched data to minimize the complexity and side effect of compression. The compression is processed in background without affecting ongoing requests. Synthetic traces and real traces are employed to evaluate the proposed method. Experimental results demonstrate that the average response time can be reduced up to 21% with the increase of the compression ratio, and the hit ratio achieves up to 3x improvement.	access time;cpu cache;disk buffer;dynamic random-access memory;hard disk drive performance characteristics;hit (internet);input/output;magnetic storage;noise (electronics);on-board data handling;page cache;response time (technology);static random-access memory;tracing (software)	Wen Jiang;Yuhui Deng;Xiaohua Meng;Cheng Hu;Yongtao Zhou	2017	2017 IEEE 19th International Conference on High Performance Computing and Communications; IEEE 15th International Conference on Smart City; IEEE 3rd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2017.13	compression ratio;static random-access memory;boosting (machine learning);real-time computing;access time;response time;cpu cache;disk buffer;computer hardware;computer science;noise (electronics)	DB	-9.817413144650798	54.94516230600834	140587
32743d0c8e1558da41f2b82e065088429599305f	scale & cap: scaling-aware resource management for consolidated multi-threaded applications	energy efficiency;multi threaded;virtual machines;power;multi core	As the number of cores per server node increases, designing multi-threaded applications has become essential to efficiently utilize the available hardware parallelism. Many application domains have started to adopt multi-threaded programming; thus, efficient management of multi-threaded applications has become a significant research problem. Efficient execution of multi-threaded workloads on cloud environments, where applications are often consolidated by means of virtualization, relies on understanding the multi-threaded specific characteristics of the applications. Furthermore, energy cost and power delivery limitations require data center server nodes to work under power caps, which bring additional challenges to runtime management of consolidated multi-threaded applications. This article proposes a dynamic resource allocation technique for consolidated multi-threaded applications for power-constrained environments. Our technique takes into account application characteristics specific to multi-threaded applications, such as power and performance scaling, to make resource distribution decisions at runtime to improve the overall performance, while accurately tracking dynamic power caps. We implement and evaluate our technique on state-of-the-art servers and show that the proposed technique improves the application performance by up to 21% under power caps compared to a default resource manager.	application domain;data center;frequency capping;genetic algorithm;hardware acceleration;heterogeneous computing;image scaling;java caps;linear programming;multi-core processor;openvms;parallel computing;performance per watt;run time (program lifecycle phase);scalability;server (computing);thread (computing);total cost of ownership	Can Hankendi;Ayse Kivilcim Coskun	2017	ACM Trans. Design Autom. Electr. Syst.	10.1145/2994145	multi-core processor;embedded system;parallel computing;real-time computing;simulation;computer science;virtual machine;operating system;power;efficient energy use	Arch	-5.054720697088429	53.838863100272036	140596
67a93836faf0afb33aeae916fa74e1f216ef43ae	hlc: software-based half-level-cell flash memory	reliability;circuit switching;software engineering;network on chip;decoding;embedded systems;memory management	In recent years, flash memory has been widely used in embedded systems, portable devices, and high-performance storage products due to its non-volatility, shock resistance, low power consumption, and high performance natures. To reduce the product cost, multi-level-cell flash memory (MLC) has been proposed; compared with the traditional single-level-cell flash memory (SLC) that only stores one bit of data per cell, each MLC cell can store two or more bits of data. Thus MLC can achieve a larger capacity and reduce the cost per unit. However, MLC also suffers from the degradation in both performance and reliability. In this paper, we try to enhance the reliability and reduce the product cost of flash-memory based storage devices from a totally different perspective. We propose a half-level-cell (HLC) management scheme to manage and reuse the worn-out space in solid-state drives (SSDs); through our management scheme, the system can treat two corrupted pages as a normal page without sacrificing performance and reliability. To the best of our knowledge, this is the first research that reclaims free space by reviving the corrupted pages. The experiment results show that the lifetime of SSD can be extended by 48.54% for the trace of general users applications with our proposed HLC management scheme.	assembly language;dummy variable (statistics);elegant degradation;embedded system;encoder;flash memory;linux;microsoft windows;mobile device;multi-level cell;non-volatile memory;physical address;solid-state drive;volatility	Han-Yi Lin;Jen-Wei Hsieh	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		flash file system;embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;flash memory emulator;computer data storage;computer memory;universal memory;network on a chip;circuit switching;computer network;memory management	EDA	-11.311792634930448	55.02759805660566	140817
4dbcd25af6aae1a1941d4ee081266cd23e46cc3a	task-set generator for schedulability analysis using the taclebench benchmark suite		Currently, real-time embedded systems evolve towards complex systems using new state of the art technologies such as multi-core processors and virtualization techniques. Both technologies require new real-time scheduling algorithms. For uniprocessor scheduling, utilization-based evaluation methodologies are well-established. For multi-core systems and virtualization, evaluating and comparing scheduling techniques using the tasks' parameters is more realistic. Evaluating such scheduling techniques requires relevant and standardised task sets. Scheduling algorithms can be evaluated at three levels: 1) using a mathematical model of the algorithm, 2) simulating the algorithm and 3) implementing the algorithm on the target platform. Generating task sets is straightforward in the case of the first two levels; only the parameters of the tasks are required. Evaluating and comparing scheduling algorithms on the target platform itself, however, requires executable tasks matching the predefined standardised task sets. Generating those executable tasks is not standardized yet.  Therefore, we developed a task-set generator that generates reproducible, standardised task sets that are suitable at all levels. Besides generating the tasks' parameters, it includes a method that generates executables by combining publicly available benchmarks with known execution times. This paper presents and evaluates this task-set generator.	algorithm;benchmark (computing);central processing unit;complex systems;embedded system;executable;mathematical model;multi-core processor;real-time clock;real-time operating system;scheduling (computing);scheduling analysis real-time systems;simulation;uniprocessor system	Yorick De Bock;Sebastian Altmeyer;Jan Broeckhove;Peter Hellinckx	2016	SIGBED Review	10.1145/3199610.3199613	virtualization;real-time computing;computer science;scheduling (computing);executable;uniprocessor system;suite	Embedded	-8.333115087516207	60.26984339976223	141081
161db55b28b0f0016ad9d4035ac30cfed59f69d8	improved thermal tracking for processors using hard and soft sensor allocation techniques	thermal management packaging;thermal management packaging infrared imaging microprocessor chips multiprocessing systems;sensors;infrared imaging thermal power characterization sensors tracking hot spot;cooling system;computer model;temperature sensors;sensor computation;resource manager;resource management;improved thermal tracking;semiconductor chips;thermal characterization data improved thermal tracking hard sensor allocation soft sensor allocation high end processors semiconductor chips embedded thermal sensors dynamic thermal management systems hot spots cooling system many core processors thermal imaging sensor computation design constraints hot spot tracking soft sensing technique infrared imaging dual core processor;temperature sensor;hot spot;hot spots;design constraints;high end processors;thermal characterization data;computational modeling;thermal imaging;infrared imaging;hard sensor allocation;many core processors;soft sensor allocation;embedded thermal sensors;soft sensing technique;characterization;dynamic thermal management systems;thermal;multiprocessing systems;temperature measurement;temperature measurement program processors temperature sensors resource management computational modeling cameras;program processors;power;cameras;tracking;microprocessor chips;dual core processor;hot spot tracking	Hot spots are a major concern in high-end processors as they constrain performance and limit the lifetime of semiconductor chips. Using embedded thermal sensors, dynamic thermal management systems track the hot spots during runtime and adjust the performance and the cooling system of the processor when necessary. In many-core processors, the locations of hot spots vary spatially and temporally depending on the configuration of active cores and the workloads running on the cores. Our work includes both theoretical advances in sensor allocation techniques and experimental advances for thermal imaging of real processors. We propose a hard sensor allocation algorithm to determine the sensor locations where hot spots can be tracked accurately given a budget number of sensors. We also propose soft sensor computation techniques to alleviate design constraints on sensor locations and to further improve the resolution of hot spot tracking. The proposed soft sensing technique combines the measurements of the hard sensors in an optimal way to estimate the temperature at any desired location. We use infrared imaging methods to characterize the thermal behavior of a real dual-core processor during operation. We execute large number of workload configurations on the processor and track the locations and temperatures of hot spots during runtime. The thermal characterization data are then used as the input to our sensor allocation techniques. We demonstrate that our sensor allocation techniques improve significantly upon the previous results in the literature and provide accurate tracking of hot spots.	algorithm;central processing unit;computation;computer cooling;design of experiments;embedded system;field-programmable gate array;hands-on computing;heuristic;iterative method;manycore processor;multi-core processor;np-hardness;overhead (computing);semiconductor;sensor;simulation;temporal logic;thermal management of high-power leds	Sherief Reda;Ryan Cochran;Abdullah Nazma Nowroz	2011	IEEE Transactions on Computers	10.1109/TC.2011.45	multi-core processor;thermal;embedded system;real-time computing;hotspot;temperature measurement;computer science;sensor;resource management;power;tracking;computational model;hot spot;water cooling	EDA	-6.199137510809892	59.863970767877575	141948
80ec4b5da786bea0af5f4df45f9ef52b1525239f	design space exploration of workload-specific last-level caches	power gated drowsy caches;energy efficient;last level cache;lstp transistors;power gating;leakage power;hp;stt ram;design space exploration;power consumption;tunnel fets;cache leakage	Leakage power of last-level caches constitute a significant part of overall power consumption. Various circuit-level and technology-based methods have been proposed to reduce cache leakage. However, from a system designer's perspective, for a particular configuration and workload, it is not clear which method will be most effective. In this work, we make a detailed evaluation and comparison of cache energy reduction techniques. Our results show that when energy is very scarce and important, the best results are obtained with highly energy efficient Tunnel-FET caches. When the available energy increases and performance becomes a bigger concern, there is no single winner. While a small number of capacity sensitive workloads benefit from increased capacity of STT-RAM caches, latency sensitive workloads prefer solutions with smaller latency penalties such as drowsy caches.	cmos;cpu cache;carpal tunnel syndrome;charge-coupled device;design space exploration;digital electronics;ee times;electrical connection;heterojunction;ieee micro;international solid-state circuits conference;international symposium on computer architecture;low-power broadcasting;microarchitecture;microsoft kin;parallel computing;power supply;rapid single flux quantum;schmitt trigger;spectral leakage;static random-access memory;systems design;thread (computing);traffic collision avoidance system;transistor;zero suppression	Karthik Swaminathan;Emre Kultursay;Vinay Saripalli;Narayanan Vijaykrishnan;Mahmut T. Kandemir	2012		10.1145/2333660.2333718	spin-transfer torque;parallel computing;horsepower;real-time computing;tag ram;computer hardware;engineering;efficient energy use;quantum mechanics	Arch	-5.470631844571449	56.011856301325935	142234
2a991b75bd5232590ca915c7dda51e9f8b7eba85	elastic dvs management in processors with discrete voltage/frequency modes	voltage control frequency dynamic voltage scaling timing processor scheduling energy consumption real time systems energy management job shop scheduling resource management;energy aware scheduling;real time computing dynamic voltage scaling dvs energy aware scheduling;rate adaptation;task model;dynamic voltage scaling;scheduling power aware computing real time systems;indexing terms;real time computing elastic dynamic voltage scaling management processors discrete voltage real time systems discrete voltage frequency modes computational resources rate adaptation techniques energy consumption discrete dynamic voltage scaling management elastic scheduling switching overhead task execution power consumption energy aware scheduling;power aware computing;real world application;energy consumption;scheduling;power consumption;dynamic voltage scaling dvs;real time computing;real time systems;time constraint	Applying classical dynamic voltage scaling (DVS) techniques to real-time systems running on processors with discrete voltage/frequency modes causes a waste of computational resources. In fact, whenever the ideal speed level computed by the DVS algorithm is not available in the system, to guarantee the feasibility of the task set, the processor speed must be set to the nearest level greater than the optimal one, thus underutilizing the system. Whenever the task set allows a certain degree of flexibility in specifying timing constraints, rate adaptation techniques can be adopted to balance performance (which is a function of task rates) versus energy consumption (which is a function of the processor speed). In this paper, we propose a new method that combines discrete DVS management with elastic scheduling to fully exploit the available computational resources. Depending on the application requirements, the algorithm can be set to improve performance or reduce energy consumption, so enhancing the flexibility of the system. A reclaiming mechanism is also used to take advantage of early completions. To make the proposed approach usable in real-world applications, the task model is enhanced to consider some of the real CPU characteristics, such as discrete voltage/frequency levels, switching overhead, task execution times nonlinear with the frequency, and tasks with different power consumption. Implementation issues and experimental results for the proposed algorithm are also discussed	algorithm;central processing unit;clock rate;coefficient;computation;computational resource;dynamic voltage scaling;embedded system;experiment;mobile robot;nonlinear system;overhead (computing);peripheral;real-time clock;real-time computing;requirement;run time (program lifecycle phase);scalability;scheduling (computing);simulation;time complexity	Mauro Marinoni;Giorgio C. Buttazzo	2007	IEEE Transactions on Industrial Informatics	10.1109/TII.2006.890494	embedded system;parallel computing;real-time computing;index term;computer science;scheduling	Embedded	-5.346574176042983	58.83934592788735	144330
86a1ad8866dd69281eabb81bc962b070e3ea8105	dynamic forest: an efficient index structure for nand flash memory	nand flash memory;flash memory;memoire flash;mise a jour;integrated circuit;index structure;nand circuit;circuito integrado;circuito logico;memoria no volatil;actualizacion;memoire non volatile;circuito noy;circuit logique;circuit non et;non volatile memory;indexation;memoria flash;d forest;logic circuit;article;circuit integre;updating	In this paper, we present an efficient index structure for NAND flash memory, called the Dynamic Forest (D-Forest). Since write operations incur high overhead on NAND flash memory, D-Forest is designed to minimize write operations for index updates. The experimental results show that D-Forest significantly reduces write operations compared to the conventional B+-tree. key words: NAND flash memory, D-Forest, index structure	b+ tree;database index;experiment;flash memory;nand gate;overhead (computing);random forest	Chul-Woong Yang;Ki Yong Lee;Myoung-Ho Kim;Yoon-Joon Lee	2009	IEICE Transactions	10.1587/transinf.E92.D.1181	flash file system;embedded system;parallel computing;non-volatile memory;logic gate;computer hardware;computer science;integrated circuit	DB	-11.426237106373753	54.56355777575855	144473
37142ad059d8cf64c8419832333358518200f29b	can large disk built-in caches really improve system performance?	control theory;non responsive traffic;system performance;traffic modeling;estimation;file system;power consumption;system simulation	Via detailed file system and disk system simulation, we examine the impact of disk built-in caches on the system performance. Our results indicate that the current trend of using large built-in caches is unnecessary and a waste of money and power for most users. Disk manufacturers could use much smaller built-in caches to reduce the cost as well as power-consumption, without affecting performance.	built-in self-test;canonical account;family computer disk system;simulation	Yingwu Zhu;Yiming Hu	2002		10.1145/511334.511382	embedded system;estimation;real-time computing;computer hardware;computer science;computer performance;statistics	Metrics	-11.327122812577915	53.574713663194224	144877
5659fd2d7ea1f53f6c6537585cb3d27fc229ba4b	comik: a predictable and cycle-accurately composable real-time microkernel	time division multiplexing timing context real time systems digital audio players processor scheduling jitter;fpga prototyped hardware platform comik virtual processors predictable composable real time microkernel cycle accurately composable real time microkernel embedded systems time criticality systems interapplication interference predictable processor virtualisation composable processor virtualisation mixed time criticality system verification;embedded systems;formal verification;virtualisation embedded systems field programmable gate arrays formal verification;field programmable gate arrays;virtualisation	The functionality of embedded systems is ever increasing. This has lead to mixed time-criticality systems, where applications with a variety of real-time requirements co-exist on the same platform and share resources. Due to inter-application interference, verifying the real-time requirements of such systems is generally non trivial. In this paper, we present the CoMik microkernel that provides temporally predictable and composable processor virtualisation. CoMik's virtual processors are cycle-accurately composable, i.e. their timing cannot affect the timing of co-existing virtual processors by even a single cycle. Real-time applications executing on dedicated virtual processors can therefore be verified and executed in isolation, simplifying the verification of mixed time-criticality systems. We demonstrate these properties through experimentation on an FPGA prototyped hardware platform.	central processing unit;cobra;criticality matrix;embedded system;experiment;field-programmable gate array;interference (communication);microkernel;nest (neural simulation tool);nl (complexity);real-time clock;real-time transcription;requirement;self-organized criticality;temporal logic;verification and validation	Andrew Nelson;Ashkan Beyranvand Nejad;Anca Mariana Molnos;Martijn Koedam;Kees G. W. Goossens	2014	2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.7873/DATE.2014.235	embedded system;parallel computing;real-time computing;formal verification;computer science;operating system;field-programmable gate array	Embedded	-8.41920418184278	58.7815018570155	145073
060564295e0ca007042f7fa6b4c45084573da1b5	mathematical programming models for scheduling in a cpu/fpga architecture with heterogeneous communication delays	heterogeneous system;task scheduling;cpu/fpga;communication delays;mixed integer program;cplex	This paper deals with the mathematical modelling of a scheduling problem in a heterogeneous CPU/FPGA architecture with heterogeneous communication delays in order to minimize the makespan, \(C_{max}\). This study was motivated by the quality of the available solvers for Mixed Integer Program. The proposed model includes the communication delay constraints in a heterogeneous case, depending on both tasks and computing units. These constraints are linearized without adding any extra variables and the obtained linear model is reduced to speed-up the solving with CPLEX up to 60 times. Computational results show that the proposed model is promising. For an average sized problem of up to 50 tasks and five computing units the solving time under CPLEX is a few seconds.	central processing unit;field-programmable gate array;mathematical optimization;scheduling (computing)	Abdessamad Ait El Cadi;Omar Souissi;Rabie Ben Atitallah;Nicolas Bélanger;Abdelhakim Artiba	2018	J. Intelligent Manufacturing	10.1007/s10845-015-1075-z	architecture;mathematical optimization;field-programmable gate array;job shop scheduling;linear model;scheduling (computing);parallel computing;integer;distributed computing;computer science	Robotics	-5.691734276057934	60.01548315072142	145884
836714f9b9ccb041b5fafd329e6ce02a12c30d01	a note on scsi bus waits	disk scheduling;simulation;fixed priority;look;scheduling policies with lookahead;scan;satf	In the SCSI-2 standard, the unique IDs of devices on the bus define a fixed priority whenever several devices compete for the use of the bus. Although the more recent SCSI-3 standard specifies an additional fair arbitration mode, it leaves such fair mode an optional feature. Despite a number of allusions to potential unfairness of the traditional SCSI bus arbitration scattered in the trade literature, there seem to be few formal studies to quantify this unfairness.In this paper, we propose a simple model of SCSI bus acquisition in which devices on the bus are viewed as sources of requests with fixed non-preemptive priorities. We use the model to assess the expected extent of unfairness, as measured by the mean bus wait, under varying load conditions. Effects of tagged command queueing are not considered in this note. Numerical results obtained with our model show that there is little unfairness as long as the workload is balanced across devices and the bus utilization is relatively low. Interestingly, even for medium bus utilization a significant fraction of bus requests find the bus free which might correlate with the service rounds noted in a recent experimental study. For unbalanced loads and higher bus utilization, the expected wait for the bus experienced by lowest priority devices can become significantly larger than the one experienced by highest priority device. This appears to be especially true if the higher priority devices have higher I/O rates and occupy the bus for longer periods. As might be expected, even for balanced workloads, unfairness tends to increase with the number of devices on the bus.	bus (computing);bus mastering;experiment;input/output;numerical method;parallel scsi;tagged command queuing;unbalanced circuit;waits	Alexandre Brandwajn	2002	SIGMETRICS Performance Evaluation Review	10.1145/588160.588166	embedded system;real-time computing;computer science;local bus;operating system;distributed computing;i/o scheduling;computer network	Metrics	-10.937963403490105	57.16943216642421	146317
43e602ecabb0eea97cf255546c7069a5b0d7e053	queuing spin lock algorithms with preemption	asymmetric multiprocessor;interrupt response;real time system;interprocessor synchronization;spin lock	Both predictable interprocessor synchronization and fast interrupt response are required for real-time systems constructed using asymmetric shared-memory multiprocessors. This paper points out that conventional spin lock algorithms cannot satisfy both requirements at the same time and describes two spin lock algorithms that have been proposed to solve this problem. These algorithms, extensions of queuing spin locks modified to be preemptable for servicing interrupts, can give upper bounds on the times to acquire and release an interprocessor lock while achieving a fast response to interrupt requests. The per-formance measurement of the algorithms demonstrates that the algorithms have required properties. To apply the algorithms to real-time kernels, we also propose an extended algorithm, which is a combination of the two algorithms.	algorithm;interrupt;lock (computer science);preemption (computing);real-time clock;real-time computing;requirement;shared memory;spinlock;the times	Hiroaki Takada;Ken Sakamura	1996	Systems and Computers in Japan	10.1002/scj.4690270502	embedded system;parallel computing;real-time computing;real-time operating system;spinlock;computer science;operating system	Embedded	-10.287945381140418	60.303020672078986	146786
8bc0e18100764366bcc01a00ebf7da08d6e32eb0	self-repairing disk arrays		As the prices of magnetic storage continue to decrease, the cost of replacing failed disks becomes increasingly dominated by the cost of the service call itself. We propose to eliminate these calls by building disk arrays that contain enough spare disks to operate without any human intervention during their whole lifetime. To evaluate the feasibility of this approach, we have simulated the behavior of two-dimensional disk arrays with n parity disks and n(n – 1)/2 data disks under realistic failure and repair assumptions. Our conclusion is that having n(n + 1)/2 spare disks is more than enough to achieve a 99.999 percent probability of not losing data over four years. We observe that the same objectives cannot be reached with RAID level 6 organizations and would require RAID stripes that could tolerate triple disk failures.	disk array;honeywell level 6;hot spare;magnetic storage;raid;stripes	Jehan-François Pâris;Ahmed M Amer;Darrell D. E. Long;Thomas J. E. Schwarz	2015	CoRR		real-time computing;simulation;computer security	OS	-11.58779581055451	55.75511657854785	147092
68bca3f49fd2e5fd69c84a06994ef34c42ddff80	3d stacking of high-performance processors	density measurement;power system measurements;stacking;three dimensional displays;processor on processor systems 3d stacking high performance processors power density storage power efficient design processor speed processor power high performance regime high performance machines parallelism;wiring;parallel processing microprocessor chips;program processors;program processors three dimensional displays wiring throughput density measurement power system measurements stacking;throughput	In most 3D work to date, people have looked at two situations: 1) a case in which power density is not a problem, and the parts of a processor and/or entire processors can be stacked atop each other, and 2) a case in which power density is limited, and storage is stacked atop processors. In this paper, we consider the case in which power density is a limitation, yet we stack processors atop processors. We also will discuss some of the physical limitations today that render many of the good ideas presented in other work impractical, and what would be required in the technology to make them feasible. In the high-performance regime, circuits are not designed to be “power efficient;” they're designed to be fast. In power-efficient design, the speed and power of a processor should be nearly proportional. In the high-performance regime, the frequency is (ever progressingly) sublinear in power. Thus, when the power density is constrained - as it is in high-performance machines, there may be opportunities to selectively exploit parallelism in workloads by running processor-on-processor systems at the same power, yet at much greater than half speed.	central processing unit;focus stacking;parallel computing;three-dimensional integrated circuit	Philip G. Emma;Alper Buyuktosunoglu;Michael B. Healy;Krishnan Kailas;Valentin Puente;Roy Yu;Allan Hartstein;Pradip Bose;Jaime H. Moreno	2014	2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2014.6835959	embedded system;throughput;parallel computing;computer science;stacking;operating system	Arch	-7.429892390463458	53.70838431472392	147481
ce15c7748a1962dfc600cc9d95616ea27afdc79f	message passing-aware power management on many-core systems		Dynamic frequency and voltage scaling (DVFS) techniques have been widely used for meeting energy constraints. Single-chip many-core systems bring new challenges owing to the large number of operating points and the shift to message passing from shared memory communication. DVFS, however, has been mostly studied on single-chip systems with one or few cores, without considering the impact of the communication among cores. This paper evaluates the impact of voltage and frequency scaling on the performance and power of many-core systems with message passing (MP) based communication, and proposes a power management policy that leverages the communication pattern information to efficiently traverse the search space for finding the optimal voltage and frequency operating point. We conduct experiments on a 48-core Intel Single-Chip Cloud Computer (SCC), as our target many-core platform. The paper first introduces the runtime monitoring infrastructure and the application suite we have designed for an in-depth evaluation of the SCC. We then quantify the effects of frequency perturbations on performance and energy efficiency. Experimental results show that runtime communication patterns lead to significant differences in power/performance tradeoffs in many-core systems with MP-based communication. We show that the proposed power management policy achieves up to the 70% energy-delayproduct (EDP) improvements compared to existing DVFS policies, while meeting the performance constraints.	dynamic frequency scaling;dynamic voltage scaling;electronic data processing;experiment;image scaling;manycore processor;message passing;operating point;power management;shared memory;single-chip cloud computer;traverse	Andrea Bartolini;Can Hankendi;Ayse Kivilcim Coskun;Luca Benini	2014	J. Low Power Electronics	10.1166/jolpe.2014.1359	message;message broker	HPC	-4.675463420757401	54.923732812621516	147762
4f64fd39a29fc2e3bb36783fba1050b82a1cbd7d	a misconception in blocking time analyses under multiprocessor synchronization protocols	blocking time analyses;multiprocessor synchronization protocols;schedulability analyses	In multiprocessor systems, synchronization protocols can result in non-trivial (remote) blocking, which can cause timing impacts in real-time systems. To analyze the schedulability or the worst-case response time, it is essential to calculate a safe upper bound on the maximum remote blocking time due to the synchronization protocols. The derivation of the upper bound is sometimes unsafe in the literature when the analyses improperly adopt a misconception that assumes the well-known critical instant theorem. In this paper, we show that the original analyses for the distributed priority ceiling protocol and the multiprocessor priority ceiling protocol are unsafe in the calculation of the blocking time due to this misconception. This results in repetition of unsafe timing analyses in the literature. This paper also provides a simple remedy for such a flaw.	best, worst and average case;blocking (computing);flaw hypothesis methodology;instant messaging;multiprocessing;priority ceiling protocol;real-time clock;real-time computing;response time (technology);scheduling (computing);synchronization (computer science);transponder timing;whole earth 'lectronic link	Maolin Yang;Jian-Jia Chen;Wen-Hung Huang	2016	Real-Time Systems	10.1007/s11241-016-9261-4	real-time computing;computer science;operating system;distributed computing;algorithm	Embedded	-9.83675168407093	60.438148817719124	148206
3cdfd77f1e1eaa4a709729183fc122da46e6df1a	performance analysis of disability based fault tolerance techniques for permanent faults in chip multiprocessors		Dynamic Voltage and Frequency Scaling (DVFS) for reducing power dissipation in Multicore Chips causes cell failure in Cache Memory. Various fault tolerance techniques have been introduced and the analysis of their impacts becomes necessary. Keeping the lowest overhead of Disabling techniques in mind, this work attempts to analyse its performance in Multicore Chips. The parameter Expected Miss Ratio for Multicore ((EMR_{MC})) as a function of Probability of Cell Failure ((P_{fail})) is proposed and evaluated. Simulation on Singlecore and Multicore system configuration is done separately to compare the results. It is observed that the Expected Miss Ratio is hardly affected below the lower bound of (P_{fail}) i.e. 1e-5 where (EMR_{MC}) remains lower than Expected Miss Ratio for Singlecore((EMR_{SC})) with a static difference. Above the lower bound, both (EMR_{SC}) and (EMR_{MC}) starts increasing and for (P_{fail}) higher than 1e-3 i.e. the upper bound, (EMR_{MC}) often converges with (EMR_{SC}). Within these bounds, (EMR_{MC}) remains up to 19.3% lower than the (EMR_{SC}).	fault tolerance;profiling (computer programming)	Avishek Choudhury;Biplab K. Sikdar	2017		10.1007/978-981-10-7470-7_22	chip;electronic engineering;discrete mathematics;voltage;fault tolerance;cpu cache;frequency scaling;computer science;multi-core processor;dissipation;upper and lower bounds	HPC	-6.430317148464259	55.9714473829282	149426
1473aa889df1449a65b8a609cfe60337cf926780	criticality- and requirement-aware bus arbitration for multi-core mixed criticality systems	interference multicore processing optical wavelength conversion switches memory management job shop scheduling aerospace electronics;interacting software components criticality aware bus arbitration requirement aware bus arbitration multicore mixed criticality systems carb shared memory bus requirement aware arbiter first class principle criticality levels processor scheduling policies honeywell avionics case study;system buses object oriented programming processor scheduling shared memory systems	This work presents CArb, an arbiter for controlling accesses to the shared memory bus in multi-core mixed criticality systems. CArb is a requirement-aware arbiter that optimally allocates service to tasks based on their requirements. It is also criticality-aware since it incorporates criticality as a first-class principle in arbitration decisions. CArb supports any number of criticality levels and does not impose any restrictions on mapping tasks to processors. Hence, it operates in tandem with existing processor scheduling policies. In addition, CArb is able to dynamically adapt memory bus arbitration at run time to respond to increases in the monitored execution times of tasks. Utilizing this adaptation, CArb is able to offset these increases; hence, postpones the system need to switch to a degraded mode. We prototype CArb, and evaluate it with an avionics case-study from Honeywell as well as synthetic experiments.	arbiter (electronics);avionics;booting;bus mastering;central processing unit;conventional pci;criticality matrix;degraded mode;experiment;interference (communication);memory bus;mixed criticality;multi-core processor;prototype;requirement;round-robin scheduling;run time (program lifecycle phase);scheduling (computing);self-organized criticality;shared memory;synthetic data;synthetic intelligence;weighted round robin;windows nt processor scheduling	Mohamed Hassan;Hiren D. Patel	2016	2016 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2016.7461327	embedded system;parallel computing;real-time computing;operating system	Embedded	-8.631550256074165	58.53549285965011	149779
331a3af789c833a4a9cdcd2e013b9963f2f3eba9	smart cache cleaning: energy efficient vulnerability reduction in embedded processors	cache write back soft error vulnerability hybrid technique smart cache architecture energy efficient;cache storage;cleaning program processors hardware embedded systems reliability arrays instruments;smart cache architecture;energy efficient;vulnerability;technology scaling;data cache;data cache protection smart cache cleaning energy efficient vulnerability reduction embedded processor soft error write through cache configuration early write back cache configuration;embedded processor;soft error;hybrid technique;cache write back	Incessant and rapid technology scaling has brought us to a point where todays, and future transistors are susceptible to transient errors induced by energy carrying particles, called soft errors. Within a processor, the sheer size and nature of data in the caches render it most vulnerable to electrical interferences on static data in the cache. Data in the cache is vulnerable to corruption by soft errors, for the time it remains in the cache. Write-through and early-write-back [17] cache configurations reduce the time for vulnerable data in the cache, at the cost of increased memory writes and therefore energy. We propose a smart cache cleaning methodology, that enables copying of only specific vulnerable cache blocks into the memory at chosen times, thereby ensuring data cache protection with minimal memory writes. Our experiments over LINPACK and Livermore benchmarks demonstrate 26% reduced energy-vulnerability product compared to that of hardware cache configurations.	cache (computing);central processing unit;embedded system;experiment;image scaling;lunpack;smart cache;soft error;sputter cleaning;transistor	Reiley Jeyapaul;Aviral Shrivastava	2011	2011 Proceedings of the 14th International Conference on Compilers, Architectures and Synthesis for Embedded Systems (CASES)	10.1145/2038698.2038716	bus sniffing;embedded system;least frequently used;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;soft error;computer hardware;vulnerability;cache;computer science;write-once;cache invalidation;operating system;write buffer;efficient energy use;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;global assembly cache	EDA	-7.227255423954266	55.26889843700886	149881
49f4b9a1c16ba2ad4906d18d009f67b92c0ed477	minimizing memory utilization of task sets in smartosek	minimisation;automobiles;resource allocation;real time operating system;upper bound;operating system;random access storage;interrupts;minimisation automobiles operating systems computers program compilers interrupts random access storage real time systems resource allocation;program compilers;operating systems random access memory read write memory automotive engineering resource management control systems standards development automobiles real time systems registers;interrupt handling memory utilization minimization smartosek automobile software smart services real time operating system osek compliant os optimizations ram reduction program compiler register allocation stack space task handling;operating systems computers;real time systems	There has been a rapid growth in the amount and the importance of software in automobiles in the past two decades. This trend makes us view automobile space as a set of smart services. The basic platform for managing these services is a real-time operating system. Due to the limited resource in ECU, this operating system must be efficient in some aspects especially in memory utilization. We have implemented an operating system SmartOSEK, an OSEK-compliant OS. Some features and optimizations to reduce the RAM needed by the operating system are presented in this paper. First, an upper bound for the register set of each task that has to be stored can be calculated off-line. It also calls for the extension of the compiler to allocate registers across tasks for task contexts. We provide an experimental implementation with its result shown in this paper. Second, the stack space is saved based on some properties of tasks and interrupt handling.	algorithm;apple sos;central processing unit;compiler;engine control unit;interrupt;mathematical optimization;osek;online and offline;random-access memory;real-time clock;real-time operating system;real-time transcription;register allocation;requirement;scheduling (computing)	Wei Chen;Zhaohui Wu;Xiuli Wang	2005	19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)	10.1109/AINA.2005.247	embedded system;minimisation;embedded operating system;real-time computing;memory management;real-time operating system;resource allocation;computer science;operating system;interrupt;database;upper and lower bounds;computer security	Embedded	-8.000989316850507	57.88658879984884	149952
b8ffde6534c727bb3e4c944fcb40af696b85c576	cpu architecture based on a hardware scheduler and independent pipeline registers	registers hardware synchronization dynamic scheduling pipelines computer architecture switches;computer architecture;real time and embedded systems hardware scheduler microprocessors and microcomputers pipeline processors;registers;synchronization;pipelines;scheduling operating systems computers resource allocation;switches;dynamic scheduling;hardware;program execution speed cpu architecture central processing unit hardware scheduler pipeline registers task switching synchronization process communication realtime operating system hardware scheduler architecture resource remapping techniques cpu working registers static task scheduling dynamic task scheduling	Task switching, synchronization, and communication between processes are major problems for each real-time operating system. Software implementation of the specific mechanisms may lead to significant delays that can affect deadline requirements for some applications. This paper presents a hardware scheduler architecture integrated into the CPU structure that uses resource remapping techniques for the pipeline registers and for the CPU working registers. We present an original implementation of the hardware structure used for static and dynamic scheduling of the task, unitary management of events, access to architecture shared resources, event generation, and a method used for assigning interrupts to tasks that insures an efficient operation in the context of real-time control. One assembler instruction is used for simultaneous task synchronization with multiple event sources. This architecture allows a task switching time of one clock cycle (with a worst case scenario of three clock cycles for special instructions used for external memory accesses) and a response time of only 1.5 clock cycles for the events. Some mechanisms for improving program execution speed are also taken in consideration.	arm cortex-m;algorithm;assembly language;central processing unit;circuit restoration;clock signal;event generator;instruction cycle;interrupt;mutual exclusion;overhead (computing);pipeline (computing);real-time operating system;real-time transcription;requirement;response time (technology);scheduling (computing);subroutine;switching time;watchdog timer;worst-case scenario	Vasile Gheorghita Gaitan;Nicoleta-Cristina Gaitan;Ioan Ungurean	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2346542	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;synchronization;computer architecture;parallel computing;real-time computing;memory type range register;dynamic priority scheduling;network switch;computer science;operating system;transport triggered architecture;pipeline transport;processor register	Embedded	-9.238823148728939	59.10755667036312	150495
17c831af78b27d010aaffa50451da51238b50186	comem: collaborative memory management for real-time operation within reactive sensor/actor networks	preemptive prioritized tasks;real time;embedded systems;dynamic memory;sensor networks;reflection	Increasing complexity and modularity of today’s WSAN applications impose demanding challenges on the system design. This especially affects real-time operation, resource sharing and dynamic memory management. Preemptive task systems are one way to retain good reactivity within dynamic environments. Yet, since memory is often too rare for static assignment, this rapidly leads to severe compositional problems among tasks with interfering and even varying requirements. We present our novel CoMem approach for maintaining high reactivity and efficient memory usage in such systems. With respect to task priorities and the typically limited resources of sensor nodes, we facilitate compositional software design by providing independently developed tasks with runtime information for yet collaborative and self-reflective memory sharing. Thereby, we require no special hardware-support like MMUs but operate entirely software-based.	actor model;algorithm;best, worst and average case;blocking (computing);embedded system;locality of reference;memory management unit;microsoft outlook for mac;multi-core processor;overhead (computing);preemption (computing);priority inversion;real-time clock;real-time computing;real-time operating system;reflective memory;relevance;requirement;run time (program lifecycle phase);sensor;shared memory;software design;systems design	Marcel Baunach	2011	Real-Time Systems	10.1007/s11241-011-9136-7	real-time computing;dynamic random-access memory;simulation;wireless sensor network;reflection;computer science;operating system;distributed computing	Embedded	-9.279332105342979	57.57889688195493	150702
54f1fec82a7a2134b33bd4eccd9ea4f3eebb6a96	big-data streaming applications scheduling with online learning and concept drift detection	cache;pcm;dynamic scheduling;data mining;learning artificial intelligence;reram;qos;scheduling;application layer;stt ram;big data;throughput;concept drift;sram;quality of service	Several techniques have been proposed to adapt Big-Data streaming applications to resource constraints. These techniques are mostly implemented at the application layer and make simplistic assumptions about the system resources and they are often agnostic to the system capabilities. Moreover, they often assume that the data streams characteristics and their processing needs are stationary, which is not true in practice. In fact, data streams are highly dynamic and may also experience concept drift, thereby requiring continuous online adaptation of the throughput and quality to each processing task. Hence, existing solutions for Big-Data streaming applications are often too conservative or too aggressive. To address these limitations, we propose an online energy-efficient scheduler which maximizes the QoS (i.e., throughput and output quality) of Big-Data streaming applications under energy and resources constraints. Our scheduler uses online adaptive reinforcement learning techniques and requires no offline information. Moreover, our scheduler is able to detect concept drifts and to smoothly adapt the scheduling strategy. Our experiments realized on a chain of tasks modeling real-life streaming application demonstrate that our scheduler is able to learn the scheduling policy and to adapt it such that it maximizes the targeted QoS given energy constraint as the Big-Data characteristics are dynamically changing.	algorithm;big data;concept drift;experiment;online and offline;quality of service;real life;reinforcement learning;scheduling (computing);shortest path problem;smoothing;stationary process;throughput	Karim Kanoun;Mihaela van der Schaar	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		pulse-code modulation;fixed-priority pre-emptive scheduling;embedded system;spin-transfer torque;electronic engineering;parallel computing;real-time computing;static random-access memory;resistive random-access memory;cache;computer science;operating system;distributed computing;scheduling	Embedded	-6.321666391877505	56.74816282373775	150992
15f848d9c5db6c3749724db8c6ad910e4e281ec3	performance analysis of hard-real-time embedded software	integer linear programming;performance evaluation;ilp;embedded systems;worst case execution time;wcet;performance analysis;hard real time;embedded software;hard real time systems	The execution time of an instruction depends on its adjacent instructions and I/O activities. Our method first iteratively determines the set of all possible execution times of each instruction. We next construct a set of linear constraints on their execution counts. The maximum value of the cost function is an upper bound of the worst-case execution time. We demonstrate the capability of this method on a machine model where a processor has an instruction cache and pipeline, and cyclestealing DMA I/O is concurrently executing. The experimental results show that our method safely and tightly bounds the worstcase execution time.	best, worst and average case;direct memory access;embedded software;embedded system;input/output;iterative method;loss function;pipeline (computing);profiling (computer programming);real-time clock;run time (program lifecycle phase);worst-case execution time	Tai-Yi Huang;Kuang-Li Huang;Yeh-Ching Chung	2006	IJES	10.1504/IJES.2006.014856	embedded system;computer architecture;parallel computing;real-time computing;integer programming;embedded software;computer science;operating system;worst-case execution time	EDA	-7.395082934930052	57.68465021074965	151413
7c19995b5fbf2161e8e0720a5f9384a8b010fa5c	pbc: prefetched blocks compaction	memory structures;prefetching;prefetching compaction yttrium encoding indexes hardware organizations;indexes;storage management cache storage data compression multiprocessing systems;compaction;yttrium;prefetching memory structures cache memories compression compaction cache design;organizations;compression;cache design;encoding;cache organization prefetched blocks compaction pbc cache compression multicore system cache block storage data patterns cache capacity off chip latency hiding;cache memories;hardware	Cache compression improves the performance of a multi-core system by being able to store more cache blocks in a compressed format. Compression is achieved by exploiting data patterns present within a block. For a given cache space, compression increases the effective cache capacity. However, this increase is limited by the number of tags that can be accommodated at the cache. Prefetching is another technique that improves system performance by fetching the cache blocks ahead of time into the cache and hiding the off-chip latency. Commonly used hardware prefetchers, such as stream and stride, fetch multiple contiguous blocks into the cache. In this paper we propose prefetched blocks compaction (PBC) wherein we exploit the data patterns present across these prefetched blocks. PBC compacts the prefetched blocks into a single block with a single tag, effectively increasing the cache capacity. We also modify the cache organization to access these multiple cache blocks residing in a single block without any need for extra tag look-ups. PBC improves the system performance by 11.1 percent with a maximum of 43.4 percent on a four-core system.	data compaction;exploit (computer security);link prefetching;multi-core processor;periodic boundary conditions	Kanakagiri Raghavendra;Biswabandan Panda;Madhu Mutyam	2016	IEEE Transactions on Computers	10.1109/TC.2015.2493533	bus sniffing;database index;compaction;least frequently used;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;organization;write-once;cache invalidation;yttrium;smart cache;mesi protocol;cache algorithms;cache pollution;compression;mesif protocol;encoding	Arch	-8.974061427023141	53.45716111635127	152002
a5da1222a422adb36cf50722fd33e4d343dbee1c	low power data prefetch for 3d image applications on coarse-grain reconfigurable architectures	search problems reconfigurable architectures power consumption storage management interactive systems image processing;3d interaction;prefetching reconfigurable architectures partitioning algorithms graphics power engineering computing time factors ray tracing data engineering power engineering and energy coprocessors;image processing;3d imaging;reconfigurable architectures;storage management;prefetching;search algorithm;3d image applications;data engineering;satisfiability;coprocessors;power engineering and energy;reconfigurable architecture;power engineering computing;time factors;data prefetch policy;low power;interactive application;coarse grain reconfigurable architectures;ray tracing;3d image applications data prefetch policy 3d interactive applications coarse grain reconfigurable architectures search algorithms;data prefetching;search problems;power consumption;3d interactive applications;coarse grained;interactive systems;graphics;search algorithms;partitioning algorithms;time constraint	In this paper we present a data prefetch policy for the execution of 3D interactive applications on coarse-grain reconfigurable architectures. User's actions not occurring at expected times combined with the volume of data involved in this kind of applications impact strongly in performance. We define a data prefetch scheme to avoid reconfigurable processing unit stalls due to operands unavailability through profiling methodologies and special search algorithms. Experimental results satisfy time constraints of interactive applications and show a power effective solution for them.	operand;profiling (computer programming);reconfigurable computing;search algorithm;unavailability	Fredy Rivera;Marcos Sanchez-Elez;Milagros Fernández;Román Hermida;Nader Bagherzadeh	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.270	instruction prefetch;computer architecture;parallel computing;real-time computing;image processing;computer science;operating system;distributed computing;search algorithm	HPC	-5.967098228576789	54.28065319275095	152259
86f30aecd86935425a7ffd7375669e632a0a7874	improved response time analysis of sporadic dag tasks for global fp scheduling		One of the major sources of pessimism in the response time analysis of globally scheduled real-time tasks is the computation of the upper-bound on the inter-task interference. This problem is further exacerbated when intra-task parallelism is permitted, because of the complex internal structure of parallel tasks. This paper considers the global fixed-priority scheduling (G-FP) of sporadic real-time tasks, each one modeled by a directed acyclic graph (DAG) of parallel subtasks. We present a response time analysis (RTA) technique based on the concept of problem window. We propose two novel techniques to derive less pessimistic upper-bounds on the workload produced by the carry-in and carry-out jobs of the interfering tasks, by taking into account the precedence constraints between their subtasks. We show that with these new upper-bounds, the proposed schedulability test does not only theoretically dominate state-of-the-art techniques but also offers significant improvements on the schedulability of DAG tasks for randomly generated task sets.	best, worst and average case;computation;directed acyclic graph;execution pattern;fixed-priority pre-emptive scheduling;interference (communication);job stream;multiprocessing;parallel computing;procedural generation;real-time clock;response time (technology);scheduling (computing);scheduling analysis real-time systems;task parallelism	José Carlos Fonseca;Geoffrey Nelissen;Vincent Nélis	2017		10.1145/3139258.3139288	real-time computing;distributed computing;computer science;workload;computation;directed acyclic graph;scheduling (computing);response time;ethernet	Embedded	-10.507048508587545	60.23611663822913	152438
abee23c7f795711300b4ac2344e43c08eff3042d	reducing energy usage in drive storage clusters through intelligent allocation of incoming commands	pareto optimisation;disk drives;data storage systems;pareto optimization;optimal scheduling;decision support systems;industrial relations	Although significant research has been undertaken to reduce high level energy consumption in a data centre, there has been very little focus on reducing storage drive energy consumption via the intelligent allocation of workload commands at the file system level. This paper presents a method for optimising drive energy consumption within a custom built storage cluster containing multiple drives, using multi-objective goal attainment optimization. Significantly, the model developed was based on actual power consumption values (from current/voltage sensors on the drives themselves), which is rare in this field. The results showed that command energy savings of up to 87% (17% overall energy) could be made by optimising the allocation of incoming commands for execution to drives within a storage cluster for different workloads. More significantly, the transparency of the method meant that it showed exactly ∗Corresponding author. Tel: +44 (0) 23 9284 4448, Fax: +44 (0) 23 9284 5192 Email addresses: edward.smart@port.ac.uk (E. Smart), david.j.brown@port.ac.uk (D. Brown), kais.toumi@seagate.com (K. Toumi Borges), nick.grangerbrown@seagate.com (N. Granger-Brown) Preprint submitted to Applied Soft Computing October 6, 2016 how such savings could be made and on which drives. It also highlighted that whilst it is well known that solid state drives use less energy than traditional hard disk drives, the difference is not consistent for different sizes of data transfers. It is far larger for small data transfers (less than or equal to 4 kB) and our algorithm utilised this. Significantly, it highlights how much larger energy savings can be made through using the optimisation results to show which drives can be safely put into a low power state without affecting storage cluster performance.		Edward Smart;Deborah Brown;K. Toumi Borges;N. Granger-Brown	2017	Appl. Soft Comput.	10.1016/j.asoc.2016.10.005	real-time computing;computer hardware;computer science;multi-objective optimization;industrial relations	OS	-9.06907672707794	55.5758477473691	152462
37146652e00f0dcc23eb7006eac5fdb08baa2004	improving read performance of phase change memories via write cancellation and write pausing	phase change materials;electric resistance;random access memory;large scale main memory systems;phase change memories;performance evaluation;read priority scheduling;storage management;system performance;priority scheduling;large scale;crystallization;write cancellation;phase change memory;pcm controller phase change memories write cancellation write pausing large scale main memory systems read latency read priority scheduling baseline pcm system;baseline pcm system;memory systems;writing;random access storage;cost effectiveness;storage management random access storage;phase change memory phase change materials delay electric resistance read write memory system performance random access memory switches crystallization costs;read latency;read write memory;switches;benchmark testing;write pausing;pcm controller	Phase Change Memory (PCM) is emerging as a promising technology to build large-scale main memory systems in a cost-effective manner. A characteristic of PCM is that it has write latency much higher than read latency. A higher write latency can typically be tolerated using buffers. However, once a write request is scheduled for service to a bank, it can still cause increased latency for later arriving read requests to the same bank. We show that for the baseline PCM system with read-priority scheduling, the write requests increase the effective read latency to 2.3x (on average), causing significant performance degradation. To reduce the read latency of PCM devices under such scenarios, we propose adaptive Write Cancellation policies. Such policies can abort the processing of a scheduled write requests if a read request arrives to the same bank within a predetermined period. We also propose Write Pausing, which exploits the iterative write algorithms used in PCM to pause at the end of each write iteration to service any pending reads. For the baseline system, the proposed technique removes 75% of the latency increase incurred by read requests and improves overall system performance by 46% (on average), while requiring negligible hardware and simple extensions to PCM controller.	algorithm;baseline (configuration management);computer data storage;data buffer;dynamic random-access memory;elegant degradation;iteration;overhead (computing);phase-change memory;run time (program lifecycle phase);scheduling (computing)	Moinuddin K. Qureshi;Michele Franceschini;Luis Alfonso Lastras-Montaño	2010	HPCA - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture	10.1109/HPCA.2010.5416645	benchmark;electrical resistance and conductance;computer architecture;parallel computing;real-time computing;cost-effectiveness analysis;phase-change memory;computer hardware;network switch;computer science;computer performance;crystallization;write combining;writing	Arch	-9.99277858159211	53.8943441544173	153100
5a04a10af20300d5c9400f15357b559fb184affa	an integrated approach to system-level cpu and memory energy efficiency on computing systems	integrated memory circuits;h states dynamic voltage frequency scaling dvfs servers integrated energy policy p states;bandwidth power demand memory management energy efficiency benchmark testing equations time frequency analysis;efficiency 9 04 percent system level cpu memory energy efficiency computing systems dynamic voltage frequency scaling policies integrated dvfs policies resources combined state spec cpu2006 benchmark	Energy-efficient computing is becoming more important with the latest technology improvements. State-of-the-art dynamic voltage/frequency scaling (DVFS) policies manage resources' voltage and frequency to achieve higher energy efficiency. A DVFS policy manages a single resource by continuously evaluating its utilization. We propose a new integrated DVFS policy that manages both CPU and memory. The policy selects their frequency/voltage based on resources' combined state, rather than evaluating isolated information about each resource. For the SPEC CPU2006 benchmark, results show that our policy has an average of 9.04% energy efficiency improvement for CPU and memory compared to 4.84% savings by an independent policy.	algorithm;benchmark (computing);cas latency;central processing unit;dynamic voltage scaling;electronic circuit;emotion markup language;flash memory;frequency scaling;graphics processing unit;image scaling;input/output;interaction	Furkan Ercan;Neven Abou Gazala;Howard David	2012	2012 International Conference on Energy Aware Computing	10.1109/ICEAC.2012.6471018	embedded system;parallel computing;real-time computing;computer science	HPC	-4.887624590068056	55.68628766548649	153103
38a64af59b3b8af69f66b93548482b1b7eb76039	opportunistic refreshing algorithm for edram memories	analytical models;microprocessors;random access memory;memory management;radiation detectors;transistors	Embedded DRAM (eDRAM) is an alternative technology that can replace the area and power consumed by SRAM cache memories. eDRAM consumes half the area and an order of magnitude less power than SRAM, but has the drawback of access blockage caused by its periodic data refreshing. This paper presents an opportunistic refreshing algorithm along with the appropriate memory architecture and skim control logic. This architecture takes advantage of the access idleness of the internal partitions of the memory and enables most of the refreshing operations to run concurrently with the ordinary R/W access. This eliminates the refreshing burden almost completely. The algorithm was simulated with industrial DSP access traces, and outperformed in a wide range of eDRAM technologies and internal memory architectures.	algorithm;cpu cache;computer data storage;digital signal processor;dynamic random-access memory;edram;simulation;smart common input method;static random-access memory;tracing (software)	Amit Kazimirsky;Shmuel Wimer	2016	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2016.2600538	parallel computing;real-time computing;cpu cache;computer hardware;computer science;electrical engineering;particle detector;transistor;memory management	EDA	-8.400514223993301	54.36794929954912	153622
ab5eb825e332846c9546adb91654ffd83f08f66b	evaluating tradeoffs in granularity and overheads in supporting nonvolatile execution semantics	semantics;energy harvesting;checkpointing;checkpointing nonvolatile memory benchmark testing energy harvesting semantics hardware energy storage;nonvolatile memory;energy storage;benchmark testing;hardware	While pausing and resuming execution using nonvolatile storage has long been possible, nonvolatile processing as a fundamental paradigm has only recently been made practical by technology advances allowing on-chip nonvolatile memories. However, even with on-chip nonvolatile storage, the granularity of ensured forward progress that a nonvolatile processor offers can still vary widely from cycle-level guarantees to software-defined checkpoints spanning potentially significant quantities of execution. Choice of supported granularity influences not only the hardware overheads, but also the complexity of avoiding potential inconsistencies between architectural and microarchitectural state in realistic memory systems. In this paper, we examine the overheads, in terms of both complexity and efficiency, for non-volatile processor designs with different granularity of forward progress guarantees.	file spanning;microarchitecture;non-volatile memory;programming language;programming paradigm	Kaisheng Ma;Minli Julie Liao;Xueqing Li;Zhixuan Huan;Jack Sampson	2017	2017 18th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2017.7918290	embedded system;benchmark;parallel computing;real-time computing;non-volatile memory;computer hardware;computer science;electrical engineering;operating system;semantics;energy storage;energy harvesting	Arch	-6.435235862851809	55.61356625753542	154304
b4d985e112e813db6e626a28cef30a3b2b2ba30c	employing two 'sandwich delay' mechanisms to enhance predictability of embedded systems which use time-triggered co-operative architectures	time triggered;sandwich delay;co operative;task overrun;multiple timer interrupts;cyclic executive;jitter	In many real-time resource-constrained embedded systems, highly-predictable system behavior is a key design requirement. The “time-triggered co-operative” (TTC) scheduling algorithm provides a good match for a wide range of low-cost embedded applications. As a consequence of the resource, timing, and power constraints, the implementation of such algorithm is often far from trivial. Thus, basic implementation of TTC algorithm can result in excessive levels of task jitter which may jeopardize the predictability of many time-critical applications using this algorithm. This paper discusses the main sources of jitter in earlier TTC implementations and develops two alternative implementations – based on the employment of “sandwich delay” (SD) mechanisms – to reduce task jitter in TTC system significantly. In addition to jitter levels at task release times, we also assess the CPU, memory and power requirements involved in practical implementations of the proposed schedulers. The paper concludes that the TTC scheduler implementation using “multiple timer interrupt” (MTI) technique achieves better performance in terms of timing behavior and resource utilization as opposed to the other implementation which is based on a simple SD mechanism. Use of MTI technique is also found to provide a simple solution to “task overrun” problem which may degrade the performance of many TTC systems.	algorithm;central processing unit;embedded software;embedded system;information systems research;interrupt;moving target indication;overhead (computing);real-time clock;requirement;run time (program lifecycle phase);scheduling (computing);software developer;timer;window of opportunity	Mouaaz Nahas	2011	JSEA	10.4236/jsea.2011.47048	cooperative;embedded system;real-time computing;jitter;computer science;engineering;operating system	Embedded	-9.305142087335092	59.110574539657804	154999
8685ddfdf289afab85482944afe62c944f5d4ff1	hybrid dynamic thermal management method with model predictive control	microprocessors;predictive control;reliability;safe temperature ceiling hybrid dynamic thermal management method model predictive control microprocessor temperature control chip reliability performance hybrid thermal regulation techniques task migration dynamic voltage frequency scaling dvfs methods;power distribution;thermal management packaging microprocessor chips multiprocessing systems power aware computing predictive control temperature control;predictive control thermal management microprocessors power demand temperature distribution power distribution reliability;power demand;thermal management;temperature distribution	Dynamic thermal management methods are important to control the temperature of microprocessor at runtime and improve the reliability performance of the chip. In this paper, a model predictive control based dynamic thermal management method is proposed with hybrid thermal regulation techniques combining task migration and dynamic voltage frequency scaling (DVFS) methods. The new method is able to track the specified safe temperature ceiling, reduce processor performance degradation, and lower temperature variation among different cores.	algorithm;dynamic voltage scaling;elegant degradation;frequency scaling;image scaling;microprocessor;run time (program lifecycle phase);thermal management of high-power leds	Jian Ma;Hai Wang;Sheldon X.-D. Tan;Chi Zhang;He Tang	2014	2014 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2014.7032888	embedded system;electronic engineering;real-time computing;thermal management of electronic devices and systems;engineering;reliability;model predictive control;statistics	EDA	-4.948248139589189	57.602231470908386	155422
2117d95c3fb66fcae925454227dd4378fcf3cf76	impact of virtual execution environments on processor energy consumption and hardware adaptation	energy efficiency;energy efficient;java virtual machine;high energy;hardware adaptation;garbage collection;data cache;energy consumption;execution environment;power dissipation;garbage collector;performance analysis;just in time;large data	During recent years, microprocessor energy consumption has been surging and efforts to reduce power and energy have received a lot of attention. At the same time, virtual execution environments (VEEs), such as Java virtual machines, have grown in popularity. Hence, it is important to evaluate the impact of virtual execution environments on microprocessor energy consumption. This paper characterizes the energy and power impact of two important components of VEEs, Just-in-time(JIT) optimization and garbage collection. We find that by reducing instruction counts, JIT optimization significantly reduces energy consumption, while garbage collection incurs runtime overhead that consumes more energy. Importantly, both JIT optimization and garbage collection decrease the average power dissipated by a program. Detailed analysis reveals that both JIT optimizer and JIT optimized code dissipate less power than un-optimized code. On the other hand, being memory bound and with low ILP, the garbage collector dissipates less power than the application code, but rarely affects the average power of the latter.Adaptive microarchitectures are another recent trend for energy reduction where microarchitectural resources can be dynamically tuned to match program runtime requirements. This research reveals that both JIT optimization and garbage collection alter a program's behavior and runtime requirements, which considerably affects the adaptation of configurable hardware units, and influences the overall energy consumption. This work also demonstrates that the adaptation preferences of the two VEE services differ substantially from those of the application code. Both VEE services prefer a simple core for high energy reduction. On the other hand, the JIT optimizer usually requires larger data caches, while the garbage collector rarely benefits from large data caches. The insights gained in this paper point to novel techniques that can further reduce microprocessor energy consumption.	adaptive grammar;garbage collection (computer science);java virtual machine;just-in-time compilation;mathematical optimization;memory bound function;microarchitecture;microprocessor;overhead (computing);requirement	Shiwen Hu;Lizy Kurian John	2006		10.1145/1134760.1134775	parallel computing;real-time computing;computer science;operating system;efficient energy use;garbage collection;programming language	Arch	-4.600354716343208	53.95265663938138	155812
ff055f694a4e141f0cdd902091830aef4c645df7	dheating: dispersed heating repair for self-healing nand flash memory	self-healing nand flash memory;nand flash management technique;flash memory cell;triple-level cell flash memory;concentrated heating;nand flash cell;worn-out nand flash cell;self-healing flash memory;nand flash memory;concentrated heating problem	Short lifetimes are becoming a critical issue in NAND flash memory with the advent of multi-level cell and triple-level cell flash memory. Researchers at Macronix have recently discovered that heating can cause worn-out NAND flash cells to become reusable and greatly prolong the lifetime of flash memory cells. However, the heating process consumes a substantial amount of power. This means that some fundamental changes are required if existing NAND flash management techniques are to be applied in self-healing NAND flash memory. In particular, all existing wear-leveling techniques are based on the principle of evenly distributing writes and erases. This causes NAND flash cells tend to wear out in a short time period. Moreover, healing these cells in a concentrated manner may cause power outages in mobile devices.  In this paper, we propose for the first time a new wear-leveling scheme called DHeating (Dispersed Heating) to solve the concentrated heating problem in self-healing flash memory. In DHeating, rather than evenly distributing writes and erases over a time period, write and erase operations are concentrated on a small portion of flash memory cells, so that these cells can be worn-out and healed by heating first. In this way, we can disperse healing to avoid the problem of concentrated power usage caused by heating. Furthermore, with the very long lifetime that results from self-healing, we can sacrifice lifetime for reliability. Therefore, we propose an early heating strategy to solve the reliability problem caused by concentrated heating. The idea is to start the healing process earlier by heating NAND flash cells before their expected endurance. We evaluate our scheme based on a real embedded platform. The experimental results show that our scheme can effectively solve the concentrated heating problem.	embedded system;flash memory;mobile device;multi-level cell;wear leveling	Renhai Chen;Yi Wang;Zili Shao	2013	2013 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)		embedded system;parallel computing;computer hardware;engineering	Embedded	-7.550632343917066	55.833008319559674	155947
87969ba5ea5bf81f522e0c6bfdc9a67f2c55861d	hardware-assisted decentralized resource management for networks on chip with qos	policy;network on chip;routing;resource allocation;resource management;best effort;qos;resource management tiles hardware quality of service operating systems routing;communication resource allocation hardware assisted decentralized resource management networks on chip qos many core systems on chip concurrent applications computation resources hardware supported decentralized noc resource management strategy decentralized reconfigurable resource management policies;concurrency control;guaranteed service;networks on chip;tiles;multiprocessing systems;resource allocation concurrency control multiprocessing systems network on chip quality of service;quality of service;best effort networks on chip quality of service qos resource management policy guaranteed service;operating systems;hardware	Networks-on-Chip have shown their scalability for future many-core systems on chip. In real world scenarios, concurrent applications with different QoS requirements affect each other through overlapping communication. Therefore computation resources may not be efficiently utilized because the required communication resources are already occupied. Hence, an efficient resource management strategy is required that ensures fair sharing of communication resources between applications. Decentralized strategies provide better scalability in many-core systems. In this paper, we propose a hardware supported decentralized NoC resource management strategy. Our concept enables to define NoC regions through decentralized reconfigurable resource management policies. It offers improved performance and communication resource allocation within the regions. The proposed concept is investigated through simulation of real world application scenarios. The simulation results highlight the performance benefit within the region and the increased probability for successful reservation of communication resources. Implementation results show the low area overhead of the proposed hardware support.	computation;manycore processor;network on a chip;overhead (computing);quality of service;requirement;scalability;simulation;system on a chip	Jan Heisswolf;Aurang Zaib;Andreas Weichslgartner;Ralf König;Thomas Wild;Jürgen Teich;Andreas Herkersdorf;Jürgen Becker	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.25	real-time computing;quality of service;resource allocation;computer science;resource management;distributed computing;computer network	HPC	-8.364124769213944	58.22602071459899	157091
152fcf049898e156be40ead29fabe13b8f32c37d	exploiting data forwarding to reduce the power budget of vliw embedded processors	vliw embedded processor;forwarding;vliw embedded architectures;exploiting data forwarding;low-power;pipeline processors;power budget;embedded processor;register file;critical path;embedded system;functional unit;power optimization;hazards;low power electronics;embedded systems;hardware;writing;power analysis;exception handling;radio frequency;cost function;vliw;registers	In this paper, a low-power approach to the design of embedded VLIW processor architectures is proposed. To solve the most part of data hazards in the pipeline, processors use forwarding (or bypassing) hardware to provide the required operands from the inter-stage pipeline registers directly to the inputs of the function units. The operands are then stored in the Register File during the write-back pipeline stage. In this paper, we propose a power optimization technique based on the exploitation of the forwarding paths in the processor to avoid the power cost of writing/reading short-lived variables to/from the Register File. In application-speci c embedded systems, experimental evidence has shown that a signi cant number of variables are short-lived, that is their liveness (from rst de nition to last use) spans only few instructions. Values of short-lived variables can be accessed directly through the forwarding registers, avoiding write-back. An application example of our solution to a VLIW embedded core, when accessing the Register File, has shown a power saving up to 35% with respect to the unoptimized approach on the given set of target benchmarks. The performance overhead is equal to one-gate delay to be added on the processor critical-path.	cache (computing);central processing unit;embedded system;hazard (computer architecture);liveness;low-power broadcasting;mathematical optimization;operand forwarding;overhead (computing);pipeline (computing);propagation delay;register file;very long instruction word	Mariagiovanna Sami;Donatella Sciuto;Cristina Silvano;Vittorio Zaccaria;Roberto Zafalon	2001			exception handling;embedded system;computer architecture;parallel computing;real-time computing;power analysis;hazard;computer science;very long instruction word;operating system;critical path method;processor register;writing;power optimization;radio frequency;register file;low-power electronics	Arch	-6.04660804782806	53.46894773245903	158255
9b50dea73a03dacd6ae612483a1ac55351182c43	impact of coprocessors on a multithreaded processor design using prioritized threads	multi threading coprocessors;multi threading;synchronization coprocessor multithreaded processor design prioritized threads processor utilization system performance simultaneous multithreading embedded devices;system performance;coprocessors;coprocessors process design yarn multithreading hardware surface mount technology system performance delay embedded system network servers;simultaneous multithreading;coarse grained;dynamic adaptation;embedded device	Recently, multithreading became a standard technique to improve the processor utilization and system performance. Hardware support is provided for coarse-grained as well as simultaneous multithreading. In particular, embedded devices combine processor cores and varying sets of coprocessors to fulfill the requirements of their dedicated application field. In this paper, a simultaneous multithreaded processor is investigated that applies dynamic priorities for each thread on the instruction level. By means of a synchronization coprocessor, priorities of threads are dynamically adapted when other threads have to wait for a given thread. Based on simulations of a network-processing workload, two strategies of dynamic priority adaptation are evaluated and compared with static prioritization. As a result, performance gain can be shown.	ace;benchmark (computing);central processing unit;coprocessor;embedded system;hardware-assisted virtualization;inter-process communication;multithreading (computer architecture);network processor;processor design;requirement;simulation;simultaneous multithreading;systemc;thread (computing);throughput	Carsten Albrecht;Andreas C. Döring;Frank Penczek;Torben Schneider;Hannes Schulz	2006	14th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP'06)	10.1109/PDP.2006.42	computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;distributed computing;computer performance;temporal multithreading;barrel processor;simultaneous multithreading;super-threading;coprocessor	EDA	-8.563027134916279	57.94170920603004	158313
9037f624a0e2a2ac950eb517bd0ecde7805c2b58	energy-efficient fixed-priority scheduling for real-time systems based on threshold work-demand analysis	energy conservation;energy saving performance energy efficient fixed priority scheduling real time system threshold work demand analysis energy consumption reduction static energy consumption dynamic energy consumption processor shut down strategy voltage scaling;energy efficient;processor scheduling;real time;look ahead;fixed priority;power aware computing;hard real time system;threshold work demand;energy consumption;demand analysis;schedules voltage control energy consumption real time systems power demand equations partial discharges;fixed priority scheduling;real time systems energy conservation energy consumption power aware computing processor scheduling;look ahead slack reclaiming fixed priority scheduling dvs threshold work demand;voltage scaling;lower bound;energy saving;look ahead slack reclaiming;real time systems;dvs	In this paper, we study the problem of reducing the energy consumption for hard real-time systems based on fixed-priority (FP) scheme. To balance the static and dynamic energy consumption, the concept of critical speed was proposed in previous research. Moreover, when combined with the processor shut-down strategy, the critical speed was widely used as the lower bound for voltage scaling in literature. In this paper, we show that this strategy might not always be more energy efficient than the traditional DVS strategy and there exits a tradeoff between these two strategies depending on the job work-demand to be finished within certain interval. To effectively address this issue, we propose a technique that combines these two strategies to achieve better energy saving performance. Our approach determines the energy efficient speeds for real-time jobs in their corresponding feasible intervals based on the threshold work - demand analysis. Our experimental results demonstrated that the proposed techniques significantly outperformed the previous research in overall energy saving performance.	dynamic voltage scaling;fixed-priority pre-emptive scheduling;image scaling;real-time clock;real-time computing;scheduling (computing)	Linwei Niu;Wei Li	2011	2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)	10.1145/2039370.2039397	embedded system;real-time computing;economics;operations management	EDA	-5.3867007393041195	58.65249602214236	158481
282a1bd71d3cb23e3b679f6d5db9cc4fbf5a6b7b	variability-aware task allocation for energy-efficient quality of service provisioning in embedded streaming multimedia applications	energy conservation;software;organization and design;bin packing;resource management multicore processing clocks multimedia communication power demand quality of service software;clocks;energy efficient;multimedia streaming;real time;resource manager;resource management;multimedia application;software engineering;embedded system;real time systems and embedded systems software software engineering operating systems organization and design;virtual prototyping;operating system;power variation;multicore processing;linear time;multimedia communication;quality of service bin packing energy conservation linear programming media streaming multiprocessing systems;real time predictability variability aware task allocation energy efficient quality of service provisioning embedded streaming multimedia application multimedia streaming application next generation parallel multiprocessor arrays process variability power variations energy efficiency runtime variability aware workload distribution bin packing linear programming;next generation;linear programming;media streaming;linear program;software software engineering;multiprocessing systems;quality of service;power demand;real time systems and embedded systems;task allocation;operating systems;real time systems	Multimedia streaming applications running on next-generation parallel multiprocessor arrays in sub-45 nm technology face new challenges related to device and process variability, leading to performance and power variations across the cores. In this context, Quality of Service (QoS), as well as energy efficiency, could be severely impacted by variability. In this work, we propose a runtime variability-aware workload distribution technique for enhancing real-time predictability and energy efficiency based on an innovative Linear-Programming + Bin-Packing formulation which can be solved in linear time. We demonstrate our approach on the virtual prototype of a next-generation industrial multicore platform running representative multimedia applications. Experimental results confirm that our technique compensates variability, while improving energy-efficiency and minimizing deadline violations in presence of performance and power variations across the cores. The proposed policy can save up to 33 percent of energy with respect to the state-of-the-art policies and 65 percent of energy with respect to one variability-unaware task allocation policy while providing better QoS.	algorithm;bin packing problem;computation;computer architecture simulator;embedded system;experiment;heart rate variability;linear programming;multi-core processor;multiprocessing;prototype;provisioning;quality of service;real-time clock;set packing;spatial variability;streaming media;time complexity;xstream library	Francesco Paterna;Andrea Acquaviva;Alberto Caprara;Francesco Papariello;Giuseppe Desoli;Luca Benini	2012	IEEE Transactions on Computers	10.1109/TC.2011.127	multi-core processor;time complexity;embedded system;bin packing problem;parallel computing;real-time computing;quality of service;energy conservation;computer science;linear programming;resource management;operating system;distributed computing;efficient energy use;computer network	Embedded	-5.780028705677791	59.05504363759983	158520
3f3699b7b3588a7b054e98f1ff617d3d042e2708	minimizing write activities to non-volatile memory via scheduling and recomputation	phase change materials;nonvolatile memory random access memory digital signal processing phase change materials read write memory flash memory embedded system scanning probe microscopy computer science phase change memory;nand flash memory;flash memory;digital signal processing;random access memory;high density;optimization technique;shock resistivity;nand flash memory write activities non volatile memory recomputation phase change memory magnetic random access memory embedded dsp system shock resistivity non volatility power economy write aware scheduling;recomputation;magnetic random access memory;embedded system;write aware scheduling;power economy;random access storage flash memories;optimal scheduling;embedded dsp system;nonvolatile memory;non volatile memory;phase change memory;ash;schedules;random access storage;write activities;computer science;read write memory;scanning probe microscopy;non volatility;flash memories	Non-volatile memories, such as flash memory, Phase Change Memory (PCM), and Magnetic Random Access Memory (MRAM), have many desirable characteristics for embedded DSP systems to employ them as main memory. These characteristics include low-cost, shock-resistivity, non-volatility, power-economy and high density. However, there are two common challenges we need to answer before we can apply non-volatile memory as main memory practically. First, non-volatile memory has limited write/erase cycles compared to DRAM. Second, a write operation is slower than a read operation on non-volatile memory. These two challenges can be answered by reducing the number of write activities on non-volatile main memory. In this paper, we propose two optimization techniques, write-aware scheduling and recomputation, to minimize write activities on non-volatile memory. With the proposed techniques, we can both speed up the completion time of programs and extend non-volatile memory's lifetime. The experimental results show that the proposed techniques can reduce the number of write activities on non-volatile memory by 55.71% on average. Thus, the lifetime of non-volatile memory is extend to 2.5 times as long as before on average. The completion time of programs can be reduced by 55.32% on systems with NOR flash memory and by 40.69% on systems with NAND flash memory on average.	computer data storage;dynamic random-access memory;embedded system;flash memory;magnetoresistive random-access memory;mathematical optimization;non-volatile memory;phase-change memory;random access;scheduling (computing);volatile memory;volatility	Jingtong Hu;Chun Jason Xue;Wei-Che Tseng;Qingfeng Zhuge;Edwin Hsing-Mean Sha	2010	2010 IEEE 8th Symposium on Application Specific Processors (SASP)	10.1109/SASP.2010.5521139	auxiliary memory;uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;sense amplifier;non-volatile memory;memory refresh;computer hardware;computer science;operating system;computer memory;overlay;extended memory;flat memory model;registered memory;cache-only memory architecture;memory map;memory management	EDA	-6.757522653548185	55.187463010846415	159089
10c1b08d576160a34920f19bf97540bd5313e9bb	"""a comment on """"an analytical model for designing memory hierarchies"""""""	analytical models;cache storage;optimal cache configuration;closed form solution;system software;optimal cache configuration analytical model memory hierarchies workload locality closed form solution;workload locality;analytical models jacobian matrices closed form solution research and development computer architecture laboratories system software;computer architecture;memory hierarchies;research and development;memory hierarchy;jacobian matrices;analytical model	"""In our paper, """"An analytical model for designing memory hierarchies"""" (see ibid., vol. 45, no. 10, p. 180-1, 194 (1996)), we made the following statement: """"Failing to apply a specific model of workload locality makes it impossible to provide an easily used, closed-form solution for the optimal cache configuration, and so the results from these papers have contained dependencies on the cache configuration-the number of levels, or the sizes and hit rates of the levels."""" Our description did not accurately reflect the contents of the paper by J.E. MacDonald and K.L. Sigworth (1975), and we regret any false impressions caused by the inaccuracy."""	memory hierarchy	Bruce Jacob;Peter M. Chen;Seth R. Silverman;Trevor N. Mudge	1997	IEEE Trans. Computers	10.1109/12.628401	embedded system;closed-form expression;cache-oblivious algorithm;parallel computing;cache coloring;cache;computer science;theoretical computer science;operating system;distributed computing;cache algorithms;algorithm	EDA	-8.514036011907198	54.765238935716354	159368
df2025e1e1e1e75887247d93fa3afff60d3d3b07	analysing the role of last level caches in controlling chip temperature		Dynamic Thermal Management (DTM) has become a major concern for the chip-designers, as it becomes a challenging task in recent power densed high performance Chip Multi-Processors (CMPs), due to integration of more on-chip components to meet ever increasing demand of processing power. The increased chip temperature incorporates severe circuit errors along with significant increment in leakage power consumption. Traditional DTM techniques apply DVFS or task migration to reduce core temperature, as cores are considered as the hottest on-chip components. Additionally, to commensurate high data demand of these high performance cores, large on-chip Last Level Caches (LLCs) are attached, which are the principal contributors to the on-chip leakage power consumption and occupy the largest on-chip area. As power consumption reduction plays the pivotal role in temperature reduction, hence, this work dynamically shrinks the cache size not only to reduce leakage power consumption, but also, to create on-chip thermal buffers for reducing average chip temperature by exploiting the heat transfer physics. Cache resizing decisions are taken based upon the generated cache hotspots and/or the access patterns, during process execution. Simulation results of the proposed thermal management method are compared with an existing DVFS based method (at cores) and a prior drowsy cache based technique to show its effectiveness.		Shounak Chakraborty;Hemangee K. Kapoor	2018	IEEE Transactions on Sustainable Computing	10.1109/TSUSC.2018.2823542	task analysis;chip;system on a chip;temperature control;real-time computing;heat transfer physics;cache;leakage (electronics);computer science;cpu cache	HPC	-5.0062232041795385	55.80734078411075	159885
d196457f0a51fdb9f19ce3b9b27e7dbfdd7795b5	bti mitigation by anti-ageing software patterns		This paper presents a time-redundant technique to mitigate Negative and Positive Bias Temperature Instability (NBTI/PBTI) ageing effects on the functional units of a processor. We have analysed the sources and effects of ageing from the device level to the Instruction Set Architecture (ISA) level, and have found that an application may stress the critical paths in such a way that the circuit has half of its nodes always NBTI-stressed. To mitigate this behaviour, we propose an application-level solution to balance the stress and put the timing-critical gates of the critical path into a relaxed (balanced) mode. The results show that the lifetime of the system can be doubled by applying balanced stress patterns at the software level during the idle time of a processor system.		Haider Muhi Abbas;Basel Halak;Mark Zwolinski	2017	Microelectronics Reliability	10.1016/j.microrel.2017.10.009	critical path method;reliability engineering;workload;electronic engineering;engineering;embedded system;software;instruction set;temperature instability;idle	EDA	-5.527246265926705	56.07340313670984	160540
1d851c8fdd77cedcb98e148b816f190afadbf315	energy reduction techniques for multimedia applications with tolerance to deadline misses	application software personal digital assistants energy consumption multimedia systems embedded computing power engineering computing power engineering and energy humans auditory system digital signal processing;offline on line minimum effort;digital signal processing;best effort approach;application s execution time;deadline misses tolerance;user level;application software;soft real time scheduling;auditory system;offline strategies;multimedia application;best effort;on line strategies;multimedia systems;soft real time;embedded system;personal digital assistants;power engineering and energy;completion ratio;embedded systems;dsp application energy reduction technique multimedia application deadline misses tolerance embedded systems power failures system s energy consumption quality of service user level completion ratio offline strategies on line strategies application s execution time best effort approach best effort energy minimization offline on line minimum effort benchmark task graphs;power engineering computing;energy consumption;best effort energy minimization;multimedia communication;dsp application;energy reduction technique;digital signal processing chips;humans;circuit cad;low power design;energy minimization;task graphs;quality of service;circuit optimisation;benchmark task graphs;voltage scaling;article;on line algorithm;system s energy consumption;power control multimedia communication circuit cad embedded systems digital signal processing chips quality of service circuit optimisation microprocessor chips;embedded computing;energy saving;power failures;microprocessor chips;power control	Many embedded systems such as PDAs require processing of the given applications with rigid power budget. However, they are able to tolerate occasional failures due to the imperfect human visual/auditory systems. The problem we address in this paper is how to utilize such tolerance to reduce multimedia system's energy consumption for providing guaranteed quality of service at the user level in terms of completion ratio. We explore a range of offline and on-line strategies that take this tolerance into account in conjunction with the modest non-determinism in application's execution time. First, we give a simple best-effort approach that achieves the maximum completion ratio; then we propose an enhanced on-line best-e.ort energy minimization (BEEM) approach and a hybrid offline/on-line minimum-effort (O2ME) approach. We prove that BEEM maintains the maximum completion ratio while consuming the provably least amount of energy and O2ME guarantees the required completion ratio statistically. We apply both approaches to a variety of benchmark task graphs, most from popular DSP applications. Simulation results show that significant energy savings (38% for BEEM and 54% for O2ME, both over the simple best-e.ort approach) can be achieved while meeting the required completion ratio requirements.	benchmark (computing);best-effort delivery;embedded system;energy minimization;online and offline;personal digital assistant;quality of service;requirement;run time (program lifecycle phase);simulation	Shaoxiong Hua;Gang Qu;Shuvra S. Bhattacharyya	2003		10.1145/775832.775868	best-effort delivery;embedded system;electronic engineering;application software;real-time computing;simulation;quality of service;power control;computer science;electrical engineering;operating system;digital signal processing;energy minimization;computer network	EDA	-4.858389645101392	58.86707391823507	160753
096ee9c89d43fd03a602aed3a37fdf43dc8e60ae	raidr: retention-aware intelligent dram refresh	dram chips;8-core system;raidr;bloom filters;dynamic random-access memory;memory controller;retention-aware intelligent dram refresh	Dynamic random-access memory (DRAM) is the building block of modern main memory systems. DRAM cells must be periodically refreshed to prevent loss of data. These refresh operations waste energy and degrade system performance by interfering with memory accesses. The negative effects of DRAM refresh increase as DRAM device capacity increases. Existing DRAM devices refresh all cells at a rate determined by the leakiest cell in the device. However, most DRAM cells can retain data for significantly longer. Therefore, many of these refreshes are unnecessary.  In this paper, we propose RAIDR (Retention-Aware Intelligent DRAM Refresh), a low-cost mechanism that can identify and skip unnecessary refreshes using knowledge of cell retention times. Our key idea is to group DRAM rows into retention time bins and apply a different refresh rate to each bin. As a result, rows containing leaky cells are refreshed as frequently as normal, while most rows are refreshed less frequently. RAIDR uses Bloom filters to efficiently implement retention time bins. RAIDR requires no modification to DRAM and minimal modification to the memory controller. In an 8-core system with 32 GB DRAM, RAIDR achieves a 74.6% refresh reduction, an average DRAM power reduction of 16.1%, and an average system performance improvement of 8.6% over existing systems, at a modest storage overhead of 1.25 KB in the memory controller. RAIDR's benefits are robust to variation in DRAM system configuration, and increase as memory capacity increases.	bloom filter;computer data storage;dynamic random-access memory;memory controller;overhead (computing);random access;refresh rate;system configuration	Jamie Liu;Ben Jaiyen;Richard Veras;Onur Mutlu	2012	2012 39th Annual International Symposium on Computer Architecture (ISCA)		parallel computing;real-time computing;memory rank;static random-access memory;cas latency;computer hardware;computer science;bloom filter;operating system;computer performance;memory controller;universal memory;particle detector	Arch	-9.302615106480522	54.30159438072426	160878
33a3888840fad791977e6660d2160cc03d0ec973	powersleep: a smart power-saving scheme with sleep for servers under response time constraint	energy conservation;voltage control;power saving;queuing model;queueing theory;servers time factors power demand voltage control queueing analysis delay;sleep m g 1 power saving response time;response time;sleep;powersleep scheme smart power saving scheme response time constraint server system design dynamic voltage scaling dvs static power consumption dynamic power management dpm sleep efficiency m g l ps queuing model;servers;time factors;queueing theory energy conservation power consumption;system design;time factor;power consumption;dynamic power management;power demand;m g 1;queueing analysis;time constraint	Reducing the power consumption while maintaining the response time constraint has been an important goal in server system design. One of the techniques widely explored in the literature to achieve this goal is dynamic voltage scaling (DVS). However, DVS is not efficient in modern systems where the overall power consumption includes a large portion of static power consumption. In this paper, we aim to reduce the static power consumption by dynamic power management (DPM) with sleep model in addition to DVS. To maximize the sleep efficiency, we propose PowerSleep, a smart power-saving scheme by carefully choosing an execution speed for the server with DVS and sleep periods while putting the system in the sleep power mode with DPM. By modeling the system with M/G/1/PS queuing model and further significant extensions, we present how to minimize the mean power consumption of the server under the given mean response time constraint. Simulation results show that our smart PowerSleep scheme significantly outperforms the simple power-saving scheme which adopts sleep mode.	dynamic voltage scaling;image scaling;power management;queueing theory;response time (technology);responsiveness;server (computing);simulation;sleep mode;systems design	Shengquan Wang;Jun Liu;Jian-Jia Chen;Xue Liu	2011	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2011.2167532	embedded system;real-time computing;energy conservation;computer hardware;computer science;operating system;sleep;queueing theory;response time;server;computer network;systems design	OS	-4.6860753127825845	58.80781278382345	161227
311f3cded19bf87ec77bd0fc21f807b429b180fa	dualstack: a high efficient dynamic page scheduling scheme in hybrid main memory		With the development of big data and multi-core processors technology, DRAM-only based main memory cannot satisfy the requirements of in-memory computing in high memory capacity and low energy consumption. The emerging memory technology-phase change memory (PCM) is proposed to break the bottleneck of the current memory system. However, its weaknesses in write endurance and long access latency make it cannot fully replace DRAM. Consequently, researchers presented the architectural design aimed at DRAM/ PCM hybrids and the corresponding page migration scheme to give full play to their merits. The urgent challenges facing by existed page migration schemes are poor performed under weak locality in data streams and further improvement need in prediction of future access tendency. In this paper, we propose an efficient page migration policy called DualStack which features dynamic page management according to global read and write information and temporal locality. It is designed to keep write-intensive pages to DRAM and read-intensive pages to PCM, and specially avoid frequent and unnecessary migration between hybrid memory media. Compared to the state-of-the-art of hybrid main memory, our experimental results indicate that DualStack can effectively improve the system I/O latency by 38%~58% on the premise of reducing the system power consumption by 20%~30%.	big data;central processing unit;computer data storage;dynamic random-access memory;high memory;ibm system i;in-memory database;in-memory processing;infinite impulse response;input/output;locality of reference;microsoft lumia;multi-core processor;phase-change memory;requirement;scheduling (computing);system migration	Zhen Zhang;Yinjin Fu;Guyu Hu	2017	2017 International Conference on Networking, Architecture, and Storage (NAS)	10.1109/NAS.2017.8026855	parallel computing;real-time computing;flat memory model;demand paging;registered memory;zero page;uniform memory access;high memory;computer science;interleaved memory;page fault	Arch	-10.450600102968794	54.26323805192496	162300
5cecf3d5af1252646bae0d61ed93ae83850ff783	symbolic analyses of dataflow graphs	buffer minimization;synchronous dataflow graphs;latency;static analysis;throughput	The synchronous dataflow model of computation is widely used to design embedded stream-processing applications under strict quality-of-service requirements (e.g., buffering size, throughput, input-output latency). The required analyses can either be performed at compile time (for design space exploration) or at runtime (for resource management and reconfigurable systems). However, these analyses have an exponential time complexity, which may cause a huge runtime overhead or make design space exploration unacceptably slow.  In this article, we argue that symbolic analyses are more appropriate since they express the system performance as a function of parameters (i.e., input and output rates, execution times). Such functions can be quickly evaluated for each different configuration or checked with respect to different quality-of-service requirements. We provide symbolic analyses for computing the maximal throughput of acyclic synchronous dataflow graphs, the minimum required buffers for which as soon as possible (ASAP) scheduling achieves this throughput, and finally, the corresponding input-output latency of the graph. The article first investigates these problems for a single parametric edge. The results are extended to general acyclic graphs using linear approximation techniques. We assess the proposed analyses experimentally on both synthetic and real benchmarks.	addendum;approximation algorithm;benchmark (computing);compile time;compiler;data dependency;data-flow analysis;dataflow;design space exploration;digital library;directed acyclic graph;embedded system;experiment;heuristic;input/output;iteration;line graph;linear approximation;maximal set;model of computation;overhead (computing);quality of service;requirement;run time (program lifecycle phase);scheduling (computing);stream processing;synthetic intelligence;throughput;time complexity;tree structure	Adnan Bouakaz;Pascal Fradet;Alain Girault	2017	ACM Trans. Design Autom. Electr. Syst.	10.1145/3007898	embedded system;throughput;latency;parallel computing;real-time computing;computer science;operating system;distributed computing;programming language;static analysis;algorithm	Embedded	-8.538372592722038	59.8350218663348	162306
7e98fa2f7e993202f3c3b94e6b0f2c5e2a66a40e	the management and optimization of queue state vector based on multi-core	multi core processor;queue state vector;parellelism;spin lock;packet forwarding	"""With the fast development of the network, the requirements for the processing speed of routers are greatly improved, which can be satisfied by the powerful parallelism pability of multi-core processors. In order to give full play to the advantages of parallelism of multi-core processors, queue state vectors (QSV) are used. Based on the application of queue state vectors, """"window"""" mode is proposed in this paper to manage the queue state vector. According to theoretical analysis and experimental results, such an optimization approach, can reduce the probability of conflict when multicore paralleling, and significantly improve the rate of packet forwarding."""	multi-core processor;program optimization;quantum state	Haitao Wu;Yakun Xu	2013		10.1109/PDCAT.2013.55	multi-core processor;double-ended queue;parallel computing;real-time computing;spinlock;multilevel queue;computer science;operating system;distributed computing;packet forwarding;queue management system;priority queue;computer network	EDA	-8.53956743375157	54.738345379401714	163016
96e2c82ccc15e629916e15de49868edce1248915	a reconfigurable real-time sdram controller for mixed time-criticality systems	protocols;reconfigurable architectures;hardware description languages;sdram time division multiplexing real time systems timing bandwidth servers switches;mixed time criticality systems reconfigurable real time sdram controller fpga vhdl tlm level systemc model active memory clients tdm slot allocations reconfiguration protocol reconfigurable time division multiplexing composable memory patterns composable service run time reconfigurable sdram controller composable virtual platforms predictable virtual platforms verification complexity resource sharing soc systems on chips real time requirements;system on chip;time division multiplexing;field programmable gate arrays;time division multiplexing dram chips field programmable gate arrays hardware description languages protocols real time systems reconfigurable architectures system on chip;dram chips;real time systems	Verifying real-time requirements of applications is increasingly complex on modern Systems-on-Chips (SoCs). More applications are integrated into one system due to power, area and cost constraints. Resource sharing makes their timing behavior interdependent, and as a result the verification complexity increases exponentially with the number of applications. Predictable and composable virtual platforms solve this problem by enabling verification in isolation, but designing SoC resources suitable to host such platforms is challenging.  This paper focuses on a reconfigurable SDRAM controller for predictable and composable virtual platforms. The main contributions are: 1) A run-time reconfigurable SDRAM controller architecture, which allows trade-offs between guaranteed bandwidth, response time and power. 2) A methodology for offering composable service to memory clients, by means of composable memory patterns. 3) A reconfigurable Time-Division Multiplexing (TDM) arbiter and an associated reconfiguration protocol. The TDM slot allocations can be changed at run time, while the predictable and composable performance guarantees offered to active memory clients are unaffected by the reconfiguration. The SDRAM controller has been implemented as a TLM-level SystemC model, and in synthesizable VHDL for use on an FPGA.	arbiter (electronics);best, worst and average case;composability;criticality matrix;elegant degradation;field-programmable gate array;interdependence;multiplexing;real-time clock;requirement;response time (technology);run time (program lifecycle phase);scheduling (computing);system on a chip;systemc;toad data modeler;vhdl;virtual machine	Sven Goossens;Jasper Kuijsten;Benny Akesson;Kees G. W. Goossens	2013	2013 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)	10.1109/CODES-ISSS.2013.6658989	embedded system;parallel computing;real-time computing;computer science	EDA	-8.271384753767997	58.692760624027564	163971
e55f4d5bce4e39337c2f4128e3db732f2fc84982	per-thread cycle accounting	quality of service improvement;multi threading;system software;resource allocation;performance;resource management;perthread progress rate;smt processors;performance predictability;proportional share performance;system buses;surface mount technology computer architecture quality of service system software multithreading resource management multicore processing microprocessors application software;computer architecture;multicore;technology and engineering;multithreaded architecture;resource sharing;perthread cycle accounting;multithreaded architectures;service differentiation;per thread cycle accounting;proportional share performance perthread cycle accounting resource sharing perthread performance multithreaded architecture system software perthread progress rate quality of service improvement service level agreement performance predictability service differentiation;resource allocation computer architecture multiprocessing systems multi threading;performance prediction;service level agreement;multiprocessing systems;quality of service;perthread performance;per thread cycle accounting multicore multithreaded architectures system software	Resource sharing unpredictably affects per-thread performance in multithreaded architectures, but system software assumes all coexecuting threads make equal progress. Per-thread cycle accounting addresses this problem by tracking per-thread progress rates for each coexecuting thread. This approach has the potential to improve Quality Of Service (QoS), Service-Level Agreements (SLA), performance predictability, service differentiation, and proportional-share performance on multithreaded architectures.	quality of service;service-level agreement;thread (computing)	Stijn Eyerman;Lieven Eeckhout	2010	IEEE Micro	10.1109/MM.2010.23	multi-core processor;shared resource;computer architecture;parallel computing;real-time computing;multithreading;quality of service;performance;resource allocation;computer science;resource management;operating system	Arch	-9.953870221966454	58.35869313220905	164691
370a605eb294e893b0e1a50c86c5546057818ec8	case study on co-scheduling for hpc applications	energy efficiency;processor scheduling parallel machines power aware computing;co scheduling;radiation detectors;autopin tool coscheduling hpc applications supercomputers energy efficiency memory bandwidth bound application compute bound application;hpc;energy measurement;energy consumption;bandwidth;radiation detectors hardware bandwidth energy measurement instruction sets power measurement energy consumption;job scheduling hpc co scheduling energy efficiency;job scheduling;power measurement;instruction sets;hardware	Most applications running on supercomputers achieve only a fraction of the peak performance of the system. In this paper we analyze the performance and energy efficiency of co-scheduling one memory bandwidth bound and one compute bound application on the same node. We present auto pin+, a tool designed to monitor and optimize co-scheduling of applications. Our analysis shows that co-scheduling can improve both energy efficiency and overall throughput of a supercomputer. At best, runtime can be decreased by 28% and the energy consumption by 12%, respectively, compared to best case dedicated execution. The overall efficiency however strongly depends on the ratio of jobs available in the queue. We furthermore present a simple adaptive strategy depending on the available jobs in the queue.	algorithm;best, worst and average case;central processing unit;job stream;memory bandwidth;multi-core processor;scheduling (computing);supercomputer;throughput;usability	Jens Breitbart;Josef Weidendorfer;Carsten Trinitis	2015	2015 44th International Conference on Parallel Processing Workshops	10.1109/ICPPW.2015.38	embedded system;supercomputer;parallel computing;real-time computing;computer science;job scheduler;operating system;instruction set;efficient energy use;particle detector;bandwidth	HPC	-5.341530881217891	54.80403992525778	164839
d7db2f130f6130599a4163ae2e14cc121a4425bb	dataflow analysis for multiprocessor systems with non-starvation-free schedulers	dataflow;scheduling;buffer capacity;throughput	Dataflow analysis techniques are suitable for the temporal analysis of real-time stream processing applications. However, the applicability of these models is currently limited to systems with starvation-free schedulers, such as Time-Division Multiplexing (TDM) schedulers. Removal of this limitation would broaden the application domain of dataflow analysis techniques significantly.  In this paper we present a temporal analysis technique for Homogeneous Synchronous Dataflow (HSDF) graphs, that is also applicable for systems with non-starvation-free schedulers. Unlike existing dataflow analysis techniques, the proposed analysis technique makes use of an enabling-jitter characterization and iterative fixed-point computation.  The presented approach is applicable for arbitrary (cyclic) graph topologies. Buffer capacity constraints are taken into account during the analysis and sufficient buffer capacities can be determined afterwards. The approach presented in this paper is the first approach that considers non-starvation-free schedulers in combination with arbitrary HSDF graphs  The proposed dataflow analysis technique is implemented in a tool. This tool is used to evaluate the analysis technique using examples that illustrate some important differences with other temporal analysis methods. The case-study discusses how the method presented in this paper can be used to solve a problem with the inaccuracy of the temporal analysis results of a real-time stream processing system. This stream processing system consists of an FM receiver together with a DAB receiver application which both share a Digital Signal Processor (DSP).	application domain;computation;data-flow analysis;dataflow;digital signal processor;fm broadcasting;iterative method;multiplexing;multiprocessing;polynomial;real-time clock;stream processing;time complexity;topological graph theory	Joost P. H. M. Hausmans;Maarten Wiggers;Stefan J. Geuns;Marco Bekooij	2013		10.1145/2463596.2463603	dataflow architecture;embedded system;throughput;parallel computing;real-time computing;computer science;operating system;dataflow;signal programming;distributed computing;programming language;scheduling	Embedded	-8.69040887409246	59.83180351982725	165396
60bf7ef43129930282131c95125e975cd5ee9a1b	enhancing the energy efficiency of journaling file system via exploiting multi-write modes on mlc nvram		Non-volatile random-access memory (NVRAM) is regarded as a great alternative storage medium owing to its attractive features, including low idle energy consumption, byte addressability, and short read/write latency. In addition, multi-level-cell (MLC) NVRAM has also been proposed to provide higher bit density. However, MLC NVRAM has lower energy efficiency and longer write latency when compared with single-level-cell (SLC) NVRAM. These drawbacks could lead to higher energy consumption of MLC NVRAM-based storage systems. The energy consumption is magnified by existing journaling file systems (JFS) on MLC NVRAM-based storage devices due to the JFS's fail-safe policy of writing the same data twice. Such observations motivate us to propose a multi-write-mode journaling file systems (mwJFS) to alleviate the drawbacks of MLC NVRAM and lower the energy consumption of MLC NVRAM-based JFS. The proposed mwJFS differentiates the data retention requirement of journaled data and applies different write modes to enhance the energy efficiency with better access performance. A series of experiments was conducted to demonstrate the capability of mwJFS on a MLC NVRAM-based storage system.	areal density (computer storage);byte;cell (microprocessor);computer data storage;experiment;fail-safe;multi-level cell;non-volatile random-access memory;overhead (computing);random access;systems design	Shuo-Han Chen;Yuan-Hao Chang;Tseng-Yi Chen;Yu-Ming Chang;Pei-Wen Hsiao;Hsin-Wen Wei;Wei-Kuan Shih	2018		10.1145/3218603.3218632	journaling file system;latency (engineering);parallel computing;real-time computing;byte;computer science;energy consumption;non-volatile random-access memory;addressability;computer data storage;idle	OS	-9.979969060186345	54.508861292387415	166016
661cee660b61cd8b4971e43dcd4416da32f59b4f	lvtppp: live-time protected pseudopartitioning of multicore shared caches	cache storage;simics lvtppp live time protected pseudopartitioning multicore shared caches partition enforcement policy cache partition online protection generation time cpu core id live time protected counter dead blocks access event sequence two cascade victim selection mechanism lru replacement policy lvtp counter pipp ucp;dead block radiation detectors resource management multicore processing partitioning algorithms history pollution monitoring shared last level cache llc cache memories cache partition;history;radiation detectors resource management multicore processing partitioning algorithms history pollution monitoring;radiation detectors;resource management;cache partition;multiprocessing systems cache storage;monitoring;multicore processing;dead block;multiprocessing systems;shared last level cache llc;partitioning algorithms;cache memories;pollution	Partition enforcement policy is essential in the cache partition, and its main function is to protect the lines and retain the cache quota of each core. This paper focuses online protection based on its generation time rather than the CPU core ID that it belongs to or the position of the replacement stack, where it is located. The basic idea is that when a line is live, it must be protected and retained in the cache; when the line is “dead,” it needs to be evicted as early as possible. Therefore, the live-time protected counter (LvtP, four bits) is augmented to trace the lines' live time. Moreover, dead blocks are predicted according to the access event sequence. This paper presents a pseudopartition approach-LvtPPP and proposes a two-cascade victim selection mechanism to alleviate dead blocks based on the LRU replacement policy and the LvtP counter. LvtPPP also supports flexible handling of allocation deviation by introducing a parameter λ to adjust the generation time of the line. There is significant improvement of the performance and fairness in LvtPPP over PIPP and UCP according to the evaluation results based on Simics.	multi-core processor	Zhibin Huang;Mingfa Zhu;Limin Xiao	2013	IEEE Trans. Parallel Distrib. Syst.	10.1109/TPDS.2012.230	multi-core processor;parallel computing;real-time computing;pollution;computer science;resource management;operating system;distributed computing;particle detector	Arch	-10.263368467433862	56.56042444845814	166076
fac64233d5dfb642c6174165a8104c7dca8d4517	vlibos: babysitting os evolution with a virtualized library os		Many applications have service requirements that are not easily met by existing operating systems. Real-time and security-critical tasks, for example, often require custom OSes to meet their needs. However, development of special purpose OSes is a time-consuming and difficult exercise. Drivers, libraries and applications have to be written from scratch or ported from existing sources. Many researchers have tackled this problem by developing ways to extend existing systems with application-specific services. However, it is often difficult to ensure an adequate degree of separation between legacy and new services, especially when security and timing requirements are at stake. Virtualization, for example, supports logical isolation of separate guest services, but suffers from inadequate temporal isolation of time-critical code required for real-time systems. This paper presents vLibOS, a master-slave paradigm for new systems, whose services are built on legacy code that is temporally and spatially isolated in separate VM domains. Existing OSes are treated as sandboxed libraries, providing legacy services that are requested by inter-VM calls, which execute with the time budget of the caller. We evaluate a realtime implementation of vLibOS. Empirical results show that vLibOS achieves as much as a 50% reduction in performance slowdown for real-time threads, when competing for a shared memory bus with a Linux VM.	application programming interface;cloud computing;device driver;experiment;hypervisor;legacy code;legacy system;library (computing);linux;memory bus;operating system;programming paradigm;prototype;quality of service;random-access memory;real-time clock;real-time computing;real-time transcription;requirement;sandbox (computer security);scheduling (computing);shared memory;six degrees of separation;systems design;temporal isolation;virtual machine;window of opportunity	Ying Ye;Zhuoqun Cheng;Soham Sinha;Richard West	2017	CoRR		virtualization;distributed computing;porting;temporal isolation;computer science;scratch;operating system;six degrees of separation;thread (computing);legacy code;shared memory	OS	-9.209854660999897	57.62435021162398	166312
9230181e1aa9cb803e95993cd41e6cd7ba766772	effective and efficient scheduling of certifiable mixed-criticality sporadic task systems	polynomial certifiable mixed criticality sporadic task system scheduling embedded system design shared hardware platform mixed criticality system real time scheduling priority list reuse scheduling fixed job priority scheduling run time complexity;computer engineering;confidence level;sporadic task system cyber pysical system real time scheduling mixed criticality;time complexity;indexes schedules real time systems complexity theory processor scheduling polynomials vectors;critical level;power efficiency;cyber pysical system;polynomials;priority scheduling;embedded systems;critical system;embedded system design;computational complexity;scheduling;real time scheduling;datorteknik;mixed criticality;scheduling computational complexity embedded systems polynomials;sporadic task system	An increasing trend in embedded system design is to integrate components with different levels of criticality into a shared hardware platform for better cost and power efficiency. Such mixed-criticality systems are subject to certifications at different levels of rigorousness, for validating the correctness of different subsystems on various confidence levels. The real-time scheduling of certifiable mixed-criticality systems has been recognized to be a challenging problem, where using traditional scheduling techniques may result in unacceptable resource waste. In this paper we present an algorithm called PLRS to schedule certifiable mixed-criticality sporadic tasks systems. PLRS uses fixed-job-priority scheduling, and assigns job priorities by exploring and balancing the asymmetric effects between the workload on different criticality levels. Comparing with the state-of-the-art algorithm by Li and Baruah for such systems, which we refer to as LB, PLRS is both more effective and more efficient: (i) The schedulability test of PLRS not only theoretically dominates, but also on average significantly outperforms LB's. (ii) The run-time complexity of PLRS is polynomial (quadratic in the number of tasks), which is much more efficient than the pseudo-polynomial run-time complexity of LB.	algorithm;best, worst and average case;correctness (computer science);criticality matrix;cyber-physical system;embedded system;interference (communication);job stream;lattice boltzmann methods;mixed criticality;multi-core processor;multiprocessing;multiprocessor scheduling;online and offline;overhead (computing);performance per watt;polynomial;real-time clock;real-time transcription;regular expression;resource contention;run time (program lifecycle phase);schedule (project management);scheduling (computing);self-organized criticality;systems design;time complexity;uniprocessor system;worst-case scenario	Nan Guan;Pontus Ekberg;Martin Stigge;Wang Yi	2011	2011 IEEE 32nd Real-Time Systems Symposium	10.1109/RTSS.2011.10	time complexity;embedded system;parallel computing;real-time computing;confidence interval;electrical efficiency;computer science;operating system;distributed computing;computational complexity theory;scheduling;algorithm;polynomial	Embedded	-8.824413405317394	59.87101825904823	166423
205bbbeec22b45c5be967f099bd4928d147b2721	design and implementation of correlating caches	cache storage;heart;leakage current;history;performance;network processor;data mining;process design;memory access;computer architecture;low power;design and implementation;energy consumption;memory architecture;application specific integrated circuits;low power electronics;data access;design;frequency;buildings	We introduce a new cache architecture that can be used to increase performance and reduce energy consumption in Network Processors. This new architecture is based on the observation that there is a strong correlation between different memory accesses. In other words, if load X and load Y are two consecutively executed load instructions, the offset between the source addresses of these instructions remain usually constant between different iterations. We utilize this information by building a correlating cache architecture. This architecture consists of a Dynamic Correlation Extractor, a Correlation History Table, and a Correlation Buffer. We first show simulation results investigating the frequency of correlating loads. Then, we evaluate our architecture using SimpleScalar/ARM. For a set of representative applications, the correlating cache architecture is able to reduce the average data access time by as much as 52.7% and 36.1% on average, while reducing the energy consumption of the caches by as much as 49.2% and 25.7% on average.	arm architecture;access time;cpu cache;cache (computing);data access;iteration;microprocessor;network processor;randomness extractor;simulation	Arindam Mallik;Matthew C. Wildrick;Gokhan Memik	2004	Proceedings of the 2004 International Symposium on Low Power Electronics and Design (IEEE Cat. No.04TH8758)	10.1145/1013235.1013255	data access;process design;embedded system;design;electronic engineering;parallel computing;real-time computing;performance;computer science;operating system;frequency;heart;network processor	Arch	-6.668235867987185	53.74799956579357	167286
35e2b40cc25ff925f0a7b61fbd68a5c659b9bdf0	uncertainty-based scheduling: energy-efficient ordering for tasks with variable execution time [proc	processor architecture;low energy;energy efficient;real time;dynamic voltage scaling;low complexity;hard real time system;energy consumption;digital systems;real time scheduling;datavetenskap datalogi;run time power management;energy management	Energy consumption reduction is today an important design issue for all kinds of digital systems. Offering both flexibility and efficient energy management, variable speed processor architectures are prefered for low energy consumption even in hard real-time systems. For this type of systems, the main approach consists in trading speed for lower energy while meeting all deadlines. For tasks with varying execution time, speed scheduling is most efficient if performed at run-time.This paper presents a new ordering technique for such tasks, that reduces the energy consumption resulting from the run-time speed scheduling. Without affecting the real-time behavior, our Uncertainty-Based Scheduling (ubs) is a low complexity but energy-efficient method that can be applied on top of already existent real-time scheduling techniques, such as edf. These claims are backed up by extensive simulation results accompanied by measurements on a platform based on an Intel i80200 XScale processor.	backup;digital electronics;real-time clock;real-time computing;real-time operating system;real-time transcription;run time (program lifecycle phase);scheduling (computing);simulation;xscale	Flavius Gruian;Krzysztof Kuchcinski	2003		10.1145/871506.871621	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;dynamic priority scheduling;microarchitecture;computer science;operating system;two-level scheduling;efficient energy use;least slack time scheduling;energy management	Embedded	-5.274355139455384	58.659480561236286	168071
1e9f4817c9a86d9c76707586d82f565c0a0ac093	scheduling predictability in i-devs by schedulability analysis	schedulability analysis;imprecise devs;real time systems	The Imprecise DEVS approach combines the advantages of imprecise computation with a formal modeling methodology in order to avoid the transient overloads on real-time systems. This process requires efficient scheduling methods to find the best schedule of the computations to guarantee the deadlines and to increase the quality of the results by reducing the discarding of the optional computations. This work introduces a solution to integrate schedulability analysis with Imprecise DEVS to improve the predictability and the feasibility for scheduling. The proposed schedulability analysis and scheduling methods are based on Earliest Deadline First, priority-driven and mandatory-first approaches and they are considered to be executed dynamically. The schedulability tests contribute to avoid unnecessary discarding of optional computations improving the quality of the results.	algorithm;computation;devs;deadlock;earliest deadline first scheduling;mutual exclusion;priority inversion;real-time clock;real-time computing;scheduling (computing);scheduling analysis real-time systems;worst-case execution time	Braulio Adriano de Mello;Gabriel A. Wainer	2016	2016 Symposium on Theory of Modeling and Simulation (TMS-DEVS)		parallel computing;real-time computing;earliest deadline first scheduling;computer science;distributed computing	Embedded	-9.231113901465422	60.26520590109966	168246
0db5b04ca3615b5bd551b994616388bb71e65c81	a survey of system level power management schemes in the dark-silicon era for many-core architectures		Power consumption in Complementary Metal Oxide Semiconductor (CMOS) technology has escalated to a point that only a fractional part of many-core chips can be powered-on at a time. Fortunately, this fraction can be increased at the expense of performance through the dark-silicon solution. However, with manycore integration set to be heading towards its thousands, power consumption and temperature increases per time, meaning the number of active nodes must be reduced drastically. Therefore, optimized techniques are demanded for continuous advancement in technology. Existing efforts try to overcome this challenge by activating nodes from different parts of the chip at the expense of communication latency. Other efforts on the other hand employ run-time power management techniques to manage the power performance of the cores trading-off performance for power. We found out that, for a significant amount of power to saved and high temperature to be avoided, focus should be on reducing the power consumption of all the on-chip components. Especially, the memory hierarchy and the interconnect. Power consumption can be minimized by, reducing the size of high leakage power dissipating elements, turning-off idle resources and integrating power saving materials. Received on 10 August 2018; accepted on 03 September 2018; published on 19 August 2018	algorithm;cmos;computer architecture;course (navigation);enterprise application integration;focal (programming language);manycore processor;mathematical optimization;memory hierarchy;multi-core processor;network on a chip;node (computer science);power management;semiconductor;software design;spectral leakage;static random-access memory;thermal profiling;uncore;while	Emmanuel Ofori-Attah;Xiaohang Wang;Michael Opoku Agyeman	2018	EAI Endorsed Trans. Indust. Netw. & Intellig. Syst.	10.4108/eai.19-9-2018.155569	real-time computing;latency (engineering);chip;power management;dark silicon;memory hierarchy;leakage (electronics);idle;computer science;cmos	EDA	-5.836283944235065	56.191584385789405	168496
1ae3f4cdaaf12ddc6f7bf1a24588af58c54e7930	performance directed energy management for main memory and disks	control autoajustable;adaptation algorithms;gestion energia;performance guarantee;economies d energie;gestion memoire;selftuning control;dispositivo potencia;control algorithm;algorithm performance;ahorros energia;storage management;dispositif puissance;chip;optimization problem;adaptive algorithm;gestion memoria;memory and disk energy management;gestion energie disque;low power;gestion energie;resultado algoritmo;performance algorithme;puissance faible;multiple power mode device;power device;energy savings;low power design;commande autoajustable;control algorithms;energy saving;energy management;potencia debil	Much research has been conducted on energy management for memory and disks. Most studies use control algorithms that dynamically transition devices to low power modes after they are idle for a certain threshold period of time. The control algorithms used in the past have two major limitations. First, they require painstaking, application-dependent manual tuning of their thresholds to achieve energy savings without significantly degrading performance. Second, they do not provide performance guarantees. In one case, they slowed down an application by 835.This paper addresses these two limitations for both memory and disks, making memory/disk energy-saving schemes practical enough to use in real systems. Specifically, we make three contributions: (1) We propose a technique that provides a performance guarantee for control algorithms. We show that our method works well for all tested cases, even with previously proposed algorithms that are not performance-aware. (2) We propose a new control algorithm, Performance-directed Dynamic (PD), that dynamically adjusts its thresholds periodically, based on available slack and recent workload characteristics. For memory, PD consumes the least energy, when compared to previous hand-tuned algorithms combined with a performance guarantee. However, for disks, PD is too complex and its self-tuning is unable to beat previous hand-tuned algorithms. (3) To improve on PD, we propose a simple, optimization-based, threshold-free control algorithm, Performance-directed Static (PS). PS periodically assigns a static configuration by solving an optimization problem that incorporates information about the available slack and recent traffic variability to different chips/disks. We find that PS is the best or close to the best across all performanceguaranteed disk algorithms, including hand-tuned versions.	algorithm;computer data storage;mathematical optimization;optimization problem;self-tuning;slack variable;spatial variability	Xiaodong Li;Zhenmin Li;Francis M. David;Pin Zhou;Yuanyuan Zhou;Sarita V. Adve;Sanjeev Kumar	2004		10.1145/1024393.1024425	chip;optimization problem;real-time computing;simulation;telecommunications;computer science;energy management	Arch	-4.834899515481899	58.89279055032535	168800
538591cc456a205d3d21e8fe9b24225db1036648	adaptive replacement policy for hybrid cache architecture	low power;non volatile memory;cache replacement policy;stt ram	Researchers have proposed several schemes with hybrid cache architecture (HCA) which contains both SRAM cells and non-volatile memory (NVM) cells to overcome the drawbacks of NVM. The existing HCA schemes try to place a write intensive block into an SRAM way when a cache miss occurs. The cache replacement policy increases the write counts of NVM when the number of write intensive blocks are small. To solve this problem, we propose an adaptive replacement policy for hybrid cache architecture to adjust the cache replacement policy based on the write intensity for victim selection. The simulation results show that the proposed mechanism reduces the write counts of NVM by 21.2% on average compared that conventional replacement policy is used.	cpu cache;hierarchical clustering;infiniband;non-volatile memory;simulation;static random-access memory;volatile memory	Ju Hee Choi;Gi-Ho Park	2014	IEICE Electronic Express	10.1587/elex.11.20140946	pipeline burst cache;spin-transfer torque;computer architecture;parallel computing;real-time computing;cache coloring;non-volatile memory;cpu cache;tag ram;computer science;smart cache;cache pollution;quantum mechanics	Arch	-7.737557934633303	54.88829955421929	169338
4d55a2171c79de42576347e14c734a2f6cc27457	energy minimization of real-time tasks on variable voltage processors with transition energy overhead	minimisation;processor scheduling;transition energy overhead energy consumption minimization real time tasks variable voltage processors;real time;minimisation real time systems low power electronics microprocessor chips processor scheduling timing;voltage timing energy consumption processor scheduling energy management power system management control systems computer science power engineering and energy multiprocessing systems;energy consumption;low power electronics;energy minimization;microprocessor chips;real time systems;timing	In this paper, we address the problem of minimizing energy consumption of real-time tasks on variable voltage processors whose transition energy overhead is not negligible. Voltage settings with minimum number of transitions are found first and sequences of lower voltage cycles are evaluated to decide voltage for each cycle of every task. Experimental results demonstrate that our approach can reduce energy consumed by transitions from 41% to 8% and save more energy.	central processing unit;energy minimization;overhead (computing);real-time clock	Yumin Zhang;Xiaobo Sharon Hu;Danny Ziyi Chen	2003		10.1145/1119772.1119786	embedded system;minimisation;parallel computing;real-time computing;computer science;engineering;energy minimization;low-power electronics	EDA	-5.1558557615279375	58.511157853722565	169420
e17d6b657df653880cf45aa1b349340ffacc5c4d	reliability enhancement of flash-memory storage systems: an efficient version-based design	flash memory;reliability engineering;reliability;queue flash memory ecc poisson distribution binomial distribution file system mlc yaffs forward copying recovery reliability two version;error correction codes;yaffs;storage management;ecc;mlc;queue;recovery;forward copying;computer architecture;file system;storage management error correction codes flash memory reliability engineering computer architecture queueing analysis;management overheads reliability enhancement flash memory storage systems version based design advanced flash memory chips page versions native file system version maintenance corrupted file multiple data versions multilevel cell flash memory space overheads;binomial distribution;reliability flash memories poisson distribution;two version;flash memories;queueing analysis;poisson distribution	In recent years, reliability has become one critical issue in the designs of flash-memory file/storage systems, due to the growing unreliability of advanced flash-memory chips. In this paper, a version-based design is proposed to effectively and efficiently maintain the consistency among page versions of a file for potential recovery needs. In particular, a two-version one for a native file system is presented with the minimal overheads in version maintenance. A recovery scheme is then presented to restore a corrupted file back to the latest consistent version. The design is later extended to maintain multiple data versions with the considerations of the write constraints of multilevel-cell flash memory. It was shown that the proposed design could significantly improve the reliability of flash memory with limited management and space overheads.	code page 437;data integrity;data redundancy;experiment;flash file system;flash memory;flash memory controller;replication (computing);serializability;space–time tradeoff	Yuan-Hao Chang;Po-Chun Huang;Pei-Han Hsu;Lue-Jane Lee;Tei-Wei Kuo;David Hung-Chang Du	2013	IEEE Transactions on Computers	10.1109/TC.2012.131	embedded system;parallel computing;recovery;computer hardware;computer science;binomial distribution;operating system;reliability;poisson distribution;queue;statistics	Embedded	-11.629176066298857	54.99615216470523	170052
7657d19422275757a59c7762a5f320cf1961cf21	hotspot-aware task-resource co-allocation for heterogeneous many-core networks-on-chip		Abstract To fully exploit the massive parallelism of many-core on a chip, this work tackles the problem of mapping large-scale applications onto heterogeneous networks-on-chip (NoCs) while minimizing hotspots. A task-resource co-optimization framework is proposed which configures the on-chip communication infrastructure and maps the applications simultaneously and coherently, aiming to minimize the peak energy under the constraints of computation power, communication capacity, and total cost budget of on-chip resources. The problem is first formulated into a linear programming model to search for optimal solution. A heuristic is further developed for fast design space exploration at design-time and run-time in large-scale NoCs. Extensive simulations are carried out under real-world benchmarks and randomly generated task graphs to demonstrate the effectiveness and efficiency of the proposed schemes. Real system simulations show the significant improvement (30–200%) in NoCs latency and throughput compared to the state-of-the-art minimum-path approach because of the diminishing hotspots and balanced load distribution.	java hotspot virtual machine;manycore processor;system on a chip	Md Farhadur Reza;Dan Zhao;Hongyi Wu;Magdy A. Bayoumi	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2018.04.019	massively parallel;chip;throughput;latency (engineering);real-time computing;design space exploration;heuristic;computer science;linear programming;exploit	AI	-5.517944071517159	57.42418106619194	170292
3274d811ddfac787486501ffda7a5a954fc9ac05	energy-aware erasure codes using xor reference matrix for ssd based raid systems	adaptive chunking;decoding;raid;encoding frequency modulation decoding throughput arrays dynamic scheduling educational institutions;exrm erasure codes;energy cost xor reference matrix rules energy aware xrm erasure codes ssd based raid storage systems cpu cycle reduction data encoding data decoding adaptive chunking strategy throughput;forward error correction;ssd;ssd adaptive chunking exrm erasure codes;raid decoding encoding forward error correction;encoding	This paper proposes an energy-aware XRM erasure codes for SSD based RAID storage systems. It removes many small random read and write operations and reduces the number of CPU cycles for encoding and decoding data. For doing this, we use adaptive chunking strategy and XRM(XOR Reference Matrix) rules. The preliminary experimental results show that the proposed method has better performance than existing method using RS code in terms of throughput and energy cost.	central processing unit;customer relationship management;erasure code;exclusive or;instruction cycle;raid;random access;reed–solomon error correction;shallow parsing;solid-state drive;throughput	Mehdi Pirahandeh;Deok-Hwan Kim	2014	2014 International Conference on Big Data and Smart Computing (BIGCOMP)	10.1109/BIGCOMP.2014.6741420	parallel computing;real-time computing;computer science;theoretical computer science;standard raid levels;raid processing unit	HPC	-10.57099667861524	55.09402514536574	171227
3f6b607f554038ddf73bbe6b581e2349f312ff4c	an energy-efficient 3d cmp design with fine-grained voltage scaling	voltage control;random access memory;cache hierarchy;energy efficient;fine grained voltage scaling;adaptive control;adaptive control energy efficient 3d cmp design fine grained voltage scaling dvfs dynamic energy consumption cache hierarchy leakage energy reduction on chip voltage regulators;system on a chip;three dimensional;chip;system on a chip voltage control three dimensional displays random access memory power demand computer architecture benchmark testing;computer architecture;power aware computing;leakage energy reduction;dynamic energy consumption;energy consumption;three dimensional displays;voltage regulator;dvfs;energy efficient 3d cmp design;on chip voltage regulators;voltage scaling;power demand;benchmark testing;microprocessor chips;power aware computing microprocessor chips	In this paper, we propose an energy-efficient 3D-stacked CMP design by both temporally and spatially finegrained tuning of processor cores and caches. In particular, temporally fine-grained DVFS is employed by each core and L2 cache to reduce the dynamic energy consumption, while spatially fine-grained DVS is applied to the cache hierarchy for the leakage energy reduction. Our tuning technique is implemented by integrating an array of on-chip voltage regulators into the original processor. Experimental results show that the proposed design can provide an energy-efficient, direct, and adaptive control to the system, leading to 20% dynamic and 89% leakage energy reductions, and an average of 34% total energy saving compared to the baseline design.	array data structure;baseline (configuration management);best, worst and average case;cpu cache;dynamic voltage scaling;image scaling;overhead (computing);spectral leakage;staggered tuning;system dynamics;temporal logic;thread (computing);voltage regulator	Jishen Zhao;Xiangyu Dong;Yuan Xie	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763278	chip;system on a chip;embedded system;three-dimensional space;benchmark;voltage regulator;parallel computing;real-time computing;adaptive control;computer science;engineering;efficient energy use	EDA	-4.920971406854174	56.056223916901736	171393
93236a630610afa5d6872e273d6dcb0db7c9b021	schedulability bound for integrated modular avionics partitions	avionics;job shop scheduling;sequential analysis;aerospace industry;stability;real time systems;linear programming;schedules;scheduling;clock domain crossing;observability;power optimization;power analysis	In the avionics industry, as a hierarchical scheduling architecture Integrated Modular Avionics System has been widely adopted for its isolating capability. In practice, in an early development phase, a system developer does not know much about task execution times, but only task periods and IMA partition information. In such a case the schedulability bound for a task in a given partition tells a developer how much of the execution time the task can have to be schedulable. Once the developer knows the bound, then the developer can deal with any combination of execution times under the bound, which is safe in terms of schedulability. We formulate the problem as linear programming that is commonly used in the avionics industry for schedulability analysis, and compare the bound with other existing ones which are obtained with no period information.	integrated modular avionics;linear programming;maximal set;run time (program lifecycle phase);scheduling (computing);scheduling analysis real-time systems	Jung-Eun Kim;Tarek F. Abdelzaher;Lui Sha	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;parallel computing;real-time computing;observability;power analysis;stability;clock domain crossing;engineering;sequential analysis;operating system;power optimization;statistics	Embedded	-8.282088142651737	59.860767721312016	172076
3118fe74af5f47220fdbff1ae556adbebc83ffff	mixed-criticality scheduling on cluster-based manycores with shared communication and storage resources	multi core many core systems;resource contention;shared memory;mixed criticality scheduling;noc	The embedded system industry is facing an increasing pressure for migrating from single-core to multi- and many-core platforms for size, performance and cost purposes. Real-time embedded system design follows this trend by integrating multiple applications with different safety criticality levels into a common platform. Scheduling mixed-criticality applications on today’s multi/many-core platforms and providing safe worst-case response time bounds for the real-time applications is challenging given the shared platform resources. For instance, sharing of memory buses introduces delays due to contention, which are non-negligible. Bounding these delays is not trivial, as one needs to model all possible interference scenarios. In this work, we introduce a combined analysis of computing, memory and communication scheduling in a mixed-criticality setting. In particular, we propose: (1) a mixed-criticality scheduling policy for cluster-based many-core systems with two shared resource classes, i.e., a shared multi-bank memory within each cluster, and a network-on-chip for inter-cluster communication and access to external memories; (2) a response time analysis for the proposed scheduling policy, which takes into account the interferences from the two classes of shared resources; and (3) a design exploration framework and algorithms for optimizing the resource utilizations under mixed-criticality timing constraints. The considered cluster-based architecture model describes closely state-of-the-art many-core platforms, such as the Kalray MPPA®-256. The applicability of the approach is demonstrated with a real-world avionics application. Also, the scheduling policy is compared against state-of-the-art scheduling policies based on extensive simulations with synthetic task sets.	algorithm;avionics;barrier (computer science);best, worst and average case;common platform;computer data storage;criticality matrix;direct memory access;electronic system-level design and verification;embedded system;extended memory;flight management system;interference (communication);level design;manycore processor;massively parallel processor array;mathematical optimization;memory bank;mixed criticality;multi-core processor;network on a chip;overhead (computing);particle filter;provable security;real-time clock;real-time transcription;response time (technology);responsiveness;run time (program lifecycle phase);runtime system;scheduling (computing);self-organized criticality;shared memory;simulation;single-core;software deployment;synchronization (computer science);synthetic intelligence;systems design	Georgia Giannopoulou;Nikolay Stoimenov;Pengcheng Huang;Lothar Thiele;Benoît Dupont de Dinechin	2015	Real-Time Systems	10.1007/s11241-015-9227-y	distributed shared memory;fair-share scheduling;shared memory;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;scheduling;network operations center;round-robin scheduling	Embedded	-7.943633010222587	58.129023244469956	172889
7a240fa794acc8151155ad6773cb1ad92e48809b	power reduction scheme of fans in a blade system by considering the imbalance of cpu temperatures	environmental factors;air conditioning;environmental variables;scheduling air conditioning computer centres environmental factors fans power aware computing;power efficiency;temperature sensors;test bed;fan speed data center power consumption power efficiency cpu temperature;computer centres;power aware computing;data center;servers;cpu temperature;scheduling;scheduling power reduction scheme blade system fans cpu temperatures data center power efficiency index environmental variables air conditioning unit cpu disk i o;indexation;blades;power reduction;temperature measurement;power consumption;power demand;power demand temperature measurement fans blades power measurement servers temperature sensors;power measurement;fan speed;fans	In order to develop a data center power efficiency index, we built a test bed of a data center and measured power components and environmental variables in some detail, including the power consumption and temperature of each node, rack and air conditioning unit, as well as load on the CPU, Disk I/O and the network. In these measurements we found that there was a significant imbalance of CPU temperatures that caused an imbalance in the power consumption of fans. We clarified the relationship between CPU load and fan speed, and showed that scheduling or rearrangement of nodes could reduce the power consumption of fans. We reduced fan power consumption by a maximum of about 350 W by changing the scheduling of five nodes, changing the nodes used from hot nodes to cool nodes.	central processing unit;clock rate;data center;input/output;performance per watt;scheduling (computing);testbed	Yuetsu Kodama;Satoshi Itoh;Toshiyuki Shimizu;Satoshi Sekiguchi;Hiroshi Nakamura;Naohiko Mori	2010	2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing	10.1109/GreenCom-CPSCom.2010.49	embedded system;electronic engineering;real-time computing;engineering	HPC	-11.531652840838998	56.239822551537934	173361
1e87f1f44cec30e18d3acf32c61a8f9cf47bf2eb	dynamic cache clustering for chip multiprocessors	non uniform cache architecture nuca;chip multiprocessor;chip;large scale;chip multiprocessor cmp;system simulation;cache management	This paper proposes DCC (Dynamic Cache Clustering), a novel distributed cache management scheme for large-scale chip multiprocessors. Using DCC, a per-core cache cluster is comprised of a number of L2 cache banks and cache clusters are constructed, expanded, and contracted dynamically to match each core's cache demand. The basic trade-offs of varying the on-chip cache clusters are average L2 access latency and L2 miss rate. DCC uniquely and efficiently optimizes both metrics and continuously tracks a near-optimal cache organization from many possible configurations. Simulation results using a full-system simulator demonstrate that DCC outperforms alternative L2 cache designs.	cpu cache;cluster analysis;computer architecture simulator;distributed cache;simulation	Mohammad Hammoud;Sangyeun Cho;Rami G. Melhem	2009		10.1145/1542275.1542289	chip;bus sniffing;pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;operating system;uncore;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol	HPC	-9.880560918918874	53.53096352934018	173394
ea9a241133ffd7f56aff2543d2b5e4686ebb4cc8	schedulability analysis of non-preemptive real-time scheduling for multicore processors with shared caches		Shared caches in multicore processors introduce serious difficulties in providing guarantees on the real-time properties of embedded software due to the interaction and the resulting contention in the shared caches. To address this problem, we develop a new schedulability analysis for real-time multicore systems with shared caches. To the best of our knowledge, this is the first work that addresses the schedulability problem with inter-core cache interference. We construct an integer programming formulation, which can be transformed to an integer linear programming formulation, to calculate an upper bound on cache interference exhibited by a task within a given execution window. Using the integer programming formulation, an iterative algorithm is presented to obtain the upper bound on cache interference a task may exhibit during one job execution. The upper bound on cache interference is subsequently integrated into the schedulability analysis to derive a new schedulability condition. A range of experiments is performed to investigate how the schedulability is degraded by shared cache interference.	algorithm;central processing unit;embedded software;embedded system;experiment;integer programming;interference (communication);iterative method;linear programming formulation;multi-core processor;preemption (computing);real-time clock;real-time transcription;scheduling (computing);scheduling analysis real-time systems	Jun Xiao;Sebastian Altmeyer;Andy D. Pimentel	2017	2017 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2017.00026	iterative method;distributed computing;cache;scheduling (computing);integer programming;embedded software;multi-core processor;computer science;shared memory;upper and lower bounds	Embedded	-10.55582881441135	60.317798948317346	174464
43644b8cd34a759e5cda4953c57dba0bb3e25805	proactive instruction fetch	instruction cache;computer engineering;computer science proactive instruction fetch carnegie mellon university babak falsafi ferdman;caching;branch prediction;prefetching;instruction streaming;control flow;michael	Fast access requirements preclude building L1 instruction caches large enough to capture the working set of server workloads. Efforts exist to mitigate limited L1 instruction cache capacity by relying on the stability and repetitiveness of the instruction stream to predict and prefetch future instruction blocks prior to their use. However, dynamic variation in cache miss sequences prevents correct and timely prediction, leaving many instruction-fetch stalls exposed, resulting in a key performance bottleneck for servers.  We observe that, while the vast majority of application instruction references are amenable to prediction, even minor control-flow variations are amplified by microarchitectural components, resulting in a major source of instability and randomness that significantly limit prefetcher utility. Control-flow variation disturbs the L1 instruction cache replacement order and branch predictor state, causing the L1 instruction cache to randomly filter the instruction stream while the branch predictor and spontaneous hardware interrupts inject the stream with unpredictable noise. Based on this observation, we show that an instruction prefetcher, previously plagued by microarchitectural instability, becomes nearly perfect when modified to operate on the correct-path, retire-order instruction stream. We propose Proactive Instruction Fetch, an instruction prefetch mechanism that achieves higher than 99.5% instruction-cache hit rate, improving server throughput by 27% and nearly matching the performance of a perfect L1 instruction cache that never misses.	branch predictor;cpu cache;cache (computing);control flow;instability;interrupt;kerrison predictor;list of code lyoko episodes;microarchitecture;prefetcher;proactive parallel suite;randomness;requirement;server (computing);spontaneous order;throughput;working set	Michael Ferdman;Cansu Kaynak;Babak Falsafi	2011	2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1145/2155620.2155638	instruction prefetch;parallel computing;real-time computing;computer hardware;computer science;out-of-order execution;minimal instruction set computer;operating system;instruction register;instruction scheduling;programming language;control flow;branch predictor	Arch	-10.035831277635735	53.588362024027376	174914
4644fff0ac15b80d48114ab6f054ea932ceb2dcd	task-level timing models for guaranteed performance in multiprocessor networks-on-chip	buffer minimization;performance evaluation;network on chip;real time;chip;upper bound;data flow graph;system on chip;multiprocessor architecture;timing analysis;real time systems	We consider a dynamic application running on a multiprocessor network-on-chip as a set of independent jobs, each job possibly running on multiple processors. To provide guaranteed quality and performance, the scheduling of jobs, jobs themselves and the hardware must be amenable to timing analysis. For a certain class of applications and multiprocessor architectures, we propose exact timing models that effectively co-model both the computation and communication of a job. The models are based on interprocessor communication (IPC) graphs [4]. Our main contribution is a precise model of network-on-chip communication, including buffer models. We use a JPEG-decoder job as an example to demonstrate that our models can be used in practice to derive upper bounds on the job execution time and to reason about optimal buffer sizes.	central processing unit;computation;inter-process communication;jpeg;job stream;multiprocessing;network on a chip;run time (program lifecycle phase);scheduling (computing);static timing analysis	Peter Poplavko;Twan Basten;Marco Bekooij;Jef L. van Meerbergen;Bart Mesman	2003		10.1145/951710.951721	chip;system on a chip;embedded system;parallel computing;real-time computing;computer science;operating system;data-flow analysis;distributed computing;network on a chip;upper and lower bounds;static timing analysis;multiprocessor scheduling	Metrics	-8.848445986910093	59.968812533883465	174974
ce1b547649ecab09de8e511af10c80708cbc02d7	virtualgc: enabling erase-free garbage collection to upgrade the performance of rewritable slc nand flash memory		Since 3D NAND flash memory could provide more reliable storage than a 2D planar flash memory by relaxing the design rule of a memory cell, a kind of brand new programming technique, namely erase-free scheme, has been proposed to further enhance the endurance of a 3D SLC NAND flash memory. The erase-free scheme brings tons of benefits to flash memory performance and endurance. For example, the erase-free scheme could reclaim invalid (page) space without physically erasing a flash block. However, current flash management designs could not fully exploit the benefits of the erase-free scheme. With the considerations of the features of the erase-free scheme, this paper is the first work to propose a novel flash management design, namely VirtualGC strategy, to deal with the erase-free garbage collection process. By taking the advantages of the erase-free scheme, the proposed strategy reduces the overhead of copying live pages so as to increase flash memory performance. The results show that the proposed strategy significantly improves the performance of rewritable 3D flash memory drives.	flash memory;garbage collection (computer science);memory cell (binary);multi-level cell;overhead (computing)	Tseng-Yi Chen;Yuan-Hao Chang;Yuan-Hung Kuan;Yu-Ming Chang	2017	2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/3061639.3062339	memory management;parallel computing;real-time computing;computer science;flash memory emulator;flash file system;universal memory;non-volatile random-access memory;computer memory;interleaved memory;flash memory	EDA	-11.644917817959515	54.40953638700263	175333
a1b055064fa0b9f17992795baa6fd21f9e112f5e	cashier: a cache energy saving technique for qos systems	qos systems;cache leakage energy saving;cache storage;cmos integrated circuits;quality of service cache storage cmos integrated circuits;llc configurations cashier cache energy saving qos systems cmos technology leakage energy chip design quality of service dynamic profiling last level cache;last level cache;online profiling;image color analysis quality of service color energy consumption logic gates program processors benchmark testing;low power;last level cache qos systems cache leakage energy saving low power online profiling;quality of service	With each CMOS technology generation, leakage energy has been increasing at an exponential rate and hence, managing the energy consumption of large, last-level caches is becoming a critical research issue in modern chip design. Saving cache energy in QoS systems is especially challenging, since, to avoid missing deadlines, a suitable balance needs to be made between energy saving and performance loss. We present CASHIER, a Cache Energy Saving Technique for Quality of Service Systems. Cashier uses dynamic profiling to estimate the memory subsystem energy and execution time of the program under multiple last level cache (LLC) configurations. It then reconfigures LLC to an energy efficient configuration with a view to meet the deadline. In QoS systems, allowed slack may be specified either as percentage of baseline execution time or as absolute slack and Cashier can work for both these cases. The experiments show the effectiveness of Cashier in saving cache energy. For example, for an L2 cache size of 2MB and 5% allowed-slack over baseline, the average saving in memory subsystem energy by using Cashier is 23.6%.	baseline (configuration management);cmos;cpu cache;experiment;ibm notes;lunar lander challenge;profiling (computer programming);quality of service;run time (program lifecycle phase);slack variable;spectral leakage;time complexity	Sparsh Mittal;Zhao Zhang;Yanan Cao	2013	2013 26th International Conference on VLSI Design and 2013 12th International Conference on Embedded Systems	10.1109/VLSID.2013.160	embedded system;parallel computing;real-time computing;cache coloring;page cache;quality of service;cache;computer science;cache invalidation;operating system;smart cache;cmos;cache algorithms;cache pollution;computer network	Embedded	-4.978104244382308	58.12711903220783	175744
b6fba86a64904d0681a186c3ca100b0864d08c09	a game-theoretic approach for run-time distributed optimization on mp-soc	distributed optimization	With forecasted hundreds of processing elements (PEs), future embedded systems will be able to handle multiple applications with very diverse running constraints. Systems will integrate distributed decision capabilities. In order to control the power and temperature, dynamic voltage frequency scalings (DVFSs) are applied at PE level. At system level, it implies to dynamically manage the different voltage/frequency couples of each tile to obtain a global optimization. This paper introduces a scalable multiobjective approach based on game theory, which adjusts at run-time the frequency of each PE. It aims at reducing the tile temperature while maintaining the synchronization between application tasks. Results show that the proposed run-time algorithm requires an average of 20 calculation cycles to find the solution for a 100-processor platform and reaches equivalent performances when comparing with an offline method. Temperature reductions of about 23% were achieved on a demonstrative test-case.		Diego Puschini;Fabien Clermidy;Pascal Benoit;Gilles Sassatelli;Lionel Torres	2008	Int. J. Reconfig. Comp.	10.1155/2008/403086	embedded system;parallel computing;real-time computing;simulation;computer science	EDA	-4.926252743555415	56.96156267585348	176486
a11056259b9ff58524e958b33d0cc4ac13dafb04	accuracy versus migration overhead in real-time multiprocessor reweighting algorithms	multiprocessing;processor scheduling;resource allocation;partitioning based schemes accuracy overhead migration overhead real time multiprocessor reweighting scheduling algorithms;real time;fair scheduling;resource management;accuracy overhead;scheduling algorithm;scheduling algorithms;real time multiprocessor reweighting;migration overhead;partitioning based schemes;multiprocessing systems;costs processor scheduling video sharing scheduling algorithm real time systems microphone arrays computer science adaptive systems hardware multicore processing;resource allocation multiprocessing systems processor scheduling real time systems;real time systems	"""We consider schemes for enacting task share changes - a process called reweighting - on real-time multiprocessor platforms. Our particular focus is reweighting schemes that are deployed in environments in which tasks may frequently request significant share changes. Prior work has shown that fair scheduling algorithms are capable of reweighting tasks with minimal allocation error. However, in such schemes preemption and migration overheads can be high. In this paper, we consider the question of whether the lower migration costs of partitioning-based schemes can provide improved average-case performance relative to fair-scheduled systems. Our conclusion is that partitioning-based schemes are capable of providing significantly lower overall error (including ''error"""" due to preemption and migration costs) than fair schemes in the average case. However, partitioning-based schemes are incapable of providing strong fairness and real-time guarantees"""	algorithm;best, worst and average case;fair-share scheduling;fairness measure;multiprocessing;overhead (computing);preemption (computing);real-time clock;real-time computing;scheduling (computing)	Aaron Block;James H. Anderson	2006	12th International Conference on Parallel and Distributed Systems - (ICPADS'06)	10.1109/ICPADS.2006.21	parallel computing;real-time computing;computer science;resource management;operating system;distributed computing;scheduling	Embedded	-11.12766487323553	60.15685334062214	176533
3efa1381e32820bb7a78da7644f9ca877adbe766	robust real-time multiprocessor interrupt handling motivated by gpus	interrupt handling;protocols;kernel;klmirqd robust real time multiprocessor interrupt handling multicore chips graphics processing units computationally intensive real time workloads i o devices real time systems gpu driven interrupts gpu drivers litmusrt;paper;schedulability gpgpu interrupt handling real time systems;real time systems device drivers graphics processing units interrupts multiprocessing systems;cuda;graphics processing unit real time systems linux instruction sets kernel vehicles protocols;gpgpu;device drivers;graphics processing units;schedulability;package;nvidia;nvidia geforce gtx 470;interrupts;linux;vehicles;multiprocessing systems;computer science;graphics processing unit;instruction sets;real time systems	Architectures in which multicore chips are augmented with graphics processing units (GPUs) have great potential in many domains in which computationally intensive real-time workloads must be supported. However, unlike standard CPUs, GPUs are treated as I/O devices and require the use of interrupts to facilitate communication with CPUs. Given their disruptive nature, interrupts must be dealt with carefully in real-time systems. With GPU-driven interrupts, such disruptiveness is further compounded by the closed-source nature of GPU drivers. In this paper, such problems are considered and a solution is presented in the form of an extension to LITMUS^RT called klmirqd. The design of klmirqd targets systems with multiple CPUs and GPUs. In such settings, interrupt-related issues arise that have not been previously addressed.	central processing unit;computer graphics;input/output;interrupt;multi-core processor;multiprocessing	Glenn A. Elliott;James H. Anderson	2012	2012 24th Euromicro Conference on Real-Time Systems	10.1109/ECRTS.2012.20	communications protocol;parallel computing;kernel;real-time computing;computer science;operating system;instruction set;interrupt;package;linux kernel;general-purpose computing on graphics processing units	EDA	-8.762206762915005	56.57816144805178	176561
16572496532a2581e3e0b117a5b588bf0f5f7805	frocm: a fair and low-overhead method in smt processor	chip multiprocessor;research method;simultaneous multithreading;market segmentation;fair value	Simultaneous Multithreading (SMT)[1][2] and chip multiprocessors (CMP) processors [3] have emerged as the mainstream computing platform in major market segments, including PC, server, and embedded domains. However, prior work on fetch policies almost focuses on throughput optimization. The issue of fairness between threads in progress rates is studied rarely. But without fairness, serious problems, such as thread starvation and priority inversion can arise and render the OS scheduler ineffective. The fairness research methods always disturb the threads running Simultaneous, such as single thread sampling [4]. In this paper, we propose an approach FROCM (Fairness Recalculate Once Cache Miss) to enhance the fairness of running multithreads in SMT processor without disturbing their running states. Using FROCM, every thread’s IPCapproximately is re-calculated in SMT processor Once Cache Miss, IPCapproximately is the approximately value of IPC when the thread runs alone. Using IPCapproximately, the instructions’ issue priority may be changed in due course. We can hold the Fairness value (Fn) higher. Fn is fairness metric defined in this paper, when it is equal to 1, it means utterly fair. Results show that using FROCM, we can hold the most of Fn larger than 0.95, and the throughput hasn’t larger change. It needs less hardware to realize the FROCM, including 4 counters, 1 shifter and 1 adder.	adder (electronics);cpu cache;central processing unit;embedded system;fairness measure;interrupt;mathematical optimization;operating system;priority inversion;sampling (signal processing);scheduling (computing);server (computing);simulation;simultaneous multithreading;starvation (computer science);throughput	Shuming Chen;Pengyong Ma	2007		10.1007/978-3-540-75444-2_54	fair value;parallel computing;real-time computing;computer science;operating system;distributed computing;simultaneous multithreading;super-threading;market segmentation	Arch	-9.514064484777174	57.44545936495713	177170
1ed4697df0c31d9c3c119a85e60c9d08af1c46b8	clotho: decoupling memory page layout from storage organization	storage management architecture;mems-based storage device;memory utilization;non-volatile storage device;storage organization;storage level;storage hierarchy;non-volatile storage;memory hierarchy;simulated mems-based storage device;decoupling memory page layout;different storage technology;cache memory;disk array	As database application performance depends on the utilization of the memory hierarchy, smart data placement plays a central role in increasing locality and in improving memory utilization. Existing techniques, however, do not optimize accesses to all levels of the memory hierarchy and for all the different workloads, because each storage level uses different technology (cache, memory, disks) and each application accesses data using different patterns. Clotho is a new buffer pool and storage management architecture that decouples inmemory page layout from data organization on non-volatile storage devices to enable independent data layout design at each level of the storage hierarchy.Clotho can maximize cache and memory utilization by (a) transparently using appropriate data layouts in memory and non-volatile storage, and (b) dynamically synthesizing data pages to follow application access patterns at each level as needed. Clotho creates in-memory pages individually tailored for compound and dynamically changing workloads, and enables efficient use of different storage technologies (e.g., disk arrays or MEMS-based storage devices). This paper describes theClotho design and prototype implementation and evaluates its performance under a variety of workloads using both disk arrays and simulated MEMS-based storage devices.	cpu cache;coupling (computer programming);disk array;emoticon;in-memory database;locality of reference;memory hierarchy;microelectromechanical systems;non-volatile memory;oracle database;page (computer memory);paging;prototype;volatile memory	Minglong Shao;Jiri Schindler;Steven W. Schlosser;Anastasia Ailamaki;Gregory R. Ganger	2004			interleaved memory;semiconductor memory;parallel computing;page cache;disk array;cpu cache;computer hardware;computer science;computer data storage;database;computer memory;sequential access memory;cache pollution;memory map;memory management	DB	-11.092413409464536	53.92589328899792	177339
ed4c422a89e4030f32b2a4de7cc071790cdea06d	priority scheduling of transactions in distributed real-time databases	phase locking;database system;transaction scheduling;real time;priority scheduling;high priority;mixed method;priority assignment;concurrency control;optimistic concurrency control;conflict resolution;article;concurrency control and distributed real time database systems;concurrency control and distributed real time database	One of the most important issues in the design of distributed real-time database system (DRTDBS) is transaction scheduling which consists of two parts: priority scheduling and real-time concurrency control. In the past studies, mostly, these issues are studied separately although they have a very close interaction with each other. In this paper, we propose new priority assignment policies for DRTDBS and study their impact on two typical real-time concurrency control protocols (RT-CCPs), High Priority Two Phase Locking (HP-2PL) and Optimistic Concurrency Control with Broadcast Commit (OCC-BC). Our performance results show that many factors, such as data conflict resolution, degree of data contention and transaction restarts, that are unique to database systems, have significant impact on the performance of the policies which in turn affect the performance of the real-time concurrency control protocols. OCC-BC is more affected by the priority assignment policies than HP-2PL owing to the late detection of conflict. In the design of priority assignment policies, we have found that neither the purely deadline driven policies nor data contention driven policies are suitable for DRTDBS. Our proposed policy, the Mixed Method (MM), which considers both transaction timeliness and data contention, outperforms other policies over a wide range of system parameter settings.	active database;algorithm;assignment problem;care-of address;clock rate;deadlock;distributed transaction;dynamic priority scheduling;earliest deadline first scheduling;experiment;global serializability;i/o scheduling;lock (computer science);nl (complexity);network model;norm (social);optimistic concurrency control;queueing theory;queuing delay;real-time clock;real-time transcription;requirement;run time (program lifecycle phase);schedule (computer science);scheduling (computing);self-organized criticality;slack variable;transaction data;two-phase locking;urban dictionary	Victor Chung Sing Lee;Kam-yiu Lam;Ben Kao	1999	Real-Time Systems	10.1023/A:1008003902423	priority inheritance;timestamp-based concurrency control;optimistic concurrency control;real-time computing;earliest deadline first scheduling;isolation;dynamic priority scheduling;distributed transaction;computer science;concurrency control;conflict resolution;deadline-monotonic scheduling;database;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability;acid;priority ceiling protocol;distributed concurrency control	DB	-10.837199738483358	59.04116934346289	178191
a9705aa7cd1cf96252d246cdfb2f95f99082026f	energy efficiency in real-time systems: a brief overview	energy conservation;dynamic power management methods energy efficiency real time systems energy optimization techniques real time hardware support power saving mechanisms battery models real time power aware scheduling;embedded systems;power aware computing;scheduling embedded systems energy conservation power aware computing;dynamic power management dpm power aware real time systems scheduling dynamic voltage scaling dvs;scheduling;real time systems batteries hardware voltage control energy consumption clocks time frequency analysis	As the embedded and real-time control systems become ubiquitous, with increasingly stringent requirements of smaller size, their energy efficiency emerges as a problem of key interest. In this paper we review the state of the art of energy optimization techniques from the real-time systems point of view. The survey approaches both hardware and software aspects, including the real-time hardware support, various power-saving mechanisms, battery models, real-time power-aware scheduling and dynamic power management methods. The relevant solutions found in the literature are classified and briefly presented. A comparative discussion of these methods, focusing on a set of key aspects, summarizes the survey.	control system;embedded system;mathematical optimization;point of view (computer hardware company);power management;real-time clock;real-time computing;real-time transcription;requirement;scheduling (computing)	Cristina Stangaciu;Mihai V. Micea;Vladimir Cretu	2013	2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI)	10.1109/SACI.2013.6608981	embedded system;parallel computing;real-time computing;energy conservation;computer science;operating system;scheduling	Embedded	-5.248115455111193	59.64119762176076	178350
0db8b58cee671bf382142a5af2c1f582a7d5eece	t-visor: a hypervisor for mixed criticality embedded real-time system with hardware virtualization support		Recently, embedded systems have not only requirements for hard real-time behavior and reliability, but also diversified functional demands, such as network functions. To satisfy these requirements, virtualization using hypervisors is promising for embedded systems. However, as most of existing hypervisors are designed for general-purpose information processing systems, they rely on large system stacks, so that they are not suitable for mixed criticality embedded real-time systems. Even in hypervisors designed for embedded systems, their schedulers do not consider the diversity of real-time requirements and rapid change in scheduling theory. We present the design and implementation of T-Visor, a hypervisor specialized for mixed criticality embedded realtime systems. T-Visor supports ARM architecture and realizes full virtualization using ARM Virtualization Extensions. To guarantee real-time behavior, T-Visor provides a flexible scheduling framework so that developers can select the most suitable scheduling algorithm for their systems. Our evaluation showed that it performed better compared to Xen/ARM. From these results, we conclude that our design and implementation are more suitable for embedded real-time systems than the existing hypervisors.	arm architecture;algorithm;criticality matrix;embedded system;full virtualization;general-purpose modeling;hardware virtualization;hypervisor;information processing;mixed criticality;real-time clock;real-time computing;real-time operating system;real-time transcription;requirement;scheduling (computing);self-organized criticality;x86 virtualization	Takumi Shimada;Takeshi Yashiro;Ken Sakamura	2018	CoRR		virtualization;real-time computing;full virtualization;scheduling (computing);hypervisor;real-time operating system;mixed criticality;computer science;arm architecture;hardware virtualization	Embedded	-9.003346344266605	58.135212254044234	178517
70de4afa0ed31800152a102cce565cae9326f249	scheduling with accurate communication delay model and scheduler implementation for multiprocessor system-on-chip	resource constraint;on chip communication delay model;scheduling;software communication architecture;scheduling problem;multiprocessor system on chip;communication delay;dynamic software synchronization;scheduler implementation;hardware resource constraints;time constraint	In multiprocessor system-on-chip, tasks and communications should be scheduled carefully since their execution order affects the performance of the entire system. When we implement an MPSoC according to the scheduling result, we may find that the scheduling result is not correct or timing constraints are not met unless it takes into account the delays of MPSoC architecture. The unexpected scheduling results are mainly caused from inaccurate communication delays and or runtime scheduler’s overhead. Due to the big complexity of scheduling problem, most previous work neglects the inter-processor communication, or just assumes a fixed delay proportional to the communication volume, without taking into consideration subtle effects like the communication congestion and synchronization delay, which may change dynamically throughout tasks execution. In this paper, we propose an accurate scheduling model of hardware/software communication architecture to improve timing accuracy by taking into account the effects of dynamic software synchroY. Cho ( ) · K. Choi Seoul National University, Seoul, Republic of Korea e-mail: rams@poppy.snu.ac.kr K. Choi e-mail: kchoi@snu.ac.kr Y. Cho · N.-E. Zergainoh Tima Laboratory, Grenoble, France Y. Cho e-mail: Youngchul.Cho@imag.fr N.-E. Zergainoh e-mail: Nacer-Eddine.Zergainoh@imag.fr S. Yoo Device and Solution Network, Samsung Electronics Inc., Soowon, Republic of Korea e-mail: sungjoo.yoo@samsung.com A.A. Jerraya CEA-LETI, MINATEC, Grenoble, France e-mail: Ahmed.Jerraya@cea.fr	algorithm;centralized computing;context switch;distributed shared memory;email;encoder;experiment;heuristic (computer science);jpeg;mpsoc;multiprocessing;network congestion;overhead (computing);scalability;schedule (computer science);scheduling (computing);synchronization (computer science);synthetic intelligence;system on a chip	Youngchul Cho;Nacer-Eddine Zergainoh;Sungjoo Yoo;Ahmed Amine Jerraya;Kiyoung Choi	2007	Design Autom. for Emb. Sys.	10.1007/s10617-007-9004-9	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;job shop scheduling;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;two-level scheduling;distributed computing;scheduling;multiprocessor scheduling	Embedded	-7.939857962127329	58.4632625683042	178786
2d55afb5cc034c9a7b7a1e8cebfc51312bea38ab	benzene: an energy-efficient distributed hybrid cache architecture for manycore systems		This article proposes Benzene, an energy-efficient distributed SRAM/STT-RAM hybrid cache for manycore systems running multiple applications. It is based on the observation that a naïve application of hybrid cache techniques to distributed caches in a manycore architecture suffers from limited energy reduction due to uneven utilization of scarce SRAM. We propose two-level optimization techniques: intra-bank and inter-bank. Intra-bank optimization leverages highly associative cache design, achieving more uniform distribution of writes within a bank. Inter-bank optimization evenly balances the amount of write-intensive data across the banks. Our evaluation results show that Benzene significantly reduces energy consumption of distributed hybrid caches.	cpu cache;distributed cache;manycore processor;mathematical optimization;multi-core processor;naivety;pointer (computer programming);scalability;static random-access memory	Namhyung Kim;Junwhan Ahn;Kiyoung Choi;Daniel Sanchez;Donghoon Yoo;Soojung Ryu	2018	TACO	10.1145/3177963	architecture;parallel computing;computer science;static random-access memory;efficient energy use;energy consumption;cache;cache-only memory architecture	Arch	-9.64492215166566	53.55735364430344	178849
40b40c110cc525f2ccaa925d980ac078dafb4926	improving energy efficiency of database clusters through prefetching and caching		The goal of this study is to optimize energy efficiency of database clusters through prefetching and caching strategies. We design a workload-skewness scheme to collectively manage a set of hot and cold nodes in a database cluster system. The prefetching mechanism fetches popular data tables to the hot nodes while keeping unpopular data in cold nodes. We leverage a power management module to aggressively turn cold nodes in the low-power mode to conserve energy consumption. We construct a prefetching model and an energy-saving model to govern the power management module in database lusters. The energy-efficient prefetching and caching mechanism is conducive to cutting back the number of power-state transitions, thereby offering high energy efficiency. We systematically evaluate energy conservation technique in the process of managing, fetching, and storing data on clusters supporting database applications. Our experimental results show that our prefetching/caching solution significantly improves energy efficiency of the existing PostgreSQL system.	cpu cache;cache (computing);experiment;high-availability cluster;link prefetching;low-power broadcasting;overhead (computing);postgresql;power management;power supply	Yi Zhou;Shubbhi Taneja;Mohammed I. Alghamdi;Xiao Qin	2018	2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)	10.1109/CCGRID.2018.00065	cluster (physics);power management;efficient energy use;energy consumption;database;energy conservation;computer science	DB	-10.897082857046687	54.40344202642986	179748
5e4d24f4cbcebf271f29023ece49c6b2742ec718	an efficient cache management scheme for capacitor equipped solid state drives		Within SSDs, random access memory (RAM) has been adopted as cache inside controller for achieving better performance. However, due to the volatility characteristic of RAM, data loss may happen when sudden power interrupts. To solve this issue, capacitor has been equipped inside emerging SSDs as interim supplier. However, the aging issue of capacitor will result in capacitance decreases over time. Once the remaining capacitance is not able to write all dirty pages in the cache back to flash memory, data loss may happen. In order to solve the above issue, an efficient cache management scheme for capacitor equipped SSDs is proposed in this work. The basic idea of the scheme is to bound the number of dirty pages in cache within the capability of the capacitor. Simulation results show that the proposed scheme achieves encourage improvement on lifetime and performance while power interruption induced data loss is avoided.	flash memory;interrupt;random access;simulation;solid-state drive;volatility	Congming Gao;Liang Shi;Yejia Di;Qiao Li;Chun Jason Xue;Edwin Hsing-Mean Sha	2018		10.1145/3194554.3194639	capacitor;solid-state;control theory;real-time computing;cache;computer science;data loss;capacitance;flash memory;random access	OS	-10.98234448561008	55.250652761802925	179766
bb048f5b899d57c299070e429d5f176f0d4a11aa	performance analysis of high-speed digital buses for multiprocessing systems	static priority;performance measure;first come first serve;fixed time;probabilistic model;distribution function;performance analysis;high speed	Current multiprocessing systems are often organized by connecting several devices with similar characteristics (usually processors) to a common bus. These devices present access with minimal delay; access is controlled by the bus arbitration algorithm. This paper presents a probabilistic analysis of several arbitration algorithms according to several criteria that reflect their relative performances in (1) rendering equal service to all competing devices and (2) allocating available bus bandwidth efficiently. The sensitivity of these criteria to the number of devices on the bus, the speed of the bus, and the distribution of interrequest times is considered.  A probabilistic model for the quantitative comparison of these algorithms is constructed in which multiple devices repeatedly issue bus requests at random intervals according to an arbitrary distribution function and are serviced according to one of the algorithms; the devices do not buffer bus requests. The algorithms studied include the static priority, fixed time slice (FTS), two dynamic priority, and first-come, first-served (FCFS) schemes. The performance measures are computed by simulation. The analysis reveals that under heavy bus loads, the dynamic priority and FCFS algorithms offer significantly better performances by these measures than do the static priority and FTS schemes.	algorithm;bus (computing);bus mastering;central processing unit;fleet telematics system;multiprocessing;performance;preemption (computing);probabilistic analysis of algorithms;profiling (computer programming);simulation;statistical model	W. L. Bain;S. R. Ahuja	1981			statistical model;embedded system;parallel computing;real-time computing;computer science;local bus;operating system;distribution function;control bus	Arch	-11.203952410137827	57.738232034943806	180189
c3305114a831c6507c819be70b23c068b07aefe9	fpu speedup estimation for task placement optimization on asymmetric multicore designs	asymmetric multicore designs fpu speedup estimation task placement optimization;resource allocation coprocessors multiprocessing systems;estimation multicore processing degradation emulation benchmark testing acceleration registers	The number of cores is increasing in processor designs. By having the same duplicated core increases dark silicon and reduces the scalability of multicore/many-core designs. Asymmetric distribution of ISA specialized units (i.e. FPU and SIMD units) in multicore designs can bring new opportunities for performance gain, area and energy savings. Basically, the task placement is driven by the requirement of the specialized resources. To improve task-mapping flexibility, solutions such as code-versioning or binary translation allow the OS to place any task on any type of core. However, a specialized execution unit such as FPU is able to speedup some part up to 1000x compared to the same function emulated with Integer execution units. On one hand, mapping all tasks on the core with the FPU creates a bottleneck, and on the other hand, mapping a high FPU-usage application on a core without FPU drastically degrades the performance. To allow a smarter placement, the OS needs to know the potential advantage of using the FPU for each thread. In this paper, we demonstrate that only using the trivial percentage utilization is not enough to have an accurate estimation. We propose a finer grain solution to estimate the speedup of the FPU at runtime. The average of the absolute error is 14 %, which is 4.8 times better than the trivial coarse estimation. Then we characterize the different types of FPU-usage through a set of common benchmarks and show the variability of the utilization during the execution.	approximation error;benchmark (computing);binary translation;bottleneck (engineering);dark silicon;emulator;energy drift;execution unit;floating-point unit;heart rate variability;linux;manycore processor;multi-core processor;multiprocessing;operating system;run time (program lifecycle phase);simd;scalability;scheduling (computing);speedup	Alexandre Aminot;Yves Lhuillier;Andrea Castagnetti;Henri-Pierre Charles	2015	2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip	10.1109/MCSoC.2015.21	parallel computing;real-time computing;computer hardware;computer science	Arch	-4.890852068788888	53.630862943458055	180720
9542b697ae0cec34320f832218544bf09056da39	an online thermal-constrained task scheduler for 3d multi-core processors	hotspots;heat sinks;mathematical model;throughput;resource management;public key encryption	Hotspots occur frequently in 3D multi-core processors (3D-MCPs) and they can adversely impact system reliability and lifetime. Moreover, frequent occurrences of hotspots lead to more dynamic voltage and frequency scaling (DVFS), resulting in degraded throughput. Therefore, a new thermal-constrained task scheduler based on thermal-pattern-aware voltage assignment (TPAVA) is proposed in this paper. By analyzing temperature profiles of different voltage assignments, TPAVA pre-emptively assigns different operating-voltage levels to cores for reducing temperature increase in 3D-MCPs. Moreover, the proposed task scheduler integrates a vertical-grouping voltage scaling (VGVS) strategy that considers thermal correlation in 3D-MCPs. Experimental results show that, compared with two previous methods, the proposed task scheduler can respectively lower hotspot occurrences by 47.13% and 53.91%, and improve throughput by 6.50% and 32.06%. As a result, TPAVA and VGVS are effectively for reducing occurrences of hotspots and optimizing throughput for 3D-MCPs under thermal constraints.	central processing unit;computer cooling;dynamic frequency scaling;dynamic voltage scaling;hotspot (wi-fi);image scaling;multi-core processor;online and offline;scheduling (computing);scientific time sharing corporation;throughput;windows task scheduler	Chien-Hui Liao;Charles H.-P. Wen;Krishnendu Chakrabarty	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;parallel computing;real-time computing;computer science;operating system;public-key cryptography	EDA	-6.539217645245505	56.43252165454525	181585
e17bd2a0c233de103b0c0cfd6b09fb9dc5dd9959	non-preemptive speed scaling	approximation algorithms;non preemption;energy efficient scheduling	We consider the following offline variant of the speed scaling problem introduced by Yao et al. We are given a set of jobs and we have a variable-speed processor to process them. The higher the processor speed, the higher the energy consumption. Each job is associated with its own release time, deadline, and processing volume. The objective is to find a feasible schedule that minimizes the energy consumption. In contrast to Yao et al., no preemption of jobs is allowed. Unlike the preemptive version that is known to be in P, the non-preemptive version of speed scaling is strongly NP-hard. In this work, we present a constant factor approximation algorithm for it. The main technical idea is to transform the problem into the unrelated machine scheduling problem with $$L_p$$ -norm objective.	image scaling;non-functional requirement	Antonios Antoniadis;Chien-Chung Huang	2013	J. Scheduling	10.1007/s10951-013-0312-6	mathematical optimization;parallel computing;real-time computing;computer science;mathematics;approximation algorithm	HPC	-5.826859648949534	59.15506951765892	181727
2f0ce316294580f10098de0e9d543d57dad8ed75	static dataflow analysis for soft real-time system design		Synchronous (deterministic) dataflow (SDF) has been extensively used to model flow constraints of digital signal processing (DSP) applications executed on (hard) real-time (RT) operating system (OS). Modern internet-of-things are, however, are often equipped with (soft) RTOSs such as embedded Linux. To reduce design iterations for the latter, the paper proposes a stochastic approach to SDF graphs where the response time of each node is modeled as probability density function (pdf). With increasing number of iterations over the graph, the individual PDFs propagate through the network. The first and second central moments of the resulting joint pdf correspond to the expected system latency and jitter, respectively. The scheduler may execute the code sequentially or in parallel. The proposed analysis tool is helpful in identifying bottlenecks within the system.		Alexander Kocian;Stefano Chessa	2018		10.1007/978-3-319-99608-0_20	digital signal processing;real-time computing;probability density function;real-time operating system;parallel computing;latency (engineering);response time;jitter;dataflow;joint probability distribution;computer science	Embedded	-8.807824825689613	59.223142943043214	182018
82455428587ff2c1e7bb2a3af1bbc78534e7d213	evaluation of existing schedulability tests for global edf	global scheduling algorithm;sustainability property;complexity theory;performance evaluation;time complexity;multiprocessor systems;processor scheduling;earliest deadline first;processor scheduling multiprocessing systems performance evaluation;testing;speedup factor schedulability tests evaluation earliest deadline first global scheduling algorithm identical multiprocessor platform multiprocessor systems schedulability analysis run time complexity sustainability property;schedulability analysis;data mining;scheduling algorithm;estimation;schedulability tests evaluation;multiprocessor systems schedulability analysis;processor scheduling scheduling algorithm finishing system testing performance evaluation multiprocessing systems runtime delay real time systems parallel processing;real time communication;run time complexity;multiprocessing systems;identical multiprocessor platform;speedup factor;hard real time;admission control;real time systems	The increasing attention on global scheduling algorithms for identical multiprocessor platforms produced different, independently developed, schedulability tests. However, the existing relations among such tests have not been sufficiently clarified, so that it is difficult to understand which strategy provides the best performances in a particular scenario. In this paper, we will summarize the main existing results for the schedulability analysis of multiprocessor systems scheduled with global \edf, showing, when possible, existing dominance relations. We will compare these algorithms taking into consideration different aspects, namely, run-time complexity, average performances over a randomly generated workload, sustainability properties and speedup factors.	algorithm;earliest deadline first scheduling;multiprocessing;performance;procedural generation;scheduling (computing);scheduling analysis real-time systems;speedup;time complexity	Marko Bertogna	2009	2009 International Conference on Parallel Processing Workshops	10.1109/ICPPW.2009.12	time complexity;estimation;parallel computing;real-time computing;earliest deadline first scheduling;computer science;operating system;distributed computing;software testing;scheduling	Embedded	-11.772908421367156	60.25659161622482	182546
6bda7a728bd1c4f334c11675f4f5aaea7b1ab1b9	co-scheduling persistent periodic and dynamic aperiodic real-time tasks on reconfigurable platforms		As task preemption/relocation with acceptably low overheads become a reality in today's reconfigurable FPGAs, they are starting to show bright prospects as platforms for executing performance critical task sets while allowing high resource utilization. Many performance sensitive real-time systems including those in automotive and avionics systems, chemical reactors, etc., often execute a set of persistent periodic safety critical control tasks along with dynamic event driven aperiodic tasks. This work presents a co-scheduling framework for the combined execution of such periodic and aperiodic real-time tasks on fully and run-time partially reconfigurable platforms. Specifically, we present an admission control strategy and preemptive scheduling methodology for dynamic aperiodic tasks in the presence of a set of persistent periodic tasks such that aperiodic task rejections may be minimized, thus resulting in high resource utilization. We used the 2D slotted area model where the floor of the FPGA is assumed to be statically equipartitioned into a set of tiles in which any arbitrary task may be feasibly mapped. The experimental results reveal that the proposed scheduling strategies are able to achieve high resource utilization with low task rejection rates over various simulation scenarios.	asynchronous circuit;avionics;computation;computational resource;control theory;earliest deadline first scheduling;fr-v (microprocessor);fair-share scheduling;field-programmable gate array;preemption (computing);real-time clock;real-time computing;real-time transcription;rejection sampling;relocation (computing);scheduling (computing);simulation	Sangeet Saha;Arnab Sarkar;Amlan Chakrabarti;Ranjan Ghosh	2018	IEEE Transactions on Multi-Scale Computing Systems	10.1109/TMSCS.2017.2691701	resource management;admission control;relocation;scheduling (computing);real-time computing;aperiodic graph;dynamic priority scheduling;fixed-priority pre-emptive scheduling;computer science;preemption;distributed computing	Embedded	-9.063666239728432	59.07713062160618	182712
0fb3f572a7a6e5c17e59b90125a53a2e378c8aa4	energy efficient operating mode assignment for real-time tasks in wireless embedded systems	radio networks;energy efficient;real time;wireless embedded systems;energy efficient operating mode assignment;turn off;null;embedded system;scheduling microprocessor chips radio networks;wireless communication;execution modes;energy consumption;scheduling;precedence constraint;schedules;runtime dynamic energy management scheme energy efficient operating mode assignment wireless embedded systems energy consumption single processor real time system radio sleep scheduling execution modes;single processor real time system;network interface;energy consumption schedules wireless communication real time systems wireless sensor networks embedded system parallel processing;real time application;wireless sensor networks;parallel processing;runtime dynamic energy management scheme;energy management;microprocessor chips;radio sleep scheduling;real time systems;time constraint	Minimizing energy consumption is a key issue in designing real-time applications on wireless embedded systems. While a lot of work has been done to manage energy consumption on single processor real-time system, few work addresses network-wide energy consumption management for real-time tasks. Moreover, existing work on network-wide energy consumption assumes that the underlying network is always connected, which is not consistent with the practice in which wireless nodes often turn off their network interfaces in asleep schedule to reduce energy consumption. In this paper, we propose solutions to minimize network-wide energy consumption for real-time tasks with precedence constraints executing on wireless embedded systems. Our solutions take the radio sleep scheduling of wireless nodes into account when adjusting the execution modes of processors. We also propose a runtime dynamic energy management scheme to further reduce energy consumption while guaranteeing the timing constraint. The experiments show that our approach significantly reduces total energy consumption compared with the previous work.	algorithm;central processing unit;embedded system;experiment;operating system;parallel computing;real-time clock;real-time computing;schedule (computer science);scheduling (computing)	Chun Jason Xue;Zhaohui Yuan;Guoliang Xing;Zili Shao;Edwin Hsing-Mean Sha	2008	2008 14th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2008.29	embedded system;parallel processing;real-time computing;wireless sensor network;schedule;computer science;network interface;operating system;distributed computing;efficient energy use;scheduling;wireless;energy management	Embedded	-5.821768217755785	59.2954588813995	182790
5e4994577bd53b87fc32144f0c8a91597051a6b5	dynamic management of key states for reinforcement learning-assisted garbage collection to reduce long tail latency in ssd		Garbage collection (GC) is one of main causes of the long-tail latency problem in storage systems. Long-tail latency due to GC is more than 100 times greater than the average latency at the 99 th percentile. Therefore, due to such a long tail latency, real-time systems and quality-critical systems cannot meet the system requirements. In this study, we propose a novel key state management technique of reinforcement learning-assisted garbage collection. The purpose of this study is to dynamically manage key states from a significant number of state candidates. Dynamic management enables us to utilize suitable and frequently recurring key states at a small area cost since the full states do not have to be managed. The experimental results show that the proposed technique reduces by 22--25% the long-tail latency compared to a state-of-the-art scheme with real-world workloads.	garbage collection (computer science);long tail;reinforcement learning;solid-state drive	Won-Kyung Kang;Sungjoo Yoo	2018		10.1109/DAC.2018.8465934	memory management;real-time computing;latency (engineering);reinforcement learning;garbage collection;computer science;state management;reinforcement;system requirements	EDA	-10.27178296808329	55.441323034828216	182821
d16f0f8f83e9820048f6c38367830fb7925469e6	time-sharing multithreading on stream-based lossless data compression		To address the performance saturation in communication data path at migrating BigData, the lossless data compression technique is a solution to enhance the bandwidth of the path. However, the recent data compression mechanism needs to treat data streams such as sensor data with very low latency to avoid overhead in the path. This paper focuses on a new loss less data compression mechanism called LCA-DLT that implements a hardware-based fast stream lossless data compression using dictionary-based symbol lookup mechanism. When applying it to a very fast path, the hardware latency increases largely and the clock speed degrades because the dictionary lookup operation becomes bottleneck of the longest delay path in the hardware. This paper proposes a performance improvement technique applying multithreading technique in the dictionary lookup operation. The technique enables a single module of the LCA-DLT to accept multiple data streams by dividing the compression timing in babble stage of the compression/decompression pipeline. According to performance evaluation by a hardware implementation with two threads, although the data compression bandwidth logically becomes half of the original single thread LCA-DLT, the time-sharing multithreading technique reduces required hardware resources and improves the clock frequency.	algorithm;big data;clock rate;dlt;data compression;dictionary;elegant degradation;embedded system;fast path;gigabyte;lookup table;microcontroller;multithreading (computer architecture);overhead (computing);pci express;performance evaluation;peripheral;simultaneous multithreading;thread (computing);throughput;time-sharing	Koichi Marumo;Shinichi Yamagiwa	2017	2017 Fifth International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2017.42	real-time computing;latency (engineering);fast path;data stream mining;clock rate;data compression;multithreading;thread (computing);computer science;lossless compression	Arch	-8.476260713643832	54.998458421716386	183021
7561e08ef121146ce6566aeb09ea9a8f36a18089	energy-aware scheduling of periodic conditional task graphs on mpsocs	dynamic voltage scaling;conditional precedence constraints;periodic conditional task graphs;energy aware task scheduling	We investigate the problem of scheduling a set of periodic conditional task graphs of non-preemtible tasks on a MPSoCs (Multi Processor System-on-Chip) with shared memory such that the total expected processor energy consumption of the tasks in each scenario is minimized under two power models, namely dynamic and static power model, and propose a novel offline scheduling approach. Our approach consists of a novel two-phase task scheduler that aims at minimizing total worst-case utilization of each processor and an optimal task execution speed selection algorithm using convex NLP (Non-Linear Programming). Furthermore, we propose an O(1) time online DVS (Dynamic Voltage Scaling) heuristic that assigns each task a speed online. Our experimental results show that our two-phase scheduler achieves a 95.2% success rate of constructing a feasible schedule, compared to a 42% success rate of the state-of-the-art. For energy saving, our offline scheduling approach achieves an average improvement, a maximum improvement and a minimum improvement of 8.03%, 14.08%, and 4.8%, respectively over our online DVS heuristic.	best, worst and average case;central processing unit;dynamic voltage scaling;heuristic;linear programming;natural language processing;online and offline;scheduling (computing);selection algorithm;shared memory;system on a chip;two-phase commit protocol;two-phase locking;windows task scheduler	Umair Ullah Tariq;Hui Wu	2017		10.1145/3007748.3007782	fixed-priority pre-emptive scheduling;parallel computing;real-time computing;computer science;distributed computing	Embedded	-5.882208896156875	58.850568899214124	183133
41bc4545cd258a88c2441ecba62d97dc20d17a0d	mixed-criticality embedded systems -- a balance ensuring partitioning and performance	rail transportation;security processes;safety processes;safety critical software embedded systems;interference;security mixed criticality systems safety processes security processes multi core safety critical systems;mixed criticality systems;multicore processing;safety;aerospace electronics;safety and security process mixed criticality embedded system balance ensuring partitioning performance mixed criticality system industrial deployment industrial perspective;security;program processors;security safety aerospace electronics interference rail transportation multicore processing program processors;multi core;safety critical systems	Mixed-criticality systems have become a mainstream in industry and research due to their potential to decrease, size, weight, and power. Often research institutions and industry interpret the term 'mixed criticality' differently. Hence research approaches and solutions are hard to deploy to industry. This paper discusses the background, the current state of research and industrial deployment of mixed-criticality systems from an industrial perspective. It presents the background of criticality, the safety and security processes, and some approaches of applications of research to real systems. The focus of this paper is partitioning, which is the separation of different applications of different criticality, and its impact on performance along with possible optimizations.	criticality matrix;embedded system;mixed criticality;self-organized criticality;software deployment	Michael Paulitsch;Oscar Medina Duarte;Hassen Karray;Kevin Mueller;Daniel Münch;Jan Nowotsch	2015	2015 Euromicro Conference on Digital System Design	10.1109/DSD.2015.100	multi-core processor;embedded system;real-time computing;computer science;information security;operating system	EDA	-7.698445365483825	58.11101351973719	183331
ebc64cc8a68350a188f33af4f6c55b4704b7b65a	tackling resource variations through adaptive multicore execution frameworks	system on chip multiprocessing systems processor scheduling reconfigurable architectures resource allocation;circuit faults;resource competition;availability;processor scheduling;resource allocation;reconfigurable architectures;degree of freedom;consumer electronics;execution reconfiguration;runtime;band structure;system on chip;multicore processing;adaptive multicore execution framework high end supercomputing low end consumer electronics resource unavailability band structure execution schedule compile time runtime resource variation task assignment preoptimized schedule scheduling heuristic;schedules;schedules multicore processing runtime dynamic scheduling timing circuit faults availability;task assignment;multiprocessing systems;multicore scheduling;resource degradation tolerance;adaptive mpsocs;resource degradation tolerance adaptive mpsocs execution reconfiguration multicore scheduling;dynamic scheduling;timing	Multicore architectures have been widely adopted to accommodate the rising performance demand in various application domains, ranging from high-end supercomputing to low-end consumer electronics. Yet due to the ever growing integration density and application complexity, such architectures suffer from increased level of core availability variations. At runtime, issues such as device failures, heat buildup, as well as resource competitions and preemptions can make computational resources unavailable, necessitating execution schedules capable of delivering diverse performance levels to match the varying resource allocations. The adaptive execution framework introduced in this paper delivers high-quality schedules capable of predictably reconfiguring execution and gracefully degrading performance in the face of resource unavailability. By adhering to a novel band structure, a set of possible execution schedules are compactly engendered in readiness at compile time, thus delivering predictable responses to runtime resource variations. More importantly, through the exploitation of an extra degree of freedom in the scheduling process, the scheduler can perform task assignments in such a way that adaptivity can be embedded within the preoptimized schedules at almost no cost. The efficacy of the proposed technique is confirmed by incorporating it into a conventional, widely adopted scheduling heuristic and experimentally verifying it in the context of single core degradations.	application domain;compile time;compiler;computation;computational resource;confluence;electronic band structure;embedded system;experiment;heuristic;learning to rank;multi-core processor;overhead (computing);preemption (computing);reconfigurability;schedule (computer science);scheduling (computing);static program analysis;supercomputer;unavailability;verification and validation	Chengmo Yang;Alex Orailoglu	2012	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2011.2166829	system on a chip;multi-core processor;embedded system;availability;parallel computing;real-time computing;dynamic priority scheduling;schedule;resource allocation;computer science;operating system;distributed computing;degrees of freedom;electronic band structure	Embedded	-4.9325062662845705	53.71138617513944	183808
141c9863748df7bf97ec0acaea213fed65a6caf7	functionally and temporally correct simulation of cyber-systems for automotive systems		The current simulation tools used in the automotive industry do not correctly model timing behaviors of cyber-systems such as varying execution times and preemptions. Thus, they cannot correctly predict the real control performance. Motivated by this limitation, this paper proposes functionally and temporally correct simulation for the cyber-side of an automotive system. The key idea is to keep the data and time correctness only at physical interaction points and enjoy freedom of scheduling simulated jobs for all other cases. This way, the proposed approach significantly improves the real-time simulation capacity of the state-of-the-art simulation methods while keeping the functional and temporal correctness.	correctness (computer science);human–computer interaction;preemption (computing);real-time clock;scheduling (computing);simulation	Kyoung-Soo We;Seunggon Kim;Wonseok Lee;Chang-Gun Lee	2017	2017 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2017.00014	distributed computing;real-time computing;correctness;scheduling (computing);cyber-physical system;real-time simulation;automotive industry;computer science	Embedded	-8.275421728857546	60.204742518414186	184337
02a87a928167532f5dbd99e5d7051579611d50b0	energy reduction techniques for systems with non-dvs components	multiprocessor platforms;voltage control;leakage current;leakage aware scheduling;energy efficient real time task scheduling;energy efficient;processor scheduling;energy consumption processor scheduling voltage control energy efficiency leakage current job shop scheduling real time systems power dissipation timing power engineering and energy;real time;heterogeneous multiprocessor;dormant modes;dynamic voltage scaling;nondvs components;multiprocessor platforms energy reduction techniques dynamic voltage scaling nondvs components leakage power leakage current dormant modes sleep modes system wide energy consumption power consumption energy efficient real time task scheduling uniprocessor;and real time systems;leakage power;energy reduction techniques;energy consumption;and real time systems task procrastination leakage aware scheduling preemption control heterogeneous multiprocessor;task procrastination;schedules;task assignment;preemption control;power consumption;task scheduling;sleep modes;power demand;program processors;system wide energy consumption;real time systems processor scheduling;real time systems;uniprocessor	Dynamic voltage scaling (DVS) has been widely adopted to reduce the energy consumption resulting from the dynamic power of modern processors. However, while the leakage power resulting from the leakage current becomes significant, how to aggregate the idle time to turn processors to the sleep or dormant modes is crucial in reducing the overall energy consumption. Moreover, for systems with non-DVS components, the execution order of tasks also affects the system-wide energy consumption. With the consideration of the dynamic and leakage power of processors as well as the power consumption resulting from non-DVS components, this paper summarizes our work on energy-efficient real-time task scheduling for both uniprocessor and multiprocessor platforms through procrastination of task executions, preemption control, and proper task assignment.	aggregate data;algorithm;central processing unit;dynamic voltage scaling;electronic system-level design and verification;image scaling;level design;multiprocessing;preemption (computing);real-time clock;scheduling (computing);slack variable;spectral leakage;uniprocessor system	Chuan-Yue Yang;Jian-Jia Chen;Tei-Wei Kuo;Lothar Thiele	2009	2009 IEEE Conference on Emerging Technologies & Factory Automation	10.1109/ETFA.2009.5347153	embedded system;uniprocessor system;parallel computing;real-time computing;schedule;computer science;engineering;leakage;efficient energy use	EDA	-5.030515984050144	58.25714179249051	184919
24366a3de51bce12c0acc96bbfa004c3ff51066b	guaranteeing optional task completions on (m,k)-firm real-time systems	firm deadlines;on line feasibility test firm deadlines m k firm real time scheduling;job shop scheduling;real time;real time systems job shop scheduling;best effort;delta modulation;on line feasibility test;scheduling algorithm;m;k m executions guaranteeing optional task completions real time systems;time factors;real time systems schedules scheduling algorithm delta modulation time factors dynamic scheduling;real time scheduling;guaranteeing optional task completions;schedules;k m executions;k firm real time scheduling;dynamic scheduling;real time systems	Previous preemptive real-time (m,k)-firm schedulers all try a best effort policy to dispatch k-m executions when m out of k consecutive task executions have met their deadlines. These additional executions can only bring value to the system if no firm deadline is missed. In this paper we present a framework whereby an on-line feasibility test is carried out on these k-m executions before dispatching them. With our scheme, all dispatched tasks are guaranteed to meet their deadlines.	best-effort delivery;computation;conformity;dynamic dispatch;embedded system;job stream;online and offline;real-time clock;real-time computing;real-time operating system;real-time transcription;requirement;schedule (computer science);scheduling (computing)	Claude Evéquoz	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.308	best-effort delivery;job shop scheduling;delta modulation;real-time computing;haplogroup m;dynamic priority scheduling;schedule;computer science;distributed computing;scheduling	Embedded	-10.146473358632932	60.43255024412137	184949
76d960422399910d2eab255cc9591958d0ba8493	criticality-cognizant clustering-based task scheduling on multicore processors in the avionics domain			avionics;computer cluster;multi-core processor;scheduling (computing);self-organized criticality	K. Nagalakshmi;N. Gomathi	2018	Int. J. Comput. Intell. Syst.	10.2991/ijcis.11.1.17	machine learning;artificial intelligence;parallel computing;avionics;mathematics;mixed criticality;scheduling (computing);criticality;cluster analysis;multi-core processor	Embedded	-7.703495534227371	57.79840770734869	184954
80bb879e5732d16a54ec4e7d34f40aeda7a3fb0e	reducing scheduling cost in list scheduling algorithms	scheduling;processor;algorithm;scheduling algorithm	In list scheduling algorithm, at each scheduling step one task is scheduled to the processor on which the task achieves the earliest start time. As a result, each task is tentatively scheduled to each processor to find the destination processor. This paper presents a technique to be applied to the list scheduling algorithms which significantly reduces scheduling cost without sacrificing performance. It is proved that only two tentative task scheduling are necessary for each task to find the earliest task start time. As the consequences, the list scheduling complexity is significantly reduces from O(V log (W)+EP)to O(V(log(W) +log (P))+E), where V and E are number of tasks and edges in the task graph respectively. W is the graph width and P is the number of processors. It is demonstrated that the modified version of scheduling algorithm using the described technique yield at a significantly more cost equal or better performance compared to original version of the list scheduling algorithms.	algorithm;central processing unit;list scheduling;scheduling (computing)	Sharanjit Singh;Kanwaljit Singh;Navpreet Singh	2006			computer science;lottery scheduling;fair-share scheduling;round-robin scheduling;distributed computing;dynamic priority scheduling;deadline-monotonic scheduling;two-level scheduling;fixed-priority pre-emptive scheduling;rate-monotonic scheduling	HPC	-11.366640890059927	59.422515005756864	185265
69fc8244eb00770246989fd7a4dbfaaa8d87dc83	architecture for adaptive resource assignment to virtualized mixed-criticality real-time systems	resource management;multicore;system virtualization	System virtualization is a powerful approach for the creation of integrated systems, which meet the high functionality and reliability requirements of complex embedded applications. It is in particular well-suited for mixed-criticality systems, since the often applied pessimistic manner of critical system engineering leads to heavily under-utilized resources. Existing static resource management approaches for virtualized systems are inappropriate for the dynamically varying resource requirements of upcoming adaptive systems. In this paper, we propose a dynamic resource management protocol for system virtualization that factors criticality levels in and allows the addition of subsystems at runtime. The two-level architecture offers flexibility across virtual machine borders and has the potential to improve the resource utilization. In addition, it provides the capability to adapt at runtime according to defects or changes of the environment.	adaptive system;critical system;criticality matrix;embedded system;hardware virtualization;integrated development environment;mixed criticality;real-time computing;real-time transcription;requirement;run time (program lifecycle phase);self-organized criticality;systems engineering;virtual machine	Stefan Grösbrink;Simon Oberthür;Daniel Baldin	2013	SIGBED Review	10.1145/2492385.2492388	multi-core processor;embedded system;real-time computing;application virtualization;computer science;resource management;operating system;distributed computing;human resource management system	Embedded	-8.977623605918248	57.94665166592061	185828
005cd367305deafde16e0676a9a8af9c59577929	an integrated approach for managing the lifetime of flash-based ssds	hardware;memory management;media;soft error;error detection and correction	As the semiconductor process is scaled down, the endurance of NAND flash memory greatly deteriorates. To overcome such a poor endurance characteristic and to provide a reasonable storage lifetime, system-level endurance enhancement techniques are rapidly adopted in recent NAND flash-based storage devices like solid-state drives (SSDs). In this paper, we propose an integrated lifetime management approach for SSDs. The proposed lifetime management technique combines several lifetime-enhancement schemes, including lossless compression, deduplication, and performance throttling, in an integrated fashion so that the lifetime of SSDs can be maximally extended. By selectively disabling less effective lifetime-enhancement schemes, the proposed technique achieves both high performance and high energy efficiency while meeting the required lifetime. Our evaluation results show that the proposed technique, over the SSDs with no lifetime management schemes, improves write performance by up to 55% and reduces energy consumption by up to 43% while satisfying a 5-year lifetime warranty.	adobe flash;data deduplication;flash memory;lossless compression;run time (program lifecycle phase);semiconductor;solid-state drive	Sungjin Lee;Taejin Kim;Jisung Park;Jihong Kim	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;real-time computing;error detection and correction;soft error;computer hardware;computer science;engineering;operating system	EDA	-11.255145937368107	55.21619953204889	186002
b059a653b5a10b29fa61acf6ceacba783d2a971b	virtual private caches	resource utilization;capacity management;chip multiprocessor;soft real time;upper bound;shared caches;bandwidth management;quality of service;performance isolation	Virtual Private Machines (VPM) provide a framework for Quality of Service (QoS) in CMP-based computer systems. VPMs incorporate microarchitecture mechanisms that allow shares of hardware resources to be allocated to executing threads, thus providing applications with an upper bound on execution time regardless of other thread activity. Virtual Private Caches (VPCs) are an important element of VPMs. VPC hardware consists of two major components: the VPC Arbiter, which manages shared cache bandwidth, and the VPC Capacity Manager, which manages the cache storage. Both the VPC Arbiter and VPC Capacity Manager provide minimum service guarantees that, when combined, achieve QoS for the cache subsystem. Simulation-based evaluation shows that conventional cache bandwidth management policies allow concurrently executing threads to affect each other significantly in an uncontrollable manner. The evaluation targets cache bandwidth because the effects of cache capacity sharing have been studied elsewhere. In contrast with the conventional policies, the VPC Arbiter meets its QoS performance objectives on all workloads studied and over a range of allocated bandwidth levels. The VPC Arbiter’s fairness policy, which distributes leftover bandwidth, mitigates the effects of cache preemption latencies, thus ensuring threads a high-degree of performance isolation. Furthermore, the VPC Arbiter eliminates negative bandwidth interference which can improve aggregate throughput and resource utilization.	aggregate data;arbiter (electronics);bandwidth management;fairness measure;interference (communication);microarchitecture;preemption (computing);quality of service;run time (program lifecycle phase);simulation;throughput;virtual private cloud	Kyle J. Nesbit;James Laudon;James E. Smith	2007		10.1145/1250662.1250671	bandwidth management;computer architecture;in situ resource utilization;parallel computing;real-time computing;quality of service;computer science;operating system;capacity management;upper and lower bounds;computer network	Arch	-9.514947424721717	56.93955913956388	186094
0e94e364061399aab24e4a0e65a192489a22b948	position paper: countering the noise-induced critical path problem	operating system noise exascale power consumption parallel efficiency;optimal power redistribution system noise estimation critical path problem high performance computing hpc system parallel computer system exascale system;runtime;exascale;operating system noise;power aware computing estimation theory optimisation parallel processing;power consumption;parallel efficiency;power demand;runtime conductors power demand operating systems supercomputers delays;supercomputers;delays;operating systems;conductors	"""As the number of cores grow in HPC systems, so does the effect of system noise on applications running on these systems. With the knowledge that future large-scale parallel computer systems, including exascale systems, will operate under an overall power bound, we claim to have found a solution that can counter the effects of noise. We present two methods that estimate the effects of noise on an application and then optimally redistributes power among nodes, such that the effects of noise are """"hidden""""."""	critical path;heuristic;image noise;iteration;parallel computing;runtime system;simulation	Rogelio Long;Shirley Moore	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.203	parallel computing;real-time computing;computer science;operating system;electrical conductor;distributed computing	HPC	-5.056343038380651	54.62671450448731	186468
6b371d54b52bcf32f64a79ba403f9662b7137c15	tlsf: a new dynamic memory allocator for real-time systems	storage allocation;system configuration;real time;real time operating system;object oriented programming;software engineering;real time systems dynamic programming software algorithms software engineering object oriented programming application software delay heuristic algorithms timing operating systems;object oriented;temporal cost dynamic memory allocation real time systems dynamic storage allocation software engineering object oriented programming real time operating systems segregated fit memory allocation memory deallocation;real time operating systems;dynamic memory allocation;memory allocation;real time application;operating systems computers;dynamic storage allocation;real time systems;object oriented programming storage allocation real time systems operating systems computers software engineering	Dynamic storage allocation (DSA) algorithms play an important role in the modern software engineering paradigms and techniques (such as object oriented programming). Using DSA increases the flexibility and functionalities of applications. There exists a large number of references to this particular issue in the literature. However, the use of DSA has been considered a source of indeterminism in the real-time domain, due to the unconstrained response time of DSA algorithms and the fragmentation problem. Nowadays, real-time applications require more flexibility: the ability to adjust system configuration in response to workload changes and application reconfiguration. This aspect adds value to the definition and implementation of dynamic storage allocation algorithms. Considering these reasons, DSA algorithms with a bounded and acceptable timing behaviour must be developed to be used by real-time operating systems (RTOSs). In this paper a DSA algorithm called two-level segregated fit memory allocator (TLSF), developed specifically to be used by RTOS, is introduced. The TLSF algorithm provides explicit allocation and deallocation of memory blocks with a temporal cost /spl Theta/(1).	algorithm;andy wellings;arbitrary code execution;bitmap;buddy system;central processing unit;data structure;fragmentation (computing);memory management;quality of service;real-time clock;real-time computing;real-time operating system;real-time transcription;requirement;response time (technology);software engineering;system configuration	Miguel Masmano;Ismael Ripoll;Alfons Crespo;Jorge Real	2004	Proceedings. 16th Euromicro Conference on Real-Time Systems, 2004. ECRTS 2004.	10.1109/ECRTS.2004.35	parallel computing;real-time computing;simulation;real-time operating system;computer science;operating system;programming language;object-oriented programming	Embedded	-11.533853015194197	59.62224522441759	186616
6acd8d1a85e2080cca1ff33ecefdf9a5df51376f	area efficient rom-embedded sram cache	read only storage;cache storage;integrated circuit design;sram design cache design random access memory ram read only memory rom rom embedded static ram sram;logic testing area efficient rom embedded sram cache math function evaluation digital signal processing built in self test on chip tables read only memories 6t static random access memory bit cell 8t static random access memory bit cell wordline write steps sram array load instruction virtual address space r cache tag arrays translation look aside buffers;sram chips cache storage integrated circuit design read only storage;random access memory read only memory transistors layout system on a chip arrays standards;sram chips	There are many important applications, such as math function evaluation, digital signal processing, and built-in self-test, whose implementations can be faster and simpler if we can have large on-chip “tables” stored as read-only memories (ROMs). We show that conventional de facto standard 6T and 8T static random access memory (SRAM) bit cells can embed ROM data without area overhead or performance degradation on the bit cells. Just by adding an extra wordline (WL) and connecting the WL to selected access transistor of the bit cell (based on whether a 0 or 1 is to be stored as ROM data in that location), the bit cell can work both in the SRAM mode and in the ROM mode. In the proposed ROM-embedded SRAM, during SRAM operations, ROM data is not available. To retrieve the ROM data, special write steps associated with proper via connections load ROM data into the SRAM array. The ROM data is read by conventional load instruction with unique virtual address space assigned to the data. This allows the ROM-embedded cache (R-cache) to bypass tag arrays and translation look-aside buffers, leading to fast ROM operations. We show example applications to illustrate how the R-cache can lead to low-cost logic testing and faster evaluation of mathematical functions.	address space;benchmark (computing);bit cell;cpu cache;cache (computing);data compression;digital signal processing;double-precision floating-point format;elegant degradation;embedded system;fault coverage;logic built-in self-test;overhead (computing);random access;read-only memory;static random-access memory;test data;transistor	Dongsoo Lee;Kaushik Roy	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2217514	computer architecture;electronic engineering;parallel computing;tag ram;computer hardware;computer science;universal memory;integrated circuit design	Arch	-7.157343913872864	53.77690986605682	187537
c87d2dd732a61f3ad6409c6dcf5ab233c81a4a66	efficient network and i/o throttling for fine-grain cycle stealing	workstations computer science educational institutions permission resource management communication system control processor scheduling delay central processing unit	This paper proposes and evaluates a new mechanism, rate windows, for I/O and network rate policing. The goal of the proposed system is to provide a simple, yet effective way to enforce resource limits on target classes of jobs in a system. This work was motivated by our Linger Longer infrastructure, which harvests idle cycles in networks of workstations. Network and I/O throttling is crucial because Linger Longer can leave guest jobs on non-idle nodes and machine owners should not be adversely affected. Our approach is quite simple. We use a sliding window of recent events to compute the average rate for a target resource. The assigned limit is enforced by the simple expedient of putting application processes to sleep when they issue requests that would bring their resource utilization out of the allowable profile. Our I/O system call intercept model makes the rate windows mechanism light-weight and highly portable. Our experimental results show that we are able to limit resource usage to within a few percent of target usages.	cycle stealing	Kyung Dong Ryu;Jeffrey K. Hollingsworth;Peter J. Keleher	2001		10.1109/SC.2001.10031	embedded system;parallel computing;real-time computing;computer science	HPC	-10.327911878910152	56.64925489572558	187667
8c588f4e9f9f6731c7bf3b7e40f3e70f1926853b	on-demand solution to minimize i-cache leakage energy with maintaining performance	microprocessors;cache storage;superdrowsy leakage control;leakage;microprocessor;instruction cache;branch predictor;on demand wake up prediction policy;reduced supply voltage;branch prediction;energy aware systems;cache memory;leakage power reduction;leakage power;leakage currents;low power electronics;microprocessor chips cache storage leakage currents low power electronics;cache leakage energy;wake up policy;logic gates threshold voltage accuracy voltage control degradation program processors reliability;low power design;energy aware systems cache memories microprocessors low power design;reduced supply voltage microprocessor cache leakage energy performance maintenance on demand wake up prediction policy leakage power reduction branch prediction superdrowsy leakage control;performance maintenance;microprocessor chips;cache memories	This paper describes a new on-demand wake-up prediction policy for reducing leakage power. The key insight is that branch prediction can be used to selectively wake up only the needed cache line. This achieves better leakage savings than the best prior policies while avoiding the performance overheads of those policies, without needing an extra prediction structure. The proposed policy reduces leakage energy by 92.7 percent with only 0.08 percent performance overhead on average. The branch-prediction-based approach requires an extra pipeline stage for wake up, which adds to the branch misprediction penalty. Fortunately, this cost is mitigated because the extra wake-up stage is overlapped with misprediction recovery. This paper assumes the superdrowsy leakage control technique using reduced supply voltage because it is well suited to the instruction cache's criticality. However, the proposed policy can be also applied to other leakage-saving circuit techniques.	best, worst and average case;branch misprediction;branch predictor;cpu cache;computer performance;criticality matrix;elegant degradation;kerrison predictor;overhead (computing);spectral leakage;wake (cipher);wake-on-lan	Sung Woo Chung;Kevin Skadron	2008	IEEE Transactions on Computers	10.1109/TC.2007.70770	parallel computing;real-time computing;computer science;operating system;branch predictor	Arch	-6.841445341354791	54.930149621324695	188129
90b7a9800a863e1a57d721cbb34a7a91699cfb80	cranarch: a feasible processor micro-architecture for cloud radio access network	wireless communication protocol;cloud data centers;cranarch micro architecture;期刊论文;user plane protocol;general processor limitation	Cloud Radio Access Network (C-RAN) becomes a promising infrastructure, which can improve hardware resource utilization of traditional Radio Access Network (RAN). For C-RAN, data centers are essential hardware platform, and these data centers are universally equipped with general commercial multi-core processors which are not designed for wireless communication protocols dedicatedly. In this paper, firstly, we evaluate performance bottlenecks of general multi-core processors with current dominant micro-architecture via typical wireless communication protocol. The results show that the dominant micro-architecture mismatches to demands of wireless communication protocols, which makes dominant general multi-core processor is inefficient when used in this application field. Secondly, we analyze the essential micro-architectural level reasons for the inefficiency in detail. A typical characteristic of wireless communication protocols is there are lots of random memory accesses in a large memory address space, which results in high miss ratio of on-chip cache hierarchies. The frequent on-chip cache misses result in amounts of off-chip memory access operations, which incurs longer memory access latency as a dominant factor to degrade overall performance. Based on the results, we identify key micro-architectural characteristics that meet demands of wireless communication protocols. Finally, we propose a feasible micro-architecture of multi-core processor for C-RAN, called as CRANarch, the performance results show its improved hardware utilization efficiency and power efficiency when used in C-RAN data centers.	microarchitecture;radio access network	Fenglong Song;Shibin Tang;Wenming Li;Futao Miao;Hao Zhang;Dongrui Fan;Zhiyong Liu	2014	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2014.08.003	uniform memory access;embedded system;parallel computing;real-time computing;telecommunications;computer science;operating system;distributed computing;computer network	Arch	-9.872421044269084	55.86508819058979	188664
57f0cbdd3fc587f3df3bdf6c0f2d549b557960e0	three-level performance optimization for heterogeneous systems based on software prefetching under power constraints		Abstract High power consumption has become one of the critical problems restricting the development of high-performance computers. Recently, there are numerous studies on optimizing the execution performance while satisfying the power constraint in recent years. However, these methods mainly focus on homogeneous systems without considering the power or speed difference of heterogeneous processors, so it is difficult to apply these methods in the heterogeneous systems with an accelerator. In this paper, by abstracting the current execution model of a heterogeneous system, we propose a new framework for managing the system power consumption with a three-level power control mechanism. The three levels from top to bottom are: system-level power controller (SPC), group-level power controller (GPC) and unit-level power controller (UPC). The study establishes a power management method for software prefetch in UPC to scale frequency and voltage of programs, select the optimal prefetch distance and guide optimization process to satisfy the constraint boundary according to power constraints. The strategy for dividing power based on key threads is put forward in GPC to preferentially allocate power to threads in key paths. In SPC, a method for evaluating the performance of heterogeneous processing engines is designed for dividing power in order to improve the overall execution performance of the system while sustaining the fairness between concurrent applications. Finally, the proposed framework is verified on a central processing unit (CPU)-graphics processing unit (GPU) heterogeneous system.	cpu cache;mathematical optimization;performance tuning	Zhuowei Wang;Wuqing Zhao;Hao Wang;Lianglun Cheng	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.03.009	power control;real-time computing;control theory;instruction prefetch;power management;execution model;thread (computing);distributed computing;software;computer science;central processing unit	Arch	-5.343049662633274	55.16898585414522	188745
2bcfdb4df38c837037e5e54920df86b08751e78e	continuous bytecode instruction counting for cpu consumption estimation	virtual machines java microprocessor chips;cpu consumption estimation;real time;java virtual machine;resource management;continuous cpu consumption;virtual machines;bytecode engineering;java resource accounting framework continuous bytecode instruction counting cpu consumption estimation java virtual machine;java resource accounting framework;continuous bytecode instruction counting;microprocessor chips;java	As an execution platform, the Java virtual machine (JVM) provides many benefits in terms of portability and security. However, this advantage turns into an obstacle when it comes to determining the computing resources (CPU, memory) a program would require to run properly in a given environment. In this paper, we build on the Java resource accounting framework, second edition (J-RAF2), to investigate the use of bytecode instruction counting (BIC) as an estimation of real CPU consumption. We show that for all of the tested platforms there is a stable, application-specific ratio of bytecodes per unit of CPU time - the experimental bytecode rate (BRexp) - that can be used as a basis for translating a BIC value into the corresponding CPU consumption	bayesian information criterion;central processing unit;java virtual machine;software portability	Andrea Camesi;Jarle Hulaas;Walter Binder	2006	Third International Conference on the Quantitative Evaluation of Systems - (QEST'06)	10.1109/QEST.2006.12	parallel computing;real-time computing;java concurrency;computer science;virtual machine;resource management;cpu time;strictfp;real time java;programming language;java;cpu shielding	DB	-5.891889231359008	54.356841678962276	189235
6ab57ffe535a3be61d7635652271b76bd0a78045	a dvs-based pipelined reconfigurable instruction memory	pipelined reconfigurable instruction memory hierarchy;voltage control;cache storage;battery operated embedded systems;voltage banks;instruction cache;clocks;cache reconfiguration points energy consumption battery operated embedded systems instruction cache idle cache banks energy savings pipelined reconfigurable instruction memory hierarchy voltage banks profile driven compilation framework;perforation;reconfigurable architectures;computer aided instruction;dynamic voltage scaling;reconfigurable memory;embedded system;cache reconfiguration points;power aware computing;low voltage;idle cache banks;low power;reconfigurable memory instruction cache low power;permission;energy consumption;profile driven compilation framework;energy savings;switches;frequency;program processors;algorithm design and analysis;benchmark testing;embedded computing;pipeline processing;energy saving;reconfigurable architectures cache storage pipeline processing power aware computing;throughput;energy consumption voltage control frequency throughput pipeline processing algorithm design and analysis permission dynamic voltage scaling computer aided instruction embedded computing	Energy consumption is of significant concern in battery operated embedded systems. In the processors of such systems, the instruction cache consumes a significant fraction of the total energy. One of the most popular methods to reduce the energy consumption is to shut down idle cache banks. However, we observe that operating idle cache banks at a reduced voltage/frequency level along with the active banks in a pipelined manner can potentially achieve even better energy savings. In this paper, we propose a novel DVS-based pipelined reconfigurable instruction memory hierarchy called PRIM. A canonical example of our proposed PRIM consists of four cache banks. Two of these cache banks can be configured at runtime to operate at lower voltage and frequency levels than that of the normal cache. Instruction fetch throughput is maintained by pipelining the accesses to the low voltage banks. We developed a profile-driven compilation framework that analyzes applications and inserts the appropriate cache reconfiguration points. Our experimental results show that PRIM can significant reduce the energy consumption for popular embedded benchmarks with minimal performance overhead. We obtained 56.6% and 45.1% energy savings for aggressive and conservative VDD settings, respectively, at the expense of a 1.66% performance overhead.	algorithm;cpu cache;central processing unit;compiler;dynamic voltage scaling;embedded system;filter bank;memory hierarchy;overhead (computing);pipeline (computing);run time (program lifecycle phase);speculative execution;throughput;value-driven design	Zhiguo Ge;Tulika Mitra;Weng-Fai Wong	2009	2009 46th ACM/IEEE Design Automation Conference	10.1145/1629911.1630142	embedded system;algorithm design;benchmark;throughput;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;network switch;cache;computer science;cache invalidation;operating system;frequency;smart cache;low voltage;cache algorithms;cache pollution	EDA	-5.342367046208787	54.78221514031918	189289
8e0d367540261af1af57c5eea4852dbf82b76ef0	dynamic ram architectures for graphics applications	high speed	This paper explores the requirements for future graphics memories and analyzes the requirements for two of the biggest problems in graphics terminal design, raster display bandwidth and random-bit update performance. To solve these problems a new access mode for dynamic RAMs called Ripplemode#8482; will be proposed. This mode directly solves the raster display bandwidth problem by providing high-speed access to a serial bit stream. Under certain circumstances this mode also improves the performance of random-bit updates.	bitstream;computer terminal;dynamic random-access memory;graph bandwidth;mainframe computer;raster graphics;requirement	Douglas L. Finke	1983		10.1145/1500676.1500736	parallel computing;real-time computing;computer hardware;computer science	Arch	-8.721636744734978	55.06593365472599	189796
dda890a660ce769816674e719ab6bb78aac12f44	energy-aware disk scheduling for soft real-time i/o requests	caracteristique energetique;economies d energie;storage system;algorithm performance;caracteristica energetica;systeme grande taille;multimedia;ahorros energia;disk scheduling;mobile device;capacity planning;energie minimale;energy efficient;real time;buffer management;large scale system;satisfiability;buffer system;disco duro;hard disk;soft real time;sistema amortiguador;chip;algorithme;prototipo;algorithm;energy characteristic;large scale;mobile environment;scheduling algorithm;planificacion;operating system;energy consumption;resultado algoritmo;scheduling;temps reel;multimedia communication;performance algorithme;tiempo real;energy savings;planning;planification;energia minima;systeme tampon;communication multimedia;prototype;non real time;ordonnancement;reglamento;sistema gran escala;disque dur;energy management;algoritmo;minimum energy	In this work, we develop energy-aware disk scheduling algorithm for soft real-time I/O. Energy consumption is one of the major factors which bar the adoption of hard disk in mobile environment. Heat dissipation of large scale storage system also calls for an energy-aware scheduling technique to further increase the storage density. The basic idea in this work is to properly determine the I/O burst size so that device can be in standby mode between consecutive I/O bursts and that it can satisfy the soft real-time requirement. We develop an elaborate model which incorporates the energy consumption characteristics, overhead of mode transition in determining the appropriate I/O burst size and the respective disk operating schedule. Efficacy of energy-aware disk scheduling algorithm greatly relies on not only disk scheduling algorithm itself but also various operating system and device firmware related concerns. It is crucial that the various operating system level and device level features need to be properly addressed within disk scheduling framework. Our energy-aware disk scheduling algorithm successfully addresses a number of outstanding issues. First, we examine the effect of OS and hard disk firmware level prefetch policy and incorporate its effect in our disk scheduling framework. Second, our energy aware scheduling framework can allocate a certain fraction of disk bandwidth to handle sporadically arriving non real-time I/O’s. Third, we examine the relationship between lock granularity of the buffer management and energy consumption. We develop a prototype software with energy-aware scheduling algorithm. In our experiment, proposed algorithm can reduce the energy consumption to one fourth if we use energy-aware disk scheduling algorithm. However, energy-aware disk scheduling algorithm increases buffer requirement significantly, e.g., from 4 to 140 KByte. We carefully argue that the buffer overhead is still justifiable given the cost of DRAM chip and importance of energy management in modern mobile devices. The result of our work not only provides the energy efficient scheduling algorithm but also provides an important guideline in capacity planning of future energy efficient mobile devices.		Youjip Won;Jongmin Kim;Wonmin Jung	2007	Multimedia Systems	10.1007/s00530-007-0107-8	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;real-time computing;earliest deadline first scheduling;simulation;telecommunications;dynamic priority scheduling;computer science;electrical engineering;rate-monotonic scheduling;operating system;two-level scheduling;lottery scheduling;round-robin scheduling;scheduling;algorithm;i/o scheduling	Embedded	-4.665656296011179	59.5209894879815	189828
ed8fa210b70c2bb2de0dad465ebb73e7e968a329	classy: a clock analysis system for rapid prototyping of embedded applications on mpsocs	model analysis;simulation;rapid prototyping;abstract clocks;embedded applications;simulation and analysis;mpsocs;adaptive system;multiprocessor system on chip;analysis;synchronous dataflow;mpsoc design;synchronous approach	This paper presents an abstract multi-clock oriented reasoning for the rapid prototyping of embedded applications executed on multiprocessor systems-on-chip (MPSoCs). The scheduling of applications on execution platforms composed of processors operating at various frequencies is described and analyzed with clocks. As in the static scheduling of synchronous dataflows (SDFs), requirements for admissible schedules are investigated, which come not only from expected application behavior, but also from execution platform. An algorithm is proposed to construct admissible schedules respecting identified requirements. It is then adapted to synthesize admissible schedules for adaptive system behaviors. The modeling, analysis and algorithms presented in this paper have been implemented in a prototype tool named CLASSY (standing for CLock AnalySis SYstem).	adaptive system;admissible heuristic;algorithm;central processing unit;embedded system;heuristic (computer science);multiprocessing;online and offline;prototype;rapid prototyping;requirement;schedule (computer science);scheduling (computing);system on a chip	Xin An;Sarra Boumedien;Abdoulaye Gamatié;Éric Rutten	2012		10.1145/2236576.2236577	embedded system;parallel computing;real-time computing;computer science;adaptive system;operating system;analysis	Embedded	-7.902155386613857	59.421721124920836	190371
24f300c3bf38fff4595d4af10f74ef885e783874	loop instruction caching for energy-efficient embedded multitasking processors	energy consumption energy efficient embedded multitasking processors power consumption processor generations energy dissipation system design die size embedded application complexity microprocessors chip multitasking applications energy efficient shared multitasking loop instruction cache context switch issues i cache access reduction energy savings smlic design;shared memory systems cache storage computational complexity embedded systems energy conservation energy consumption memory architecture microprocessor chips power aware computing	With the exponential increase of power consumption in processor generations, energy dissipation has become one of the most critical constraints in system design. Cache memories are usually the most energy consuming components on the processor chip due to their large die size occupation and frequent access operations. Furthermore, in step with the increased complexity of modern embedded applications, microprocessors are increasingly executing multitasking applications. In multitasking processors, the conventional L1 instruction cache (I-cache) is usually shared by multiple tasks and thereby suffering a highly intensive read/write operations, which can be even more energy-consuming than used in a single-task based system. This paper presents an energy-efficient shared multitasking loop instruction cache (SMLIC), which is designed to address the tasks sharing and context switch issues so that it can be efficiently utilized to reduce the I-cache accesses for energy savings in multitasking processors. Experiments on a set of multitasking applications demonstrate that the proposed SMLIC design scheme can reduce I-cache accesses by 12∼86% and energy consumption in instruction supply by 11∼79% for multitasking system, depending on various frequencies of context switch.	cpu cache;cache (computing);central processing unit;computer multitasking;context switch;embedded system;microprocessor;read-write memory;systems design;time complexity	Ji Gu;Tohru Ishihara;Kyungsoo Lee	2012	2012 IEEE 10th Symposium on Embedded Systems for Real-time Multimedia	10.1109/ESTIMedia.2012.6507036	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;context switch;process	Embedded	-4.843997586774289	55.21371424485775	190532
40480a38bfa487431d029e746c8ab4bbc02f9f46	improving the worst-case execution time accuracy by inter-task instruction cache analysis	cache storage;instruction cache;intertask instruction cache analysis;application software;processor scheduling;computer aided instruction;single task approach;data flow analyses worst case execution time;computational method;data analysis;worst case execution time;task analysis;single task approach worst case execution time accuracy wcet computation method intertask instruction cache analysis;performance analysis;data flow analyses;computer science;data flow;worst case execution time accuracy;task analysis cache storage instruction sets;wcet computation method;cause effect analysis;hard real time;instruction sets;entry and exit;timing real time systems processor scheduling computer aided instruction hardware cause effect analysis data analysis performance analysis computer science application software;hardware;real time systems;timing;time constraint	In hard real-time applications, WCET is used to check time constraints of the whole system but is only computed at the task level. While most WCET computation methods assume a conservative approach to handle the processor state before the execution of a task, the inter-task analysis of long effect hardware facilities should improve the accuracy of the result. As an example, we developed an analysis of a direct-mapped instruction cache behavior, that combines inter-and intra-task instruction cache analysis to estimate more accurately the number of cache misses due to task chaining by considering task entry and exit states along the inter-task analysis. The initial tasks WCET can be computed by any existing single-task approach that models the instruction cache behavior.	cpu cache;computation;real-time computing;real-time web;task analysis;worst-case execution time	Fadia Nemer;Hugues Cassé;Pascal Sainrat;Ali Awada	2007	2007 International Symposium on Industrial Embedded Systems	10.1109/SIES.2007.4297313	data flow diagram;computer architecture;cache-oblivious algorithm;application software;parallel computing;real-time computing;cache coloring;cache;computer science;cache invalidation;operating system;instruction set;task analysis;smart cache;data analysis;cache algorithms;cache pollution;worst-case execution time	Embedded	-7.701674814795745	57.38084125628219	190577
1ef0af57045f4e529ccaed368bcbaf1bdb61bf67	using a victim buffer in an application-specific memory hierarchy	memory-access energy;reduced energy;embedded system design;victim buffer;application-specific memory;direct-mapped cache;traditional cache parameter;performance overhead;design approach;memory hierarchy;cache parameter;application specific integrated circuits;embedded systems	Customizing a memory hierarchy to a particular application or applications is becoming increasingly common in embedded system design, with one benefit being reduced energy. Adding a victim buffer to the memory hierarchy is known to reduce energy and improve performance on average, yet victim buffers are not typically found in commercial embeddedprocessors. One problem with such buffers is, while they work well on average, they tend to hurt performance for many applications. We show that a victim buffer can be very effective if it is considered as a parameter in designing a memory hierarchy, like the traditional cache parameters of total size, associativity, and line size. We describe experiments on PowerStoneand MediaBench benchmarks, showing that having the option of adding a victim buffer to a direct-mapped cache can reduce memory-access energy by a factor of 3 in some cases. Furthermore, even when other cache parameters are configurable, we show that a victim buffer can still reduce energy by 43%. By treating the victim buffer as a parameter, meaning the buffer can be included or excluded, we can avoid performance overhead of up to 4% on some examples. We discuss the victim buffer in the context of both core-based and pre-fabricated platform based design approaches.	embedded system;experiment;memory hierarchy;overhead (computing);systems design;write buffer	Chuanjun Zhang;Frank Vahid	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;write buffer;application-specific integrated circuit	EDA	-6.108753824723307	53.95611480857659	190799
3122fc1baa177380863e022756a6f5338810da7a	improvement of the tolerated raw bit error rate in nand flash-based ssds with the help of embedded statistics		Solid-state drives (SSDs) based on NAND flash memories provide an attractive storage solution as they are faster and less power hungry than traditional hard-disc drives (HDDs). Aggressive storage density improvements in flash memories enabled reductions of the cost per gigabit but also caused reliability degradations. A recent large-scale study revealed that the uncorrectable bit error rates (UBER) in data center SSDs may fall far below the JEDEC standard recommendations. Here, a technique is proposed to improve the tolerated raw bit error rate (RBER) based on the observation that (a) a small SSD ratio may have a much higher RBER than the rest and (b) the RBER is dominated by the retention error rate. Instead of employing stronger but costly error-correcting codes a statistical approach is used to estimate the remaining retention time, i.e., the reliable data storage time, of flash memory pages. This estimation can be performed each time a memory page is read based on the number of detected retention errors and the elapsed time since data was programmed. The fact that the estimated remaining retention time is smaller than a maximum time interval before the next read operation is an indication that data needs to be refreshed. It is estimated that the tolerated RBER can be increased by more than a decade over a storage period of 3 years if the stored data are verified on a monthly basis and refreshed only if necessary. The proposed technique has the ability to adapt the average time between refresh operations to the actual RBER. This enables performance overhead reductions with factors between 8x and 12x as compared to systematic refresh schemes.	areal density (computer storage);code;computer data storage;data center;embedded system;emoticon;flash memory;forward error correction;gigabit;hard disk drive;memory refresh;overhead (computing);paging;residual bit error rate;solid-state drive	Valentin Gherman;Emna Farjallah;Jean-Marc Armani;Marcelino Seif;Luigi Dilillo	2017	2017 IEEE International Test Conference (ITC)	10.1109/TEST.2017.8242066	real-time computing;bit error rate;computer science;adaptability;gigabit;page;nand gate;computer data storage;flash memory;jedec memory standards	Embedded	-11.385786392517547	55.52159005872851	190841
d70e1c126fa8f7b898095ee7085109605c6d073f	leakage aware scheduling for maximum temperature minimization	minimization;leakage;leakage current;processor scheduling;real time;schedules real time systems power demand processor scheduling programming optimal scheduling minimization;dynamic voltage scaling;leakage real time scheduling dynamic voltage scaling;program optimization;power aware computing;scheduling algorithm;optimal scheduling;scheduling;real time scheduling;schedules;semiconductor technology leakage aware scheduling maximum temperature minimization power consumption thermal management optimal constant speed schedule step down scheduling algorithm processor speed dynamic voltage scaling;power consumption;scheduling power aware computing;power demand;programming;thermal management;real time systems	As power consumption continues to increase dramatically in real-time systems, the thermal management has become a prominent issue. Taking leakage current into account, this paper focuses on the maximum temperature minimization for the processor executing a set of real-time tasks with a common deadline. We prove that, for a specific interval, constant-speed schedule applying the lowest constant speed will be superior to any other schedule using higher constant speed in maximum temperature minimization. By dividing the interval into two subintervals, we develop a step-down scheduling algorithm, providing each subinterval a unique processor speed to further reduce the maximum temperature. Compared with the optimal constant-speed schedule, the proposed algorithm significantly reduces the maximum temperature by up to 12%.	algorithm;avionics;clock rate;dynamic voltage scaling;iso 10303;image scaling;real-time clock;real-time computing;schedule (computer science);scheduling (computing);spectral leakage;system integration;thermal management of high-power leds	Jinming Yue;Tiefei Zhang;Licheng Yu;Tianzhou Chen	2011	2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2011.49	embedded system;parallel computing;real-time computing;computer science;operating system;leakage;scheduling	EDA	-5.175039823086277	58.5927343487263	191354
6fc54e3ef6d6ad9d1078e9c9f158a08d0fbf4fbb	quality-driven dynamic scheduling for real-time adaptive applications on multiprocessor systems	voltage control;optimisation;processor scheduling;adaptiveness embedded multiprocessors scheduling quality of service;adaptiveness;runtime;receivers;embedded systems;scheduling;heuristic algorithms;local scaling quality driven dynamic adaptive application scheduling scheme real time adaptive applications multiprocessor systems quality adaptable applications embedded systems optimal quality output runtime slack distribution maximized execution quality dynamic leakage energy constraints slack receiver group heuristic guided search algorithm optimal processor frequency application execution quality slack receiver selection methodology quality maximization interprocessor communications slack inaccuracies transmission variations local scaling approach jpeg2000 codec;video codecs dynamic scheduling embedded systems multiprocessing systems optimisation processor scheduling;embedded multiprocessors;receivers dynamic scheduling heuristic algorithms runtime processor scheduling voltage control;video codecs;multiprocessing systems;quality of service;dynamic scheduling	While quality-adaptable applications are gaining increased popularity on embedded systems (especially multimedia applications), efficient scheduling techniques are necessary to explore this feature to achieve the optimal quality output. In addition to conventional real-time requirements, emerging challenges such as leakage power and multiprocessors further complicate the formulation and solution of adaptive application scheduling problems. In this paper, we propose a dynamic adaptive application scheduling scheme that efficiently distributes the runtime slack to achieve maximized execution quality under timing and dynamic/leakage energy constraints. Our proposed methods are threefold: First, for each task in the slack receiver group, a heuristic guided-search algorithm is proposed to select the optimal processor frequency to maximize the application execution quality. Second, we present an efficient slack receiver selection methodology aiming at identifying optimal slack receivers for quality maximization. Third, our framework is further extended to consider constraints brought by interprocessor communications, where we study the effects of slack inaccuracies introduced by transmission variations, and propose a local scaling approach to compensate the induced quality loss. Experimental results on synthesized tasks and a JPEG2000 codec show that the guided-search algorithm, aided by slack receiver selection, effectively outperforms contemporary approaches with at most 88 percent more quality improvement, whereas the local scaling contributes as large as 16.9 percent on top of the guided-search results.	codec;computation;elegant degradation;embedded system;entropy maximization;heuristic;image scaling;jpeg 2000;multiprocessing;operating system;real-time clock;real-time transcription;requirement;scheduling (computing);search algorithm;slack variable;spectral leakage	Heng Yu;Yajun Ha;Bharadwaj Veeravalli	2013	IEEE Transactions on Computers	10.1109/TC.2012.194	embedded system;parallel computing;real-time computing;quality of service;dynamic priority scheduling;computer science;operating system;least slack time scheduling;scheduling	EDA	-4.938768236879888	58.823411053726964	191376
642f72cdee8f3e9a5275e47cad844e1c54b57b83	energy-aware scheduling for real-time systems: a survey	software;single core;energy;idle;hardware and architecture;sleep;multicore;low power;real time scheduling;dynamic power management;power;dynamic voltage and frequency scaling	This article presents a survey of energy-aware scheduling algorithms proposed for real-time systems. The analysis presents the main results starting from the middle 1990s until today, showing how the proposed solutions evolved to address the evolution of the platform's features and needs. The survey first presents a taxonomy to classify the existing approaches for uniprocessor systems, distinguishing them according to the technology exploited for reducing energy consumption, that is, Dynamic Voltage and Frequency Scaling (DVFS), Dynamic Power Management (DPM), or both. Then, the survey discusses the approaches proposed in the literature to deal with the additional problems related to the evolution of computing platforms toward multicore architectures.	algorithm;dynamic frequency scaling;dynamic voltage scaling;multi-core processor;power management;real-time clock;real-time computing;real-time transcription;scheduling (computing);taxonomy (general);uniprocessor system	Mario Bambagini;Mauro Marinoni;Hakan Aydin;Giorgio C. Buttazzo	2016	ACM Trans. Embedded Comput. Syst.	10.1145/2808231	multi-core processor;embedded system;idle;parallel computing;real-time computing;energy;computer science;operating system;power;sleep	Embedded	-5.525786636071925	59.594799174798986	191421
8d1737d982daaf4053834af7961071e74eb14646	fusion: design tradeoffs in coherent cache hierarchies for accelerators	histograms;protocols;datacenter;acceleration;memory architecture cache storage energy conservation;host accelerator data ping ponging design tradeoffs chip designers fixed function coprocessors multicore designs energy efficiency fine grain offloading functions granularity loops granularity sequential program program region data sharing energy cost wires caches data movement energy benefits fusion lightweight coherent cache hierarchy scratchpad based architecture cpu energy saving temporal coherence accelerator tile hit energy per tile shared cache localized sharing interaccelerator sharing dynamic energy;power management;energy storage;coherence;acceleration histograms coherence protocols	Chip designers have shown increasing interest in integrating specialized fixed-function coprocessors into multicore designs to improve energy efficiency. Recent work in academia [11, 37] and industry [16] has sought to enable more fine-grain offloading at the granularity of functions and loops. The sequential program now needs to migrate across the chip utilizing the appropriate accelerator for each program region. As the execution migrates, it has become increasingly challenging to retain the temporal and spatial locality of the original program as well as manage the data sharing.  We show that with the increasing energy cost of wires and caches relative to compute operations, it is imperative to optimize data movement to retain the energy benefits of accelerators. We develop FUSION, a lightweight coherent cache hierarchy for accelerators and study the tradeoffs compared to a scratchpad based architecture. We find that coherency, both between the accelerators and with the CPU, can help minimize data movement and save energy. FUSION leverages temporal coherence [32] to optimize data movement within the accelerator tile. The accelerator tile includes small per-accelerator L0 caches to minimize hit energy and a per-tile shared cache to improve localized-sharing between accelerators and minimize data exchanges with the host LLC. We find that overall FUSION improves performance by 4.3× compared to an oracle DMA that pushes data into the scratchpad. In workloads with inter-accelerator sharing we save up to 10x the dynamic energy of the cache hierarchy by minimizing the host-accelerator data ping-ponging.	cpu cache;cache coherence;central processing unit;coherence (physics);coprocessor;direct memory access;fixed-function;imperative programming;locality of reference;memory hierarchy;multi-core processor;principle of locality;scratchpad memory	Snehasish Kumar;Arrvindh Shriraman;Naveen Vedula	2015	2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)	10.1145/2749469.2750421	acceleration;communications protocol;data center;computer architecture;parallel computing;real-time computing;coherence;computer hardware;computer science;operating system;histogram;cache algorithms;cache pollution;energy storage	Arch	-6.562642638811795	54.25093739421802	191638
45531101c01f6aadd26d9f58224da7ea9154ba2e	rbwl: recency-based static wear leveling for lifetime extension and overhead reduction in nand flash memory systems			flash memory;wear leveling	Sang-Ho Hwang;Jong Wook Kwak	2018	IEICE Transactions		computer science;computer vision;artificial intelligence;computer hardware;nand gate;wear leveling;flash memory	EDA	-10.962829019890115	54.69656465224144	191902
c6912b946b6a8a1aec2bdacd3d0e04e05b5c4b29	towards the automatic derivation of computer performance models from the real time and embedded systems design	queueing network;simulation techniques automatic derivation computer performance models real time systems design embedded systems design performance constraints queueing network performance model execution time estimation delays logical locking resource use conflicts analytical techniques;performance evaluation;computer performance delay real time systems embedded system testing process design algorithm design and analysis phase estimation computational modeling analytical models;time delay;resource use;performance evaluation real time systems delays systems analysis;design technique;design method;systems analysis;simulation technique;performance model;delays;real time systems;real time and embedded systems;performance modelling	Real time and embedded systems have always performance constraints. With conventional design techniques these constraints are verified at the testing phase and the designer has no performance constraint estimation during the design. From the design information it is possible to deduce a queueing network performance model, but its execution requires supplementary information concerning estimations of the execution times. Delays generated by logical locking and by resource use conflicts can be computed, via analytical or simulation techniques, from such a model. This paper presents experience acquired in the integration of design methods and the pet$onnance modelling techniques.	computer performance;embedded system;lock (computer science);network performance;queueing theory;simulation;systems design;two-phase locking	Ramón Puigjaner;Jacek Szymanski	1994		10.1109/MASCOT.1994.284457	systems analysis;real-time computing;simulation;design methods;computer science	Embedded	-7.477737261905392	59.87959903112962	193474
57bfdb81c7e223b5ec2a532fc1557a229d53924e	minimizing bank conflict delay for real-time embedded multicore systems via bank mapping		Multi-core architectures may meet the increasing performance requirement of real-time systems. However, it is harder to compute the WCET estimation in multi-core platforms due to inter-task interference that tasks suffer when accessing shared hardware resources. In this paper, we propose a finer grained approach to analyze the inter-task interference for multi-core platforms with the TDMA policy and bank-column cache partitioning, and our approach can reasonably estimate inter-task interference delays. Moreover, we make bank-to-core mapping to optimize the interference delays, and develop an algorithm for finding the best bank-to-core mapping. The experimental results show that our interference analysis approach can improve the tightness of interference delays by 14.68% on average compared to Upper Bound Delay (UBD) approach, and the optimized bank-to-core mapping can achieve the WCET improvement by 9.27% on average.	embedded system;multi-core processor;real-time transcription	Zhihua Gan;Mingquan Zhang;Zhimin Gu;Jizan Zhang;Hai Tan	2016		10.1007/978-3-319-52015-5_2	embedded system;real-time computing;engineering;operations management	EDA	-7.896300776971625	58.244928908340185	193766
b0e1085030cb0900b3735ab78198e2fc0d77c88e	energy, power, and performance characterization of gpgpu benchmark programs	energy efficiency;graphics processing units benchmark testing instruction sets runtime energy efficiency hardware power measurement;power aware computing error correction codes graphics processing units;runtime;compute bound codes energy characterization power characterization performance characterization gpgpu benchmark programs gpu energy consumption gpu power draw memory clock frequencies ecc error correcting code k20c gpu memory bound codes;graphics processing units;benchmark testing;power measurement;instruction sets;hardware	This paper studies the effects on energy consumption, power draw, and runtime of a modern compute GPU when changing the core and memory clock frequencies, enabling or disabling ECC, using alternate implementations, and varying the program inputs. We evaluate 34 applications from 5 benchmark suites and measure their power draw over time on a K20c GPU. Our results show that changing the frequency or the program implementation can alter the energy, power, and performance by a factor of two or more. Interestingly, some changes affect these three aspects very unevenly. ECC can greatly increase the runtime and energy consumption, but only on memory-bound codes. Compute-bound codes tend to behave quite differently from memory-bound codes, in particular regarding their power draw. On irregular programs, a small change in frequency can result in a large change in runtime and energy consumption.	benchmark (computing);clock rate;code;computer performance;ecc memory;general-purpose computing on graphics processing units;graphics processing unit;memory bound function;run time (program lifecycle phase)	Jared Coplin;Martin Burtscher	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.164	benchmark;parallel computing;real-time computing;computer hardware;computer science;operating system;instruction set;efficient energy use	Arch	-4.623805721530543	53.92278417319762	193928
0a8ed493c2ffd7d8b8c833abd8e129fab7baa284	energy-efficient scheduling of real-time tasks on heterogeneous multicores using task splitting	processor scheduling;resource management;dvfs real time energy efficient scheduling heterogeneous;energy savings energy efficient scheduling c d task splitting approach single isa heterogeneous multicore system ashm algorithm real time task spliting real time task allocation;multicore processing real time systems processor scheduling schedules scheduling energy consumption resource management;energy consumption;scheduling;multicore processing;schedules;real time systems multiprocessing systems power aware computing processor scheduling;real time systems	In this paper, we investigate the problem of using the state-of-the-art C=D task-splitting approach to energy efficiently schedule real-time tasks on a single-ISA heterogeneous multicore system. We first extend the existing task-splitting approach for heterogeneous multicore systems. Based on our extension, we propose an algorithm, called ASHM, to allocate and split realtime tasks on a heterogeneous multicore system. The experimental results demonstrate the effectiveness of our proposed ASHM algorithm compared to existing allocation approaches in terms of energy savings.	algorithm;multi-core processor;real-time clock;real-time transcription;scheduling (computing);symmetric multiprocessing	Di Liu;Jelena Spasic;Todor Stefanov	2016	2016 IEEE 22nd International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)	10.1109/RTCSA.2016.40	multi-core processor;fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;dynamic priority scheduling;schedule;computer science;rate-monotonic scheduling;resource management;operating system;two-level scheduling;scheduling	Embedded	-6.198094548993629	58.763802489306286	195583
7355c77e5a032ca48fe28af471e0178f45abd8c1	an algorithm for deciding minimal cache sizes in real-time systems	cost saving;cache memory;greedy algorithms;schedulability analysis;memory access;schedulability;greedy algorithm;genetic algorithm;genetic algorithms;real time system;locking cache memory;power consumption;real time systems	When designing real-time systems, predictability is of utmost importance. A locking cache is a cache memory that allows loading and locking instructions, thus avoiding their replacement. This way, regarding memory accesses, execution time of instructions is constant since it does not depend on the sequence of memory references. With a predictable behaviour, locking cache memories are a practical alternative to conventional caches for real-time systems. Offering similar performance to conventional caches, locking caches allow an accurate yet simple schedulability analysis.  Locking caches may also help to reduce the size of a system, by means of reducing cache size. When reducing cache size, also cost and power consumption may be reduced. This way, both predictability and cost saving is provided by means of locking cache.  This work presents a set of algorithms, aimed to select the contents of a locking cache that provides the minimum locking cache size, while the system remains schedulable. Compared to a previous approach, the algorithms presented in this paper are able to select a set of main memory blocks that result in a smaller cache size.	algorithm;cpu cache;computer data storage;lock (computer science);real-time clock;real-time computing;run time (program lifecycle phase);scheduling analysis real-time systems;two-phase locking	Antonio Martí Campoy;Francisco Rodríguez-Ballester;Eugenio Tamura Morimitsu;Rafael Ors Carot	2011		10.1145/2001576.2001733	bus sniffing;least frequently used;pipeline burst cache;cache coherence;mathematical optimization;greedy algorithm;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;real-time operating system;page cache;genetic algorithm;cpu cache;tag ram;cache;computer science;write-once;artificial intelligence;cache invalidation;machine learning;distributed computing;smart cache;mesi protocol;cache algorithms;cache pollution;cache-only memory architecture;non-uniform memory access	Embedded	-6.1927464693627075	55.14500003134135	195710
15b650756ca60f137495cb16ed6b57c1abb0b707	resource allocation for independent real-time tasks in heterogeneous systems for energy minimization	linearization heuristic;energy minimization;task allocation;generalized assignment problem;real-time;real time;greedy heuristic;resource allocation;energy dissipation;upper bound	In recent years, power management and power reduction have become critical issues in portable systems that are designed for real-time use. In this paper, we study the problem of statically allocating a set of independent real-time tasks to a system consisting of heterogeneous processing elements, each enabled with discrete Dynamic Voltage Scaling. The goal is to minimize the overall energy dissipation of the system without violating the real-time requirements of the tasks. The problem is first formulated as an extended Generalized Assignment Problem. A linearization heuristic (LR-heuristic) is then extended to solve the problem. An analysis of the upper bound on the number of tasks that the heuristic may fail to allocate is also presented. Our experiments show that when the average utilization of the system is high, the LR-heuristic achieves 15% off the optimal energy dissipation for small size problems, while the performance of a classic greedy heuristic is around 90% off the optimal. A relative performance improvement of up-to 40% over the classic greedy heuristic is also observed for large size problems. Finally, an analytical performance comparison between the LR-heuristic and the greedy heuristic is presented.	dynamic voltage scaling;energy minimization;experiment;generalized assignment problem;greedy algorithm;heuristic;lr parser;power management;real-time clock;real-time transcription;requirement	Yang Yu;Viktor K. Prasanna	2003	J. Inf. Sci. Eng.			Embedded	-5.8404811591616275	59.22589351198512	195755
0651078a9e4af6a6f885e6356ae40c87b50efd7a	flushing policies for nvcache enabled hard disks	rotating media;flash memory;cache storage;hard disks energy consumption energy management drives operating systems flash memory power system management power system reliability random access memory nonvolatile memory;satisfiability;nonvolatile flash memory;performance improvement;low power;rotating media flushing policies nvcache enabled hard disks power consumption reduction nonvolatile flash memory;hard discs cache storage flash memories;flushing policies;power consumption;hard discs;power consumption reduction;flash memories;nvcache enabled hard disks	"""One of the goals of upcoming hybrid hard disks is to reduce power consumption by adding a small amount of non-volatile flash memory (NVCache) to the drive itself. By using the NVCache to satisfy writes while the rotating media is spun-down, hard disk power consumption can be decreased by lengthening low-power periods. However, the NVCache must eventually be flushed back to the rotating media in order to cache additional data. In this paper we explore two questions: when and how should NVCache content be flushed to rotating media in order to minimize the overhead of data synchronization. We show that by using traditional I/O mechanisms such as merging and reordering, combined with a """"flush only when full"""" policy, flushing performance improves significantly."""	algorithm;allocate-on-flush;data synchronization;disk storage;flash memory;floppy disk;hard disk drive;hard disk drive performance characteristics;input/output;low-power broadcasting;non-volatile memory;overhead (computing)	Timothy Bisson;Scott A. Brandt	2007	24th IEEE Conference on Mass Storage Systems and Technologies (MSST 2007)	10.1109/MSST.2007.16	parallel computing;real-time computing;computer hardware;computer science	Embedded	-10.431575765559664	54.72660529868708	195893
cdf9d4d5539b3078607a452a8f00063788873635	an experimental evaluation of the cache partitioning impact on multicore real-time schedulers	cache storage;processor scheduling;partitioned scheduling shared cache partitioning real time operating systems global scheduling;image color analysis real time systems program processors multicore processing color hardware memory management;data structures;p edf cache partitioning impact multicore real time systems task workloads system predictability multicore processors general purpose os kernel activities interrupt handlers context switching partitioned scheduler shared cache partitioning mechanism multicore component based rtos internal os data structures global multicore real time scheduling algorithms g edf partitioned multicore real time scheduling algorithms;interrupts;multiprocessing systems;operating systems computers;real time systems cache storage data structures interrupts multiprocessing systems operating systems computers processor scheduling;real time systems	Shared cache partitioning is a well-known technique used in multicore real-time systems to isolate task workloads and improve system predictability. Presently, the state-of-the-art studies that evaluate shared cache partitioning on multicore processors lack two key issues. First, the cache partitioning mechanism is typically implemented either in a simulation environment or in a general-purpose OS, and so the impact of kernel activities, such as interrupt handlers and context switching, on the task partitions tend to be overlooked. Second, the evaluation is typically restricted to either a global or partitioned scheduler, thereby by falling to compare the performance of cache partitioning when tasks are scheduled by different schedulers. In this work, we design and implement a shared cache partitioning mechanism in a multicore component-based RTOS capable of assigning partitions to internal OS data structures, including task and system stacks and interrupt handlers data. We evaluate our shared cache partitioning mechanism running task sets under global (G-EDF) and partitioned (P-EDF) multicore real-time scheduling algorithms. Our results indicate that a lightweight RTOS does not impact real-time tasks, and shared cache partitioning has different behavior depending on the scheduler and the task's working set size.	algorithm;cpu cache;cache (computing);cache coloring;central processing unit;color;component-based software engineering;context switch;data structure;earliest deadline first scheduling;general-purpose markup language;graph coloring;h.264/mpeg-4 avc;hardware performance counter;interference (communication);interrupt handler;memory management;motion estimation;multi-core processor;point of sale;real-time clock;real-time computing;real-time operating system;scheduling (computing);simulation;working set size	Giovani Gracioli;Antônio Augusto Fröhlich	2013	2013 IEEE 19th International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2013.6732205	computer architecture;snoopy cache;parallel computing;real-time computing;cache coloring;data structure;cache;computer science;cache invalidation;operating system;interrupt;smart cache;programming language;cache algorithms;cache pollution	Embedded	-9.739445897131317	56.98076651903723	196964
3690b6f7dead6e819db255cdac8bee985e6fbed4	an endurance-aware metadata allocation strategy for mlc nand flash memory storage systems	reliability;error correction codes;shared page nand flash memory mlc reliability metadata;msb pages endurance aware metadata allocation strategy mlc nand flash memory storage systems reliability aware metadata allocation strategy scatter single level cell multiple level cell nand flash memory storage system scatter slc least significant bit pages lsb pages most significant bit pages;error analysis;time factors;ash;ash reliability error correction codes programming time factors error analysis;programming;storage allocation flash memories meta data	This paper presents a reliability-aware metadata allocation strategy called scatter-single-level cell (SLC) for multiple-level cell (MLC) NAND flash memory storage systems. In scatter-SLC, metadata is kept in least significant bit (LSB) pages and corresponding most significant bit (MSB) pages are bypassed. Without partitioning SLC and MLC blocks, scatter-SLC can eliminate the unbalanced lifetime between SLC and MLC blocks while achieving the similar error rate as the method to store metadata in SLC blocks. We implemented scatter-SLC on a real-hardware platform. The experiment results show that scatter-SLC can reduce uncorrectable page errors by 93.54% while incurring less than 1% time overhead on average compared with the previous work.	cell (microprocessor);flash memory;least significant bit;most significant bit;multi-level cell;overhead (computing);unbalanced circuit	Min Huang;Zhaoqing Liu;Liyan Qiao;Yi Wang;Zili Shao	2016	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2474394	programming;parallel computing;real-time computing;computer hardware;computer science;reliability	EDA	-10.906016408750135	54.44620701768944	197293
43e195e8f39f3ed4b20109d9fc4ee1eab1f2a8d2	improving throughput of power-constrained gpus using dynamic voltage/frequency and core scaling	power aware computing cache storage graphics processing units parallel processing;cache storage;core scaling;memory management;dynamic voltage;frequency domain analysis;gpu;power constraint;system on a chip;chip;throughput gpu dynamic voltage frequency core scaling power constraint;power aware computing;graphics processing units;dynamic scaling;graphic processing unit;bandwidth;frequency;graphics processing unit;throughput graphics processing unit system on a chip bandwidth power demand memory management frequency domain analysis;power demand;off chip memory throughput improvement power constrained gpu dynamic voltage frequency scaling core scaling graphic processing units parallel applications on chip interconnects on chip caches;parallel applications;parallel processing;dynamic voltage frequency and core scaling;throughput	State-of-the-art graphic processing units (GPUs) can offer very high computational throughput for highly parallel applications using hundreds of integrated cores. In general, the peak throughput of a GPU is proportional to the product of the number of cores and their frequency. However, the product is often limited by a power constraint. Although the throughput can be increased with more cores for some applications, it cannot for others because parallelism of applications and/or bandwidth of on-chip interconnects/caches and off-chip memory are limited. In this paper, first, we demonstrate that adjusting the number of operating cores and the voltage/frequency of cores and/or on-chip interconnects/caches for different applications can improve the throughput of GPUs under a power constraint. Second, we show that dynamically scaling the number of operating cores and the voltages/frequencies of both cores and on-chip interconnects/caches at runtime can improve the throughput of application even further. Our experimental results show that a GPU adopting our runtime dynamic voltage/frequency and core scaling technique can provide up to 38% (and nearly 20% on average) higher throughput than the baseline GPU under the same power constraint.	baseline (configuration management);computer memory;electrical connection;graphics processing unit;ibm notes;image scaling;norm (social);parallel computing;run time (program lifecycle phase);speaker wire;throughput	Jungseob Lee;Vijay Sathisha;Michael J. Schulte;Katherine Compton;Nam Sung Kim	2011	2011 International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2011.17	chip;system on a chip;parallel processing;throughput;computer architecture;parallel computing;computer hardware;computer science;operating system;frequency;frequency domain;bandwidth;memory management	HPC	-5.112609395108577	54.99572527413756	198051
e1f287fa9e15bd88cb14574c4802fd7a6017c449	low-energy heterogeneous non-volatile memory systems for mobile systems	hand held device;nand flash memory;flash memory;low energy;energy requirement;embedded system;low power;energy consumption;non volatile memory;memory systems;code size;memory allocation;mobile systems;trace driven simulation	Memory systems consume significant energy in hand-held embedded systems. Existing techniques for reducing memory energy requirements in low-power systems have addressed energy consumption when the system is turned on; but we also consider data retention energy during the power-off period. Semiconductor non-volatile memory is indispensable for hand-held devices that cannot afford magnetic disks due to excessive space, weight, cost and energy consumption. Current hand-held systems are generally equipped with more than one type of non-volatile storage device, such as battery-backed SDRAM, NOR Flash memory or NAND Flash memory, because each technology has its distinct and complementary features. In this paper, we introduce an energy-aware memory allocation in heterogeneous non-volatile memory systems to maximize the battery life. For this purpose, we first characterize cycle-accurate active mode energy and the data retention energy of non-volatile memory systems. Next, we present an energy-aware memory allocation for a given task set, taking into account arrival rate, execution time, code size, user data size and the number of memory transactions; we do this using trace-driven simulation. Experiments demonstrate that an optimized allocation can save up to 26% of the memory system energy compared with traditional allocation schemes.	embedded system;experiment;flash memory;ibm power systems;low-power broadcasting;mobile device;non-volatile memory;queueing theory;requirement;run time (program lifecycle phase);semiconductor;shutdown (computing);simulation;software transactional memory;volatile memory	Hyung Gyu Lee;Naehyuck Chang	2005	J. Low Power Electronics	10.1166/jolpe.2005.001	flash file system;auxiliary memory;embedded system;interleaved memory;semiconductor memory;parallel computing;sense amplifier;non-volatile memory;memory refresh;computer hardware;computer science;operating system;volatile memory;computer memory;overlay;non-volatile random-access memory;conventional memory;extended memory;flat memory model;registered memory;memory management	Embedded	-5.734804561207096	55.771389622897146	198067
4ca253decc77736cf19b3b06087aecd99493a970	an efficient time annotation technique in abstract rtos simulations for multiprocessor task migration	real time;real time operating system;embedded system;hard real time	Complex control oriented embedded systems with hard real-time constraints require real-time operation system (RTOS) for predictable timing behavior. To support the evaluation of different scheduling strategies and task priorities, we use an abstract RTOS model based on SystemC. In this article, we present an annotation method for time estimation that supports flexible simulation and validation of real-time-constraints for task migration between different target processors without loss of simulation performance and less memory overhead.	array data structure;assembly language;atmel avr;central processing unit;compile time;compiler;embedded system;hard coding;linear equation;multiprocessing;overhead (computing);real-time clock;real-time computing;real-time operating system;scheduling (computing);simulation;system of linear equations;systemc	Henning Zabel;Wolfgang Müller	2008		10.1007/978-0-387-09661-2_18	embedded system;real-time computing;operating system	Embedded	-8.170599781127393	59.13414220541674	199041
7f6653e70c3860b537354875677722565e707efd	sparta: runtime task allocation for energy efficient heterogeneous manycores	resource management;runtime;mobile web browser;big little;computer architecture;power management;javascript engine;linux;predictive models;quality of service;dvfs;throughput;heterogeneous multi processing	To meet the performance and energy efficiency demands of emerging complex and variable workloads, heterogeneous many-core architectures are increasingly being deployed, necessitating operating systems support for adaptive task allocation to efficiently exploit this heterogeneity in the face of unpredictable workloads. We present SPARTA, a throughput-aware runtime task allocation approach for Heterogeneous Many-core Platforms (HMPs) to achieve energy efficiency. SPARTA collects sensor data to characterize tasks at runtime and uses this information to prioritize tasks when performing allocation in order to maximize energy-efficiency (instructions-per-Joule) without sacrificing performance. Our experimental results on heterogeneous many-core architectures executing mixes of MiBench and PARSEC benchmarks demonstrate energy reductions of up to 23% when compared to state-of-the-art alternatives. SPARTA is also scalable with low overhead, enabling energy savings in large-scale architectures with up to hundreds of cores.	benchmark (computing);joule;manycore processor;operating system;overhead (computing);parsec;performance per watt;run time (program lifecycle phase);scalability;throughput	Bryan Donyanavard;Tiago Mück;Santanu Sarma;Nikil D. Dutt	2016	2016 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)	10.1145/2968456.2968459	parallel computing;real-time computing;computer science;operating system	HPC	-5.090590573177931	54.05340626570712	199265
2b735ada9ff763bd0ecae01666d2001ba0b3a168	dynamic low-power reconfiguration of real-time systems with periodic and probabilistic tasks	probabilistic logic program processors real time systems power demand runtime educational institutions computer architecture;runtime;computer architecture;real time system agent based architecture dynamic reconfiguration edf scheduling low power;agent effectiveness dynamic low power reconfiguration real time systems periodic task probabilistic task hard soft deadline power consumption software agent based architecture intelligent agent temporal parameters virtual processors;software agents power aware computing real time systems;probabilistic logic;power demand;program processors;real time systems	This paper deals with the dynamic low-power reconfiguration of a real-time system. It processes periodic and probabilistic tasks that have hard/soft deadlines corresponding to internal/external events. A runtime event-based reconfiguration scenario is a dynamic operation allowing the addition/removal of the assumed periodic/probabilistic tasks. Thereafter, some tasks may miss their hard deadlines and the power consumption may increase. In order to reconfigure the system to be feasible, i.e., satisfying its real-time constraints with low-power consumption, this research presents a software-agent-based architecture. An intelligent agent is developed, which provides four solutions to reconfigure the system at runtime. For these solutions, in order to reconfigure the probabilistic tasks to be feasible, the agent modifies their temporal parameters dynamically; moreover, in order to feasibly serve the probabilistic tasks and reduce the system's power consumption, the agent provides three virtual processors by dynamically extending the periods of the periodic tasks. A simulation study verifies the effectiveness of the agent.	agent-based model;central processing unit;intelligent agent;low-power broadcasting;real-time clock;real-time computing;real-time operating system;real-time transcription;run time (program lifecycle phase);simulation;software agent	Xi Wang;Imen Khemaissia;Mohamed Khalgui;Zhiwu Li;Olfa Mosbahi;Mengchu Zhou	2015	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2014.2309479	parallel computing;real-time computing;computer science;operating system;distributed computing;probabilistic logic	Embedded	-6.336693615323658	59.55991572518364	199298
728e5c28653c3e46cabfcd4ffbc81e9de39cd25b	fault and timing analysis in critical multi-core systems: a survey with an avionics perspective		With more functionality added to future safety-critical avionics systems, new platforms are required to offer the computational capacity needed. Multi-core processors offer a potential that is promising, but they also suffer from two issues that are only recently being addressed in the safety-critical contexts: lack of methods for assuring timing determinism, and higher sensitivity to permanent and transient faults due to shrinking transistor sizes. This paper reviews major contributions that assess the impact of fault tolerance on worst-case execution time of processes running on a multi-core platform. We consider the classic approach for analyzing the impact of faults in such systems, namely fault injection. The review therefore explores the area in which timing effects are studied when fault injection methods are used. We conclude that there are few works that address the intricate timing effects that appear when inter-core interferences due to simultaneous accesses of shared resources are combined with fault tolerance techniques. We assess the applicability of the methods to currently available multi-core processors used in avionics. Dark spots on the research map of the integration problem of hardware reliability and timing predictability for multi-core avionics systems are identified.	avionics;best, worst and average case;cosmic;central processing unit;certificate authority;computer architecture simulator;experiment;fault injection;fault tolerance;formal methods;interference (communication);multi-core processor;multiprocessing;response time (technology);run time (program lifecycle phase);scheduling (computing);simulation;static timing analysis;transistor;worst-case execution time	Andreas Löfwenmark;Simin Nadjm-Tehrani	2018	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2018.04.001	determinism;real-time computing;fault tolerance;avionics;fault injection;predictability;computer science;static timing analysis;multi-core processor	EDA	-7.151162941067641	58.25620293851113	199389
ddb025fddeb95afe62acbaf11ae948f96e7cf408	efficient two-level scheduling for concurrent graph processing		With the rapidly growing demand of graph processing in the real scene, they have to efficiently handle massive concurrent jobs. Although existing work enable to efficiently handle single graph processing job, there are plenty of memory access redundancy caused by ignoring the characteristic of data access correlations. Motivated such an observation, we proposed two-level scheduling strategy in this paper, which enables to enhance the efficiency of data access and to accelerate the convergence speed of concurrent jobs. Firstly, correlations-aware job scheduling allows concurrent jobs to process the same graph data in Cache, which fundamentally alleviates the challenge of CPU repeatedly accessing the same graph data in memory. Secondly, multiple priority-based data scheduling provides the support of prioritized iteration for concurrent jobs, which is based on the global priority generated by individual priority of each job. Simultaneously, we adopt block priority instead of fine-grained priority to schedule graph data to decrease the computation cost. In particular, two-level scheduling significantly advance over the state-of-the-art because it works in the interlayer between data and systems.	central processing unit;computation;data access;graph (abstract data type);iteration;job scheduler;job shop scheduling;job stream;scheduling (computing);series acceleration;two-level scheduling	Jin Zhao	2018	CoRR		real-time computing;distributed computing;redundancy (engineering);computation;scheduling (computing);two-level scheduling;cache;job scheduler;data access;computer science;convergence (routing)	HPC	-11.17369128179017	57.972419645419826	199410
b0a836c3f186cfc3f9c5f8236590eb172d9920c0	phased tag cache: an efficient low power cache system	low voltage;frequency;algorithm design and analysis;low power electronics;hardware;energy dissipation	In this paper, we propose a low power cache design, namely phased tag cache, for reducing the power consumption of set-associative caches. In the phased tag cache, the tag is compared in two phases. A small part of the tag is compared in the first phase to determine the data way which a memory reference falls into. The remaining bits of the tag are compared in the second phase to verify if the result from the first phase is valid. By doing so we can eliminate most of the unnecessary activities on the entire tag. We used the CACTI cache model to show that the time overhead of the phased tag cache is small. Simulation results based on Spec2000 benchmark applications suggest that the phased tag cache design has small impact on the cache performance. The power model shows the phased tag design reduces the power consumption by 30-50%, compared to conventional caches used by the Itanium2 processor.	benchmark (computing);cpu cache;cache (computing);central processing unit;elegant degradation;itanium;karp's 21 np-complete problems;overhead (computing);simulation;system requirements;tag cloud	Rui Min;Wen-Ben Jone;Yiming Hu	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)		bus sniffing;pipeline burst cache;algorithm design;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;tag ram;computer hardware;cache;computer science;write-once;electrical engineering;dissipation;cache invalidation;frequency;smart cache;low voltage;cache algorithms;cache pollution;quantum mechanics;low-power electronics	Arch	-7.079309914261191	54.632194086131506	199833
