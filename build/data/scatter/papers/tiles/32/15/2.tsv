id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
bfabb927ddc7a122d6759a7dcd96e9fec17a2112	feature extraction of automatic speaker recognition, analysis and evaluation in real environment		An Automatic Speaker Recognition is a biometric system that allows you to identify and verify people, using voice as a discriminatory feature. The purpose of this paper is the feature extraction stage, performing an analysis of effectiveness in real environment. The features extraction has as objective to capture the associated characteristic space of the speaker, being the Mel features and its linear variant the most used methods. In real conditions, the environment over which the speech signal is processed tends not to be ideal, nor is the duration of the speech, so it’s necessary to use robust techniques for assuring a lower degradation grade of system effectiveness; techniques such as Power Normalization, Hilbert Envelope and Modulation of Mean Duration are described, analyzed and evaluated.	feature extraction;speaker recognition	Ermira Begu;Gabriel Hernández;José Ramón Calvo de Lara	2018		10.1007/978-3-030-01132-1_43	modulation;normalization (statistics);speaker recognition;robustness (computer science);feature extraction;biometrics;pattern recognition;artificial intelligence;computer science	Vision	-11.176611504031465	-91.58427875491276	183240
83b6bebcae96f9140e1b30d6e34ea99415538a1e	on the analysis and evaluation of prosody conversion techniques		Voice conversion is a process of modifying the characteristics of source speaker such as spectrum or/and prosody, to sound as if it was spoken by another speaker. In this paper, we study the evaluation of prosody transformation, in particular, the evaluation of Fundamental Frequency (F0) conversion. F0 is an essential prosody feature that should be taken care of in a compressive voice conversion framework. So far, the evaluation of the converted prosody features is performed mainly by looking at Pearson Correlation Coefficient and Root Mean Square Error (RMSE). Unfortunately, these techniques do not explicitly measure the F0 alignment between the source and target signals. We believe that an evaluation measure that takes into account the time alignment of F0 is needed to provide a new perspective. Therefore, in this paper, we study a new technique to assess the accuracy of prosody transformation. In our experiments with different prosody transformation techniques, we report that the proposed evaluation approach achieves consistent results with the baseline evaluation metrics.	baseline (configuration management);care-of address;coefficient;experiment;lazy evaluation;loudspeaker time alignment;semantic prosody;sparse approximation;sparse matrix	Berrak Sisman;Grandee Lee;Haizhou Li;Kay Chen Tan	2017	2017 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2017.8300542	pattern recognition;artificial intelligence;pearson product-moment correlation coefficient;prosody;computer science;fundamental frequency;mean squared error	SE	-9.878218019859752	-88.3353238035506	183300
382dba7d6c645d55ebe9ef6735d502521f3b33dc	features for content-based audio retrieval	audio segmentation;building block;audio retrieval;automatic speech recognition;feature extraction;music information retrieval;taxonomy;audio feature extraction;literature survey;content based retrieval;content based audio features	Today, a large number of audio features exists in audio retrieval for different purposes, such as automatic speech recognition, music information retrieval, audio segmentation, and environmental sound retrieval. The goal of this paper is to review latest research in the context of audio feature extraction and to give an application-independent overview of the most important existing techniques. We survey state-of-the-art features from various domains and propose a novel taxonomy for the organization of audio features. Additionally, we identify the building blocks of audio features and propose a scheme that allows for the description of arbitrary features. We present an extensive literature survey and provide more than 200 references to relevant high quality publications.	audio signal processing;display resolution;feature extraction;information retrieval;speech recognition	Dalibor Mitrovic;Matthias Zeppelzauer;Christian Breiteneder	2010	Advances in Computers	10.1016/S0065-2458(10)78003-7	audio mining;speech recognition;feature extraction;computer science;acoustic model;multimedia;information retrieval;taxonomy	AI	-6.294298822823869	-91.5298747226787	184048
20a680451909f387934096eb8dfbbac116b0e74d	audio surveillance using a bag of aural words classifier	video surveillance;audio signal processing;audio event detection algorithm audio surveillance bag of aural words classifier;support vector machines;vectors training noise measurement support vector machines feature extraction event detection computer architecture;training;video surveillance audio signal processing;event detection;noise measurement;computer architecture;vectors;feature extraction	In this paper we propose a novel approach for the audio-based detection of events. The approach adopts the bag of words paradigm, and has two main advantages over other techniques present in the literature: the ability to automatically adapt (through a learning phase) to both short, impulsive sounds and long, sustained ones, and the ability to work in noisy environments where the sounds of interest are superimposed to background sounds possibly having similar characteristics. The proposed method has been experimentally validated on a large database of sounds, including several kinds of background noise, which are superimposed to the sounds to be recognized. The obtained performance has been compared with the results of another audio event detection algorithm from the literature, showing a significant improvement.	algorithm;bag-of-words model;categorization;document classification;experiment;learning vector quantization;programming paradigm;statistical classification	Vincenzo Carletti;Pasquale Foggia;Gennaro Percannella;Alessia Saggese;Nicola Strisciuglio;Mario Vento	2013	2013 10th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2013.6636620	support vector machine;computer vision;speech recognition;audio signal processing;feature extraction;computer science;noise measurement;machine learning;pattern recognition	Vision	-5.664563345980997	-89.22253535565378	184883
6a75d9733da408334df900ca50332eaf5ea6a056	singer identification based on computational auditory scene analysis and missing feature methods	reconstruction;computational auditory scene analysis casa;singer identification;marginalization	A major challenge for the identification of singers from monaural popular music recording is to remove or alleviate the influence of accompaniments. Our system is realized in two stages. In the first stage, we exploit computational auditory scene analysis (CASA) to segregate the singing voice units from a mixture signal. First, the pitch of singing voice is estimated to extract the pitch-based features of each unit in an acoustic vector. These features are then exploited to estimate the binary time-frequency (T-F) masks, where 1 indicates that the corresponding T-F unit is dominated by the singing voice, and 0 indicates otherwise. These regions dominated by the singing voice are considered reliable, and other units are unreliable or missing. Thus the acoustic vector is incomplete. In the second stage, two missing feature methods, the reconstruction of acoustic vector and the marginalization, are used to identify the singer by dealing with the incomplete acoustic vectors. For the reconstruction of acoustic vector, the complete acoustic vector is first reconstructed and then converted to obtain the Gammatone frequency cepstral coefficients (GFCCs), which are further used to identify the singer. For the marginalization, the probabilities that the voice belonging to a certain singer are computed on the basis of only the reliable components. We find that the reconstruction method outperforms the marginalization method, while both methods have significantly good performances, especially at signal-to-accompaniment ratios (SARs) of 0 dB and − 3 dB, in contrast to another system.	acoustic cryptanalysis;cepstrum;coefficient;computational auditory scene analysis;horner's method;performance	Ying Hu;Guizhong Liu	2013	Journal of Intelligent Information Systems	10.1007/s10844-013-0271-6	speech recognition;social exclusion	ML	-10.572769870026407	-90.81963416119945	185948
9c8bb7ba51f92e961da1da73396328057c1f9db0	novel deep autoencoder features for non-intrusive speech quality assessment	neural networks;speech processing;speech;noise measurement;quality assessment;feature extraction;signal processing algorithms	To emulate the human perception in quality assessment, an objective metric or assessment method is required, which is a challenging task. Moreover, assessing the quality of speech without any reference or the ground truth is altogether more difficult. In this paper, we propose a new non-intrusive speech quality assessment metric for objective evaluation of speech quality. The originality of proposed scheme lies in using deep autoencoder to extract low-dimensional features from a spectrum of the speech signal and finds a mapping between features and subjective scores using an artificial neural network (ANN). We have shown that autoencoder features capture noise information in a better way than state-of-the-art Filterbank Energies (FBEs). Quantification of our experimental results suggests that proposed metric gives more accurate and correlated scores than an existing benchmark for objective, non-intrusive quality assessment metric ITU-T P.563 standard.	acoustic cryptanalysis;artificial neural network;autoencoder;benchmark (computing);channel (communications);filter bank;ground truth;image noise;mel-frequency cepstrum	Meet H. Soni;Hemant A. Patil	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760662	speech recognition;computer science;machine learning;pattern recognition	NLP	-10.761242975126118	-89.55149694347433	186478
c8d1eeb3a278e09804476049fd86d45d3f3c82af	reducing the computational complexity of two-dimensional lstms		Long Short-Term Memory Recurrent Neural Networks (LSTMs) are good at modeling temporal variations in speech recognition tasks, and have become an integral component of many state-of-the-art ASR systems. More recently, LSTMs have been extended to model variations in the speech signal in two dimensions, namely time and frequency [1, 2]. However, one of the problems with two-dimensional LSTMs, such as Grid-LSTMs, is that the processing in both time and frequency occurs sequentially, thus increasing computational complexity. In this work, we look at minimizing the dependence of the Grid-LSTM with respect to previous time and frequency points in the sequence, thus reducing computational complexity. Specifically, we compare reducing computation using a bidirectional Grid-LSTM (biGrid-LSTM) with non-overlapping frequency sub-band processing, a PyraMiD-LSTM [3] and a frequency-block Grid-LSTM (fbGrid-LSTM) for parallel time-frequency processing. We find that the fbGrid-LSTM can reduce computation costs by a factor of four with no loss in accuracy, on a 12,500 hour Voice Search task.	analysis of algorithms;computation;computational complexity theory;long short-term memory;neural networks;recurrent neural network;speech recognition	Bo Li;Tara N. Sainath	2017			machine learning;pattern recognition;computational complexity theory;computer science;artificial intelligence	NLP	-11.008256099151396	-90.42066503603286	187766
26aedfb5d0900a1f85fc208d1cda9408871820d8	a versatile speech enhancement system based on perceptual wavelet denoising	feedforward systems;feed forward;degradation;speech intelligibility;pwt;feedforward;average normalized time frequency energy unvoiced speech enhancement system perceptual wavelet denoising psychoacoustic model feedforward subsystem thresholding speech signal intelligibility perceptual wavelet transform pwt soft threshold based denoising scheme;speech processing;time frequency;soft threshold based denoising scheme;speech analysis;average normalized time frequency energy;speech enhancement;wavelet transforms;wavelet transform;psychoacoustic model;unvoiced speech enhancement system;signal processing;noise reduction;perceptual wavelet denoising;feedforward subsystem thresholding;signal denoising speech enhancement wavelet transforms feedforward speech intelligibility;perceptual wavelet transform;psychoacoustic models;subjective evaluation;speech enhancement noise reduction psychoacoustic models feedforward systems wavelet transforms time frequency analysis speech processing signal processing speech analysis degradation;time frequency analysis;speech signal intelligibility;wavelet denoising;signal denoising	This paper presents a new speech enhancement system. A psychoacoustic model is incorporated into the wavelet denoising technique to combat different adverse noise conditions. The system is composed of a feed-forward subsystem connected to the perceptual wavelet transform (PWT) processor, a soft-threshold based denoising scheme, and an unvoiced speech enhancement. The noisy speech is first decomposed into critical bands by the PWT from which a set of weights is extracted by the feed-forward subsystem. The average normalized time-frequency energy is used to guide the feedforward subsystem thresholding to reduce the stationary noise while the non-stationary and correlated noises are reduced by an improved wavelet denoising technique with soft-thresholding. Finally, the unvoiced speech enhancement is applied to further improve the intelligibility of the processed speech signal. Simulation results showed that this new system is capable of reducing noise with little speech degradation and the overall performance is superior to several competitive methods in both objective and subjective evaluations.	acoustic cryptanalysis;coefficient;critical band;display resolution;elegant degradation;feedforward neural network;intelligibility (philosophy);noise reduction;psychoacoustics;simulation;speech enhancement;stationary process;thresholding (image processing);wavelet transform	Yu Shao;Chip Hong Chang	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464725	computer vision;speech recognition;time–frequency analysis;computer science;signal processing;speech processing;feed forward;wavelet transform	EDA	-8.671784620165365	-88.50092015511241	188987
992899993b2b1ebd4b25ef209becb7c11363c59d	music scene-adaptive harmonic dictionary for unsupervised note-event detection	databases;harmonic atom;matching pursuit algorithms;iterative method;analisis armonico;music visualization;audio signal processing;instruments;acoustic properties;signal detection audio signal processing iterative methods music;instrumentation;transcription automatique;detection signal;automatic music transcription matching pursuits sparse decomposition harmonic atom spectral envelope music visualization;dictionaries music multiple signal classification instruments power system harmonics visualization layout acoustic signal detection matching pursuit algorithms databases;event analysis;classification non supervisee;taux erreur;instrumentacion;persecusion adaptativa;signal detection;poursuite adaptative;document musical;armonica;musical score;appareillage;acoustic signal processing;harmonic;documento musical;event detection;layout;high precision;base de donnees audio;musical instruments;transcripcion automatica;metodo iterativo;musical sound;iterative methods;automatic music transcription music scene adaptive harmonic dictionary unsupervised note event detection harmonic decompositions sparse decomposition spectral envelope;matching pursuits;visualization;dictionnaire;spectral envelope;multiple signal classification;deteccion senal;analyse d evenement;musical instrument;analyse harmonique;instrumento musical;harmonique;sparse decomposition;harmonic decompositions;traitement signal audio;representation signal;methode iterative;precision elevee;clasificacion no supervisada;signal representation;dictionaries;signal classification;instrument musique;precision elevada;acoustic signal detection;classification signal;unsupervised classification;error rate;unsupervised note event detection;matching pursuit;traitement signal acoustique;audio databases;son musical;propiedad acustica;power system harmonics;music scene adaptive harmonic dictionary;automatic transcription;indice error;diccionario;music;music transcription;automatic music transcription;propriete acoustique	Harmonic decompositions are a powerful tool dealing with polyphonic music signals in some potential applications such as music visualization, music transcription and instrument recognition. The usefulness of a harmonic decomposition relies on the design of a proper harmonic dictionary. Music scene-adaptive harmonic atoms have been used with this purpose. These atoms are adapted to the musical instruments and to the music scene, including aspects related with the venue, musician, and other relevant acoustic properties. In this paper, an unsupervised process to obtain music scene-adaptive spectral patterns for each MIDI-note is proposed. Furthermore, the obtained harmonic dictionary is applied to note-event detection with matching pursuits. In the case of a music database that only consists of one-instrument signals, promising results (high accuracy and low error rate) have been achieved for note-event detection.	acoustic cryptanalysis;algorithm;cluster analysis;covox speech thing;dictionary;host media processing;iterative method;jaquet-droz automata;list of online music databases;midi;microphone;module file;music visualization;non-negative matrix factorization;numerical stability;online and offline;oracle database;pattern language;sampling (signal processing);scott continuity;source separation;stellar classification;transcription (software);unsupervised learning;venue (sound system)	Julio J. Carabias-Orti;Pedro Vera-Candeas;Francisco J. Cañadas-Quesada;Nicolás Ruiz-Reyes	2010	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2009.2038824	speech recognition;acoustics;computer science;harmonic analysis;iterative method	ML	-7.659857656601728	-92.83238667487446	189391
435d0cd56ce093d170846202dbaca007b7199a41	enhanced local feature approach for overlapping sound event recognition	databases;spectrogram;training;speech;signal classification audio signal processing hough transforms;feature extraction;transforms;nonstationary background noise enhanced local feature based approach overlapping sound event recognition single channel audio local spectrogram features lsf local spectral representation generalised hough transform voting system ght;spectrogram training noise transforms speech feature extraction databases;noise	In this paper, we propose a feature-based approach to address the challenging task of recognising overlapping sound events from single channel audio. Our approach is based on our previous work on Local Spectrogram Features (LSFs), where we combined a local spectral representation of the spectrogram with the Generalised Hough Transform (GHT) voting system for recognition. Here we propose to take the output from the GHT and use it as a feature for classification, and demonstrate that such an approach can improve upon the previous knowledge-based scoring system. Experiments are carried out on a challenging set of five overlapping sound events, with the addition of non-stationary background noise and volume change. The results show that the proposed system can achieve a detection rate of 99% and 91% in clean and 0dB noise conditions respectively, which is a strong improvement over our previous work.	experiment;generalised hough transform;map;random forest;spectrogram;stationary process;statistical classification	Jonathan William Dennis;Tran Huy Dat	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041646	computer vision;speech recognition;computer science;pattern recognition	Vision	-10.958765563603938	-91.5722125243283	189396
b1b9d291bc0be49c2ca0ad61e56ecb0257c0d6ac	a composite source model for speaker and isolated word recognition	data compression;speech processing;speech analysis;linear predictive coding;stochastic processes;random process;switches stochastic processes speech recognition data compression random processes speech processing speech analysis linear predictive coding performance analysis production systems;indexation;random processes;performance analysis;word recognition;speech recognition;production systems;time domain;point of view;switches;speech production	A composite source is an indexed family of random processes (subsources) together with a switch which chooses from among these processes in a stochastic fashion. Such a source has often been proposed as a model for speech and other processes having piece-wise, or quasi, stationary behavior. Until recently, however, very little has been known about such models from either a theoretical or a practical perspective. in this paper, we consider a speaker/isolated word recognition system derived from a composite source model for speech production. In particular, estimates of the underlying subsources are obtained using a modified data compression algorithm. Switch sequences are then derived from these estimates for each utterance. Finally, switch sequences are compared in the time domain (using Levenshtein 's metric) and from a statistical point of view (via variation distance). Both modes of comparison are seen to be highly correlated and produce a recognition procedure with very encouraging results	algorithm;data compression;levenshtein automaton;network switch;open-source software;stationary process;stochastic process	Robert J. Fontana;Michael S. Fox	1981		10.1109/ICASSP.1981.1171128	data compression;natural language processing;stochastic process;speech production;linear predictive coding;speech recognition;network switch;word recognition;time domain;computer science;production system;statistics	ML	-11.179435731349129	-87.30515605331298	189694
6bb6275665384e2e164be05c9ff23dc200a0dea6	predicting face movements from speech acoustics using spectral dynamics	image motion analysis;speech synthesis;facial animation speech enhancement autocorrelation spatial databases speech synthesis acoustic measurements nonlinear acoustics neuroscience ear filters;speech processing;dynamic model;acoustic correlation;feature extraction;feature extraction speech processing speech synthesis acoustic correlation spectral analysis filtering theory image motion analysis;spectral analysis;feature extraction face movement prediction speech acoustics spectral dynamics autocorrelation causal filter noncausal filter dynamic features qualisys system acoustic features visual speech synthesis;filtering theory	This paper introduces a new dynamical model which enhances the relationship between face movements and speech acoustics. Based on the autocorrelation of the acoustics and of the face movements, a causal and a non-causal filter are proposed to approximate dynamical features in the speech signals. The database consisted of sentences recorded acoustically, and by using a Qualisys system to capture face movements with 20 reflectors put on the face, simultaneously. Speech signals are represented by 16-order LSPs and log-energy. With the filtered dynamical features, the acoustic features account for more than 80% of the variance of face movements.	acoustic cryptanalysis;approximation algorithm;autocorrelation;causal filter	Jintao Jiang;Abeer Alwan;Lynne E. Bernstein;Edward T. Auer;Patricia A. Keating	2002		10.1109/ICME.2002.1035748	speech recognition;feature extraction;computer science;speech processing;speech synthesis	Vision	-11.135415167302666	-89.78357088163095	189965
6de761eb25eaf7986094a4734fe0e52e9b5d6232	periodicity transforms	acoustic signal processing;signal processing;transforms;pattern recognition;spectral analysis;astronomical techniques;music	This paper presents a method of detecting periodicities in data that exploits a series of projections onto “periodic subspaces.” The algorithm finds its own set of nonorthogonal basis elements (based on the data), rather than assuming a fixed predetermined basis as in the Fourier, Gabor, and wavelet transforms. A major strength of the approach is that it is linearin-period rather than linear-in-frequency or linear-in-scale. The algorithm is derived and analyzed, and its output is compared to that of the Fourier transform in a number of examples. One application is the finding and grouping of rhythms in a musical score, another is the separation of periodic waveforms with overlapping spectra, and a third is the finding of patterns in astronomical data. Examples demonstrate both the strengths and weaknesses of the method.	algorithm;quasiperiodicity;sensor;wavelet transform	William A. Sethares;T. W. Staley	1999	IEEE Trans. Signal Processing	10.1109/78.796431	arithmetic;speech recognition;computer science;signal processing;music;mathematics;algorithm	Visualization	-8.119519693523792	-91.59831652984475	190397
a02a4068814607c9b58d7aec0ad9bc26ee707a76	automatic classification of musical genres using inter-genre similarity	music genre classification;texture features;feature space;mel frequency cepstral coefficient;musical genre classification;feature extraction;music information retrieval;pattern classification;information retrieval systems;inter genre similarity igs modeling;genre classification;automatic classification;mel frequency cepstral coefficients mfcc;music	Musical genre classification is an essential tool for music information retrieval systems and it has potential to become a highly demanded application in various media platforms. Two important problems of the automatic musical genre classification are feature extraction and classifier design. In this letter, we propose two novel classifiers using inter-genre similarity (IGS) modeling and investigate the use of dynamic timbral texture features in order to improve automatic musical genre classification performance. Inter-genre similarity is modeled over hard-to-classify samples of the musical genre feature space. In the classification, samples within inter-genre similarity class are eliminated to reduce inter-genre confusion and to improve genre classification performance. Experimental results show that the proposed classifiers provide better classification rates than the existing methods.	feature extraction;feature vector;information retrieval	Ulas Bagci;Engin Erzin	2007	IEEE Signal Processing Letters	10.1109/LSP.2006.891320	speech recognition;feature extraction;computer science;machine learning;pattern recognition;music	Web+IR	-8.794785270650912	-92.12884838618196	193100
7391fcca555150a9ebfb5b44b406c92d481fcae3	a novel spoken keyword spotting system using support vector machine	spoken keyword detection;misclassification rate;spoken keyword spotting;mel frequency cepstral coefficients;support vector machine	Spoken keyword spotting is crucial to classify expertly a lot of hours of audio stuffing such as meetings and radio news. These systems are technologically advanced with the purpose of indexing huge audio databases or of differentiating keywords in uninterrupted speech streams. The proposed work involves sliding a frame-based keyword template along the speech signal and using support vector machine (SVM) misclassification rates obtained from the hyperplane of two classes efficiently search for a match. This work framed a novel spoken keyword detection algorithm. The experimental results show that the proposed approach competes with the keyword detection methods described in the literature and it is an alternative technique to the prevailing keyword detection approaches. & 2014 Elsevier Ltd. All rights reserved.	algorithm;database;support vector machine	J. Sangeetha;S. Jothilakshmi	2014	Eng. Appl. of AI	10.1016/j.engappai.2014.07.014	natural language processing;support vector machine;speech recognition;computer science;machine learning;pattern recognition;mel-frequency cepstrum	AI	-9.620629755293463	-91.67562638940028	193457
fba90bb9be7bee47a948cef01d77605fa15f32c0	gender identification using a general audio classifier	indexing mel frequency cepstral coefficient signal processing statistics speech recognition neural networks robustness audio compression automatic speech recognition context modeling;speech signal;audio visual data audio classifier gender identification content based multimedia indexing speech signal spectrum statistics neural networks audio compression;audio signal processing;neural networks;neural nets;indexing speech recognition neural nets audio signal processing spectral analysis;video indexing;mel frequency cepstral coefficient;automatic speech recognition;first order;indexing;content based multimedia indexing;gender identification;signal processing;statistics;speech recognition;robustness;audio visual;multimedia indexing;spectral analysis;spectrum statistics;audio classifier;context modeling;audio compression;audio visual data;neural network	In the context of content-based multimedia indexing gender identification using speech signal is an important task. Existing techniques are dependent on the quality of the speech signal making them unsuitable for the video indexing problems. In this paper we introduce a novel gender identification approach based on a general audio classifier. The audio classifier models the audio signal by the first order spectrum’s statistics in 1s windows and uses a set of neural networks as classifiers. The presented technique shows robustness to adverse audio compression and it is language independent. We show how practical considerations about the speech in audio-visual data, such as the continuity of speech, can further improve the classification results which attain 92%.	artificial neural network;data compression;microsoft windows;scott continuity	Hadi Harb;Liming Chen	2003		10.1109/ICME.2003.1221721	natural language processing;search engine indexing;audio mining;dynamic range compression;speech recognition;audio signal processing;computer science;machine learning;speech coding;pattern recognition;first-order logic;speech processing;acoustic model;context model;artificial neural network;robustness	ML	-8.824780259321615	-92.01224147280368	194649
7d968ae1382033d248e3d1c3df299602cb0eab21	a perception and pde based nonlinear transformation for processing spoken words		Speech signals are often produced or received in the presence of noise, which is known to degrade the performance of a speech recognition system. In this paper, a perceptionand PDE-based nonlinear transformation was developed to process spoken words in noisy environment. Our goal is to distinguish essential speech features and suppress noise so that the processed words are better recognized by a computer software. The nonlinear transformation was made on the spectrogram (short-term Fourier spectra) of speech signals, which reveals the signal energy distribution in time and frequency. The transformation reduces noise through time adaptation (reducing temporally slowly varying portions of spectra) and enhances spectral peaks (formants) by evolving a focusing quadratic fourth-order PDE. Short-term spectra of speech signals were initially divided into three (low, mid and high) frequency bands based on the critical bandwidth of human audition. An algorithm was developed to trace the upper and lower intensity envelopes of signal in each band. The difference between the upper and lower envelopes reflects the signal-to-noise (SNR) ratio of each band. Constant, low SNR signals in each band were adaptively decreased to reduce noise. Then evolution of the focusing PDE was used to enhance the spectral peaks, and further reduce noise interference. Numerical results on noisy spoken words indicated that the transformed spectral pattern of the spoken words was insensitive to noise for SNR ranging from 0 to 20 dB (decibel). The spectral distances between noisy words and original words decreased after the transformation. A numerical experiment was performed on 11 spoken words at SNR = 5 dB. A noisy word is recognized numerically by computing the closest L2 spectral distance from the clean template. The experiment reached a recognition rate as high as 100%. Analyses on the properties of the transformation are provided. © 2001 Elsevier Science B.V. All rights reserved. MSC:35L60; 35R35; 41A60; 65D30	algorithm;critical band;decibel;experiment;frequency band;image noise;interference (communication);nonlinear system;numerical analysis;numerical method;signal-to-noise ratio;spectrogram;speech recognition;temporal logic	Yingyong Qi;Jack Xin	2000			speech recognition;nonlinear system;natural language processing;computer science;perception;artificial intelligence	ML	-10.686686849881355	-90.3908676946754	196009
42c8edb80a0df6c2a91d2b5eb7dcb1b96b9d2b4d	robust sound event classification by using denoising autoencoder	spectrogram;noise measurement;mel frequency cepstral coefficient;feature extraction;image reconstruction;noise reduction;robustness	Over the last decade, a lot of research has been done on sound event classification. But a main problem with sound event classification is that the performance sharply degrades in the presence of noise. As spectrogram-based image features and denoising auto encoder reportedly have superior performance in noisy conditions, this paper proposes a new robust feature called denoising auto encoder image feature (DIF) for sound event classification which is an image feature extracted from an image-like representation produced by denoising auto encoder. Performance of the feature is evaluated by a classification experiment using a SVM classifier on audio examples with different noise levels, and compared with that of baseline features including mel-frequency cepstral coefficients (MFCC) and spectrogram image feature. The proposed DIF demonstrates better performance under noise-corrupted conditions.	a library for support vector machines;autoencoder;baseline (configuration management);coefficient;data integrity field;encoder;feature (computer vision);mel-frequency cepstrum;noise reduction;simulation;spectrogram;statistical classification;support vector machine;vii;window function	Jianchao Zhou;Liqun Peng;Xiaoou Chen;Deshun Yang	2016	2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2016.7813376	iterative reconstruction;computer vision;speech recognition;feature extraction;computer science;noise measurement;spectrogram;pattern recognition;noise reduction;mathematics;feature;robustness	Vision	-10.796554534585965	-92.48557285543794	197429
d138297989d4d226fe83656823ddde0ec56e2e86	similarity based join over audio feeds in a multimedia data stream management system	speaker recognition similarity based join over audio feeds multimedia data stream management system data processing video feeds dynamic multimedia data streams security threats audio feed processing signal parameters homomorphic processing method cepstral analysis speech recognition systems digitized voice samples mmdsms;speech processing;data communication;speaker recognition audio streaming cepstral analysis;speaker recognition;streaming media;multimedia communication;multimedia databases;streaming media multimedia communication speech processing multimedia databases speaker recognition data communication	Over the last several years, processing of high performance data streams has become very important in various domains. A new type of data processing is needed for applications where input data streams are modeled as multimedia data streams, such as audio and video feeds. For example, in the public safety sector, monitoring and automatic identification of particular individuals suspected of terrorist or criminal activity requires the processing of complex audio and video streams, which is beyond the capabilities of a typical data stream management system (DSMS). The concept of a multimedia data stream management system (MMDSMS) has recently been introduced in order to effectively process continuous queries over dynamic multimedia data streams. In this paper, we address MMDSMS functionalities related to speaker recognition problems in the area of detecting individuals who may pose security threats. We focus on audio feed processing using our novel similarity-based join and on parameterization of the multimedia signal for the process of recognition. We propose a set of signal parameters which a clearly discriminate among individual voices by describing the signal using a homomorphic processing method. Our research was primarily focused on assessing the applicability of cepstral analysis in speech recognition systems, based on a set of acquired digitized voice samples. We developed a research prototype to assess the proposed concepts, and verified the effectiveness of our framework in a lab environment.	automatic identification and data capture;bibliographic database;cepstrum;contextual query language;experiment;matlab;prototype;sensor;speaker recognition;speech recognition;streaming media;text corpus	Rafal Maison;Ewelina Majda;Andrzej P. Dobrowolski;Maciej Zakrzewicz	2013	Bell Labs Technical Journal	10.1002/bltj.21599	speaker recognition;speaker diarisation;audio mining;speech recognition;computer science;speech processing;multimedia	DB	-8.7132518853397	-92.46172457119054	197843
32418c544396ba796deb52767a3dd2f671e883ab	using audio-visual information to understand speaker activity: tracking active speakers on and off screen		We present a system that associates faces with voices in a video by fusing information from the audio and visual signals. The thesis underlying our work is that an extreme simple approach to generating (weak) speech clusters can be combined with strong visual signals to effectively associate faces and voices by aggregating statistics across a video. This approach does not need any training data specific to this task and leverages the natural coherence of information in the audio and visual streams. It is particularly applicable to tracking speakers in videos on the web where a priori information about the environment (e.g., number of speakers, spatial signals for beamforming) is not available.	beamforming;computer cluster;powered speakers	Ken Hoover;Sourish Chaudhuri;Caroline Pantofaru;Ian Sturdy;Malcolm Slaney	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461891	task analysis;streams;speech recognition;image segmentation;visualization;beamforming;artificial intelligence;pattern recognition;training set;computer science	Robotics	-11.799253249886181	-92.28270414148452	198160
f64011b6c25c9ad6f523bcc000bf0b368d37894a	real time pattern based melodic query for music continuation system		This paper presents a music continuation system using pattern matching to find patterns within a library of MIDI files using a realtime algorithm to build a system which can be used as interactive DJ system. This paper also looks at the influence of different kinds of pattern matching on MIDI file analysis. Many pattern-matching algorithms have been developed for text analysis, voice recognition and Bio-informatics but as the domain knowledge and nature of the problems are different these algorithms are not ideally suitable for real time MIDI processing for interactive music continuation system. By taking patterns in real-time, via MIDI keyboard, the system searches patterns within a corpus of MIDI files and continues playing from the user's musical input. Four different types of pattern matching are used in this system (i.e. exact pattern matching, reverse pattern matching, pattern matching with mismatch and combinatorial pattern matching in a single system). After computing the results of the four types of pattern matching of each MIDI file, the system compares the results and locates the highest pattern matching possibility MIDI file within the library.		Sanjay Majumder;Benjamin D. Smith	2018		10.1145/3243274.3243283	melody;continuation;pattern matching;domain knowledge;music information retrieval;midi;speech recognition;computer science	HPC	-6.641342486515481	-93.1584358401233	198609
