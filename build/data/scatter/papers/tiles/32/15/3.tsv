id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
627527665cd7661a6cecde744bb025d34b822c60	the relation between larynx height and f0 during the four tones of mandarin in x-ray movie	mandarin tones;larynx speech motion pictures x ray imaging correlation shape fluctuations;tones;speech synthesis;motion pictures;x ray imaging;fluctuations;vertical larynx movement;mandarin;biological organs;vocal tract;biomechanics;speech;biomedical imaging;natural languages;monosyllables;speech synthesis larynx height mandarin tones x ray movie vertical larynx movement vocal frequency change monosyllables vertical laryngeal position articulatory model vocal tract;vertical laryngeal position;larynx;shape;articulatory model;vocal frequency change;larynx height;speech biological organs biomechanics biomedical imaging diagnostic radiography natural languages;laryngeal position;x ray laryngeal position tones mandarin;correlation;x ray;x ray movie;diagnostic radiography;x rays	The relation between vertical larynx movement and vocal frequency (F0) change has attracted the attention of many researchers. This paper studies the vertical laryngeal position (larynx height) during the four tones of Mandarin, based on the X-ray movie data from one female speaker, with 36 monosyllables in tones. The location and movement of the larynx have been measured, and compared with the variation of the F0 during the four tones. Results show that, during the first tone, the larynx height is weakly correlative with F0, while during the other three tones the larynx height is positively correlative with F0. This suggests that the upward movement of larynx is partially (in tone 1) independent of F0, while the downward movement is accompanied by decreasing F0. And the quantitive mechanism of larynx height and F0 during the four tones can be integrated into articulatory model of vocal tract in Mandarin for better speech synthesis.	speech synthesis;star height;super robot monkey team hyperforce go!;tract (literature)	Gaowu Wang;Jiangping Kong	2010	2010 7th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2010.5684877	vocal tract;speech recognition;mandarin chinese;shape;speech;biomechanics;linguistics;natural language;speech synthesis;correlation	Arch	-9.30692170654152	-83.14968350372341	182674
f1ec48b73339608547bad4d200441e03623be5d0	non-audible murmur (nam) recognition	non audible murmur recognition;interfase usuario;tecnologia electronica telecomunicaciones;nam;user interface;wearable computers;microfono;wearable comuting;automatic recognition;reconocimiento voz;interface;speech recognition;interface utilisateur;reconnaissance parole;wearable computing;tecnologias;grupo a;non audible murmur;reconocimiento automatico;reconnaissance automatique;ordinateur vetement;microphone	We propose a new practical input interface for the recognition of Non-Audible Murmur (NAM), which is defined as articulated respiratory sound without vocal-fold vibration transmitted through the soft tissues of the head. We developed a microphone attachment, which adheres to the skin, by applying the principle of a medical stethoscope, found the ideal position for sampling flesh-conducted NAM sound vibration and retrained an acoustic model with NAM samples. Then using the Julius Japanese Dictation Toolkit, we tested the feasibility of using this method in place of an external microphone for analyzing air-conducted voice sound.	list of english terms of venery, by animal;nam	Yoshitaka Nakajima;Hideki Kashioka;Nick Campbell;Kiyohiro Shikano	2006	IEICE Transactions	10.1093/ietisy/e89-d.1.1	speech recognition;wearable computer;computer science;programming language	Vision	-6.741936432798652	-84.72775443696293	182834
98f6961d3b349915717332f54bff0d4196a84e8d	effects of typicality and interstimulus interval on the discrimination of speech stimuli: within-subject comparison				Minoru Tsuzaki	1992			speech recognition;pattern recognition;artificial intelligence;interstimulus interval;stimulus (physiology);computer science	NLP	-7.651857172405329	-83.44046951990344	183844
24a0ae52ba1a1e17010567fdbfe0b5915b5447f7	a causal inference model explains perception of the mcgurk effect and other incongruent audiovisual speech		"""Audiovisual speech integration combines information from auditory speech (talker's voice) and visual speech (talker's mouth movements) to improve perceptual accuracy. However, if the auditory and visual speech emanate from different talkers, integration decreases accuracy. Therefore, a key step in audiovisual speech perception is deciding whether auditory and visual speech have the same source, a process known as causal inference. A well-known illusion, the McGurk Effect, consists of incongruent audiovisual syllables, such as auditory """"ba"""" + visual """"ga"""" (AbaVga), that are integrated to produce a fused percept (""""da""""). This illusion raises two fundamental questions: first, given the incongruence between the auditory and visual syllables in the McGurk stimulus, why are they integrated; and second, why does the McGurk effect not occur for other, very similar syllables (e.g., AgaVba). We describe a simplified model of causal inference in multisensory speech perception (CIMS) that predicts the perception of arbitrary combinations of auditory and visual speech. We applied this model to behavioral data collected from 60 subjects perceiving both McGurk and non-McGurk incongruent speech stimuli. The CIMS model successfully predicted both the audiovisual integration observed for McGurk stimuli and the lack of integration observed for non-McGurk stimuli. An identical model without causal inference failed to accurately predict perception for either form of incongruent speech. The CIMS model uses causal inference to provide a computational framework for studying how the brain performs one of its most important tasks, integrating auditory and visual speech cues to allow us to communicate with others."""	causal filter;causal inference;causality;illusions;mcgurk effect;movement;perception;speech disorders;syllable;voice disorders	John F. Magnotti;Michael S. Beauchamp	2017		10.1371/journal.pcbi.1005229	speech recognition	ML	-6.867554987286731	-81.1012322565241	184072
695ba50a6a7cae54177b3e8c79d17f9e39960c40	quantitative analysis of multimodal speech data	correlation map analysis;flowanalyzer;multimodal speech;bodily gesture;communicative context;time-varying coordination	This study presents techniques for quantitatively analyzing coordination and kinematics in multimodal speech using video, audio and electromagnetic articulography (EMA) data. Multimodal speech research has flourished due to recent improvements in technology, yet gesture detection/annotation strategies vary widely, leading to difficulty in generalizing across studies and in advancing this field of research. We describe how FlowAnalyzer software can be used to extract kinematic signals from basic video recordings; and we apply a technique, derived from speech kinematic research, to detect bodily gestures in these kinematic signals. We investigate whether kinematic characteristics of multimodal speech differ dependent on communicative context, and we find that these contexts can be distinguished quantitatively, suggesting a way to improve and standardize existing gesture identification/annotation strategy. We also discuss a method, Correlation Map Analysis (CMA), for quantifying the relationship between speech and bodily gesture kinematics over time. We describe potential applications of CMA to multimodal speech research, such as describing characteristics of speech-gesture coordination in different communicative contexts. The use of the techniques presented here can improve and advance multimodal speech and gesture research by applying quantitative methods in the detection and description of multimodal speech.		Samantha Gordon Danner;Adriano Vilela Barbosa;Louis Goldstein	2018	Journal of phonetics	10.1016/j.wocn.2018.09.007	speech recognition;gesture recognition;electromagnetic articulography;psychology;software;kinematics;gesture;annotation	HCI	-7.517824589151728	-81.61134526132426	184331
a8d57c43244fd54b3b2e782853be8942ac17d987	spectral and prosodic transformations of hearing-impaired mandarin speech	signal conversion;metodo espectral;transformation prosodique;time scale;auditory disorder;amelioration parole;conversion senal;hearing impaired speaker;transformation spectrale;prosodic modification;conversion signal;chino;speech enhancement;sinusoidal model;inteligibilidad;voice conversion;hearing impaired;gaussian mixture model;trouble audition;reconocimiento voz;senal voceada;prosodie;spectral method;signal voise;speech recognition;voiced signal;methode spectrale;trastorno auditivo;orthogonal transformation;reconnaissance parole;prosody;chinois;chinese;intelligibilite;intelligibility;article;spectral conversion;speech production;prosodia	This paper studies the combined use of spectral and prosodic conversions to enhance the hearing-impaired Mandarin speech. The analysis-synthesis system is based on a sinusoidal representation of the speech production mechanism. By taking advantage of the tone structure in Mandarin speech, pitch contours are orthogonally transformed and applied within the sinusoidal framework to perform pitch modification. Also proposed is a time-scale modification algorithm that finds accurate alignments between hearing-impaired and normal utterances. Using the alignments, spectral conversion is performed on subsyllabic acoustic units by a continuous probabilistic transform based on a Gaussian mixture model. Results of perceptual evaluation indicate that the proposed system greatly improves the intelligibility and the naturalness of hearing-impaired Mandarin speech. 2005 Elsevier B.V. All rights reserved.	acoustic cryptanalysis;algorithm;biconnected component;experiment;image scaling;intelligibility (philosophy);mixture model;national supercomputer centre in sweden;performance;pitch (music);polynomial;super robot monkey team hyperforce go!;syllable;text corpus	Cheng-Lung Lee;Wen-Whei Chang;Yuan-Chuan Chiang	2006	Speech Communication	10.1016/j.specom.2005.08.001	speech production;speech recognition;linguistics;prosody;sinusoidal model;chinese;intelligibility;spectral method;orthogonal transformation	NLP	-10.201913641364666	-86.79042211833111	185206
a253557ef430a7696e3180219a868e7dbf0e6b71	a formant-trajectory model and its usage in comparing coarticulatory effects in dysarthric and normal speech		Dysarthria is a diverse group of motor speech disorders that typically are associated with impaired intelligibility. As part of a project to develop augmentative communication technologies for intelligibility enhancement of dysarthric speech, a quantitative method is proposed for measuring the relative contributions to impaired intelligibility of vowels of three factors: First, target shift: Dysarthric speakers may have spectral targets that differ from those of normal speakers. Second, coarticulation: The degree of contextual influence on articulation may be greater in dysarthric speech than in normal speech. Third, random variability: Dysarthric speakers may articulate the same phoneme in the same context with more variability. The method is based on a linear model of formant trajectories of vowels in consonant contexts. The results from analysis of a dysarthric and a normal speech sample showed surprisingly similar target values, but increased coarticulation and random variability for the dysarthric sample.	biconnected component;intelligibility (philosophy);linear model;spatial variability	Xiaochuan Niu;Jan P. H. van Santen	2003			formant;normal speech;natural language processing;trajectory;artificial intelligence;speech recognition;computer science	HCI	-10.201400118313083	-83.00683638695024	185879
ff04de313bc49e9047a07cefbeee1b5d17b48204	clustering of strokes from pen-based music notation: an experimental study		A comfortable way of digitizing a new music composition is by using a pen-based recognition system, in which the digital score is created with the sole effort of the composition itself. In this kind of systems, the input consist of a set of pen strokes. However, it is hitherto unclear the different types of strokes that must be considered for this task. This paper presents an experimental study on automatic labeling of these strokes using the well-known k-medoids algorithm. Since recognition of pen-based music scores is highly related to stroke recognition, it may be profitable to repeat the process when new data is received through user interaction. Therefore, our intention is not to propose some stroke labeling but to show which stroke dissimilarities perform better within the clustering process. Results show that there can be found good methods in the trade-off between cluster complexity and classification accuracy, whereas others offer a very poor performance.	algorithm;cluster analysis;experiment;floating-point unit;k-medoids;linear algebra;medoid;sheffer stroke;whole earth 'lectronic link	Jorge Calvo-Zaragoza;José Oncina	2015		10.1007/978-3-319-19390-8_71	speech recognition;computer science;multimedia	HCI	-11.631601068598426	-87.04783103914649	186174
16fdf88dfc3abff5e979d1de8559fbf5091c5dc5	contextual effect in second language perception and production of mandarin tones		Abstract A robust contextual effect is well documented in native tone perception and production but less well studied in non-native speech. The current study examined English-speaking learners’ Mandarin tone perception and production with varying preceding and following tones. Fifteen intermediate-level learners performed an Identification and a Reading task with disyllabic stimuli encompassing various tone combinations. The results revealed that the learners’ accuracy rates and error patterns varied in the initial and final position as well as in different tonal environments. For example, the learners predominantly misidentified T3 as T4 in the initial position when the following tone was T1, but rarely made such an error in the final position. In the Reading task, on the other hand, the learners frequently misproduced T3 as T2 in the final position, but often misproduced T2 as T3 in the initial position, especially when the following tone was T1. The learners’ overall accuracy and error rates in Identification correlated with those in Reading, indicating comparable tone perception and production abilities. However, there were some significant differences between perception and production with regard to T3. These findings suggest that the prosodic position and surrounding tones both have a significant effect on the learners’ performance with L2 tones, and that the effect differs in perception and production.	super robot monkey team hyperforce go!	Yen-Chen Hao	2018	Speech Communication	10.1016/j.specom.2017.12.015	communication;computer science;artificial intelligence;mandarin chinese;perception;pattern recognition	HCI	-11.398113120263057	-81.77647172221013	186708
86d305088054c7892b10031e25395b33de57a407	cepstral analysis of vocal dysperiodicities in disordered connected speech		Several studies have shown that the amplitude of the first rahmonic peak (R1) in the cepstrum is an indicator of hoarse voice quality. The cepstrum is obtained by taking the inverse Fourier Transform of the log-magnitude spectrum. In the present study, a number of spectral analysis processing steps are implemented, including period-synchronous and periodasynchronous analysis, as well as harmonic-synchronous and harmonic-asynchronous spectral band-limitation prior to computing the cepstrum. The analysis is applied to connected speech signals. The correlation between amplitude R1 and perceptual ratings is examined for a corpus comprising 28 normophonic and 223 dysphonic speakers. One observes that the correlation between R1 and perceptual ratings increases when the spectrum is band-limited prior to computing the cepstrum. In addition, comparisons are made with a popular cepstral cue which is the cepstral peak prominence (CPP).	bandlimiting;cepstrum;spectrum analyzer;text corpus	Ali Alpan;Jean Schoentgen;Youri Maryn;Francis Grenez;Paul Murphy	2009			speech recognition;hoarse voice quality;connected speech;fourier transform;artificial intelligence;amplitude;cepstrum;pattern recognition;computer science;correlation	AI	-9.852240308401575	-86.80602261170954	187119
0d4a388986d1fbd78fc125e2190fc909aa4dcec7	subjective evaluation of join cost and smoothing methods for unit selection speech synthesis	modelo dinamico;join cost;evaluation performance;unit selection;performance evaluation;spectral function;speech synthesis;cost function;analyse linguistique;smoothing method;evaluacion prestacion;dynamic model;modele lineaire;speech;linear dynamic models ldm;concatenacion;modelo lineal;join cost functions;indexing terms;funcion espectral;funcion coste;concatenation;linguistic analysis;fonction spectrale;etat actuel;unit selection join cost linear dynamic models ldm perceptual listening tests smoothing speech synthesis;text to speech system subjective evaluation smoothing methods selection based concatenative speech synthesis join cost functions concatenation discontinuities;smoothing methods;concatenation discontinuities;evaluation subjective;smoothing;methode lissage;smoothing methods speech synthesis cost function testing cepstral analysis databases humans power measurement viterbi algorithm lattices;analisis linguistico;state of the art;modele dynamique;linear model;selection based concatenative speech synthesis;text to speech;fonction cout;estado actual;sintesis palabra;text to speech system;perceptual listening tests;subjective evaluation;article;synthese parole;speech synthesis smoothing methods;evaluacion subjetiva	In unit selection-based concatenative speech synthesis, join cost (also known as concatenation cost), which measures how well two units can be joined together, is one of the main criteria for selecting appropriate units from the inventory. Usually, some form of local parameter smoothing is also needed to disguise the remaining discontinuities. This paper presents a subjective evaluation of three join cost functions and three smoothing methods. We also describe the design and performance of a listening test. The three join cost functions were taken from our previous study, where we proposed join cost functions derived from spectral distances, which have good correlations with perceptual scores obtained for a range of concatenation discontinuities. This evaluation allows us to further validate their ability to predict concatenation discontinuities. The units for synthesis stimuli are obtained from a state-of-the-art unit selection text-to-speech system: rVoice from Rhetorical Systems Ltd. In this paper, we report listeners' preferences for each join cost in combination with each smoothing method	concatenation;smoothing;speech synthesis	Jithendra Vepa;Simon King	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TSA.2005.858548	concatenation;speech recognition;index term;computer science;speech;linear model;mathematics;linguistics;speech synthesis;algorithm;statistics;smoothing	DB	-10.206391311918992	-87.16712006934522	189425
b957e7e33aa8fa144d48b191a2d3b7dfaef8f7e7	perception of speech and non-speech sounds by listeners with real and simulated sensorineural hearing loss	and forward;normal hearing;spectrum;speech perception;detection threshold;consonant vowel;hearing loss;sensorineural hearing loss;non speech sound	Abstract   The effects of sensorineural and simulated hearing loss on the perception of speech and non-speech sounds are evaluated and compared. Two simulations of hearing loss are considered: one introduces additive masking noise to elevate the detection thresholds of listeners with normal hearing, the other applies multiband dynamic expansion to attenuate sound components towards these detection thresholds. Measures of speech perception are derived from intelligibility tests for consonant-vowel syllables presented in a background of speech-spectrum noise. The perception of non-speech sounds was evaluated through measurements of the detection of tones in quiet and in the presence of broadband and narrowband masking noise, psychoacoustic tuning curves measured under forward masking conditions, and intensity discrimination for tone pulses. For mild and moderate hearing losses, both types of simulation alter the perception of speech in noise in a manner similar to sensorineural impairment. These simulations also produce effects similar to sensorineural impairment on the detection of tones in quiet and on simultaneous and forward masking. The expansion simulation tends to improve the ability to discriminate tone intensities, an effect generally not seen in listeners with sensorineural impairments.		David Lum;Louis D. Braida	2000	J. Phonetics	10.1006/jpho.2000.0124	spectrum;speech recognition;speech perception;acoustics;linguistics	NLP	-9.861768051291621	-84.47864789510841	189597
1ac6e500752edb40446d46a69301c4ac1204154b	voice source and vocal tract variations as cues to emotional states perceived from expressive conversational speech	vocal tract	Speech parameters originating from voice source and vocal tract were analyzed to find acoustic correlates of dimensional descriptions of emotional states. To achieve this goal best, we adopted the Utsunomiya University Spoken Dialogue Database, which was designed for studies on paralinguistic information in expressive conversational speech. Analyses for four female and two male speakers showed: (i) Prosodic parameters were highly correlated especially with the activation dimension, (ii) The aperiodicity-related voice source parameter showed that breathy phonation was mainly used in unpleasant utterances for three females, (iii) Due to smiling facial expression, formant frequencies were higher in pleasant utterances for a female.	acoustic cryptanalysis;activation function;tract (literature)	Hiroki Mori;Hideki Kasuya	2007			speech recognition;computer science;voice analysis;human voice;vocal tract	NLP	-9.529273089422269	-82.85861450162793	189902
4bb4f39963d39bd56240b998ba066b98fc725119	language specificity in speech perception: perception of mandarin tones by native and nonnative listeners	systeme phonologique;phonological system;contour tonal;vol 67;experience;chinois mandarin;locuteur non natif;tone contour;speech perception;phonetica 2010;mandarin chinese;phonology;lexical tone;perception de la parole;native speaker;phonologie;locuteur natif;experiment;no 4;non native speaker;255105;american english;anglais americain	The results reported in this paper indicate that native speakers of Mandarin Chinese rate the perceptual similarities among the lexical tones of Mandarin differently than do native speakers of American English. Mandarin listeners were sensitive to tone contour while English listeners attended to pitch levels. Chinese listeners also rated tones that are neutralized by phonological tone sandhi rules in Mandarin as more similar to each other than did English speakers--indicating a role of phonology in determining perceptual salience. In two further experiments, we found that some of these differences were eliminated when the listening task focused listeners' attention on the auditory properties of the stimuli, but, interestingly, a degree of language specificity remained even in the most purely psychophysical listening tasks with speech stimuli.	auditory perception;contour line;experiment;rating (action);rule (guideline);sensitivity and specificity;speech disorders;super robot monkey team hyperforce go!;phonology	Tsan Huang;Keith Johnson	2010	Phonetica	10.1159/000327392	psychology;experiment;speech recognition;speech perception;philosophy;mandarin chinese;first language;linguistics;sociology;communication;phonology	NLP	-10.49584559315776	-81.77481480893073	190057
c610e666759bbfd98e4e061cadd2bee12c57bd1a	homogeneity vs heterogeneity in indian english: investigating influences of l1 on f0 range		We present an exploratory analysis of several long-term distributional measures of f0 range in the speech of universityeducated speakers of Indian English from four L1 backgrounds (Telugu, Tamil, Hindi and Bengali). The aim of this study is to investigate the degree of homogeneity in Indian English prosody and any similarities between the speakers’ productions in English and their L1. Following recent studies, we examine three aspects of f0 range: pitch level (relative height of habitual f0), pitch span and pitch dynamism. Overall, across varieties, pitch level measures reveal individual speaker differences and only weak L1 effects on max f0 and median f0. Some speakers show higher f0 in their L1 productions compared to their English productions. More robust patterns were found for pitch span and dynamism: for all measures (maximum-minimum f0, pitch dynamism quotient and standard deviation), significant differences were found between L1 and English (p<0.001) for Bengali and Telugu L1 speakers. The relative weakness of L1 effects would suggest a degree of homogeneity in Indian English, at least for the prosodic parameters investigated. Evidence of a shift in pitch span when talking in English, regardless of L1, further suggests a convergent speech variety.	difference quotient;pitch (music);pitch shift;semantic prosody	Olga Maxwell;Elinor Payne;Rosey Billington	2018		10.21437/Interspeech.2018-1476	homogeneity (statistics);indian english;artificial intelligence;pattern recognition;computer science	NLP	-11.190936277694174	-82.0931707506972	190143
12d8a44258f7a161096b8174fd9767a903a1c6ab	on the perception of the neutral tone in taiwan mandarin	conference item	The neutral tone in Taiwan Mandarin (TM) behaves differently from that of Standard Mandarin. Many neutral-tone syllables in TM are not reduced and have a low pitch target regardless of the preceding tone. These neutral-tone syllables are therefore acoustically similar to Tone 3 syllables in TM. This paper investigates how TM listeners distinguish Tone 3 (low tone) from the neutral tone. The result shows that the end pitch is the primary cue. When the end pitch is higher than -1 z-score (the pitch ranges between -2 to 2 z-score), the listeners are more likely to perceive the stimulus as a neutral tone. Also, a convex pitch contour is more likely to be perceived as a neutral tone. Lastly, when the pitch information is ambiguous, the listeners rely on the vowel quality or the phonation type to distinguish the pairs. The results suggest that the neutral tone in TM slowly reaches to a mid-low target.	ambiguous grammar;neutral monism;super robot monkey team hyperforce go!	Karen Huang	2011			psychology;speech recognition;advertising;communication	Crypto	-10.666273648779999	-81.89911917127048	190610
59a2ab6238af6e541f1a88e47d93940b4278d7a6	levels of representation in the electrophysiology of speech perception	speech perception;phonetics;electrophysiology;mismatch negativity	Mapping from acoustic signals to lexical representations is a complex process mediated by a number of different levels of representation. This paper reviews certain properties of the phonetic and phonological levels of representation and hypotheses about how category structure is represented at each of these levels, and evaluates these in light of relevant electrophysiological studies of phonetics and phonology. The phonetic level is characterized by diverse (and competing) hypotheses about how categories could be represented in the brain, and there is a growing body of electrophysiological work which can be brought to bear on these hypotheses. Existing electrophysiological evidence is consistent with behavioral evidence for the heterogeneity of phonetic representations. Furthermore, some evidence supports the existence of multiple phonetic levels of representation. Meanwhile, rather less is known about how representations at the phonological level could be coded in the brain. These representations are more abstract than phonetic representations, and are probably more homogeneous than phonetic representations.	acoustic cryptanalysis;computational neuroscience;deficit round robin;erp;magnetoencephalography;speech processing	Colin Phillips	2001	Cognitive Science	10.1016/S0364-0213(01)00049-0	psychology;phonetics;phonetic form;electrophysiology;speech recognition;speech perception;mismatch negativity;signal processing;neurolinguistics;linguistics;communication;phonology	NLP	-8.36426574448855	-80.95341672481727	191157
7d7ea29c942aa106928a8098b7e7b42c26904274	visualising musical structure through performance gesture		A musical performance is seen as the performer’s interpretation of a musical score, illuminating the interaction between the musical structure and implied emotive character [1]. It has been demonstrated that performers’ physical gestures correlate with structural and emotional aspects of the piece they are performing and that this information can be decoded by an audience when presented with a visualonly performance [2]. This paper investigates the relationship between direction of physical movement and underlying musical structures. The Vicon motion capture system is used to record 3D movements made by nine university-level pianists performing Chopin preludes op.28 Nos 6 and 7. The examination of several pianists provides insight into the similarity and differences in gestures between performers and how these relate to structure. Principal Component Analysis (PCA) of these performances and consequent analysis of variance reveals a relationship between extrema of the first six significant components and timing of phrasing structure in Prelude 7 where motion troughs consistently lag behind the occurence of phrase boundaries in the audio. This relationship is then examined for Prelude 6 which encompasses longer, expanded phrases and changes in rhythm. These expanded phrases are associated with elongated or split gestures, and variations of the motif with changes in movement.	motif;motion capture;performance;principal component analysis	Jennifer MacRitchie;Bryony Buck;Nicholas J. Bailey	2009			speech recognition;artificial intelligence	HCI	-8.107499774511792	-81.48218170188531	192174
8d78a1ecd997ee73141ffdc8c59bdb595bc4d400	adult and infant sensitivity to phonotactic features in spoken japanese.		Japanese speakers perceive an epenthetic vowel between consonants in words, reflecting an adaptation to Japanese phonotactics. However, there are some contexts in which CC (successive consonant) clusters are acceptable in Japanese speech. This study explored how Japanese speakers perceive phoneme sequences according to Japanese phonotactics. In Experiment 1, adults rated goodness of nonsense words with CV and CC sequences as exemplars of Japanese words. The adults were sensitive to the legitimacy of vowel devoicing. They considered phoneme sequences following the Japanese phonotactics to be better than exceptional but possible sequences. Experiment 2 investigated 6-, 12-, and 18-monthold infants’ sensitivity to phoneme changes in words. Infants of all age groups detected vowel changes (CVCC vs. CVCVCV) in vowel-devoicing contexts and only 18-month infants detected consonant changes (CVC vs. CVCC). The results indicate that native phonotactic constraints have a large effect on adult perception but a small effect on infant perception.	experiment;phoneme	Kajikawa Sachiyo;Laurel Fais;Shigeaki Amano;Werker Janet	2004			speech recognition	NLP	-10.763056213794131	-81.74597399042487	192557
8b743a505e94879a454ca4936b4690322b989235	automatic classification of voice disorders in course of neurodegenerative disease		The study presented in this publication is the first from the planned complex, interdisciplinary studies. The examination was carried out on patients of CM-UJ clinic in Krakow who suffered from neurodegenerative disease with the damage of the extrapiramidal system with dysarthria-type changes in speech. Control examinations of healthy persons have also been carried out. The elements whose realization was tested had been chosen based on the linguistic knowledge in the scope of phonetics as well as on experience resulting from longterm practice as a speech pathologist. The linguistic material was selected in such a way as to pinpoint voice changes characteristic for patients with dysarthria. During the examination, phrases based on Polish idioms were aslo recorded for further analyses.	color gradient;digital signal (signal processing);programming idiom;search algorithm	Tomasz Orzechowski;Andrzej Izworski;Izabela Gatkowska;Monika Rudzinska	2005			disease;pathology;medicine	NLP	-6.168168416472536	-84.61517410195734	193833
a8bd04fceb247a4f472b362d858fd16ab0773784	instantaneous evaluation of the sense of presence in audio-visual content	audio visual information;content presence;method of continuous judgment by category	The sense of presence is crucial to evaluate the performance of audio-visual (AV) equipment and content. Previously, the overall presence was evaluated for a set of AV content items by asking subjects to judge the presence of the entire content item. In this study, the sense of presence is evaluated for a time-series using the method of continuous judgment by category. Specifically, the audio signals of 40 content items with durations of approximately 30 s each were recorded with a dummy head, and then presented as stimuli to subjects via headphones. The corresponding visual signals were recorded using a video camera in the fullHD format, and reproduced on a 65-inch display. In the experiments, 20 subjects evaluated the instantaneous sense of presence of each item on a seven-point scale under two conditions: audio-only or audio-visual. At the end of the time-series, the subjects also evaluated the overall presence of the item by seven categories. Based on these results, the effects of visual information on the sense of presence were examined. The overall presence is highly correlated with the ten-percentile exceeded presence score, S 10, which is the score that is exceeded for the 10% of the time during the responses. Based on the instantaneous presence data in this study, we are one step closer to our ultimate goal of developing a real-time operational presence meter. key words: content presence, method of continuous judgment by category, audio-visual information	anomalous experiences;dummy variable (statistics);experiment;headphones;real-time transcription;time series	Kenji Ozawa;Shota Tsukahara;Yuichiro Kinoshita;Masanori Morise	2015	IEICE Transactions	10.1587/transinf.2014MUP0019	computer vision;computer science;multimedia	HCI	-5.018332587361383	-82.51133625753855	193895
f3ff3f01cde06b754e12cd9994e9d634642e64f2	inter and intra-speaker variability in french: an analysis of oral vowels and its implication for automatic speaker verification		Intra and inter-speaker variability is studied as a way to better understand how voice can be used as biometric data. Formant values from 328,016 exemplars of the 10 French oral vowels uttered by 111 speakers were compared to estimate their speaker discrimination power. The vowels /œ/, /ɛ/ and /a/ appear to convey more idiosyncratic information than other oral vowels. A more comprehensive phonetic analysis is carried out for each speaker on 2 samples leading to either high or low discrimination performance when used in the Alize/spkDet SVS. However, no direct explanation can be drawn from phonetic measures to predict	biometrics;heart rate variability;linear discriminant analysis;oral and maxillofacial radiology;spatial variability;speaker recognition	Juliette Kahn;Nicolas Audibert;Jean-François Bonastre;Solange Rossato	2011			speech recognition;nasal vowel;computer science	HCI	-10.187064987271999	-83.43117226150515	194228
321d6c46c95b615617d37bc8b883d9937d9aadd6	evaluation of a physical method for estimating speech intelligibility in auditoria	reverberation;speech intelligibility;frequency modulation;performance evaluation;transfer functions;speech analysis testing frequency modulation reverberation laboratories intensity modulation noise measurement performance evaluation interference transfer functions;speech analysis;testing;interference;noise measurement;intensity modulation;speech transmission index	The physical measure RASTI (Rapid Speech Transmission Index) has been developed for assessing speech intelligibility in auditoria. For evaluating this method, a set of fourteen auditorium conditions (plus two replicas) with various degrees of reverberation and/or interfering noise were subjected to: (1) RASTI measurements, (2) articulation tests performed by laboratories in eleven different countries. The various listening experiments show substantial differences in the ranking of the fourteen conditions. For instance, it appears that the absence of a carrier phrase in some of the articulation tests has great influence on the relative importance of reverberation as compared to noise interference. When considering only the tests using an appropriate carrier phrase (seven countries), it is found that the RASTI values are in good agreement with the mean results of these articulation tests.		Herman J. M. Steeneken;Tammo Houtgast	1982		10.1109/ICASSP.1982.1171905	frequency modulation;speech recognition;telecommunications;reverberation;computer science;noise measurement;intensity modulation;interference;software testing;transfer function;intelligibility	NLP	-10.262411824314384	-84.54485014991067	194377
c32c0c4f6f56b35806007e8d81e87fa7b5d3255f	methodological issues in assessing perceptual representation of consonant sounds in thai		This work is an attempt to evaluate different experimental methods, ABX vs. AXB, and the use of reaction time (RT) measurement in assessing perceptual sensitivity to phonemic similarity based on perceptual representation of Thai initial consonants [1]. Thirty phoneme pairs are selected to represent varying degrees of similarity: highly similar, moderately similar, and clearly distinct. All the phoneme pairs are presented in noise in ABX and AXB tasks to twenty-two normal hearing Thai listeners. Order of the two tasks is counter-balanced across listener groups. Percent correct responses (p(C)), RTs, and preference rating are collected. The findings show that, p(C) is significantly higher in AXB than ABX despite no significant difference in RT values. In both ABX and AXB, listeners’ p(C) across 3 levels of similarity varies significantly with the highest score in the clearly distinct group, and lowest score in the highly similar group. RT values across the 3 levels follow similar patterns but are not always statistically significant. ABX and AXB tasks could systematically be used to assess perceptual representation of speech sounds, with AXB eliciting higher p(C) and preference rating. It is suggested that some irregular patterns found in one part of the RT data may reflect some perceptual sensitivity pertaining to perceptual phoneme-cluster boundary.		Charturong Tantibundhit;Chutamanee Onsuwan;P. Phienphanich;Chai Wutiwiwatchai	2012			speech recognition;consonant;computer science;perception	HCI	-10.663266907448648	-82.53300467246625	194841
f7a833416e3fcd523551a6bde01adc21a9ee993f	more on acoustic correlates of stress.		The power of various relatively unknown parameters of stress [1, 2] was investigated. They were either derived from the latter, or from the physiological process of phonation. Two stimulus Afrikaans words in and out of focal accented sentence position were read by three Afrikaans female participants. The stressed and unstressed vowel /A/ was investigated in the two contexts. Effect sizes and multiple regression analysis results were used in determining the descriptive and explanatory power of the parameters as to the acoustic correlates of stress. The results substantiate the groundbreaking work of [1, 2] in this regard, but in some instances they disprove these. Many of the parameters proved to be quite powerful constructs, in some cases surpassing the known ones in strength. The successful derivation of such parameters from the physiological basis of the phonation process is demonstrated. Special attention is paid as to the description of vowel reduction.	acoustic cryptanalysis;focal (programming language)	Daan Wissing	2007			speech recognition;artificial intelligence;pattern recognition;computer science	HCI	-10.724977706761385	-82.58387638296283	195087
51865f5e6f12517f840dc936dc23d6be79e61fe4	voice impersonation using generative adversarial networks		Voice impersonation is not the same as voice transformation, although the latter is an essential element of it. In voice impersonation, the resultant voice must convincingly convey the impression of having been naturally produced by the target speaker, mimicking not only the pitch and other perceivable signal qualities, but also the style of the target speaker. In this paper, we propose a novel neural-network based speech quality- and style-mimicry framework for the synthesis of impersonated voices. The framework is built upon a fast and accurate generative adversarial network model. Given spectrographic representations of source and target speakersu0027 voices, the model learns to mimic the target speakeru0027s voice quality and style, regardless of the linguistic content of eitheru0027s voice, generating a synthetic spectrogram from which the time-domain signal is reconstructed using the Griffin-Lim method. In effect, this model reframes the well-known problem of style-transfer for images as the problem of style-transfer for speech signals, while intrinsically addressing the problem of durational variability of speech sounds. Experiments demonstrate that the model can generate extremely convincing samples of impersonated speech. It is even able to impersonate voices across different genders effectively. Results are qualitatively evaluated using standard procedures for evaluating synthesized voices.	artificial neural network;expanded memory;experiment;generative adversarial networks;heart rate variability;network model;pitch (music);resultant;spectrogram;standard operating procedure;synthetic intelligence;whole earth 'lectronic link	Yang Gao;Rita Singh;Bhiksha Raj	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462018	adversarial system;speech recognition;generative grammar;network model;impression;data modeling;spectrogram;computer science	Vision	-10.983737606815218	-87.01806140920084	196085
4297863f74c5c3ac198bb86e80341f1b68d08e0d	temporal relationship between auditory and visual prosodic cues		It has been reported that non-articulatory visual cues to prosody tend to align with auditory cues, emphasizing auditory events that are in close alignment (visual alignment hypothesis). We investigated the temporal relationship between visual and auditory prosodic cues in a large corpus of utterances to determine the extent to which non-articulatory visual prosodic cues align with auditory ones. Six speakers saying 30 sentences in three prosodic conditions (x2 repetitions) were recorded in a dialogue exchange task, to measure how often eyebrow movements and rigid head tilts aligned with auditory prosodic cues, the temporal distribution of such movements, and the variation across prosodic conditions. The timing of brow raises and head tilts were not aligned with auditory cues, and the occurrence of visual cues was inconsistent, lending little support for the visual alignment hypothesis. Different types of visual cues may combine with auditory cues in different ways to signal prosody.	align (company);semantic prosody;speech corpus;text corpus	Erin Cvejic;Jeesun Kim;Chris Irwin Davis	2011			artificial intelligence;pattern recognition;computer science	ML	-9.402204372470884	-80.53945520218119	196297
dacf970e87dfd7d52d3089f232d4247ed45ec06b	natural statistics of binaural sounds		Binaural sound localization is usually considered a discrimination task, where interaural time (ITD) and level (ILD) disparities at pure frequency channels are utilized to identify a position of a sound source. In natural conditions binaural circuits are exposed to a stimulation by sound waves originating from multiple, often moving and overlapping sources. Therefore statistics of binaural cues depend on acoustic properties and the spatial configuration of the environment. In order to process binaural sounds efficiently, the auditory system should be adapted to naturally encountered cue distributions. Statistics of cues encountered naturally and their dependence on the physical properties of an auditory scene have not been studied before. Here, we performed binaural recordings of three auditory scenes with varying spatial properties. We have analyzed empirical cue distributions from each scene by fitting them with parametric probability density functions which allowed for an easy comparison of different scenes. Higher order statistics of binaural waveforms were analyzed by performing Independent Component Analysis (ICA) and studying properties of learned basis functions. Obtained results can be related to known neuronal mechanisms and suggest how binaural hearing can be understood in terms of adaptation to the natural signal statistics.	acoustic cryptanalysis;basis function;binaural beats;covox speech thing;depth perception;independent computing architecture;independent component analysis;internet listing display	Wiktor Mlynarski;Jürgen Jost	2014	CoRR		speech recognition;acoustics;communication	ML	-6.832372320631195	-81.69917737275034	196423
5d2f33884549468828c3d37147cf2098bfb343f4	implementation and evaluation of listenability-centered sound separation system	microphones;arrays;games;speech recognition;signal to noise ratio;correlation coefficient	We developed a sound separation system that uses a microphone array to separate children's voices from background noise in general living environments. Our focus in the development of this system is on “a clear separation of the human voice” so that it will be easy for children to use. Evaluations conducted of sound separation performance in previous research have not always measured how easily listeners hear separate sounds. In this paper, we examine the validity of our system design by evaluating this factor from the perspectives of engineering and psychology.	microphone;systems design	Takahiro Nakadai;Tomoki Taguchi;Hiroshi Mizoguchi;Ryohei Egusa;Etsuji Yamaguchi;Shigenori Inagaki;Yoshiaki Takeda;Miki Namatame;Masanori Sugimoto;Fusako Kusunoki	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090695	games;speech recognition;acoustics;computer science;engineering;signal-to-noise ratio	Robotics	-9.661757477598236	-84.8885099468991	196602
8f51948b39167673cf840fb1ba33dd25ff0ca040	analysis of mrate, shimmer, jitter, and f0 contour features across stress and speaking style in the susas database	speaking rate;stress;means;mrate;soft style;ordered set;signal sampling;speech processing;speech analysis;signal classification speech processing feature extraction jitter;neutral style;database;slow style;length measurement;susas database;data mining;voiced frames;shimmer;statistical analysis;speaking styles;feature extraction;signal classification;fundamental frequency contours;angry style;jitter stress feature extraction data mining length measurement signal processing algorithms signal sampling speech analysis analysis of variance statistical analysis;analysis of variance;fast style;f 0 contour features;lombard style;loud style;jitter;signal processing algorithms;fundamental frequency;question style;question style mrate shimmer jitter f sub 0 contour features speaking styles susas database stress speaking rate fundamental frequency contours means voiced frames fast style neutral style slow style loud style soft style database angry style lombard style	This paper highlights the results of an investigation of several features across the style classes of the “simulated” portion of the SUSAS database. The features considered here include a recently-introduced measure of speaking rate called mrate, measures of shimmer, measures of jitter, and features derived from fundamental frequency (F0) contours. The F0 contour features are the means of F0 and F0 over the first, middle, and last thirds of the ordered set of voiced frames for each word. Mrate exhibits differences between the Fast, Neutral, and Slow styles and between the Loud, Neutral, and Soft styles. Shimmer and jitter exhibit differences that are similar to those of mrate; however, the shimmer and jitter differences are less consistent than the mrate differences across the speakers in the database. Several F0 contour features exhibit differences between the Angry, Loud, Lombard, and Question styles and most of the other styles.		Raymond E. Slyh;W. Todd Nelson;Eric G. Hansen	1999		10.1109/ICASSP.1999.758345	natural language processing;speech recognition;jitter;analysis of variance;feature extraction;length measurement;computer science;speech processing;fundamental frequency;stress;statistics	Vision	-10.78955550470583	-84.1376688077638	196627
190a6d1429d147b70a9869c96baa56e6940c4c45	reanalyze fundamental frequency peak delay in mandarin		In Mandarin, Fundamental Frequency (F0) peak delay has been reported to occur frequently in the rising (R) tone or high (H) tone succeeding by a low (L) tone. Its occurrence was ascribed to articulatory constraints within a conflicting tonal context: a high offset target followed by a low onset target. To further examine the underlying mechanism of the phenomenon, the current study tests the possibility that valley delay, as opposed to peak delay, may occur in an L+H tonal context; and peak or valley delay may also occur within a compatible tonal context where adjacent tonal values are identical or similar. An experiment was done on Annotated Speech Corpus of Chinese Discourse to investigate the frequency of occurrence and amount of peak and valley delay. The results indicated that: F0 peak and valley delay frequently occurred in both conflicting and compatible tonal contexts; the phenomenon was found extensively in R tone and F (falling) tone, but barely in H tone and L tone. The findings suggest that while peak or valley delay is partially due to articulatory constraints in certain tonal contexts, the speakers’ active effort-distribution strategy based on economical principle is also behind the phenomenon.	chinese room;onset (audio);speech corpus;super robot monkey team hyperforce go!	Lixia Hao;Wei Zhang;Yanlu Xie;Jinsong Zhang	2017			speech recognition;mandarin chinese;pattern recognition;artificial intelligence;fundamental frequency;computer science	HCI	-10.666721343178873	-81.41458868984897	196923
eaf47e69fcd442ae11286b75d46273abb2206648	feasibility of vocal emotion conversion on modulation spectrogram for simulated cochlear implants		Cochlear implant (CI) listeners were found to have great difficulty with vocal emotion recognition because of the limited spectral cues provided by CI devices. Previous studies have shown that the modulation spectral features of temporal envelopes may be important cues for vocal emotion recognition of noise-vocoded speech (NVS) as simulated CIs. In this paper, the feasibility of vocal emotion conversion on a modulation spectrogram for simulated CIs for correctly recognizing vocal emotion is confirmed. A method based on a linear prediction scheme is proposed to modify the modulation spectrogram and its features of neutral speech to match that of emotional speech. The logic of this approach is that if vocal emotion perception of NVS is based on the modulation spectral features, NVS with similar modulation spectral features of emotional speech will be recognized as the same emotion. As a result, it was found that the modulation spectrogram of neutral speech can be successfully converted to that of emotional speech. The results of the evaluation experiment showed the feasibility of vocal emotion conversion on the modulation spectrogram for simulated CIs. The vocal emotion enhancement on the modulation spectrogram was also further discussed.	cochlear implant;emotion recognition;modulation;spectrogram;vocoder	Zhi Zhu;Ryota Miyauchi;Yukiko Araki;Masashi Unoki	2017	2017 25th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2017.8081526	linear prediction;modulation;emotion recognition;frequency modulation;spectrogram;speech recognition;cochlear implant;emotion perception;computer science	NLP	-9.697715811581748	-86.41917896618972	197244
32c054eb4748ac7a82a407fccbdce9511f91b356	prosodic and segmental rubrics in emotion identification	humans speech synthesis spatial databases speech recognition prototypes telecommunications appraisal feature extraction speech analysis emotion recognition;speech synthesis;bayes methods;prototypes;speech analysis;speech coding bayes methods emotion recognition natural languages speaker recognition;emotion identification;speech coding;statistical significance;emotion recognition;natural languages;spectrum;speaker recognition;prosodic;appraisal;feature extraction;spatial databases;spanish;universal background emotion codebook;speech recognition;segmental spectrum;humans;bayes classifier;universal background emotion codebook prosodic segmental rubrics emotion identification segmental spectrum bayes classifier spanish;telecommunications;segmental rubrics	It is well known that the emotional state of a speaker usually alters the way she/he speaks. Although all the components of the voice can be affected by emotion in some statistically-significant way, not all these deviations from a neutral voice are identified by human listeners as conveying emotional information. In this paper we have carried out several perceptual and objective experiments that show the relevance of prosody and segmental spectrum in the characterization and identification of four emotions in Spanish. A Bayes classifier has been used in the objective emotion identification task. Emotion models were generated as the contribution of every emotion to the build-up of a universal background emotion codebook. According to our experiments, surprise is primarily identified by humans through its prosodic rubric (in spite of some automatically-identifiable segmental characteristics); while for anger the situation is just the opposite. Sadness and happiness need a combination of prosodic and segmental rubrics to be reliably identified	codebook;experiment;relevance;sadness;semantic prosody;voice stress analysis	Roberto Barra-Chicote;Juan Manuel Montero-Martínez;Javier Macías Guarasa;Luis Fernando D'Haro;Rubén San-Segundo-Hernández;Ricardo de Córdoba	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660213	natural language processing;speaker recognition;spectrum;bayes classifier;speech recognition;feature extraction;computer science;speech coding;statistical significance;speech synthesis;spanish	Robotics	-11.098202726746793	-84.83112519044928	198092
1401b88af642a59601995bfe7c9b86d6116b0819	extraction of articulators in x-ray image sequences	x ray imaging;vocal tract;vision;x rays	We describe a method for tracking tongue, lips, and throat in X-ray films showing the side-view of the vocal tract. The technique uses specialized histogram normalization techniques and a new tracking method that is robust against occlusion, noise, and spontaneous, nonlinear deformations of articulators. The tracking results characterize the configuration of the vocal tract over time and can be used in different areas of speech research.	nonlinear system;radiography;spontaneous order;tract (literature)	G. Thimm;Juergen Luettin	1999			speech recognition;acoustics;engineering;communication	Vision	-7.620899299999544	-85.1531244418119	198538
39aba0e075dcf8e422d92dfa14133100501e7f95	phonetic convergence and language talent within native-nonnative interactions		The notion of phonetic convergence covers all adaptations in articulatory and acoustic features towards those of a communicative partner, or in other terms an increase in segmental and suprasegmental similarity between them (Pardo 2006). Up until now most of the experiments on convergence were designed for monolingual dyads, with very few investigations of convergence in native-nonnative interactions in a foreign language learning environment. We tried to analyze the convergent behavior of nonnative speakers of English in dialog with native speakers and the persistence of the effect in relation to their rated phonetic talent. In this paper we present first results for a global measurement of convergence the comparison of amplitude envelope signals.	acoustic cryptanalysis;experiment;interaction;persistence (computer science);dialog	Natalie Lewandowski;Travis Wade;Grzegorz Dogil	2008			communication;convergence (routing);computer science	NLP	-11.688410328656838	-83.30618065927683	198603
b962c6f3cf698133da860908db35028b7190b859	experiences collecting genuine spoken enquiries using woz techniques	unscripted speech data;woz technique;valuable insight;limited quantity;genuine telephone-based route planning;future speech-based human-machine interaction	1. I N T R O D U C T I O N Many laboratories have now used the so-called 'Wizard of Oz' (WOZ) technique for eliciting spontaneous spoken human-machine dialogue in order (i) to study the resulting speech and (ii) to evaluate the necessary speech technology and natural language processing systems [1]. The technique is particularly valuable because it enables user behaviour to be studied under conditions which are not constrained by the limitations of current technological (or theoretical) capabilities. However, many such exercises involve 'volunteer' users whose behaviour is prescribed by a pre-prepared scenario [6] thereby removing one potentially crucial aspect of human-machine interaction, namely any behavioural events which are unique to genuine (i.e. motivated) and un-preprepared transactions [4]. This paper presents $RU's first experiences collecting unscripted speech data using the WOZ technique by the provision of a genuine voice-based telephone enquiry service to personnel on the RSRE site. 2. T H E T A S K D O M A I N The enquiry service was configured around a commercially available route planning software package. The package runs on a PC and contains map and gazetteer information covering the majority of the United Kingdom. Its main feature is its ability to find the shortest and/or quickest routes between two locations in accordance with a range of specifyable variables such as preferences for certain classes of roads and driving speeds. Alternative routes can also be found. Clearly the behaviour of the wizard will very much influence the nature of the resulting corpus, and constraints (such as restricting the vocabulary) can be placed on a wizard in a variety of ways [2]. However, in order not to restrict or influence callers' behaviour, it was decided that very few restrictions should be placed on the wizard apart from the use of a stock opening phrase and the provision of a few standard reply templates simply in order to reduce the wizard's work load. In general, the design for the wizard's behaviour was based on information derived from the procedures employed by a commercial company who already provide a route planning service over the telephone (in this case the enquiries being made by tone dialling) based on the same software package. Every at tempt was made to remove all distinctly human characteristics from the wizard's speech such as false starts and stutters, and great care was taken to ensure that breath noise and key clicks were not audible to the caller. 4 . T H E E X P E R I M E N T A L C O N F I G U R A T I O N As well as implementing a genuine telephone-based enquiry service using WOZ techniques, it was also decided to compare wizard-type transactions with normal human-human interaction for the same task. Hence the experimental set-up was configured to operate with two incoming telephone lines one assigned to the normal human operator and one assigned to the wizard. Appropriate equipment was installed to provide automatic detection of incoming calls and initiation of recording and digitisation. In order for there to be minimal differences between the operator 's behaviour in both the human-human and human-wizard conditions, the same operator was used in each case. As a consequence the only difference between	experience;human–computer interaction;integrated woz machine;natural language processing;speech technology;spontaneous order;telephone line;vocabulary;wizard (software)	Roger Moore;Angela Morris	1992			computer science;artificial intelligence;data mining	NLP	-6.241395238023732	-82.49878652773627	199348
727f1569e2ab24d7ae25a9a602b7504c087db05b	enhanced robot speech recognition using biomimetic binaural sound source localization		Inspired by the behavior of humans talking in noisy environments, we propose an embodied embedded cognition approach to improve automatic speech recognition (ASR) systems for robots in challenging environments, such as with ego noise, using binaural sound source localization (SSL). The approach is verified by measuring the impact of SSL with a humanoid robot head on the performance of an ASR system. More specifically, a robot orients itself toward the angle where the signal-to-noise ratio (SNR) of speech is maximized for one microphone before doing an ASR task. First, a spiking neural network inspired by the midbrain auditory system based on our previous work is applied to calculate the sound signal angle. Then, a feedforward neural network is used to handle high levels of ego noise and reverberation in the signal. Finally, the sound signal is fed into an ASR system. For ASR, we use a system developed by our group and compare its performance with and without the support from SSL. We test our SSL and ASR systems on two humanoid platforms with different structural and material properties. With our approach we halve the sentence error rate with respect to the common downmixing of both channels. Surprisingly, the ASR performance is more than two times better when the angle between the humanoid head and the sound source allows sound waves to be reflected most intensely from the pinna to the ear microphone, rather than when sound waves arrive perpendicularly to the membrane.		Jorge Dávila-Chacón;Jindong Liu;Stefan Wermter	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2018.2830119	humanoid robot;binaural recording;spiking neural network;word error rate;embodied embedded cognition;speech recognition;audio signal;computer science;reverberation;microphone	Robotics	-6.592543856723609	-82.34875091708128	199502
f5276f9ec5804a17f0ccec4b9062178c7e8115ba	measuring the naturalness of synthetic speech	synthetic speech;perception;naturalness;intelligibility;part of speech;diagnostic test;text to speech;rule based	Even the highest quality synthetic speech generated by rule sounds unlike human sppech. As the intelligibility of rule-based synthetic speech improves, and the number of applications for synthetic speech increases, the naturalness of synthetic speech will become an important factor in determining its use. In order to improve this aspect of the quality of synthetic speech it is necessary to have diagnostic tests that can measure naturalness. Currently, all of the available metrics for evaluating the acceptability of synthetic speech do not distinguish sufficiently between measuring overall acceptability (including naturalness) and simply measuring the ability of listeners to extract intelligible information from the signal. In this paper we propose a new methodology for measuring the naturalness of particular aspects of synthesized speech, independent of the intelligibility of the speech. Although naturalness is a multidimensional, subjective quality of speech, this methodology makes it possible to assess the separate contributions of prosodic, segmental, and source characteristics of the utterance. In two experiments, listeners reliably differentiated the naturalness of speech produced by two male talkers and two text-to-speech systems. Furthermore, they reliably differentiated between the two text-to-speech systems. The results of these experiments demonstrate that perception of naturalness is affected by information contained within the smallest part of speech, the glottal pulse, and by information contained within the prosodic structure of a syllable. These results shown that this new methodology does provide a solid basis for measuring and diagnosing the naturalness of synthetic speech.	speech synthesis;synthetic intelligence	Howard C. Nusbaum;Alexander L. Francis;Anne S. Henly	1995	I. J. Speech Technology	10.1007/BF02277176	rule-based system;speech recognition;part of speech;computer science;linguistics;speech synthesis;intelligibility;diagnostic test	NLP	-10.266887437069897	-85.30329548424893	199646
1499d4c60ebc643173c124406603a8d25ec7d811	direct acoustic feature using iterative em algorithm and spectral energy for classifying suicidal speech	em algorithm	Research has shown that the voice itself contains important information about immediate psychological state and certain vocal parameters are capable of distinguishing speaking patterns of speech signal affected by emotional disturbances (i.e., clinical depression). In this study, the GMM based feature of the vocal tract system response and spectral energy have been studied and found to be a primary acoustic feature set for separating two groups of female patients carrying a diagnosis of depression and suicidal risk.	acoustic cryptanalysis;expectation–maximization algorithm;google map maker;iterative method;mental state;tract (literature)	Thaweesak Yingthawornsuk;Hande Kaymaz-Keskinpala;D. Mitchell Wilkes;Richard G. Shiavi;Ronald M. Salomon	2007			speech recognition;expectation–maximization algorithm;computer science	NLP	-6.566193209992243	-86.10168798543671	199893
