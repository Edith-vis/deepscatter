id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
37b1364fad15394590af6e51eb3bd33677a7055c	reversible data hiding based on histogram modification of pixel differences	filigranage numerique;protection information;analisis imagen;digital watermarking;histograms;watermarking;evaluation performance;overflow computer arithmetics;performance evaluation;structure arborescente;estudio comparativo;compression algorithms;evaluacion prestacion;circuit sans perte;authentication;performance comparison;biomedical imaging;tree data structures;binary trees;data encapsulation;lossless watermarking;etude comparative;reversible data hiding image authentication lossless watermarking;histogram;image authentication;histogramme;proteccion informacion;data encapsulation histograms watermarking binary trees biomedical imaging compression algorithms computer science authentication humans visual system;estructura arborescente;arbol binario;information protection;filigrana digital;tree structure;arbre binaire;histogram modification;comparative study;rebasamiento capacidad;lossless circuit;image analysis;depassement capacite;humans;computer science;circuito sin perdida;histograma;analyse image;visual system;histogram shifting technique reversible data hiding histogram modification pixel differences binary tree structure;tree data structures data encapsulation;pixel differences;histogram shifting technique;reversible data hiding;binary tree structure;binary tree	In this letter, we present a reversible data hiding scheme based on histogram modification. We exploit a binary tree structure to solve the problem of communicating pairs of peak points. Distribution of pixel differences is used to achieve large hiding capacity while keeping the distortion low. We also adopt a histogram shifting technique to prevent overflow and underflow. Performance comparisons with other existing schemes are provided to demonstrate the superiority of the proposed scheme.	arithmetic underflow;binary tree;channel (communications);distortion;pitch shift;pixel;tree structure	Wei-Liang Tai;Chia-Ming Yeh;Chin-Chen Chang	2009	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2009.2017409	computer vision;image analysis;binary tree;digital watermarking;computer science;histogram matching;theoretical computer science;histogram;mathematics;algorithm;statistics	Visualization	43.892897731071386	-12.235512959303225	188306
026d37864b7bb3ffc3787352269399427fcde065	windowed all phase biorthogonal transform and its application in jpeg-like image compression	jpeg algorithm;discrete cosine transform dct;gpsa;image compression;期刊论文;wapbt	—This paper proposes new concepts of the windowed all phase biorthogonal transform (WAPBT), which is inspired by the all phase biorthogonal transform (APBT). In the light of windowed all phase digital filter theory, windowed all phase biorthogonal transforms is proposed. The matrices of WAPBT based on DFT, WHT, DCT and IDCT are deduced, which can be used in image compression instead of the conventional DCT, and the image compression scheme proposed in this paper is called WAPBT-based JPEG (WAPBT-JPEG). With optimal window sequence of WAPBT for image compression obtained by using generalized pattern search algorithm (GPSA), the peak signal to noise ratio (PSNR) and visual quality of the reconstructed images using the WAPBT-JPEG is outgoing DCT-based JPEG (DCT-JPEG) and APBT-based JPEG (APBTJPEG) approximately at all bit rates. What is more, by comparison with DCT-JPEG, the advantage of proposed scheme is that the quantization table is simplified and the transform coefficients can be quantized uniformly. Therefore, the computing time becomes shorter and the hardware implementation is easier.	coefficient;digital filter;discrete cosine transform;filter design;h.264/mpeg-4 avc;image compression;image processing;image retrieval;jpeg;pattern search (optimization);peak signal-to-noise ratio;quantization (signal processing);search algorithm;window function	Qiming Fu;Xiao Zhou;Cheng-You Wang;Baochen Jiang	2015	JCM	10.12720/jcm.10.4.284-293	lossless jpeg;computer vision;image compression;computer science;theoretical computer science;discrete cosine transform;jpeg;jpeg 2000;mathematics;quantization;algorithm;computer graphics (images)	Vision	43.194726182629594	-14.450118351440368	188421
12e906f4ca9f640b9ca4f7f43459dc3bd650dd5a	robust wavelet-based blind image watermarking against geometrical attacks	discrete wavelet transforms;watermarking;embedded watermark imperceptibility human visual system wavelet based blind image watermarking geometrical attack robustness discrete wavelet transform rotation scaling translation rotation invariant log polar mapping pixel wise masking method;discrete wavelet transforms watermarking image coding;image coding;discrete wavelet transform;log polar mapping;information science;geometrical attack robustness;wavelet transforms;protection;rotation invariance;scaling;wavelet transform;embedded watermark imperceptibility;robustness watermarking wavelet transforms discrete wavelet transforms fourier transforms information science wavelet domain protection digital images humans;translation;human visual system;fourier transforms;robustness;rotation scaling and translation;humans;pixel wise masking method;image watermarking;rotation invariant log polar mapping;wavelet domain;rotation;digital images;wavelet based blind image watermarking	Given its strong similarity to the human visual system, the wavelet transform has been applied in watermarking successfully. However, due to its sensitivity to rotation, scaling and translation, the wavelet-based watermarking algorithms are vulnerable to geometrical attacks. In this paper, we design a novel wavelet-based watermarking scheme resilient to geometrical attacks using a rotation-invariant log-polar mapping to eliminate all the effects of the geometrical operations in the input image before computing its discrete wavelet transform. A pixel-wise masking method used in the wavelet domain to scale the watermark is applied in order to achieve the best compromise between the requirements of robustness and imperceptibility for the embedded watermark. Experimental results demonstrate the advantages of our scheme, particularly its robustness against geometrical attacks.	algorithm;digital watermarking;discrete wavelet transform;embedded system;image scaling;performance;pixel;requirement;signal processing	Gui Xie;Hong Shen	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)	10.1109/ICME.2004.1394668	computer vision;discrete mathematics;second-generation wavelet transform;information science;computer science;theoretical computer science;mathematics;lifting scheme;wavelet transform	EDA	41.243995706720696	-10.369050121346396	188671
77458bcb94378e94887a861ff0d8c0df6a1f4013	shape adaptive all phase biorthogonal transform and its application in image coding	image coding;discrete cosine transform dct;all phase biorthogonal transform apbt;region of interest roi;journal;shape adaptive;期刊论文	In this paper shape adaptive all phase biorthogonal transform (SA-APBT) for coding arbitrarily shaped image segments is proposed. The proposed transform can be used in region-based image compression instead of the shape adaptive discrete cosine transform (SA-DCT). Region-based image coding method is made up of three procedures: image segmentation, contour coding and texture coding. An image is divided into region-of-interest (ROI) and background area by image segmentation. Both areas are encoded and decoded independently. Experimental results on test images show that compared with SA-DCT at the same bit rates, the PSNRs and visual effects of reconstructed images based on SA-APBT are comparable to the one based on SA-DCT especially at low bit rates. And the SA-APBT transform coefficients are quantized uniformly, while the SA-DCT transform coefficients are quantized according to different quantization steps in the quantization table. So the quantization based on SA-APBT is simpler and computational complexity is reduced.	coefficient;computational complexity theory;discrete cosine transform;image compression;image segmentation;region of interest;visual effects	Baochen Jiang;Ai-Ping Yang;Cheng-You Wang;Zheng-Xin Hou	2013	JCM	10.12720/jcm.8.5.330-336	sub-band coding;computer vision;transform coding;speech recognition;s transform;lapped transform;discrete cosine transform;mathematics;discrete fourier transform;top-hat transform;computer graphics (images)	Vision	43.35284176800799	-14.520898223861963	189128
3803b45418c9cb3cce8b8de16eb6f9e867c400ee	audio watermarking via emd	cuantificacion senal;filigranage numerique;quantization index modulation;protection information;digital watermarking;sound quality;metodo adaptativo;calidad sonora;filtering;evaluation performance;filtrage;audio signal processing;calculateur embarque;qualite sonore;methode empirique;performance evaluation;debit information;algoritmo adaptativo;signal audio;low frequency;information transmission;metodo descomposicion;evaluacion prestacion;metodo empirico;filtrado;audio signal;simulation;empirical method;methode decomposition;additive noise;ruido aditivo;compresion senal;simulacion;bruit additif;acoustic signal processing;methode adaptative;adaptive codes;indice informacion;compression signal;quantisation signal;synchronisation;decomposition method;adaptive algorithm;mp3 compression empirical mode decomposition emd adaptive audio watermarking intrinsic oscillatory components intrinsic mode functions imf synchronization codes audio perceptual quality host signal data embedding rate hidden watermark additive noise;algorithme adaptatif;signal quantization;proteccion informacion;decomposition modale empirique mode empirique tatouage audio modulation de l indice de quantification code de synchronisation;intrinsic mode function;traitement signal audio;synchronization;information protection;robustesse;quantification signal;synchronization code empirical mode decomposition intrinsic mode function audio watermarking quantization index modulation;filigrana digital;adaptive method;signal compression;basse frequence;boarded computer;information rate;robustness;watermarking robustness signal to noise ratio bit error rate synchronization digital audio players payloads;methode reechantillonnage;baja frecuencia;traitement signal acoustique;sincronizacion;transmision informacion;resampling method;transmission information;calculador embarque;senal audio;synchronisation adaptive codes audio watermarking quantisation signal;synchronization code;empirical mode decomposition;robustez	In this paper a new adaptive audio watermarking algorithm based on Empirical Mode Decomposition (EMD) is introduced. The audio signal is divided into frames and each one is decomposed adaptively, by EMD, into intrinsic oscillatory components called Intrinsic Mode Functions (IMFs). The watermark and the synchronization codes are embedded into the extrema of the last IMF, a low frequency mode stable under different attacks and preserving audio perceptual quality of the host signal. The data embedding rate of the proposed algorithm is 46.9-50.3 b/s. Relying on exhaustive simulations, we show the robustness of the hidden watermark for additive noise, MP3 compression, re-quantization, filtering, cropping and resampling. The comparison analysis shows that our method has better performance than watermarking schemes reported recently.	additive white gaussian noise;algorithm;code;digital watermarking;embedded system;hilbert–huang transform;mp3;simulation;utility functions on indivisible goods;watermark (data file)	Kais Khaldi;Abdel-Ouahab Boudraa	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2227733	synchronization;speech recognition;telecommunications;computer science	EDA	43.81018332194543	-10.10767440495088	189516
f4a927d45ec8336dd383d18daed8786fda31a94c	an efficient fingerprint image compression technique based on wave atoms decomposition and multistage vector quantization	multistage vector quantization;wave atoms decomposition;image compression;fingerprint images;arithmetic codings	Modern fingerprint image compression and reconstruction standards used by the US Federal Bureau of Investigation (FBI) are based upon the popular biorthogonal 9-7 discrete wavelet transform. Multiresolution analysis tools have been successfully applied to fingerprint image compression for more than a decade; we propose a novel fingerprint image compression technique based on wave atoms decomposition and multistage vector quantization. Wave atoms decomposition has been specifically designed for enhanced representation of oscillatory patterns and to convey precise temporal and spatial information. Our proposed compression scheme is based upon multistage vector quantization of processed wave atoms representation of fingerprint images. Wave atoms expansion is processed using mathematical morphological operators to emphasize and retain significant coefficients for transmission. Quantized information is encoded using arithmetic entropy scheme. The proposed image compression standard outperforms other well established methods and achieves PSNR gain up to 8.07 dB in comparison to FBI’s wavelet scalar quantization. Data mining, law enforcement, border security, and forensic applications can potentially benefit from our compression scheme.	coefficient;computation;data mining;decibel;discrete wavelet transform;fingerprint;image compression;mathematical morphology;multiresolution analysis;multistage amplifier;peak signal-to-noise ratio;quantization (signal processing);vector quantization;wavelet scalar quantization	Abdul Adeel Mohammed;Rashid Minhas;Q. M. Jonathan Wu;Maher A. Sid-Ahmed	2010	Integrated Computer-Aided Engineering	10.3233/ICA-2010-0327	computer vision;speech recognition;image compression;computer science;theoretical computer science;vector quantization	Vision	42.175022250081504	-14.955412148910485	189612
7e98d1743c6887a4522137eaa1d1f042b473fd7b	a reversible data hiding scheme for smvq indices		Reversible data hiding is a method that can guarantee that the cover image can be reconstructed correctly after the secret message has been extracted. Recently, some reversible data hiding schemes have concentrated on the VQ compression domain. In this paper, we present a new reversible data hiding scheme based on VQ and SMVQ techniques to enhance embedding capacity and compression rate. Experimental results show that our proposed scheme achieves higher embedding capacity and smaller average compression rate than some previous methods. Moreover, our proposed scheme maintains the high level of visual quality of the reconstructed image.	high-level programming language;lu decomposition;scheme;transmitter;vector quantization;yang	Chin-Chen Chang;Thai Son Nguyen	2014	Informatica, Lith. Acad. Sci.		discrete mathematics;theoretical computer science;mathematics;algorithm	EDA	39.76843572458013	-11.243121720627204	189952
c0097564642c68ab365b683ad20c1173aaa3dbd3	an image compression algorithm based on neural networks	cuantificacion senal;algorithm performance;data compression;quantifier;huffman codes;huffman code;compression image;cuantificacion vectorial;signal quantization;vector quantization;image compression;codigo huffman;resultado algoritmo;quantification signal;code huffman;quantificateur;performance algorithme;predictive vector quantization;compresion dato;reseau neuronal;cuantificador;red neuronal;compression donnee;neural network;compresion imagen;quantification vectorielle	In this paper a combination of algorithms useful for image compression standard is discussed. The main algorithm, named predictive vector quantization (PVQ), is based on competitive neural networks quantizer and neural networks predictor. Additionally, the noiseless Huffman coding is used. The experimental results are presented and the performance of the algorithm is discussed.	algorithm;data compression;image compression;neural networks	Robert Cierniak	2004		10.1007/978-3-540-24844-6_108	speech recognition;computer science;theoretical computer science;machine learning;artificial neural network;algorithm;huffman coding	ML	45.68201071059014	-11.645949692452454	190093
fc4e473661a19c554b388e37d3a7c6db32243afb	mde‐based image steganography with large embedding capacity	pixel pair matching;steganography;reference matrix;modification direction exploiting	The big data era calls for image steganography with large embedding capacity and good image quality. Previous methods described in the literature pay more attention to image quality rather than payload. This paper proposes a large capacity steganographic method based on modification direction exploitation and pixel pair matching. By virtue of a reference matrix with particular properties, one or two 9-ary digits can be embedded into each cover pixel pair depending on different payloads. Experimental results demonstrate high embedding capacity as well as good image quality and security. Copyright © 2015 John Wiley u0026 Sons, Ltd.		Zhao-Xia Yin;Bin Luo	2016	Security and Communication Networks	10.1002/sec.1275	steganography tools;computer science;theoretical computer science;pattern recognition;steganography;world wide web	ML	40.01776212736993	-11.876562194217174	190202
c7828eea8498f78651ec440151ab22ed2a0525d3	efficient transform-based texture image retrieval techniques under quantization effects	content-based image retrieval;image compression;transform domain;moment preserving quantization;distribution preserving quantization;feature extraction	With the great demand for storing and transmitting images as well as their managing, the retrieval of compressed images is a field of intensive research. While most of the works have been devoted to the case of losslessly encoded images (by extracting features from the unquantized transform coefficients), new studies have shown that lossy compression has a negative impact on the performance of conventional retrieval systems. In this work, we investigate three different quantization schemes and propose for each one an efficient retrieval approach. More precisely, the uniform quantizer, the moment preserving quantizer and the distribution preserving quantizer are considered. The inherent properties of each quantizer are then exploited to design an efficient retrieval strategy, and hence, to reduce the drop of retrieval performances resulting from the quantization effect. Experimental results, carried out on three standard texture databases and a color dataset, show the benefits which can be drawn from the proposed retrieval approaches.	coefficient;database;image retrieval;lossless compression;lossy compression;performance;quantization (signal processing);transmitter	Amani Chaker;Mounir Kaaniche;Amel Benazza-Benyahia;Marc Antonini	2016	Multimedia Tools and Applications	10.1007/s11042-016-4205-5	computer vision;computer science;theoretical computer science;information retrieval	Vision	41.67950202023703	-15.107300802178766	190517
efa9ec029d08604a0f9c291cc3b38800f078b053	a single-pass-based localized adaptive interpolation filter for video coding	adaptive interpolation filter;wiener filters encoding interpolation statistical analysis video coding;interpolation;prediction error;single pass;wiener filters;optimal interpolation;coding gain;local adaptation;memory access;encoding wiener filter interpolation video coding filtering algorithms filtering theory;video coding;wiener filter adaptive interpolation filter h264 avc localized filtering motion compensated prediction single pass;statistical analysis;filtering algorithms;computational complexity;wiener filter;bjontegaard distortion bit rate reduction single pass based localized adaptive interpolation filtering video coding fractional pixel interpolation interprediction error training based wiener filter multipass encoding structure spl aif algorithm statistical characteristic h 264 avc interpolation filter rate distortion optimization macroblock level predictive coding method;motion compensated prediction;localized filtering;rate distortion optimization;encoding;predictive coding;filtering theory;h264 avc	Recently, an efficient coding tool named adaptive interpolation filtering (AIF) has been proposed to hybrid video coding scheme. By introducing Wiener filter into the fractional-pixel interpolation procedure, AIF can reduce the inter-prediction error and improve coding efficiency significantly. However, the training-based Wiener filter mechanism brings AIF an inherent multi-pass encoding structure, which imposes big burdens on the encoder in terms of huge computational complexity and memory access. In this paper, we propose a single-pass-based localized adaptive interpolation filtering (SPL-AIF) algorithm for video coding, which can reduce the complexity of AIF dramatically without sacrifice of its outstanding coding performance. The proposed SPL-AIF algorithm is based on the observation that there is a high correlation among optimal interpolation filters of consecutive frames, and different regions in a frame often possess different statistical characteristics. Accordingly, the proposed algorithm can be designed including two major parts. First, a competitive filter set which includes the optimal interpolation filters of several previous frames as well as the fixed H.264/AVC interpolation filters is built up for the coding of the current frame. Then a rate-distortion optimization criterion is used to select the best one at macroblock (MB) level. In order to reduce overhead, a predictive coding method is used to compress the filter signaling flag for each MB. Experimental results show that, by using the proposed algorithm, the encoding complexity can be reduced significantly while the average coding gain in Bjöntegaard distortion bit-rate reduction can be improved about 1% compared with the multi-pass AIF. The proposed method has been adopted into the Video Coding Expert Group Key Technology Area software.	algorithm;algorithmic efficiency;coding gain;computational complexity theory;data compression;distortion;encoder;experiment;filesystem-level encryption;group key;h.264/mpeg-4 avc;interpolation;macroblock;mathematical optimization;overhead (computing);pixel;raster document object;rate–distortion optimization;wiener filter	Kai Zhang;Xun Guo;Jicheng An;Yu-Wen Huang;Shawmin Lei;Wen Gao	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2157194	computer vision;interpolation;theoretical computer science;context-adaptive variable-length coding;mean squared prediction error;coding gain;coding tree unit;mathematics;wiener filter;rate–distortion optimization;context-adaptive binary arithmetic coding;computational complexity theory;encoding;statistics	Vision	44.70512362104478	-15.996876237938022	190600
2bddfea5d2007bd21045e6e0c460979c10ff10fc	improving embedding efficiency by incorporating sdcs and wpc	secret data bits;double layer;sum and difference covering set;steganography cryptography security of data;information hiding;steganographic security;wet paper code information hiding steganography embedding efficiency sum and difference covering set;data mining;receivers;manganese;arrays;steganography;cryptography;pixel;double layered embedding;embedding efficiency;wet paper code;security;embedding distortion embedding efficiency secret data bits steganography steganographic security sum and difference covering set wet paper code double layered embedding;security of data;embedding distortion;graphics;steganography data security computer security computer science educational technology laboratories embedded software computer science education art digital images	Embedding efficiency, which is defined as the average number of secret data bits embedded per one embedding change, is an important attribute directly influencing the security of steganography. Higher embedding efficiency is usually expected to get more secure steganographic method. In this paper, by incorporating two previously introduced techniques: the “sum and difference covering set” (SDCS) and the “wet paper code” (WPC), a novel double-layered embedding method is presented. As compared with the original SDCS-based steganography, the novel method can embed more bits without introducing additional embedding distortion, therefore the embedding efficiency is improved. In summary, the proposed method provides some steganographic schemes with diverse embedding rates and good embedding efficiency.	distortion;embedded system;steganography;williams pinball controller	Chengcheng Liu;Xiaolong Li;Xiaoqing Lu;Bin Yang	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202670	computer science;cryptography;graphics;information security;manganese;theoretical computer science;mathematics;steganography;internet privacy;information hiding;world wide web;double layer;pixel;statistics	Vision	39.26780460338019	-10.72466363206558	190984
20f303aef2c29d9d5d539385782ab3539eada666	a novel image coding scheme by using two-channel complex-valued filter banks	image coding;lattices;transform coding;wavelet transforms;signal processing;signal processing algorithms	A novel image coding scheme by using two-channel complex-valued filter banks (CFBs) is proposed in this paper. Up to now, most of the image coding schemes have used real-valued filter banks (RFBs). In other words, there are no efficient image coding framework using CFBs since the amount of samples transformed by CFBs is twice as much as that of RFBs. However, CFBs are expected to have better frequency characteristics than RFBs, since CFBs have more design parameters than that of RFBs. Thus, if the problem is solved, CFBs could be efficient for image coding. In this paper, a new algorithm which can preserve the amount of information when CFBs are applied to image coding is introduced. It is shown that the proposed image coding scheme using CFBs is superior to conventional real-valued wavelet-based image coding.	algorithm;coefficient;filter bank;image compression;simulation;wavelet	Seisuke Kyochi;Yuichi Tanaka;Masaaki Ikehara	2007	2007 15th European Signal Processing Conference		computer vision;electronic engineering;theoretical computer science;mathematics	Robotics	43.31895714539133	-14.783143424305038	191021
4cc8bc4e5cc67897da3759e7b1e051645dd4044d	data hiding in spatial color images on smartphones by adaptive r-g-b lsb replacement			least significant bit;smartphone	Haeyoung Lee	2018	IEICE Transactions		artificial intelligence;information hiding;computer vision;pattern recognition;computer science;least significant bit	Vision	41.769326673717266	-12.216552271969631	191403
99cd74bee7d262bddcee738c4a381a9d728dbe87	a novel design of criticially sampled contourlet transform and its application to image coding	parallelogram filter banks;quadrant filter banks;frequency synthesizers;bottom up;image coding;filter bank;image coding critically sampled contourlet quadrant filter banks parallelogram filter banks wavelet transform;top down;csct;indexing terms;critically sampled contourlet transform;wavelet transform;design method;image reconstruction;transforms;target frequency plane partition critically sampled contourlet transform image coding csct;critically sampled contourlet;image coding filter bank frequency synthesizers design methodology frequency conversion computed tomography image processing wavelet transforms image sampling tree data structures;plane partition;frequency conversion;design methodology;target frequency plane partition	"""In this paper, a novel design method of critically sampled contourlet transform (CSCT) is proposed. Although, several types of CSCT have been proposed, they have some problems on efficiency and flexibility of their frequency plane partition patterns. In contrast to the way in conventional design methods based on a """"top-down"""" approach, the proposed one is based on a """"bottom-up"""" one. That is, the proposed CSCT decomposes the frequency plane into small directional sub- bands, and then synthesizes them up to a target frequency plane partition, while the conventional ones decompose into it directly. By this way, the proposed CSCT can provide an efficient and flexible frequency plane partition for image coding."""	bottom-up parsing;contourlet;top-down and bottom-up design	Shizuka Higaki;Seisuke Kyochi;Yuichi Tanaka;Masaaki Ikehara	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712396	computer vision;speech recognition;design methods;top-down and bottom-up design;mathematics	Robotics	44.399901938131144	-15.239754775942698	192348
23eb059344ac43a697ebdb811f4d0ff6d9527585	an interpolative scheme for fractal image compression in the wavelet domain	transformation ondelette;rate distortion;interpolation;image coding;interpolacion;image;wavelet decomposition;problema inverso;fixed point;codificacion;wavelet transform;inverse problem;imagen;compresion;coding;fractal scheme;fractal;transformacion ondita;compression;wavelet domain;probleme inverse;wavelet transformation;codage;intermediate representation;fractal image compression	Standard fractal image coding methods seek to nd a contractive fractal transform operator T that best scales and copies subsets of a target image I (its domain blocks) onto smaller subsets (its range blocks). The xed point of this operator is an approximation to the image I and can be generated by iteration of T . Although generally good delity is achieved at signi cant compression rates, the method can su er from blockiness artifacts. This inspired the introduction of fractal-wavelet transforms which operate on the wavelet representations of functions: Wavelet coe cient subtrees are scaled and copied onto lower subtrees. We propose a simple adaptive and unrestricted fractal-wavelet scheme that adopts a dynamic partitioning of the wavelet decomposition tree, resulting in intermediate representations between the various dyadic levels. In this way, one may (i) generate a continuous and relatively smooth rate distortion curve and (ii) encode images at a pre-de ned bit rate or representation tolerance error.	approximation;distortion;dyadic transformation;encode;fractal compression;fractal transform;image compression;iteration;rate–distortion theory;tree (data structure);wavelet transform	Mohsen Ghazel;Edward R. Vrscay;Amir K. Khandani	1999		10.1007/3-540-48375-6_24	combinatorics;mathematical analysis;discrete mathematics;interpolation;mathematics;fractal compression;statistics	Vision	45.10841450078334	-13.859292558803789	193505
e107751ca8f3b5e58d5197d3d4ae14dd79f5b673	a wavelet-tree-based watermarking method using fast ica	discrete wavelet transforms;blind watermarking;watermarking;signal processing attacks;blind image watermarking;fast ica;image coding;discrete wavelet transform;psnr;chaos;wavelet tree structure;blind source separation;discrete wavelet transform dwt;wavelet transforms blind source separation image coding watermarking;independent component analysis;data mining;watermarking independent component analysis discrete wavelet transforms chaos mathematical model logistics data mining entropy signal processing algorithms signal processing;wavelet transforms;human visual system;signal processing;tree structure;image watermarking;discrete wavelet transform dwt blind watermarking chaos sequence fast independent component analysis fast ica;fast independent component analysis fast ica;independent component analysis fast ica blind image watermarking wavelet tree structure human visual system signal processing attacks;chaos sequence	A new blind image watermarking scheme was proposed based on Fast ICA and the wavelet tree structure, which featured the following characteristics as scrambling the watermark to enhance the security, taking advantage of the human visual system properties and extracting the watermark without original image. The experimental results show that the proposed blind image watermarking scheme is invisible and robust against various signal processing attacks such as compression, noise adding, cropping and so on.	fastica;independent computing architecture;wavelet tree	Shijie Ren;Xin Su	2009		10.1109/ISCID.2009.188	independent component analysis;computer vision;speech recognition;peak signal-to-noise ratio;digital watermarking;computer science;machine learning;signal processing;pattern recognition;mathematics;blind signal separation;tree structure;human visual system model;discrete wavelet transform;watermark;wavelet transform	Robotics	41.40328421363563	-10.923247070290014	193733
c0e547d7009a9a2340ba55238b2b921fe82507cf	high capacity reversible data hiding and content protection for radiographic images	watermarking;data hiding;biomedical image processing	The watermarking of digital images has an important role in the protection of digital content with respect to many aspects. In this paper we present a reversible watermarking algorithm for hiding information into medical images having luminance histograms with particular characteristics. Some radiographic images have the property that not all the gray levels are present; this leads to sequences of 0 values (0-runs) in the corresponding histograms. It is possible to use these 0-runs to encode information by modifying pixels having gray levels contiguous to these runs; by encoding also the run information it is possible to restore the original image after extracting the stored data. In this work we present a novel reversible watermarking technique capable of exploiting all the 0-runs in the image histogram to achieve high capacity. We show that an optimization problem arises for those cases in which two or more non-zero frequency gray levels are contiguous to 0-runs. Part of the watermark information may be devoted to a digital signature of the original image, whose authenticity may be also verified by a user.		Davide Cavagnino;Maurizio Lucenteforte;Marco Grangetto	2015	Signal Processing	10.1016/j.sigpro.2015.05.020	computer vision;digital watermarking;computer science;theoretical computer science;data mining;information hiding	Graphics	39.236441829567724	-11.288407674809061	193861
b142daef7c7d0fcf4ae2b904a671d8b080a8b34e	transform-domain wiener filtering for h.264/avc video encoding and its implementation	wiener filters;denoising scheme video encoding h 264 transform domain wiener filtering scheme noise reduction integer transformed block multiplication factor quantization process filter coefficient matrix;transform coding;matrix algebra;video coding;noise reduction;image denoising video coding filtering theory wiener filters transform coding matrix algebra;image denoising;wiener filter;wiener filter automatic voltage control encoding noise reduction filtering quantization video sequences discrete cosine transforms additive noise video coding;filtering theory	For coding efficiency as well as noise reduction, efficient de-noising needs to be performed prior to video encoding. This paper proposes a transform-domain Wiener filtering scheme in an H.264/AVC video encoder. We show that the generalized Wiener filtering for each integer-transformed block is equivalent to multiplication of multiplication factor (MF) in a block with a proper filter coefficient matrix in a quantization process. Also, we implement efficiently the proposed scheme by employing several predetermined modified MF's for quantization. Experimental results show that the proposed de-noising scheme provides outstanding coding efficiency as well as noise reduction in an H.264/AVC video encoder.	algorithmic efficiency;coefficient;data compression;encoder;h.264/mpeg-4 avc;image scaling;noise reduction;quantization (signal processing);simulation	Byung Cheol Song;Nak Hoon Kim;Kang Wook Chun	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530445	computer vision;transform coding;computer science;theoretical computer science;noise reduction;mathematics;wiener filter;context-adaptive binary arithmetic coding;video denoising;statistics;wiener deconvolution	Vision	44.76004440275319	-16.368233665845043	194384
259ade2b66d2067b5574da08a72f44098e87cb71	correlation-based motion vector processing with adaptive interpolation scheme for motion-compensated frame interpolation	union bidireccional;artefacto;traitement signal;motion vector processing;metodo correlacion;metodo adaptativo;correlation based motion vector processing;interpolation;reliability;metodo vectorial;estimation mouvement;liaison bidirectionnelle;motion compensated frame interpolation;motion compensation;correlation method;interpolacion;estimacion movimiento;motion estimation;video sequences;methode adaptative;qualite image;visual quality;motion estimation interpolation motion compensation;motion vector correlation;video occlusions frame rate up conversion motion compensated frame interpolation mcfi motion vector correlation motion vector processing;artefact;motion compensated;compensation mouvement;visualization;senal video;signal video;motion vector;periodic structures;signal processing;image quality;pixel;vector method;adaptive method;image sequence;motion vector correction correlation based motion vector processing adaptive frame interpolation scheme motion compensated frame interpolation motion vector reliability classification;merging;video signal;video occlusions;methode vectorielle;motion vector reliability classification;secuencia imagen;calidad imagen;frame rate up conversion;motion compensated frame interpolation mcfi;motion vector correction;interpolation motion estimation decoding motion detection motion analysis robustness video sequences consumer electronics electronics industry;bidirectional link;procesamiento senal;adaptive frame interpolation scheme;sequence image;methode correlation	In this paper, we address the problems of unreliable motion vectors that cause visual artifacts but cannot be detected by high residual energy or bidirectional prediction difference in motion-compensated frame interpolation. A correlation-based motion vector processing method is proposed to detect and correct those unreliable motion vectors by explicitly considering motion vector correlation in the motion vector reliability classification, motion vector correction, and frame interpolation stages. Since our method gradually corrects unreliable motion vectors based on their reliability, we can effectively discover the areas where no motion is reliable to be used, such as occlusions and deformed structures. We also propose an adaptive frame interpolation scheme for the occlusion areas based on the analysis of their surrounding motion distribution. As a result, the interpolated frames using the proposed scheme have clearer structure edges and ghost artifacts are also greatly reduced. Experimental results show that our interpolated results have better visual quality than other methods. In addition, the proposed scheme is robust even for those video sequences that contain multiple and fast motions.	authorization;frame (physical object);ieee xplore;interpolation imputation technique;megabyte;morphologic artifacts;motion interpolation;obstruction;personnameuse - assigned;pixel;recursion;ringing artifacts;vector processor;visual artifact	Ai-Mei Huang;Truong Q. Nguyen	2009	IEEE Transactions on Image Processing	10.1109/TIP.2008.2010206	computer vision;interpolation;quarter-pixel motion;computer science;signal processing;motion estimation;control theory;mathematics;block-matching algorithm;motion field;algorithm;statistics;computer graphics (images)	Vision	46.35710073791485	-14.843831360494885	195314
6fea91ab6ce7a278744b3b61e5bfd37ace23e938	hiding data in spatial domain images with distortion tolerance	watermarking;quantization;data hiding;secret image;authentication;copyright;distortion tolerance;algorithm;steganography;lsb substitution;image quality;half tone images	Data hiding is a technique that is used to embed secret information into a cover media. It has been widely used in protecting copyright and transmitting sensitive data over an insecure channel. Conventional data hiding schemes only focus on how to reduce the distortion when sensitive data is embedded into the cover image. However, the transmitted images may be compressed or occur transmitting errors. If such errors occur, the receiver cannot extract the correct information from the stego-image. In this paper, we proposed a novel hiding data scheme with distortion tolerance. The proposed scheme not only can prevent the quality of the processed image from being seriously degraded, but also can simultaneously achieve distortion tolerance. Experimental results show that the proposed scheme indeed can obtain a good image quality and is superior to the other schemes in terms of its distortion tolerance.	distortion;rate–distortion theory	Iuon-Chang Lin;Yang-Bin Lin;Chung-Ming Wang	2009	Computer Standards & Interfaces	10.1016/j.csi.2008.05.010	image quality;computer vision;quantization;digital watermarking;computer science;theoretical computer science;authentication;steganography;internet privacy;information hiding	Graphics	39.659734028955874	-11.775937048828057	195553
69c54b692f0c2d2c5b84087cda14b34d944580f9	reversible image hiding scheme for high quality based on histogram shifting	image coding;histogram shifting;modulo operation histogram shifting reversible image hiding scheme pixel values cover image peak point zero point psnr hiding capacity image quality gradient adjusted prediction gap;data encapsulation;steganography;gradient adjusted prediction reversible data hiding histogram shifting steganography modulo operation;gradient methods;gradient adjusted prediction;modulo operation;reversible data hiding;histograms nickel image quality educational institutions psnr data mining prediction algorithms;image coding data encapsulation gradient methods	In 2006, Ni et al. proposed reversible image hiding scheme using histogram shifting. Their scheme modified the pixel values of the cover-image between the peak point and the zero point in the histogram. PSNR of almost every histogram shifting based methods was about 48dB because many pixels that don't hide the secret data was modified. In this paper, to the higher hiding capacity and image quality improved Ni et al.'s scheme using Gradient-adjusted prediction (GAP) and modulo operation. In experimental results, the hiding capacity of the proposed scheme is superior to Ni et al.'s scheme. Also the image quality of the proposed scheme is increased by about 7 dB than Ni et al.'s scheme.	decibel;gradient;image histogram;image quality;modulo operation;peak signal-to-noise ratio;pitch shift;pixel;polynomial-time approximation scheme	Dae-Soo Kim;Gil-Je Lee;Kee-Young Yoo	2013	2013 10th International Conference on Information Technology: New Generations	10.1109/ITNG.2013.61	computer vision;histogram matching;theoretical computer science;steganography;modulo operation;statistics;image histogram	Vision	43.96191492637655	-16.678287232400557	195857
4c72064797e493e65ed9b1e5c1e4a16ef1adf638	securing high resolution grayscale facial captures using a blockwise coevolutionary ga	evolutionary computation;intelligent watermarking;biometrics;grayscale texture masks;genetic algorithms;cooperative coevolution	In biometric systems, reference facial images captured during enrollment are commonly secured using watermarking, where invisible watermark bits are embedded into these images. Evolutionary Computation (EC) is widely used to optimize embedding parameters in intelligent watermarking (IW) systems. Traditional IW methods represent all blocks of a cover image as candidate embedding solutions of EC algorithms, and suffer from premature convergence when dealing with high resolution grayscale facial images. For instance, the dimensionality of the optimization problem to process a 2048 Â 1536 pixel grayscale facial image that embeds 1 bit per 8 Â 8 pixel block involves 49k variables represented with 293k binary bits. Such Large-Scale Global Optimization problems cannot be decomposed into smaller independent ones because watermarking metrics are calculated for the entire image. In this paper, a Blockwise Coevolutionary Genetic Algorithm (BCGA) is proposed for high dimensional IW optimization of embedding parameters of high resolution images. BCGA is based on the cooperative coevolution between different candidate solutions at the block level, using a local Block Watermarking Metric (BWM). It is characterized by a novel elitism mechanism that is driven by local blockwise metrics, where the blocks with higher BWM values are selected to form higher global fitness candidate solutions. The crossover and mutation operators of BCGA are performed on block level. Experimental results on PUT face image database indicate a 17% improvement of fitness produced by BCGA compared to classical GA. Due to improved exploration capabilities, BCGA convergence is reached in fewer generations indicating an optimization speedup. In face recognition, high resolution facial capture images are commonly transferred and archived for various access control or surveillance applications. Since these images are vulnerable to unauthorized manipulations, watermarking is widely used to secure such images by embedding invisible watermark bits. The watermarking process should satisfy the trade-off between the distortion resulting from embedding the watermark (quality), and the resistance of the watermarked image to manipulations (robust-ness). The distortion resulting from the embedding is commonly measured using watermark quality metrics which is based on Human Vision System (HVS). The resistance of the watermarked image to manipulations is commonly measured using watermark robustness metrics, these metrics compare the original watermark with the extracted one from the manipulated watermarked image. Computational intelligence techniques are widely used to find the optimal watermark embedding parameters to satisfy the trade-off between watermark quality and robustness. Different Evolutionary Computation (EC) techniques have been proposed in intelligent watermarking …	1-bit architecture;access control;archive;authorization;biometrics;chebyshev polynomials;computational intelligence;concatenation;cooperative coevolution;digital watermarking;distortion;earthbound;embedded system;evolutionary computation;facial motion capture;facial recognition system;genetic algorithm;global optimization;grayscale;human visual system model;iw engine;image resolution;iteration;mathematical optimization;optimization problem;pixel;premature convergence;simulation;social network aggregation;software release life cycle;speedup;very-large-scale integration;weight function	Bassem S. Rabil;Safa Tliba;Eric Granger;Robert Sabourin	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.043	computer vision;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;biometrics;evolutionary computation	Vision	40.428789765998815	-10.291036936321058	196144
84e6f37f04fac08850af4af01538b8b32c78ce81	a blind watermarking scheme using new nontensor product wavelet filter banks	transformation ondelette;blind watermarking;filigranage numerique;protection information;digital watermarking;traitement signal;watermarking;computer graphics computer security copyright image processing computer assisted pattern recognition automated sensitivity and specificity signal processing computer assisted wavelet analysis;empirical study;new product;singularite;protection copie;tensile stress;filter bank;image resolution;symmetric matrices blind watermarking nontensor product wavelet filter banks digital product copyright protection illegal usage multiresolution property wavelet transform image representation image resolution wavelet decomposition human visual system human eyes image singularity;banc filtre;digital product copyright protection;copy protection;subband decomposition;human eyes;producto nuevo;watermarking filter bank wavelet domain humans copyright protection wavelet transforms image resolution visual system eyes frequency;wavelet decomposition;analyse multiresolution;matriz simetrica;symmetric matrix;copyright protection;algorithme;symmetric matrices;wavelet transforms;algorithm;haute frequence;multiresolution property;wavelet transform;descomposicion subbanda;proteccion informacion;matrix decomposition;human visual system;image representation;signal processing;information protection;wavelet transforms filtering theory image representation image resolution matrix decomposition watermarking;banco filtro;filigrana digital;nontensor product wavelet filter banks;singularidad;nontensor product wavelet filter;produit nouveau;matrice symetrique;robustness;singularities;illegal usage;image singularity;transformacion ondita;decomposition sous bande;wavelet domain;alta frecuencia;multiresolution analysis;procesamiento senal;high frequency;filtering theory;wavelet transformation;analisis multiresolucion;watermarking nontensor product wavelet filter singularities;singularity;algoritmo	As an effective method for copyright protection of digital products against illegal usage, watermarking in wavelet domain has recently received considerable attention due to the desirable multiresolution property of wavelet transform. In general, images can be represented with different resolutions by the wavelet decomposition, analogous to the human visual system (HVS). Usually, human eyes are insensitive to image singularities revealed by different high frequency subbands of wavelet decomposed images. Hence, adding watermarks into these singularities will improve the imperceptibility that is a desired property of a watermarking scheme. That is, the capability for revealing singularities of images plays a key role in designing wavelet-based watermarking algorithms. Unfortunately, the existing wavelets have a limited ability in revealing singularities in different directions. This motivates us to construct new wavelet filter banks that can reveal singularities in all directions. In this paper, we utilize special symmetric matrices to construct the new nontensor product wavelet filter banks, which can capture the singularities in all directions. Empirical studies will show their advantages of revealing singularities in comparison with the existing wavelets. Based upon these new wavelet filter banks, we, therefore, propose a modified significant difference watermarking algorithm. Experimental results show its promising results.	bank (environment);cns disorder;coefficient;computer vision;digital watermarking;effective method;embedding;eye;filter bank;html link type - copyright;human visual system model;normal statistical distribution;pattern recognition;visually impaired persons;wavelet transform;algorithm;vetispiradiene synthase activity	Xinge You;Liang Du;Yiu-ming Cheung;Qiuhui Chen	2010	IEEE Transactions on Image Processing	10.1109/TIP.2010.2055570	wavelet;computer vision;second-generation wavelet transform;digital watermarking;computer science;theoretical computer science;signal processing;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;algorithm;symmetric matrix;wavelet transform	Graphics	43.78375670385647	-10.049133111019238	197134
f571da23a60f022679eb4b6d03d46761132562be	secure steganography in compressed video bitstreams	data hiding;security properties;statistical invisibility;data compression;information security;information science;videoconference;availability;video compression;embedding strategy;compressed video bitstreams;decompression process;video steganalysis;secure steganography;video coding;steganography;streaming media;feedback loop;cryptography;statistical invisibility video steganography data hiding;video coding cryptography data compression security of data;sun;closed loop feedback secure steganography compressed video bitstreams compressed video secure steganography algorithm decompression process embedding strategy video steganalysis;video steganography;closed loop feedback;compressed video secure steganography algorithm;computer bugs;steganography video compression videoconference streaming media availability information security sun information science feedback loop computer bugs;security of data;compressed video	A new compressed video secure steganography (CVSS) algorithm is proposed. In the algorithm, embedding and detection operations are both executed entirely in the compressed domain, with no need for the decompression process. The new criteria employing statistical invisibility of contiguous frames is used to adjust the embedding strategy and capacity, which increases the security of proposed algorithm. Therefore, the collusion resistant properties are obtained. Video steganalysis with closed loop feedback manner is design as a checker to find out obvious bugs. Experimental results showed this scheme can be applied on compressed video steganography with high security properties.	algorithm;data compression;elegant degradation;pixel;software bug;steganalysis;steganography	Bin Liu;Fenlin Liu;Chunfang Yang;Yifeng Sun	2008	2008 Third International Conference on Availability, Reliability and Security	10.1109/ARES.2008.140	steganography tools;computer science;theoretical computer science;internet privacy;computer security	EDA	39.31496632138177	-12.567191064284978	197829
b0082e7ea337f4a9761e18707bf554fe2fd2fb48	saliency map based image steganography	saliency map perceptual degradation steganography;image information image steganography saliency map electronics data security watermarking copyright secret number unauthorized reproduction steganography technique statistical block analysis saliency computation;steganography;statistical analysis;steganography image watermarking security of data statistical analysis;image watermarking;degradation visualization data mining statistical analysis discrete cosine transforms data security;security of data	Steganography is an emerging field for hiding secure data. This is useful in electronics data security domains such as watermarking, copyright, secret number, unauthorized reproduction etc. In this paper we propose a saliency based steganography technique, which uses a statistical block analysis of the saliency map to identify regions for secure data embedding. Saliency computation is also an active research area, and for best result, different techniques are combined to generate the saliency map. Various steganography techniques such as sequential, block, and transform domain do not take into account the variation in perceptual information of an image. In saliency based embedding, visual attention is considered for selecting regions for embedding of secure data. The proposed saliency based steganography provides a new dimension for steganography, where embedding of secure data causes minimum affect on visual objects of an image as well as little distortion in segmentation boundaries.	authorization;computation;data security;digital watermarking;distortion;elegant degradation;image segmentation;mike lesser;steganography;visual objects	Rama Kant Singh;Brejesh Lall	2013	2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013)	10.1109/IVCNZ.2013.6727053	computer vision;steganography tools;computer science;theoretical computer science;steganography;internet privacy;statistics	Vision	41.111627037092	-11.293152587082556	198222
0436cbbc3fc87ca9abb82154c1ab05c7d0c00058	adaptive wavelet codec for noisy image compression	quantization;image coding;codecs;psnr;data compression;lossy compression;wavelet transforms codecs data compression image coding;noise measurement;wavelet transforms;image compression;noisy image compression adaptive wavelet codec threshold interval quantization interval ssim criteria psnr criteria;quantization codecs noise measurement image coding wavelet coefficients psnr;wavelet coefficients	It is proposed an adaptive, data-driven wavelet-based method for lossy compression of noisy images. The suggested method uses a common criterion to simultaneously estimate the values of threshold and quantization interval. The results of modeling show the advantage of the designed codec comparing to well-known codecs in the sense of PSNR and SSIM criteria.	codec;image compression;lossy compression;peak signal-to-noise ratio;structural similarity;wavelet	Yuri S. Bekhtin	2011	2011 9th East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2011.6116587	data compression;wavelet;data compression ratio;electronic engineering;speech recognition;image compression;pattern recognition;lossless compression;wavelet packet decomposition;wavelet transform	Embedded	43.48041426276823	-15.159448804386818	199089
4344f85d36e0fd8c83ccfa14d87e57a4a10c60e7	reversible dual-image-based hiding scheme using block folding technique		The concept of a dual-image based scheme in information sharing consists of concealing secret messages in two cover images; only someone who has both stego-images can extract the secret messages. In 2015, Lu et al. proposed a center-folding strategy where each secret symbol is folded into the reduced digit to reduce the distortion of the stego-image. Then, in 2016, Lu et al. used a frequency-based encoding strategy to reduce the distortion of the frequency of occurrence of the maximum absolute value. Because the folding strategy can obviously reduce the value, the proposed scheme includes the folding operation twice to further decrease the reduced digit. We use a frequency-based encoding strategy to encode a secret message and then use the block folding technique by performing the center-folding operation twice to embed secret messages. An indicator is needed to identify the sequence number of the folding operation. The proposed scheme collects several indicators to produce a combined code and hides the code in a pixel to reduce the size of the indicators. The experimental results show that the proposed method can achieve higher image quality under the same embedding rate or higher payload, which is better than other methods.	arithmetic underflow;chi;distortion;encode;image quality;lu decomposition;pixel;reed–solomon error correction;steganalysis;steganography	Tzu-Chuen Lu;Hui-Shih Leng	2017	Symmetry	10.3390/sym9100223	information hiding;pixel;real-time computing;combinatorics;theoretical computer science;mathematics;image quality;distortion;absolute value	EDA	40.90659687599078	-12.564277768884164	199155
42ec66b090a0bada74d7f6d01e9fd0104466155e	adaptive wavelet transforms for image coding using lifting	image coding;data compression;predictive value;edge regions adaptive wavelet transforms image coding prediction residuals image compression smooth image regions wavelet coefficients nonlinearities spatial domain framework lifting formalism linear prediction operation local image properties high order polynomial predictors;nonlinear filter;wavelet transforms image coding polynomials decoding wavelet domain buildings bit rate image reconstruction harmonic analysis educational institutions;transform coding;adaptive codes;polynomials image coding image representation transform coding adaptive codes data compression wavelet transforms prediction theory;linear predictive;polynomials;wavelet transforms;wavelet transform;prediction theory;image compression;image representation	Image compression relies on e cient representations of images, and within smooth image regions, the wavelet transform provides such a representation. However, near edges, wavelet coe cients decay slowly and are expensive to code. We focus on improving the transform by incorporating adaptivity. Construction of non-linear lter banks has been discussed [1, 2], but the question of how to utilize the non-linearities remained. We answer this question by describing our transform via lifting [3]. Lifting provides a spatial domain framework for the wavelet transform. In the lifting formalism, wavelet coe cients are seen as prediction residuals from a linear prediction operation. Wavelet coe cients are large near edges because the linear predictors are built to interpolate low order polynomials. Our goal is to avoid this problem by adapting the predictor based on local image properties. In smooth regions of the image, we use high order polynomial predictors. We adaptively reduce the prediction order to avoid attempting to predict values across discontinuities. The adaption mechanism complicates our transform and makes it non-linear. There are some important issues we must resolve. First, how do we interpret the scaling function coe cients in our non-linear transform? Second, how do we ensure that the new non-linear transform will be stably invertible? We address both issues by building our non-linear transform from an unconventional form of a linear wavelet transform [4]. Our starting point is a wavelet transform that factors into an updating lifting step followed by a predicting lifting step (more standard transforms perform these steps in the opposite order). This modi cation enables us to construct a stably invertible transform and ensures that we can maintain synchronization between the encoder and the decoder. For our adaptive transform we choose an N 2 f1; 3; 5; 7g predictor by analyzing each 7-point prediction window. Because the decoder and encoder are synchronized, they make identical prediction decisions. For a given bit rate, our adaptive transform results in sharper horizontal and vertical edges with reduced ringing. However, there are still some ringing artifacts along diagonals, since we use a separable transform; future work will address this issue by utilizing non-separable prediction windows.	encoder;filter bank;image compression;image scaling;interpolation;kerrison predictor;lambda lifting;lifting scheme;microsoft windows;nonlinear system;polynomial;ringing (signal);ringing artifacts;semantics (computer science);wavelet transform	Roger Claypool;Geoffrey M. Davis;Wim Sweldens;Richard G. Baraniuk	1998		10.1109/DCC.1998.672259	wavelet;computer vision;mathematical optimization;discrete mathematics;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Vision	44.49801649946958	-16.13717656601603	199594
