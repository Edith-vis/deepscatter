id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
217ae3956a5daf433179da971e6eccd8786e6fb2	target tracking using sir and mcmc particle filters by multiple cameras and laser range finders	particle filters target tracking robot sensing systems cameras level set tracking robots;robot sensing systems;probabilistic manner;high resolution;range data;probability;sensor network system;distributed sensor;level set;low resolution;laser range finders;sensor network;distributed sensors;distance measurement;laser range finder;particle filter;distributed sensor target tracking sequential importance resampling particle filter markov chain monte carlo particle filter laser range finders sensor network system distributed cameras objects tracking level set method probabilistic manner;robots;object tracking;target tracking cameras distance measurement distributed sensors markov processes monte carlo methods object detection particle filtering numerical methods probability;objects tracking;markov chain monte carlo particle filter;markov processes;distributed cameras;particle filters;target tracking;multiple object tracking;processing speed;level set method;monte carlo methods;cameras;sequential importance resampling particle filter;tracking;object detection;particle filtering numerical methods;new combination	This paper presents a sensor network system consisting of distributed cameras and laser range finders for multiple objects tracking. Sensory information from cameras is processed by the level set method in real time and integrated with range data obtained by laser range finders in a probabilistic manner using novel SIR/MCMC combined particle filters. Though the conventional SIR particle filter is a popular technique for object tracking, it has been pointed out that the conventional particle filter has some disadvantages in practical applications such as its low tracking performance for multiple targets due to the degeneracy problem. In this paper, the new combined particle filters consisting of a low-resolution MCMC particle filter and a high-resolution SIR particle filter is proposed. Simultaneous tracking experiments for multiple moving targets are successfully carried out and it is verified that the combined particle filters has higher performance than the conventional particle filters in terms of the number of particles, the processing speed, and the tracking performance for multiple targets.	degeneracy (graph theory);experiment;image resolution;markov chain monte carlo;particle filter;tracking system	Ryo Kurazume;Hiroyuki Yamada;Kouji Murakami;Yumi Iwashita;Tsutomu Hasegawa	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650634	computer vision;electronic engineering;simulation;image resolution;particle filter;computer science;physics;statistics	Robotics	46.11762813068292	-46.967192804353246	152972
fd47d70877868b8e36e0f4f67bc3b674df22dd76	adaptive multi-cue based particle swarm optimization guided particle filter tracking in infrared videos	particle filter;particle swarm optimization;adaptive weight adjustment;visual tracking	This paper presents a multi-cue based particle swarm optimization (PSO) guided particle filter (PF) tracking framework. In the proposed tracking framework, PSO is incorporated into the probabilistic framework of PF as an optimization scheme for the propagation of particles, which can make particles move toward the high likelihood area to find the optimal position in the state transition stage, and Furthermore, likelihood measure functions employing multi-cue are explored to improve the robustness and accuracy of tracking. Here, each cue weight is self-adaptively adjusted by PSO algorithm throughout the tracking process. Experiments performed on several challenging public infrared video sequences demonstrate that our proposed tracking approach achieves considerable performances. & 2013 Elsevier B.V. All rights reserved.	algorithm;mathematical optimization;particle filter;particle swarm optimization;performance;phase-shift oscillator;software propagation;state transition table	Miaohui Zhang;Ming Xin;Jie Yang	2013	Neurocomputing	10.1016/j.neucom.2013.05.041	computer vision;mathematical optimization;multi-swarm optimization;simulation;particle filter;eye tracking;computer science;particle swarm optimization	Vision	45.33953834414291	-47.62244659697307	153159
ff6ec5b4e82c61232ed317e8f35d54d2dec6597e	background subtraction under sudden illumination changes	eigenvalues and eigenfunctions;histograms;rough reconstruction;statistical analysis eigenvalues and eigenfunctions image reconstruction image segmentation lighting;image segmentation;eigenbackground algorithm;statistical illumination model;adaptation model;statistical analysis;image reconstruction;pixel;background subtraction;pixel lighting image reconstruction image segmentation histograms hafnium adaptation model;online spatial likelihood model;lighting;online spatial likelihood model background subtraction sudden illumination changes eigenbackground algorithm statistical illumination model rough reconstruction foreground segmentation;foreground segmentation;sudden illumination changes;hafnium	Robust background subtraction under sudden illuminationchanges is a challenging problem. In this paper, wepropose an approach to address this issue, which combinesthe Eigenbackground algorithm together with a statisticalillumination model. The rst algorithm is used to give arough reconstruction of the input frame, while the secondone improves the foreground segmentation. We introduce anonline spatial likelihood model by detecting reliable backgroundand foreground pixels. Experimental results illustratethat our approach achieves consistently higher accuracycompared to several state-of-the-art algorithms	algorithm;background subtraction;experiment;list of common shading algorithms;online and offline;pixel;sensor	Luc P. J. Vosters;Caifeng Shan;Tommaso Gritti	2010	2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2010.72	iterative reconstruction;computer vision;background subtraction;computer science;pattern recognition;lighting;histogram;image segmentation;hafnium;pixel;statistics;computer graphics (images)	Vision	44.52052276692319	-50.26384728402702	153180
aefb61185c181edf073dd54045030c406d9205b0	tracking objects with adaptive feature patches for ptz camera visual surveillance	video surveillance object tracking algorithm adaptive feature patches ptz camera visual surveillance ptz camera based tracking patch based object models motion consistency measurements;ptz camera based tracking;video surveillance image matching image motion analysis object detection target tracking video cameras;background modeling;video surveillance;image motion analysis;surveillance;image matching;feature pursuit ptz based tracking patch based object models;adaptive feature patches;ptz camera visual surveillance;patch based object models;visual surveillance;visualization;object tracking algorithm;adaptation model;video cameras;feature pursuit;ptz based tracking;target tracking adaptation model videos cameras surveillance visualization;applications of visualization;motion consistency measurements;target tracking;cameras;object detection;videos;object model	Compared to the traditional tracking with fixed cameras, the PTZ-camera-based tracking is more challenging due to (i) lacking of reliable background modeling and subtraction; (ii) the appearance and scale of target changing suddenly and drastically. Tackling these problems, this paper proposes a novel tracking algorithm using patch-based object models and demonstrates its advantages with the PTZ-camera in the application of visual surveillance. In our method, the target model is learned and represented by a set of feature patches whose discriminative power is higher than others. The target model is matched and evaluated by both appearance and motion consistency measurements. The homography between frames is also calculated for scale adaptation. The experiment on several surveillance videos shows that our method outperforms the state-of-arts approaches.	algorithm;experiment;homography (computer vision);pan–tilt–zoom camera;patch (computing)	Yi Xie;Liang Lin;Yunde Jia	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.430	computer vision;simulation;visualization;object model;computer science;computer graphics (images)	Vision	45.04128864847943	-47.40607415218675	153511
da71ce9ccc66f50b73d34eebfd13dfbabc5f8cb6	panoramic gaussian mixture model and large-scale range background substraction method for ptz camera-based surveillance systems	foreground detection;multi-layered propagation;object tracking;panoramic gaussian mixture background;ptz camera	In this paper, we present a novel approach for constructing a large-scale range panoramic background model that provides fast registration of the observed frame and localizes the foreground targets with arbitrary camera direction and scale in a Pan–tilt–zoom (PTZ) camera-based surveillance system. Our method consists of three stages. (1) In the first stage, a panoramic Gaussian mixture model (PGMM) of the PTZ camera’s field of view is generated off-line for later use in on-line foreground detection. (2) In the second stage, a multi-layered correspondence ensemble is generated off-line from frames captured at different scales which is used by the correspondence propagation method to register observed frames online to the PGMM. (3) In the third stage, foreground is detected and the PGMM is updated. The proposed method has the capacity to deal with the PTZ camera’s ability to cover a wide field of view (FOV) and large-scale range. We demonstrate the advantages of the proposed PGMM background subtraction method by incorporating it with a tracking system for surveillance applications.	background subtraction;field of view in video games;mixture model;online and offline;pan–tilt–zoom camera;software propagation;tracking system	Kang Xue;Yue Liu;Gbolabo Ogunmakin;Jing Chen;Jiangen Zhang	2012	Machine Vision and Applications	10.1007/s00138-012-0426-4	computer vision;computer graphics (images)	Vision	44.39245052584106	-47.90779623118966	153712
0415e317b168e5951e8d1abf62bce62bc3c9dc3f	tracking many objects using subordinated condensation	scale effect;individual object;multiple objectives;multiple object tracking	We describe a novel extension to the CONDENSATION algorithm for tracking multiple objects of the same type. Previous extensions for multiple object tracking do not scale effectively to large numbers of objects. The new approach – subordinated CONDENSATION – deals effectively with arbitrary numbers of objects in an efficient manner, providing a robust means of tracking individual objects across heavily populated and cluttered scenes. The key innovation is the introduction of bindings ( ubordination) amongst particles which enables multiple occlusions to be handled in a natural way within the standard CONDENSATION framework. The effectiveness of the approach is demonstrated by tracking multiple animals of the same species in cluttered wildlife footage.	condensation algorithm;population	David Tweed;Andrew Calway	2002		10.5244/C.16.26	computer vision;simulation;mathematics	Vision	45.41458450812636	-48.54270420207223	153892
2c664bb1c694be721f5757f32b957a19a0290bca	a system identification approach for video-based face recognition	autoregressive processes;face recognition;image sequences;video signal processing;autoregressive and moving average model;dynamical system identification;gallery video sequences;linear dynamical system;video-based face recognition	The paper poses video-to-video face recognition as a dynamical system identification and classification problem. We model a moving face as a linear dynamical system whose appearance changes with pose. An autoregressive and moving average (ARMA) model is used to represent such a system. The choice of ARMA model is based on its ability to take care of the change in appearance while modeling the dynamics of pose, expression etc. Recognition is performed using the concept of sub space angles to compute distances between probe and gallery video sequences. The results obtained are very promising given the extent of pose, expression and illumination variation in the video data used for experiments.	autoregressive model;calculus of variations;care-of address;dynamical system;experiment;facial recognition system;pose (computer vision);system identification	Gaurav Aggarwal;Amit K. Roy-Chowdhury;Rama Chellappa	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333732	facial recognition system;autoregressive–moving-average model;linear dynamical system;computer vision;speech recognition;system identification;computer science;dynamical system;video tracking;pattern recognition;3d single-object recognition;moving average;statistics	Vision	43.48990568714965	-47.959061378437255	154197
d286cf5a4310cf37f7f526f3d280125698fe47b4	tracking of multiple targets using on-line learning for appearance model adaptation	model adaptation;skin color;multiple objectives;particle filter;close relationships;genetic algorithm;numerical experiment;visual tracking;on line learning	We propose visual tracking of multiple objects (faces of people) in a meeting scenario based on low-level features such as skin-color, target motion, and target size. Based on these features automatic initialization and termination of objects is performed. Furthermore, on-line learning is used to incrementally update the models of the tracked objects to reflect the appearance changes. For tracking a particle filter is incorporated to propagate sample distributions over time. We discuss the close relationship between our implemented tracker based on particle filters and genetic algorithms. Numerous experiments on meeting data demonstrate the capabilities of our tracking approach.		Franz Pernkopf	2007		10.1007/978-3-540-74260-9_54	computer vision;simulation;genetic algorithm;particle filter;eye tracking;computer science;machine learning	Vision	44.35331974642128	-48.15135977167014	154486
b75c3cf22f725377ce740bd7b5820e076837130b	video stabilization based on saliency driven sift matching and discriminative ransac	motion analysis;visual attention model;saliency map;kalman filter;discriminative sift feature;stabilized method;spatial distribution;video stabilization;human vision system;visual attention	Inspired by the stability functions of human vision system, we present a novel video stabilization method based on saliency driven SIFT matching and discriminative RANSAC. Firstly, a saliency detection method is adopted to estimate the spatial distribution of visual attention degrees in each frame of the video, and the SIFT features are extracted from the salient regions indicated by the saliency map. Then, we further achieve a modified version of RANSAC method using the discriminative features to estimate the trajectory of inter-frame motion and reduce the errors caused by the foreground vector. Finally, Kalman filter is applied to complete the motion smoothing task. Experimental results demonstrate that our approach is efficient and promising compared with state-of-the-art methods.	kalman filter;random sample consensus;scale-invariant feature transform;smoothing	Yanhao Zhang;Hongxun Yao;Pengfei Xu;Rongrong Ji;Xiaoshuai Sun;Xianming Liu	2011		10.1145/2043674.2043693	computer vision;geography;machine learning;pattern recognition	Vision	43.244618137680405	-48.608950906429236	154655
8d7df276e22ca47b08b3ba2259c51218cc6a7662	real-time monocular pose estimation of 3d objects using temporally consistent local color histograms		We present a novel approach to 6DOF pose estimation and segmentation of rigid 3D objects using a single monocular RGB camera based on temporally consistent, local color histograms. We show that this approach outperforms previous methods in cases of cluttered backgrounds, heterogenous objects, and occlusions. The proposed histograms can be used as statistical object descriptors within a template matching strategy for pose recovery after temporary tracking loss e.g. caused by massive occlusion or if the object leaves the camera’s field of view. The descriptors can be trained online within a couple of seconds moving a handheld object in front of a camera. During the training stage, our approach is already capable to recover from accidental tracking loss. We demonstrate the performance of our method in comparison to the state of the art in different challenging experiments including a popular public data set.	3d pose estimation;error-tolerant design;experiment;handheld game console;information privacy;real-time clock;real-time locating system;template matching	Henning Tjaden;Ulrich Schwanecke;Elmar Schömer	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.23	computer vision;artificial intelligence;image segmentation;template matching;pose;histogram;monocular;field of view;local color;computer science;rgb color model;pattern recognition	Vision	43.49812202523789	-47.29940808285211	155249
0b4cd2ef457ae8ab60a75ab632e08c658090b606	an implementation of an affine brisk for mobile heterogeneous parallel processors	feature extraction graphics processing units mobile communication robustness mobile handsets cameras;feature extraction;graphics processing units;mobile communication;parallel processing affine transforms computational complexity computer vision feature extraction;mobile handsets;robustness;galaxy lte a mobile mobile heterogeneous parallel processors computer vision feature extraction affine sift feature asift feature computational complexity scale change robustness rotation change robustness viewpoint change robustness affine brisk affine transformation viewpoint invariance mobile cpu mobile gpu;cameras	As the computing power of a mobile processor increases, a number of computer vision applications are widely used by a mobile device. Feature extraction is one of the crucial tasks in a computer vision area and it is used for object recognition, panorama image generation and so on. The affine SIFT (ASIFT) feature is robust to scale, rotation and viewpoint changes but its high computational complexity makes it challenging for ASIFT to be used in applications by a mobile device. To reduce the complexity of ASIFT while maintaining affine invariance, this paper proposes an affine BRISK which replaces SIFT by BRISK of which complexity is much less than SIFT. By fully exploiting the parallelism of affine transformation for viewpoint invariance, this paper achieves the speed up by parallel execution of feature extraction and affine transformation running on the mobile CPU and GPU, respectively. The proposed affine BRISK feature is robust to a viewpoint change with a comparable accuracy. The processing time is 290.64ms on a Galaxy LTE-A mobile.	central processing unit;compaq lte;computational complexity theory;computer vision;feature extraction;glossary of computer graphics;graphics processing unit;mobile device;mobile processor;outline of object recognition;parallel computing;scale-invariant feature transform	Chulhee Lee;Chae-Eun Rhee;Hyuk-Jae Lee	2016	2016 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2016.7430680	computer vision;simulation;mobile telephony;feature extraction;computer science;theoretical computer science;robustness	Vision	41.14093629405358	-51.136433933778406	155297
3697bd346b6ecbdb71d8751ea97c168244bbe9c0	video object tracking with differential structural similarity index	video object;video signal processing;video tracking;mean shift;gradient ascent;video signal processing gradient methods image sequences monte carlo methods object tracking particle filtering numerical methods;distortion measurement;decision support system;current measurement;target tracking decision support systems pixel image color analysis robustness distortion measurement current measurement;computational complexity;particle filter;image color analysis;video sequences video object tracking differential structural similarity index structural similarity measures sequential monte carlo approach particle filter computational complexity gradient ascent procedure video frames;state space;pixel;indexation;object tracking;decision support systems;gradient methods;robustness;gradient ascent tracking structural similarity;target tracking;structural similarity;monte carlo methods;tracking;particle filtering numerical methods;tk electrical engineering electronics nuclear engineering;sequential monte carlo;image sequences	The Structural SIMilarity Measure (SSIM) combined with the sequential Monte Carlo approach has been shown [1] to achieve more reliable video object tracking performance, compared with similar methods based on colour and edge histograms and Bhattacharyya distance. However, the combined use of the structural similarity and a particle filter results in increased computational complexity of the algorithm. In this paper, a novel fast approach for video tracking based on the structural similarity measure is presented. The tracking algorithm proposed determines the state of the target (location, size) based on the gradient ascent procedure applied to the structural similarity surface of the video frame, thus avoiding computationally expensive sampling of the state space. The new method, while being computationally less expensive, has shown higher accuracy compared with the standard mean shift algorithm and the SSIM Particle Filter (SSIM-PF) [1] and its performance is illustrated over real video sequences.	algorithm;analysis of algorithms;computational complexity theory;gradient descent;mean shift;monte carlo method;particle filter;sampling (signal processing);similarity measure;state space;structural similarity;times ascent;video tracking	Artur Loza;Fanglin Wang;Jie Yang;Lyudmila Mihaylova	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946676	computer vision;mathematical optimization;simulation;decision support system;particle filter;computer science;video tracking;mathematics;statistics	Vision	46.4647820596354	-48.72059225323153	155389
bd5ecb6ab60fef190184eba28f34afaa980502e8	towards a frequency domain processor for real-time sift-based filtering		The Scale Invariant Feature Transform (SIFT) extracts relevant features from images and video frames. The extracted features are robust against luminance variations, geometrical transformations, and image resolution. Due to its performances, the SIFT algorithm is of great importance in fields such as object recognition, content retrieval from image databases, robotic navigation, and gesture recognition. Main drawback of the SIFT algorithm is the high computational complexity. This paper presents the development of a hardware filtering accelerator for the implementation of SIFT-based visual search. The accelerator works in the frequency domain, operating on a block-by-block basis. This enables to work faithfully to the original Scale-Space theory, which employes non-separable Laplacian of Gaussian (LoG) filters. The targeted throughput is of (sim )20 fps, making the coprocessor suitable for real time processing.	real-time transcription	Giorgio Lopez;Ettore Napoli;Antonio Giuseppe Maria Strollo	2014		10.1007/978-3-319-20227-3_21	gesture recognition;blob detection;frequency domain;computational complexity theory;filter (signal processing);computer vision;artificial intelligence;scale-invariant feature transform;coprocessor;image resolution;computer science	Embedded	41.11302881909832	-51.43751955892842	155703
5524825b828e6a58c01ce49573aaeac2f4fbb0a8	a mixture of manhattan frames: beyond the manhattan world	plane segmentation mixture of manhattan frames model scene representation computer vision probabilistic model markov chain monte carlo sampling algorithm metropolis hastings algorithm focal length calibration depth cameras;depth camera calibration bayesian model manhattan world scene representation plane segmentation;sampling methods calibration cameras computer vision image representation image segmentation markov processes monte carlo methods probability;plane segmentation;depth camera calibration;probabilistic logic three dimensional displays computational modeling calibration cameras inference algorithms robustness;manhattan world;scene representation;bayesian model	Objects and structures within man-made environments typically exhibit a high degree of organization in the form of orthogonal and parallel planes. Traditional approaches to scene representation exploit this phenomenon via the somewhat restrictive assumption that every plane is perpendicular to one of the axes of a single coordinate system. Known as the Manhattan-World model, this assumption is widely used in computer vision and robotics. The complexity of many real-world scenes, however, necessitates a more flexible model. We propose a novel probabilistic model that describes the world as a mixture of Manhattan frames: each frame defines a different orthogonal coordinate system. This results in a more expressive model that still exploits the orthogonality constraints. We propose an adaptive Markov-Chain Monte-Carlo sampling algorithm with Metropolis-Hastings split/merge moves that utilizes the geometry of the unit sphere. We demonstrate the versatility of our Mixture-of-Manhattan-Frames model by describing complex scenes using depth images of indoor scenes as well as aerial-LiDAR measurements of an urban center. Additionally, we show that the model lends itself to focal-length calibration of depth cameras and to plane segmentation.	aerial photography;computer vision;cross-validation (statistics);focal (programming language);gibbs sampling;markov chain monte carlo;metropolis;metropolis–hastings algorithm;range imaging;robotics;sampling (signal processing);statistical model;visual odometry	Julian Straub;Guy Rosman;Oren Freifeld;John J. Leonard;John W. Fisher	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.488	computer vision;simulation;computer science;mathematics;bayesian inference;statistics;computer graphics (images)	Vision	46.00776007394701	-51.17147378984041	156228
cad580bafa144cd23f1ee489c3139ff387dbf858	probability hypothesis density approach for multi-camera multi-object tracking	data association;multiple objectives;state space;object tracking;probability hypothesis density;multiple object tracking	Object tracking with multiple cameras is more efficient than tracking with one camera. In this paper, we propose a multiple-camera multiple-object tracking system that can track 3D object locations even when objects are occluded at cameras. Our system tracks objects and fuses data from multiple cameras by using the probability hypothesis density filter. This method avoids data association between observations and states of objects, and tracks multiple objects in single-object state space. Hence, it has lower computation than methods using joint state space. Moreover, our system can track varying number of objects. The results demonstrate that our method has a high reliability when tracking 3D locations of objects.	computation;correspondence problem;hidden surface determination;particle filter;state space;tracking system	Nam Trung Pham;Weimin Huang;Sim Heng Ong	2007		10.1007/978-3-540-76386-4_83	computer vision;simulation;computer science;state space;machine learning;video tracking;mathematics	Vision	46.19018455663051	-46.79567850698784	156326
76650b12b2e8198218b5abb08f8ae477789f6f29	surveillance video summarisation by jointly applying moving object detection and tracking	key frames and trajectories;surveillance;video summarisation;object tracking;object detection	With the growth of massive storage of surveillance video data, it has become imperative to design efficient tools for video content browsing and management. This paper describes an integrative approach for surveillance video summarisation that jointly apply moving object detection and tracking. In the proposed scheme, moving objects are first detected and tracked. The static summarisation is generated to contain some key frames which provide details of the moving objects. The main advantages of our approach include the preservation of important information and economic computational cost. The high performance background modelling with Gaussian mixture model, together with the multi-scale morphological processing, brings together a highly accurate moving object detection tool. The proposed matching criterions for Kalman filtering enhances the tracking accuracy. We experimented with highway surveillance videos and outdoor surveillance videos, demonstrating satisfactory performances.	object detection	Ying Chen;Bailing Zhang	2014	IJCVR	10.1504/IJCVR.2014.062936	computer vision;simulation;computer science;video tracking;multimedia	Vision	40.914564624052225	-46.88492490222462	156338
ca97777cc2d4be87ec7e7b50dc8c2390dfcd4546	reconstruction of 3d dynamic expressions from single facial image	3d dynamic expression reconstruction 3d dynamic expression generation multiple subspace statistical learning 2d facial image automatic facial expression single facial image;subspace learning 3d dynamic expression reconstruction;image reconstruction;statistics;statistics image reconstruction learning artificial intelligence;learning artificial intelligence	Recently automatic facial expression analysis and recognition is rapidly gaining more and more interest in the field of computer vision. The capture and construction of 3D dynamic expressions often take large time and need specialized hardware, which limits its possible applications. In this paper, we try to reconstruct 3D dynamic expression images from single 2D facial image. The proposed method is based on statistical learning, where multiple subspaces are learned and support for 3D dynamic expression generation. The results show that the proposed method can effectively generate 3D dynamic expressions using only one input 2D facial image.	computer vision;machine learning	Shunya Osawa;Guifang Duan;Masataka Seo;Takanori Igarashi;Yen-Wei Chen	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738285	iterative reconstruction;computer vision;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	39.88161685475622	-49.96452741591797	156342
915796bd87405369e4b58f90293a22001a3a3751	classification of facial expression using svm for emotion care service system	video signal processing emotion recognition face recognition image sequences medical image processing patient care support vector machines;emotion care service system;facial expression recognition;image motion analysis;facial feature tracking;service system;support vector machines;video signal processing;real time;emotional state;wellbeing life care system;emotion recognition;patient care;computer vision;face recognition;personal emotion care service;personal emotion care service facial expression classification svm emotion care service system input image sequences wellbeing life care system video images database construction support vector machine;video images;medical image processing;database construction;facial features face recognition face support vector machines computer vision image motion analysis head;facial features;svm;face;optical flow;head;support vector machine;facial expression;facial feature tracking facial expression classification svm emotional state;input image sequences;facial expression classification;image sequences	This paper presents a real-time approach to classify facial expression from a sequence of input images to provide emotion care service in developing a wellbeing life care system. The facial expression recognition from video images is useful to handle with sequential changes of facial expression. However, it needs more cost in training images and constructing database rather than using a still image. In this paper, we present automatic technique which infers emotions by recognizing facial expression from input video in real time. To classify the facial expression the feature displacements traced by the optical flow are used for input parameters to a support vector machine (SVM). The classification result of facial expression from input video will be used for providing personal emotion-care service depending on the emotional state.	database;optical flow;real-time transcription;support vector machine	Byungsung Lee;Junchul Chun;Peom Park	2008	2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing	10.1109/SNPD.2008.60	facial recognition system;support vector machine;computer vision;computer science;machine learning;pattern recognition;face hallucination	Vision	39.50178847534786	-48.521573866728716	156629
80de45c40925d05ec9c498b2a7c3801f163de13b	co-occurrence-based adaptive background model for robust object detection	histograms;joints;co occurrence character co occurrence based adaptive background model robust object detection illumination invariant background model dynamic scenes sudden illumination fluctuation burst moving background dynamic background;lighting correlation histograms noise robustness joints adaptation models;robustness;lighting;correlation;adaptation models;object detection;noise	An illumination-invariant background model for detecting objects in dynamic scenes is proposed. It is robust in the cases of sudden illumination fluctuation as well as burst moving background. Unlike previous works, it distinguishes objects from a dynamic background using co-occurrence character between a target pixel and its supporting pixels in the form of multiple pixel pairs. Experiments used several challenging datasets that proved the robust performance of object detection in various environments.	global illumination;object detection;pixel;quantum fluctuation;sensor	Dong Liang;Shun'ichi Kaneko;Manabu Hashimoto;Kenji Iwata;Xinyue Zhao;Yutaka Satoh	2013	2013 10th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2013.6636673	computer vision;background subtraction;noise;lighting;histogram;correlation;robustness;computer graphics (images)	Vision	43.91001942638474	-50.11662639430154	158075
7a078987c929b1aef72f4ca283dddef187c48132	adaptive correlation filters with long-term and short-term memory for object tracking	object tracking;adaptive correlation filters;short-term memory;long-term memory;appearance model	Object tracking is challenging as target objects often undergo drastic appearance changes over time. Recently, adaptive correlation filters have been successfully applied to object tracking. However, tracking algorithms relying on highly adaptive correlation filters are prone to drift due to noisy updates. Moreover, as these algorithms do not maintain long-term memory of target appearance, they cannot recover from tracking failures caused by heavy occlusion or target disappearance in the camera view. In this paper, we propose to learn multiple adaptive correlation filters with both long-term and short-term memory of target appearance for robust object tracking. First, we learn a kernelized correlation filter with an aggressive learning rate for locating target objects precisely. We take into account the appropriate size of surrounding context and the feature representations. Second, we learn a correlation filter over a feature pyramid centered at the estimated target position for predicting scale changes. Third, we learn a complementary correlation filter with a conservative learning rate to maintain long-term memory of target appearance. We use the output responses of this long-term filter to determine if tracking failure occurs. In the case of tracking failures, we apply an incrementally learned detector to recover the target position in a sliding window fashion. Extensive experimental results on large-scale benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness.	adaptive filter;algorithm;benchmark (computing);kernel method	Chao Ma;Jia-Bin Huang;Xiaokang Yang;Ming-Hsuan Yang	2018	International Journal of Computer Vision	10.1007/s11263-018-1076-4	sliding window protocol;robustness (computer science);computer vision;artificial intelligence;computer science;short-term memory;video tracking;active appearance model;detector;long-term memory;correlation	Vision	42.42571380411031	-48.90091716017583	158097
46e0112b51ea6416dbc2f4fefe6fb434cf3c853f	detection of non-conventional events on video scenes	instance based learning;event detection layout hidden markov models feature extraction cameras histograms security object detection tracking spatial databases;image motion analysis;video signal processing;real time;security camera;scene features;real time video;feature vector;caviar database;feature extraction;object tracking;parking lot database;instance based learning paradigm;object movement;video scenes;nonconventional events detection;caviar database nonconventional events detection video scenes real time video security camera object tracking scene features feature vector instance based learning paradigm object movement parking lot database;object detection;video signal processing feature extraction image motion analysis image sequences object detection;image sequences	This article presents a novel approach for detection of non-conventional events in videos scenes. This novel approach consists in analyzing in real-time video from a security camera to detect, segment and tracking objects in movement to further classify its movement as conventional or non-conventional. From each tracked object in the scene features such as position, speed, changes in directions and in the bounding box sizes are extracted. These features make up a feature vector. At the classification step, feature vectors generated from objects in movement in the scene are matched almost in real-time against reference feature vectors previously labeled which are stored in a database and an algorithm based on the instance-based learning paradigm is used to classify the object movement as conventional or non-conventional. Experimental results on video clips from two databases (Parking Lot and CAVIAR) have shown that the proposed approach is able to detect non-conventional events with accuracies between 77% and 82%.	algorithm;closed-circuit television;database;feature vector;instance-based learning;minimum bounding box;programming paradigm;real life;real-time clock;real-time locating system;video clip	Andre G. Hochuli;Alceu de Souza Britto;Alessandro L. Koerich	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414089	computer vision;instance-based learning;feature vector;feature extraction;computer science;machine learning;video tracking;pattern recognition;computer graphics (images)	Vision	39.857381284266616	-47.335648815441026	158464
c65f39247c6c055097f769b79792287637e5b29d	pedestrian lane detection in unstructured scenes for assistive navigation	benchmark dataset;assistive and autonomous navigation;pedestrian lane detection;vanishing point estimation	"""Automatic detection of the pedestrian lane in a scene is an important task in assistive and autonomous navigation. This paper presents a vision-based algorithm for pedestrian lane detection in unstructured scenes, where lanes vary significantly in color, texture, and shape and are not indicated by any painted markers. In the proposed method, a lane appearance model is constructed adaptively from a sample image region, which is identified automatically from the image vanishing point. This paper also introduces a fast and robust vanishing point estimation method based on the color tensor and dominant orientations of color edge pixels. The proposed pedestrian lane detection method is evaluated on a new benchmark dataset that contains images from various indoor and outdoor scenes with different types of unmarked lanes. Experimental results are presented which demonstrate its efficiency and robustness in comparison with several existing methods. Disciplines Engineering | Science and Technology Studies Publication Details S. Phung, M. Cuong. Le & A. Bouzerdoum, """"Pedestrian lane detection in unstructured scenes for assistive navigation,"""" Computer Vision and Image Understanding, vol. 149, pp. 186-196, 2016. This journal article is available at Research Online: http://ro.uow.edu.au/eispapers/6206 Computer Vision and Image Understanding 149 (2016) 186–196"""	algorithm;assistive technology;autonomous robot;benchmark (computing);computer vision;pixel;robustness (computer science);vanishing point	Son Lam Phung;Manh Cuong Le;Abdesselam Bouzerdoum	2016	Computer Vision and Image Understanding	10.1016/j.cviu.2016.01.011	computer vision;simulation;computer graphics (images)	Vision	43.01477430194995	-46.294921271722835	158556
fb2ecaf1c257fe5bfe9560ad47d05bcd04fceacc	video segmentation framework by dynamic background modelling		Detecting moving objects in video streams is the first relevant step of information extraction in many computer vision applications, e.g. video surveillance systems. In this work, a video segmentation framework by dynamic background modelling is presented. Our approach aims to update suitably the background model of a scene that is recorded by a static camera. For such purpose, we develop an optical flow based methodology to suitable track moving objects, which can stop or change smoothly their movement along the video. Moreover, a light variations identification stage, is employed to avoid possible confusions between illumination changes and objects in movement. Regarding this, our approach is able to ensure a suitable background modelling in real world scenarios. Attained results show that our framework outperforms, in well-known datasets, state of the art methodologies.		Santiago Molina-Giraldo;Andrés Marino Álvarez-Meza;Julio C. García-Álvarez;Germán Castellanos-Domínguez	2013		10.1007/978-3-642-41181-6_85	computer vision;segmentation-based object categorization;scale-space segmentation	Vision	42.065405659797584	-46.77345951136115	159246
a1d9f695e14418af80dd0ff2b4e77de3a11c43cf	shot break detection and camera motion classification in digital video		In this paper we describe methods using 2D-pixel motion elds for detecting shot breaks and classifying camera motion in digital video. These ow elds can be calculated with optical ow or correlation matching. The Karhunen-Loe ve transform (KLT), based on statistical properties, insures extraction of optimal linear features from ow elds representing a particular motion class. Two methods for classiication are discussed, and experimental results for each method are presented.	digital video;experiment;moving picture experts group;pixel;sensor;usability	Micha Haas;Michael S. Lew;Dionysius P. Huijsmans	1998		10.1142/9789812797988_0016	camera resectioning;computer science;camera auto-calibration;artificial intelligence;computer vision	Vision	39.47450926314097	-50.82149178866799	159462
abd1ad9cfba8771b552921b935d309087abca817	a background subtraction algorithm based on pixel state	background modeling;computer vision;background subtraction;motion detection	Most of current background subtraction algorithms have issues of ghost and foreground aperture when they process the crowded video sequences in the outdoor scenes. In this paper we present a novel method based on the pixel state to solve the issues. Every pixel in a video steam is assumed to own two different states --- active or inactive. Via the pixel state, we divide the whole observing time into many short units. Meanwhile, a new concept, confidence, is proposed to measure the significance of each cluster. By observing small units of time, our method automatically selects the clusters with the highest confidence as the background model. The experimental results show our method not only provides the accurate motion detection of crowded video sequences, but also handles the light change and performs in real time.	algorithm;background subtraction;motion detector;pixel;steam	Ruoxi Deng;Dangfu Yang;Xinru Liu;Shengjun Liu	2014		10.1145/2670473.2670503	computer vision;simulation;background subtraction;computer science;computer graphics (images)	Vision	43.28741802335927	-45.946662689619615	159696
b5e1bca451227bfe8406728b8ca9b04d707ef152	a new bayesian edge-linking algorithm using single-target tracking techniques	single target tracking;edge linking;boundary detection	This paper proposes novel edge-linking algorithms capable of producing a set of edge segments from a binary edge map generated by a conventional edge-detection algorithm. These proposed algorithms transform the conventional edge-linking problem into a single-target tracking problem, which is a well-known problem in object tracking. The conversion of the problem enables us to apply sophisticated Bayesian inference to connect the edge points. We test our proposed approaches on real images that are corrupted with noise.	algorithm;edge detection;gibbs sampling;markov chain monte carlo;sampling (signal processing);variational principle	Ji Won Yoon	2016	Symmetry	10.3390/sym8120143	computer vision;mathematical optimization;machine learning;mathematics	Vision	45.87711695187429	-49.93225337136145	160053
cab64d8b9a2206ae42c3d5ef09b6ebda65dfd1eb	robust head pose estimation by machine learning	subset classification machine learning robust head pose estimation support vector machines head orientation angle estimation video environment real time multi talker tracking system facial criterion video images lighting conditions zooming conditions video conferencing surveillance scenarios;teleconferencing;tracking system;video signal processing;surveillance;real time;learning artificial intelligence parameter estimation learning automata teleconferencing video signal processing surveillance feature extraction image classification;image classification;learning automata;head pose estimation;machine learning;video conferencing;feature extraction;support vector machine;parameter estimation;robustness magnetic heads machine learning support vector machines videoconference cameras face detection facial features eyes mouth;learning artificial intelligence	Support vector machines are applied for estimating the head orientation angle of talkers in a video environment. The procedure is capable of accurately evaluating head orientations over a complete 360 degree interval and has been designed to function as part of an existing real-time, multi-talker tracking system. By relying on a facial criterion that is easily extracted from video images acquired across a range of lighting and zooming conditions, the estimator is designed to be effective in practical situations such as those encountered in video conferencing or surveillance scenarios.	3d pose estimation;machine learning	Ce Wang;Michael S. Brandstein	2000		10.1109/ICIP.2000.899332	support vector machine;computer vision;contextual image classification;teleconference;tracking system;feature extraction;computer science;machine learning;video tracking;pattern recognition;videoconferencing;estimation theory	Robotics	43.792185309556004	-47.725230526922445	160064
94eb1a33bbe1a1a04167e9cab2a3329037c69133	robust arm and hand tracking by unsupervised context learning	hand;unsupervised learning;imaging three dimensional;object;photography;features;recognition;image interpretation computer assisted;gestures;particle filter;random forest;context learning;video recording;hand tracking;artificial intelligence;algorithms;arm;pattern recognition automated;humans;importance sampling	Hand tracking in video is an increasingly popular research field due to the rise of novel human-computer interaction methods. However, robust and real-time hand tracking in unconstrained environments remains a challenging task due to the high number of degrees of freedom and the non-rigid character of the human hand. In this paper, we propose an unsupervised method to automatically learn the context in which a hand is embedded. This context includes the arm and any other object that coherently moves along with the hand. We introduce two novel methods to incorporate this context information into a probabilistic tracking framework, and introduce a simple yet effective solution to estimate the position of the arm. Finally, we show that our method greatly increases robustness against occlusion and cluttered background, without degrading tracking performance if no contextual information is available. The proposed real-time algorithm is shown to outperform the current state-of-the-art by evaluating it on three publicly available video datasets. Furthermore, a novel dataset is created and made publicly available for the research community.	alessandro vespignani;algorithm;conflict (psychology);embedded system;embedding;frame (physical object);human–computer interaction;muscle rigidity;necrotizing ulcerative gingivitis;obstruction;particle filter;physical object;real-time clock;real-time locating system;rough set;silo (dataset);technical support;unsupervised learning;wilfried brauer;biologic segmentation	Vincent Spruyt;Alessandro Ledda;Wilfried Philips	2014		10.3390/s140712023	unsupervised learning;random forest;computer vision;simulation;particle filter;importance sampling;computer science;photography;object;machine learning;video tracking;gesture;arm architecture	Vision	43.851116836245644	-46.82125910820061	160594
8046cfa1c834f966cfc88aa3dca56fb828f241d0	fast visual tracking with robustifying kernelized correlation filters		Robust visual tracking is a challenging work because the target object suffers appearance variations over time. Tracking algorithms based on correlation filter have presently attracted much attention because of their high efficiency and computation speed. However, these algorithms can easily drift for the noisy updates. Moreover, they are out of action and cannot re-track when trackers failure caused by heavy occlusion or target being out of view. In this paper, we propose a robust correlation filter that is constructed by considering all the extracted target appearances from the initial image to the current image. The numerator and denominator of the filter model are updated separately instead of linearly interpolated only by storing the current model. Strategies, such as reducing feature dimensionality and interpolating correlation scores, are investigated to reduce computational cost for fast tracking. Occlusion and fast motion problems can be effectively solved by the expansion of the search area. In addition, model updates occur under the condition of a confidence metric (i.e., peak-to-sidelobe ratio) threshold. Comprehensive experiments were conducted on object tracking data sets and the results showed that our method performs well compared to the other competitive methods. Moreover, it runs on a single central processing unit at a speed of 69.5 frames per second, which is suitable for real-time application.	algorithm;central processing unit;computation;computational complexity theory;experiment;kernel method;linear interpolation;real-time computing;real-time transcription;video tracking	Qianbo Liu;Guoqing Hu;Md Mojahidul Islam	2018	IEEE Access	10.1109/ACCESS.2018.2861827	interpolation;robustness (computer science);kernel (linear algebra);frame rate;video tracking;distributed computing;computer science;curse of dimensionality;eye tracking;pattern recognition;artificial intelligence;data set	Vision	42.24539452727203	-49.02304356140593	160890
038a087a64a90ec21ff427b1c7f1983a330f7350	unsupervised time-series clustering of distorted and asynchronous temporal patterns	time series alignment;unsupervised clustering;time series image sequences pattern clustering;spatiotemporal mean shift;time series alignment unsupervised clustering dynamic time warping spatiotemporal mean shift;dynamic time warping;multispectral satellite image sequences unsupervised time series clustering methods asynchronous temporal patterns k means k medoids learning step unsupervised clustering technique spatiotemporal mean shift optimal time series warping dynamic time warping spatiotemporal filtering technique synchronized temporal patterns clustering algorithm trajectory constraint dtw associations synthetic data brain magnetic resonance;lesions spatiotemporal phenomena measurement image color analysis image sequences clustering algorithms trajectory	Most time-series clustering methods, such as k-means or k-medoids, are initialized by prior knowledge about the number of classes or by a learning step. We propose an unsupervised clustering technique based on spatiotemporal mean-shift and optimal time series warping using dynamic time warping (DTW). Our main contribution consists in combining a spatiotemporal filtering technique, which gathers similar and synchronized temporal patterns in image sequences, with a clustering algorithm that applies a trajectory constraint on the DTW associations, thereby discriminating between similar time-series that are temporally shifted or warped. We assess the method's robustness on synthetic data, and demonstrate its versatility on brain magnetic resonance and multispectral satellite image sequences.	algorithm;cluster analysis;dynamic time warping;k-means clustering;k-medoids;mean shift;medoid;multispectral image;resonance;synthetic data;time series;unsupervised learning	Simon Mure;Thomas Grenier;Charles R. G. Guttmann;Hugues Benoit-Cattin	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471879	correlation clustering;computer vision;computer science;canopy clustering algorithm;machine learning;dynamic time warping;pattern recognition;cure data clustering algorithm;cluster analysis	DB	40.40622340678628	-48.624109715371354	161240
5c0692163d771a4ef80ad101d2abb72993c2510c	detecting objects of variable shape structure with hidden state shape models	hand;analisis imagen;modelizacion;dynamic programming;image features;sensitivity and specificity;taper;metodo polinomial;chamfer distance matching;modelo markov oculto;clutter;programacion dinamica;image motion analysis;data interpretation statistical;estimation mouvement;modelo markov;dynamic programming object detection shape modeling probabilistic algorithms;modele markov cache;localizacion objeto;hidden markov model;image matching;analisis forma;estimacion movimiento;object location;algorithms artificial intelligence biometry computer simulation data interpretation statistical hand humans image enhancement image interpretation computer assisted information storage and retrieval markov chains models statistical pattern recognition automated reproducibility of results sensitivity and specificity;avellanado;biological system modeling;temporal constraint;probabilistic algorithm;motion estimation;inference mechanisms;dynamic program;intelligence artificielle;probabilistic approach;high precision;polynomials;probabilistic algorithms;motion tracking;computer vision;polynomial inference algorithm;modelisation;detection objet;detector proximidad;nonrigid hand motion tracking;image enhancement;temporal constraints;fouillis echo;markov model;hidden markov models;shape;hidden state shape models;image interpretation computer assisted;object detection hidden markov models image matching image motion analysis inference mechanisms;object oriented;polynomial method;heuristic algorithms;enfoque probabilista;approche probabiliste;precision elevee;confusion eco;shape object detection hidden markov models polynomials biological system modeling inference algorithms tracking heuristic algorithms dynamic programming computer vision;inferencia;reproducibility of results;programmation dynamique;precision elevada;constrenimiento temporal;models statistical;oriente objet;artificial intelligence;algorithms	"""This paper proposes a method for detecting object classes that exhibit variable shape structure in heavily cluttered images. The term """"variable shape structure"""" is used to characterize object classes in which some shape parts can be repeated an arbitrary number of times, some parts can be optional, and some parts can have several alternative appearances. Hidden state shape models (HSSMs), a generalization of hidden Markov models (HMMs), are introduced to model object classes of variable shape structure using a probabilistic framework. A polynomial inference algorithm automatically determines object location, orientation, scale, and structure by finding the globally optimal registration of model states with the image features, even in the presence of clutter. Experiments with real images demonstrate that the proposed method can localize objects of variable shape structure with high accuracy. For the task of hand shape localization and structure identification, the proposed method is significantly more accurate than previously proposed methods based on chamfer-distance matching. Furthermore, by integrating simple temporal constraints, the proposed method gains speed-ups of more than an order of magnitude and produces highly accurate results in experiments on nonrigid hand motion tracking."""	algorithm;chamfer;class;clutter;domain-driven design;experiment;generalization (psychology);greater than;hidden markov model;inference;matching;malignant fibrous histiocytoma;markov chain;maxima and minima;physical object;polynomial;sensor;shape context	Jingbin Wang;Vassilis Athitsos;Stan Sclaroff;Margrit Betke	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.1178	active shape model;computer vision;markov chain;probabilistic analysis of algorithms;systems modeling;shape;computer science;machine learning;dynamic programming;pattern recognition;motion estimation;shape analysis;mathematics;clutter;tracking;markov model;randomized algorithm;object-oriented programming;feature;hidden markov model;statistics;polynomial	Vision	45.83910233006761	-50.57844298470854	161483
4dd1bbf68cd72c381a88492b14e922418ecacfcb	multi-illuminant estimation with conditional random fields	random processes image colour analysis;multi illuminant color constancy crf;image colour analysis;image color analysis estimation lighting labeling robustness minimization materials;random processes;pixel wise ground truth illuminant information multiilluminant estimation color constancy algorithms uniform illumination real world scenes colors estimation spatial distribution energy minimization task conditional random field local illuminant estimates two dominant illuminant images indoor scenes outdoor scenes	Most existing color constancy algorithms assume uniform illumination. However, in real-world scenes, this is not often the case. Thus, we propose a novel framework for estimating the colors of multiple illuminants and their spatial distribution in the scene. We formulate this problem as an energy minimization task within a conditional random field over a set of local illuminant estimates. In order to quantitatively evaluate the proposed method, we created a novel data set of two-dominant-illuminant images comprised of laboratory, indoor, and outdoor scenes. Unlike prior work, our database includes accurate pixel-wise ground truth illuminant information. The performance of our method is evaluated on multiple data sets. Experimental results show that our framework clearly outperforms single illuminant estimators as well as a recently proposed multi-illuminant estimation approach.	algorithm;color;conditional random field;energy minimization;estimated;ground truth;numerous;pixel	Shida Beigpour;Christian Riess;Joost van de Weijer;Elli Angelopoulou	2014	IEEE Transactions on Image Processing	10.1109/TIP.2013.2286327	stochastic process;computer vision;mathematics;statistics;computer graphics (images)	Vision	45.72202421841423	-51.02733515588495	161542
3db05f7c449945bc540dbf012d98ae5615cf2736	combining color-based invariant gradient detector with hog descriptors for robust image detection in scenes under cast shadows	detectors;histograms;urban scene classification;scene classification;detectors robustness layout object detection lighting histograms robots spatial resolution robust control technological innovation;robust image detection;comunicacion de congreso;image classification;outdoor scenes;intensity gradient detectors;hog descriptors;computer vision;local features;image color analysis;intensity gradient detectors color based invariant gradient detector hog descriptors robust image detection cast shadows outdoor scenes urban scene classification person detection;image colour analysis;feature extraction;person detection;robots;detection rate;image colour analysis image classification;robustness;lighting;color based invariant gradient detector;cast shadows;invariant feature	In this work we present a robust detection method in outdoor scenes under cast shadows using color based invariant gradients in combination with HoG local features. The method achieves good detection rates in urban scene classification and person detection outperforming traditional methods based on intensity gradient detectors which are sensible to illumination variations but not to cast shadows. The method uses color based invariant gradients that emphasize material changes and extract relevant and invariant features for detection while neglecting shadow contours. This method allows to train and detect objects and scenes independently of scene illumination, cast and self shadows. Moreover, it allows to do training in one shot, that is, when the robot visits the scene for the first time.	color;global illumination;gradient;scene graph;self-shadowing;sensor	Michael Villamizar;Jorge Scandaliaris;Alberto Sanfeliu;Juan Andrade-Cetto	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152429	robot;computer vision;detector;contextual image classification;feature extraction;computer science;artificial intelligence;pattern recognition;lighting;histogram;robustness;computer graphics (images)	Robotics	42.14699234154172	-45.95468102611897	162252
c1bf72ff7d805a0cb8940b91d1b4ba9e4bcb8e76	visual object tracking via enhanced structural correlation filter		In this study, we aim to build a robust correlation-based visual object tracking system. The function of traditional correlation filters for visual tracking is to search the most likely position of target by circularly shifting the search image patch. However, the search image patch needs to be large enough to cover both the object and background, which results in the algorithm being sensitive to changes in background. To alleviate this problem, we first propose an efficient object-surrounding histogram model to suppress the background. In this model, we build a Bayes classifier based on the initial given object, and we then apply it to each pixel in subsequent frames. With this model, the original image can be enhanced in order to eliminate the impact of circular shifting. Moreover, we develop a structural correlation filter that consists of both holistic and local object parts. The multiple object parts are adaptively weighted and further aggregated to predict the relative motion from the last frame. We conduct extensive experiments on frequently used benchmarks with 51 video sequences. The experimental results show that the proposed algorithm achieves outstanding performance, especially in terms of heavy occlusion and severe deformation.	algorithm;benchmark (computing);circular shift;experiment;hidden surface determination;holism;image scaling;pixel;structural cut-off;tracking system;video tracking	Kai Chen;Wenbing Tao;Shoudong Han	2017	Inf. Sci.	10.1016/j.ins.2017.02.012	computer vision;simulation;machine learning;mathematics	Vision	42.84175060718531	-49.937348195887076	162839
f3d286bc78713ceb48c4f959cefc6f9bb7fd55bb	a spatiotemporal saliency framework	motion analysis;image motion analysis;motion prediction bioinspired spatiotemporal saliency framework spatial feature detection feature tracking;saliency map;video signal processing;biological system modeling;feature tracking;indexing terms;feature extraction;spatiotemporal phenomena;spatiotemporal phenomena biological system modeling humans computer vision predictive models motion detection motion measurement layout image segmentation prototypes;multidimensional signal detection;real time implementation;tracking feature extraction image motion analysis spatiotemporal phenomena;tracking;multidimensional signal detection video signal processing biological system modeling motion analysis	This paper presents a novel bio-inspired spatiotemporal saliency framework. The framework incorporates spatial feature detection, feature tracking and motion prediction in order to generate a spatiotemporal saliency map. Experimental results demonstrate its ability and robustness to produce saliency responses to motion pop-up phenomena that are in line with humans responses. Moreover, the limited storage requirements permit real-time implementations of the proposed framework.	algorithmic efficiency;british informatics olympiad;feature detection (computer vision);feature detection (web development);high- and low-level;kalman filter;kerrison predictor;markov chain monte carlo;monte carlo method;motion estimation;object detection;particle filter;real-time clock;requirement;resampling (statistics);spatiotemporal database;spatiotemporal pattern	Yang Liu;Christos-Savvas Bouganis;Peter Y. K. Cheung	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312447	computer vision;index term;feature extraction;computer science;machine learning;pattern recognition;tracking	Robotics	46.00341105209227	-48.78858286150748	162967
e3de697339e81358f5eb02693ff3d33b671288ff	registering multiple cartographic models with the hierarchical mixture of experts algorithm	experts algorithm;image recognition;initialisation multiple cartographic models registration hierarchical mixture experts algorithm noisy radar data hierarchical mixture of experts algorithm maximum likelihood registration parameters matching probabilities hypothesis verification model retrieval database image noise;maximum likelihood;application software;matching probabilities;information retrieval;database;noisy radar data;hierarchical mixture of experts;testing;model retrieval;noise robustness;image noise;hierarchical mixture of experts algorithm;computer vision;navigation;maximum likelihood registration parameters;machine vision;radar imaging;multiple cartographic models registration;radar applications;radar imaging navigation computer science application software radar applications information retrieval noise robustness image recognition machine vision testing;cartography;computer vision cartography;hypothesis verification;computer science;hierarchical mixture;initialisation	This paper describes an application of the hierarchical mixture of experts algorithm (HME) to the registration of multiple cartographic models to noisy radar data. According to the HME algorithm each model is represented by a set of maximum likelihood registration parameters together with a set of matching probabilities. This architecture can be viewed as providing simultaneous registration and hypothesis verification. The maps in the cartographic data-base compete to account for radar data through the imposed probability normalisation. The resulting matching algorithm can be regarded as a generic tool for model retrieval from a database. Our evaluation on radar images illustrates some of the characteristics of the algorithm. Our main conclusions are that the method is both robust to added image noise and poor initialisation.	algorithm	Simon Moss;Edwin R. Hancock	1997		10.1109/CVPR.1997.609436	image noise;computer vision;navigation;application software;machine vision;computer science;machine learning;pattern recognition;software testing;maximum likelihood;radar imaging	Vision	45.60761343922756	-51.506132902995944	163003
77d1ce4c308596d60fd0765f8c9cb7a265e3c5f2	early facial expression recognition with high-frame rate 3d sensing	adaboost algorithm facial expression recognition rate high frame rate 3d sensing wavelet spectral subtraction method facial expression feature;image sensors;wavelet transforms face recognition image sensors;wavelet transforms;face recognition;trajectory;three dimensional displays;early facial expression recognition wavelet spectral subtraction early adaboost;wavelet spectral subtraction;noise wavelet packets face recognition three dimensional displays face trajectory;face;wavelet packets;early facial expression recognition;early adaboost;noise	This work investigates a new challenging problem: how to exactly recognize facial expression as early as possible, while most works generally focus on improving the recognition rate of facial expression recognition. The features of facial expressions in their early stage are unfortunately very sensitive to noise due to their low intensity. So, we propose a novel wavelet spectral subtraction method to spatio-temporally refine the subtle facial expression features. Moreover, in order to achieve early facial expression recognition, we newly introduce an early AdaBoost algorithm for facial expression recognition problem. Experiments using our database established by using a high-frame rate 3D sensing showed that the proposed method has a promising performance on early facial expression recognition.	adaboost;algorithm;spectral density;wavelet	Lumei Su;Shiro Kumano;Kazuhiro Otsuka;Dan Mikami;Junji Yamato;Yoichi Sato	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6084179	facial recognition system;face;computer vision;speech recognition;noise;trajectory;pattern recognition;image sensor;three-dimensional face recognition;mathematics;wavelet packet decomposition;face hallucination;wavelet transform	Vision	40.838045298400715	-50.598887219165704	163077
1ab528064b3949371ade5196beb11db4d8e5ab35	automatic lip tracking: bayesian segmentation and active contours in a cooperative scheme	bayesian methods active contours mouth face detection image segmentation data mining video sequences spatiotemporal phenomena automatic speech recognition laboratories;image motion analysis;active contour;image segmentation;bayesian approach;color space;bayes methods;speechreading;markov random fields;speech recognition automatic lip tracking bayesian segmentation active contours cooperative scheme lip contour extraction color video sequence lighting conditions logarithmic color transform color space bayesian approach markov random field modelling motion analysis spatiotemporal neighbourhood region of interest boundary point extraction spatially varying coefficients;active contours;lip tracking;markov random field;bayesian segmentation;image colour analysis;feature extraction;region of interest;markov processes feature extraction image motion analysis image segmentation speech recognition image sequences image colour analysis bayes methods;speech recognition;markov processes;image sequences	An algorithm for speaker’s lip contour extraction is presented in this paper. A color video sequence of speaker’s face is acquired, under natural lighting conditions and without any particular make-up. First, a logarithmic color transform is performed from RGB to HI (hue, intensity) color space. A bayesian approach segments the mouth area using Markov random field modelling. Motion is combined with red hue lip information into a spatiotemporal neighbourhood. Simultaneously, a Region Of Interest and relevant boundaries points are automatically extracted. Next, an active contour using spatially varying coefficients is initialised with the results of the preprocessing st age. Finally, an accurate lip shape with inner and outer borders is obtained with good quality results in this challenging situation.	active contour model;algorithm;coefficient;color management;color space;markov chain;markov random field;neighbourhood (graph theory);preprocessor;region of interest	Marc Liévin;Patrice Delmas;Pierre-Yves Coulon;Franck Luthon;Vincent Fristot	1999		10.1109/MMCS.1999.779283	computer vision;speech recognition;feature extraction;bayesian probability;computer science;pattern recognition;active contour model;image segmentation;markov process;color space;region of interest	Vision	44.0802876939546	-50.684671236023085	163442
6ff06f8f6331f45aa38799f281ae8cd69e497716	effective occlusion handling for fast correlation filter-based trackers		Correlation filter-based trackers heavily suffer from the problem of multiple peaks in their response maps incurred by occlusions. Moreover, the whole tracking pipeline may break down due to the uncertainties brought by shifting among peaks, which will further lead to the degraded correlation filter model. To alleviate the drift problem caused by occlusions, we propose a novel scheme to choose the specific filter model according to different scenarios. Specifically, an effective measurement function is designed to evaluate the quality of filter response. A sophisticated strategy is employed to judge whether occlusions occur, and then decide how to update the filter models. In addition, we take advantage of both log-polar method and pyramid-like approach to estimate the best scale of the target. We evaluate our proposed approach on VOT2018 challenge and OTB100 dataset, whose experimental result shows that the proposed tracker achieves the promising performance compared against the state-of-the-art trackers.	benchmark (computing);map;real-time clock	Zheng Zhang;Yang Li;Jinwei Ren;Jianke Zhu	2018	CoRR		pattern recognition;artificial intelligence;computer science;bittorrent tracker;correlation	Vision	42.522657387313544	-48.94950788451736	163486
902efe33f9ffe32f9e0ae1f2a2dc6af8cfdf09a8	target tracking based on optimized particle filter algorithm	mcmc;multi-target tracking;particle filter;sequential important sampling	Particle filter is a probability estimation method based on Bayesian framework and it has unique advantage to describe the target tracking non-linear and non-Gaussian. In this paper, Firstly, analyses the particle degeneracy and sample impoverishment in particle filter multi-target tracking algorithm, and secondly, it applies Markov Chain Monte Carlo (MCMC) method to improve re-sampling process and enhance performance of particle filter algorithm. Finally, the performance of the proposed method is certificated by experiment that tracking multiple targets of similar appearance and complex motion. The results show the efficacy of the proposed method in multi-target tracking.	particle filter;peterson's algorithm	Junying Meng;Jiaomin Liu;Juan Wang;Ming Han	2013	JSW		monte carlo localization;mathematical optimization;ensemble kalman filter;particle filter;markov chain monte carlo;auxiliary particle filter;computer science;statistics	Vision	45.68323449198905	-47.87394091363454	164074
5cdfc95cac4fd41aacd8928f11418d7c75fafd0b	hand gesture recognition based on free-form contours and probabilistic inference	image sequence analysis;wnioskowanie stochastyczne;active contours;hand pose detection;analiza sekwencji obrazu;hand tracking;śledzenie dloni;stochastic inference	A computer vision system is described that captures color image sequences, detects and recognizes static hand poses (i.e., “letters”) and interprets pose sequences in terms of gestures (i.e., “words”). The hand object is detected with a double-active contour-based method. A tracking of the hand pose in a short sequence allows detecting “modified poses”, like diacritic letters in national alphabets. The static hand pose set corresponds to hand signs of a thumb alphabet. Finally, by tracking hand poses in a longer image sequence, the pose sequence is interpreted in terms of gestures. Dynamic Bayesian models and their inference methods (particle filter and Viterbi search) are applied at this stage, allowing a bi-driven control of the entire system.	active contour model;application domain;color image;computation;computer vision;gesture recognition;particle filter;real-time computing;real-time transcription;sensor;viterbi algorithm	Wlodzimierz Kasprzak;Artur Wilkowski;Karol Czapnik	2012	Applied Mathematics and Computer Science	10.2478/v10006-012-0033-6	computer vision;speech recognition;computer science;pattern recognition	Vision	39.59986622468224	-48.463483869498525	164077
535255da8b5559f10dcd137301d349499d85ed28	robust scale-adaptive mean-shift for tracking	object tracking;mean shift	Mean-Shift tracking is a popular algorithm for object tracking since it is easy to implement and it is fast and robust. In this paper, we address the problem of scale adaptation of the Hellinger distance based Mean-Shift tracker. We start from a theoretical derivation of scale estimation in the Mean-Shift framework. To make the scale estimation robust and suitable for tracking, we introduce regularization terms that counter two major problem: (i) scale expansion caused by background clutter and (ii) scale implosion on self-similar objects. To further robustify the scale estimate, it is validated by a forward-backward consistency check. The proposed Mean-shift tracker with scale selection is compared with recent state-of-the-art algorithms on a dataset of 48 public color sequences and it achieved excellent results.	algorithm;clutter;matrix regularization;mean shift;robustification;robustness (computer science);self-similarity	Tomás Vojír;Jana Noskova;Jiri Matas	2013	Pattern Recognition Letters	10.1007/978-3-642-38886-6_61	tracking system	Vision	44.51533967918067	-48.42330802710817	164649
0fd700d8db6b98a7ea123d799ef552a90d3861b7	new feature descriptor: extended symmetrical-diagonal hexadecimal pattern for efficient background subtraction and object tracking		Now-a-days the biggest challenge of computer vision technology is perfect detection of an object and tracking in an outdoor environment. Many approaches have been proposed to address some of the challenges, but still some problems persist, for instance high frequency motion of dynamic texture, motion of camera, abrupt changes in illumination, etc. To cope with these challenging scenarios effectively, a novel Extended SymmetricalDiagonal Hexadecimal Pattern (ES-DHP) is proposed for both background subtraction and object tracking. Each object is represented by its corresponding ES-DHP. Metrics like Fscore, FNR, FPR, MMR and elapsed time are used to measure the performance of the proposed system. Extensive experimental evaluations on a wide range of benchmark data sets validate the efficiency of ES-DHP when compared with other existing methods for unconstrained video analytics. From the outcomes, it is viewed that the proposed framework provides great outcomes for background subtraction and tracking. © 2017 Elsevier Ltd. All rights reserved.	background subtraction;benchmark (computing);cma-es;computer vision;film-type patterned retarder;hexadecimal;multi-master replication;opengl es;video content analysis;visual descriptor	D. Jeyabharathi;Dejey	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2017.11.001	video tracking;computer vision;computer science;data set;diagonal;hexadecimal;artificial intelligence;background subtraction;analytics	Vision	41.92632129939153	-48.41999067354139	164840
063a1d17e5b0931c0889b41aae4189d2e695dd2c	robust lane marking detection using boundary-based inverse perspective mapping	roads feature extraction birds robustness calibration vehicles safety;road detection;inverse perspective mapping;roads calibration driver information systems object detection;visual perceptive ability illumination robust lane marking detection approach road detection boundary based ipm bird road boundaries automatic ipm method ipm parameters inverse perspective mapping parameters one time calibration lane markings driver assistance systems;inverse perspective mapping road detection lane detection;lane detection	Road detection, which brings a visual perceptive ability to vehicles, is essential to build driver assistance systems. To help detect lane markings in challenging scenarios, one-time calibration of inverse perspective mapping (IPM) parameters is employed to build a bird's eye view of the road image. We propose an automatic IPM method based on road boundaries called BIRD (Boundary-based IPM for Road Detection), avoiding common problems of fixed IPM. Furthermore, integrating top-down and bottom-up attention, an illumination-robust lane marking detection approach using BIRD is proposed.	bird's-eye view;bottom-up proteomics;item unique identification;top-down and bottom-up design	Zhenqiang Ying;Ge Li	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472011	computer vision;simulation	Robotics	42.30405565886993	-45.18067585448333	165063
e9db99c34c5cedfaaca0680a5ddb4e16af0d349f	a reliability assessment paradigm for automated video tracking systems	reliability;uncertainty problem reliability assessment paradigm automated video tracking systems performance evaluation methods video sequences statistical index;performance evaluation;video tracking;video sequences;uncertainty handling;performance evaluation reliability theory lighting robustness mathematical model;reliability theory;uncertainty problem;reliability assessment;statistical analysis;indexation;automated video tracking systems;mathematical model;performance evaluation methods;statistical index;uncertainty handling image sequences performance evaluation reliability statistical analysis;robustness;lighting;image sequences;reliability assessment paradigm	Most existing performance evaluation methods concentrate on defining separate metrics over a wide range of conditions and generating standard benchmarking video sequences for examining the effectiveness of video tracking systems. In other words, these methods attempt to design a robustness margin or factor for the system. These methods are deterministic in which a robustness factor, for example, 2 or 3 times the expected number of subjects to track or the strength of illumination would be required in the design. This often results in over design, thus increasing costs, or under design causing failure by unanticipated factors. In order to overcome these limitations, we propose in this paper an alternative framework to analyze the physics of the failure process and, through the concept of reliability, determine the time to failure in automated video tracking systems. The benefit of our proposed framework is that we can provide a unified and statistical index to evaluate the performance of automated video tracking system for a task to be performed. At the same time, the uncertainty problem about a failure process, which may be caused by the system’s complexity, imprecise measurements of the relevant physical constants and variables, or the indeterminate nature of future events, can be addressed accordingly based on our proposed framework.	indeterminacy in concurrent computation;paradigm;performance evaluation;robotics;tracking system;usability;video tracking	Chung-Hao Chen;Yi Yao;Andreas F. Koschan;Mongi A. Abidi	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.125	simulation;reliability theory;computer science;video tracking;mathematical model;data mining;reliability;lighting;statistics;robustness	Robotics	45.30019491288261	-46.71009348009479	165701
274c0501089a57c0b182ddf0b947ffce27f760fd	edge recognition in dynamic vision	kalman filters;picture processing;pattern recognition;image edge detection stochastic processes pixel machine vision signal to noise ratio robots stochastic resonance filtering theory nonlinear filters convolution;picture processing kalman filters pattern recognition;extended kalman filter;edge localization edge recognition pattern recognition picture processing dynamic vision stochastic filtering kalman filter convolutions	Using a model of an edge’s motion through a sequence of images, the problem of its localization can be formulated as a stochastic filtering problem. Tlie Extended Kalman Filter for such a system is considered in detail and is shown to be interpretable as a sequence of oriented spatial convolutions. Results are presented which show that the edge localization obtained using this filter is substantially better than that obtained using tlie either the Sobel or Canny edge operators on each image individually.	canny edge detector;convolution;extended kalman filter;sobel operator;stochastic control	Alan M. McIvor	1989		10.1109/CVPR.1989.37838	kalman filter;computer vision;invariant extended kalman filter;fast kalman filter;computer science;machine learning;pattern recognition;filtering problem;mathematics;extended kalman filter;canny edge detector;moving horizon estimation	Vision	45.59878321248237	-50.36895577126322	167294
0191e322146aea376d627234ef85ce29f1912adb	a lightweight multiview tracked person descriptor for camera sensor networks	cameras sensor phenomena and characterization biological system modeling assembly surveillance humans torso footwear histograms head;video surveillance;indexing terms;sensor network;ease of use;multiple views;video indexing;3d model;video indexing camera networks surveillance appearance modeling tracking;object tracking;video indexing lightweight multiview tracked person descriptor camera sensor networks object tracking surveillance video;video surveillance object detection;camera network;object detection	We present a simple multiple view 3D model for object tracking and identification in camera networks. Our model is composed of 8 distinct views in the interval [0, 7pi/4]. Each of the 8 parts describes the person's appearance from that particular viewpoint. The model contains both color and structure information about each view which are assembled into a single entity and is meant as a simple, lightweight object representation for use in camera sensor networks. It is versatile in that it can be gradually assembled on-line while a person is tracked. The model's ease of use and effectiveness for identification in surveillance video is demonstrated.	closed-circuit television;color;data descriptor;image sensor;online and offline;usability	Michael J. Quinn;Thomas Kuo;B. S. Manjunath	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712170	smart camera;computer vision;simulation;wireless sensor network;index term;usability;computer science;video tracking;multimedia	Vision	46.42328952480437	-45.60133020422816	167939
8877c5afa16b025452e444e0798292fe7ee4dca6	occlusion robust symbol level fusion for multiple people tracking		In single view visual target tracking, an occlusion is one of the most challenging problems since target’s features are partially/fully covered by other targets as occlusion occurred. Instead of a limited single view, a target can be observed from multiple viewpoints using a network of cameras to mitigate the occlusion problem. However, information coming from different views must be fused by relying less on views with heavy occlusion and relying more on views with no/small occlusion. To address this need, we proposed a new fusion method which fuses the locally estimated positions of a person by the smart cameras observing from different viewpoints while taking into account the occlusion in each view. The genericity and scalability of the proposed fusion method is high since it needs only the position estimates from the smart cameras. Uncertainty for each local estimate is locally computed in a fusion center from the simulated occlusion assessment based on the camera’s projective geometry. These uncertainties together with the local estimates are used to model the probabilistic distributions required for the Bayesian fusion of the local estimates. The performance evaluation on three challenging video sequences shows that our method achieves higher accuracy than the local estimates as well as the tracking results using a classical triangulation method. Our method outperforms two state-ofthe-art trackers on a publicly available multi-camera video sequence.	algorithm;computation;experiment;generic programming;hidden surface determination;match moving;national fund for scientific research;performance evaluation;scalability;smart camera;symbol level;triangulation (geometry)	Nyan Bo Bo;Peter Veelaert;Wilfried Philips	2017		10.5220/0006127602160226	computer vision;speech recognition	Vision	43.26354886255976	-47.47382328014746	168406
898783f8f63698f633f3819bda58aa8b95723b6f	detection-assisted initialization, adaptation and fusion of body region trackers for robust multiperson tracking	error reduction;image segmentation;3d position estimates;intelligent fusion algorithm;object tracking detection assisted initialization body region tracking multiperson tracking smartroom multiple cameras foreground segmentation maps appearance based object detectors color histograms image characteristics 3d position estimates intelligent fusion algorithm triangulation error reduction cluttered environments;multiperson tracking;color histogram;target tracking feature extraction image colour analysis image segmentation knowledge based systems object detection sensor fusion stereo image processing;body regions robustness histograms image segmentation detectors object detection pattern recognition interactive systems smart cameras pixel;detection assisted initialization;image characteristics;appearance based object detectors;triangulation error reduction;cluttered environments;foreground segmentation maps;image colour analysis;feature extraction;object tracking;stereo image processing;position estimation;color histograms;body region tracking;sensor fusion;target tracking;multiple object tracking;multiple cameras;knowledge based systems;object detection;smartroom	In this paper, we present a system for simultaneous tracking of multiple persons in a smartroom using multiple cameras. Robust person tracks are created, continuously adapted, and deleted by fusing cues from foreground segmentation maps and various appearance-based object detectors. Tracking is performed using color histograms which are automatically filtered and adapted based on local image characteristics. Tracks from the various 2D views are merged to 3D position estimates by an intelligent fusion algorithm based on triangulation error reduction. The approach allows to robustly track moving, standing or sitting persons in cluttered environments and to successfully recover lost tracks at any point in the room. We also introduce a new set of metrics to measure multiple object tracking performance. Our system reaches a high tracking accuracy with average position errors of less than 17cm	algorithm;baseline (configuration management);integrated project support environment;map;pattern recognition;robustness (computer science);sensor	Keni Bernardin;Alexander Elbs;Rainer Stiefelhagen	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.445	color histogram;computer vision;tracking system;feature extraction;computer science;knowledge-based systems;machine learning;video tracking;pattern recognition;sensor fusion;image segmentation	Vision	44.034348563923274	-49.37234506131419	168736
5fd02172672dba532abc344c3773586023200959	monitoring head/eye motion for driver alertness with one camera	rotation classification;image motion analysis;eye motion;colored noise;motion pictures;occlusion;head motion;color predicates;motion estimation;monitoring image sequences image colour analysis motion estimation;testing;computerized monitoring;eyes;monitoring;driver alertness;image colour analysis;3d gaze recovery eye motion head motion driver alertness optical flow color predicates facial features rotation classification occlusion eye blinking;3d gaze recovery;eye blinking;facial features;optical flow;head;vehicles;computer science;cameras eyes image motion analysis head colored noise motion pictures vehicles testing computerized monitoring computer science;cameras;image sequences	We describe a system for analyzing human driver alertness. It relies on optical flow and color predicates to robustly track a person’s head and facial features. Our system classifies rotation in all viewing directions, detects eye/mouth occlusion, detects eye blinking, and recovers the 3D gaze of the eyes. We show results and discuss how this system can be used for monitoring driver alertness.	daylight;hidden surface determination;optical flow;vocabulary	Paul Smith;Mubarak Shah;Niels da Vitoria Lobo	2000		10.1109/ICPR.2000.902999	computer vision;colors of noise;computer science;motion estimation;optical flow;software testing;head;computer graphics (images)	Vision	44.77165802726344	-45.65110762353756	168799
0deea3f11bbb7808f60ec83f554757b2cc7f8a8d	human body tracking with auxiliary measurements	image sampling;image motion analysis;state vector automatic initialization human body tracking particle filtering scheme auxiliary measurements optical flow cues sampling distribution individual body parts detection state parameters;particle filter;human body;humans particle filters sampling methods particle tracking optical filters torso monte carlo methods face detection intelligent robots filtering;optical flow;tracking image sequences image sampling image motion analysis gesture recognition;gesture recognition;tracking;image sequences	This paper presents two techniques for improving human body tracking within the particle filtering scheme. Both techniques explore the use of auxiliary measurements. The first technique uses optical flow cues to improve the sampling distribution. The second technique involves the detection of individual body parts, namely the hand, head and torso; and using these detection results to provide additional inference on subsets of state parameters. This method enables the automatic initialization of state vector and allows recovering from tracking failures. These two methods improve the overall accuracy, efficiency and robustness of human body tracking as illustrated by the experimental results.	optical flow;particle filter;sampling (signal processing)	Mun Wai Lee;Isaac Cohen	2003		10.1109/AMFG.2003.1240832	computer vision;human body;simulation;particle filter;tracking system;computer science;gesture recognition;optical flow;tracking	Vision	46.07097542870358	-47.38089635527232	169152
17ac2bae1c2b778a4bb9675eb221a03f33a254f1	robust visual tracking using structural region hierarchy and graph matching	graph matching;tracking;image hierarchy	Visual tracking aims to match objects of interest in consecutive video frames. This paper proposes a novel and robust algorithm to address the problem of object tracking. To this end, we investigate the fusion of state-of-the-art image segmentation hierarchies and graph matching. More specifically, (i) we represent the object to be tracked using a hierarchy of regions, each of which is described with as a graph matching problem, which is solved by minimizing an energy function incorporating appearance and geometry contexts; and (iii) more importantly, an effective graph updating mechanism is proposed to adapt to the object changes over time for ensuring the tracking robustness. Experiments are carried out on several challenging sequences and results show that our method performs well in terms of object tracking, even in the presence of variations of scale and illumination, moving camera, occlusion, and background clutter. Crown Copyright & 2012 Published by Elsevier B.V. All rights reserved.	algorithm;clutter;crown group;image segmentation;matching (graph theory);mathematical optimization;video tracking	Yi-Zhe Song;Chuan Li;Liang Wang;Peter M. Hall;Peiyi Shen	2012	Neurocomputing	10.1016/j.neucom.2011.11.030	computer vision;machine learning;video tracking;pattern recognition;mathematics;tracking;matching	Vision	42.67108114642128	-49.82471119959805	169539
86f918a351efd6e0c283dd9b526c9da2330f18c3	detection and tracking of multiple humans with extensive pose articulation	pattern clustering multiple human detection multiple human tracking extensive pose articulation image resolution embedded silhouette manifold learning object detection object weighted appearance model probabilistic pose based transition model sliding window buffer surveillance video pedestrian detector;pattern clustering;video surveillance;embedded silhouette manifold learning;probability;image resolution;video signal processing;humans cameras detectors shape surveillance object detection videos intelligent robots image resolution layout;multiple human detection;video surveillance image resolution learning artificial intelligence object detection pattern clustering pose estimation probability video signal processing;surveillance video;multiple human tracking;probabilistic pose based transition model;object weighted appearance model;learning artificial intelligence;pedestrian detector;sliding window buffer;sliding window;object detection;extensive pose articulation;pose estimation	We describe a method for detecting and tracking humans. Different from most of the previous work, we focus on humans with extensive pose articulations, under situations where there is typically only a single camera, multiple humans are present and the image resolution is low. In our method pose clusters are learned from an embedded silhouette manifold. A set of object detectors, each of which corresponds to one pose cluster, are trained based on a novel Object-Weighted Appearance Model. A probabilistic pose-based transition model is used to track multiple objects within a sliding window buffer, making use of the detection responses. The track segments in the sliding windows are connected sequentially into full trajectories. Experiments on a set of challenging surveillance videos are presented; these show good performance of our approach compared to standard pedestrian detectors, under difficult conditions.	biconnected component;cluster analysis;embedded system;generalised hough transform;humans;image resolution;manifold regularization;microsoft windows;sensor	Xiang Lin;Bo Wu;Ramakant Nevatia	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4408940	sliding window protocol;computer vision;simulation;pose;image resolution;3d pose estimation;computer science;pattern recognition;probability;articulated body pose estimation;statistics	Vision	40.545657375842715	-47.4046539639079	169588
aa8c3eb6e821cb44ed5a15a2f09fba332e5561c6	object detection in multi-view x-ray images		Motivated by aiding human operators in the detection of dangerous objects in passenger luggage, such as in airports, we develop an automatic object detection approach for multi-view X-ray image data. We make three main contributions: First, we systematically analyze the appearance variations of objects in X-ray images from inspection systems. We then address these variations by adapting standard appearance-based object detection approaches to the specifics of dual-energy X-ray data and the inspection scenario itself. To that end we reduce projection distortions, extend the feature representation, and address both in-plane and out-of-plane object rotations, which are a key challenge compared to many detection tasks in photographic images. Finally, we propose a novel multi-view (multi-camera) detection approach that combines single-view detections from multiple views and takes advantage of the mutual reinforcement of geometrically consistent hypotheses. While our multi-view approach can be used atop arbitrary single-view detectors, thus also for multi-camera detection in photographic images, we evaluate our method on detecting handguns in carry-on luggage. Our results show significant performance gains from all components.	closing (morphology);computation;distortion;object detection;overhead (computing);radiography;sensor;visual descriptor;x-ray (amazon kindle);xfig;zero suppression	Thorsten Franzel;Uwe Schmidt;Stefan Roth	2012		10.1007/978-3-642-32717-9_15	computer vision;simulation;object-class detection;engineering;computer graphics (images)	Vision	42.347081306559716	-50.56927395164562	169662
53de3a4fc5645113af1860bb59ac180c9fcf141f	a fast image matching technique for the panoramic-based localization	sorting hat;image segmentation;panorama;sorting;mobile handsets feature extraction mobile communication image segmentation sorting target tracking cameras;reckon view image based positioning panorama brisk sorting hat;image based positioning;sorting feature extraction image filtering image matching mobile computing;feature extraction;mobile communication;mobile handsets;reckon view mechanism fast image matching technique panoramic based localization image tracking image based positioning indoor map feature points mobile device image matching sorting hat approach uncorrelated feature pairs brisk feature detection positioning process;reckon view;target tracking;brisk;cameras	This paper proposes a novel image tracking technique for the image-based positioning study on the mobile device. The study uses the panorama as the indoor map to localize the user. Since the number of feature points of the panorama is much larger than feature points from the image of the mobile device, the performance of matching mobile device image with the panorama will significantly affect the efficiency of the positioning. Hence, a Sorting Hat approach is proposed to filter out uncorrelated feature pairs as early as possible. The Sorting Hat is designed according to the characteristic of BRISK which is adopted as feature detection technique in this study. However, since the coordinates of the panorama is not the same as that of mobile device image, a coordinates unification is proposed to improve the effectiveness of image matching. To further improve the image matching speed during the positioning process, a Reckon-View mechanism on the panorama is also proposed. Our experiments show that the Sorting Hat approach can decide if two feature points are correlated within 0.824ms at 99.6% confidence. Furthermore, the experiment also prove that the Reckon-View mechanism can reduce the number of feature pairs computation to two-thirds of the original.	algorithm;computation;experiment;eye tracking;feature detection (computer vision);feature detection (web development);image registration;mobile device;motion estimation;prototype;run time (program lifecycle phase);smartphone;sorting;tablet computer;unification (computer science)	Jiung-yao Huang;Su-Hui Lee;Chung-Hsien Tsai	2016	2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2016.7550792	computer vision;feature detection;mobile telephony;feature extraction;computer science;sorting;machine learning;pattern recognition;image segmentation;feature;computer graphics (images)	Robotics	41.802990275491865	-51.13751288391877	169818
b37b1ef5a502d108dc61d10ab0ac5dd4ba03e290	stableflow: a novel real-time method for digital video stabilization	shock absorbers;springs;trajectory;streaming media;three dimensional displays;mathematical model;cameras	Digital video stabilization is crucial in many applications such as object detection and tracking. It has been studied for decades yielding an extensive amount of literature in the field, however, current approaches suffer from either being computationally expensive or under-performing in terms of visual quality . In this paper, we present StableFlow, a novel real-time method that was inspired by the mass-spring-damper physical model. In StableFlow, a video frame is modelled as a mass suspended in each direction by a critically dampened spring and damper which can be fine-tuned to adapt with different shaking patterns. The proposed method is tested on video sequences that have different types of shakiness and diverse video contents. The obtained results are then compared to current state-of-the-art stabilization algorithms including Youtube stabilization and it is found that the proposed method significantly outperforms other algorithms in terms of visual quality while performing in real time.	algorithm;analysis of algorithms;digital video;object detection;real-time clock;real-time computing;real-time transcription	Abdelrahman Ahmed;Mohamed S. Shehata	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900088	video compression picture types;computer vision;simulation;trajectory;video tracking;shock absorber;mathematical model;statistics;computer graphics (images)	Vision	42.72229151218132	-46.557718851162136	170085
002291f6a214b70ae130366f2d10e216e3cb8a96	pedestrian detection embedded in level-set-based moving objects detection and tracking	geometric active contours;video surveillance;image motion analysis;image segmentation;feature extraction level set vectors tracking training histograms cameras;pedestrian detection system level set based moving object detection level set based moving object tracking video surveillance systems motion cues background positions histogram of oriented gradients stationary cameras boundary based tracking algorithm hog descriptor encode narrow band zero level set individual segmentation;level set method video surveillance pedestrian detection histogram of oriented gradients geometric active contours;histogram of oriented gradients;pedestrians;object tracking;pedestrian detection;traffic engineering computing;level set method;video surveillance image motion analysis image segmentation object detection object tracking pedestrians traffic engineering computing;object detection	This paper presents pedestrian detection embedded into a framework of level-set-based moving object detection and tracking. Most video surveillance systems use stationary cameras to watch scenes. On the premise that the targets are active in the scene, this paper propose to use motion cues to exclude the background positions from evaluation of histograms of oriented gradients (HOG). Discovering that it is the shape that has distinguishable characteristics to differentiate pedestrian from other categories, the boundary based tracking algorithm is adopted to detect and track moving objects. By using the level set method to implement the model, we encode the HOG descriptor only using the pixels located on a narrow band around the zero level set. Considering that an extracted contour may enclose pedestrians in a group, a method for segmenting individuals is also proposed. A set of experiments demonstrate the efficiency and performance of the proposed pedestrian detection system.	active contour model;algorithm;closed-circuit television;data descriptor;encode;embedded system;experiment;gradient;object detection;pedestrian detection;pixel;stationary process	Wei-Gang Chen	2012	2012 IEEE 12th International Conference on Computer and Information Technology	10.1109/CIT.2012.121	computer vision;simulation;object-class detection;histogram of oriented gradients;computer science;viola–jones object detection framework;video tracking;image segmentation;level set method;computer graphics (images)	Vision	40.01837922515207	-47.73931034000063	170262
c228b9e274e383f2a68256590c4e1efbbf7a392d	a double layer background model to detect unusual events	moving object;double layer;background modeling;red light;image sequence;mixture of gaussians	A double layer background representation to detect novelty in image sequences is shown. The model is capable of handling non-stationary scenarios, such as vehicle intersections. In the first layer, an adaptive pixel appearance background model is computed. Its subtraction with respect to the current image results in a blob description of moving objects. In the second layer, motion direction analysis is performed by a Mixture of Gaussians on the blobs. We have used both layers for representing the usual space of activities and for detecting unusual activity. Our experiments clearly showed that the proposed scheme is able to detect activities such as vehicles running on red light or making forbidden turns.		Joaquín Salas;Hugo Jiménez-Hernández;José-Joel González-Barbosa;Juan-Bautista Hurtado-Ramos;Sandra Luz Canchola-Magdaleno	2007		10.1007/978-3-540-74607-2_37	computer vision;simulation;background subtraction;computer science;mixture model;double layer	HCI	40.53012710268737	-46.44576474316571	170356
0d55e81620978c07029697a3b63e4fefd5164b6f	a contrario detection of faces: a case example		The a contrario framework is a statistical formulation of a perception principle that permits one to detect meaningful structures in data. It has been applied to the detection of lines and contours in images, moving objects in video, etc., but no attempt has been made to use it for the detection of faces. The goal of this paper is to show that the a contrario formulation can be adapted to the face detection method described by Viola and Jones in their seminal work. We propose an alternative to the cascade of classifiers proposed by the authors by introducing a stochastic a contrario model for the detections of a single classifier, from which adaptive detection thresholds may be inferred. The result is a single classifier whose detection rates are similar to those of a cascade of classifiers. Moreover, we show how a very short cascade of classifiers can be constructed, which improves the accuracy of a classical cascade, at a much lower computational cost. The results prove the validity of the a contrario a...		Jose Luis Lisani;Silvia Ramis;Francisco J. Perales López	2017	SIAM J. Imaging Sciences	10.1137/17M1118774	mathematics;face detection;cascade;machine learning;artificial intelligence	Theory	41.760777496299454	-46.848857487342606	170873
f0ae665f5b4a9314c77dc9ec285a335ee6ecc15b	a heuristic deformable pedestrian detection method	computer vision;pedestrian detection	Pedestrian detection is an important application in computer vision. Currently, most pedestrian detection methods focus on learning one or multiple fixed models. These algorithms rely heavily on training data and do not perform well in handling various pedestrian deformations. To address this problem, we analyze the cause of pedestrian deformation and propose a method to adaptively describe the state of pedestrians’ parts. This is valuable to resolve the pedestrian deformation problem. Experimental results on the INRIA human dataset and our pedestrian pose database demonstrate the effectiveness of our method.	algorithm;computer vision;free-form deformation;heuristic;pedestrian detection;pose (computer vision)	Yongzhen Huang;Kaiqi Huang;Tieniu Tan	2010		10.1007/978-3-642-19309-5_42	computer vision;simulation;computer science;computer graphics (images)	Vision	42.45592016623802	-46.670932124566676	170921
e1097e6501d5fb4ef95022b036f822cc1855e59b	automatic determination of scene changes in mpeg compressed video	median filter;data compression;peak detection;layout transform coding video compression video sequences motion pictures information systems information retrieval tv ieee news indexing;median filters data compression video coding;sports video;video coding;median filters;compressed video;sports video automatic determination mpeg compressed video peak detection algorithm median filter difference measure correlation measure abrupt scene changes motion light action movies	"""This paper presents a peak-detection algorithm based on the median filter. When used with difference or correlation measures between contiguous video frames, this algorithm can determine significant peaks at """"abrupt scene changes"""" in MPEG compressed video. Experimental data shows the algorithm is effective even for video with significant motion or sudden light changes such as in action movies, or sports video."""	moving picture experts group	Hain-Ching Liu;Gregory L. Zick	1995		10.1109/ISCAS.1995.521629	video compression picture types;data compression;scalable video coding;composite video;median filter;computer vision;video;uncompressed video;computer science;video quality;deblocking filter;video capture;video tracking;block-matching algorithm;multimedia;video processing;smacker video;rate–distortion optimization;motion compensation;h.261;video denoising;statistics;s-video;multiview video coding;computer graphics (images)	Vision	39.53909570193314	-51.637174432196915	171315
621006e42054bc7546f3427edf57c910a2e87b2c	discriminative leaf based hough forest for vehicle detection	discriminative leaf vehicle detection hough forest;traffic engineering computing automobiles object detection;uiuc car datasets discriminative leaf based hough forest vehicle detection hough votes negative test patches noise votes pixel location positive test patches object patches detection efficiency;vehicle detection training noise computer vision object detection uncertainty transforms	This paper introduces a discriminative framework for the task of vehicle detection based on Hough Forest. The leaf nodes in Hough Forest framework are not discriminative enough, which means that they do not have the ability to classify whether the test patches ended up in each leaf are positive or negative. Hough votes are assigned to all test patches by Hough forest, including negative test patches, which will introduce a large number of noise votes, even lead to false alarms. Furthermore, most of test patches are negative (background), and negative patches do not contribute informative Hough votes. Aggregating voting information from all patches extracted at each pixel location is really time-consuming. Thus we have developed a framework to classify whether the test patches are positive or not, and only positive test patches (object patches) participate in Hough voting, which will not only reduce noise votes but also increase detection efficiency. We demonstrate that discriminative leaf node framework improves the results of Hough Forest framework and achieves desirable performance on UIUC car datasets.	enhanced entity–relationship model;hough transform;information;pixel;random forest;tree (data structure)	Xue Fan;Teng Yu;Jingchun Piao;Hyunchul Shin	2014	2014 International Conference on Computer, Information and Telecommunication Systems (CITS)	10.1109/CITS.2014.6878960	computer vision;simulation;geography;pattern recognition	Vision	41.15930490380529	-46.284163965111276	171625
d63c300745ce575366db777efb98045ca1742651	kernel-based on-line object tracking combining both local description and global representation	classifier updating;kernel function;keypoint matching;object tracking		kernel (operating system)	Quan Miao;Guijin Wang;Xinggang Lin	2013	IEICE Transactions		kernel;computer vision;computer science;machine learning;video tracking;pattern recognition;statistics	Vision	41.17680449330054	-48.99246437355464	171752
91e0fc4030e7369fd5ec3d5dc6dbb72c5be84c45	monitoring head dynamics for driver assistance systems: a multi-perspective approach	real world driving data driver head dynamics monitoring driver assistance systems driving environment vision based head pose algorithms single camera self occlusions facial features shape feature based multiperspective framework distributed camera setup head movements confidence measure head pose estimate;cameras vehicles facial features face monitoring estimation;algorithms;drivers;head;driver information systems;cameras;driver support systems;pose estimation driver information systems;pose estimation	A visually demanding driving environment, where elements surrounding a driver are constantly and rapidly changing, requires a driver to make spatially large head turns. Many existing state of the art vision based head pose algorithms, however, still have difficulties in continuously monitoring the head dynamics of a driver. This occurs because, from the perspective of a single camera, spatially large head turns induce self-occlusions of facial features, which are key elements in determining head pose. In this paper, we introduce a shape feature based multi-perspective framework for continuously monitoring the driver's head dynamics. The proposed approach utilizes a distributed camera setup to observe the driver over a wide range of head movements. Using head dynamics and a confidence measure based on symmetry of facial features, a particular perspective is chosen to provide the final head pose estimate. Our analysis on real world driving data shows promising results.	algorithm;device driver	Sujitha Martin;Ashish Tawari;Mohan Manubhai Trivedi	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728568	computer vision;simulation;engineering;computer graphics (images)	Robotics	44.00102592417323	-46.2862982419964	171754
10e456dcc5d06ca6135c95a1a7d0061d7568dc5e	performance evaluation procedure for vision based object feature extraction algorithms	object recognition;performance evaluation;features extraction;illumination;occlusion;scale;cluttering	In this paper, we introduce the performance evaluation procedure for vision based object feature extraction algorithms. Vision based object feature extraction algorithms are widely employed in object recognition and localization for robots. Our purpose is to establish evaluation procedure and performance measures for these algorithms. The object database, called OFEX(Obejct Feature EXtraction), has been constructed to test the proposed procedures. We also examine experimental results for one of the feature extraction algorithms.	algorithm;feature extraction;outline of object recognition;performance evaluation;robot	Minku Kang;Wonkook Choo;Seungbin Moon	2010		10.1145/2377576.2377583	computer vision;feature extraction;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;3d single-object recognition;feature	AI	41.331012138644574	-49.79825375230907	171788
03674f2f42791bfdc1fe934498e7ecd0426bdf7b	a unified approach to scene change detection in uncompressed and compressed video	database indexing;parameter estimation video coding data compression database indexing image retrieval signal detection image sequences statistical analysis feature extraction;data compression;content type unified approach scene change detection uncompressed video compressed video indexing fast retrieval scene analysis reliable scene change detection algorithms mpeg 2 compressed video sequences statistical features;video signal processing;efficient algorithm;signal detection;video sequence scene change detection efficient algorithm mpeg 2 uncompressed video automatic key information extraction indexing fast retrieval scene analysis reliable scene change detection algorithms unified approach mpeg 2 compressed video statistical properties statistical features estimation uncompressed domain algorithms abrupt transitions detection gradual transitions detection detected transition region accuracy s sequences;statistical analysis indexing image sequences data compression video signal processing feature extraction content based retrieval;statistical properties;video coding;statistical analysis;indexing;feature extraction;indexation;layout video compression image coding transform coding change detection algorithms data mining indexing information retrieval image analysis detection algorithms;layout video compression transform coding image coding data mining indexing information retrieval image analysis detection algorithms change detection algorithms;scene change detection;parameter estimation;content based retrieval;compressed video;scene analysis;transition region;image sequences;image retrieval	There is an urgent need to extract key information automatically from video for the purposes of indexing, fast retrieval and scene analysis. To support this vision, reliable scene change detection algorithms must be developed. This paper describes a novel unified algorithm for scene change detection in uncompressed and MPEG-2 compressed video sequences using statistical features of images. Results on video of various content types are reported and validated with the proposed scheme in uncompressed and MPEG-2 compressed video. Furthermore, results show that the accuracy of the detected transitions is above 95% and 90% for uncompressed and MPEG-2 compressed video respectively.	algorithm;data compression;mpeg-2	Warnakulasuriya Anil Chandana Fernando;Cedric Nishan Canagarajah;David R. Bull	2000	2000 Digest of Technical Papers. International Conference on Consumer Electronics. Nineteenth in the Series (Cat. No.00CH37102)	10.1109/30.883445	video compression picture types;data compression;database index;computer vision;search engine indexing;uncompressed video;feature extraction;image retrieval;computer science;video tracking;pattern recognition;estimation theory;video denoising;information retrieval;statistics;detection theory	Vision	39.233364568709874	-51.81921524467601	172784
6bd60e611bcd528327626e9fa6663472fd178f39	a dual-stage robust vehicle detection and tracking for real-time traffic monitoring	dual stage vehicle detection;automatic vehicle location;dynamic thresholding;occlusion reasoning dual stage vehicle detection vehicle tracking real time highway traffic monitoring outdoor dynamic scene real time traffic surveillance system foreground object extraction background scene multibackground modelling dynamic thresholding low scale quasi connected component image object grouping image object cleaning block energy function;video surveillance;motor vehicles;surveillance system;real time highway traffic monitoring;real time traffic;vehicle detection;block energy function;outdoor dynamic scene;regional analysis;real time traffic surveillance system;energy function;occlusion reasoning;dynamic threshold;background scene;monitoring;image object grouping;low scale quasi connected component;foreground object extraction;traffic engineering computing;traffic surveillance;vehicle tracking;traffic monitoring;multibackground modelling;robustness vehicle detection monitoring layout vehicle dynamics real time systems traffic control surveillance object detection pixel;image object cleaning;connected component;video surveillance monitoring object detection tracking traffic engineering computing;tracking;object detection;dynamic scenes	This paper addresses the important problem of detecting and tracking vehicles in outdoor dynamic scenes as part of a real-time traffic surveillance system. The proposed solution is based on a dual-stage approach, using a pixel-level stage to extract foreground object from background scenes and a block-level stage to detect and track vehicles. The pixel-level stage combines a multi-background modelling with a dynamic thresholding, using a low-scale quasi-connected-components as a first stage for image object grouping/cleaning. The block-level performs a 8 times 8 block-region analysis defining a block energy function that is used to label the blocks belonging to different vehicles and track them over a stack of images. This approach has proven to be very helpful for occlusion reasoning. The proposed solution has the ability to overcome some of the most difficult problem that arise in outdoor scenes such as illumination variations, shadow-casts and waving movement resulting from trees and camera vibration. The performance and robustness of the proposed algorithm is shown using real highway traffic monitoring situations	algorithm;connected component (graph theory);illumination (image);mathematical optimization;pixel;real-time clock;real-time transcription;sensor;shadow volume;sputter cleaning;thresholding (image processing)	Jorge Batista;Paulo Peixoto;Catarina Cunha Fernandes;Miguel Ribeiro	2006	2006 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2006.1706795	embedded system;computer vision;simulation;geography	Vision	42.39856377296658	-45.06607232135028	172815
655a1d1c70b0d8b4f8ceeee707d5ab443bb4ef5d	detecting persons using hough circle transform in surveillance video	elevator surveillance;hough circle transform;person detection	Robust person detection in real-world images is interesting and important for a variety of applications, such as visual surveillance. We address the task of detecting persons in elevator surveillance scenes in this paper. To get more passengers in the lift car, the camera usually installed at the corner of ceiling. However, the high and space of lift car are limited, which makes person occluded by each other or some parts of body invisible in captured images. In this paper, we propose a novel approach to detect head contours, which includes three main steps: pre-processing, head contour detection and post-processing. Hough circle transform is adopted in the second stage, which is robust to discontinuous boundaries in circle detection. Proposed pre-processing and post-processing methods are efficient to remove false alarms on background or body part. Experimental results show our proposed approach is time saving and has better person detection results than some other methods.	elevator algorithm;hough transform;preprocessor;sensor;video post-processing	Hantian Liu;Yueliang Qian;Shouxun Lin	2010			computer vision;computer graphics (images)	Vision	42.42799364598325	-45.14038103740416	172984
9c711291eef29e451da9d306453dc1f1dfb57885	lightweight random ferns using binary representation	object recognition;storage management image classification image representation object recognition random processes statistical distributions;storage management;image classification;statistical distributions;image representation;random processes;radio frequency memory management probability distribution training real time systems runtime augmented reality;probabilities lightweight random ferns binary representation real time keypoint recognition runtime performance offline training phase frame rate performance memory requirement reduction	In many applications which require real-time keypoint recognition such as Augmented Reality, Random Ferns (RF) is widely used due to its runtime performance. It relies on an offline training phase during which runtime computational burdens are delegated. This leads to robust, accurate, and framerate performance. However, it requires significant amounts of memory, and this has been an obstacle to its use in industry, especially in mobile environments. In this paper, we propose Lightweight Random Ferns to reduce the memory requirements of RF by modifying the representation of probabilities used in ferns to a single bit from floating point. As a result, the total memory requirements of RF are significantly reduced.	augmented reality;binary number;online and offline;radio frequency;real-time locating system;requirement;run time (program lifecycle phase)	Suwon Lee;Sang-Wook Lee;Yeong Nam Chae;Hyun Seung Yang	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		probability distribution;stochastic process;computer vision;contextual image classification;simulation;computer science;theoretical computer science;cognitive neuroscience of visual object recognition;machine learning;statistics	HPC	40.71730804156215	-50.794594502743536	173001
cab693554a76c287e5e1f61c58ff11aaef906f4b	rotating adaptive haar wavelet transform for human tracking in thermal omnidirectional vision	thermal vision;omnidirectional vision;kinematics;thermal vision omnidirectional vision visual tracking;computer vision;wavelet transforms;adaptive filters;target tracking kinematics particle filters adaptation models wavelet transforms;infrared imaging;object tracking;wavelet transforms adaptive filters computer vision haar transforms infrared imaging object tracking particle filtering numerical methods;particle filters;target tracking;haar transforms;visual tracking;adaptation models;short term occlusion human tracking surveillance system thermal omnidirectional vision system global field of view rotating adaptive haar wavelet transform nonisotropic distortion catadioptric omnidirectional vision cov robust tracking rotational kinematic model adaptive particle filter tracking algorithm;particle filtering numerical methods	In this paper, a novel surveillance system, thermal omnidirectional vision system, is introduced which is robust to illumination and has a global field of view. According to the characteristic of the proposed system, a rotating adaptive Haar wavelet transform is developed for human tracking in thermal omnidirectional vision. The proposed feature can effectively handle the nonisotropic distortion of catadioptric omnidirectional vision (COV). For robust tracking, we develop a rotational kinematic model based adaptive particle filter, which can handle various movements including rapid movement. Since the involvement of the rotational kinematic model, the proposed tracking algorithm can well deal with the short term occlusion. Finally, a series of experiments verify the effectiveness of the proposed rotating adaptive Haar wavelet transform and the rotational kinematic model based adaptive particle filter for human tracking in thermal omnidirectional vision.	algorithm;distortion;experiment;haar wavelet;particle filter;wavelet transform	Yazhe Tang;Y. F. Li;Tianxiang Bai;Xiaolong Zhou	2012	2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2012.6343068	computer vision;simulation;geography;optics	Robotics	43.884543541671825	-46.24481550899594	173713
26c840ba2c9c4259d4888670585f7d18266570f9	"""""""hybrid cone-cylinder"""" codebook model for foreground detection with shadow and highlight suppression"""	fluctuations;application software;surveillance;best practice;layout;robustness;people tracking;lighting;cameras;object detection;light sources	"""In the interest of 24-7 long-term surveillance, a truly robust, adaptive, and fast background-foreground segmentation technique is required. This paper deals with the especially difficult but extremely common problems of moving backgrounds, shadows, highlights, and illumination changes. To produce reliable foreground extraction in the face of these problems, the best practical aspects of two algorithms, Codebook Segmentation[6] and HSV Shadow Suppression[2] are combined. The main contribution of this paper is the introduction of the """"Hybrid Cone-Cylinder"""" Codebook (HC3) model. Results show superior speed and quantitatively better performance in many different conditions and environments. Applications include people-tracking with Omni-directional cameras and vehicle-counting with rectilinear cameras."""	algorithm;block cipher;codebook;cone;cylinder-head-sector;global illumination;illumination (image);regular grid;shadow paging;zero suppression	Anup Doshi;Mohan Manubhai Trivedi	2006	2006 IEEE International Conference on Video and Signal Based Surveillance	10.1109/AVSS.2006.1	layout;computer vision;application software;simulation;computer science;lighting;robustness;best practice;computer graphics (images)	Robotics	42.884794188603	-45.81690744104989	173913
3c184972cd88df1affbc582b9657de942e4f2e89	on fuzzy clustering and content based access to networked video databases	search and retrieval;global information infrastructure;query refinement;video segmentation;video segmentation fuzzy clustering content based access networked video databases video on demand global information infrastructure video querying user interaction feedback based query refinement network traffic video segments video browsing video searching video retrieval video sequences representative frames scene change detection;fuzzy logic;fuzzy clustering;compact representation;video on demand layout databases feedback telecommunication traffic video sequences change detection algorithms clustering algorithms gunshot detection systems detection algorithms;detection algorithm;scene change detection;video browsing;video database;user interaction	Video Databases and video on demand represent an important application of the evolving Global Information Infrastructure. However, video querying involves a lot of user interaction and feedback based query refinement, which can generate large traffic volumes on the network if full video segments are sent. To aid in efficient Video browsing, search and retrieval across the network, we need to find good compact representations for long video sequences. Representative Frames (Rframes) provide such a representation. Extant algorithms use use scene change detection to segment video into shots and pick Rframes. However, scene change detection techniques fail badly in presence of gradual scene changes which are quite prevalent in most videos. We present another way of finding Rframe using fuzzy clustering without dealing with any scene change detection algorithms. Fuzzy clusters provide a more natural approach to this problem since membership of a frame in some particular scene is not binary. This allows us to handle gradual scene changes. We report on our approach, present preliminary experimental results, and discuss ongoing work.	algorithm;cluster analysis;database;failing badly;fuzzy clustering;grammar-based code;refinement (computing)	Anupam Joshi;Sansanee Auephanwiriyakul;Raghu Krishnapuram	1998		10.1109/RIDE.1998.658277	video compression picture types;computer vision;computer science;video tracking;block-matching algorithm;multimedia;smacker video;motion compensation;video post-processing;world wide web	Vision	39.520926666343364	-52.01072593556044	174121
def9699e648b0ad46d7f12a8c1fef1205278ba6b	high and low level object descriptions for video tracking process	em method object description video tracking process real time traffic scene region clustering brightness variation occlusion geometric attribute model motion estimation model background update approach spatialtemporal segmentation algorithm approach expectation maximization method;video signal processing brightness expectation maximisation algorithm image segmentation motion estimation pattern clustering road traffic spatiotemporal phenomena;image segmentation abstracts object recognition adaptation models mathematical model	In this paper a new segmentation algorithm approach for real time traffic scenes is proposed, combining high level and low level object descriptions. Both descriptions make it possible to develop a tracking method, robust regarding occlusions, region clustering and brightness variations. High level description is defined by geometric attributes and motion model. Updating these features (associated to each object) can be obtained by a low level segmentation which is based on a background update approach, associated with a spatial-temporal segmentation. This spatial-temporal segmentation is built on a motion estimation taken out from a modified Expectation-Maximization (EM) method. These two descriptions leads to a really efficient strategy in terms of robustness, over or sub-segmentations and occlusions. Furthermore, under severe brightness changes, our new temporal algorithm also permits a perfect background update control. Some real traffic examples are included at the end of this paper.	cluster analysis;expectation–maximization algorithm;high- and low-level;high-level programming language;motion estimation;robustness (computer science);spatial reference system;unsupervised learning;video tracking	David Izquierdo;Yannick Berthoumieu	2002	2002 11th European Signal Processing Conference		computer vision;geography;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	44.7424026674588	-50.12044264049155	174437
08e93b51102467d27c6d454e481d2917766e61c5	multiple-target tracking for intelligent headlights control	vehicles vectors cameras merging shape roads image color analysis;lighting control;moving target indicators;glare;vehicle detection;target tracking computer vision intelligent control lighting control markov processes pattern classification road vehicles;classification;intelligent control;system performance;data association;markov random field multiple target tracking intelligent headlights control intelligent vehicle lighting systems computer vision software road lamps reflective elements poles traffic signs specialized supervised classifiers blob features lighting system performance temporal consistency classifier decision maximum a posteriori inference;headlamps;markov random field;multiple target tracking;light;computer vision;electronic controllers;intelligent headlights;graphical models;shape;vectors;roads;belief propagation;image color analysis;intelligent vehicles;pattern classification;merging;graphical model;algorithms;vehicle detection belief propagation computer vision data association graphical models intelligent headlights;vehicles;markov processes;target tracking;detection and identification systems;cameras;quantitative evaluation;road vehicles	Intelligent vehicle lighting systems aim at automatically regulating the headlights' beam to illuminate as much of the road ahead as possible while avoiding dazzling other drivers. A key component of such a system is computer vision software that is able to distinguish blobs due to vehicles' headlights and rear lights from those due to road lamps and reflective elements such as poles and traffic signs. In a previous work, we have devised a set of specialized supervised classifiers to make such decisions based on blob features related to its intensity and shape. Despite the overall good performance, there remain challenging that have yet to be solved: notably, faint and tiny blobs corresponding to quite distant vehicles. In fact, for such distant blobs, classification decisions can be taken after observing them during a few frames. Hence, incorporating tracking could improve the overall lighting system performance by enforcing the temporal consistency of the classifier decision. Accordingly, this paper focuses on the problem of constructing blob tracks, which is actually one of multiple-target tracking (MTT), but under two special conditions: We have to deal with frequent occlusions, as well as blob splits and merges. We approach it in a novel way by formulating the problem as a maximum a posteriori inference on a Markov random field. The qualitative (in video form) and quantitative evaluation of our new MTT method shows good tracking results. In addition, we will also see that the classification performance of the problematic blobs improves due to the proposed MTT algorithm.	algorithm;computer vision;design of the fat file system;encode;feature model;genetic algorithm;many-to-many;markov chain;markov random field;meaning–text theory;pixel;statistical model;supervised learning;usb hub	José C. Rubio;Joan Serrat;Antonio Manuel López Peña;Daniel Ponsa	2010	13th International IEEE Conference on Intelligent Transportation Systems	10.1109/TITS.2011.2175219	computer vision;simulation;computer science;engineering;artificial intelligence;machine learning;graphical model;intelligent control	Vision	41.703743029075795	-45.7416906314639	174864
25554df51451fd7abefc9040f8e694cf59d00bf2	compressive tracking based on superpixel segmentation	compressive sensing;visual tracking;superpixel	The compressive sensing trackers, which utilize a very sparse measurement matrix to capture the targets' appearance model, perform well when the tracked targets are well defined. However, such trackers often run into drifting problems due to the fact that the tracking result is a bounding box which also includes background information, especially in the case of occlusion and low contrast situations. In this paper, we propose an online compressive tracking algorithm based on superpixel segmentation (SPCT). The proposed algorithm employs a weighted multi-scale random measurement matrix along with an efficient superpixel segmentation to preserve the image structure of the targets during tracking. The superpixel segmentation is used to distinguish the target from its surrounding background, to obtain the weighted features within the bounding box. Furthermore, a feedback strategy is also proposed to update the classifier model to reduce the drifting risk. Extensive experimental results have demonstrated that our proposed algorithm outperforms several state-of-the-art tracking algorithms as well as the compressive trackers.	algorithm;compressed sensing;emoticon;image segmentation;integrated circuit layout design protection;minimum bounding box;pixel;sequence logo;sparse matrix;sylvester's sequence;video tracking	Tieling Chen;Hichem Sahli;Yanning Zhang;Tao Yang;Lingyan Ran	2016		10.1145/3007120.3011074	computer vision;simulation;eye tracking;computer science;machine learning;compressed sensing	Vision	42.91071048406185	-48.61910431618148	175290
b8923ec9b6885ea41c4276a7566796af162ca198	mean shift using novel weight computation and model update	mean shift	The tracking accuracy of mean shift algorithm employing the Bhattacharyya coefficient as similarity metric is low. The reason is analyzed on the basis of effect identity of the histogram bins in the target model and the candidate. A novel weighting method which is derived from the enhanced histogram intersection is proposed. Several improvements are considered in the implementation of the algorithm to handle rapid object motion or scene interference with similar color to target. Then we give a novel model update method based on the prior information of the tracked target. Experimental results illustrate that the proposed method outperforms the standard mean shift algorithm.	algorithm;coefficient;computation;interference (communication);jaccard index;mean shift	Guocheng An;Fengjun Zhang;Guozhong Dai	2010			mean-shift;computer science;histogram matching;artificial intelligence;machine learning;pattern recognition;mathematics	Vision	43.36192557071984	-51.197211569640885	176215
04ab3cff5e438b3913a9df26625b32f446872520	automatic dense visual semantic mapping from street-level imagery	geophysical image processing;image segmentation;random processes geophysical image processing image segmentation;semantics;visualization;overhead ground plane map automatic dense visual semantic mapping semantic mapping multiview street level imagery bird s eye view semantic object labels conditional random fields semantic image segmentation geometrical function;roads;random processes;vehicles;semantics cameras image segmentation vehicles labeling visualization roads;cameras;labeling	This paper describes a method for producing a semantic map from multi-view street-level imagery. We define a semantic map as an overhead, or bird's eye view of a region with associated semantic object labels, such as car, road and pavement. We formulate the problem using two conditional random fields. The first is used to model the semantic image segmentation of the street view imagery treating each image independently. The outputs of this stage are then aggregated over many images to form the input for our semantic map that is a second random field defined over a ground plane. Each image is related by a simple, yet effective, geometrical function that back projects a region from the street view image into the overhead ground plane map. We introduce, and make publicly available, a new dataset created from real world data. Our qualitative evaluation is performed on this data consisting of a 14.8 km track, and we also quantify our results on a representative subset.	bird's-eye view;conditional random field;google street view;image segmentation;overhead (computing);semantic mapper	Sunando Sengupta;Paul Sturgess;Lubor Ladicky;Philip H. S. Torr	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6385958	computer vision;labeling theory;visualization;computer science;machine learning;semantics;image segmentation;computer graphics (images)	Robotics	43.79832879042502	-51.217145333038516	176401
55af59de254750fcf82b6ac6c6a1825a6217061e	spatiotemporal visual attention architecture for video analysis	video surveillance;spatiotemporal processing;video signal processing;surveillance;video analysis;surveillance video signal processing image sequences;conference papers;scene analysis spatiotemporal visual attention architecture video analysis video sequence video surveillance;spatiotemporal phenomena computer architecture colored noise image analysis image sequence analysis noise generators computer science postal services video surveillance image segmentation;visual attention;scene analysis;image sequences	Several visual attention (VA) schemes have been proposed with the saliency-based ones being the most popular. The proposed work provides an extension towards VA in video sequences by incorporating the temporal dimension. The architecture is presented in detail and potential applications are investigated. We expect that the extended VA scheme can reveal interesting events across the sequence like occlusions and short occurrences of objects, providing a basis for video surveillance (e.g. intruder detection), segmentation, and summarization applications.	closed-circuit television;intruder detection;video content analysis	Konstantinos Rapantzikos;Nicolas Tsapatsoulis;Yannis S. Avrithis	2004	IEEE 6th Workshop on Multimedia Signal Processing, 2004.	10.1109/MMSP.2004.1436423	video compression picture types;computer vision;image processing;computer science;video tracking;multimedia;video processing;computer graphics (images)	Vision	39.43450069673644	-47.20177042484827	177327
e18303f05ce81e110fc7802cf6ffdc6e0381de88	understanding interactions and guiding visual surveillance by tracking attention	randomised fern;decision branch;interesting region;real-time visual tracking;central tenet;coarse classification;automatic method;guiding visual surveillance;multi-camera surveillance system;human interaction;gradient orientation	randomised fern;decision branch;interesting region;real-time visual tracking;central tenet;coarse classification;automatic method;guiding visual surveillance;multi-camera surveillance system;human interaction;gradient orientation	interaction	Ian D. Reid;Ben Benfold;Alonso Patron-Perez;Eric Sommerlade	2010		10.1007/978-3-642-22822-3_38	computer vision;simulation	Vision	42.049356501614696	-46.65704892088263	177423
3d2cf5d529befd77183b267a4d987e6b1c127ed9	libldb: a library for extracting ultrafast and distinctive binary feature description	efficiency;mobile object recognition and tracking;distinctiveness;mobile handheld devices;mobile augmented reality;binary feature description	This paper gives an overview of libLDB -- a C++ library for extracting an ultrafast and distinctive binary feature LDB (Local Difference Binary) from an image patch. LDB directly computes a binary string using simple intensity and gradient difference tests on pairwise grid cells within the patch. Relying on integral images, the average intensity and gradients of each grid cell can be obtained by only 4~8 add/subtract operations, yielding an ultrafast runtime. A multiple gridding strategy is applied to capture the distinct patterns of the patch at different spatial granularities, leading to a high distinctiveness of LDB. LDB is very suitable for vision apps which require real-time performance, especially for apps running on mobile handheld devices, such as real-time mobile object recognition and tracking, markerless mobile augmented reality, mobile panorama stitching. This software is available under the GNU General Public License (GPL) v3.	augmented reality;c++;gnu;gradient;image stitching;mobile device;outline of object recognition;real-time clock	Xin Yang;Chong Huang;Kwang-Ting Cheng	2014		10.1145/2647868.2654888	optimal distinctiveness theory;computer vision;simulation;computer science;operating system;efficiency;world wide web;computer graphics (images)	Robotics	40.89146877785664	-50.766570436744246	177608
9006c2b93a7c6eb0875fa30d76982c70bb805d72	depth image based 3d hand pose estimation framework	image motion analysis;depth image;real time;pixel classification;kinect depth images;real time 3d motion capture;hci;mean shift;image classification;kinect depth images real time 3d motion capture 3d hand pose estimation framework hci depth image synthetic depth images random decision forests rdf mean shift method joint location esstimation pixel classification;3d hand pose estimation framework;three dimensional;joint location esstimation;computer vision;motion capture;estimation;three dimensional displays;pattern recognition;synthetic depth images;humans;random decision forests;real time systems three dimensional displays estimation humans computer vision pattern recognition conferences;mean shift method;rdf;conferences;pose estimation image classification image motion analysis;real time systems;pose estimation	Real-time 3D motion capture for the human hand opens many avenues for HCI. This work describes our framework for fitting a 3D skeleton to the human hand using depth images. We represent a human hand by a 3D skeleton with 15 joints. Using this model, various synthetic depth images are generated. Random Decision Forests (RDF) are trained and used to assign each pixel to a hand part. A mean-shift method is used for estimating joint locations using pixel classification results. Our system runs in real time at 30 fps on Kinect depth images.	3d pose estimation;human–computer interaction;kinect;mean shift;motion capture;pixel;real-time transcription;synthetic intelligence	Furkan Kiraç;Yunus Emre Kara;Cem Keskin;Lale Akarun	2012	2012 20th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2012.6204850	three-dimensional space;computer vision;estimation;contextual image classification;motion capture;simulation;pose;mean-shift;computer science;rdf;computer graphics (images)	Vision	40.96119777575904	-47.60030779147544	177973
659fc18b1ec79a7437e6e7b1dce145d423e82199	real time person detection and tracking by mobile robots using rgb-d images	detectors;reliability;kalman filters;mobile robots;human poses real time person detection real time person tracking mobile robots rgb d images human robot interaction human detection partial occlusions;detectors kalman filters mobile robots face reliability target tracking;face;target tracking;robot vision human robot interaction image colour analysis mobile robots object detection object tracking pose estimation real time systems	Detecting and tracking humans are key problems for human-robot interaction. In this paper we present an algorithm for mobile robots to detect and track people reliably, even when humans go through different illumination conditions, often change in a wide variety of poses, and are frequently occluded. We have improved the performance of face and upper body detection to quickly find people in each frame. This combination enhances the efficiency of human detection in dealing with partial occlusions and changes in human poses. To cope with the higher challenges of complex changes of human poses and occlusions, we at the same time combine a fast compressive tracker with a Kalman filter to track the detected humans. Experimental results on a challenging database show that our method achieves high performance and can run in real time on mobile robots.	algorithm;human–robot interaction;kalman filter;mobile robot;sensor;tracking system	Vo Duc My;Lixing Jiang;Andreas Zell	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090411	face;kalman filter;mobile robot;computer vision;detector;simulation;computer science;reliability	Robotics	45.31664632758791	-45.410370539809904	178016
37a62c697f85b27194d7e450203fc4d0146f84fe	foreground classification using active template in the scene context for visual surveillance	background modeling;real time;contextual information;maximum matching;visual surveillance;target recognition;scene context;deformable template;template matching;exhaustive search	This paper presents an integrated framework for real-time target category recognition integrating the active template matching and the context of visual surveillance. The active templates are in the form of a dictionary of active features (bases), which are allowed to slightly shift at different locations and orientations. They can be learned for each object type from a small set of positive samples that roughly aligned. With these learned deformable templates, the moving foregrounds subtracted from background model are recognized through searching maximum matching likelihood. To avoid the exhaustive search for template matching and reduce the noise disturbance, a scheme to estimate target size and pose at specific location is developed based on the contextual information of scene geometry. This framework can be an independent module embedded into a visual surveillance system. Its performance and benefit of using context are quantitatively demonstrated on public dataset with comparisons.	brute-force search;dictionary;embedded system;matching (graph theory);object type (object-oriented programming);real-time clock;template matching	Xiaoying Sha;Xiaobai Liu;Jianting Wen	2009		10.1145/1529282.1529488	computer vision;template matching;computer science;machine learning;pattern recognition;brute-force search;matching	Vision	40.718516361877036	-49.509802613307805	178073
a417d5eb6da93a23bebb35a54d025fca274d6078	robust visual tracking by integrating multiple cues based on co-inference learning	real time;co inference;dynamic environment;cue integration;state space;variational analysis;factorized graphical model;graphical model;parameter estimation;sensor fusion;importance sampling;visual tracking;sequential monte carlo	Visual tracking can be treated as a parameter estimation problem that infers target states based on image observations from video sequences. A richer target representation may incur better chances of successful tracking in cluttered and dynamic environments, and thus enhance the robustness. Richer representations can be constructed by either specifying a detailed model of a single cue or combining a set of rough models of multiple cues. Both approaches increase the dimensionality of the state space, which results in a dramatic increase of computation. To investigate the integration of rough models from multiple cues and to explore computationally efficient algorithms, this paper formulates the problem of multiple cue integration and tracking in a probabilistic framework based on a factorized graphical model. Structured variational analysis of such a graphical model factorizes different modalities and suggests a co-inference process among these modalities. Based on the importance sampling technique, a sequential Monte Carlo algorithm is proposed to provide an efficient simulation and approximation of the co-inferencing of multiple cues. This algorithm runs in real-time at around 30 Hz. Our extensive experiments show that the proposed algorithm performs robustly in a large variety of tracking scenarios. The approach presented in this paper has the potential to solve other problems including sensor fusion problems.	algorithmic efficiency;approximation;computation;estimation theory;experiment;graphical model;importance sampling;monte carlo algorithm;monte carlo method;real-time clock;robustness (computer science);rough set;sampling (signal processing);sensor web;simulation;state space;variational analysis	Ying Wu;Thomas S. Huang	2004	International Journal of Computer Vision	10.1023/B:VISI.0000016147.97880.cd	computer vision;mathematical optimization;particle filter;eye tracking;variational analysis;importance sampling;computer science;state space;machine learning;mathematics;sensor fusion;graphical model;estimation theory;statistics	Vision	45.51715358884545	-48.17806539370025	178698
067dc35f2f811655dbc602a98ff9858eb060af1c	collaborative tracking in video sequences using corners and gradient information	video object;kernel;video signal processing bayes methods gradient methods image sequences tracking;video signal processing;collaborative tracking;crowded scenes;bayes methods;collaboration;video sequences;joints;gradient based information;bayesian method;multiple video objects tracking;shape;estimation;occlusion handling;mathematical model;crowded scenes collaborative tracking video sequences multiple video objects tracking gradient based information bayesian method;gradient methods;shape estimation collaboration target tracking kernel mathematical model joints;shape based tracking;target tracking;shape based tracking collaborative tracking occlusion handling;tracking;image sequences	In this paper the problem of the simultaneous tracking of multiple video objects is addressed. In the proposed approach, each tracker behaves independently using corners and gradient-based information until an interaction with other trackers is reported. During the interaction, a new Bayesian method that allows the exploitation of the information of each tracker in a collaborative way is used. By using this method, it will be shown that it is possible to improve the global correctness of the tracking and targets model estimation by fusing the information owned locally by each tracker in a collaborative way. The reported experimental results indicate good performances of the algorithm in crowded scenes.	algorithm;coefficient;correctness (computer science);gradient descent;local shared object;performance;wavelet	Francesco Monti;Majid Asadi;Carlo S. Regazzoni	2008	2008 11th International Conference on Information Fusion		computer vision;simulation;computer science;pattern recognition	Robotics	45.84946094439647	-49.12848927329872	179098
2651fc501120f1c3fbc176e1d0a57edbedc2f742	efficient optimal kernel placement for reliable visual tracking	observability;reliability engineering;kernel;theory and practice;convolution;real time;motion estimation;shape measurement;null;theoretical analysis;particle tracking;target tracking;visual tracking;algorithm design and analysis;scale invariance;kernel target tracking motion estimation shape measurement observability reliability engineering algorithm design and analysis convolution optimization methods particle tracking;optimization methods	This paper describes a novel approach to optimal kernel placement in kernel-based tracking. If kernels are placed at arbitrary places, kernel-based methods are likely to be trapped in ill-conditioned locations, which prevents the reliable recovery of the motion parameters and jeopardizes the tracking performance. The theoretical analysis presented in this paper indicates that the optimal kernel placement can be evaluated based on a closed-form criterion, and achieved efficiently by a novel gradient-based algorithm. Based on that, new methods for temporal-stable multiple kernel placement and scale-invariant kernel placement are proposed. These new theoretical results and new algorithms greatly advance the study of kernel-based tracking in both theory and practice. Extensive real-time experimental results demonstrate the improved tracking reliability.	algorithm;condition number;eisenstein's criterion;gradient;kernel (operating system);real-time clock	Zhimin Fan;Ming Yang;Ying Wu;Gang Hua;Ting Yu	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.109	algorithm design;computer vision;mathematical optimization;kernel;observability;eye tracking;computer science;scale invariance;motion estimation;control theory;mathematics;convolution	Vision	46.194109699538835	-46.59007685769132	179754
ab586ac15e3af49f8230a5f683d470ed3e350677	a robust real-time indoor navigation technique based on gpu-accelerated feature matching	indoor navigation;kernel;dogs;acceleration;graphics processing units;real time systems	Robust feature tracking is a basic requisite for indoor navigation. The Scale Invariant Features Transform (SIFT) features are invariant to image translation, scaling, rotation, and partially invariant to illumination changes. Therefore, it is widely used for image matching based indoor navigation. However, the implementation of the traditional SIFT algorithm needs excessive computation operations, which is relatively time consuming and difficult for indoor navigation like real-time application. This article proposes a SIFT acceleration strategy based on graphics processing unit (GPU). It is proposed as the following four steps. Firstly, Gaussian pyramid is divided into different blocks. And in each GPU block, DoG (Difference of Gaussian) scale-space is computed. Secondly, local keypoints detection is done in GPU, and each keypoint is processed in one block to calculate gradient orientation and magnitude. Thirdly, the keypoint descriptor is formulated as vectors, and each vector is computed in one GPU block. Finally, GPU-accelerated SIFT is introduced into the indoor navigation system to address viewpoint changes. Our main contribution of this article is using an optimized GPU accelerated scheme for real-time indoor navigation. The experiments have proved that such approach can improve the computing efficiency and reduce the chances of mismatches.	algorithm;cuda;computation;computer graphics;difference of gaussians;embedded system;experiment;gaussian blur;gradient;graphics processing unit;image registration;image scaling;mobile device;motion estimation;real-time clock;real-time computing;real-time locating system;scale space;speedup	Jianghua Cheng;Xiangwei Zhu;Wenxia Ding;Gui Gao	2016	2016 International Conference on Indoor Positioning and Indoor Navigation (IPIN)	10.1109/IPIN.2016.7743624	computer vision;simulation;geography;computer graphics (images)	Robotics	41.34532427454196	-51.20960855612945	179788
82b8ed41c7ff6c1fe8f7837eca54925a33d71932	estimating the number of soccer players using simulation-based occlusion handling		Estimating the number of soccer players is crucial information for occupancy analysis and other monitoring activities in sports analysis. It depends on player detection in the field that should be independent of the environment and light conditions. Thermal cameras are therefore a better option over normal RGB cameras. Detection of non-occluded players is doable but precise estimation of number of the players in groups is hard to achieve. Here we propose a novel method for estimating number of the players in groups using computer graphics and virtual simulations. Occlusion conditions are first classified by using distinctive set of features trained by a bagged tree classifier. Estimation of the number of players is then performed by maximum likelihood of probability density based approach to further classify the occluded players. The results show that the implemented strategy is capable of providing precise results even during occlusion conditions.		Noor ul Huda;Kasper Halkjær Jensen;Rikke Gade;Thomas B. Moeslund	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00236	computer vision;probability density function;pattern recognition;maximum likelihood;rgb color model;artificial intelligence;computer science;occlusion;computer graphics	Vision	43.984860287346095	-47.46233202074432	179995
3ece3bbfad6fd4abf0ec62a8789868692bd60a36	estimation of traffic sign visibility considering temporal environmental changes for smart driver assistance	traffic scene traffic sign visibility estimation temporal environmental changes smart driver assistance nuisance free driver assistance systems invehicle camera instantaneous visibility;nuisance free driver assistance systems;road traffic;traffic sign visibility estimation;instantaneous visibility;temporal environmental changes;smart driver assistance;driver circuits visualization cameras estimation videos feature extraction image color analysis;visualization;invehicle camera;estimation;traffic scene;image color analysis;feature extraction;driver circuits;driver information systems;cameras;road traffic driver information systems feature extraction;videos	We propose a visibility estimation method for traffic signs considering temporal environmental changes, as a part of work for the realization of nuisance-free driver assistance systems. Recently, the number of driver assistance systems in a vehicle is increasing. Accordingly, it is becoming important to sort out appropriate information provided from them, because providing too much information may cause driver distraction. To solve such a problem, we focus on a visibility estimation method for controlling the information according to the visibility of a traffic sign. The proposed method sequentially captures a traffic sign by an in-vehicle camera, and estimates its accumulative visibility by integrating a series of instantaneous visibility. By this way, even if the environmental conditions may change temporally and complicatedly, we can still accurately estimate the visibility that the driver perceives in an actual traffic scene. We also investigate the performance of the proposed method and show its effectiveness.	expect;temporal logic;vii	Keisuke Doman;Daisuke Deguchi;Tomokazu Takahashi;Yoshito Mekada;Ichiro Ide;Hiroshi Murase;Yukimasa Tamatsu	2011	2011 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2011.5940467	computer vision;simulation;geography;computer security	Vision	41.890569179318646	-45.281815591299214	180900
8450b0b17668918dd86e7df711e579acbb190606	an algorithm for wipe detection	image sequences video signal processing signal detection statistical analysis;video signal processing;signal detection;video segmentation;camera motion;statistical analysis;compressed domain wipe detection algorithm shot transitions detection video programs video content object motion camera motion statistical information structural information tv programs dc sequence mpeg stream;gunshot detection systems motion detection object detection cameras tv layout detection algorithms data mining streaming media multimedia computing;image sequences	The detection of transitions between shots in video programs is an important first step in analyzing video content. The wipe is a frequently used transitional form between shots. Wipe detection is more involved than the detection of abrupt and other gradual transitions because a wipe may take various patterns and because of the difficulty in discriminating a wipe from object and camera motion. In this paper, we propose an algorithm for detecting wipes using both structural and statistical information. The algorithm can efFectively detect most wipes used in current TV programs. It uses the DC sequence which can be easily extracted from the MPEG stream without full decompression.	algorithm;data compression;digital video;moving picture experts group;sensor;transitional fossil	Min Wu;Wayne H. Wolf;Bede Liu	1998		10.1109/ICIP.1998.723664	computer vision;object-class detection;computer science;mathematics;multimedia;statistics;detection theory;computer graphics (images)	Vision	39.557447345026446	-51.74487130923347	181599
9257c88484247ac19e25c34de2261d34e7a06b41	comal tracking: tracking points at the object boundaries		Traditional point tracking algorithms such as the KLT use local 2D information aggregation for feature detection and tracking, due to which their performance degrades at the object boundaries that separate multiple objects. Recently, CoMaL Features have been proposed that handle such a case. However, they proposed a simple tracking framework where the points are re-detected in each frame and matched. This is inefficient and may also lose many points that are not re-detected in the next frame. We propose a novel tracking algorithm to accurately and efficiently track CoMaL points. For this, the level line segment associated with the CoMaL points is matched to MSER segments in the next frame using shape-based matching and the matches are further filtered using texture-based matching. Experiments show improvements over a simple re-detect-and-match framework as well as KLT in terms of speed/accuracy on different real-world applications, especially at the object boundaries.	algorithm;comal;chamfer;experiment;feature detection (computer vision);feature detection (web development);feature vector;graphics processing unit;maximally stable extremal regions;real-time transcription;solid-state drive	Santhosh K. Ramakrishnan;Swarna Kamlam Ravindran;Anurag Mittal	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.265	computer vision;robustness (computer science);artificial intelligence;feature extraction;pattern recognition;computer science;image segmentation;feature detection;line segment	Vision	41.587641397129325	-51.73758446071694	181964
ced1691cf89c67c37d978c5a09e8ab8afa0f5fa6	object bounding box-critic networks for occlusion-robust object detection in road scene		Object detection in a road scene has received a significant attention from research fields of developing autonomous vehicle and automatic road monitoring systems. However, object occlusion problems frequently occur in generic road scenes. Due to such occlusion problems, previous object detection methods have limitations of not being able to detect objects accurately. In this paper, we propose a novel object detection network which is robust in occlusions. For effective object detection even with occlusion, the proposed network mainly consists of two parts; 1) Object detection framework, 2) Multiple object bounding box (OBB)-Critic network for predicting a BB map which estimates both ob-j ect region and occlusion region. Comprehensive experimental results on a KITTI Vision Benchmark Suite dataset showed that the proposed object detection network outperformed the state-of-the-art methods.	autonomous car;autonomous robot;benchmark (computing);minimum bounding box;object detection	Jung Uk Kim;Jungsu Kwon;Hak Gu Kim;Haesung Lee;Yong Man Ro	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451034	robustness (computer science);computer vision;feature extraction;object detection;artificial intelligence;minimum bounding box;pattern recognition;occlusion;computer science	Vision	42.32791110333952	-45.782967426588485	183081
a80259ed152ed75cd64c2ff35cc330d07447fb31	a pixel-wise local information-based background subtraction approach	pixel gabor filters computational modeling feature extraction lighting mathematical model computer vision;video surveillance;image motion analysis;video surveillance computer vision feature extraction gabor filters gaussian processes image motion analysis image resolution image sequences object detection;image resolution;gaussian processes;spatial feature vector;gabor filters;indexing terms;computer vision;feature vector;gabor filter;gaussian mixture model;computational modeling;feature extraction;source image sequence;pixel;background subtraction;moving object detection;mathematical model;gaussian mixture model moving object detection computer vision video surveillance systems pixel wise local information based background subtraction method gabor filters spatial feature vectors extraction source image sequence;video surveillance systems;spatial feature vectors extraction;lighting;pixel wise local information based background subtraction method;gaussian mixture model background subtraction gabor filters spatial feature vector;object detection;image sequences	Background subtraction is a widely used method for moving object detection in computer vision. It is usually applied in video surveillance systems. There are two major kinds of background subtraction approaches: pixel-based and block based. Yet there are three problems that can not be simultaneously solved by either method: the robustness to illumination changes, the effectiveness in suppressing shadows, and the smoothness of foregroundpsilas boundary. In order to solve these problems, a pixel-wise local information-based background subtraction method is proposed in this paper. In the proposed method, Gabor filters are performed to extract the spatial feature vectors for each pixel from the source image sequence. Then, the spatial feature vectors are modeled by Gaussian Mixture Model, and then moving objects are detected. Experiments show the validity of the proposed method.	background subtraction;closed-circuit television;computer vision;experiment;feature vector;gabor filter;mixture model;object detection;pixel	Zhong Wei;Shuqiang Jiang;Qingming Huang	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607731	computer vision;index term;image resolution;feature vector;background subtraction;feature extraction;computer science;machine learning;pattern recognition;mixture model;mathematical model;lighting;gaussian process;computational model;pixel;statistics	Vision	44.359224424738386	-50.357084092995635	183853
f08508a5368fc467cded864baa0a56146129a589	visual tracking with automatic confident region extraction	intelligent video surveillance;confidence region;mean shift;object tracking;confident region selection;visual tracking	In this work, a novel efficient algorithm for visual object tracking in complex conditions is proposed. The main component of this work includes two parts: Bayesian decision based confident region extraction, and mean shift iteration based tracking. A unique characteristic of the proposed algorithm is that instead of tracking the entire object, the method automatically extracts the confident region of the object through fusing multiple cues in the Bayesian framework. Those cues contain object's color feature, motion character, and dynamic surrounding color information. We tested the performance of the algorithm with video sequences under difficult conditions (complex and dynamic background, fast camera motion, object maneuvering, rotations and partial occlusion) and achieved satisfied results.		Tao Yang;Jing Li;Quan Pan;Yongmei Cheng	2008	Int. J. Image Graphics	10.1142/S0219467808003143	computer vision;simulation;mean-shift;eye tracking;computer science;confidence region;video tracking;pattern recognition;statistics	Vision	44.15336574769319	-48.06329183311273	184352
c2702231485be2d7d8f60a770603f11c0cfd94b5	gesture recognition using depth-based hand tracking for contactless controller application	probability;object tracking;hand weighted probability depth based hand tracking algorithm contactless controller application gesture recognition system depth image;probability gesture recognition object tracking;gesture recognition;gesture recognition target tracking image color analysis wrist hidden markov models histograms	This paper proposes a gesture recognition system capable of providing a contactless controller via depth-based hand tracking. Main proposal is a hand tracking algorithm in depth image by calculating a hand weighted probability. An implementation of the proposed system into a contactless controller application demonstrated its effectiveness.	algorithm;contactless smart card;gesture recognition	Cheoljong Yang;Yujeong Jang;Jounghoon Beh;David K. Han;Hanseok Ko	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161876	computer vision;speech recognition;tracking system;computer science;video tracking;probability;gesture recognition;statistics	Robotics	39.98364017710631	-48.52670229325023	184513
70b507eddf88328f14fef89e5227c65de598a88a	online object tracking via novel adaptive multicue based particle filter framework for video surveillance		Multicue based object tracking frameworks have been extensively explored due to their numerous applications in the field of computer vision. However, the online adaptive fusion of multicue under scale and illumination variations, partial or full occlusion, background clutters and object deformation remains an open challenge problem. In order to address this, we propose an online visual tracking algorithm using adaptive integration of multicue in a particle filter framework. The particle level fusion process is modelled as Shafer’s model with a power set defined over two focal elements. Partial conflicting masses and conjunctive consensus among three cues are estimated for each evaluated particle. Partial conflicts among cues are redistributed using Dezert-Smarandache Theory (DSmT) based proportional conflict redistribution rules (PCR-6). Additionally, context sensitive transductive cues reliabilities are used for discounting the particle likelihoods for quick adaptation of tracker. In the proposed model, ...	closed-circuit television;particle filter	Gurjit Singh Walia;Rajiv Kapoor	2018	International Journal on Artificial Intelligence Tools	10.1142/S0218213018500239	artificial intelligence;computer science;pattern recognition;video tracking;eye tracking;transduction (machine learning);particle filter	Vision	42.89786097352072	-49.00805993727937	184714
e415be1574c2ff7f4512ab7d77ee54b0cd391ff0	a modified klt multiple objects tracking framework based on global segmentation and adaptive template	kanade lucas tomasi;probability;image segmentation;hidden feature removal;probabilistic model;target tracking adaptation models probabilistic logic mathematical model object tracking robustness principal component analysis;multiple objectives;image colour analysis;object tracking;probability hidden feature removal image colour analysis image segmentation image sequences object tracking;multiple object tracking;video sequences modified klt multiple object tracking framework global segmentation adaptive template kanade lucas tomasi tracking framework global pixel level probabilistic model adaptive rgb template model klt tracker partial occlusions merge and split algorithm;image sequences	This paper presents a modified Kanade-Lucas-Tomasi (KLT) tracking framework for multiple objects tracking applications. First, the framework includes a global pixel-level probabilistic model and an adaptive RGB template model to modify traditional KLT tracker more robust to track multiple objects and partial occlusions. Meanwhile, a Merge and Split algorithm is introduced in the proposed framework to track complete occlusions. The advantage of our method is demonstrated on a variety of challenging video sequences.	algorithm;bittorrent tracker;kanade–lucas–tomasi feature tracker;pixel;statistical model;tomasi–kanade factorization	Kang Xue;Patricio A. Vela;Yue Liu;Yongtian Wang	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		statistical model;computer vision;computer science;machine learning;video tracking;pattern recognition;probability;mathematics;image segmentation;statistics	Vision	45.20011152794672	-49.59482428246808	185073
144c9e5fa7dcffd72ddb14a01933987972bdf16d	joint probabilistic techniques for tracking objects using multiple visual cues	probability;target tracking head state estimation filters computer vision robustness switches technological innovation current measurement computed tomography;state estimation;robot vision;optical tracking;visual cues;joint probabilistic data association;autonomous navigation;state estimation optical tracking robot vision probability;snake based tracking object tracking multiple visual cues primary sensor autonomous navigation grasping tasks image based tracking algorithms information sharing disparate state estimation processes visual object joint probabilistic data association robustness distinctiveness occlusion probability tracker deactivation erroneous state estimates color region based tracking	Autonomous robots relying on vision as a primary sensor frequently must identify and track common ob jects such as people and cars in order to successfully perform navigation interaction and grasping tasks These objects comprise many visual parts and attributes yet image based tracking algorithms are of ten keyed to only one of a target s identifying char acteristics In this paper we present a framework for sharing information among disparate state estima tion processes operating on the same underlying visual object Well known techniques for joint probabilistic data association are adapted to yield increased robust ness when multiple trackers attuned to di erent cues such as color and shape are deployed simultaneously The utility of each cue varies according to image con ditions necessitating adaptation in the weighting of the various methods to avoid bias This is achieved by formulating a measure of tracker con dence based on distinctiveness and occlusion probability which per mits deactivating trackers before erroneous state esti mates adversely a ect the ensemble We will discuss experiments using color region based tracking in tan dem with snake tracking that demonstrate the e cacy	algorithm;correspondence problem;earthbound;experiment;naruto shippuden: clash of ninja revolution 3;os-tan;robot;timation	Christopher Rasmussen;Gregory D. Hager	1998		10.1109/IROS.1998.724618	computer vision;simulation;sensory cue;tracking system;probability;statistics	Robotics	46.14350594116477	-46.82304677089524	185184
136b25fdbb9c0a83fb929bdbacaac87b53ea023b	real-time detection of children's skin on social networking sites using markov random field modelling	mrf models;skin models;forensics;child exploitation	Social networking sites are increasingly being used as the source for paedophiles to search for, download and exchange child exploitation images. Law Enforcement Agencies (LEAs) around the world face a difficult challenge to combat technologically-savvy paedophiles. In this paper, we propose a framework for detecting images containing children's pictures in different poses, with the ultimate view of identifying and classifying images as corresponding to the COPINE scale. To achieve the goal of automatic detection, we present a novel stochastic vision model based on a Markov Random Fields (MRF) prior, which will employ a skin model and human affine-invariant geometric descriptor to detect and identify skin regions containing pornographic contexts.	markov chain;markov random field;real-time transcription	Mofakharul Islam;Paul A. Watters;John Yearwood	2011	Inf. Sec. Techn. Report	10.1016/j.istr.2011.09.004	computer vision;simulation;data mining;forensic science;computer security	ML	40.841742546354	-47.05886254549774	185386
0ee3a5b975917173107b4b571d3ee82368957eab	efficient and robust motion segmentation via adaptive similarity metric		This paper introduces an efficient and robust method that segments long motion capture data into distinct behaviors. The method is unsupervised, and is fully automatic. We first apply spectral clustering on motion affinity matrix to get a rough segmentation. We combined two statistical filters to remove the noises and get a good initial guess on the cut points as well as on the number of segments. Then, we analyzed joint usage information within each rough segment and recomputed an adaptive affinity matrix for the motion. Applying spectral clustering again on this adaptive affinity matrix produced a robust and accurate segmentation compared with the ground-truth. The experiments showed that the proposed approach outperformed the available methods on the CMU Mocap database.	affinity analysis;cluster analysis;euler;experiment;motion capture;processor affinity;spectral clustering;unsupervised learning	Xiaoyan Hu;Shunbo Xie	2017		10.1145/3095140.3095174	spectral clustering;computer science;computer vision;motion capture;matrix (mathematics);pattern recognition;segmentation;artificial intelligence	Vision	44.09692543282089	-51.4075816529712	185953
6ab066896eebc26d55ed9643bea1582367f90171	application of dynamic distributional clauses for multi-hypothesis initialization in model-based object tracking	cameras solid modeling object tracking three dimensional displays random variables particle filters robots;distributional clauses object tracking robot vision particle filters	In this position paper we propose the use of the Distributional Clauses Particle Filter in conjunction with a model-based 3D object tracking method in monocular camera sequences. We describe the model based object tracking method that is based on contour and edge features for 3D pose relative estimation. We also describe the application of the Distributional Clauses Particle Filter that takes into account inputs from object tracking. We argue that objects' dynamics can be modeled via probabilistic rules, which makes possible to predict and utilise a pose hypothesis space for fully occluded or ‘invisible’ (hidden-away) objects that may re-appear in the camera field of view. Important issues, such as losing track of the object in a ‘total occlusion’ scenario, are discussed.	particle filter	Davide Nitti;Georgios Chliveros;Maria Pateraki;Luc De Raedt;Emmanouil Hourdakis;Panos E. Trahanias	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004654002560261	computer vision;simulation;computer science;communication	Vision	46.186801496543566	-48.90970153436784	186480
d27898f0839ca09b108fda6518f27467577c9865	long-term reliable visual tracking with uavs		In the paper, we propose an effective long-term real-time tracking method to address the problem of robustness and tracking failure in visual tracking with UAVs. Most existing trackers only consider short-term tracking, therefore are unable to cope with partial and complete occlusion, which finally leads to object drifting or loss. Our method still follows the tracking-by-detection framework. However, after choosing kernelized correlation filter as the tracker baseline, we introduce the confidence of candidate patches to measure tracking reliability, and trigger redetection process with random forest and learned object model when needed. We further improve object update strategy to make the object model with memory more robust against object drift. Extensive experiment results on UAV videos show that our algorithm performs better than widely used TLD, KCF, and LCT methods.	algorithm;baseline (configuration management);feature selection;kernel method;linear canonical transformation;random forest;real-time clock;unmanned aerial vehicle;video tracking	Zhenshen Qu;Xiao Lv;Junyu Liu;Li Jiang;Liang Liang;Weinan Xie	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122912	machine learning;robustness (computer science);feature extraction;object model;computer vision;computer science;random forest;eye tracking;bittorrent tracker;artificial intelligence	Robotics	42.19189766878906	-48.38020392389556	186939
dd4e100569a7370ff953d522b1fa4053eb7ef670	is it safe to change the lane? — visual exploration of adjacent lanes for autonomous driving	image processing;german highways adjacent lane exploration autonomous driving multilane roads radar based system vision based method pattern recognition strategy image categorization side mirror mounted fish eye camera appearance based method video sequence;detection and identification;computer vision;video signal processing computer vision image sequences object recognition road safety traffic engineering computing;lane occupancy;feature extraction;tires vehicles feature extraction cameras rain optical imaging sun;tires;lane changing;autonomous vehicle guidance	Lane changes on multi-lane roads are an important and complex task for autonomous driving because the system has to be sure that the adjacent lane is not occupied by any other object. Existing radar-based systems can be complemented by vision-based methods to increase their reliability. This work presents new methods based on multiple pattern recognition strategies, such as image categorization, applied to serially-produced, side-mirror mounted fish-eye cameras. The focus is on appearance-based methods, such as tire detection and structure analysis, and motion-based methods, such as optical flow. Extensive experiments evaluate all presented methods on long video sequences on German highways. The proposed approach is shown to be effective for all kinds of vehicles, all relevant situations, and under varying weather conditions.	angularjs;autonomous car;c++;categorization;experiment;optical flow;pattern recognition;radar;real-time clock;real-time computing;sensor	Björn Fröhlich;Julian Bock;Uwe Franke	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6958059	computer vision;simulation;geography;cartography	Robotics	42.01191705435065	-45.22940228613761	187271
1f0b47f1e83f68f16f3583ffda488975b1659dea	panoramic vision and laser range finder fusion for multiple person tracking	range data;gaussian processes;robot vision gaussian processes laser ranging;laser ranging;indexing terms;laser fusion target tracking particle tracking laser modes particle filters robot vision systems gaussian processes systems engineering and theory mobile robots cameras;colour information multiple person tracking laser range finder fusion panoramic vision stationary robot particle filters gaussians background subtraction algorithm;laser range finder multiple person tracking panoramic vision;robot vision;laser range finder;panoramic vision;particle filter;multiple person tracking;background subtraction;mixture of gaussians	This paper describes a fusion of panoramic vision and laser range data to track multiple persons simultaneously from a stationary robot. Particle filters are used to track people in the plane of the laser and a mixture of Gaussians background subtraction algorithm is used to maintain a colour model for each person being tracked. Colour information is used to recognize lost targets that have reentered the scene	algorithm;background subtraction;mixture model;particle filter;stationary process	Punarjay Chakravarty;Ray A. Jarvis	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282149	computer vision;simulation;index term;background subtraction;particle filter;computer science;mixture model;gaussian process;optics;statistics	Robotics	46.158992562387404	-46.8375144312227	187892
3a39e06da4ef59cd7b4acc8ab1fcc9f9ed00aa03	sequential reliable-inference for rapid detection of human actions	reliability engineering;legged locomotion;surveillance;humans legged locomotion robot vision systems cameras computer science reliability engineering surveillance video recording mobile robots robotics and automation;mobile robots;low latency;video recording;humans;computer science;robot vision systems;robotics and automation;cameras	We present a probabilistic reliable-inference framework to address the issue of rapid-and-reliable detection of human actions. The approach determines the shortest video exposure needed for low-latency recognition by sequentially evaluating a series of posterior class ratios to find the earliest reliable decision point. Results are presented for a set of people walking, running, and standing at different styles and multiple viewpoints, and compared to an alternative ML approach.	maximal set;rs-232;statistical classification	James W. Davis	2004	2004 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPR.2004.435	mobile robot;embedded system;computer vision;simulation;computer science;low latency	Vision	45.278364573083074	-45.256978733344404	188408
f5e6ff91cf49fac4dbbc23e01c10b6b27a9a15e2	a novel ant colony detection using multi-region histogram for object tracking		Efficient object tracking become more popular in video processing domain. In recent years, many researchers have developed excellent models and methods for complicated tracking problems in real environment. Among those approaches, object feature definition is one of the most important component to obtain better accuracy in tracking. In this paper, we propose a novel multi-region feature selection method which defines histogram values of basic areas and random areas (MRH) and combined with continuous ant colony filter detection to represent the original target. The proposed approach also achieves smooth tracking on different video sequences, especially with Motion Blur problem. This approach is designed and tested in MATLAB 2016b environment. The experiment result demonstrates better and faster tracking performance and shows continuous tracking trajectory and competitive outcomes regarding to traditional methods.	ant colony	Seid Miad Zandavi;Feng Sha;Vera Chung;Zhicheng Lu;Weiming Zhi	2017		10.1007/978-3-319-70090-8_3	motion blur;ant colony;artificial intelligence;computer science;video tracking;pattern recognition;feature selection;trajectory;histogram;video processing	Vision	42.463560876267344	-48.03178384104312	188572
1af8b9efe40cb85fd48fb466545141a5e102796b	performance assessment of morphological dynamic link architecture under optimal and real operating conditions	access control;operant conditioning	In this paper, the performance of morphological dynamic link architecture (MDLA) is assessed under optimal and real operating conditions. It is shown that MDLA achieves a very low equal error rate on the extended M2VTS database which contains 295 persons’ video data in 8 shots recorded under optimal conditions. However, its performance severely deteriorates, when it is applied to the IBERMATICA database that has been recorded under conditions simulating a real access-control to an automatic teller machine. The compensation for the image variations attributed to the variable recording conditions, i.e., changes in illumination, face size differences and varying face position, is addressed next. The use of simple and powerful pre-processing techniques aiming at compensating for the aforementioned variations prior to the application of MDLA is proposed. The first results obtained indicate that the proposed approach overcomes the image variations and stabilizes the performance of MDLA.	illumination (image);preprocessor;simulation	Constantine Kotropoulos;Anastasios Tefas;Ioannis Pitas;Cristina Fernández;F. Fernández	1999			access control;architecture;word error rate;control engineering;computer science	Vision	41.40949640030402	-49.41015164116579	188872
ae37057a8a3d3a852e022ba865ed57c26f20c900	visual-based spatiotemporal analysis for nighttime vehicle braking event detection	brake light detection;spatiotemporal analysis	In this paper, we propose a novel visual-based approach that can detect brake lights at night by analyzing the tail lights based on the theedimensional Nakagami imaging which can provide robust information of brake lights. Instead of using the knowledge of the heuristic features, such as symmetry and position of rear facing vehicle, size and so forth, we focus on extracting the invariant features based on modeling the scattering of brake lights and therefore can conduct the detection process in a part-based manner. Experiment from extensive dataset shows that our proposed system can effectively detect vehicle braking under different lighting and traffic conditions, and thus prove its feasibility in real-world environments.	heuristic	Duan-Yu Chen;Chia-Hsun Chen	2012		10.1007/978-3-642-27355-1_81	computer vision;simulation;computer science	AI	41.75887200719573	-45.4679222408852	189383
8b74e1749ed60395082b0eb50d7bd1ab5f93f031	pedestrian registration in static images with unconstrained background	human body registration;image matching;statistical model;feature extraction;human body;statistical modeling;similarity measure	This paper introduces a human body contour registration method for static pedestrian images with unconstrained background. By using a statistical compound model to impose structural and textural constraints on valid pedestrian appearances, the matching process is robust to image clutter. Experimental results show that the proposed method register pedestrian contours in complex background effectively.	clutter;emoticon;image registration	Lixin Fan;Kah Kay Sung;Teck Khim Ng	2003	Pattern Recognition	10.1016/S0031-3203(02)00126-7	statistical model;computer vision;speech recognition;computer science;pattern recognition;mathematics;statistics	Vision	44.56723114141069	-51.740882445574925	189492
1d174cd9686bdeced964197385aab0d59389d44f	a new visual object tracking algorithm using bayesian kalman filter	object tracking visualization noise complexity theory target tracking probabilistic logic bayes methods;mean shift algorithm visual object tracking algorithm bayesian kalman filter simplified gaussian mixture;object tracking gaussian processes kalman filters mixture models;mean shift object tracking baysian kalman filter	This paper proposes a new visual object tracking algorithm using a novel Bayesian Kalman filter (BKF) with simplified Gaussian mixture (BKF-SGM). The new BKF-SGM employs a GM representation of the state and noise densities and a novel direct density simplifying algorithm for avoiding the exponential complexity growth of conventional KFs using GM. Together with an improved mean shift (MS) algorithm, a new BKF-SGM with improved MS (BKF-SGM-IMS) algorithm with more robust tracking performance is also proposed. Experimental results show that our method can successfully handle complex scenarios with good performance and low arithmetic complexity.	algorithm;image noise;kalman filter;mean shift;second generation multiplex;time complexity;visual objects	Xueying Zhang;Shing-Chow Chan;Bin Liao;Kai Man Tsui	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865187	computer vision;invariant extended kalman filter;ensemble kalman filter;bayesian programming;fast kalman filter;machine learning;pattern recognition;extended kalman filter;moving horizon estimation	Robotics	45.61157657218348	-48.94953440886312	189880
4fc98b8465c6442e073313cd219d416c660690bb	real-time dynamic texture recognition using random sampling and dimension reduction	infrared videos real time dynamic texture video recognition random sampling dimension reduction random hyperplanes deep neural network filters spatio temporal blocks feature extraction local binary patterns lbp computational cost reduction feature vectors dynamic texture database real time flame detection;deep neural networks dynamic texture local binary patterns real time random hyperplanes;video signal processing feature extraction image filtering image sampling image texture infrared imaging neural nets random processes;feature extraction videos neural networks real time systems training databases standards	In this paper, we propose a real-time dynamic texture recognition method using projections onto random hyperplanes and deep neural network filters. We divide dynamic texture videos into spatio-temporal blocks and extract features using local binary patterns (LBP). We reduce the computational cost of the exhaustive LBP method by using randomly sampled subset of pixels in a given spatio-temporal block. We use random hyperplanes and deep neural network filters to reduce the dimensionality of the final feature vectors. We test the performance of the proposed method in a dynamic texture database. We also propose an application of the proposed method to real-time detection of flames in infrared videos. We observe that the approach based on random hyperplanes produces the best results.	algorithmic efficiency;artificial neural network;belief propagation;computation;computer vision;deep learning;dimensionality reduction;feature vector;local binary patterns;pixel;randomness;real-time clock;sampling (signal processing)	Osman Günay;A. Enis Çetin	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351371	image texture;computer vision;computer science;machine learning;pattern recognition;texture compression	Robotics	40.59656075063309	-50.161462286528625	190155
90cbae78105e8fceb50e722c3ebc768af627b1fa	abort and retry in grasping	grasping;edge detection;probabilistic model;image edge detection;image color analysis;robots;markov processes image color analysis robots grasping probabilistic logic tin image edge detection;markov process;markov processes;probabilistic logic;tin;markov chain	Iteration is often sufficient for a simple hand to accomplish complex tasks, at the cost of an increase in the expected time to completion. In this paper, we minimize that overhead time by allowing a simple hand to abort early and retry as soon as it realizes that the task is likely to fail. We present two key contributions. First, we learn a probabilistic model of the relationship between the likelihood of success of a grasp and its grasp signature—the trace of the state of the hand along the entire grasp motion. Second, we model the iterative process of early abort and retry as a Markov chain and optimize the expected time to completion of the grasping task by effectively thresholding the likelihood of success. Experiments with our simple hand prototype tasked with grasping and singulating parts from a bin show that early abort and retry significantly increases efficiency.	average-case complexity;iteration;markov chain;overhead (computing);prototype;retry;statistical model;thresholding (image processing)	Alberto Rodriguez;Matthew T. Mason;Siddhartha S. Srinivasa;Matthew Bernstein;Alex Zirbel	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6095100	simulation;computer science;artificial intelligence;machine learning;markov process;statistics	Robotics	45.05659601105886	-49.166257452547754	190381
d63a36d0b2a0141ad03a5de989d871f8d5c9c40e	event detection on motion activities using a dynamic grid	event detection;event patterns;pedestrian activities;dynamic grid feature	Event detection based on using features from a static grid can give poor results from the viewpoint of two main aspects: the position of the camera and the position of the event that is occurring in the scene. The former causes problems when training and test events are at different distances from the camera to the actual position of the event. The latter can be a source of problems when training events take place in any position in the scene, and the test events take place in a position different from the training events. Both issues degrade the accuracy of the static grid method. Therefore, this work proposes a method called a dynamic grid for event detection, which can tackle both aspects of the problem. In our experiment, we used the dynamic grid method to detect four types of event patterns: implosion, explosion, two-way, and one-way using a Multimedia Analysis and Discovery (MAD) pedestrian dataset. The experimental results show that the proposed method can detect the four types of event patterns with high accuracy. Additionally, the performance of the proposed method is better than the static grid method and the proposed method achieves higher accuracy than the previous method regarding the aforementioned aspects.	adaptive mesh refinement;mad;one-way function	Jitdumrong Preechasuk;Punpiti Piamsa-nga	2015	JIPS	10.3745/JIPS.02.0035	real-time computing;simulation;computer science;data mining	Robotics	42.78263046310764	-46.4935034208903	190798
1d55ad6fc60a0ae54dffac6bdd512792b62023ff	detecting moving objects, ghosts, and shadows in video streams	reactivity to changes;moving object;background modeling;video surveillance;image motion analysis;video streaming;image segmentation;shadow detection;journal article;object segmentation;object level knowledge;color segmentation;color segmentation moving object detection ghosts video streams background subtraction methods traffic monitoring human motion capture video surveillance statistical assumptions object level knowledge apparent objects pixel processing object based selective update color information shadow detection object segmentation background update background modeling;human motion;background subtraction;moving object detection;object detection streaming media shape video surveillance layout traffic control object segmentation application software us department of transportation monitoring;traffic monitoring;object detection;image segmentation image motion analysis object detection	Background subtraction methods are widely exploited for moving object detection in videos in many applications, such as traffic monitoring, human motion capture and video surveillance. How to correctly and efficiently model and update the background model and how to deal with shadows are two of the most distinguishing and challenging aspects of such approaches. This work proposes a general-purpose method which combines statistical assumptions with the object-level knowledge of moving objects, apparent objects (ghosts) and shadows acquired in the processing of the previous frames. Pixels belonging to moving objects, ghosts and shadows are processed differently in order to supply an object-based selective update. The proposed approach exploits color information for both background subtraction and shadow detection to improve object segmentation and background update. The approach proves fast, flexible and precise in terms of both pixel accuracy and reactivity to background changes.	background subtraction;closed-circuit television;general-purpose modeling;kinesiology;motion capture;object detection;object-based language;pixel;sensor;streaming media	Rita Cucchiara;Costantino Grana;Massimo Piccardi;Andrea Prati	2003	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2003.1233909	computer vision;background subtraction;computer science;multimedia;image segmentation;computer graphics (images)	Vision	43.59180697660088	-46.950556621952565	191151
1dec17603dff4e230f35045941c266ee2962c78e	assessing temporal coherence for posture classification with large occlusions	histograms;system reliability;surveillance;hidden markov model;kalman filters;biological system modeling;probabilistic model;hidden markov models;shape;coherence;robustness;humans;temporal coherence;hidden markov models biological system modeling humans coherence histograms kalman filters surveillance shape robustness cameras;cameras	In this paper we present a people posture classification approach especially devoted to cope with occlusions. In particular, the approach aims at assessing temporal coherence of visual data over probabilistic models. A mixed predictive and probabilistic tracking is proposed: a probabilistic tracking maintains along time the actual appearance of detected people and evaluates the occlusion probability; an additional tracking with Kalman prediction improves the estimation of the people position inside the room. Probabilistic Projection Maps (PPMs) created with a learning phase are matched against the appearance mask of the track. Finally, an Hidden Markov Model formulation of the posture corrects the frame-by-frame classification uncertainties and makes the system reliable even in presence of occlusions. Results obtained over real indoor sequences are discussed.	coherence (physics);hidden markov model;kalman filter;markov chain;poor posture	Rita Cucchiara;Roberto Vezzani	2005	2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1	10.1109/ACVMOT.2005.22	kalman filter;statistical model;computer vision;simulation;coherence;shape;computer science;machine learning;pattern recognition;histogram;hidden markov model;statistics;robustness	Vision	44.850530574208264	-47.19269871248093	191201
15b9cdc59dfa70a0cec469ab6414d09064f8552c	the extension of statistical face detection to face tracking	filtering;image motion analysis;optical filters;skin;face tracking;face detection target tracking particle tracking skin optical filters predictive models real time systems image motion analysis videos filtering;predictive models;particle tracking;target tracking;face detection;videos;real time systems	A real time probablistic face tracking system using monocular vision is presented based on face target acquisition and subsequent particle filtering techniques. First, the face target acquisition and initialization stage used a skin color classification and statistical face model matching apprroach to find the face target. Subsequently, the particle filtering technique is used to track the state space of face movements. And finally, the optical flow information was used to find motion information for sample redistrbution. The system places emphasis on the automatic face target initialzation stage, which has been assumed to be solved or labled manually in most other face detection and instillation stage is executed in less than 250 msec and the subsequent face tracking stage functions at 30 fps comfortably with 160x120 pixel resolution live videos.	computer display standard;face detection;facial motion capture;optical flow;particle filter;pixel;state space;tracking system	Haisheng Wu;John S. Zelek	2004	First Canadian Conference on Computer and Robot Vision, 2004. Proceedings.	10.1109/CCCRV.2004.1301415	filter;computer vision;face detection;facial motion capture;simulation;object-class detection;tracking system;computer science;optical filter;predictive modelling;skin;computer graphics (images)	Vision	44.53223088315012	-45.92553176268152	192054
ca35464685ae79b288d23008767397fe259fddfb	a likelihood-based background model for real time processing of color filter array videos		One of the first tasks executed by a vision system made of fixed cameras is the background (BG) subtraction and a particularly challenging context for real time applications is the athletic one because of illumination changes, moving objects and cluttered scenes. The aim of this work is to extract a BG model based on statistical likelihood able to process color filter array (CFA) images taking into account the intrin- sic variance of each gray level of the sensor, named Likelihood Bayer Background (LBB). The BG model should be not so computationally complex while highly responsive to extract a robust foreground. More- over, the mathematical operations used in the formulation should be parallelizable, working on image patches, and computationally efficient, exploiting the dynamics of a pixel within its integer range. Both simu- lations and experiments on real video sequences demonstrate that this BG model approach shows great performances and robustness during the real time processing of scenes extracted from a soccer match.	color filter array	Vito Renó;Roberto Marani;Nicola Mosca;Massimiliano Nitti;Tiziana D'Orazio;Ettore Stella	2015		10.1007/978-3-319-23222-5_27	computer vision;simulation;background subtraction;computer graphics (images)	Robotics	44.227026914944375	-47.64866549256741	192384
0b84130957188006a3fdc7ced44b47644f2f9f47	efficient coarser-to-fine holistic traffic sign detection for occlusion handling				Yawar Rehman;Jameel Ahmed Khan;Hyunchul Shin	2018	IET Image Processing	10.1049/iet-ipr.2018.5424	computer vision;artificial intelligence;mathematics;pattern recognition;occlusion	Vision	41.23084899779643	-46.79758327631709	192531
2ae7ee2140581dfce7cf032e137ef6d71a042db8	adaptive mixture observation models for multiple object tracking	motion tracking;configuration space;particle filter;object tracking;feature selection;mixture tracker;multiple object tracking;high efficiency;cui peng sun lifeng yang shiqiang 多目标跟踪 粒子过滤器 混合物追踪 运动跟踪 算法 adaptive mixture observation models for multiple object tracking;online feature selection	Multiple object tracking (MOT) poses many difficulties to conventional well-studied single object tracking (SOT) algorithms, such as severe expansion of configuration space, high complexity of motion conditions, and visual ambiguities among nearby targets, among which the visual ambiguity problem is the central challenge. In this paper, we address this problem by embedding adaptive mixture observation models (AMOM) into a mixture tracker which is implemented in Particle Filter framework. In AMOM, the extracted multiple features for appearance description are combined according to their discriminative power between ambiguity prone objects, where the discriminability of features are evaluated by online entropy-based feature selection techniques. The induction of AMOM can help to surmount the incapability of conventional mixture tracker in handling object occlusions, and meanwhile retain its merits of flexibility and high efficiency. The final experiments show significant improvement in MOT scenarios compared with other methods.	algorithm;bittorrent tracker;experiment;feature selection;particle filter	Peng Cui;Lifeng Sun;Shiqiang Yang	2009	Science in China Series F: Information Sciences	10.1007/s11432-009-0054-4	configuration space;computer vision;particle filter;computer science;machine learning;video tracking;pattern recognition;mathematics;feature selection	Vision	43.36977674781696	-48.47540029146523	192765
2fd15306befb7ecba087239c5168f0bcd6b61d6b	analysis and compression of facial animation parameter set (faps)	facial expression recognition;facial animation financial advantage program principal component analysis recurrent neural networks computational complexity rendering computer graphics hidden markov models covariance matrix face recognition image sampling;image coding;data compression;data representation;face recognition;compact representation;computational complexity;data structures;principal component analysis;facial animation;sampling rate facial animation parameter set principal component analysis facial expression recognition algorithm recurrent neural network data representation computational complexity;data compression face recognition recurrent neural nets computer animation data structures computational complexity image coding;recurrent neural nets;recurrent neural network;computer animation	In this paper, a new representation of FAPs based on principal component analysis is proposed. Based on this compact representation, a FAPs compression scheme is designed. A facial expression recognition algorithm using recurrent neural network is also investigated. The inputs to the network are the most significant components of this new data representation. Experimental results show that computational complexity is reduced and expressions can be correctly recognized even with changed sampling rate.		Hai Tao;Homer H. Chen;Thomas S. Huang	1997		10.1109/MMSP.1997.602643	data compression;facial recognition system;computer vision;computer facial animation;data structure;computer science;recurrent neural network;machine learning;pattern recognition;external data representation;computer animation;computational complexity theory;principal component analysis	Graphics	39.752977208031844	-49.849435364892834	192809
3af53b6f2eb6c8ff4783f19bbc35ca0f1dcd5782	timing mark detection on nuclear detonation video	detectors;detection algorithms;testing;timing explosions detection algorithms transforms testing detectors;video signal processing detonation nuclear explosions object detection;transforms;explosions;washed out videos timing mark detection nuclear detonation video 3d video occluded videos;timing	During the 1950s and 1960s the United States conducted and filmed over 200 atmospheric nuclear tests establishing the foundations of atmospheric nuclear detonation behavior. Each explosion was documented with about 20 videos from three or four points of view. Synthesizing the videos into a 3D video will improve yield estimates and reduce error factors. The videos were captured at a nominal 2500 frames per second, but range from 2300-3100 frames per second during operation. In order to combine them into one 3D video, individual video frames need to be correlated in time with each other. When the videos were captured a timing system was used that shined light in a video every 5 milliseconds creating a small circle exposed in the frame. This paper investigates several method of extracting the timing from images in the cases when the timing marks are occluded and washed out, as well as when the films are exposed as expected. Results show an improvement over past techniques. For normal videos, occluded videos, and washed out videos, timing is detected with 99.3%, 77.3%, and 88.6% probability with a 2.6%, 11.3%, 5.9% false alarm rate, respectively.	video	Daniel T. Schmitt;Gilbert L. Peterson	2014	2014 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)	10.1109/AIPR.2014.7041902	computer vision;simulation;engineering;computer graphics (images)	Vision	43.878619818427254	-45.24557919816558	192947
871ea2608a31b22b0d92e76955a4ceecf5b694dd	visual object tracking via one-class svm	one class svm;object tracking;tracking sample set	In this paper, we propose a new visual object tracking approach via one-class SVM (OC-SVM), inspired by the fact that OC-SVM’s support vectors can form a hyper-sphere, whose center can be regarded as a robust object estimation from samples. In the tracking approach, a set of tracking samples are constructed in a predefined searching window of a video frame. And then a threshold strategy is proposed to select examples from the tracking sample set. Selected examples are used to train an OC-SVM model which estimates a hyper-sphere encircling most of the examples. Finally, we locate the center of the hyper sphere as the tracked object in the current frame. Extensive experiments demonstrate the effectiveness and robustness of the proposed approach in complex background.	alexander horned sphere;experiment	Li Li;Zhenjun Han;Qixiang Ye;Jianbin Jiao	2010		10.1007/978-3-642-22822-3_22	computer vision;simulation;computer science;machine learning;video tracking	Vision	42.50127384096185	-48.832679588362694	193259
e5cb67852f4542ec7153f06287e03299cc73d5c4	probabilistic background model by density forests for tracking		This paper proposes an approach for a robust tracking method to the objects intersection with appearances similar to a target object. The target is image sequences taken by a moving camera in this paper. Tracking methods using color information tend to track mistakenly a background region or an object with color similar to the target object since the proposed method is based on the particle filter. The method constructs the probabilistic background model by the histogram of the optical flow and defines the likelihood function so that the likelihood in the region of the target object may become large. This leads to increasing the accuracy of tracking. The probabilistic background model is made by the density forests. It can infer a probabilistic density fast. The proposed method can process faster than the authors' previous approach by introducing the density forests. Results are demonstrated by experiments using the real videos of outdoor scenes.		Daimu Oiwa;Shinji Fukui;Yuji Iwahori;Tsuyoshi Nakamura;Boonserm Kijsirikul;Manas Kamal Bhuyan	2017	IJSI	10.4018/IJSI.2017040101	computer vision;econometrics;machine learning	Vision	45.09595894043147	-49.99256116686882	193283
b5ef9846d98a020bebddd05cbe5dfc4800d7e6aa	pedestrian detection in surveillance videos based on cs-lbp feature	qa75 electronic computers computer science szamitastechnika;detectors;histograms;video surveillance;szamitogeptudomany;surveillance;intelligent transportation systems;pedestrian detection video surveillance;computational modeling;video surveillance feature extraction image colour analysis image segmentation image texture intelligent transportation systems object detection pedestrians;feature extraction;pedestrian detection;surveillance videos pedestrian detection cs lbp feature object categories detection image content video content computer vision intelligent transportation systems its features extraction multiscale center symmetric local binary pattern feature gradient information texture information scale information foreground segmentation method caviar sequences support vector machines;feature extraction computational modeling detectors videos histograms surveillance intelligent transportation systems;videos	Detecting different categories of objects in an image and video content is one of the fundamental tasks in computer vision research. Pedestrian detection is a hot research topic, with several applications including robotics, surveillance and automotive safety. Pedestrians are key participants in transportation systems, so pedestrian detection in video surveillance systems is of great significance to the research and application of Intelligent Transportation Systems (ITS). Pedestrian detection is a challenging problem due to the variance of illumination, color, scale, pose, and so forth. Extraction of effictive features is a key to this task. In this work, we present the multi-scale Center-symmetric Local Binary Pattern feature for pedestrian detection. The proposed feature captures gradient information and some texture and scale information. We completed the detection task with a foreground segmentation method. Experiments on CAVIAR sequences show that the proposed feature with support vector machines can detect pedestrians in real-time effectively in surveillance videos.	cs games;closed-circuit television;computer vision;digital video;experiment;gradient descent;local binary patterns;pedestrian detection;real-time clock;robotics;support vector machine	Domonkos Varga;László Havasi;Tamás Szirányi	2015	2015 International Conference on Models and Technologies for Intelligent Transportation Systems (MT-ITS)	10.1109/MTITS.2015.7223288	computer vision;intelligent transportation system;detector;feature detection;simulation;object-class detection;feature extraction;computer science;machine learning;pattern recognition;histogram;computational model;feature;statistics	Robotics	39.86087886435222	-47.15649779631088	194074
6cba4f69763fb10e742cfd9d4a7610e2beeb1dfe	alignment of deep features in 3d models for camera pose estimation		Using a set of semantically annotated RGB-D images with known camera poses, many existing 3D reconstruction algorithms can integrate these images into a single 3D model of the scene. The semantically annotated scene model facilitates the construction of a video surveillance system using a moving camera if we can efficiently compute the depth maps of the captured images and estimate the poses of the camera. The proposed model-based video surveillance consists of two phases, i.e. the modeling phase and the inspection phase. In the modeling phase, we carefully calibrate the parameters of the camera that captures the multi-view video for modeling the target 3D scene. However, in the inspection phase, the camera pose parameters and the depth maps of the captured RGB images are often unknown or noisy when we use a moving camera to inspect the completeness of the object. In this paper, the 3D model is first transformed into a colored point cloud, which is then indexed by clustering—with each cluster representing a surface fragment of the scene. The clustering results are then used to train a model-specific convolution neural network (CNN) that annotates each pixel of an input RGB image with a correct fragment class. The prestored camera parameters and depth information of fragment classes are then fused together to estimate the depth map and the camera pose of the current input RGB image. The experimental results show that the proposed approach outperforms the compared methods in terms of the accuracy of camera pose estimation.		Jui-Yuan Su;Shyi-Chyi Cheng;Chin-Chun Chang;Jun-Wei Hsieh	2019		10.1007/978-3-030-05716-9_36	convolutional neural network;pixel;point cloud;3d reconstruction;computer science;pattern recognition;artificial intelligence;computer vision;pose;cluster analysis;rgb color model;depth map	Vision	42.7526072155336	-51.14822171502835	194816
4f322fff7b614a9108f50d629bd18fe31ac04774	investigation on parallel computing techniques for multiple-image matching ammgc model	parallel computing;sensor technology;matching procedure;image matching;ammgc;image space based ammgc multiple image matching model;multiple image matching ammgc model;ammgc multiple image matching parallel;data mining;data partitioning;multiple image;arrays;reliable matching;computational modeling;parallel algorithms image matching;matching;parallel;parallel computer;parallel processing concurrent computing image matching image sensors grid computing remote sensing partitioning algorithms image analysis solid modeling image resolution;mathematical model;image grid point;correlation;reliable matching parallel computing multiple image matching ammgc model sensor technology image space based ammgc multiple image matching model image grid point image data partition matching procedure;image data partition;parallel processing;parallel algorithms	Stimulated by sensor technology, image matching techniques are greatly innovated towards multiple-image matching. In this paper, an image-space-based AMMGC multiple-image matching model is introduced. However, AMMGC model is quite complex and involves massive computing amount, especially for dense image grid point. So, two parallel computing methods are analyzed comprehensively from the point of average image data partition. The methods can be greatly integrated with AMMGC model to provide an easy matching procedure. Experimental results prove that AMMGC model has quite reliable matching quality, and can be greatly combined with parallel computing techniques to reduce matching time, and at the same time, enormously improve the matching speed and scale.	facial recognition system;image registration;parallel computing	Chen-guang Dai;Song Ji;Yongsheng Zhang	2009	2009 Eighth International Conference on Grid and Cooperative Computing	10.1109/GCC.2009.70	matching;parallel processing;computer vision;computer science;sensor;theoretical computer science;machine learning;parallel;mathematical model;parallel algorithm;computational model;correlation	HPC	41.57584377060785	-51.55926636272691	194892
ce7468240793db7e483e30e8284036d240a2b518	real time multiple people tracking and pose estimation	real time;multiple people tracking;particle filter;particle filtering;people tracking;probability estimation;human pose estimation;pose estimation	In this paper we present a combined probability estimation approach to detect and track multiple people for pose estimation at the same time. It can deal with partial and total occlusion between persons by adding torso appearance to the tracker. Moreover, the upper body of each individual is further segmented into head, torso, upper arm and lower arm in a hierarchical way. The simplicity of the feature and the simplified model allow close real time performance of the tracker. The experimental results show that the proposed method can deal with most of the inner-occlusion between persons, as well as certain self-occlusion. It's also much faster than the existing methods with comparable accuracy.	3d pose estimation;hidden surface determination	Feifei Huo;Emile A. Hendriks	2010		10.1145/1878039.1878042	computer vision;simulation;3d pose estimation;geography;control theory;articulated body pose estimation	Vision	44.62665235788964	-46.77583825043118	195756
c2688532f064fb4b84c5fd6896b3ad94acf74cd5	tracking with the support of couplers and historical models	clutter;discriminative classifier coupler supported tracking historical models visual tracking;model correction tracking spatial context historical model coupler;computational modeling;object tracking image classification;robustness;target tracking;couplers;context;target tracking couplers computational modeling robustness clutter context	Visual tracking is a significant but challenging field in computer vision. Long-term robust tracking in unconstrained environments is insurmountable due to arbitrary deformation, occlusion and background clutters. In this paper, we propose a framework to integrate couplers and historical models into a discriminative classifier for robust long-term tracking. Model updating inevitably introduces errors into the tracker, and the appearance model cannot match the object well. A best fitted model is retrieved from historical models to correct the tracker. Couplers, which are key points around the object that tightly coupled with the target, are detected to predict the position of the target when the tracker is fully occluded or distracted by other similar objects. Both quantitative and qualitative experimental results validate the superiority of our tracker over other state-of-the-art methods.	computer vision;discriminative model;pattern recognition	Ke He;Ningning Li;Borui Mo;Bo Yang;Aidong Men	2016	2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2016.7574684	computer vision;simulation;computer science;machine learning;clutter;programming language;computational model;robustness	Vision	44.857441905054976	-47.50893044119768	196773
5fe8bb0412e2c15e272d7d8e6297695253c50b6c	research on the video segmentation method with integrated multi-features based on gmm	background modeling;background modeling gaussian mixture models video segmentation;image segmentation;video signal processing;gaussian processes;video segmentation;method integration;computer vision;brightness;gaussian mixture model;computational modeling;adaptation model;image segmentation videoconference brightness image motion analysis digital images educational institutions feature extraction uncertainty computer vision image processing;gaussian mixture model video segmentation pertinent problem integrated multifeature extraction image color analysis;video signal processing feature extraction gaussian processes image colour analysis image segmentation learning artificial intelligence;streaming media;image color analysis;image colour analysis;feature extraction;gaussian mixture models;pixel;pertinent problem;learning artificial intelligence;integrated multifeature extraction	Video segmentation is a hot issue in the image research field. In the current video segmentation method, the pixel color feature in a frame is only considered. The pertinent problem between adjacent pixels is not taken into account. This paper proposes a video segmentation method based on GMM (Gaussian Mixture Model) modeling, meanwhile a method integrating the neighborhood characteristic of a pixel, such as pixel color and brightness characteristic is considered. The neighbor characteristic of a pixel can be a good solution for the bad segmentation result because of the tiny change in the background. The characteristic of brightness and chromaticity can solve the problem arising from the light and shadow change. In this method, the Gaussian mixture models for each pixel are built firstly. Then the relevant parameters are trained and identified. Combining the neighbor characteristic of pixel, brightness and chromaticity, the video can be segmented. Experiment results show that this method compared with other methods improves the video segmentation results.	color;fractal dimension;google map maker;mixture model;pixel;relevance;video tracking	Herong Zheng;Zhi Liu;Xiaofeng Wang	2008	2009 International Conference on Digital Image Processing	10.1109/ICDIP.2009.38	computer vision;range segmentation;computer science;morphological gradient;machine learning;pattern recognition;image segmentation;scale-space segmentation	Vision	44.04691902763465	-50.373316735227384	196848
5a86903fc2184f63797fe49255990ca6ba62aa3c	learning context-based feature descriptors for object tracking	feature extraction;object detection;pattern clustering;clusters;context-based feature descriptors;learning;natural scenes;object representations;object tracking;scene context;clustering;descriptor adaptation;feature-based tracking;particle filter	A major problem with previous object tracking approaches is adapting object representations depending on scene context to account for changes in illumination, viewpoint changes, etc. To adapt our previous approach to deal with background changes, here we first derive some clusters from a training sequence and the corresponding object representations for those clusters. Next, for each frame of a separate test sequence, its nearest background cluster is determined and then the corresponding descriptor of that cluster is used for object representation in this frame. Experiments show that the proposed approach tracks objects and persons in natural scenes more effectively.	feature vector	Ali Borji;Simone Frintrop	2010	2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1734454.1734481	human–robot interaction;computer vision;method;object model;particle filter;computer science;artificial intelligence;machine learning;pattern recognition	Robotics	41.84695994590631	-50.02018621139727	197050
4f1cc1286dfa1eb36e2687ba5da38bcceaef6048	a naive approach to compose aerial images in a mosaic fashion	old films;settore inf 01 informatica;image segmentation;image databases;application software;image sequence analysis;mosaic;testing;layout image databases data visualization image sequence analysis image reconstruction testing aircraft humans application software robustness;multiple sequence image analysis;layout;aerial image;aerial sequences;geophysics computing;long distance;image representation;image reconstruction;remote sensing;data visualization;geophysics computing remote sensing image segmentation image sequences image representation image reconstruction;aerial images;robustness;image analysis;humans;image mosaicing;aircraft;image sequences;aerial sequences aerial images mosaic multiple sequence image analysis image representation image mosaicing image reconstruction old films	• Session 1 Object and Shape Detection (Chairman Gerard Sommer) 9:20 – 9:40 Automatic Character Location and Recognition in Color Scene Images. H. Wang 9:40 – 10:10 Lip detection and tracking. A. Caplier 10:10 – 10:30 Recognition of Shape-Changing Hand Gestures Based on Switching Linear Model. M. Jeong, Y. Kuno, N. Shimada, and Y. Shirai 10:30 – 10:50 2D shape recognition by Hidden Markov Models. M. Bicego and V. Murino	aerial photography;hidden markov model;linear model;markov chain	Domenico Tegolo;Cesare Valenti	2001		10.1109/ICIAP.2001.957061	iterative reconstruction;layout;computer vision;mosaic;application software;image analysis;computer science;software testing;image segmentation;data visualization;robustness;computer graphics (images)	Vision	45.29337456338499	-49.585523117228576	197295
29c362ad0d13f9e60a25442c2298a1933c163431	occlusion detection and localization from kinect depth images	occlusion detection;binary classifier;depth images;kinect sensor	Faces captured in a real-world scenario may suffer from large variations in shape and occlusions due to difference in illumination, variation in pose and orientation of a facial image. Automated face recognition or security reinforcement by surveillance techniques would be useless if the faces are occluded. Therefore, face occlusion detection has become very important not only for effective face recognition but also to prevent security threats. In this paper, for the very first time an occlusion detection method is proposed based on the depth information provided by Kinect RGB-D cameras. Uniform Local Binary Pattern (LBP) is used to effectively extract the features from the depth images and SVM binary classifier is then applied to identify the front face and the occluded face. For localizing occluded regions in the face image, a threshold based approach is proposed to identify the areas close to the camera. In the depth images, an object close to the camera has a higher pixel intensity than the object further from the camera. Thus, we assume that occluded regions have lower distance from the camera, i.e. higher intensity values. Based on this hypothesis, we extract the connected component with highest energy values as the potential occluded region from the depth image. The boundary of the detected occluded region is then corrected using the reference front face image. The occlusion detection and localization method have been evaluated on EUROKOM Kinect face database containing different types of occluded and unoccluded faces with neutral expressions. Experimental results show that the proposed method provides an average detection rate of 98.50% for front and occluded face images. We have also compared our proposed method with existing methods that use faces acquired using 3D scanners for occlusion detection.	3d scanner;binary classification;connected component (graph theory);facial recognition system;fuzzy logic;illumination (image);image quality;kinect;local binary patterns;nonlinear system;pixel;pose (computer vision)	Fatema Tuz Zohra;Md Wasiur Rahman;Marina L. Gavrilova	2016	2016 International Conference on Cyberworlds (CW)	10.1109/CW.2016.40	binary classification;computer vision;object-class detection;computer science;machine learning;pattern recognition;computer graphics (images)	Vision	41.85717143049716	-51.69816363098775	197458
346bae9c6af21fad52534a0bbd5bb66d711e0830	a particle filtering framework for joint video tracking and pose estimation	filtering;sylvester equation;algorithms image enhancement image interpretation computer assisted imaging three dimensional movement pattern recognition automated posture reproducibility of results sensitivity and specificity subtraction technique;video signal processing;3d pose estimation;singular value decomposition;video signal processing feature extraction image sequences motion estimation object detection particle filtering numerical methods pose estimation singular value decomposition target tracking;2 d image sequences;sampling density;video tracking;motion estimation;filtering particle tracking equations image sequences motion estimation transforms feature extraction monte carlo methods computer simulation robustness;classical svd method;scale invariant feature transform;particle filtering framework;particle filter;feature extraction;object tracking;image sequence;transforms;sylvester s equation;joint video tracking;real world videos demonstration particle filtering framework joint video tracking object motion tracking 2 d image sequences scale invariant feature transform sylvester equation classical svd method 3d 3d pose estimation sampling density;robustness;real world videos demonstration;particle tracking;synthetic data;target tracking;importance sampling;video tracking pose estimation sylvester s equation;computer simulation;object motion tracking;3d 3d pose estimation;monte carlo methods;object detection;particle filtering numerical methods;image sequences;pose estimation	A method is introduced to track the object's motion and estimate its pose directly from 2-D image sequences. Scale-invariant feature transform (SIFT) is used to extract corresponding feature points from image sequences. We demonstrate that pose estimation from the corresponding feature points can be formed as a solution to Sylvester's equation. We show that the proposed approach to the solution of Sylvester's equation is equivalent to the classical SVD method for 3D-3D pose estimation. However, whereas classical SVD cannot be used for pose estimation directly from 2-D image sequences, our method based on Sylvester's equation provides a new approach to pose estimation. Smooth video tracking and pose estimation is finally obtained by using the solution to Sylvester's equation within the importance sampling density of the particle filtering framework. Finally, computer simulation experiments conducted over synthetic data and real-world videos demonstrate the effectiveness of our method in both robustness and speed compared with other similar object tracking and pose estimation methods.	3d modeling;3d pose estimation;algorithm;centralized computing;clinical use template;computer simulation;elegant degradation;entity–relationship model;error analysis (mathematics);estimation theory;experiment;importance sampling;particle filter;pose (computer vision);sampling (signal processing);scale-invariant feature transform;singular value decomposition;synthetic data;template matching;tree accumulation;video tracking	Chong Chen;Dan Schonfeld	2010	IEEE Transactions on Image Processing	10.1109/TIP.2010.2043009	computer simulation;computer vision;mathematical optimization;3d pose estimation;computer science;video tracking;pattern recognition;mathematics;statistics	Vision	46.11117377997845	-49.061527959042856	197463
4baf3b165489122a1f8b574240c2a7fa9b6a7a14	composite statistical inference for semantic segmentation	recombination of segments;image segmentation object segmentation semantics histograms proposals computational modeling training;image segmentation;statistical analysis;composite statistical inference pascal voc segmentation em algorithm statistical model composite likelihood inference framework inferred super pixel statistics object segmentation unary pixel pairwise pixel pixel potentials image segmentation inference procedure semantic segmentation;composite statistical inference;composite likelihood;statistical analysis image segmentation object detection;semantic segmentation;recombination of segments composite statistical inference composite likelihood semantic segmentation;object detection	In this paper we present an inference procedure for the semantic segmentation of images. Different from many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or super pixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on super pixels obtained by multiple intersections of segments, then output the optimal segments from the inferred super pixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.	best-first search;conditional random field;dillon's rolling western;expectation–maximization algorithm;extended euclidean algorithm;graphical model;ibm notes;interaction;international symposium on fundamentals of computation theory;latent variable;mathematical optimization;pixel;statistical model;unary operation	Fuxin Li;João Carreira;Guy Lebanon;Cristian Sminchisescu	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.424	computer vision;range segmentation;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;quasi-maximum likelihood;scale-space segmentation;statistics	Vision	45.56255471176089	-51.19671748207728	198374
