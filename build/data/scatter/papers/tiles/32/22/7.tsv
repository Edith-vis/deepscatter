id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
5173b61aa6bf9416de62f87780a25a400b934ca9	spatial and feature space clustering: applications in image analysis	image features;range data;image segmentation;colour image segmentation;line detection;feature space;motion segmentation;image analysis	We propose a novel approach to image segmentation, called feature and spatial domain clustering. The method is devised to group pixel data by taking into account simultaneously both their feature space similarity and spatial coherence. The FSD algorithm is practically application independent. It has been successfully tested on a wide range of image segmentation problems, including grey and colour image segmentation, edge and line detection, range data and motion segmentation. In comparison with existing segmentation approaches, the method can resolve image features even if their distributions significantly overlap in the feature space. It can distinguish between noisy regions and genuine fine texture. Moreover, if required, FSD clustering can produce partial segmentation by identifying salient regions only.	3d scanner;adrian ettlinger;algorithm;basic stamp;cluster analysis;coherence (physics);color image;edge detection;experiment;feature vector;image analysis;image segmentation;optical flow;pixel;texture mapping	Jiri Matas;Josef Kittler	1995		10.1007/3-540-60268-2_293	image texture;computer vision;feature detection;range segmentation;scale space;image analysis;feature vector;computer science;morphological gradient;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;feature	Vision	44.375914237080444	-55.70025370582303	158770
320dc20e159707a5e7374c5133914f472f99ad7b	recognizing two handed gestures with generative, discriminative and ensemble methods via fisher kernels	modelizacion;analisis contenido;image tridimensionnelle;variabilidad;modelo markov oculto;interfase usuario;vision ordenador;centro gravitacional;base donnee;pistage;human computer interaction;multimedia;analisis estadistico;centre gravite;image processing;modele agrege;ensemble method;modelo markov;modele markov cache;methode noyau;user interface;hidden markov model;gesture;center of mass;securite informatique;database;rastreo;procesamiento imagen;base dato;modelo agregado;model based approach;aprendizaje probabilidades;classification;traitement image;computer vision;man machine system;computer security;discriminant analysis;analyse discriminante;modelisation;reconstruction image;analisis discriminante;content analysis;markov model;combining classifier;statistical analysis;reconstruccion imagen;image reconstruction;seguridad informatica;metodo nucleo;machine exemple support;analyse statistique;aggregate model;error rate;tridimensional image;sistema hombre maquina;apprentissage probabilites;kernel method;interface utilisateur;vision ordinateur;modele donnee;support vector machine;maquina ejemplo soporte;analyse contenu;modele markov;vector support machine;variability;variabilite;modeling;geste;clasificacion;3d reconstruction;probability learning;tracking;imagen tridimensional;data models;gesto;systeme homme machine	Use of gestures extends Human Computer Interaction (HCI) possibilities in multimodal environments. However, the great variability in gestures, both in time, size, and position, as well as interpersonal differences, makes the recognition task difficult. With their power in modeling sequence data and processing variable length sequences, modeling hand gestures using Hidden Markov Models (HMM) is a natural extension. On the other hand, discriminative methods such as Support Vector Machines (SVM), compared to model based approaches such as HMMs, have flexible decision boundaries and better classification performance. By extracting features from gesture sequences via Fisher Kernels based on HMMs, classification can be done by a discriminative classifier. We compared the performance of this combined classifier with generative and discriminative classifiers on a small database of two handed gestures recorded with two cameras. We used Kalman tracking of hands from two cameras using center-of-mass and blob tracking. The results show that (i) blob tracking incorporates general hand shape with hand motion and performs better than simple center-of-mass tracking, and (ii) in a stereo camera setup, even if 3D reconstruction is not possible, combining 2D information from each camera at feature level decreases the error rates, (iii) Fisher Score methodology combines the powers of generative and discriminative approaches and increases the classification performance.	3d reconstruction;blob detection;discriminative model;embedded system;experiment;feature vector;fisher information;generative model;heart rate variability;hidden markov model;human computer;human–computer interaction;markov chain;multimodal interaction;pattern recognition;sampling (signal processing);statistical classification;stereo camera;support vector machine	Oya Aran;Lale Akarun	2006		10.1007/11848035_23	3d reconstruction;iterative reconstruction;center of mass;data modeling;support vector machine;kernel method;speech recognition;systems modeling;content analysis;image processing;biological classification;word error rate;computer science;artificial intelligence;machine learning;tracking;markov model;user interface;gesture;hidden markov model;discriminative model	Vision	45.81177972593554	-58.34936189651336	159011
7940a5c08d23b7781cb60d6983c57ead661797e8	reliable rejection of mismatching candidates for efficient zncc template matching	exhaustive template matching zncc cross correlation fast;object recognition;mismatching candidate rejection;complexity theory;image processing;image detection mismatching candidate rejection zncc template matching zero mean normalized cross correlation computational cost;cross correlation;exhaustive;image matching;zncc template matching;zero mean normalized cross correlation;correlation methods;indexing terms;object detection correlation methods image matching;fast;zncc;zinc face detection image edge detection computer science computational efficiency costs quality control object detection robot kinematics navigation;image detection;pixel;fast fourier transforms;computational cost;lighting;correlation;template matching;object detection	This paper presents a method that reduces the computational cost of template matching based on the zero-mean normalized cross-correlation (ZNCC) without compromising the accuracy of the results. A very effective condition is determined at a small and fixed cost that allow to rapidly detect a large number of mismatching candidates with no need to compute the ZNCC score. Then, thanks to the use of an additional set of conditions, the computation of the whole ZNCC function is typically required only for a very small number of candidates. Experimental results demonstrate the effectiveness of our approach.	computation;computational complexity theory;cross-correlation;rejection sampling;template matching	Stefano Mattoccia;Federico Tombari;Luigi di Stefano	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711888	computer vision;fast fourier transform;template matching;index term;image processing;computer science;cross-correlation;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;lighting;mathematics;correlation;pixel	Robotics	46.08868908189078	-53.30022857971775	159873
fb4948e7df2fcd0e2a1f3f4794fba5654f55498c	leveraging mutual information in local descriptions: from local binary patterns to the image		Local image descriptors provide robust descriptions of image localities. Their geometric arrangement provides additional information about the image they describe, a fact often ignored when employing them to that wide slew of tasks from image registration to scene classification. On the premise that descriptor quality could be assessed in terms of its expressiveness of image content, we investigate the use of the described as well as that additional geometric information to the task of recovering the image from its local descriptors. This paper uses Local Binary Patterns, an operator nested in a dense geometry, to study how this additional information in the form of constraints among pixels dictates the intensity estimated for a pixel. We determine that constraints propagate from regional extrema to regions around them that observe the same constraint class, and that the intensity for any of the region’s pixels influences that for all others. We build a directed constraint graph of pixel nodes such that the arcs on the graph are strongly k-consistent, and propagate intensity estimates from extremum nodes. Evaluations are run on the SIPI texture and the BSD500 datasets. The estimates preserve the local structure of the image, as shown by the Mean Absolute Error of about (15%) and (18%) respectively and Structural Texture SIMilarity of about (92%) for both datasets, in addition to observing (100%) constraint satisfaction.	local binary patterns;mutual information	Tahir Q. Syed;Sadaf I. Behlim;Alishan K. Merchant;Alexis Thomas;Furqan Muhammad Khan	2015		10.1007/978-3-319-23234-8_23	pattern recognition	ML	42.35960112839504	-55.406074362693595	161022
0566f65086ad4a96d380ce3a2747077043561c9c	linear-time robot localization and pose tracking using matching signatures	signature matching;slam;data association;feature based navigation;global localization;multi hypothesis tracking	A feature-based method for global localization of mobile robot using a concept of matching signatures is presented. A group of geometric features, their geometric constraints invariant to frame transform, and location dependent constraints, together are utilized in defining signature of a feature. Plausible global poses are found out by matching signatures of observed features with signatures of global map features. The concept of matching signatures is so developed that the proposed method provides a very efficient solution for global localization. Worst-case complexity of the method for estimating and verifying global poses is linear with the size of global reference map. It will also be shown that with the approach of random sampling the proposed algorithm becomes linear with both the size of global map and number of observed features. In order to avoid pose ambiguity, simultaneous tracking of multiple pose hypotheses staying within the same framework of the proposed method is also addressed. Results obtained from simulation as well as from real world experiment demonstrate the performance and effectiveness of the method.	pattern matching;robotic mapping;time complexity;type signature	Asim Kar	2012	Robotics and Autonomous Systems	10.1016/j.robot.2011.11.010	computer vision;machine learning;pattern recognition	Robotics	43.17834525342421	-54.33045600870816	161412
ae5289d8c49befc030d14eaf39df309c96a55e57	an image identifier based on hausdorff shape trace transform	image identification;robust method;hausdorff distance;digital image;trace transform;image retrieval	This paper presents a robust method for digital image identification under conditions of variant illumination, compression, flip, scaling, rotation and gray scale conversion. Techniques introduced in this work are composed of two parts. The first one is the signature of image is to be detected by the Trace Transform [6]. Then, in the second part, the notion of Hausdorff distance [8] and Modified Shape Context [10] are employed to measure and to determine the similarity between the models and tested images. Finally, our approach is evaluated with experiments on a set of over 60,000 unique images and one billion images pairs. The experimental result has show that the average of accuracy rate is higher than 83%.	hausdorff dimension;identifier	Rerkchai Fooprateepsiri;Werasak Kurutach;Sutthipong Tamsumpaolerd	2009		10.1007/978-3-642-10677-4_90	hausdorff distance;computer vision;topology;image retrieval;computer science;mathematics;geometry;distance transform;top-hat transform;digital image	Vision	41.68445755654201	-58.95333269325296	161450
8088ddf0d5860ba922a068b70f0c5581f5bda667	peg-free human hand shape analysis and recognition	shape distance methods;peg free human hand shape analysis;charge coupled image sensors;object recognition;handcrafted feature methods;humans fingers biometrics area measurement shape measurement charge coupled devices charge coupled image sensors data mining feature extraction information geometry;deformable hand shape recognition;finger matching;biometrics access control;image matching;biometrics;shape analysis;shape recognition;shape measurement;data mining;hand images;charge coupled devices;information geometry;image matching biometrics access control object recognition;biometric systems;feature extraction;elliptical model;fingers;area measurement;humans;multiple rigid fingers;positioning aids;hand images peg free human hand shape analysis deformable hand shape recognition biometric systems positioning aids multiple rigid fingers elliptical model finger matching handcrafted feature methods shape distance methods positive recognition accuracy;positive recognition accuracy	The paper addresses the problem of deformable hand shape recognition in biometric systems without positioning aids. We separate and recognize multiple rigid fingers. An elliptical model is introduced to represent fingers and accelerate the matching of them. Technically, our method bridges the traditional handcrafted-feature methods and the shape-distance methods. We have tested it using our 108-person, 540-sample database with significantly increased positive recognition accuracy.	biometrics;shape analysis (digital geometry)	Wei Xiong;Changsheng Xu;Sim Heng Ong	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415345	computer vision;speech recognition;feature extraction;computer science;cognitive neuroscience of visual object recognition;pattern recognition;shape analysis;information geometry;biometrics	Robotics	41.03138545032404	-56.56382122273453	162351
ee97359bedb87b63201f59d01b580d7f88870d9b	object representation and recognition in shape spaces	object representation;object recognition;high dimensionality;statistical shape analysis;legendre polynomial;geodesic distance;3d object recognition;invariants;similarity transformation;legendre polynomials;shape space	In this paper, we describe a shape space based approach for invariant object representation and recognition. In this approach, an object and all its similarity transformed versions are identified with a single point in a high-dimensional manifold called the shape space. Object recognition is achieved by measuring the geodesic distance between an observed object and a model in the shape space. This approach produced promising results in 2D object recognition experiments: it is invariant to similarity transformations and is relatively insensitive to noise and occlusion. Potentially, it can also be used for 3D object recognition.	shape context	Jun Zhang;Xiangrong Zhang;Hamid Krim;Gilbert G. Walter	2003	Pattern Recognition	10.1016/S0031-3203(02)00226-1	active shape model;computer vision;topology;object model;axis-aligned object;legendre polynomials;shape analysis;mathematics;geometry;3d single-object recognition	Vision	41.54814406795897	-56.858059562870814	163120
c8ff5cbe7c5a594612182c4120121991154e77c7	small object discovery and recognition using actively guided robot	search problems object recognition robots cameras vectors feature extraction accuracy;rgb depth based algorithm small object discovery small object recognition actively guided robot search space reduction existential probability identification closer 3d analysis gaussian mixture models object recognition;robot vision gaussian processes image colour analysis mixture models object recognition	In the field of active perception, object search is a widely studied problem. To search for an object in large rooms, it would be expensive to explore and check each object's similarity with the object of interest. The expense could uncontrollably bloat as the number of objects to be searched increases. If the objects are of the order of a 2-5cm, they appear very small, making it difficult for the present algorithms to recognize them. A general human strategy in such cases is to sparsely identify, from far away (4-6m), if the object of interest is present in the scene. Subsequently, each of the possible objects is analysed from closer proximity to recognize, for further manipulation. In this work, we present a similar framework. We reduce search-space, by identifying existential probability of a small object from a distance followed by a closer 3-D analysis of its point cloud to accurately recognize it. This is achieved by 2-D modelling of the objects using Gaussian Mixture Models followed by recognizing objects using efficient RGB-Depth based algorithm.	algorithm;mixture model;point cloud;robot;software bloat	Sudhanshu Mittal;M. Siva Karthik;Suryansh Kumar;K. Madhava Krishna	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.742	computer vision;method;deep-sky object;pose;object model;computer science;artificial intelligence;viola–jones object detection framework;object-oriented design;machine learning;3d single-object recognition	Robotics	44.11444244675986	-52.653541140686414	163186
258c7727280b2e203ee98675d7ae70099b9d3e53	trademark image retrieval using an integrated shape descriptor	t technology general;trademarks;knowledge management;feature matching;qa75 electronic computers computer science;feature extraction;intellectual property management;image retrieval	Trademarks are distinctive visual symbols with high reputational value, due to the perception of quality and innovation associated with them. They are important reputational assets used as a marketing tool to convey a certain assurance of quality, innovation, and the standards, which the manufacturer seeks to maintain. This motivates the need for trademark protection by providing a solution to prevent infringement. This problem can be addressed by developing retrieval systems capable of comparing the visual similarity of trademarks. This paper contributes to the research in this field by proposing an innovative trademark retrieval technique with improved retrieval performance due to the integration of global and local descriptors. The global descriptor employed is the Zernike moment’s coefficients. The local descriptor is the edge-gradient co-occurrence matrix, derived from the contour information that is considered very important in human perception of visual similarity. The proposed retrieval technique is tested using the standard MPEG-7 shape database of 1400 images and the MPEG-7 trademark database of 3260 images. The results show 5% precision/recall improvement in the case of the MPEG-7 shape database, as well as 2.35% Bull’s eye score improvement and 19.8% NMRR score improvement for the 10 randomly selected trademarks from the MPEG-7 trademarks database. 2012 Elsevier Ltd. All rights reserved.	co-occurrence matrix;coefficient;computation;computational complexity theory;database;document-term matrix;euclidean distance;experiment;expert system;gradient;gradient descent;image moment;image retrieval;mpeg-7;randomness;shape analysis (digital geometry);taxicab geometry;visual inspection	Fatahiyah Mohd Anuar;Rossitza Setchi;Yu-Kun Lai	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.07.031	computer vision;visual word;feature extraction;image retrieval;computer science;machine learning;data mining;information retrieval	Vision	39.712040267195455	-58.76517748931622	163280
644c6412d54b9398bdae3ec3a6d9551ae3075d52	the illumination-invariant recognition of 3d objects using local color invariants	local color pixel distributions;object recognition;cluttered scenes illumination invariant recognition 3d objects local color invariants three dimensional object recognition image formation local color pixel distributions discriminatory power textured surfaces color image regions geometric information hypothesis verification pose estimation;local color invariants;image segmentation;discriminatory power;image formation;reflectivity;color constancy;illumination invariant recognition;color;illumination invariant;3d objects;computer vision object recognition image colour analysis;illumination correction;geometry;layout;surface texture;three dimensional;computer vision;three dimensional object recognition;image colour analysis;machine vision;color image regions;pixel;spectral reflectance;indexation;hypothesis verification;textured surfaces;lighting;physical model;power system modeling;cluttered scenes;geometric information;computational efficiency;illumination invariance;layout object recognition power system modeling surface texture geometry lighting pixel reflectivity color image segmentation;color image;color vision;pose estimation	Traditional approaches to three dimensional object recognition exploit the relationship between three dimensional object geometry and two dimensional image geometry. The capability of object recognition systems can be improved by also incorporating information about the color of object surfaces. Using physical models for image formation, the authors derive invariants of local color pixel distributions that are independent of viewpoint and the configuration, intensity, and spectral content of the scene illumination. These invariants capture information about the distribution of spectral reflectance which is intrinsic to a surface and thereby provide substantial discriminatory power for identifying a wide range of surfaces including many textured surfaces. These invariants can be computed efficiently from color image regions without requiring any form of segmentation. The authors have implemented an object recognition system that indexes into a database of models using the invariants and that uses associated geometric information for hypothesis verification and pose estimation. The approach to recognition is based on the computation of local invariants and is therefore relatively insensitive to occlusion. The authors present several examples demonstrating the system's ability to recognize model objects in cluttered scenes independent of object configuration and scene illumination. The discriminatory power of the invariants has been demonstrated by the system's ability to process a large set of regions over complex scenes without generating false hypotheses.		David Slater;Glenn Healey	1996	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.481544	computer vision;machine vision;computer science;mathematics;reflectivity;computer graphics (images)	Vision	44.154210532439556	-54.36550301369208	163304
bae1d7574b94eec513a2c009af6ffac3b76dfed1	3d face recognition with geometrically localized surface shape indexes	shape index;3d face recognition;shape indexes;support vector machines;facial geometry;weighted distance matching;independent component analysis;three dimensional;distinctive facial features;face recognition;shape indexes 3d face recognition;indexation;geometrically localized surface shape indexes;facial features;independent component analysis 3d face recognition geometrically localized surface shape indexes distinctive facial features invariant facial feature points extraction facial geometry normalized face data shape index weighted distance matching support vector machine;support vector machine;face recognition shape facial features support vector machines independent component analysis eyes nose mouth data mining geometry;invariant facial feature points extraction;normalized face data;support vector machines face recognition independent component analysis	This paper describes a pose invariant three-dimensional (3D) face recognition method using distinctive facial features. A face has its structural components like the eyes, nose and mouth. The positions and the shapes of the facial components are very important characteristics of a face. We extract invariant facial feature points on those components using the facial geometry from a normalized face data and calculate relative features using these feature points. We also calculate a shape index on each area of facial feature point to represent curvature characteristics of facial components. We perform recognition by using weighted distance matching, support vector machine (SVM) and independent component analysis (ICA)	algorithm;data acquisition;facial recognition system;feature recognition;feature selection;feature vector;independent computing architecture;independent component analysis;preprocessor;shape table;structured light;support vector machine;three-dimensional face recognition	Hyoungchul Shin;Kwanghoon Sohn	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345192	facial recognition system;support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;mathematics;face hallucination	Robotics	40.60882440434294	-58.04006417686018	163853
97e9fcb0ad56f87f8e8535185a96c653059513cc	coarse-to-fine adaptive masks for appearance matching of occluded scenes	object recognition;occlusion;appearance matching;global illumination;multiple objectives;coarse to fine search;similarity measure;image similarity metrics;image similarity	In this paper, we discuss an appearance-matching approach to the difficult problem of interpreting color scenes containing occluded objects. We have explored the use of an iterative, coarse-to-fine sum-squared-error method that uses information from hypothesized occlusion events to perform run-time modification of scene-to-template similarity measures. These adjustments are performed by using a binary mask to adaptively exclude regions of the template image from the squared-error computation. At each iteration higher resolution scene data as well as information derived from the occluding interactions between multiple object hypotheses are used to adjust these masks. We present results which demonstrate that such a technique is reasonably robust over a large database of color test scenes containing objects at a variety of scales, and tolerates minor 3D object rotations and global illumination variations.	computation;global illumination;interaction;iteration	Jeff L. Edwards;Hiroshi Murase	1998	Machine Vision and Applications	10.1007/s001380050075	computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;global illumination	Vision	43.89727476520941	-54.9553405872266	164272
8a5714ab95b8c5e17103b8c7a8fadbdda08c3a5b	an improved sift algorithm for infringement retrieval	infringement retrieval;scale invariant feature transformation (sift);geographical statistics (geostat);coarse-to-fine retrieval	To achieve powerful infringement retrieval for reference images in digital publications, a new improved Scale Invariant Feature Transformation (SIFT) algorithm has been proposed in this paper. The retrieval process in the improved algorithm is innovatively divided into two stages to achieve coarse retrieval and fine retrieval respectively. In coarse retrieval, Geographical Statistics (GeoStat) is creatively used to describe the global spatial relationship of key-points in different orientations in an image and then generate a 144-dimensional feature vector to represent each image. In fine retrieval, only partial images, which are highly similar to the query image obtained from results of coarse retrieval, need to be considered. And the indexing and matching process in the improved algorithm is improved by adding a judgment process to improve the matching speed and reduce the mistaken matching rate. Experimental results show that the proposed algorithm has more advantages in retrieval speed and higher retrieval accuracy than the original SIFT algorithm. And the proposed algorithm is also more suitable for infringement retrieval of reference images in digital publications than the original one.	algorithm;feature vector;interference (communication);scale-invariant feature transform;speeded up robust features	Guangmin Sun;Chenyang Wang;Beichuan Ma;Xiaomeng Wang	2017	Multimedia Tools and Applications	10.1007/s11042-017-5060-8	computer vision;visual word;artificial intelligence;pattern recognition;computer science;image retrieval;scale-invariant feature transform;search engine indexing;feature vector	Vision	39.28606986361049	-58.97374847504665	164370
7bd4a9b448c66fccf745253f23608a5bc0d23800	variable window for outlier detection and impulsive noise recognition in range images	quality assessment impulsive noise recognition range image denoising impulsive noise denoising discriminant criteria dropout in detection outlier in detection nearest nonin neighbors searching process index distance weighted mean filter invader occlusion dynamic environment adaptive variable window size determination multiline surface computational complexity;noise noise reduction image denoising algorithm design and analysis indexes educational institutions wavelet transforms;index distance weighted mean filter variable window range image denoising outlier detection impulsive noise recognition;impulse noise computational complexity image denoising image recognition	To improve comprehensive performance of denoising range images, an impulsive noise (IN) denoising method with variable windows is proposed in this paper. Founded on several discriminant criteria, the principles of dropout IN detection and outlier IN detection are provided. Subsequently, a nearest non-IN neighbors searching process and an Index Distance Weighted Mean filter is combined for IN denoising. As key factors of adapatablity of the proposed denoising method, the sizes of two windows for outlier INs detection and INs denoising are investigated. Originated from a theoretical model of invader occlusion, variable window is presented for adapting window size to dynamic environment of each point, accompanying with practical criteria of adaptive variable window size determination. Experiments on real range images of multi-line surface are proceeded with evaluations in terms of computational complexity and quality assessment with comparison analysis among a few other popular methods. It is indicated that the proposed method can detect the impulsive noises with high accuracy, meanwhile, denoise them with strong adaptability with the help of variable window.	anomaly detection;computational complexity theory;discriminant;dropout (neural networks);experiment;microsoft windows;noise reduction	Jian Wang;Lin Mei;Yi Li;Jian-Ye Li;Kun Zhao;Yuan Yao	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.49	step detection;machine learning;pattern recognition;non-local means;video denoising;salt-and-pepper noise	Robotics	41.68799106781725	-54.1695797495144	164375
117d576d72515e900e6fc5a4a0e7f1d0142a8924	an efficient dense and scale-invariant spatio-temporal interest point detector	time scale;interest points;scale space;action recognition;computational efficiency;scale invariance;quantitative evaluation;invariant feature	Over the years, several spatio-temporal interest point detectors have been proposed. While some detectors can only extract a sparse set of scaleinvariant features, others allow for the detection of a larger amount of features at user-defined scales. This paper presents for the first time spatio-temporal interest points that are at the same time scale-invariant (both spatially and temporally) and densely cover the video content. Moreover, as opposed to earlier work, the features can be computed efficiently. Applying scale-space theory, we show that this can be achieved by using the determinant of the Hessian as the saliency measure. Computations are speeded-up further through the use of approximative box-filter operations on an integral video structure. A quantitative evaluation and experimental results on action recognition show the strengths of the proposed detector in terms of repeatability, accuracy and speed, in comparison with previously proposed detectors.	batch processing;blob detection;computation;digital video;executable;hessian;interest point detection;iterative method;national fund for scientific research;repeatability;scale space;scale-invariant feature transform;sensor;sparse language;sparse matrix	Geert Willems;Tinne Tuytelaars;Luc Van Gool	2008		10.1007/978-3-540-88688-4_48	computer vision;mathematical optimization;scale space;simulation;computer science;scale invariance;mathematics;statistics	Vision	39.832927511264856	-54.71994909385346	165941
6dc901c3224b153a14d9c235f3cf5a3cbd85a8d9	a visual exploration algorithm using semantic cues that constructs image based hybrid maps	metric embedding;graph theory;robot sensing systems;edge representation;loop detection;measurement;semantic cues;support vector machines;local feature based exploration algorithm;edge detection;semantics support vector machines measurement robot sensing systems logic gates probabilistic logic;semantics;robot poses;mobile robots;semantic construct;robot vision;logic gates;local features;image representation;feature extraction;image reconstruction;svm classifier;visual exploration algorithm;hybrid maps;topological graph;support vector machines edge detection feature extraction graph theory image reconstruction image representation mobile robots pose estimation robot vision slam robots;visual exploration algorithm semantic cues image reconstruction hybrid maps topological maps edge representation topological graph image representation local feature based exploration algorithm loop detection robot poses semantic construct svm classifier;probabilistic logic;topological maps;slam robots;bag of words;transition region;pose estimation	A vision based exploration algorithm that invokes semantic cues for constructing a hybrid map of images - a combination of semantic and topological maps is presented in this paper. At the top level the map is a graph of semantic constructs. Each node in the graph is a semantic construct or label such as a room or a corridor, the edge represented by a transition region such as a doorway that links the two semantic constructs. Each semantic node embeds within it a topological graph that constitutes the map at the middle level. The topological graph is a set of nodes, each node representing an image of the higher semantic construct. At the low level the topological graph embeds metric values and relations, where each node embeds the pose of the robot from which the image was taken and any two nodes in the graph are related by a transformation consisting of a rotation and translation. The exploration algorithm explores a semantic construct completely before moving or branching onto a new construct. Within each semantic construct it uses a local feature based exploration algorithm that uses a combination of local and global decisions to decide the next best place to move. During the process of exploring a semantic construct it identifies transition regions that serve as gateways to move from that construct to another. The exploration is deemed complete when all transition regions are marked visited. Loop detection happens at transition regions and graph relaxation techniques are used to close loops when detected to obtain a consistent metric embedding of the robot poses. Semantic constructs are labeled using a visual bag of words(VBOW) representation with a probabilistic SVM classifier.	algorithm;cycle detection;graph (discrete mathematics);linear programming relaxation;map;robot;topological graph	Aravindhan K. Krishnan;K. Madhava Krishna	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5649870	iterative reconstruction;mobile robot;support vector machine;computer vision;semantic similarity;topological graph;pose;edge detection;logic gate;feature extraction;computer science;graph theory;bag-of-words model;machine learning;pattern recognition;mathematics;semantics;probabilistic logic;graph database;measurement	Robotics	43.51389226496077	-56.09460606988764	166448
40110f225e39edac914a214a5b39f451b16ea8f5	a variant of the hough transform for the combined detection of corners, segments, and polylines	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	The Hough Transform (HT) is an effective and popular technique for detecting image features such as lines and curves. From its standard form, numerous variants have emerged with the objective, in many cases, of extending the kind of image features that could be detected. Particularly, corner and line segment detection using HT has been separately addressed by several approaches. To deal with the combined detection of both image features (corners and segments), this paper presents a new variant of the Hough Transform. The proposed method provides an accurate detection of segment endpoints, even if they do not correspond to intersection points between line segments. Segments are detected from their endpoints, producing not only a set of isolated segments but also a collection of polylines. This provides a direct representation of the polygonal contours of the image despite imperfections in the input data such as missing or noisy feature points. It is also shown how this proposal can be extended to detect predefined polygonal shapes. The paper describes in detail every stage of the proposed method and includes experimental results obtained from real images showing the benefits of the proposal in comparison with other approaches.	hough transform	Pilar Bachiller;Luis Manso;Pablo Bustos	2017	EURASIP J. Image and Video Processing	10.1186/s13640-017-0180-7	computer vision;artificial intelligence;feature (computer vision);real image;pattern recognition;corner detection;computer science;polygon;line segment;feature detection (computer vision);hough transform	Vision	41.33133348766337	-57.854145696626276	166774
9730391d0be0dd7ed441ab50c6524a52f430ad90	connecting image similarity retrieval with consistent labeling problem by introducing a match-all label	best partial labeling;convergence;image segmentation;trademarks;image databases;information retrieval;color;image matching;image matching image retrieval;satisfiability;over specified constraints;premature information removal;joining processes;trademark images;image similarity retrieval;relaxation labeling;compatibility model;joining processes image retrieval labeling convergence image segmentation content based retrieval image databases trademarks information retrieval color;image pair;consistent labeling;global correspondences;content based retrieval;trademark images image similarity retrieval consistent labeling match all label relaxation labeling convergence over specified constraints premature information removal labeling probability global correspondences compatibility model best partial labeling image pair;labeling;match all label;labeling probability;image retrieval;image similarity	Our work in image similarity retrieval by relaxation labeling processes [1] required adding local constraints to obtain an initial set of compatible objects and labels on pairs of images to ensure labeling consistency at convergence. This approach suffers from the problem of over-specified constraints that leads to potentially invaluable information being prematurely removed. To address this problem, we introduce the idea of a Match-all label for objects that failed these constraints. It serves to give them a defined labeling probability as well as allowing them participate in global correspondences through the compatibility model. We show that this enhanced formulation still meets the conditions for a theorem on labeling consistency in Hummel and Zucker [2] to be satisfied. At convergence, the set of objects and their most consistent labels constitute a best partial labeling for the pair of images.	linear programming relaxation;vergence	Paul Wing Hing Kwan;Keisuke Kameyama;Kazuo Toraichi	2001		10.1109/FUZZ.2001.1008916	sequence labeling;computer vision;labeling theory;convergence;image retrieval;computer science;pattern recognition;mathematics;image segmentation;information retrieval;satisfiability	Vision	42.834168569347995	-54.98194586903405	166841
0119c7d8d19cf03f96a1a3cbc25df3f3c16b6a4d	reliable image matching based on relative gradients	minimisation;approximate candidate poses;training database;iterative algorithms;image databases;application software;image alignment;image matching;pattern search;image matching inspection lighting robustness application software iterative algorithms feature extraction pattern matching image databases spatial databases;iterative energy minimization procedure reliable image matching relative gradients image alignment algorithm learning based approximate pattern search training database training feature vectors template image uniform sampling geometric transformation space approximate candidate poses;image alignment algorithm;inspection;search problems image matching minimisation iterative methods learning artificial intelligence;template image;reliable image matching;feature vector;iterative methods;geometric transformation space;pattern matching;feature extraction;spatial databases;relative gradients;iterative energy minimization procedure;robustness;search problems;energy minimization;lighting;learning based approximate pattern search;learning artificial intelligence;uniform sampling;training feature vectors	We present an image alignment algorithm based on the matching of relative gradient maps between images. This algorithm consists of two stages; namely, a learning-based approximate pattern search and an iterative energy-minimization procedure for matching relative image gradient. The first stage finds some candidate poses of the pattern in the image through a fast search of the best match of the relative gradient features from the database of training feature vectors. The training database is obtained from the synthesis of the template image under a number of uniform samplings in a range of the geometric transformation space. Subsequently, the approximate candidate poses are further verified and refined by matching the relative gradient images through an iterative energy-minimization procedure. This approach based on the matching of relative gradients has the advantage of robustness against inhomogeneous illumination variations. Some experimental results are shown to demonstrate the efficiency and robustness of the proposed algorithm.	image gradient;image registration	Shang-Hong Lai;Shou-Der Wei	2002		10.1109/ICPR.2002.1048424	pattern search;computer vision;minimisation;application software;template matching;feature vector;inspection;feature extraction;computer science;machine learning;pattern matching;pattern recognition;lighting;mathematics;iterative method;energy minimization;robustness	Vision	46.110117761979886	-53.27537028294605	167049
b6d27af9e61115d41b802c00ce45125747139586	indexing and teaching focus mining of lecture videos	indexing education videos;change detection;teaching focus mining system;video signal processing;distance learning systems indexing focus mining system teaching focus mining system lecture videos edge based shot change detection algorithm;edge detection;video signal processing data mining distance learning edge detection educational aids indexing teaching video retrieval;skin;distance learning;video retrieval;data mining;edge based shot change detection algorithm;lecture videos;educational aids;image edge detection;indexing;video recording;humans;indexing focus mining system;distance learning systems;teaching;videos	This paper proposes an indexing and teaching focus mining system for lecture videos recorded in an unconstraint environment. The slide structure can be reconstructed by an edge-based shot change detection algorithm. Besides, the teaching focus can be extracted according to instructor’s behavior, including the gesture, the lecture time for each slide, and the speech speed. Experiment results show the feasibility of the proposed method, that is, the slide shots can be correctly detected even if the illumination conditions is variant or the slides are obstructed by the instructor or students, and the teaching focus can be well extracted to provide learners an efficient way to study.	algorithm	Yu-Tzu Lin;Bai-Jang Yen;Chia-Hu Chang;Huei-Fang Yang;Greg C. Lee	2009	2009 11th IEEE International Symposium on Multimedia	10.1109/ISM.2009.37	distance education;education;computer vision;search engine indexing;speech recognition;edge detection;computer science;multimedia;skin;change detection;statistics	Robotics	40.455042944846056	-52.45818814093924	167184
8dd57d31cc7bac0cb42661ed04f7e4856dd5feed	texture analysis for shadow removing in video-surveillance systems	video surveillance;image processing;video signal processing;surveillance;2d gabor dictionary texture analysis shadow removing video surveillance system object detection matching pursuit algorithm texture information;image texture;iterative methods;texture analysis;video signal processing image texture surveillance object detection iterative methods image sequences;matching pursuit;object detection image edge detection dictionaries matching pursuit algorithms image processing image analysis shape layout photometry image recognition;object detection;image sequences	The paper presents a new approach for detecting and removing shadows from objects and pedestrians, since shadow removing is a fundamental step in video-surveillance systems for accurate object detection. In order to precisely remove the unwanted shadows, a novel approach is proposed, focused on the problem of representing texture information in terms of redundant systems of functions (frame). The method for discriminating shadows is based on the matching pursuit (MP) algorithm using an over-complete dictionary: the basic idea is to use MP for selecting the best little set of atoms (dictionary functions) of 2D Gabor dictionary and representing texture as linear combination of frame elements. The approach proves how MP is a powerful scheme able to compactly capture detailed textural information of little regions of the image, so MP decomposition coefficients can be used as an exhaustive features in the shadow points detection process. Experimental results validate the algorithm's performance.	algorithm;coefficient;computer performance;dictionary;matching pursuit;object detection;sensor	Alessandro Leone;Cosimo Distante;Nicola Ancona;Ettore Stella;Pietro Siciliano	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401393	image texture;computer vision;feature detection;image processing;computer science;pattern recognition;iterative method;matching pursuit;computer graphics (images)	Robotics	44.571860280122046	-53.894741301207084	167830
3eb1495b54a5e2f2e5dfb559aeed8baa7d8cf6c2	an analysis of facial description in static images and video streams	analisis imagen;streaming;video streaming;image processing;info eu repo semantics conferenceobject;facies;imagen fija;race;acces concurrent;transmision continua;real time;procesamiento imagen;classification;traitement image;transmission en continu;acceso simultaneo;senal video;fixed image;signal video;temps reel;120304 inteligencia artificial;pattern recognition;video signal;tiempo real;image fixe;image analysis;reconnaissance forme;reconocimiento patron;analyse image;clasificacion	This paper describes an analysis performed for facial description in static images and video streams. The still image context is first analyzed in order to decide the optimal classifier configuration for each problem: gender recognition, race classification, and glasses and moustache presence. These results are later applied to significant samples which are automatically extracted in real-time from video streams achieving promising results in the facial description of 70 individuals by means of gender, race and the presence of glasses and moustache.	experiment;real-time locating system;statistical classification;streaming media;test set	Modesto Castrillón Santana;Javier Lorenzo-Navarro;Daniel Hernández-Sosa;Yeray Rodríguez-Domínguez	2005		10.1007/11492429_56	computer vision;image analysis;facies;image processing;biological classification;computer science;race;computer graphics (images)	Vision	45.71072499015147	-57.63105101708138	167908
073cb614b077f0f51c260c29ed450db3aa32d2d2	foreground object extraction using thresholding with automatic shadow removal	threshold;radiometric similarity threshold multiband trivariate autocorrelation;object detection computer vision feature extraction image colour analysis image segmentation;trivariate;computer vision foreground object extraction silhouette extraction method shadow removal algorithms shadow detection algorithms automatic shadow removal method self shadow moving cast shadows image segmentation rgb intensities background subtraction moving object extraction intelligent transportation system visual surveillance;multiband;yttrium histograms color image color analysis correlation mathematical model surveillance;radiometric similarity;autocorrelation	In Computer Vision, Visual Surveillance or in caseof Intelligent Transportation System, one of the most challengingtasks is the extraction of moving object or foreground extraction. Background Subtraction is the most intuitive method using singlefixed threshold for foreground extraction. Some techniques makeuse of correlated nature of RGB intensities, but can't resolvethe ambiguities of using a single fixed threshold. Moreover, segmentation has side effects like moving cast shadows and SelfShadow, both of which reduce accuracy. In this paper, we presentan automatic shadow removal method to extract objects. Theanalytical comparison with other foreground extraction, shadowdetection and removal algorithms and their results are alsopresented for better understanding. The proposed method extractsilhouette without any pre or post processing step of shadow removal.	algorithm;background subtraction;computer vision;erosion (morphology);mathematical morphology;otsu's method;preprocessor;sentence extraction;thresholding (image processing)	Mohit Kumar Singh;Neeta Nain;Subhash Panwar	2015	2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)	10.1109/SITIS.2015.73	computer vision;autocorrelation;pattern recognition;statistics;computer graphics (images)	Vision	44.99638123822076	-54.11666787064183	168201
dd292802fb81661f8001c6b944d955fa5349f7ab	word spotting using radial descriptor graph	image segmentation;measurement;training;learning systems;feature extraction;merging;computer science	In this paper we present, the Radial Descriptor Graph, a novel approach to compare pictorial representation of handwritten text, which is based on the radial descriptor. To build a radial descriptor graph, we compute the radial descriptor and generate feature points. These points are the nodes of the graph, and each adjacent points are connected to its adjacent node to form a planar graph. Then we iteratively reduce the edges of the graph, by merging adjacent nodes, to form a multilevel hierarchical representation of the graph. To compare two pictorial representations, we measure the distance between their correspondence planar graphs, after calculating the dominant signal for each node. The graph matching is based on optimizing the function that takes into account the distance between the feature points and the structure of the graphs. The distance between two radial descriptors is computed by measuring the difference between their corresponding dominant signals. We have tested our approach on three different datasets and obtained encouraging results.	image;matching (graph theory);planar graph;radial (radio);radial basis function	Majeed Kassis;Jihad El-Sana	2016	2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)	10.1109/ICFHR.2016.0019	lattice graph;graph power;computer vision;geometric graph theory;graph embedding;graph bandwidth;null graph;feature extraction;computer science;machine learning;pattern recognition;voltage graph;image segmentation;quartic graph;complement graph;string graph;strength of a graph;measurement	Vision	40.57008360578823	-57.97863798243302	168247
39a5bb55dd70be0a38e3f2f2a1c2c61b2a24bfa1	3d objects coding and recognition using surface signatures	topology;object recognition;image representation surface point signature image coding 3d object recognition image compression pose estimation surface signature representation;image coding;data compression;application software;3d pose estimation;3d object recognition;computer vision;surface signature representation;visualization;shape;image compression;image representation;stereo image processing;performance analysis;computer science;surface point signature;robot vision systems;image representation object recognition stereo image processing image coding data compression;shape object recognition image coding computer science application software computer vision robot vision systems visualization performance analysis topology;pose estimation	This paper presents a new concept for 3D coding of freeform surfaces. The proposed coding technique uses the surface signature representation scheme [1]. This representation scheme captures the 3-D curvature information of any free-form surface and encodes it into a 2-D image corresponding to a certain point on the surface. This image is unique for this point and is independent of the object translation or orientation in space. For this reason this image is called “Surface Point Signature” (SPS). Using SPS in 3D coding has many applications in 3D compression, 3D pose estimation and in 3D object recognition.	3d pose estimation;3d single-object recognition;freeform surface modelling;ibm 1401 symbolic programming system;outline of object recognition	Sameh M. Yamany;Aly A. Farag	2000		10.1109/ICPR.2000.902984	data compression;computer vision;application software;pose;visualization;3d pose estimation;shape;image compression;computer science;theoretical computer science;cognitive neuroscience of visual object recognition;pattern recognition	Vision	42.662743503003774	-57.038923250714	168304
cf20d802e67f20b01b9559fee782b0144cf6bdf2	face recognition based on invariant eigenvectors and hausdorff fraction distance	eigenvalues and eigenfunctions;similarities evaluation;hausdorff fraction distance;minute edge alteration;time complexity;edge detection;invariant eigenvectors;image edge extraction;face recognition;minute edge alteration face recognition invariant eigenvectors hausdorff fraction distance image edge extraction similarities evaluation orl face database image rotation;image edge detection;orl face database;face;humans;lighting;face recognition lighting image recognition humans computer science image databases shape robustness linear discriminant analysis principal component analysis;face recognition edge detection eigenvalues and eigenfunctions;image rotation;eigenvectors	A method for face recognition based on invariant eigenvectors and Hausdorff Fraction Distance is proposed. With this method, the invariant eigenvectors based on the image edge are firstly extracted. Then by computing the Hausdorff Fraction Distance between the invariant eigenvectors, the process for similarities evaluation is accomplished. Experimental results on the ORL face database validate that the proposed method is invariant to image rotation, minute edge alteration and illumination conditions, and can improve recognition precision and reduce time complexity simultaneously.	facial recognition system;hausdorff dimension;return loss;time complexity	Yonghua Xie;Lokesh Setia;Hans Burkhardt	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.275	facial recognition system;face;time complexity;hausdorff distance;computer vision;edge detection;topology;eigenvalues and eigenvectors;computer science;lighting;mathematics;geometry	Vision	41.68453345064134	-58.66579850927647	168417
34e549cf4d790ea14cf5f88673110c59b61e5d8e	combining harris interest points and the sift descriptor for fast scale-invariant object recognition	databases;detectors;nearest neighbor searches;image recognition;object recognition;image resolution;time 500 ms to 600 ms;interest points;training;data mining;local system;fast scale invariant object recognition;image texture;object recognition spatial databases detectors principal component analysis robustness gabor filters testing intelligent robots usa councils time factors;harris corner detector;object localization;scale space;transforms image recognition image texture;feature extraction;transforms;region based features;scale invariant feature transform descriptor;experimental evaluation;harris interest points;visual servoing;scale invariance;time 150 ms to 240 ms;time 150 ms to 240 ms harris interest points scale invariant feature transform descriptor fast scale invariant object recognition object localization region based features harris corner detector image texture time 500 ms to 600 ms	In the recent past, the recognition and localization of objects based on local point features has become a widely accepted and utilized method. Among the most popular features are currently the SIFT features, the more recent SURF features, and region-based features such as the MSER. For time-critical application of object recognition and localization systems operating on such features, the SIFT features are too slow (500-600 ms for images of size 640×480 on a 3 GHz CPU). The faster SURF achieve a computation time of 150-240 ms, which is still too slow for active tracking of objects or visual servoing applications. In this paper, we present a combination of the Harris corner detector and the SIFT descriptor, which computes features with a high repeatability and very good matching properties within approx. 20 ms. While just computing the SIFT descriptors for computed Harris interest points would lead to an approach that is not scale-invariant, we will show how scale-invariance can be achieved without a time-consuming scale space analysis. Furthermore, we will present results of successful application of the proposed features within our system for recognition and localization of textured objects. An extensive experimental evaluation proves the practical applicability of our approach.	approximation;central processing unit;computation;corner detection;display resolution;harris affine region detector;interest point detection;internationalization and localization;maximally stable extremal regions;outline of object recognition;repeatability;scale space;scale-invariant feature transform;speeded up robust features;time complexity;visual servoing;window of opportunity	Pedram Azad;Tamim Asfour;Rüdiger Dillmann	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354611	corner detection;image texture;computer vision;detector;scale space;speech recognition;image resolution;feature extraction;computer science;cognitive neuroscience of visual object recognition;scale invariance;pattern recognition;mathematics;3d single-object recognition;visual servoing;local system	Robotics	40.205146066092276	-54.705348987051806	168909
c3433a02ada61c5d234ebfae44e9ac7ad465cc26	efficient recognition of planar object using intensity based matching evaluation	object recognition;robust tracking;augmented reality	In this paper, we propose a method of intensity-based image matching evaluation to achieve efficient planar object detection with small number of feature points. To detect target object from a camera image, establishing matches between feature points of camera image and those of objects is processed. As correctness in matching procedure effects overall performance of detection task, it is important to determine error-prone matches and filter them out. In previous researches, they have usually focused on probabilistic approaches to figure out error of matches. In their approaches, with large number of matched feature points, it performs optimization process which is iterating estimation of projectivity with randomly selected matched feature points until it finds converging projectivity result. The matched feature points which contribute the projectivity result is assumed to be correct. In these approaches, to acquire reliable result, there should be large number of matched feature points and most of matches should be correct. In this paper, we propose an efficient way to detect the target object with small number of matches. It becomes possible by filtering out wrong matches and increasing reliability of set of matches which is small. To filter out error matches, we exploit intensity information of image. To make intensity information usable, we have devised a method to define geometric structure between feature points and model the intensity information based on it. The experimental results show that it is possible to acquire reliable projectivity with small number of correct matches and enhance overall performance by minimizing the burden for optimization process.	cognitive dimensions of notations;correctness (computer science);feature vector;image registration;mathematical optimization;object detection;randomness	Jooyoung Lee;HyungSeok Kim;BoYu Gao;Jee-In Kim	2016		10.1145/2987386.2987436	computer vision;machine learning;pattern recognition;mathematics	Vision	43.95372411686616	-52.9504251778368	169510
2b5ea3ff1b69b9e8fced2f21760087ede54e1aeb	automatic estimation and segmentation of partial blur in natural images		Digital images may contain undesired blurred regions. Automatic detection of such regions and estimation of the amount of blurriness in a given image are important issues in many computer vision applications. This paper presents a simple and effective method to automatically detect blurred regions. The proposed method consists of two main parts. First, a novel blur metric, which can significantly distinguish blur and non-blur regions, is proposed. This metric is then used to generate a blur map to encode the amount of blurriness for individual pixels in a given image. Finally, the estimated blur map is used to segment the input image into blurred/non-blurred regions by applying a pixon-based technique. The proposed approach is evaluated for out-of-focus and motion-blurred natural images. By conducting experiments on a large dataset containing real images with defocus blur and partial motion blur regions, qualitative and quantitative measures are performed. The obtained results in this paper show that the proposed approach outperforms the state-of-the-art methods for blur estimation in digital images.	box blur;computer vision;deblurring;digital image;encode;effective method;experiment;gaussian blur;image retrieval;pixel;sensor	Taiebeh Askari Javaran;Hamid Hassanpour;Vahid Abolghasemi	2015	The Visual Computer	10.1007/s00371-015-1166-z	image restoration;computer vision;computer graphics (images)	Vision	45.0551011907314	-54.05221152051647	169511
1fd798bd71df03dbd9e61f2fd05569d81d9058df	palm print recognition using oriented hausdorff distance transform	palm print;image matching;image edge detection databases feature extraction transforms robustness biometrics lighting;robust oriented hausdorff similarity palm print recognition oriented hausdorff distance transform personal recognition palm print matching rohs;transforms image matching palmprint recognition;similarity measure hausdorff distance identification palm print region of interest;region of interest;identification;transforms;palmprint recognition;hausdorff distance;human identification;similarity measure	Out of various biometrics being used for personal recognition, palm-print matching has gained vast acceptance due to its reliability and practicability. This paper proposes a robust palm-print recognition algorithm for human identification. The algorithm is based on Robust Oriented Hausdorff Similarity (ROHS) measure. The use of ROHS in matching is comparatively more tolerant to noise and occlusion as compared to the traditional Hausdorff distance based matching. A region of interest (128X128) is extracted from the original image and ROHS based matching is performed. The experimental results demonstrate the practicability of this algorithm.	algorithm;biometrics;distance transform;hausdorff dimension;palm print;region of interest	MianMahmood Ali;Mubeen Ghafoor;Imtiaz A. Taj;Khizar Hayat	2011	2011 Frontiers of Information Technology	10.1109/FIT.2011.23	computer vision;speech recognition;pattern recognition;mathematics	Vision	39.92460115424859	-58.78253317977675	169997
416691d8a42ae0d6ef607981afe97385313c090c	spatiotemporal motion analysis for the detection and classification of moving targets	motion analysis;moving target detection;object recognition;vision ordenador;base de datos temporal;streaming;video surveillance;image motion analysis;estimation mouvement;video streaming;object recognition spatiotemporal motion analysis moving target detection moving target classification video surveillance video stream spatiotemporal oriented energy analysis;modele agrege;supervised learning;analisis datos;surveillance;real time;base donnee temporelle;estimacion movimiento;modelo agregado;image classification;analyse mouvement;video surveillance feature extraction image classification image motion analysis object recognition video streaming;condicion estacionaria;motion estimation;condition stationnaire;blanco movil;moving target classification;classification;motor cycle;spatial database;spatiotemporal phenomena motion analysis motion detection video surveillance cameras streaming media real time systems bicycles motorcycles vehicles;computer vision;data analysis;detection mouvement;transmission en continu;motorcycles;vigilancia;senal video;signal video;streaming media;motocicleta;spatiotemporal oriented energy analysis;feature extraction;bicycles;temps reel;analyse energetique;spatiotemporal motion analysis;energy analysis;spatiotemporal phenomena;stationary condition;deteccion movimiento;aggregate model;motocyclette;video signal;tiempo real;crowd;cible mobile;analyse donnee;spatiotemporal analysis;vision ordinateur;temporal databases;base dato especial;video surveillance object classification spatiotemporal analysis;object classification;vehicles;transmision fluyente;apprentissage supervise;base de donnees spatiale;analisis movimiento;multitud;video stream;aprendizaje supervisado;motion detection;clasificacion;foule;analisis energetico;cameras;moving target;real time systems	This paper presents a video surveillance system in the environment of a stationary camera that can extract moving targets from a video stream in real time and classify them into predefined categories according to their spatiotemporal properties. Targets are detected by computing the pixel-wise difference between consecutive frames, and then classified with a temporally boosted classifier and ldquospatiotemporal-oriented energyrdquo analysis. We demonstrate that the proposed classifier can successfully recognize five types of objects: a person, a bicycle, a motorcycle, a vehicle, and a person with an umbrella. In addition, we process targets that do not match any of the AdaBoost-based classifier's categories by using a secondary classification module that categorizes such targets as crowds of individuals or non-crowds. We show that the above classification task can be performed effectively by analyzing a target's spatiotemporal-oriented energies, which provide a rich description of the target's spatial and dynamic features. Our experiment results demonstrate that the proposed system is extremely effective in recognizing all predefined object classes.	adaboost;categorization;closed-circuit television;naive bayes classifier;pixel;stationary process;streaming media	Duan-Yu Chen;Kevin J. Cannons;Hsiao-Rong Tyan;Sheng-Wen Shih;Hong-Yuan Mark Liao	2008	IEEE Transactions on Multimedia	10.1109/TMM.2008.2007289	computer vision;contextual image classification;simulation;feature extraction;biological classification;computer science;cognitive neuroscience of visual object recognition;machine learning;motion estimation;temporal database;supervised learning;data analysis;spatial database;computer graphics (images)	Vision	45.73074523736197	-57.33339359003029	170939
f9d3bce98d9ba72a5ad32c45d586af796b0c1397	localizing and extracting caption in news video using multi-frame average	edge detection;caption detection;news videos;multi frame average	News video is a very important video source. Caption in a news video can help us to understand the semantics of video content directly. A caption localization and extraction approach for news video will be proposed. This approach applies a new Multi-Frame Average (MFA) method to reduce the complexity of the background of the image. A time-based average pixel value search is employed and a Canny edge detection is performed to get the edge map. Then, a horizontal scan and a vertical scan on this edge map are used to obtain the top, bottom, left and right boundaries of the rectangles of candidate captions. Then, some rules are applied to confirm the caption. Experimental results show that the proposed approach can reduce the background complexity in most cases, and achieves a high precision and recall. Finally, we analyze the relationship between background variation of frame sequence and detection performance in detail.	frame language	Jinlin Guo;Songyang Lao;Haitao Liu;Jiang Bu	2008		10.3233/978-1-58603-904-2-46	computer vision;computer science;multimedia;computer graphics (images)	Vision	39.57166981140028	-52.76823707929409	170960
717fa4f551b2a5f7f890a72620be3c3ca5916f5f	personal identification by extracting sift features from laser speckle patterns	transforms feature extraction image recognition;identification system;image recognition;speckle;optical reflection;recognition;optical imaging;sift;recognition laser speckle patterns identification system sift;personal identification card sift feature extraction laser speckle patterns unique object feature extraction scale invariant feature transform algorithm invariant speckle capturing device recognition criteria security applications;feature extraction;speckle feature extraction adaptive optics optical imaging optical sensors surface emitting lasers optical reflection;transforms;optical sensors;surface emitting lasers;adaptive optics;laser speckle patterns	This paper presents a novel personal identification method by extracting unique object features from optical speckle patterns using the SIFT (Scale Invariant Feature Transform) algorithm. Accurate identification is achieved by developing an invariant speckle capturing device and recognition criteria. Experimental results show that optical speckle pattern of a given material is invariant after slight movement and the patterns captured from different areas of the same material are distinct. Therefore, this merit can be adopted for security applications by using the surface of specific object as the personal identification card and extracting speckle patterns from this surface to recognize the identity of certain subject.	algorithm;computer vision;displacement mapping;feature extraction;feature recognition;random sample consensus;scale-invariant feature transform	Chih-Ming Liao;Ping Sheng Huang;Chung-Cheng Chiu;Yi-Yuh Hwang	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288138	speckle pattern;computer vision;feature extraction;computer science;optical imaging;scale-invariant feature transform;adaptive optics	Robotics	40.68101620179014	-55.84392512084895	171189
a3be66d56ac2cc93e2981874dddaa22ffd2e6cc1	adaptive shadow detection using global texture and sampling deduction	high detection accuracy;high detection accuracy global texture sampling deduction adaptive shadow detection algorithm interference elimination object detection yuv colour space adaptive threshold estimator adaptive capacity lighting conditions edge detection method statistical calculations real time moving shadow detection;adaptive shadow detection algorithm;global texture;adaptive threshold estimator;real time moving shadow detection;yuv colour space;lighting conditions;edge detection method;adaptive capacity;statistical analysis edge detection image colour analysis image matching image sampling image texture interference suppression lighting object detection real time systems;interference elimination;object detection;sampling deduction;statistical calculations	An adaptive shadow detection algorithm is proposed to eliminate interference on object detection from the shadow. The algorithm uses three components in YUV colour space to identify shadow pixels from the candidate foreground. An adaptive threshold estimator is designed to improve shadow detection accuracy and adaptive capacity in various lighting conditions. This estimator uses edge detection method to obtain global texture, as well statistical calculations to obtain the thresholds. Algorithm has the characteristic of low complexity and little restraint; hence it is suitable for real time-moving shadow detection in various lighting conditions. Experiment results show that this algorithm can obtain a high detection accuracy and the time-assume is greatly shortened compared with other algorithms with similar accuracy.	natural deduction;sampling (signal processing)	Ke Jiang;Aihua Li;Zhigao Cui;Tao Wang;Yanzhao Su	2013	IET Computer Vision	10.1049/iet-cvi.2012.0106	computer vision;mathematics;computer graphics (images)	Vision	44.61796215917689	-54.29973060657744	171295
3b032c8107c3d3d8e9c38ef71555209dafea355e	selecting distinctive 3d shape descriptors for similarity retrieval	geometric matching;design automation;local shape descriptor;shape database;query processing;application software;shape descriptor;computer graphics;information retrieval;image matching;global shape descriptor;visual databases computational geometry gaussian distribution image matching image retrieval;computational geometry;testing;similarity retrieval;multivariate gaussian distribution;shape;shape spatial databases information retrieval application software computer graphics computer science phase estimation gaussian distribution testing design automation;shape similarity;phase estimation;spatial databases;shape distinction;shape distinction shape database global shape descriptor local shape descriptor query processing multivariate gaussian distribution shape retrieval similarity retrieval geometric matching;shape retrieval;computer science;gaussian distribution;visual databases;image retrieval	Databases of 3D shapes have become widespread for a variety of applications, and a key research problem is searching these databases for similar shapes. This paper introduces a method for finding distinctive features of a shape that are useful for determining shape similarity. Although global shape descriptors have been developed to facilitate retrieval, they fail when local shape properties are the distinctive features of a class. Alternatively, local shape descriptors can be generated over the surface of shapes, but then storage and search of the descriptors becomes unnecessarily expensive, as perhaps only a few descriptors are sufficient to distinguish classes. The challenge is to select local descriptors from a query shape that are most distinctive for retrieval. Our approach is to define distinction as the retrieval performance of a local shape descriptor. During a training phase, we estimate descriptor likelihood using a multi-variate Gaussian distribution of real-valued shape descriptors, evaluate the retrieval performance of each descriptor from a training set, and average these performance values at every likelihood value. For each query, we evaluate the likelihood of local shape descriptors on its surface and lookup the expected retrieval values learned from the training set to determine their predicted distinction values. We show that querying with the most distinctive shape descriptors provides favorable retrieval performance during tests with a database of common graphics objects	approximation algorithm;cluster analysis;database;definite clause grammar;experiment;feature vector;graphics;lookup table;randomness;shape analysis (digital geometry);shape context;statistical classification;test set	Philip Shilane;Thomas A. Funkhouser	2006	IEEE International Conference on Shape Modeling and Applications 2006 (SMI'06)	10.1109/SMI.2006.34	active shape model;computer vision;pattern recognition;mathematics;information retrieval	Vision	39.48934275733294	-58.03139491184199	171550
fcde913e10ac58fae81f938139e9f0072e0d7025	on a feature extraction by lmcuh algorithm for a ubiquitous computing	informatique mobile;image processing;occlusion;facies;color space;pervasive computing;occultation;extraction forme;procesamiento imagen;oclusion;traitement image;informatica difusa;histogram;histogramme;extraccion forma;informatique diffuse;eclairage;feature extraction;pattern recognition;ubiquitous computing;lighting;reconnaissance forme;extraction caracteristique;reseau neuronal;reconocimiento patron;ocultacion;mobile computing;imagen color;histograma;pattern extraction;red neuronal;image couleur;training algorithm;artificial neural network;color image;alumbrado;neural network;ubiquitous computing environment	This paper proposes an algorithm to detect human faces under various environments. In the first step, information on three color spaces of various features is used to determine histogram of color in the first frame of an image. The histogram obtained by interpolation after combining three color of the image is used as an input of LMCUH network. In the second step, the neural network of Levenberg – Marquadt training algorithm minimizes the error. Next, we find the face in test image by using the trained sets. This method is especially suited for various scales, rotations, lighting levels, or occlusions of the target image. Experimental results show that two – dimensional images of a face can be effectively implemented by using artificial neural network training under various environments. Thus, we can detect the face effectively and this can inevitably lead to the Ubiquitous Computing Environment.		Jin Ok Kim;Jun Yeong Jang;Chin Hyun Chung	2006		10.1007/11751540_105	computer vision;color image;facies;occultation;feature extraction;computer science;artificial intelligence;machine learning;lighting;histogram;color space;ubiquitous computing;artificial neural network;computer graphics (images)	HCI	44.83841676679024	-59.03904955491695	171843
c1206330ba57a27fd0acc29dd39825bbf2d91060	a shape reconstructability measure of object part importance with applications to object detection and localization	object recognition and detection;shape reconstruction;shape part;part importance	We propose a computational model which computes the importance of 2-D object shape parts, and we apply it to detect and localize objects with and without occlusions. The importance of a shape part (a localized contour fragment) is considered from the perspective of its contribution to the perception and recognition of the global shape of the object. Accordingly, the part importance measure is defined based on the ability to estimate/recall the global shapes of objects from the local part, namely the part’s “shape reconstructability”. More precisely, the shape reconstructability of a part is determined by two factors–part variation and part uniqueness. (i) Part variation measures the precision of the global shape reconstruction, i.e. the consistency of the reconstructed global shape with the true object shape; and (ii) part uniqueness quantifies the ambiguity of matching the part to the object, i.e. taking into account that the part could be matched to the object at several different locations. Taking both these factors into consideration, an information theoretic formulation is proposed to measure part importance by the conditional entropy of the reconstruction of the object shape from the part. Experimental results demonstrate the benefit with the proposed part importance in object detection, including the improvement of detection rate, localization accuracy, and detection efficiency. By comparing with other state-of-the-art object detectors in a challenging but common scenario, object detection with occlusions, we show a considerable improvement using the proposed importance measure, with the detection rate increased over $$10~\%$$ 10 % . On a subset of the challenging PASCAL dataset, the Interpolated Average Precision (as used in the PASCAL VOC challenge) is improved by 4–8 %. Moreover, we perform a psychological experiment which provides evidence suggesting that humans use a similar measure for part importance when perceiving and recognizing shapes.	biconnected component;computation;computational model;conditional entropy;experiment;human reliability;int (x86 instruction);information retrieval;information theory;interpolation;mathematical model;object detection;sensor;supervised learning;visual instruction set	Ge Guo;Yizhou Wang;Tingting Jiang;Alan L. Yuille;Fang Fang;Wen Gao	2014	International Journal of Computer Vision	10.1007/s11263-014-0705-9	active shape model;computer vision;shape analysis;mathematics;geometry	Vision	42.90123363845839	-54.79859529061565	171861
4f0a52efa1b0febfbc5a0cde2c9cb50ceecac990	an approach to the segmentation of textured dynamic scenes		An approach to the segmentation of dynamic scenes containing textured objects moving against a textured background is presented. This multistage approach first uses differencing to obtain active regions in the frame which contain moving objects. In the next stage, a Hough transform technique is used to determine the motion parameters associated with each active region. Finally, the intensity changes and the motion parameters are combined to obtain the masks of the moving objects. Our experiments illustrate the efficacy of the approach for moving textured objects even in the presence of occlusion. An indicator that signals the presence of the rotational component of the object motion can also be extracted.		S. N. Jayaramamurthy;Ramesh Jain	1982	Computer Graphics and Image Processing	10.1016/0146-664X(82)90050-8	computer science	Graphics	45.320826368575155	-55.282699099964844	172136
33f87a6190d8bd0c00ed985b6f905ef3e308cbb9	shrec’08 entry: local volumetric features for 3d model retrieval	3d local features shrec 08 entry local volumetric features 3d model retrieval voxel representation 2d image feature;2d image feature;image features;shrec 08 entry;local shape descriptor;image retrieval content based retrieval feature extraction image representation;generic model;shape descriptor;voxel representation;image converters;information retrieval;information filtering;local volumetric features;computational geometry;multi scale feature;3d local features;indexing terms;h 3 3 information search and retrieval information filtering;i 3 5 computational geometry and object modeling surface based 3d shape models;3d model retrieval;3d model;scale invariant feature transform;local features;image representation;feature extraction;solid modeling;i 4 8 scene analysis object recognition content based retrieval multi scale feature local shape descriptor voxel representation scale invariant feature transform h 3 3 information search and retrieval information filtering i 3 5 computational geometry and object modeling surface based 3d shape models;i 4 8 scene analysis object recognition;rendering computer graphics;computational efficiency;content based retrieval;information search and retrieval;feature extraction shape computational efficiency solid modeling rendering computer graphics image converters content based retrieval information retrieval information filtering computational geometry;image retrieval	In this paper, we describe a method of shape-based 3D model retrieval that employs a set of 3D, local, multi-scale features extracted from a voxel representation of a 3D model to be compared. The method first convert a surface based 3D model into a voxel model. Then, a novel 3D extension of the popular 2D image feature, the scale invariant feature transform by David Lowe, is applied to extract a set of 3D local features. The 3D feature is invariant to rotation, uniform scaling, and translation in 3D space. A 3D model typically yields thousands of such local 3D features. Our method extracts thousands of 3D local features from a model to compare its shape. Our evaluation showed that the method is quite effective, achieving Means First Tier of 58% for the Query Set 1 of the SHREC'08 Generic Models Track.	3d modeling;feature (computer vision);image scaling;multitier architecture;polygonal modeling;scale-invariant feature transform;voxel	Kunio Osada;Takahiko Furuya;Ryutarou Ohbuchi	2008	2008 IEEE International Conference on Shape Modeling and Applications	10.1109/SMI.2008.4547989	computer vision;computer science;pattern recognition;information retrieval	Vision	39.27467206896787	-58.15684455367952	172578
30089f3d19b03f424983c204d44f14aaa70ccc75	a 3d surface matching method using keypoint- based covariance matrix descriptors		A 3D surface is considered one of the most promising tools for representing and recognizing 3D objects. Therefore, 3D surface matching is widely applied to 3D object recognition, retrieval, and so on. In this paper, a 3D surface matching method using a keypoint-based covariance matrix descriptor is proposed, whose purpose is to find correspondences between 3D surfaces (e.g., 3D model and scene) by matching feature points, which are highly repeatable keypoints described by a multi-scale covariance matrix descriptor. A keypoint is detected by analyzing the surface variation index and eigenvalue variation index of a local neighborhood centered at a point. A multi-scale covariance matrix descriptor of a keypoint describes the geometric relation, surface variation gradient, and eigenvalue variation gradient between the keypoint and its neighborhood. The rationale for adopting the keypoint-based covariance matrix descriptor in our proposed 3D surface matching method is that a small number of keypoints with high repeatability can greatly enhance the matching effect of a 3D surface, after being described by a multi-scale covariance matrix descriptor with high descriptiveness. The experimental results also show that our proposed keypoint detection algorithm has higher repeatability than the surface variation index-based and eigenvalue variation index-based detection algorithms; our proposed multi-scale covariance matrix descriptor has higher descriptiveness than spin image, PFH, and 3DSC; our proposed bidirectional nearest-neighbor distance ratio algorithm can obtain better feature matching effect than the nearest-neighbor-based and nearest-neighbor distance-ratio-based feature matching. Finally, our proposed 3D surface matching method has a better matching effect than 3D surface matching methods based on spin image, PFH, and 3DSC on the Stanford 3D Scanning Repository, UWA data set and Bologna data set.	3d computer graphics;3d scanner;3d single-object recognition;algorithm;bidirectional search;design rationale;gradient;outline of object recognition;polygonal modeling;repeatability	Xiong Fengguang;Han Xie	2017	IEEE Access	10.1109/ACCESS.2017.2727066	repeatability;feature extraction;eigenvalues and eigenvectors;covariance matrix;small number;spin-½;computer vision;mathematics;solid modeling;pattern recognition;artificial intelligence	Vision	39.7510811843521	-57.2201212133082	172979
b4facda3a3fc16113d740362c885ae1afcacc860	real-time iris segmentation based on image morphology	adaptive thresholding;connected components;real time;adaptive threshold;spectrum;iris segmentation;spectrum image and circular hough transform;specular reflection;hough transform;connected component	This paper introduces an efficient iris segmentation approach for unconstrained images. Proposed technique is robust to occlusion, specular reflection, variation in illumination and non-centered gaze. For pupil localisation, the input iris image is binarised using an adaptive threshold determined based on number of connected components. Further, pupil center and radius are obtained using spectrum image based approach. The proposed technique performs accurately (>97%) with low computation (<0.4 seconds/image). It has been observed that the proposed approach can be deployed to real-time biometric systems where time as well as accuracy cannot be compromised.	biometrics;computation;connected component (graph theory);galaxy morphological classification;hidden surface determination;image segmentation;real-time clock;real-time transcription	Sambit Bakshi;Hunny Mehrotra;Banshidhar Majhi	2011		10.1145/1947940.1948010	computer vision;mathematics;optics;scale-space segmentation;computer graphics (images)	Vision	45.13813445152289	-55.119237042533065	173433
52a95245d71734cca702b5ce9e07851823e5f34f	statistical and geometric methods for shape-driven segmentation and tracking	active contours;computer vision;samuel;dissertation;electrical engineering statistical and geometric methods for shape driven segmentation and tracking georgia institute of technology allen tannenbaum dambreville;variational methods	Computer Vision aims at developing techniques to extract and exploit information from images. The successful applications of computer vision approaches are multiple and have benefited diverse fields such as manufacturing, medicine or defense. Some of the most challenging tasks performed by computer vision systems are arguably segmentation and tracking. Segmentation can be defined as the partitioning of an image into homogeneous or meaningful regions. Tracking also aims at extracting meaning or information from images, however, it is a dynamic task that operates on temporal (video) sequences. Active contours have been proven to be quite valuable at performing the two aforementioned tasks. The active contours framework is an example of variational approaches, in which a problem is compactly (and elegantly) described and solved in terms of energy functionals. The objective of the proposed research is to develop statistical and shape-based tools inspired from or completing the geometric active contours methodology. These tools are designed to perform segmentation and tracking. The approaches developed in the thesis make an extensive use of partial differential equations and differential geometry to address the problems at hand. Most of the proposed approaches are cast into a variational framework. The contributions of the thesis can be summarized as follows: (1) An algorithm is presented that allows one to robustly track the position and the shape of a deformable object. (2) A variational segmentation algorithm is proposed that adopts a shape-driven point of view. (3) Diverse approaches are introduced for including prior knowledge on shapes in the geometric active contour framework. (4) A framework is proposed that combines statistical information extracted from images with shape information learned a priori from example. (5) A technique is developed to jointly segment a 3D object of arbitrary shape in a 2D image and estimate its 3D pose with respect to a referential attached to a unique calibrated camera. (6) A methodology for the non-deterministic evolution of curves is presented, based on the theory of interacting particles systems.		Samuel Dambreville	2008			computer vision;simulation;computer science;artificial intelligence	Vision	40.094383562520825	-54.206014937997125	173632
70fdb0b122310f7c870cb025f915906e7df1fd0b	automatic generation of the statistical model of a non-rigid object in a multiple-camera environment	edge detection;statistical model;automatic generation;computer vision;optical tracking;principal component analysis;active shape model object detection mathematical model layout principal component analysis spline motion detection tracking humans deformable models;learning algorithm statistical model nonrigid object principal component analysis snakes dynamic contours multiple camera environment tracking;learning artificial intelligence;point of view;learning artificial intelligence computer vision optical tracking edge detection principal component analysis	A new method for modeling non-rigid objects in a multiple-camera guarded environment is proposed. The statistical model of the shape of a non-rigid object takes into account the correlations between different points of view. The model is automatically generated by processing a typical training set for the considered shape by a principal component analysis algorithm. Snakes and dynamic contours are used to describe the shape of the non-rigid object.	statistical model	Lucio Marcenaro;Carlo S. Regazzoni;Gianni Vernazza	2000		10.1109/ICPR.2000.905392	active shape model;statistical model;point distribution model;computer vision;active appearance model;edge detection;computer science;machine learning;pattern recognition;active contour model;principal component analysis	SE	45.94244660446026	-52.153050034346116	173823
d25b83ddac2034bf34049e8cbb712fb8498ef0b6	recognition of human front faces using knowledge-based feature extraction and neurofuzzy algorithm	fuzzy membership function;logique floue;hombre;logica difusa;backpropagation;fuzzy logic;retropropagation;a priori knowledge;face recognition;feature extraction;neuro fuzzy;human;membership function;pattern recognition;face;sistema difuso;reconnaissance forme;systeme flou;reseau neuronal;reconocimiento patron;retropropagacion;feature membership function;computer simulation;red neuronal;fuzzy system;homme;neural network;knowledge base;cara	Abstract   A recognition method of human front faces using knowledge-based feature extraction and a neuro-fuzzy algorithm is proposed. In the preprocessing step we extract the face part from the homogeneous background by tracking face boundaries, where we assume that the face part is located in the center of a captured image. Then, based on a priori knowledge of human faces, we extract five normalized features. In the recognition step we propose a neuro-fuzzy algorithm that employs a trapezoidal fuzzy membership function and modified error backpropagation (EBP) algorithm. The former absorbs variation of feature values and the latter shows good learning efficiency.  Computer simulation results with 80 test images of 20 persons show that the proposed neuro-fuzzy method yields higher recognition rate than the conventional ones.	algorithm;feature extraction	S. Y. Lee;Young Kug Ham;Rae-Hong Park	1996	Pattern Recognition	10.1016/0031-3203(96)00030-1	computer simulation;fuzzy logic;face;computer vision;knowledge base;a priori and a posteriori;membership function;feature extraction;computer science;artificial intelligence;backpropagation;neuro-fuzzy;machine learning;pattern recognition;algorithm	Vision	44.05760859195835	-59.0528253845927	174806
b43fbac58abebce257237cb9155d2975304e6013	linear regression for head pose analysis	3d face linear regression pose analysis human face;imm face database linear regression head pose analysis head pose estimation facial feature detection landmarks location face parts occlusion illumination face images pose space 3d reconstruction 3d projection method cmu multipie face database;regression analysis face recognition feature extraction image reconstruction pose estimation;face image reconstruction three dimensional displays linear regression estimation training facial features	Extensive research has been conducted to estimate and analyze head poses for various applications. Most existing methods tend to detect facial features and locate landmarks on a face for pose estimation. However, the sensitivity to occlusion of some face parts with key features and uncontrolled illumination of face images make the facial feature detection vulnerable. In this paper, we propose a framework for pose estimation without the need of face features or landmarks detection. Specifically, we formulate the pose estimation as a linear regression applied to the pose space. This method is based on the assumption that pose space cannot be linearly approximated in the pose subspace. The experimental results strongly support this assumption. In cases where the database does not obtain various poses in the intraclass, we propose to generate those poses through a 3D reconstruction and projection method. The experiment conducted on the CMU MultiPIE and IMM Face database has shown the effectiveness of the proposed method.	3d pose estimation;3d reconstruction;approximation algorithm;feature detection (computer vision);feature detection (web development);illumination (image);uncontrolled format string	Huiye Qiu;Honghai Liu	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889676	computer vision;speech recognition;3d pose estimation;pattern recognition	Vision	42.76795446832221	-52.424263742335995	175157
f7f93b25c3e9f5573353f6c1809179b3248d1e48	a back projection scheme for accurate mean shift based tracking	histograms;multiple feature fusion mean shift tracking;image motion analysis;image fusion;mean shift based tracking;mean shift;pixel histograms face target tracking image color analysis shape;similarity assessment;multiple feature fusion back projection scheme mean shift based tracking object tracking histogram bin similarity assessment;shape;image color analysis;feature extraction;pixel;object tracking;object tracking feature extraction image fusion image motion analysis;face;feature fusion;back projection scheme;target tracking;multiple feature fusion;tracking;histogram bin	A new scheme for back-projection of weights for mean shift based object tracking is proposed. Weights are calculated based on relative counts of histogram bins for each feature used in similarity assessment. A fusion scheme is proposed to combine the back-projected weights from different features, such that the dissimilarities between the object being tracked and the background are boosted. A mechanism is proposed to calculate the overall similarity between reference and candidate windows, without actually doing the back-projection, and just by few computations based on histogram bin counts. Moreover, the reference and candidate windows do not need to have the same size. The proposed scheme shows better tracking results compared to the traditional mean shift based tracking, especially in case of fast object movement and high background clutter.	clutter;computation;mean shift;microsoft windows	Ishtiaq Rasool Khan;Farzam Farbiz	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652577	computer vision;computer science;machine learning;pattern recognition;histogram;mathematics;statistics	Vision	41.18796462896127	-54.82390267484161	175356
530ffd73c2787f550cd78d5244c6e442a38ecd83	a feature point matching based on spatial order constraints bilateral-neighbor vote	wgtm computer vision application robust feature point matching algorithm spatial order constraints bilateral neighbor vote socbv nearest k neighbor graph knn graph binary discrimination problem inlier class probability knn density estimation posterior class probability associative inliers set directed graph ransac rsoc gtm soc;convergence;estimation;image edge detection;system on chip;bilateral neighbour vote feature point matching directed nearestneighbour graph spatial order constraints;robustness;probability computer vision directed graphs feature extraction image matching matrix algebra;system on chip algorithm design and analysis robustness convergence image edge detection estimation labeling;algorithm design and analysis;labeling	Feature point matching is a fundamental and challenging problem in many computer vision applications. In this paper, a robust feature point matching algorithm named spatial order constraints bilateral-neighbor vote (SOCBV) is proposed to remove outliers for a set of matches (including outliers) between two images. A directed k nearest neighbor (knn) graph of match sets is generated, and the problem of feature point matching is formulated as a binary discrimination problem. In the discrimination process, the class labeled matrix is built via the spatial order constraints defined on the edges that connect a point to its knn. Then, the posterior inlier class probability of each match is estimated with the knn density estimation and spatial order constraints. The vote of each match is determined by averaging all posterior class probabilities that originate from its associative inliers set and is used for removing outliers. The algorithm iteratively removes outliers from the directed graph and recomputes the votes until the stopping condition is satisfied. Compared with other popular algorithms, such as RANSAC, RSOC, GTM, SOC and WGTM, experiments under various testing data sets demonstrate strong robustness for the proposed algorithm.	bilateral filter;computer vision;directed graph;experiment;generative topographic map;graph - visual representation;k-nearest neighbors algorithm;matching;meddra system organ class;muscle rigidity;name;neighbourhood (graph theory);probability;random sample consensus;single linkage cluster analysis;system on a chip;tracer	Fanyang Meng;Xia Li;Jihong Pei	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2456633	system on a chip;algorithm design;estimation;labeling theory;combinatorics;convergence;computer science;machine learning;pattern recognition;mathematics;statistics;robustness	Vision	44.921742875168036	-52.9473803550552	175698
aa6010bf6d4440a9f950cd42f89df41bfb14d608	an accurate feature point matching algorithm for automatic remote sensing image registration		Remote sensing image registration is still a challenging task because of diverse image types and the lack of a consistent transformation. To improve image registration in remote sensing, this paper develops a robust and accurate feature point matching framework. A modified scale-invariant feature transform (SIFT) method is first introduced for feature detection and pair matching. Based on the properties of matched pairs, the standard grouping boundary (SGB) and confidence elliptical boundary (CEB) are computed for further examination. The SGB is utilized to categorize matched pairs according to the pair slopes. The CEB is further employed to remove outliers whose feature positions are outside the elliptical contour. Finally, the random sample consensus (RANSAC) approach is implemented to acquire the most appropriate transformation function for image registration. The proposed image registration algorithm has been tested on numerous multi-temporal remote sensing images. Experimental results validated the improvement in feature matching accuracy, which resulted in better registration performance over state-of-the-art methods. This new framework is of potential in many remote sensing applications that require automatic image registration.	algorithm;categorization;feature detection (computer vision);feature detection (web development);image registration;random sample consensus;ssi ceb;scale-invariant feature transform;super game boy	Guan-Long Wu;Herng-Hua Chang	2017	2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2017.8227403	robustness (computer science);artificial intelligence;transformation (function);point set registration;ransac;computer vision;pattern recognition;algorithm;remote sensing;computer science;feature extraction;image registration;remote sensing application;scale-invariant feature transform	Vision	41.989679063052925	-56.13856318222828	175878
37b5bee10853187bce63e079cb66f77f78a4c311	edge plane detection in spatio-temporal images by using edge vector and edge reliability	object recognition;stereo image processing object recognition edge detection feature extraction vectors motion estimation;3d imaging;edge detection;motion estimation;feature extraction edge detection 3d images edge vector edge reliability spatio temporal images object recognition depth estimation;vectors;feature extraction;stereo image processing;image edge detection detectors layout surface fitting data mining object recognition object detection computer vision image recognition stacking;connected component	We propose a method of edge plane detection in 3D images by utilizing features of edge vector and edge reliability. This method can detect only reliable edge planes against noise influences. For the 2D case, we proposed an edge detection method which calculates edge vectors and edge reliability and can detect reliable edges at more accurate positions than Canny's method. This 2D edge vector detector is generalized to the 3D case. In applying this method to a 3D image, edge vectors and edge reliability are calculated. For finding edge surfaces (3D boundaries), the connected components of edge points detected from those features are composed by labeling. Finally, each edge surface is classified into edge planes using the directions of edge vectors. The proposed method was applied to spatio-temporal images and the depths of ridges in the 3D scene were estimated from the detected edge planes.		Fumiaki Yamana;Takahiro Sugiyama;Keiichi Abe	2000		10.1109/ICPR.2000.903631	stereoscopy;computer vision;connected component;edge detection;image gradient;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;deriche edge detector;pattern recognition;motion estimation;mathematics;canny edge detector	Vision	44.684593774155665	-54.72892105133819	176002
74fdd5480ca0f76e34ad0283f75e5b72b4e86a6b	robust shadow removal algorithm for accurate object detection based on background subtraction method	bsm robust shadow removal algorithm accurate object detection background subtraction method illumination condition shadow candidate region background image;image recognition;image segmentation;background subtraction;object detection image recognition image segmentation;object detection;vectors image color analysis lighting object detection search problems motion estimation conferences	This paper presents a new shadow removal algorithm for accurate object detection based on the background subtraction method. The algorithm requires no threshold to determine shadow regions, whereas the conventional algorithms suffer from specifying the thresholds for various illumination conditions. It finds shadow regions by means of measuring relative similarities between shadow candidate regions and a background image given by the BSM. We empirically verify that it is very effective to remove the shadow regions.	algorithm;background subtraction;object detection;openbsm	Yun-Ho Ko;Jong-Won Park;Hyun-Soo Kang	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161780	computer vision;background subtraction;shadow and highlight enhancement;computer science;image segmentation;computer graphics (images)	Robotics	44.44225202700718	-52.69367390023001	176352
454f1a768508f6a3ff10e12cc048f03beefe2269	determining structure in continuously recorded videos	fuzzy representation;spectral clustering;scene detection	In this paper, we present a scene detection framework on continuously recorded videos. Conventional temporal scene segmentation methods work for the videos composed of discrete shots, where shot boundaries are clearly defined. The proposed method detects scene segments by the spectral clustering technique and fuzzy analysis. The detected scenes are represented by the corresponding representative feature values of the feature clusters, rather than abrupt temporal boundaries. The feature clusters are generated using the spectral clustering technique. The video units have the fuzzy memberships to the feature clusters, which are generated using the Hyperbolic tangent fuzzy function. The final output is collected from the candidate scenes from all clusters. The proposed method has been tested on several video sequences, and very promising results have been obtained.	cluster analysis;horner's method;scene graph;spectral clustering	Yun Zhai;Mubarak Shah	2005		10.1145/1101149.1101260	computer vision;fuzzy clustering;flame clustering;computer science;machine learning;pattern recognition;spectral clustering	Vision	39.27783984394376	-52.4964168838669	176442
fa33417eb5f998df69cdaa37cf911b88820cab20	analyzing invariance of frequency domain based features from videos with repeating motion		This paper discusses an approach, which allows classifying videos by frequency spectra. Many videos contain activities with repeating movements. Sports videos, home improvement videos, or videos showing mechanical motion are some example areas. Motion of these areas usually repeats with a certain main frequency and several side frequencies. Transforming repeating motion to its frequency domain via FFT reveals these frequency features. In this paper we explain how to compute frequency features for video clips and how to use them for classifying. The experimental stage of this work focuses on the invariance of these features with respect to rotation, reflection, scaling, translation and time shift.	fast fourier transform;image moment;image scaling;k-nearest neighbors algorithm;reflection (computer programming);role-based collaboration;timeshift;video clip	Kahraman Ayyildiz;Stefan Conrad	2012			frequency domain;pattern recognition;computer vision;artificial intelligence;invariant (physics);computer science	Vision	39.55586522353575	-53.45960191573456	176866
02194d808ab94a7f24a839edf851116da33f28be	sar image segmentation based on hierarchical visual semantic and adaptive neighborhood multinomial latent model	image segmentation;remote sensing by radar geophysical image processing image coding image segmentation linear codes pattern clustering radar imaging;semantics;visualization;urban areas;sar image segmentation structural regions line object preservation edge preservation adaptive window selection geometric structure window based multinomial latent model line object location locality constrained linear coding based hierarchical clustering homogeneous region image gray value urban areas synthetic aperture radar imaging system adaptive neighborhood multinomial latent model hierarchical visual semantic;image segmentation synthetic aperture radar semantics visualization urban areas adaptation models context modeling;adaptation models;context modeling;synthetic aperture radar sar image segmentation adaptive neighborhood hierarchical visual semantic multinomial latent model regional map;synthetic aperture radar	A synthetic aperture radar (SAR) imaging system usually produces pairs of bright area and dark area when depicting the ground objects, such as a building or tree and its shadow. Many buildings (trees) are aggregated together to form urban areas (forests). It means that the pairs of bright and dark areas often exist in the aggregated scenes. Conventional unsupervised segmentation approaches usually segment the scenes (e.g., urban areas and forests) into different regions simply according to the gray values of the image. However, a more convincing way is to regard them as the consistent regions. In this paper, we aim at addressing this issue and propose a new SAR image segmentation approach via a hierarchical visual semantic and adaptive neighborhood multinomial latent model. In this approach, the hierarchical visual semantic of SAR images is proposed, which divides SAR images into aggregated, structural, and homogeneous regions. Based on the division, different segmentation methods are chosen for these regions with different characteristics. For the aggregated region, locality-constrained linear coding-based hierarchical clustering is used for segmentation. For the structural region, visual semantic rules are designed for line object location, and a geometric structure window-based multinomial latent model is proposed for segmentation. For the homogeneous region, a multinomial latent model with adaptive window selection is proposed for segmentation. Finally, these results are integrated together to obtain the final segmentation. Experiments on both synthetic and real SAR images indicate that the proposed method achieves promising performances in terms of the consistencies of the regions and the preservations of the edges and line objects.	apache axis;aperture (software);cluster analysis;computer vision;degree (graph theory);digimon;emoticon;experiment;hierarchical clustering;human visual system model;image histogram;image segmentation;linear code;locality of reference;memory segmentation;multinomial logistic regression;performance;synthetic data;synthetic intelligence;trust region;window function;xfig	Fang Liu;Yiping Duan;Lingling Li;Licheng Jiao;Jie Wu;Shuyuan Yang;Xiangrong Zhang;Jialing Yuan	2016	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2016.2539155	computer vision;synthetic aperture radar;visualization;segmentation-based object categorization;pattern recognition;semantics;region growing;context model;image segmentation;scale-space segmentation;remote sensing	Vision	44.08570100523158	-56.17065716476863	177302
9cde9f03290e9cdbc5702b94a6a6ff7dfd665ec9	hierarchical search of optimal limbus circle matching for gaze tracking systems		The core technology of visible light gaze tracking (VLGT) system is the determination of iris contour (limbus circle) on the eye image. In this paper, we proposed a hierarchical search scheme for the matching between the ellipse models and the iris contour on the eye image. The experimental results show the proposed hierarchical search scheme efficiently matches the rotating models of human eyes with the limbus circle on an input image.	eye tracking;tracking system	Wen-Chung Kao;Yi-Chin Chiu	2017	2017 IEEE 7th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2017.8210635	gaze;computer vision;visible spectrum;ellipse;tracking system;artificial intelligence;computer science	Vision	41.71992188166991	-57.91798242435878	177359
d478258f94ae46980364c7459bc8804bae74d2e5	detecting human heads and face orientations under dynamic environment	reconnaissance visage;image features;cabeza;facies;edge detection;deteccion contorno;detection contour;dynamic environment;detection mouvement;human face;face recognition;deteccion movimiento;methode deux pas;head;tete;false positive;visage humain;motion detection	We propose a two-step method for detecting human heads and estimating face orientations under the dynamic environment. In the first step, the method employs an ellipse as the contour model of humanhead appearances to deal with wide variety of appearances. Our method then evaluates the ellipse to detect possible human heads. In the second step, on the other hand, our method focuses on features, such as eyes, the mouth or cheeks, inside the ellipse to model facial components. The method evaluates not only such components themselves but also their geometric configuration to eliminate false positives in the first step and, at the same time, to estimate face orientations. In the both steps, our method employs robust image-features against lighting conditions in evaluating the models. Consequently, our method can correctly and stably detect human heads and estimate face orientations even under the dynamic environment. Our intensive experiments show the effectiveness of the proposed method.	experiment;sensor;wavelet	Akihiro Sugimoto;Mitsuhiro Kimura;Takashi Matsuyama	2004		10.1007/978-3-540-30074-8_17	facial recognition system;computer vision;edge detection;facies;type i and type ii errors;computer science;head;feature;computer graphics (images)	Vision	45.86962370705254	-58.23818395449952	177636
2608c24d52c9a939a13ebf03bad0c023a096d71f	saliency detection via divergence analysis: a unified perspective	visual perception computer vision feature extraction object detection statistical distributions;divergence;silicon visualization detection algorithms feature extraction humans sun estimation;saliency detection;computer vision;statistical distributions;feature extraction;visual perception;bottom up visual saliency detection algorithms divergence analysis psychophysical study human vision feature distribution estimation saliency estimation algorithm center surround selection strategy;visual attention;object detection	A number of bottom-up saliency detection algorithms have been proposed in the literature. Since these have been developed from intuition and principles inspired by psychophysical studies of human vision, the theoretical relations among them are unclear. In this paper, we present a unifying perspective. Saliency of an image area is defined in terms of divergence between certain feature distributions estimated from the central part and its surround. We show that various, seemingly different saliency estimation algorithms are in fact closely related. We also discuss some commonly used center-surround selection strategies. Experiments with two datasets are presented to quantify the relative advantages of these algorithms.	approximation algorithm;bottom-up proteomics;experiment;vergence	Jia-Bin Huang;Narendra Ahuja	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		probability distribution;computer vision;visual perception;feature extraction;computer science;kadir–brady saliency detector;machine learning;pattern recognition;divergence	Vision	39.65619237924038	-54.210381401065646	178152
43010792bf5cdb536a95fba16b8841c534ded316	towards general motion-based face recognition	databases;human faces;appearance based face recognition;anthropometry;image motion analysis;image databases;legged locomotion;biometrics access control;biometrics;bridges;layout;psychology;human faces motion based face recognition appearance based face recognition facial motion local deformation profile video database biometrics motion based perception;computer vision;motion based perception;face recognition;shape;pixel;spatial databases;sun;local deformation profile;face;humans;face recognition layout humans sun image databases computer vision anthropometry bridges legged locomotion spatial databases;facial expression;psychology biometrics access control face recognition image motion analysis;motion based face recognition;motion measurement;video database;facial motion	Motion-based face recognition is a young research topic, inspired mainly by psychological studies on motion-based perception of human faces. Unlike its close relative, appearance-based face recognition, motion-based face recognition extracts personal characteristics from facial motion (e.g. smile) and uses the information to recognize human identity. However, existing studies in this field are limited to fixed motion, that is - a subject must perform a specific type of facial motion in order to be correctly recognized. In this paper, we try to overcome this limitation by investigating the patterns of local skin deformation exhibited in facial motion. We are pushing the state-of-the-art towards general motion-based face recognition. Our approach is able to extract identity evidence from various types of facial motion, as long as those facial motions are at least, in some part of the face, locally similar to the facial motions used in training. We call our approach Local Deformation Profile (or LDP). This approach is tested through several experiments conducted over a video database of facial expression. The experiment results demonstrate the potential of LDP to be used for biometrics. We also evaluate LDP under extremely heavy face makeup, showing its usefulness to recognize faces even in disguise.	biometrics;data acquisition;database;displacement mapping;experiment;facial recognition system;is-a;video post-processing	Ning Ye;Terence Sim	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539971	facial recognition system;face;layout;computer vision;face detection;facial motion capture;speech recognition;shape;computer science;three-dimensional face recognition;geometry;anthropometry;facial expression;pixel;face hallucination;biometrics	Vision	40.81996397037698	-52.894918203413624	178435
5050ad53ae55e6d8d3fd2fe6affaa8943d03c0ee	bfsift: a novel method to find feature matches for sar image registration	bilateral filtering;construction process;kernel;edge detection;random sample feature matching sar image registration bilateral filter scale invariant feature transform synthetic aperture radar image registration anisotropic scale dual matching strategy;image matching;random sampling;bilateral filter bf;feature matching;equations feature extraction image registration remote sensing kernel image edge detection transforms;random sample;scale space;image edge detection;synthetic aperture radar feature extraction filtering theory image matching radar imaging;scale invariant feature transform;synthetic aperture radar sar;sar image registration;range image;feature extraction;remote sensing;image registration;radar imaging;sar image;transforms;scale invariant feature transform sift;anisotropic scale space ass;bilateral filter;dual matching strategy;synthetic aperture radar sar anisotropic scale space ass bilateral filter bf scale invariant feature transform sift sar image registration;synthetic aperture radar image registration;filtering theory;anisotropic scale;synthetic aperture radar	In this letter, we propose a novel method based on bilateral filter (BF) scale-invariant feature transform (SIFT) (BFSIFT) to find feature matches for synthetic aperture radar (SAR) image registration. First, the anisotropic scale space of the image is constructed using BFs. The constructing process is noniterative and fast. Compared with the Gaussian scale space used in SIFT, more accurately located matches can be found in the anisotropic one. Then, keypoints are detected and described in the coarser scales using SIFT. At last, dual-matching strategy and random sample consensus are used to establish matches. The probability of correct matching is significantly increased by skipping the finest scale and by the dual-matching strategy. Experiments on various slant range images demonstrate the applicability of BFSIFT to find feature matches for SAR image registration.	aperture (software);bilateral filter;brainfuck;breadth-first search;experiment;image registration;random sample consensus;scale space;scale-invariant feature transform;synthetic data	Shanhu Wang;Hongjian You;Kun Fu	2012	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2011.2177437	sampling;computer vision;computer science;pattern recognition;bilateral filter;remote sensing	Vision	42.00140167303267	-56.17641186251024	178740
ccb952376bd7bb62b7d7c8425c0f16bba14c7df3	multi-fourier spectra descriptor and augmentation with spectral clustering for 3d shape retrieval	eigenvalue decomposition;spectral clustering;feature vector;minimum spanning tree;shape retrieval;shape modeling;similarity search	We propose a new method of similarity search for 3D shape models, given an arbitrary 3D shape as a query. The method features the high search performance enabled in part by our unique feature vector called Multi-Fourier Spectra Descriptor (MFSD), and in part by augmenting the feature vector with spectral clustering. The MFSD is composed of four independent Fourier spectra with periphery enhancement. It allows us to faithfully capture the inherent characteristics of an arbitrary 3D shape object regardless of the dimension, orientation, and original location of the object when it is first defined. Given a 3D shape database, the augmentation with spectral clustering is done first by computing the p-minimum spanning tree of the whole data set, where p is a number usually much less than m, the size of the whole 3D shape data set. We then define the affinity matrix, which is a square matrix of size m by m, where each element of the matrix denotes the distance between two shape objects. The distance is computed in advance by traversing the p-minimum spanning tree. The eigenvalue decomposition is then applied to the affinity matrix to reduce dimensionality of the matrix, followed by grouping into k clusters. The cluster information is kept for augmenting the search performance when a query is given. With a series of benchmark data sets, we will demonstrate that our approach outperforms previously known methods for 3D shape retrieval.	affinity analysis;benchmark (computing);cluster analysis;feature vector;file spanning;minimum spanning tree;processor affinity;rca spectra 70;similarity search;spectral clustering;the matrix	Atsushi Tatsuma;Masaki Aono	2008	The Visual Computer	10.1007/s00371-008-0304-2	active shape model;combinatorics;eigendecomposition of a matrix;feature vector;computer science;heat kernel signature;minimum spanning tree;machine learning;pattern recognition;mathematics;spectral clustering	Vision	40.148388975510926	-57.60277101078698	178840
f762056e0feb91d4a07c7821ed1ba7b5cd5a757e	head detection of the car occupant based on contour models and support vector machines	model based reasoning;cabeza;raisonnement base sur modele;analisis estadistico;automovil;intelligence artificielle;computer vision;skin color;statistical analysis;automobile;motor car;machine exemple support;analyse statistique;stereo vision;artificial intelligence;head;inteligencia artificial;support vector machine;maquina ejemplo soporte;vector support machine;tete;infrared	Head detection is a relatively well studied problem in computer vision. Several methods for head detection are proposed such as motion based method [6], disparity map [7], skin color method [8] and a head-shoulder contour [9]. Those methods are mainly based on stereo vision and color information for detecting the head. Our application domain is the telematics, especially within the car. In such environment, the illumination level is very variable. Moreover, we may need to use infrared illumination to capture the occupant in the night. Therefore, it is difficult to utilize the color information.	contour line;support vector machine	Yong-Guk Kim;Jeong-Eom Lee;Sang-Jun Kim;Soo-Mi Choi;Gwi-Tae Park	2005		10.1007/11504894_8	support vector machine;computer vision;simulation;infrared;computer science;stereopsis;artificial intelligence;model-based reasoning;head	Vision	46.03590227501827	-58.37728524579525	179350
4bada97ccdb9fc99f5cefcb8acc6f96ccebadd9f	robust pairwise matching of interest points with complex wavelets	scale invariant feature transform sift dual tree wavelet transform dtcwt object matching pairwise spatial constraints polar matching;object recognition;object matching;computational efficiency robust pairwise matching framework image feature spatial information pairwise similarity score 2d similarity space robust correspondence interest point detection stage polar matching dual tree complex wavelet transform feature spatial constraint;approximation algorithms;image matching;prediction algorithms;wavelet transforms image matching trees mathematics;trees mathematics;dual tree wavelet transform dtcwt;robustness clustering algorithms algorithm design and analysis approximation algorithms optimization object recognition prediction algorithms;wavelet transforms;pairwise spatial constraints;polar matching;scale invariant feature transform sift;clustering algorithms;robustness;optimization;algorithm design and analysis	We present a matching framework to find robust correspondences between image features by considering the spatial information between them. To achieve this, we define spatial constraints on the relative orientation and change in scale between pairs of features. A pairwise similarity score, which measures the similarity of features based on these spatial constraints, is considered. The pairwise similarity scores for all pairs of candidate correspondences are then accumulated in a 2-D similarity space. Robust correspondences can be found by searching for clusters in the similarity space, since actual correspondences are expected to form clusters that satisfy similar spatial constraints in this space. As it is difficult to achieve reliable and consistent estimates of scale and orientation, an additional contribution is that these parameters do not need to be determined at the interest point detection stage, which differs from conventional methods. Polar matching of dual-tree complex wavelet transform features is used, since it fits naturally into the framework with the defined spatial constraints. Our tests show that the proposed framework is capable of producing robust correspondences with higher correspondence ratios and reasonable computational efficiency, compared to other well-known algorithms.	algorithm;classification;coefficient;complex wavelet transform;computation;computer vision;detectors;distortion;dual;estimated;fits;feature detection (computer vision);feature detection (web development);image processing;interest point detection;matching;published comment;semantic similarity	Ee Sin Ng;Nick G. Kingsbury	2012	IEEE Transactions on Image Processing	10.1109/TIP.2012.2195012	algorithm design;mathematical optimization;prediction;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;cluster analysis;approximation algorithm;robustness;wavelet transform	Vision	42.79414595868087	-57.11224364631902	179464
ad5931bc52a337463c578f1256fba30df0e365b5	self-induced color correction for skin tracking under varying illumination	tracking system;neural nets;color constancy color correction skin tracking skin color based tracking system varying illumination conditions automatic transformation palm region skin color palette neural network back propagation learning rule condensation algorithm;skin;backpropagation;image sequences skin tracking neural nets backpropagation image colour analysis;color skin lighting robustness neural networks image sequences tracking tires optical propagation shape;skin color;image colour analysis;color correction;back propagation;tracking;neural network;image sequences	An important challenge of any skin color based tracking system is to accommodate varying illumination conditions. We present a method for automatic transformation of the color planes to match the skin color model learnt for a fixed illumination. The first couple of initial frames are used to automatically extract the palm region which in turn serves as an observed skin color palette which needs to he colortransformed to match a similar palette under a known illumination. A neural network implementing hack-propagation learning rule then performs the color correction for the entire sequence. We use a condensation algorithm for tracking after the color correction for improved performance.	artificial neural network;color;condensation algorithm;learning rule;palette (computing);real-time clock;software propagation;tracking system	Arvind Nayak;Subhasis Chaudhuri	2003		10.1109/ICIP.2003.1247418	color histogram;computer vision;color normalization;computer science;backpropagation;machine learning;color balance;histogram equalization;artificial neural network;computer graphics (images)	Vision	45.879174568597115	-52.96628413295004	179549
fa265e519af18fbf9387ac32c4d5e21f229c01cc	building surface refinement using cluster of repeated local features by cross ratio	hierarchical system;cross ratio;repeated local feature recognition;feature vector;vanishing point;local features;building surface	This paper describes an approach to recognize building surfaces. A building image is analyzed to extract the natural characters such as the surfaces and their areas, vanishing points, wall region and a list of SIFT feature vectors. These characters are organized as a hierarchical system of features to describe a model of building and then stored in a database. Given a new image, the characters are computed in the same form with in database. Then the new image is compared against the database to choose the best candidate. A cross ratio based algorithm, a novel approach, is used to verify the correct match. Finally, the correct match is used to update the model of building. The experiments show that the approach method clearly decreases the size of database, obtains high recognition rate. Furthermore, the problem of multiple buildings can be solved by separately analyzing each surface of building.		Hoang-Hon Trinh;Dae-Nyeon Kim;Kang-Hyun Jo	2008		10.1007/978-3-540-69052-8_3	feature vector;vanishing point;computer science;machine learning;pattern recognition;data mining;hierarchical control system;cross-ratio	Robotics	39.96264233089071	-57.96014811582361	180028
1e961891421bcc5253c60e4779cb2c7e2dbb31bc	a novel approach to retrieval of similar patterns in biological images		Novel descriptors of keypoints are proposed for matching (primarily) biological images. The descriptors incorporate characteristics of limited-size neighborhoods of keypoints. Descriptors are quantized into small vocabularies representing photometry of images (SIFT words) and geometry of their neighborhoods, so that significant distortions can be tolerated. In order to keep precision at a high level, Harris-Affine and Hessian-Affine detectors are independently applied. The retrieval results are accepted only if confirmed by both techniques. Using several test datasets, we preliminarily show that the method can retrieve semantically meaningful data from unknown and unpredictable images without any training or supervision. Low computational complexity of the method makes it a good candidate for scalable analysis of biological (e.g. zoological or botanical) visual databases.		Andrzej Stefan Sluzek	2013		10.1007/978-3-642-41939-3_63	computer science;scalability;pattern recognition;computer vision;visual word;artificial intelligence;computational complexity theory;scale-invariant feature transform	Vision	39.251002274209036	-55.721418928354204	181081
5dc6641c47e7cb1e7ee7c2c09fe086f8aeae3252	in search of inliers: 3d correspondence by local and global voting	object detection computer vision geometry image matching;correspondences;shape matching;object detection correspondences shape matching;three dimensional displays noise noise measurement solid modeling estimation shape robustness;object detection;higher level vision inlier separation 3d correspondence local voting global voting 3d models feature correspondences low level geometric invariants covariant constraints matching procedure controlled testing comparative testing 3d object detection	We present a method for finding correspondence between 3D models. From an initial set of feature correspondences, our method uses a fast voting scheme to separate the inliers from the outliers. The novelty of our method lies in the use of a combination of local and global constraints to determine if a vote should be cast. On a local scale, we use simple, low-level geometric invariants. On a global scale, we apply covariant constraints for finding compatible correspondences. We guide the sampling for collecting voters by downward dependencies on previous voting stages. All of this together results in an accurate matching procedure. We evaluate our algorithm by controlled and comparative testing on different datasets, giving superior performance compared to state of the art methods. In a final experiment, we apply our method for 3D object detection, showing potential use of our method within higher-level vision.	3d modeling;algorithm;feature model;high- and low-level;object detection;outline of object recognition;real life;sampling (signal processing);scalability	Anders Glent Buch;Yang Yang;Norbert Krüger;Henrik Gordon Petersen	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.266	computer vision;machine learning;pattern recognition;mathematics	Vision	45.169572678930734	-52.615457916875826	181299
42d0c6d9faee60ae828037a50936f8a9b7dd9e32	human action recognition by inference of stochastic regular grammars	vision ordenador;estimation mouvement;analisis estadistico;behavioral analysis;chain code;estimacion movimiento;hombre;motion estimation;intelligence artificielle;cuerpo humano;analisis automatico;corps humain;computer vision;projective plane;posture;automatic analysis;statistical analysis;human body;action recognition;analyse comportementale;analyse statistique;human;postura;pattern recognition;analyse automatique;artificial intelligence;analisis conductual;vision ordinateur;inteligencia artificial;reconnaissance forme;reconocimiento patron;homme	In this paper, we present a new method of recognizing human actions by inference of stochastic grammars for the purpose of automatic analysis of nonverbal actions of human beings. We applied the principle that a human action can be defined as a combination of multiple articulation movements. We measure and quantize each articulation movements in 3D and represent two sets of 4-connected chain code for xy and zy projection planes, so that they are appropriate for the stochastic grammar inference method. This recognition method is tested by using 900 actions of human upper body. The result shows a comparatively successful achievement of 93.8% recognition rate through the experiments of 8 action types of head and 84.9% recognition rate of 60 action types of upper body.		Kyungeun Cho;Hyungje Cho;Kyhyun Um	2004		10.1007/978-3-540-27868-9_41	projective plane;computer vision;human body;computer science;artificial intelligence;motion estimation;chain code	Vision	46.1653262838393	-58.02569018344829	181470
42fb683179beb3e8632f0631dea9e7e4744529e6	stable recognition of specular objects by the eigenwindow method		This paper proposes a recognition method for partially occluded objects in bin-picking tasks using principal component analysis. Although it is well known to be effective in recognizing an isolated object, as was shown by Murase and Nayar, the current method cannot be applied to partially occluded objects that are typical in bin-picking tasks. The analysis also requires that the object be centered in an image before recognition. These limitations of eigenspace analysis are due to the fact that the whole appearance of an object is utilized as a template for the analysis. We propose a new method, referred to as the eigenwindow method, that stores multiple partial appearances of an object in an eigenspace. Such partial appearances require a large amount of memory space. Three measurements, detectability, uniqueness, and reliability, among windows are developed to eliminate redundant windows and thereby reduce memory requirements. Using a pose-clustering method among windows, the method determines the pose of an object and the object type of itself. We have implemented the method and verified its validity with recognition of multispecularity objects. © 1998 Scripta Technica, Syst Comp Jpn, 29(7): 1220, 1998	cluster analysis;dspace;digi-comp i;microsoft windows;object type (object-oriented programming);oren–nayar reflectance model;principal component analysis;requirement	Kohtaro Ohba;Katsushi Ikeuchi	1998	Systems and Computers in Japan	10.1002/(SICI)1520-684X(19980630)29:7%3C12::AID-SCJ2%3E3.0.CO;2-L	computer vision;computer science;3d single-object recognition;computer graphics (images)	Robotics	43.17953467567497	-53.96650177072842	182667
affb884a730281d6c3d70afcc153c35b7788d2c5	a novel approach on silhouette based human motion analysis for gait recognition	motion analysis;analisis imagen;image features;correlacion;vision ordenador;mouvement corporel;estimation mouvement;image processing;biometrie;estimacion movimiento;gait;biometrics;biometria;procesamiento imagen;analyse mouvement;motion estimation;marcha;gait recognition;traitement image;computer vision;posture;image sequence;postura;human motion analysis;human identification;image analysis;vision ordinateur;secuencia imagen;correlation;analisis movimiento;movimiento corporal;allure;analyse image;body movement;sequence image	This paper presents a novel view independent approach on silhouette based human motion analysis for gait recognition applications. Spatio-temporal 1-D signals based on the differences between the outer of binarized silhouette of a motion object and a bounding box placed around silhouette are chosen as the basic image features called the distance vectors. The distance vectors are extracted using four view directions to silhouette. Gait cycle estimation and motion analysis are then performed by using normalized correlation on the distance vectors. Initial experiments for human identification are finally presented. Experimental results on the different test image sequences demonstrate that the proposed algorithm has an encouraging performance with relatively robust, low computational cost, and recognition rate for gait-based human identification.	algorithm;algorithmic efficiency;computation;experiment;gait analysis;kinesiology;minimum bounding box;standard test image	Murat Ekinci;Eyüp Gedikli	2005		10.1007/11595755_27	computer vision;image analysis;image processing;computer science;artificial intelligence;motion estimation;gait;correlation;feature;biometrics	Vision	45.836212774756326	-58.7192116067726	182840
aa71c6cd2e02b9b87122d557a5a0c17809ce0063	translation-, rotation- and scale- invariant recognition of hand-drawn symbols in schematic diagrams	fast minimum square error transform;distance measure;cad;hand drawn symbol recognition;attributed graph	In this paper, a model-based scheme for recognizing hand-drawn symbols in schematic diagrams using attributed graph (AG) matching in the absence of any information concerning their pose (translation, rotation and scale) is described. The process of AG matching proceeds as follows. First, an observed AG (AGO) is constructed from single-pixel-width line-representation of an observed symbol. Second, the pose of the AGO is estimated in terms of translation, rotation and scale with respect to the model AGs (AGMs). The search space is effectively pruned by introducing the concept of control vertex and applying geometrical constraints in an early stage. In this step, a small number of candidate AGMs are selected. Third, correspondences between components of the observed AG after normalization (AGON) and those of the AGMs are found for the given poses. Fourth, distance measures between the AGON and the AGMs are calculated, based upon the correspondences. Finally, the AGON is classified as the AGM with the minimum distance. Experimental results for hand-drawn symbols with and without templates show that using AG matching is very efficient and successful for translation-, rotation- and scale-invariant recognition of hand-drawn symbols in schematic diagrams.	diagram;schematic	Seong-Whan Lee;Jin Hyung Kim;Frans C. A. Groen	1990	IJPRAI	10.1142/S0218001490000022	combinatorics;cad;mathematics;geometry	Vision	43.766917495733004	-54.79897832856526	183290
e26e6326288b3f2087d938bbbaa782657204e3a7	template-guided 3d fragment reassembly using gds		Computer-aided fragment reassembly becomes more and more significant in recent years. The state of the art methods mainly utilize the fracture surface of the fragment. However, some fracture surfaces are often eroded and the features are not discriminative enough for matching. In this paper, we proposed a template-guided 3D fragment reassembly algorithm using Geodesic Disk Spectrum (GDS), which conducts matching between the intact surface of the fragment and the template. A two-step procedure is proposed for the first time with GDS-based matching and ICP-based registration for the reassembly task. The largest enclosed geodesic disk of the fragment is extracted and the matching to the template is found by GDS. In order to reduce the computational complexity, a k-layer Normal Distribution Descriptor (NDD) is also proposed. Transformation of the matched geodesic disks is obtained using the Iterative Closest Points (ICP) algorithm, and the registration between the fragment and the template is achieved. Our algorithm has been tested on various fragments and accurate results are obtained. A higher precision is achieved by comparing with existing algorithms, which proves the efficiency.	graph dynamical system;reassembly	Congli Yin;Mingquan Zhou;Yachun Fan;Wuyang Shui	2018		10.1007/978-981-13-1702-6_43	discriminative model;geodesic;computational complexity theory;normal distribution;artificial intelligence;pattern recognition;mathematics	Vision	41.240225185351754	-57.255622576030085	183376
c9ce4a592a7490d8d02fbea0e79438b3a1ea4624	occluded leaf matching with full leaf databases using explicit occlusion modelling		"""Matching an occluded contour with all the full contours in a database is an NP-hard problem. We present a suboptimal solution for this problem in this paper. We demonstrate the efficacy of our algorithm by matching partially occluded leaves with a database of full leaves. We smooth the leaf contours using a beta spline and then use the Discrete Contour Evaluation (DCE) algorithm to extract feature points. We then use subgraph matching, using the DCE points as graph nodes. This algorithm decomposes each closed contour into many open contours. We compute a number of similarity parameters for each open contour and the occluded contour. We perform an inverse similarity transform on the occluded contour. This allows the occluded contour and any open contour to be overlaid"""". We that compute the quality of matching for each such pair of open contours using the Fréchet distance metric. We select the best eta matched contours. Since the Fréchet distance metric is computationally cheap to compute but not always guaranteed to produce the best answer we then use an energy functional that always find best match among the best eta matches but is considerably more expensive to compute. The functional uses local and global curvature String Context descriptors and String Cut features. We minimize this energy functional using the well known GNCCP algorithm for the eta open contours yielding the best match. Experiments on a publicly available leaf image database shows that our method is both effective and efficient significantly outperforming other current state-of-the-art leaf matching methods when faced with leaf occlusion."""		Ayan Chaudhury;John L. Barron	2018	2018 15th Conference on Computer and Robot Vision (CRV)	10.1109/CRV.2018.00012	computer science;pattern recognition;artificial intelligence;curvature;inverse;database;feature extraction;fréchet distance;energy functional;occlusion;spline (mathematics);graph	Vision	40.852794771906446	-57.419524535462855	183640
0dc96bfa868e89f7662de3e60bda951446dd0798	polynomial local shape descriptor on interest points for 3d part-in-whole matching	local shape descriptor;interest points;part in whole matching;attributed graph	Part-in-whole 3D shape matching is to recognize query shapes as sub-parts of a target intact 3D object. It plays a pivotal role in a large number of engineering applications. The most critical component in a part-in-whole search system is the local shape descriptor which encapsulates the identified local feature on the query part and is matched with the local shape descriptors of the parts in the database. We propose a novel local shape descriptor based on the concept that the evolution pattern of geodesic iso-contour's length is a good representative for surface features. Our local shape descriptor enjoys a unique advantage over most existing ones by being sensitive to the geodesic radius of the local region, and thus is able to capture more comprehensive shape information if the query portion of the shape is larger and includes more complicated surface features. Through a simple approximation scheme, our local shape descriptor is defined as a vector piecewise polynomial function of the geodesic radius of the interest point, thus enabling local matching to be performed quickly by simple curve evaluations. We also introduce a new schema of interest points sampling so that we can reserve the most corresponding information of the model by a small number of local feature descriptors. The proposed part-in-whole matching approach outperforms many existing approaches in matching efficiency and requiring a smaller input region. It is a shortcut solution for incomplete model matching/retrieval. We solve the part-in-whole matching problem for 3D models in engineering domain.A local descriptor is proposed to describe the local shape of the interest points.The proposed descriptor is sensitive to the geodesic radius.Interest points are extracted based on the local saliency of the sharp features.We report the properties and the matching performance of the proposed method.	interest point detection;polynomial	Lulin Quan;Kai Tang	2015	Computer-Aided Design	10.1016/j.cad.2014.09.005	mathematical optimization;local binary patterns;topology;heat kernel signature;mathematics;geometry	EDA	40.2639686124195	-58.01986925821977	183980
10bc8e317a1088d0d5958b512e674b0282b0db37	university of sheffield at trecvid 2007: shot boundary detection and rushes summarisation	difference set	This year we conducted experiments on shot boundary detection and rushes video summarisation. For the shot boundary determination task, we focused on detection of ‘cut’. The approach calculated the ‘exclusive or’ of two frames in the grey scale in order to measure the amount of discontinuity at a pixel level between two shots. Five runs were submitted with different sets of parameters, resulting in the performance of as high as 87% recall and 85% precision. For the rushes video task, the summary duration was fixed at 4% of the video length. We joined a number of continuous frames that were extracted from the middle of each shot detected. We submitted a single run, resulting in the average level performance. 1 Shot Boundary Detection Shot boundary detection is a process of identifying boundaries between shots from a sequence of video frames. The key idea is to choose a right set of features and measures that capture the dissimilarity between shots. The difference between adjacent frame pair is calculated from features. A shot boundary is assigned when the value is greater than a predefined thresholds. To date there have been a large number of shot boundary detection algorithms proposed (Pye et al., 1998; Lienhart, 2001; Pickering & Ruger, 2001; Deng & Manjunath, 2001; Miene et al., 2003; Qi et al., 2003; Quenot et al., 2003; Ren & Singh, 2004; Ngo et al., 2005). Features such as motion, colour, and edges have been tested using a number of different criteria for comparison. In general, the difficulty of the task depends on the complexity of shot transition, video structure and quality. We present the approach to detecting ‘cut’, the most frequent shot transitions. It is relatively simple to identify ‘cut’ because a clear discontinuity can be observed at a pixel level. Thus our approach assigns shot boundaries based on pixel changes between two adjacent shots. 1.1 Approach Our approach aims to measure the amount of discontinuity at a pixel level between two consecutive frames. It is captured by calculating the ‘exclusive or’ of two frames in the grey scale: BWXi = ∑ j=1...J Pi−1(j) ⊕ Pi(j) (1) where ⊕ implies the ‘exclusive or’ operation, J is the total number of pixels per frame, and Pi−1(j) and Pi(j) are the black/white value of pixel j in frames i − 1 and i respectively. If the value for BWXi is greater than a predefined threshold, ‘cut’ is assigned between frames i − 1 and i. A sample of this operation is illustrated in Figure 1. For the test video ‘BG 34901 ’, Figure 2 demonstrates the calculation of BWXi and (BWXi+1 −BWXi) for the entire length of video. In the experiment, each spike is treated as a shot boundary candidate. 1.2 Experiment The Netherlands Institute for Sound and Vision provided a sound and vision collection including news magazine, science news, reports, documentaries, and educational programming (TRECVID, 2007). The test collection consisted of 15 mpeg-1 videos with the total length of six hours. The visualDub 1 software was applied to extract individual RGB frames of 352×288 pixels from the collection at a rate of 25 frames/second. 1available at http://www.virtualdub.org/ (a) original frame sequence (b) ‘exclusive or’ of the black/white values between two adjacent frames Figure 1: Detection of ‘cut’ using the ‘exclusive or’.	algorithm;emoticon;exclusive or;experiment;grayscale;job control (unix);mpeg-1;pixel;reflections of signals on conducting lines;sensor;shot transition detection;word lists by frequency	Siripinyo Chantamunee;Yoshihiko Gotoh	2007			pixel;speech recognition;trecvid;computer science	Vision	39.748178649507935	-52.93868407164178	184030
0c247ac797a5d4035469abc3f9a0a2ccba49f4d8	an efficient landmark localization for face occlusion	face gabor filters lighting databases feature extraction face recognition kernel;active shape model asm landmark localization gabor filters image warping;wavelet transforms face recognition gabor filters image representation lighting;gabor filters;face alignment;wavelet transforms;feature vector;gabor filter;face recognition;image representation;feature extraction;facial features;lighting;yale face database_b landmark localization face occlusion facial representation face images modified active shape model gabor filters illumination normalization method gabor wavelets gabor kernel reduction ar face database;gabor wavelets;active shape model;image warping	Landmark localization for facial representation is usually affected by the variation of illumination or occlusion, in order to reduce these factors, in this paper, we propose an efficient landmark localization method for face images based on modified active shape model (MASM) and Gabor filters to reduce the drawbacks of intensity contrast and occlusion. The approach mainly consists of two phases. First, face images are preprocessed by the proposed illumination normalization method using Gabor wavelets. Then, upgrading the performance of Gabor kernels reduction based on the similarity of Gabor features vector, the location of facial features can fit more efficient and fast by the proposed feature-based weighted warping. The advantages of this proposed method not only obtain the better face alignment, but also overcome the active shape model (ASM) method which caused the failure for aligned target. The experimental results verify that this approach can achieve in AR face database with face occlusion cases and Yale face database_B with varied illumination conditions.	active shape model;gabor atom;gabor filter;illumination (image);microsoft macro assembler;wavelet	Hui-Yu Huang;Shih-Hang Hsu	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6016715	active shape model;facial recognition system;image warping;computer vision;speech recognition;feature vector;feature extraction;computer science;machine learning;pattern recognition;lighting;mathematics;wavelet transform	Vision	39.75926446306414	-57.437657678803824	184894
119ebab52fa7e801452ede299d695a4a888140ce	scale invariance without scale selection	image sampling;boundary based model;translation invariant;filtering;scale invariant descriptor;object detection image sampling image edge detection statistics image converters fourier transforms image segmentation band pass filters smoothing methods data mining;contour segment;image segmentation;band pass filters;spatially varying smoothing;image converters;object detection scale invariant descriptor log polar sampling spatially varying smoothing image scaling image translation fourier transform modulus local image structure contour segment boundary based model;image translation;fourier transform modulus;data mining;distance measurement;physical sciences and mathematics;smoothing methods;image edge detection;log polar sampling;feature extraction;fourier transforms;statistics;sampling methods fourier transforms image segmentation object detection;image scaling;sampling methods;local image structure;scale invariance;object detection	In this work we construct scale invariant descriptors (SIDs) without requiring the estimation of image scale; we thereby avoid scale selection which is often unreliable. Our starting point is a combination of log-polar sampling and spatially-varying smoothing that converts image scalings and rotations into translations. Scale invariance can then be guaranteed by estimating the Fourier transform modulus (FTM) of the formed signal as the FTM is translation invariant. We build our descriptors using phase, orientation and amplitude features that compactly capture the local image structure. Our results show that the constructed SIDs outperform state-of-the-art descriptors on standard datasets. A main advantage of SIDs is that they are applicable to a broader range of image structures, such as edges, for which scale selection is unreliable. We demonstrate this by combining SIDs with contour segments and show that the performance of a boundary-based model is systematically improved on an object detection task.	algorithm;contour line;data structure;emoticon;feature extraction;kernel (operating system);modulus of continuity;object detection;sampling (signal processing);scale-invariant feature transform;smoothing	Iasonas Kokkinos;Alan L. Yuille	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587798	filter;fourier transform;sampling;computer vision;mathematical optimization;feature extraction;computer science;scale invariance;pattern recognition;mathematics;band-pass filter;image segmentation;image scaling	Vision	42.35983230063142	-57.54663910058426	185011
d4d5f6cc1626584786944af00578c9553aebca68	a framework for shape representation and recognition	image recognition;object recognition;image recognition object recognition image representation inverse problems;shape representation;inverse problem;complex i;image representation;principal component analysis;shape joints animation marine animals skeleton torso bones fasteners deformable models tail;object recognition shape representation recognition animate objects silhouettes primitives mid grained shapes grammar deformations principal component analysis generic low dimensional parameterized representation inverse problem;inverse problems	We describe a novel framework which represents and recognizes animate objects from their silhouettes. We model animate objects at three levels of complexity: (i) primitives, (ii) mid-grained shapes, which are deformations of the primitives, and (iii) objects constructed by using a grammar to join mid-grained shapes together. The deformations of the primitives can be characterized by principal component analysis. This framework provides a generic low dimensional parameterized representation for animate objects and it also gives a formalism for solving the inverse problem of object recognition. This paper is mainly focused on generating and representing animate objects. We briefly describe how this representation can be automatically extracted for recognition, the complete system is described in [8]	outline of object recognition;principal component analysis;semantics (computer science)	Song-Chun Zhu;Alan L. Yuille	1994		10.1109/ICIP.1994.413399	computer vision;computer science;inverse problem;machine learning;mathematics;geometry	Vision	42.538130545789016	-56.176575164088995	185142
93d8fb079eccc5d2fccf57d483b2efafe5d457e3	flexible object recognition in cluttered scenes using relative point distribution models	object recognition;optimisation;clutter;object recognition layout shape deformable models noise robustness viterbi algorithm image edge detection distributed computing humans computer vision;edge detection;training;deformable models;spatial structure;optimisation problem relative point distribution models flexible object recognition cluttered scenes edge based object recognition method point matching task viterbi algorithm;deformable objects;computational modeling;shape;viterbi algorithm;local features;optimisation edge detection object recognition;optimization;spatial relationships;point distribution model	This paper introduces an edge-based object recognition method that is robust with respect to clutter, occlusion and object deformations. The method combines the use of local features and their spatial relationships to identify the point correspondences between the object-of-interest and the input scene. Local features encode information from their neighbourhood, and this renders them insensitive to noise at a distance. However, they have moderate discriminating power, and the proposed method exploits their spatial structure to compensate for this. Our flexible localisation technique, which is based on point distribution models, makes the method also applicable to deformable objects. The point matching task is formulated as an optimisation problem that is solved using the Viterbi algorithm. The method has been validated on challenging real scenes.	clutter;encode;hidden surface determination;image noise;mathematical optimization;neighbourhood (graph theory);outline of object recognition;rendering (computer graphics);viterbi algorithm	Alexandros Bouganis;Murray Shanahan	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761712	spatial relation;point distribution model;computer vision;edge detection;shape;viterbi algorithm;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;clutter;computational model	Vision	45.2290827769993	-52.39268243056944	185300
2a4511b5d5e2ae730622c20322ddaf29f9077461	single image per person face recognition with images synthesized by non-linear approximation	minimisation;wavelet transforms approximation theory curvelet transforms face recognition minimisation;wavelet transforms approximation theory curvelet transforms face recognition minimisation surfacelets single image per person face recognition nonlinear approximation edge information;curvelet face recognition wavelet;image synthesis minimization problem transform domain voting based approach wavelets curvelets contourlets;surfacelets single image per person face recognition nonlinear approximation edge information;non linear approximation;wavelet transforms;approximation theory;face recognition;curvelet transforms;image synthesis minimization problem transform domain voting based approach wavelets curvelets contourlets databases transforms face image edge detection training face recognition approximation methods curvelet face recognition wavelet;face recognition image databases image generation spatial resolution image resolution machine learning matrix decomposition image reconstruction face detection multiresolution analysis	This paper addresses the problem of identifying faces when the training face database consists of one face image of each person. It proposes a new approach that synthesizes new face samples of varying degrees of edge information; the synthesized images are generated from the original image and form non-linear approximations of the latter. The approximation is framed as an l 1 minimization problem in a transform domain. The paper also shows that a voting based approach to recognize faces from single available samples yields better results than previous works that only augmented the available database. The proposed approach yields considerably better results (about 6% increase in recognition accuracy) than the SPCA method, which was tailored for addressing this problem.	database;facial recognition system;linear approximation;nonlinear system	Angshul Majumdar;Rabab Kreidieh Ward	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712361	facial recognition system;computer vision;minimisation;machine learning;curvelet;pattern recognition;mathematics;wavelet transform;approximation theory	Vision	43.4306575816227	-57.60228232074503	185672
9468913ea555d017de2d9ee9538732f6f6f71e6e	rotation invariant histogram filters for similarity and distance measures between digital images	as filtering algorithms;rotation invariant histogram filters;image recognition;distance measure;search space;color histogram;search space rotation invariant histogram filters distance measures digital images fast rotation invariant filtering algorithms pattern template large image distance similarity measures correlation information as filtering algorithms 2d case color histograms filtered positions;filtered positions;fast rotation invariant filtering algorithms;distance similarity measures;distance measurement;rotation invariance;pattern template;large image;distance measures;distance measurement image recognition filtering theory search problems;correlation information;2d case;color histograms;search problems;digital image;similarity measure;digital images;filtering theory;histograms digital filters rotation measurement digital images pattern matching indexing position measurement filtering image databases velocity measurement	Fast rotation invariant filtering algorithms for are presented for speeding–up the search of the position of the given pattern template from large image. The algorithms work for several distance/similarity measures. The algorithms give useful correlation information as such, and can be used as filtering algorithms for more sophisticated methods. The results are presented in the 2–D case, but extensions to 3–D are straight–forward. The algorithms are based on color histograms computed from the pattern. A technique is described for pruning the filtered positions incrementally, using simple, yet effective boundary that divides the search space in two.	approximation algorithm;coefficient;computation;expectation–maximization algorithm;experiment;interpolation;perimeter;pixel;signal processing;solid-state drive;sum of absolute differences;template matching;template method pattern;the circle (file system);time complexity	Kimmo Fredriksson	2000		10.1109/SPIRE.2000.878185	computer vision;discrete mathematics;computer science;pattern recognition;mathematics;digital image	Vision	41.37934189516819	-58.92901346204757	186072
01aeb9563ea6f8abcd1f2762808bacb1e8a6b29d	robust and accurate pattern matching in fuzzy space for fiducial mark alignment	fiducial mark;alignment;pattern matching;distance transform;fuzzy space	This paper presents a new pattern matching method for fiducial mark alignment in a fuzzy space. The membership functions of fuzzy sets are designed by distance transforms, and their levels are set in the fuzzy space for fast matching of a specific fiducial mark. After the fuzzification, a sub-pixel level translation is estimated by a fuzzy similarity measure and an interpolation using fuzzified model and target images. This paper also proposes a method of coarse-to-fine rotation estimation in sub-pixel level. Experiments show that the proposed fuzzy space pattern matching algorithm outperforms commercial pattern matching algorithms based on correlation or edge.	and-or-invert;algorithm;automated optical inspection;cross-validation (statistics);distance transform;experiment;fiducial marker;fuzzy set;image scaling;interpolation;pattern matching;pixel;robustness (computer science);similarity measure	Xuenan Cui;Hakil Kim;Eunsoo Park;Hyohoon Choi	2012	Machine Vision and Applications	10.1007/s00138-012-0433-5	mathematical optimization;pattern recognition;data mining;mathematics	Vision	42.53933888357687	-58.24490162467079	186428
f642aff71987c0231e190b918ce705a9fffb21cd	symmetry-aware human shape correspondence using skeleton	symmetric flip problem;kinect;shape correspondence;conference proceeding	In this paper, we propose a symmetry-aware human shape correspondence extraction method. We address the symmetric flip problem which exists in establishing correspondences for intrinsically symmetric models and improve the accuracy of the final corresponding pairs. To achieve this goal, we extended the state-of-the-art approach by using skeleton information to further remove symmetric flipped shape correspondences. Traditional approaches that only rely on surface geometry information can hardly discriminate surface points which are symmetric. With the appearance of inexpensive RGB-D camera, such as Kinect, skeleton information can be easily obtained along with mesh. Therefore, after the initial correspondences are achieved, we extend the candidate sets for each point on the template, followed by making use of skeleton to remove the symmetric flipped false candidates. In the remaining candidates, final correspondences are achieved by choosing those with minimum geodesic distortion from base vertex set, which is formed by sampling on the mesh. Experiments demonstrate that the proposed method can effectively remove all the symmetric flipped candidates. Moreover, the final correspondence pair is more accurate than those of the state of the arts.		Zongyi Xu;Qianni Zhang	2016		10.1007/978-3-319-27671-7_53	computer vision;computer science;natural user interface	Vision	44.00785357562765	-52.974633462725706	186638
cc059d8028b1a384754f408775595f40fbfe338b	footprint tracking and recognition using a pressure sensing floor	tracking image recognition pattern clustering;image recognition;pattern clustering;sensors;foot;indexing terms;foot floors sensor arrays shape power engineering and energy clustering algorithms convergence art kernel biometrics;shape;footprint tracking pressure sensing floor;feature extraction;foot clusters footprint tracking footprint recognition pressure sensing floor pressure data;clustering algorithms;footprint tracking;pressure sensing floor;tracking;floors	This paper presents an approach to clustering, tracking and recognizing footprints of a single subject from pressure data obtained using a pressure sensing floor. The proposed method clusters active footprint areas on the floor and recognizes and tracks the footprints according to their 2D shapes and geometrical relationships among foot clusters. Experimental results show the efficacy of the proposed approach.	cluster analysis;ibm notes;rs-232;shape analysis (digital geometry)	Jiqing Zhang;Gang Qian;Assegid Kidané	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414148	computer vision;simulation;index term;feature extraction;shape;computer science;sensor;machine learning;tracking;cluster analysis;foot	Robotics	42.849785497103845	-56.54866493241861	187056
850d3ab0aef553a95cfe9ca4d3ba47db11b0c3b2	a multiscale assembly inspection algorithm	inspection gray scale image generation manufacturing automation assembly systems image databases spatial databases stochastic processes robotics and automation robotic assembly;computer graphics;cad;automatic optical inspection;learning systems;image generation;assembling;computer graphics multiscale assembly inspection algorithm synthetic images cad model real images multiresolution template matching object trees learning systems cad database object detection;object detection;object detection assembling automatic optical inspection cad visual databases learning systems computer graphics;visual databases	An important aspect of robust automated assembly is an accurate and efficient method for the inspection of finished assemblies. This novel algorithm is trained on synthetic images generated using the CAD model of the different components of the assembly. Once trained on synthetic images, the algorithm can detect assembly errors by examining real images of the assembled product.	algorithm	Khalid W. Khawaja;Anthony A. Maciejewski;Daniel Tretter;Charles A. Bouman	1996	IEEE Robot. Automat. Mag.	10.1109/100.511780	computer vision;computer science;cad;computer graphics;engineering drawing;computer graphics (images)	Robotics	45.8629632515999	-53.90939469468683	187089
a13ee89fb23e0b7043c1f76ccbf5ad3aa3792726	3d binary signatures	3d object recognition;3d descriptors;3d matching	In this paper, we propose a novel binary descriptor for 3D point clouds. The proposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the matching efficiency of the binary descriptors for 2D images. 3DBS describes keypoints from point clouds with a binary vector resulting in extremely fast matching. The method uses keypoints from standard keypoint detectors. The descriptor is built by constructing a Local Reference Frame and aligning a local surface patch accordingly. The local surface patch constitutes of identifying nearest neighbours based upon an angular constraint among them. The points are ordered with respect to the distance from the keypoints. The normals of the ordered pairs of these keypoints are projected on the axes and the relative magnitude is used to assign a binary digit. The vector thus constituted is used as a signature for representing the keypoints. The matching is done by using hamming distance. We show that 3DBS outperforms state of the art descriptors on various evaluation metrics.	angularjs;bit array;hamming distance;ordered pair;point cloud;reference frame (video);sensor;type signature	Siddharth Srivastava;Brejesh Lall	2016		10.1145/3009977.3010009	computer vision;pattern recognition;data mining;mathematics	Vision	41.649131788378696	-55.463598717801204	187399
0f7d8a2183ce6451085d8f6c297848bc0b9d0e9d	direction analysis algorithm using statistical approaches		Statistical approaches become very important tools that interfere and overlap in our daily life and become inevitable event that help us in every tiny details of our life, in this paper; we are going to present a new technique for analyzing the two principal component of any given object by calculating the direction over the occupied coordinates using mean, variance, and covariance statistical functions, and by finding some relationship between those statistical functions; we have extracted the angle degree of the processed object, for pattern recognition applications; this object can be adjusted accordingly to overcome the rotation perturbation shortcoming that hinders the extraction of a unified features especially for object recognition purposes in which we have to present many samples per single pose which makes the processing of this increasing size of the database is a noticeable burden, we have achieved a dramatically results with almost zero time of calculation since the statistical functions applied need little processing time to finish. Statistical operation; principal component analysis; object direction; rotation invariant features; object adjustment; variance function; covariance function.	algorithm;outline of object recognition;pattern recognition;principal component analysis	Mokhtar M. Hasan;Pramod Kumar Mishra	2012		10.1117/12.946046	computer science;artificial intelligence;data mining;statistics	Robotics	42.47923386270232	-53.91136707627927	187405
39ad0497bb43e4d901f6e5722ea82c53703c5433	bipolarity and projective invariant-based zebra-crossing detection for the visually impaired	image segmentation;roads cameras surface treatment image segmentation layout portable computers systems engineering and theory feature extraction statistics multimedia computing;layout;systems engineering and theory;multimedia computing;surface treatment;roads;portable computers;feature extraction;statistics;visual impairment;constant width;cameras	A safe road-crossing system is an extreme necessity to improve the mobility of the visually impaired. It is important for a blind person to know whether a frontal area is a crossing or not. A crossing is characterized by zebra pattern i.e. by constant width periodic white stripes on a usual black road surface, which can be treated as a bipolar pattern. In this paper, a fast and stable algorithm for detecting the location of a pedestrian crossing using an image captured by a single camera is presented. This is achieved through bipolarity-based segmentation and projective invariant-based recognition. The algorithm includes three steps. First, we segment the image on the basis of bipolarity and pick the crossing candidates on the basis of area. Second, feature points are extracted on the candidate area based on Fisher criterion. Third, we use projective invariants to recognize the crossing. The experiments on a large number of street scenes with and without crossing demonstrate the effectiveness of the proposed algorithm.	algorithm;experiment;feedback;illumination (image);sensor;stripes;zebra patterning	Mohammad Shorif Uddin;Tadayoshi Shioyama	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.423	layout;computer vision;simulation;feature extraction;computer science;machine learning;mathematics;geometry;image segmentation;statistics;computer graphics (images)	Vision	41.508982767943536	-54.664812308547326	187877
ad7b6d2e8d66f720cc83323a0700c25006d49609	face recognition under varying illumination using gradientfaces	reconnaissance visage;artefacto;image gradient domain;mimica;gradientfaces;degradation;object artifacts face recognition illumination gradient faces image gradient domain feature extraction pie database uncontrolled natural lighting conditions yale database image noise;image segmentation;image processing;illumination;image databases;biometrie;mimique;biometrics;database;biometria;procesamiento imagen;base dato;object artifacts;traitement image;image noise;artefact;yale database;face recognition lighting image databases feature extraction image processing image segmentation robustness pixel power measurement degradation;gradient domain;automatic recognition;installation exterieure;face recognition;instalacion exterior;outdoor installation;theoretical analysis;uncontrolled natural lighting conditions;feature extraction;illumination insensitive measure;pixel;illumination insensitive measure face recognition gradientfaces gradient domain;base de donnees;algorithms databases factual face humans image processing computer assisted lighting normal distribution pattern recognition automated;pie database;pattern recognition;robustness;lighting;reconnaissance forme;facial expression;reconocimiento patron;feature extraction face recognition;gradient faces;reconocimiento automatico;power measurement;reconnaissance automatique	In this correspondence, we propose a novel method to extract illumination insensitive features for face recognition under varying lighting called the gradient faces. Theoretical analysis shows gradient faces is an illumination insensitive measure, and robust to different illumination, including uncontrolled, natural lighting. In addition, gradient faces is derived from the image gradient domain such that it can discover underlying inherent structure of face images since the gradient domain explicitly considers the relationships between neighboring pixel points. Therefore, gradient faces has more discriminating power than the illumination insensitive measure extracted from the pixel domain. Recognition rates of 99.83% achieved on PIE database of 68 subjects, 98.96% achieved on Yale B of ten subjects, and 95.61% achieved on Outdoor database of 132 subjects under uncontrolled natural lighting conditions show that gradient faces is an effective method for face recognition under varying illumination. Furthermore, the experimental results on Yale database validate that gradient faces is also insensitive to image noise and object artifacts (such as facial expressions).	database;effective method;extraction;face;facial recognition system;illumination (image);image gradient;image noise;morphologic artifacts;pixel;preprocessor;uncontrolled format string	Taiping Zhang;Yuan Yan Tang;Bin Fang;Zhaowei Shang;Xiaoyu Liu	2009	IEEE Transactions on Image Processing	10.1109/TIP.2009.2028255	computer vision;speech recognition;image processing;computer science;lighting;computer graphics (images)	Vision	44.35268705699255	-59.05170177155486	187942
ecf2a71c29ff344181a0462188cd8411d343efab	distinctive 3d surface entropy features for place recognition	image recognition;sure features distinctive 3d surface entropy features interest point detector interest point descriptor 3d point clouds depth images semantically distinct place recognition indoor environments sure interest operator distinctive point selection surface orientation surface normals view pose invariant descriptor local surface properties colored texture information feature detector narf;image texture;image colour analysis;feature extraction;histograms entropy feature extraction detectors shape image color analysis three dimensional displays;entropy;place recognition surface interest points local shape texture descriptor;image texture entropy feature extraction image colour analysis image recognition	In this paper, we present a variant of SURE, an interest point detector and descriptor for 3D point clouds and depth images and use it for recognizing semantically distinct places in indoor environments. The SURE interest operator selects distinctive points on surfaces by measuring the variation in surface orientation based on surface normals in the local vicinity of a point. Furthermore SURE includes a view-pose-invariant descriptor that captures local surface properties and incorporates colored texture information. In experiments, we compare our approach to a state-of-the-art feature detector in depth images (NARF). Finally, we evaluate the use of SURE features for recognizing places and demonstrate its advantages.	bag-of-words model in computer vision;display resolution;experiment;normal (geometry);point cloud;simultaneous localization and mapping;surfel	Torsten Fiolka;Jörg Stückler;Dominik A. Klein;Dirk Schulz;Sven Behnke	2013	2013 European Conference on Mobile Robots	10.1109/ECMR.2013.6698843	image texture;computer vision;entropy;feature extraction;computer science;machine learning;pattern recognition;mathematics	Vision	40.11098920272754	-56.44754760751885	188069
827d2d768f58759e193ed17da6d6f7e87749686a	fingerprint matching by thin-plate spline modelling of elastic deformations	thin plate spline models;theoretical model;elastic deformations;registration;minutiae matching;thin plate spline;matching method;fingerprint verification	This paper presents a novel minutiae matching method that describes elastic distortions in 4ngerprints by means of a thin-plate spline model, which is estimated using a local and a global matching stage. After registration of the 4ngerprints according to the estimated model, the number of matching minutiae can be counted using very tight matching thresholds. For deformed 4ngerprints, the algorithm gives considerably higher matching scores compared to rigid matching algorithms, while only taking 100 ms on a 1 GHz P-III machine. Furthermore, it is shown that the observed deformations are di6erent from those described by theoretical models proposed in the literature. ? 2003 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	algorithm;distortion;elastic matching;elastic net regularization;fingerprint;minutiae;nonlinear system;pattern recognition;thin plate spline	Asker M. Bazen;Sabih H. Gerez	2003	Pattern Recognition	10.1016/S0031-3203(03)00036-0	fingerprint verification competition;computer vision;computer science;machine learning;pattern recognition;mathematics;geometry;thin plate spline	Vision	41.24209818581265	-56.87228843895873	188329
3384a7231e2ec02a00291c62a63a05b678ecdbf9	smoke detection for static cameras	boundary roughness smoke detection color probability edge density;probability;image resolution;morphology;edge density smoke detection static cameras background subtraction moving object determination color characteristics morphology operations component labeling methods boundary roughness;image edge detection;image color analysis;image color analysis cameras image edge detection videos image resolution morphology probability;smoke detectors cameras edge detection image colour analysis image motion analysis;cameras;videos	This paper describes the smoke detection for static cameras. The background subtraction was used to determine moving objects. Color characteristics were utilized to distinguish smoke regions and other scene members. Separate pixels were united into blobs by morphology operations and connected components labeling methods. The image is then refined by boundary roughness and edge density to decrease amount of false detections. Results of the current frame are compared to the previous one in order to check the behavior of objects in time domain.	background subtraction;color;connected-component labeling;mathematical morphology;pixel;sensor	Alexander Filonenko;Danilo Cáceres Hernández;Kang-Hyun Jo	2015	2015 21st Korea-Japan Joint Workshop on Frontiers of Computer Vision (FCV)	10.1109/FCV.2015.7103719	computer vision;feature detection;edge detection;background subtraction;color image;image gradient;geography;optics;computer graphics (images)	Vision	45.03244270188008	-54.245731902555725	188363
2a55b8d6e8f18a504ecd4f8b8d0e7fe87c2520c6	optimal colour-based mean shift algorithm for tracking objects	object target localization;rgb colour histogram;3d colour histogram bins;colour distribution;computer vision;colour statistics;sequential images;object tracking;confidence map;uniform quantisation method;histogram agglomeration extraction;target image representation;optimal colour based mean shift algorithm;uniform distribution;computation cost	The mean-shift method is widely used to locate a target object quickly in sequential images. The mean-shift algorithm takes advantage of a colour distribution with a uniform quantisation. However, the quantisation method ignores the close relationship of colour statistics. The uniform distribution also results in a colour histogram with many empty bins, which introduces additional computation cost in the tracking procedure. To reduce the number of these redundant, empty bins, the authors present a new optimal colour-based, mean-shift algorithm for tracking objects. In the proposed method, the optimal colours are extracted by a histogram agglomeration, which clusters three-dimensional (3D) colour histogram bins with the frequency ratios of 3D colour values. After obtaining optimal colours in a RGB colour histogram, the target image is represented by the indices of the optimal colours. The mean-shift algorithm thus creates a confidence map in a candidate image based on the optimal colour histogram in the target image. It then finds the peak of the confidence map near the previous position of an object area. Comparative experiments with the conventional mean-shift method showed that our method has the advantages of decreased processing time and improved tracking accuracy.	algorithm;mean shift	Xiaowei An;Jaedo Kim;Youngjoon Han	2014	IET Computer Vision	10.1049/iet-cvi.2013.0004	computer vision;color normalization;computer science;histogram matching;machine learning;video tracking;pattern recognition;mathematics;uniform distribution;image histogram	Vision	41.30490489690895	-55.029751577934775	188788
8748d8a86e8e596789a25bc85f3e0ec13c9d18e5	automatic key-frame selection for content-based video indexing and access	traitement automatise;extraction information;analisis contenido;contenu image;toma vista;image content;audiovisual;multimedia;information extraction;abstracting;real time;image;methode;segmentation;cadre;algorithme;video indexing;algorithm;data storage;content analysis;imagen;audiovisuel;tratamiento automatizado;prise vue;video recording;elaboracion resumen;photographing;registro video;frame;cuadro;analyse contenu;video;enregistrement video;elaboration resume;metodo;method;segmentacion;automated processing;algoritmo;extraction informacion	This paper addresses key-frame selection for content-based video indexing and access. The proposed key-frame selection method is aimed to operate in real-time irrespective of the available computation resources and memory. Hence, we provide three solutions to content-based key-frame selection with different costs, and suggests three operation levels. The suggested key-frame selection method has two major parts: (i) segmentation of the video into shots; (ii) analysis of the motion and color activity within each video shot to selected additional frames. We also prove a new color based approach to key-frame selection and discuss how to fuse color and motion based key-frame selection results.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	key frame	Candemir Toklu;Shih-Ping Liou	2000		10.1117/12.373588	telecommunications;computer science;multimedia;computer graphics (images)	Vision	45.24334225810779	-57.622417808548725	188844
aa2d4d1ef5f09104bba0e28c4715bef6f4af1f48	object recognition using point uncertainty regions as pose uncertainty regions	image features;object recognition;measurement uncertainty;uncertainty regions;lookup table;pose estimation	In this paper, a recognition algorithm based on point features is presented. In this algorithm sets of hypothesized matches between model and image points are generated. From them the pose of the object is estimated and stored in a lookup table. When two similar poses are found the pose is assumed to be correct and the hypothesis is verified. The main contribution of this paper is that poses and their uncertainties are represented by the uncertainty regions of the projections of several 3D points which are circles in the image. These uncertainty regions are due to the measurement uncertainty of the image features which result in uncertainty in the recovered pose. When two poses are consistent, the pairs of uncertainty regions of the same 3D point will have a non-empty intersection. The algorithm exploits the fact that these uncertainty regions can be computed easily and accurately. The algorithm has been implemented and tested on real images.	algorithm;cluster analysis;clutter;experiment;generalised hough transform;lookup table;outline of object recognition;speedup;time complexity	Ilan Shimshoni;Aviva Sasporta	2006	Image Vision Comput.	10.1016/j.imavis.2005.11.003	computer vision;pose;3d pose estimation;lookup table;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;feature;measurement uncertainty	Vision	43.232784203237316	-54.74658930490418	189186
b52f42e8760328eea97d977dc11b0dadef6454f0	detection and classification of shot transitions	motion estimation;video indexing;correlation coefficient	The process of shot break detection is a fundamental component in automatic video indexing, editing and archiving. This paper introduces a novel approach to the detection and classification of shot transitions in video sequences including cuts, fades and dissolves. It uses the average interframe correlation coefficient and block-based motion estimation to track image blocks through the video sequence and to distinguish changes caused by shot transitions from those caused by camera and object motion. We achieve better results compared with two established techniques.	archive;coefficient;motion estimation;multiresolution analysis;precision and recall;region of interest	Sarah V. Porter;Majid Mirmehdi;Barry T. Thomas	2001		10.5244/C.15.9	computer vision;speech recognition;computer science;motion estimation;computer graphics (images)	Vision	39.33677591304927	-52.186136316484586	189730
b3a6a0c7f2c36c11887e93c519f44cb8d9a93435	a biologically-inspired computational model for transformation invariant target recognition	dynamic programming;biology computing;face authentication;image recognition;object recognition;learning algorithm;image resolution;image databases;military operations;biologically inspired computational model;neuro dynamic programming;dual heuristic dynamic programming;computer model;reinforcement learning;neurodynamic programming;biological system modeling;stochastic processes computational complexity dynamic programming face recognition image resolution learning artificial intelligence;geographic scene analysis;heuristic dynamic programming;biomedical imaging;medical practices;robotics;biology;medical robotics;large scale;visualization;artificial neural networks;computational modeling;face recognition;stochastic processes;adaptive critic design;target recognition;computational complexity;adaptive critic design framework;success rate;computational complexity biologically inspired computational model transformation invariant target recognition image recognition military operations robotics medical practices geographic scene analysis object recognition reinforcement learning adaptive critic design framework neurodynamic programming dual heuristic dynamic programming large scale umist 2d face database face authentication;artificial neural networks visualization biology biological system modeling image recognition dynamic programming image resolution;large scale umist 2d face database;learning artificial intelligence;military computing;transformation invariant target recognition;scene analysis	Transformation invariant image recognition has been an active research area due to its widespread applications in a variety of fields such as military operations, robotics, medical practices, geographic scene analysis, and many others. One of the primary challenges is detection and recognition of objects in the presence of transformations such as resolution, rotation, translation, scale and occlusion. In this work, we investigate a biologically-inspired computational modeling approach that exploits reinforcement learning (RL) for transformation-invariant image recognition. The RL is implemented in an adaptive critic design (ACD) framework to approximate the neuro-dynamic programming. Two ACD algorithms such as heuristic dynamic programming (HDP) and dual heuristic dynamic programming (DHP) are investigated and compared for transformation invariant recognition. The two learning algorithms are evaluated statistically using simulated transformations in 2-D images as well as with a large-scale UMIST 2-D face database with pose variations. Our simulations show promising results for both HDP and DHP for transformation-invariant image recognition as well as face authentication. Comparing the two algorithms, DHP outperforms HDP in learning capability, as DHP takes fewer steps to perform a successful recognition task in general. On the other hand, HDP is more robust than DHP as far as success rate across the database is concerned when applied in a stochastic and uncertain environment, and the computational complexity involved in HDP is much less.	approximation algorithm;authentication;automatic target recognition;computational complexity theory;computational model;computer vision;dynamic programming;heuristic;image resolution;machine learning;reinforcement learning;robotics;simulation	Khan M. Iftekharuddin;Yaqin Li	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633928	computer vision;visualization;image resolution;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;dynamic programming;robotics;computational complexity theory;computational model;reinforcement learning;artificial neural network	Vision	45.903366634723376	-52.23594992549219	189995
ec3f6eabc618d54bb87b30faacf86e201258799f	application of cooperative co-evolution in pedestrian detection systems	technology;computer science artificial intelligence;search trees;science technology;computer science interdisciplinary applications;pedestrian detection;computer science;computer science information systems;computer science theory methods;cooperative co evolution	In general, a shape-based[1] pedestrian detection system includes the following two steps:#R##N##R##N#(a) finding out and tracking a possible pedestrian figure, and#R##N##R##N#(b) determining if the candidate pedestrian figure is really a pedestrian figure by checking if it matches with any of the pedestrian templates.#R##N##R##N#Since there are a large number of templates, it is necessary to build up a search tree for the Match process [2,3]. Each node in the tree is one feature of the corresponding templates that can be used for classification and where each branch is one pedestrian template. Usually, the search tree is not adjustable during the matching process.	pedestrian detection	Xianbin Cao;Hong Qiao;Fei-Yue Wang;Xinzheng Zhang	2005		10.1007/11427995_98	simulation;computer science;artificial intelligence;machine learning;technology	NLP	42.055345541094546	-54.12752563661178	190136
b8f6613ef91767fce764c150290761e158bb00c2	pruning and matching scheme for rotation invariant leaf image retrieval	range search;rotation invariance;k nn;shape based image retrieval;sequence matching;dynamic time warping;image retrieval	For efficient content-based image retrieval, diverse visual features such as color, texture, and shape have been widely used. In the case of leaf images, further improvement can be achieved based on the following observations. Most plants have unique shape of leaves that consist of one or more blades. Hence, blade-based matching can be more efficient than whole shape-based matching since the number and shape of blades are very effective to filtering out dissimilar leaves. Guaranteeing rotational invariance is critical for matching accuracy. In this paper, we propose a new shape representation, indexing and matching scheme for leaf image retrieval. For leaf shape representation, we generated a distance curve that is a sequence of distances between the leaf’s center and all the contour points. For matching, we developed a blade-based matching algorithm called rotation invariant - partial dynamic time warping (RI-PDTW). To speed up the matching, we suggest two additional techniques: ⅰ) priority queue-based pruning of unnecessary blade sequences for rotational invariance, and ⅱ) lower bound-based pruning of unnecessary partial dynamic time warping (PDTW) calculations. We implemented a prototype system on the GEMINI framework [1][2]. Using experimental results, we showed that our scheme achieves excellent performance compared to competitive schemes.	image retrieval	Yoonsik Tak;Eenjun Hwang	2008	TIIS	10.3837/tiis.2008.06.001	computer vision;image retrieval;computer science;dynamic time warping;pattern recognition	Vision	39.93684655178812	-57.80027653396806	190537
30a82fc4d74c98718eb5a57a70431351ddbad468	detection of contours of wheels based on improved codebook	hough transform wheel contour detection circle contour detection codebook model codebook algorithm video recording binarization image optimal threshold algorithm;video recording edge detection feature extraction hough transforms image segmentation object detection traffic engineering computing;abstracts;detection of contours of wheels codebook model circularity circle detection	In this paper, a detection method of contours of wheels for high real-time is proposed. Since the classic codebook algorithm is difficult to deal with the situation such as global sudden light changes, a improved codebook algorithm is presented. First of all, the moving vehicle is extracted in the video recording the side of vehicle by the improved codebook algorithm. And then the contours of the object are extracted in the binarization image processed by the optimal threshold algorithm. Then all the circularity of the contours are calculated by the circularity formula to locate the circular contours. At last the circular contours located by calculating the circularity are verified by Hough transform circle detection method. Experimental results show that this method can locate the wheels of vehicle with high real-time and strong robustness.	algorithm;binary image;block cipher;circular shift;codebook;hough transform;real-time clock;video;wheels	Jun Zhang;Zhi-Qiang Zhu;Ping Liu;Wei-Han Zhang	2013	2013 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2013.6890348	computer vision;speech recognition;computer science;pattern recognition	Vision	41.05551078568777	-53.92400931500573	191512
ad02c9136298d6d0131f45f4d457d1b79cff964b	high-speed human motion recognition based on a motion history image and an eigenspace	motion analysis;eigenspace;tecnologia electronica telecomunicaciones;estimation mouvement;image processing;estimacion movimiento;procesamiento imagen;analyse mouvement;motion estimation;traitement image;motion recognition;feature extraction;human motion;high speed recognition;pattern recognition;espace propre;reconnaissance forme;motion history images;extraction caracteristique;analisis movimiento;tecnologias;reconocimiento patron;grupo a;motion history image;high speed	This paper proposes an efficient technique for human motion recognition based on motion history images and an eigenspace technique. In recent years, human motion recognition has become one of the most popular research fields. It is expected to be applied in a security system, man-machine communication, and so on. In the proposed technique, we use two feature images and the eigenspace technique to realize highspeed recognition. An experiment was performed on recognizing six human motions and the results showed satisfactory performance of the technique. key words: motion recognition, high-speed recognition, motion history images, eigenspace	kinesiology;motion history images;speech recognition	Takehito Ogata;Joo Kooi Tan;Seiji Ishikawa	2006	IEICE Transactions	10.1093/ietisy/e89-d.1.281	computer vision;image processing;feature extraction;eigenvalues and eigenvectors;computer science;motion estimation;computer graphics (images)	Robotics	45.817998048798565	-58.90092766812371	191873
11dd7b929c4f6d11dfb5fa4e8f8e534e646eb1b1	geometric probing of dense range data	decision tree classifier;object recognition;range data;decision tree;geometric probing;pose determination;pattern recognition object recognition;range image;pattern recognition;template matching geometric probing dense range data pose determination 3d objects dense range image data image point binary decision tree classifier tree leaf nodes voxel templates image seed location object recognition;decision trees classification tree analysis layout;template matching	A new method is presented for the efficient and reliable pose determination of 3D objects in dense range image data. The method is based upon a minimalistic Geometric Probing strategy that hypothesizes the intersection of the object with some selected image point, and searches for additional surface data at locations relative to that point. The strategy is implemented in the discrete domain as a binary decision tree classifier. The tree leaf nodes represent individual voxel templates of the model, with one template per distinct model pose. The internal nodes represent the union of the templates of their descendant leaf nodes. The union of all leaf node templates is the complete template set of the model over its discrete pose space. Each internal node also encodes a single voxel which is the most common element of its child node templates. Traversing the free is equivalent to efficiently matching the large set of templates at a selected image seed location. The method was implemented and extensive experiments were conducted for a variety of combinations of tree designs and traversals under isolated, cluttered, and occluded scene conditions. The results demonstrated a tradeoff between efficiency and reliability. It was concluded that there exist combinations of tree design and traversal which are both highly efficient and reliable.		Michael A. Greenspan	2002	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.993557	segment tree;computer vision;template matching;decision tree learning;computer science;cognitive neuroscience of visual object recognition;machine learning;decision tree;pattern recognition;interval tree;mathematics;tree traversal	Vision	42.49631603046986	-55.44397172659455	192388
662e969282d74a7c640340911abf19c37ddb77b6	model-based object recognition using range images by combining morphological feature extraction and geometric hashing	object recognition;mathematical morphology;image segmentation;information science;computational geometry;object recognition feature extraction voting layout image segmentation skeleton information science computer science solid modeling surface morphology;geometric hashing;morphological operation;3d object recognition model based object recognition range images morphological feature extraction geometric hashing image segmentation skeleton points salient feature primitives basis similarity constraint;salient feature primitives;morphological feature extraction;layout;skeleton;surface morphology;3d object recognition;computer vision;skeleton points;voting;range image;feature extraction;solid modeling;stereo image processing;range images;computer science;connected component;model based object recognition;basis similarity constraint	This paper proposes a new approach for model-based object recognition with range images by combining morphological feature extraction and geometric hashing. In low-level processing, range images are segmented into 3D-connected surface patches. In middle-level processing: each connected component is processed by using morphological operations to extract the skeletons of high-variation regions. These skeleton points can be viewed as invariant salient feature primitives. In high-level processing, geometric hashing is used to recognize objects. To reduce the number of spurious hypotheses, we propose a basis-similarity constraint. Experimental results have shown that the proposed method is effective and has great potential for model-based object recognition using range images.	feature extraction;geometric hashing;outline of object recognition	Chu-Song Chen;Yi-Ping Hung;Ja-Ling Wu	1996		10.1109/ICPR.1996.546089	layout;computer vision;mathematical morphology;connected component;voting;feature extraction;information science;computational geometry;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;image segmentation;solid modeling;skeleton	Robotics	41.76848697688494	-57.69258917383783	193818
cdba015be9db1e047a51b7e06403528b3551587e	shog - spherical hog descriptors for rotation invariant 3d object detection		We present a method for densely computing local spherical histograms of oriented gradients (SHOG) in volumetric images. The descriptors are based on the continuous representation of the orientation histograms in the harmonic domain, which we compute very efficiently via spherical tensor products and the fast Fourier transformation. Building upon these local spherical histogram representations, we utilize the Harmonic Filter to create a generic rotation invariant object detection system that benefits from both the highly discriminative representation of local image patches in terms of histograms of oriented gradients and an adaptable trainable voting scheme that forms the filter. We exemplarily demonstrate the effectiveness of such dense spherical 3D descriptors in a detection task on biological 3D images. In a direct comparison to existing approaches, our new filter reveals superior performance.	computation;executable;experiment;fast fourier transform;gradient;object detection;spherical basis	Henrik Skibbe;Marco Reisert;Hans Burkhardt	2011		10.1007/978-3-642-23123-0_15	computer vision;pattern recognition;mathematics;geometry	Vision	39.412724744825816	-56.459869971785295	194330
c20880cf66485418572fb1196ce1b7056687e09b	fast multi-face detection using facial component based validation		A human skin color range is used to find the candidates by segmentation and the rules of fuzzy factor are established by using facial characteristics to determine whether the candidate is face or not. We have separated two connected faces. In this paper, after segmentation with skin color, facial components are used instead of facial features to build fuzzy factors for verifying face candidates. Concurrently with detection, we separate connected candidates by Fast Marching but a fuzzy factor is built to choose radius which depends on the size of a candidate and the limit of spread. We use probability of rules and calculate probability of face. Experimental results demonstrate a successful face detection over a wide range of facial variations in color, position, scale, orientation, 3D pose, and expression in images from several photo collections (both indoors and outdoors). The computation time in this study is the same as that of the previous research, but the failure rate in the proposed algorithm is lower than that of the previous method.	algorithm;computation;face detection;failure rate;time complexity;verification and validation	Pham The Bao	2006				Vision	43.35606295461097	-56.95610755927074	194617
24e5350e8e508595fd29c378d082471ca5358a82	3d face computational photography using pca spaces	experimental tests;spatial coherence;computer vision;principal components analysis;range image;principal component analysis;computational photography;facial expression;3d face reconstruction;human perception;active shape model	In this paper, we present a 3D face photography system based on a facial expression training dataset, composed of both facial range images (3D geometry) and facial texture (2D photography). The proposed system allows one to obtain a 3D geometry representation of a given face provided as a 2D photography, which undergoes a series of transformations through the texture and geometry spaces estimated. In the training phase of the system, the facial landmarks are obtained by an active shape model (ASM) extracted from the 2D gray-level photography. Principal components analysis (PCA) is then used to represent the face dataset, thus defining an orthonormal basis of texture and another of geometry. In the reconstruction phase, an input is given by a face image to which the ASM is matched. The extracted facial landmarks and the face image are fed to the PCA basis transform, and a 3D version of the 2D input image is built. Experimental tests using a new dataset of 70 facial expressions belonging to ten subjects as training set show rapid reconstructed 3D faces which maintain spatial coherence similar to the human perception, thus corroborating the efficiency and the applicability of the proposed system.	active shape model;coherence (physics);computational photography;facial recognition system;principal component analysis;test set	Jesús P. Mena-Chalco;Ives Macedo;Luiz Velho;Roberto Marcondes Cesar Junior	2009	The Visual Computer	10.1007/s00371-009-0373-x	computer vision;computational photography;computer science;pattern recognition;mathematics;face hallucination;principal component analysis;computer graphics (images)	Vision	40.0151915487191	-57.37093778467131	194665
fb7146fbcc0035a18b2e107f019de38a83e1e7d7	3-d object recognition and orientation from both noisy and occluded 2-d data		Occluded 2-D Data D.P.IHing*, P.T.Fairney & R.J.Wiltshire,* *Dept. of Mathematics and Computing, +Dept. of Computer Studies, The Polytechnic of Wales, Pontypridd, Mid Glamorgan, CF37 1DL The generalised Hough transform[13] is commonly used [11,14,15,16,17]. Such approaches involve an accumulation of evidence which either supports or weakens a hypothesis on the pose of an object in the image. Objects are recognised by means of a rotation, scaling, translation (RST) transformation which maps observed image features onto features in the model. This is achieved by assuming that the object in the image can be represented by an instance of the model having undergone some RST transformation and that the object is rigid. Even with such limitations the approach is viable. A match is still possible even if some model features are missing, and if spurious features are found. As a result the method is able to cope with both noisy and partially occluded objects. We present a method of recognising 3-D objects and determining their position and orientation from single 2-D images of both noisy and partially occluded shapes.	algorithm;cluster analysis;definition;directed graph;hough transform;image scaling;intel matrix raid;map;outline of object recognition;precomputation;tree accumulation	D. P. Illing;P. T. Fairney;R. J. Wiltshire	1990		10.5244/C.4.38	computer vision;pattern recognition	Vision	43.26119353247845	-55.0606185330253	194934
44d829415ea8b3967c3cd91538c17e351007f721	a low-dimensional representation of human faces for arbitrary lighting conditions	computer vision face recognition lighting;computer vision;surface geometry;object recognition lighting image representations image analysis machine vision;face recognition;self shadowing surfaces human faces lighting conditions fixed object fixed viewpoint image intensity lighting changes face image;lighting	When recognizing a fixed object from a fixed viewpoint, the dominant source of variation in image intensity is lighting changes. We propose a low-dimensional model for human faces that can both synthesize a face image when given lighting conditions and can estimate lighting conditions when given a face image. The model can handle non-Lambertian and self-shadowing surfaces such as faces because it does not make any assumptions about either the surface geometry or bidirectional reflectance function. The model can be adapted to handle any arbitrary lighting condition, and is easily extendable to any other viewpoint or to any other object. >		Peter W. Hallinan	1994		10.1109/CVPR.1994.323941	facial recognition system;computer vision;computer science;volumetric lighting;lighting;per-pixel lighting;image-based lighting;computer graphics (images)	Vision	42.75205128231621	-52.98493236074751	195055
df6daa91b62dbbb691243f24def06f747d82b5b3	unsupervised approach for object matching using speeded up robust features	transforms image matching pattern clustering;homographic transform speeded up robust features surf density based clustering dbscan;density based clustering dbscan;speeded up robust features surf;homographic transform;transforms prototypes robustness clustering algorithms feature extraction filtering theory image edge detection;dbscan object matching speeded up robust features density based clustering scene image homography transforms surf points centroid based algorithm	Autonomous object counting system is of great use in retail stores, industries and also in research processes. In this paper, a Speeded Up Robust Feature (SURF) based robust algorithm for identifying, counting and locating all instances of a defined object in any image, has been proposed. The defined object is referred to as prototype and the image in which one wishes to count the prototype is referred to as scene image. The algorithm starts by detecting the interest points for SURF in both, prototype and scene images. The SURF points on prototype are first clustered using density based clustering; then SURF points in each cluster are matched with those in scene image. The SURF points in scene image that have been matched w.r.t. a single cluster, are clustered using the same clustering algorithm. Each cluster formed in scene image represents an instance of prototype object in the image. Homography transforms are further used to give exact location and span of each prototype object in the scene image. Once the span of each prototype is defined, SURF points within this span are matched with the prototype image and then Homography transform is once again applied while considering the newly matched SURF points; thus eliminating noisy detection/s of prototype. While the same process is repeated with each cluster, a novel centroid based algorithm for merging repeated detections of same prototype instance is used. Carrying the benefits of SURF and Homography transforms, the algorithm is capable of detecting all prototype instances present in scene image, irrespective of their scale and orientation. The complete algorithm has also been integrated into a desktop application, which uses camera feed to report the real time count of the prototype in the scene image.	algorithm;cluster analysis;dbscan;desktop computer;homography (computer vision);image resolution;prototype;sensor;speeded up robust features;test case;unsupervised learning	A. Harsha Vardhan;Nishchal K. Verma;Rahul Kumar Sevakula;Al Salour	2015	2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)	10.1109/AIPR.2015.7444541	computer vision;machine learning;pattern recognition;mathematics	Vision	43.511071272864434	-53.65845759697305	195442
c1f72b41abf54ad6d64791063f2d56b91856fc0f	real time video data mining for surveillance video streams	extraction information;analyse amas;streaming;estimation mouvement;video streaming;image processing;analisis datos;information extraction;surveillance;real time;estimacion movimiento;procesamiento imagen;motion estimation;video segmentation;segmentation;data mining;classification;traitement image;data analysis;transmission en continu;vigilancia;cluster analysis;senal video;signal video;monitoring;fouille donnee;temps reel;video signal;tiempo real;analyse donnee;analisis cluster;monitorage;monitoreo;busca dato;clasificacion;extraccion informacion;segmentacion	We extend our previous work [1] of the general framework for video data mining to further address the issue such as how to mine video data. To extract motions, we use an accumulation of quantized pixel differences among all frames in a video segment. As a result, the accumulated motions of segment are represented as a two dimensional matrix. Further, we develop how to capture the location of motions occurring in a segment using the same matrix generated for the calculation of the amount. We study how to cluster those segmented pieces using the features (the amount and the location of motions) we extract by the matrix above. We investigate an algorithm to find whether a segment has normal or abnormal events by clustering and modeling normal events, which occur mostly. In addition to deciding normal or abnormal, the algorithm computes Degree of Abnormality of a segment, which represents to what extent a segment is distant to the existing segments in relation with normal events. Our experimental studies indicate that the proposed techniques are promising.	algorithm;cluster analysis;data mining;pixel;streaming media;the matrix;tree accumulation	Jung Hwan Oh;JeongKyu Lee;Sanjaykumar Kote	2003		10.1007/3-540-36175-8_22	computer vision;image processing;biological classification;computer science;motion estimation;data mining;cluster analysis;data analysis;segmentation;information extraction;computer graphics (images)	ML	45.83520399578281	-57.28137982545218	196451
bc91d2c511baa470b4f37f7ea4f053d5461aea4c	model-based pose proposal for 2-d object recognition	object recognition	We consider the problem of nding a known two-dimensional object in an image, or verifying that it does not appear in the image. We adopt the strategy of doing a fast scan for potential places in the image where the object could be; we call this scan pose proposal. Each pose hypothesis is a set of edges that correspond to a subset of the transformed object boundary. Our algorithm works by nding U-shaped segments of object boundaries, doing a quick match process between U-shaped segments in the image and the model, and combining the matches into overall pose hypotheses. Analysis and experiments show that the algorithm runs eeciently, and does a good job of discarding all but a few spots in the image as possible pose hypotheses.	algorithm;experiment;outline of object recognition;verification and validation	Hemant D. Tagare;Drew McDermott	1996		10.1007/3-540-61859-7_16	deep-sky object;pose;computer science;viola–jones object detection framework;cognitive neuroscience of visual object recognition;3d single-object recognition	Vision	43.28180646982441	-54.457561668702446	196459
0ee52bcabdba529d145808d9c4e2d260ab499583	threshold-based lip segmentation using feedback of shape information		The shape and movement of the human lips convey valuable visual information which is used in various applications including: automatic lip-reading (ALR), emotion recognition, biometric speaker identification, and virtual face animation. The image processing to extract visual information from the lips typically involves three stages: face detection, location of the region of interest (ROI), and lip segmentation. This research focuses on lip segmentation as the accuracy of this component is crucial to the performance of the overall system. The challenge of lip segmentation arises from variability in the speaker profile (colour, shape, facial hair, make-up); and the dynamic ROI which changes as the teeth and tongue appear during speech movements. Lip segmentation techniques can be classified into two broad categories: colour-based techniques and model-based techniques. Colour-based techniques operate at pixel or neighbourhood level, and attempt to differentiate between lip and skin pixels based on colour features. While these techniques are simple and efficient, the major limitation is the automatic computation of robust thresholds. The model-based approach uses prior knowledge of the lip shape to construct a lip model, which is fitted to the image by optimising a cost function. Model-based techniques generally offer improved accuracy over colour-based techniques, however minimising a cost function can be computationally expensive which may affect real-time performance.	analysis of algorithms;automated lip reading;biometrics;computation;emotion recognition;face detection;image processing;loss function;neighbourhood (graph theory);pixel;real-time clock;region of interest;spatial variability;speaker recognition	Ashley D. Gritzman;Vered Aharonson;David M. Rubin;Adam Pantanowitz	2015		10.1145/2813852.2813856	computer vision;speech recognition	Vision	41.615290204640694	-53.9743944832667	196738
41d54c93513554493669e52972aa52d53600f2f6	deformable pedal curves with application to face contour extraction	curve evolution;level set;computational geometry;computational geometry face recognition feature extraction;fixed point;face recognition;human face representation deformable pedal curve face contour extraction pedal point global parameterized shape curve evolution hybrid geometric active model;feature extraction;shape deformable models solid modeling level set active contours object detection robotics and automation intelligent robots humans face	Pedal curves are the loci of the feet of perpendiculars to the tangents of a fixed curve to a fixed point called the pedal point. By varying the location of the pedal point, deformable pedal curves have an important feature of incorporating a global parameterized shape into the curve evolution framework. In this paper, a hybrid geometric active model based on deformable pedal curves for face contour extraction is presented. Taking advantage of the deformable pedal curves, the proposed model can allow for representation of global and local shape characteristic of human face. Moreover, by implementing the model in a level set framework, automatic topological changes can be achieved naturally. Experimental results show the validity of our approach.	chan's algorithm;contour line;fixed point (mathematics);gradient	Fuzhen Huang;Jianbo Su	2003		10.1109/CVPR.2003.1211371	facial recognition system;computer vision;simulation;feature extraction;computational geometry;computer science;level set;mathematics;geometry;fixed point	Vision	42.62181750457605	-56.46253551916168	197393
ef3e4f4f2fc5abce469540b37446e96cd9fe0613	pattern analysis for an automatic and low-cost 3d face acquisition technique	3d point cloud;dynamic programming algorithm;adaptive optimization;dynamic program;epipolar geometry;stereo matching;face modeling;geometric model;pattern analysis	This paper proposes an automatic 3D face modeling and localizing, based on active stereovision. In offline stage, the optical and geometrical parameters of the stereosensor are estimated. In online acquisition stage, alternate complementary patterns are successively projected. The captured right and left images are separately analyzed in order to localize left and right primitives with a sub pixel precision. Also, this analysis provides us an efficient segmentation of facial informative region. Epipolar geometry transforms stereo matching problem into a one-dimensional search problem. Indeed, we employ an adapted and optimized dynamic programming algorithm to pairs of primitives which are already located in each epiline. 3D geometry is retrieved by computing the intersection of optical rays coming from the pair of matched features. A pipeline of geometric modeling techniques is applied to densify obtained 3D point cloud, mesh and texturize 3D final face model. An adequate evaluation strategy is proposed and experimental results are provided.		Karima Ouji;Mohsen Ardabilian;Liming Chen;Faouzi Ghorbel	2009		10.1007/978-3-642-04697-1_28	adaptive optimization;computer vision;computer science;theoretical computer science;geometric modeling;machine learning;dynamic programming;epipolar geometry	Vision	42.52547172421286	-55.489140337247	197902
5f79c56512e54cda1ebb0ed0c2706f49e1934212	a theorem proving based pattern recognition system	topology;object recognition;range data;search space;layout;theorem proving;laser theory;filtering algorithms;pattern matching;pattern recognition;pattern recognition topology object recognition layout artificial intelligence laboratories computer science pattern matching filtering algorithms laser theory;artificial intelligence;computer science;object identification	The r e c o g n i t i o n  p r o c e s s  u e s a th ree  phase approach .  F i r s t ,  hypotheses are generated  which co r re spond  t o model d e s c r i p t o r s t h a t are l i k e l y t o match the data.  Evidence is a p p l i e d  t o  v i a b l e hypo theses  t o  p roduce a p a r t i a l match. The partial match is then used t o c o n s t r a i n t h e f u l l r e c o g n i t i o n p r o c e s s  w h i c h  l e a d s  t o  o b j e c t  i d e n t i f i c a t i o n . T h i s	artificial intelligence;automated theorem proving;pattern recognition;station hypo	Michael J. Magee;Mitchell Nathan	1986		10.1109/ROBOT.1986.1087455	layout;computer vision;feature;computer science;artificial intelligence;theoretical computer science;cognitive neuroscience of visual object recognition;machine learning;pattern matching;mathematics;automated theorem proving;3d single-object recognition;sketch recognition	AI	45.76468110524835	-54.11048757720578	198791
9e837df910b7795db3f5eecb305aeae1f2711f0f	learning face localization using hierarchical recurrent networks	reconnaissance visage;local recurrence;interfase usuario;hierarchical neural networks;user interface;imagen nivel gris;computer applications;application informatique;face recognition;recurrent network;image niveau gris;pattern recognition;interface utilisateur;reseau neuronal recurrent;reconnaissance forme;recurrent neural networks;learning artificial intelligence;reseau neuronal;reconocimiento patron;grey level image;red neuronal;human computer interface;neural network;apprentissage intelligence artificielle	One of the major parts in human-computer interface applications, such as face recognition and video-telephony, consists in the exact localization of a face in an image.Here, we propose to use hierarchical neural networks with local recurrent connectivity to solve this task, even in presence of complex backgrounds, difficult lighting, and noise. Our network is trained using a database of gray-scale still images and manually determined eye coordinates. It is able to produce reliable and accurate eye coordinates for unknown images by iteratively refining an initial solution.The performance of the proposed approach is evaluated against a large test set. The fast network update allows for real-time operation.		Sven Behnke	2002		10.1007/3-540-46084-5_213	facial recognition system;simulation;computer science;artificial intelligence;recurrent neural network;machine learning;computer applications;user interface;artificial neural network	Vision	44.882671249931136	-58.10284049013319	199135
ce95419c42b1612d01fda00e6b578afb3db8f89f	a comparative analysis of face recognition performance with visible and thermal infrared imagery	radiometric calibration;comparative analysis;thermal infrared;thermal images;false alarm rate;calibration face recognition infrared imaging;infrared images;face anatomy;recognition;radiometrically calibrated thermal imagery comparative analysis face recognition performance visible infrared imagery thermal infrared imagery ir imagery performance analysis appearance based face recognition methodologies false alarm rates;face recognition;infrared imaging;performance analysis;image analysis performance analysis face recognition infrared imaging image recognition lighting infrared image sensors biometrics thermal sensors infrared spectra;algorithms;methodology;calibration;thermal imagery	We present a comprehensive performance analysis of multiple appearance-based face recognition methodologies, on visible and thermal infrared imagery. We compare algorithms within and between modalities in terms of recognition performance, false alarm rates and requirements to achieve specified performance levels. The effect of illumination conditions on recognition performance is emphasized, as it underlines the relative advantage of radiometrically calibrated thermal imagery for face recognition.	algorithm;biometrics;facial recognition system;requirement;uncontrolled format string	Diego A. Socolinsky;Andrea Salgian	2002		10.1109/ICPR.2002.1047436	facial recognition system;qualitative comparative analysis;computer vision;calibration;speech recognition;computer science;methodology;constant false alarm rate	Vision	44.072493679880616	-57.08207550990229	199328
039fcfba791ff88e41b63786eb583e62fd973387	robust content-based video copy identification in a large reference database	television;analisis contenido;base donnee;distance measure;recherche image;interest points;real time;localization;database;base dato;localizacion;soft real time;production process;vecino mas cercano;content analysis;localisation;local features;tolerancia;temps reel;diffusion donnee;difusion dato;tiempo real;plus proche voisin;approximate nearest neighbor;nearest neighbour;tolerance;data broadcast;analyse contenu;cumulant;video database;content based retrieval;recherche par contenu;image retrieval	This paper proposes a novel scheme for video content-based copy identification dedicated to TV broadcast with a reference video database exceeding 1000 hours of video. It enables the monitoring of a TV channel in soft real-time with a good tolerance to strong transformations that one can meet in any TV post-production process like: clipping, cropping, shifting, resizing, objects encrusting or color variations. Contrary to most of the existing schemes, the recognition is not based on global features but on local features extracted around interest points. This allows the selection and the localization of fully discriminant local patterns which can be compared according to a distance measure. Retrieval is performed using an efficient approximate Nearest Neighbors search and a final decision based on several matches cumulated in time.		Alexis Joly;Carl Frélicot;Olivier Buisson	2003		10.1007/3-540-45113-7_41	speech recognition;internationalization and localization;content analysis;image retrieval;computer science;pattern recognition;data mining;database;scheduling;television;world wide web;statistics;cumulant	Vision	44.95974616453738	-58.19306986481133	199735
eece4e232f5b4d39dec013123a987192c1fe4865	a new algorithm of counting human based on segmentation of human faces in color image	rgb color space;color space;humans image segmentation color face detection skin colored noise hair shape brightness eyes;skin;image colour analysis face recognition feature extraction;hsv color space;data mining;hue saturation value;red green blue;face recognition;human counting;shape;mixed model;image color analysis;image colour analysis;feature extraction;pixel;face contour;human face segmentation;humans;face contour human counting human face segmentation color image face detection rgb color space hsv color space;face detection;color image;hair	The face detection problem has attracted wide attention. Some achievements have been obtained in the past decade. The paper presented a novel algorithm of face detection based on segmentation of human faces in color image. The number of human can be counted and can be used to control the lights in a classroom to be all turned on or not based on proposed method in the paper. First, this paper constructs a mixed model in both RGB (Red, Green, Blue) and HSV (Hue, Saturation, Value) color space, which can be used to acquire possible skin areas in original image. Second, non-face objects such as human hands and arms in the image are eliminated with the character of face contour. Thus, the candidate faces are extracted. Third, confirmed faces can be extracted by using the color and shape characteristics of hair. Experiment result shows this algorithm is efficient and available.	algorithm;coat of arms;color image;color space;face detection;mixed model	Li-hua Zou;Yun-cheng Liu	2009	2009 International Conference on Computational Intelligence and Security	10.1109/CIS.2009.16	facial recognition system;color histogram;computer vision;hsl and hsv;computer science;computer graphics (images)	Vision	40.78431205759711	-52.630395473909694	200000
