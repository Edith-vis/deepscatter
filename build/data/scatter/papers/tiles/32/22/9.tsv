id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4aad6387e9040e88800c0f666f979f2680aa96d8	real-time road traffic classification using on-board bus video camera	cameras vehicle detection cities and towns intelligent transportation systems videoconference road safety image segmentation computer vision global positioning system detectors;obstacle detection;feature detection;image segmentation;odometer;video signal processing;road traffic;intelligent transportation systems;real time;city traffic management authority video based real time urban road traffic classification on board bus video camera analysis road safety vehicle speed estimation feature detection feature tracking image segmentation odometer gps data computational complexity;video analysis;feature tracking;buses;image classification;video signal processing feature extraction image classification image segmentation road traffic road vehicles traffic engineering computing video cameras;real time information;traffic management;classification;on board bus video camera analysis;computational complexity;video cameras;feature extraction;it adoption;gps data;cost effectiveness;traffic engineering computing;vehicle speed estimation;traffic characteristics;city traffic management authority;road safety;conferences;video based real time urban road traffic classification;road vehicles	"""On-board video analysis has attracted a lot of interest over the two last decades, mainly for safety improvement (through e.g. obstacles detection or drivers assistance). In this context, our study aims at providing a video-based real-time understanding of the urban road traffic. Considering a video camera fixed on the front of a public bus, we propose a cost-effective approach to estimate the speed of the vehicles on the adjacent lanes when the bus operates on its reserved lane. We propose to work on 1-D segments drawn in the image space, aligned with the road lanes. The relative speed of the vehicles is computed by detecting and tracking features along each of these segments, while the absolute speed of vehicles is estimated from the relative one thanks to odometer and/or GPS data. Using pre-defined speed thresholds, the traffic can be classified in real-time into different categories such as """"fluid"""", """"congestion""""... As demonstrated in the evaluation stage, the proposed solution offers both good performances and low computing complexity, and is also compatible with cheap video cameras, which allows its adoption by city traffic management authorities."""	global positioning system;network congestion;on-board data handling;performance;real-time computing;real-time locating system;real-time transcription;reserved word;sensor;traffic classification;video content analysis	Christophe Parisot;Jérôme Meessen;Cyril Carincotte;Xavier Desurmont	2008	2008 11th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2008.4732628	computer vision;simulation;floating car data;geography;video tracking;computer security	Vision	42.81469986196423	-42.80461362831182	153129
7abad6720d8dc41c3a63798df2cba19367701fbb	pathology growth model based on particles	surgical simulation;virtual reality;growth process;growth model	Virtual reality based surgical simulators offer the possibility to provide training on a wide range of findings of different pathologies. Current research aims at a high fidelity hysteroscopy simulator. Different methods for the generation of pathologies have been investigated to realize the first surgical simulator that challenges the trainee with a new scene in every training session. In this paper, a particles-based tumor growth model is presented that overcomes different limitations of previous approaches. It allows for a realistic generation of both polyps and myomas protruding to different extents into the uterine cavity. The model incorporates several biological as well as mechanical factors, which influence the growth process and thus the appearance of the pathologies.	algorithm;coherence (physics);colon classification;computer stereo vision;computer vision;finite element method;population dynamics;simulation;virtual reality	Raimundo Sierra;Michael Bajka;Gábor Székely	2003		10.1007/978-3-540-39899-8_4	simulation;computer science;virtual reality;surgery	Vision	39.19692927142132	-38.62666462219907	153313
06ba744d7af9c0bcc17fc3c971b8d3e3fbadb125	automated abnormal behavior detection for ubiquitous healthcare application in daytime and nighttime	microphones;visualization detection algorithms cameras video sequences microphones joints medical services;statistical moment;image motion analysis;detection algorithms;ubiquitous computing computer vision handicapped aids health care image motion analysis image sensors image sequences statistical analysis;shape analysis;video sequences;joints;image sensors;computer vision;visualization;handicapped aids;medical services;statistical analysis;image sequence;detection algorithm;ubiquitous computing;test image sequences automated abnormal behavior detection ubiquitous healthcare application daytime night time computer vision algorithm normal daily activities statistical moment analysis human shape analysis image sensor based application elderly people fall event detection;image sensor;cameras;image sequences;health care	Abnormal behaviors such as falls are the most significant issues in ubiquitous healthcare applications for the elderly. The goal of this research is to develop a novel computer vision algorithm, which can allow discriminating between abnormal behaviors and normal daily activities using statistical moment analysis and human shape analysis in daytime and nighttime environment. Many researchers recently presented their work aimed at developing an image sensor based application to detect unexpected fall events. Until now, most studies on the abnormal behavior detection of the elderly have been presented for the systems in daytime. However, they cannot give any solution to detect falls in both daytime and nighttime. To overcome the problem, our method is implemented to distinguish abnormal activities from normal activities. Experimental results show very promising results on test image sequences of normal daily activities and simulated fall activities.	algorithm;computer vision;image sensor;shape analysis (digital geometry);standard test image	Young-Sook Lee;Nguyen Trung Hau	2012	Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics	10.1109/BHI.2012.6211545	computer vision;simulation;computer science;multimedia	Robotics	39.910622902541895	-44.89878519773859	153419
1c869993ee8a908b05ee7c179bac36ee8a15ef77	design of an audience voting system for the olympic games	image processing;judging board;audience voting system;olympic games	In this paper we describe an audience voting system which can be used for all kinds of judged sport events like diving, synchronised swimming, gymnastics, and ice-skating. The basis of the system is cameras, which are fixed to the ceiling. Each camera can cover approximately 1000 spectators of the audience. The image processing software recognises the judging, displayed by boards from every individual spectator. The cost of the solution is quite low, because we need less than 20 cameras for an audience of up to 15,000 spectators and you can use them for surveillance purposes, too.	image processing	Ramon Schalleck;Marcin Bober;Heiko Drewes	2004		10.1145/985921.986174	simulation;image processing;computer science;multimedia	Mobile	44.80361739033838	-40.971420368432604	154414
a7d7af8363a6bcc1f31a998bfce215c0f800e767	pedestrian detection by range imaging	effective potential;change detection;time of flight;range imaging;3d camera;range image;pedestrian detection;stereo;changing illumination;shadow effects	Remote detection by camera offers a versatile means for recording people activities. Relying principally on changes in video images, the method tends to fail in presence of shadows and illumination changes. This paper explores a possible remedy to these problems by using range cameras instead of conventional video cameras. As range is an intrinsic measure of object geometry, it is basically not affected by illumination. The study described in this paper considers range detection by two state-of-the art cameras, namely a stereo and a time-of-flight camera. Performed investigations consider typical situations of pedestrian detection. The presented results are analyzed and compared in performance with conventional results. The study shows the effective potential of range camera to get rid of light change problems like shadow effects but also presents some current limitations of range cameras.	digital camera;pedestrian detection;range imaging;time-of-flight camera	Heinz Hügli;Thierry Zamofing	2007			stereo camera;computer vision;time of flight;effective potential;stereophonic sound;three-ccd camera;change detection;statistics;computer graphics (images)	Vision	45.59794533825537	-42.719086575132266	155789
66dcd246d143f919cb7239f8792edbaccf48dc2f	real-time stereo matching system		Getting depth information by stereo matching is one of the key steps in 3D reconstruct. In many practical applications, there are high requirements for the speed of processing and the accuracy of the results. Many algorithms have obtained good results in processing precision, like SGM. However, the processing speed often does not meet the real-time requirements. In this paper, we improve the traditional stereo matching method SGM so that it is able to meet the real-time requirements. We implement our improved algorithm in TX2 with parallel programming. And in the experiments, it shows that our algorithm obtains 21 fps for the video gained by ZED camera size of 640 * 360 pixels, 32 disparity levels and using 4 path directions for the traditional SGM method. To measure the accuracy of our method, we use Middlebury dataset as indoor scene and the video obtained by ZED camera as outdoor scene to exam our algorithm separately. The results show that we get great balance between the speed and the accuracy.	computer stereo vision;real-time operating system;real-time transcription	Angfan Zhu;Zhiguo Cao;Yang Xiao	2018		10.1007/978-3-319-97589-4_32	control engineering;engineering;pixel;computer vision;artificial intelligence	Vision	44.30067610228211	-39.341295112926645	155916
7bd6622f54bb3f9a511d93d897295b6e3ab0f7d3	vision-based absolute localization for unmanned aerial vehicles	cameras global positioning system robustness estimation image registration mutual information;robot vision autonomous aerial vehicles cameras image registration;flight tests vision based absolute localization unmanned aerial vehicles georeferenced aerial images multiple usage localization algorithm mutual information mi global scene variations local scene variations georeferenced images uav camera	This paper presents a method for localizing an Unmanned Aerial Vehicle (UAV) using georeferenced aerial images. Easily maneuverable and more and more affordable, UAVs have become a real center of interest. In the last few years, their utilization has significantly increased. Today, they are used for multiple tasks such as navigation, transportation or vigilance. Nevertheless, the success of these tasks could not be possible without a highly accurate localization which can, unfortunately be often laborious. Here we provide a multiple usage localization algorithm based on vision only. However, a major drawback with vision-based algorithms is the lack of robustness. Most of the approaches are sensitive to scene variations (like season or environment changes) due to the fact that they use the Sum of Squared Differences (SSD). To prevent that, we choose to use the Mutual Information (MI) which is very robust toward local and global scene variations. However, dense approaches are often related to drift disadvantages. Here, we solve this problem by using georeferenced images. The localization algorithm has been implemented and experimental results are presented demonstrating the localization of a hexarotor UAV fitted with a downward looking camera during real flight tests.	aerial photography;algorithm;central processing unit;concept drift;image registration;mutual information;real-time clock;solid-state drive;unmanned aerial vehicle	Aurelien Yol;Bertrand Delabarre;Amaury Dame;Jean-Emile Dartois;Éric Marchand	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943040	computer vision;simulation;remote sensing	Robotics	44.764250715226304	-39.4255866956761	156359
be88e52248f6eb0152f3f4f8c329e4818fc9c1a0	a real world system for detection and tracking	microcontrollers;image motion analysis;image segmentation;image processing;false detections;computer vision task;real time;object detection image motion analysis image segmentation;pixel recursive comparison;video system;reference frame;computer system;detection segmentation foreground background extraction;segmentation;detection;data mining;motion detection computer vision adaptive algorithm image processing application software real time systems tracking hardware microcontrollers computer interfaces;motion tracking;computer vision;adaptive algorithm;background;foreground;missed detections;pixel;background subtraction;computer system background subtraction computer vision task pixel recursive comparison image processing video system motion real time detection motion tracking false detections missed detections;motion real time detection;real time application;motion detection;cameras;extraction;tracking;object detection;hardware	As Background subtraction is a common computer vision task; we analyze the usual pixel-level approach. We develop an efficient adaptive algorithm system based on pixel comparison. Recursive comparison of pixel is used between the present frame and the reference frame. This algorithm is implemented using image processing in MatLAB Environment and we work on making it a real-time applicable tool for various possible applications. Thus an attempt to build a video system for real-time detection and tracking of motion which has the ability to minimize both false detections and missed detections, interfaced with a hardware unit based on microcontroller, communicating serially with the computer system as a control unit panel prototype. It is capable of processing 320×240 video at 28fps, excluding post processing	adaptive algorithm;background subtraction;computer vision;control unit;graphics display resolution;image processing;microcontroller;pixel;prototype;real-time clock;real-time locating system;recursion (computer science);reference frame (video);sensor;vii;world-system	I. Sreenivasa Rao;P. H. S. T. Murthy;Subhabrata Nanday;Veera Malleswara Rao	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.162	reference frame;microcontroller;computer vision;extraction;real-time computing;background subtraction;image processing;computer science;foreground-background;tracking;image segmentation;segmentation;pixel;computer graphics (images)	Robotics	45.07371154642004	-42.482619186478004	157280
49cc6612d986e494a44556fed4dd031866013ff5	a single target voting scheme for traffic sign detection	detectors;hough like detectors;object recognition;probability;computed tomography;road traffic;advanced driver assistance systems;pixel image color analysis detectors image edge detection shape computed tomography transforms;single target voting scheme;road traffic driver information systems image colour analysis object detection object recognition probability;triangular shape detection;image processing chain;shape;circular shape detection;image edge detection;triangular shape detection single target voting scheme traffic sign detection traffic sign recognition advanced driver assistance systems image processing chain hough like detectors probabilistic measurement circular shape detection;image color analysis;image colour analysis;traffic sign recognition;pixel;transforms;traffic sign detection;driver information systems;object detection;probabilistic measurement	Traffic sign detection and recognition is an important part of advanced driver assistance systems. Many prototype solutions for this task have been developed, and first commercial systems have just become available. Their image processing chain can be devided into three steps, preprocessing, detection, and recognition. Albeit several reliable sign recognition algorithms exist by now sign detection under real-world conditions is still unstable. Therefore, we address the first two steps of the processing chain presenting an analysis of widely used detectors, namely Hough-like methods. We evaluate several preprocessing steps and tweaks to increase their performance. Hence, the detectors are applied to a large, publicly available set of images from real-life traffic scenes. As main result we establish a new probabilistic measure for traffic sign colour detection and, based on the findings in our analysis, propose a novel Hough-like algorithm for detecting circular and triangular shapes. These improvements significantly increased detection performance in our experiments.	algorithm;control theory;experiment;hough transform;image processing;preprocessor;prototype;real life;sensor	Sebastian Houben	2011	2011 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2011.5940429	computer vision;simulation;computer science;computer graphics (images)	Vision	42.6530742300924	-44.69946173053176	157357
390f73623783fc57f60e8ae4d47d66a289460531	on the identification of snow movements on roads	false alarm;automatic incident detection;video streaming object detection road traffic snow transportation video cameras;video streaming;intelligent transportation systems real time snow detection automatic incident detection;snow;intelligent transport system;road traffic;intelligent transportation systems;real time;glare processing;snow intelligent transportation systems cameras vehicles cities and towns rain layout robustness road accidents environmental factors;intelligent transportation system;false alarm rate;aid system;real time detection;real time snow detection;video cameras;transportation;snow map generation;real time detection snow movements identification video stream intelligent transportation system automatic incident detection aid system false alarm glare processing background generation snow sample correlation snow map generation traffic video camera;traffic video;background generation;snow movements identification;video stream;object detection;camera;snow sample correlation	One of the key components of an intelligent transportation systems is video-based automatic incident detection (AID). An AID system is able to detect incidents that require operator intervention. However, the accuracy of an AID system operating during the winter suffers from high false alarm rates due to the movement of snow on the roads. In this paper, a robust algorithm is proposed to detect moving snow in video streams and improve the rate of detection by having the AID system to reduce its sensitivity in the area that has snow movement. The proposed algorithm conducts glare processing, background generation & differencing, snow sample correlation and final snow map generation. The feasibility of the proposed algorithm has been evaluated using traffic videos captured from several cameras from the City of Calgary. This algorithm demonstrates accurate and real-time detection of moving snow	algorithm;autoregressive integrated moving average;real-time clock;snow;streaming media	Jun Cai;Muzamil S. Pervez;Mohamed S. Shehata;Robert Johannesson;Wael M. Badawy;Ahmad Radmanesh	2006	2006 IEEE Workshop on Signal Processing Systems Design and Implementation	10.1109/SIPS.2006.352608	embedded system;computer vision;intelligent transportation system;simulation;computer science	Robotics	42.713772369901314	-42.84223388703098	157865
23fbd5e41af1e8015ef55854fff9ca4c6d9fbddc	plined: vision-based power lines detection for unmanned aerial vehicles		It is commonly accepted that one of the most important factors for assuring the high performance of an electrical network is the surveillance and the respective preventive maintenance. From a long time ago that TSOs and DSOs incorporate in their maintenance plans the surveillance of the grid, where is included the aerial power lines inspection. Those inspections started by human patrol, including structure climbing when needed and later were substituted by helicopters with powerful sensors and specialised technicians. More recently the Unmanned Aerial Vehicles (UAV) technology has been used, taking advantage of its numerous advantages. This paper addresses the problem of improving the real-time perception capabilities of UAVs for endowing them with capabilities for safe and robust autonomous and semi-autonomous operations. It presents a new vision based power line detection algorithm denoted by PLineD, able to improve the detection robustness even in the presence of image with background noise. The algorithm is tested in real outdoor images of a dataset with multiple backgrounds and weather conditions. The experimental results demonstrate that the proposed approach is effective and able to implemented in real-time image processing pipeline.	aerial photography;algorithm;autonomous robot;dsos;edge detection;image processing;real-time clock;semiconductor industry;sensor;unmanned aerial vehicle	T. Santos;M. Moreira;J. Almeida;Amândio A Dias;A. Martins;J. Dinis;J. Formiga;Eduardo P. da Silva	2017	2017 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)	10.1109/ICARSC.2017.7964084	image processing;electric power transmission;grid;robustness (computer science);noise measurement;background noise;computer vision;image segmentation;climbing;computer science;artificial intelligence	Robotics	44.013603522848726	-40.596842478879374	159766
4ffe733169650118fa0ef78eb3cc315b99758b70	experimental evaluation of intelligent fault detection system for inspection of sewer pipes	mechanical engineering computing;edge detection;cost reduction;intelligent systems fault detection inspection surface cracks intelligent robots costs cameras radar detection infrared detectors building materials;inspection;gui experimental evaluation intelligent fault detection system sewer pipes inspection industrial automation underground facilities inspection real time system cost reduction time reduction inspection robot edge detection;underground facilities inspection;intelligent fault detection system;time reduction;graphical user interfaces;robot vision;false positive rate;automatic detection;robot vision crack detection edge detection fault diagnosis graphical user interfaces inspection mechanical engineering computing pipes;inspection robot;crack detection;fault detection;intelligent system;gui;real time system;experimental evaluation;industrial automation;pipes;sewer pipes inspection;fault diagnosis;real time systems	Automation is an important issue in industry, particularly in inspection of underground facilities. This paper describes experimental evaluation of intelligent system for automatically detecting faulty areas in a real sewer pipe system. The proposed system can detect various types of faults and be implemented in a real time system. Its detection performance is 100%, when the false positive rate is 34%. This ratio is acceptable for sewer inspection, and the reduction of time and cost is also realized.	artificial intelligence;automation;fault detection and isolation;named pipe;real-time computing;sensor	Alireza Ahrary;Masumi Ishikawa;M. Okada	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399400	embedded system;computer vision;simulation;real-time operating system;computer science;engineering;graphical user interface;forensic engineering	Robotics	43.928023391055824	-42.12675238769377	160223
959e894cbbe1151284ca98998c517dd9c96ab953	a real-time people counter	image processing;real time tracking;real time;traffic flow;imaging system;heavy traffic;tracking	This paper describes an implementation method for the people counting system which detects and tracks moving people using a fixed single camera. This system counts the number of moving objects (people) entering the security door. Moreover, the detected objects are tracked by the proposed tracking algorithm before entering the door. The proposed system with Intel Pentium IV operates at an average rate of 10 frames a second on real world scenes where up to 6 persons come into the view of a vertically mounted camera.	algorithm;p5 (microarchitecture);people counter;real-time clock	Gary Conrad;Richard Johnsonbaugh	1994		10.1145/326619.326649	computer vision;real-time computing;simulation;tracking system;image processing;computer science;traffic flow;tracking	Vision	45.321110538019965	-41.69801283873727	160266
65f9943fbcea6154c39a5483bddd0d4286d888df	real-time accurate crowd counting based on rgb-d information	video surveillance image colour analysis image matching image segmentation image sensors object detection pedestrians;video surveillance;image segmentation;image matching;image sensors;pedestrians;image colour analysis;real time systems buildings surveillance image segmentation robustness cameras skeleton;real time system surveillance crowd counting people counting rgb d data;fast crowd counting real time accurate crowd counting rgb d information intelligent visual surveillance systems passing people rgb plus depth commodity depth camera head shoulder surveillance region fast template matching depth information pedestrian filling convex hull segmentation;object detection	Real-time accurate crowd counting is one of important tasks in intelligent visual surveillance systems. Most previous works can only count passing people robustly without heavy occlusions which are very common in the practical surveillance scenes. To solve this difficult problem, we propose a new method for crowd counting for RGB-D (RGB plus depth) data using a commodity depth camera. In our method, we first detect each head-shoulder of the passing or still person in the surveillance region with fast template matching based on depth information including pedestrian filling with convex hull segmentation. Then, we track and count each detected head-shoulder based on RGB information bidirectionally. By using this approach, we have built a practical system for robust and fast crowd counting. Extensive experimental results show that our method achieves significant improvement comparing to states-of-the-art approach, and the built system is not only robust to heavy occlusions, but also can be deployed in the real time crowd counting application scenes.	convex hull;duplex (telecommunications);left 4 dead 2;real-time transcription;template matching	Huiyuan Fu;Huadong Ma;Hongtian Xiao	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467452	computer vision;simulation;computer science;image sensor;image segmentation;computer graphics (images)	Robotics	43.622238207119715	-43.582518322751355	160676
afb1149e8e86aba7d3da8489f7886cc7cabd7ef2	car park occupancy analysis using uav images		With the development of unmanned aerial vehicles (UAVs) and the relevant techniques, UAVs become common and popular for civilian applications such as remote sensing tasks. The reason is because they are cheap, flexible, and easy to set up. Car park occupancy analysis is important for authorities to make decisions on the design, plan and management of car parks. To have a quick knowledge of current parking situations, we proposed to use UAV images to count how many cars are parked during different periods. In this paper, our major contribution is a novel car counting approach for UAV images. Different from traditional detection- or segmentation-based counting techniques, the proposed counting method is density estimation based that does not need intense collection and learning procedures. We transform the car counting problem into the estimation of density values over pixels of an image. Experimental results have been conducted on real car park scenarios and all the results show that our method can provide a promising estimation of car numbers.	aerial photography;counting problem (complexity);ground truth;pixel;unmanned aerial vehicle	Hailing Zhou;Lei Wei;Michael Fielding;Douglas C. Creighton;Sameer Deshpande;Saeid Nahavandi	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123131	occupancy;pixel;computer vision;machine learning;density estimation;counting problem;computer science;artificial intelligence	Robotics	42.53684107564028	-41.67337042165884	160883
d17b312de8df427bffdf2edc5438d47f0b61b3e6	reasoning about threats: from observables to situation assessment	threat assessment;image motion analysis;image processing;rating;video signal processing;hidden markov model;training;situation understanding;data processing;observables;ts technical sciences;inference mechanisms;video features;crowd control;conditional random field situation assessment reasoning threat assessment antiterrorism crowd control learning examples train station crowd dynamics video features shape features motion features;hidden markov models;video signal processing image motion analysis inference mechanisms public administration;threat recognition architecture evaluation framework information processing observables situation understanding;negative rates;architecture evaluation;information processing;experiments;conditional random field;pattern recognition;crowd dynamics;ii intelligent imaging;real world experiment;threat recognition;robustness;train stations;sensor signals;probabilistic logic;false positive;pattern recognition hidden markov models probabilistic logic robustness training information processing;architecture;situation assessment;physics electronics;terrorism;evaluation framework;public administration	We propose a mechanism to assess threats that are based on observables. Observables are properties of persons, i.e., their behavior and interaction with other persons and objects. We consider observables that can be extracted from sensor signals and intelligence. In this paper, we discuss situation assessment that is based on observables for threat assessment. In the experiments, the assessment is evaluated for scenarios that are relevant to antiterrorism and crowd control. The experiments are performed within an evaluation framework, where the setup is such that conclusions can be drawn concerning: 1) the accuracy and robustness of an architecture to assess situations with respect to threats; and 2) the architecture's dependence of the underlying observables in terms of their false positive and negative rates. One of the interesting conclusions is that discriminative assessment of threatening situations can be achieved by combining generic observables. Situations can be assessed with a precision of 90% at a false positive and negative rate of 15% using only eight learning examples. In a real-world experiment at a large train station, we have classified various types of crowd dynamics. Using simple video features of shape and motion, we have proposed a scheme to translate such features into observables that can be classified by a conditional random field (CRF). The implemented CRF shows to classify successfully the crowd dynamics up to 80 % accuracy.	clutter;conditional random field;experiment;film-type patterned retarder;noise (electronics);observable;state (computer science);test case;test set;threat (computer)	Gertjan J. Burghouts;Jan-Willem Marck	2011	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2011.2135344	computer vision;simulation;computer science;artificial intelligence;architecture;machine learning;data mining;probabilistic logic;terrorism;conditional random field;hidden markov model;statistics;robustness	Vision	39.43661614859108	-44.84241784469384	161190
441d2e918d3471da524619192f734ba8273e3aa2	seenav: seamless and energy-efficient indoor navigation using augmented reality		Augmented Reality (AR) based navigation has emerged as an impressive, yet seamless way of guiding users in unknown environments. Its quality of experience depends on many factors, including the accuracy of camera pose estimation, response delay, and energy consumption. In this paper, we present SeeNav - a seamless and energy-efficient AR navigation system for indoor environments. SeeNav combines image-based localization and inertial tracking to provide an accurate and robust camera pose estimation. As vision processing is much more compute intensive than the processing of inertial sensor data, SeeNav offloads the former one from resource-constrained mobile devices to a cloud to improve tracking performance and reduce power consumption. More than that, SeeNav implements a context-aware task scheduling algorithm that further minimizes energy consumption while maintaining the accuracy of camera pose estimation. Our experimental results, including a user study, show that SeeNav provides seamless navigation experience and reduces the overall energy consumption by 21.56% with context-aware task scheduling.	3d pose estimation;algorithm;augmented reality;cloud computing;handy board;mobile device;scheduling (computing);seamless3d;usability testing	Marius Noreikis;Yu Xiao;Antti Ylä-Jääski	2017		10.1145/3126686.3126733	computer vision;artificial intelligence;navigation system;energy consumption;cloud computing;efficient energy use;augmented reality;pose;quality of experience;computer science;mobile device	Robotics	44.49278816834485	-39.086471819614424	161585
8d22ed6856e079d710921522f2ca5863846ff909	real-time background modeling based on classified dynamic objects for human robot application	histograms;human robot application;object displacement;robotic application background management computer vision classification;image segmention;video streaming;image segmentation;chaotic unstructured environments;background management;high level information;human robot interaction;real time background modeling;classification;computer vision;video streaming human robot interaction image segmentation image sequences object detection robot vision;unpredictable object dynamic problems real time background modeling classified dynamic objects human robot application robust background management algorithm image segmention video stream sequence indoor environment intelligent room applications human detection applications chaotic unstructured environments human displacement object displacement low level information high level information;robot vision;streaming media;image color analysis;heuristic algorithms;human detection applications;classified dynamic objects;low level information;indoor environment;video stream sequence;classification algorithms;intelligent room applications;robust background management algorithm;humans;human displacement;unpredictable object dynamic problems;graphics processing unit;robotic application;histograms heuristic algorithms classification algorithms graphics processing unit image color analysis humans streaming media;object detection;image sequences	The aim of this paper is to describe a flexible and robust background management algorithm. In these years various techniques were proposed to segment the images from a video stream sequence, and detect interesting dynamic objects. Many works faced the problem to segment the image in indoor environment for human detection and intelligent room applications. In these works, both accuracy and efficiency depend on the background model they used. Specific high performances models suffer of some limitations in chaotic unstructured environments. Long video stream sequences changes in light condition, and object and human displacement. In those environments, dynamic to stable objects and humans can be absorbed in the background, and then become invisible to the system. In this work we propose an approach to combine low level and high level information to improve the background management and to solve unpredictable object dynamic problems. Experimental recall and precision results show improved performances with respect to popular background management algorithms. Finally, a real application is shown and discussed.	algorithm;displacement mapping;high-level programming language;performance;precision and recall;real-time locating system;streaming media	Alessandro Moro;Enzo Mumolo;Massimiliano Nolich;Kenji Terabayashi;Kazunori Umeda	2012	2012 Ninth International Conference on Networked Sensing (INSS)	10.1109/INSS.2012.6240548	computer vision;simulation;background subtraction;computer science;multimedia	Robotics	43.24903011979475	-43.800271546680555	162518
fff12919cf912347776b70aa76af7635280dc401	are object detection assessment criteria ready for maritime computer vision		Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. Maritime environment offers its own unique requirements and challenges. Assessment of quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in maritime setting. Thus, a large body of related work in computer vision appears inapplicable to maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.	computer vision;object detection;requirement;sensor	Dilip K. Prasad;Deepu Rajan;Hiok Chai Quek	2018	CoRR		sight;object detection;computer science;computer vision;artificial intelligence	Vision	43.551570038060476	-40.45771410464045	162816
7dce19e86d469813f854ab5574e8d911d55b9e77	enhanced real-time head pose estimation system for mobile device	adaboost;head pose estimation;mobile devices	This article proposes a real-time Head Pose Estimation (HPE) technique designed to be used in mobile devices. The method enables the interaction between the user and mobile devices using the device’s inbuilt camera. The proposed technique is composed of different computer vision methods, which were optimized to operate in a restricted environment. The method has three Degrees of Freedom (DOF), roll, yaw and pitch. The HPE is obtained using these movements. Experiments were conducted using 363 videos of 27 different people taken in varied scenarios with changes in illumination and background. The results demonstrate the robustness and efficiency of the proposed method.	3d pose estimation;algorithm;computer performance;computer vision;control unit;experiment;face detection;graphics processing unit;image resolution;kanade–lucas–tomasi feature tracker;mobile device;mobile interaction;motion capture;overhead (computing);real-time clock;yaws	Euclides N. Arcoverde Neto;Rafael M. Duarte;Rafael M. Barreto;João Paulo Magalhães;Carlos A. C. M. Bastos;Ing Ren Tsang;George D. C. Cavalcanti	2014	Integrated Computer-Aided Engineering	10.3233/ICA-140462	robustness (computer science);adaboost;mobile device;computer vision;pose;engineering;artificial intelligence	Robotics	45.42079859138743	-43.5746044834891	164109
4c7ef260a2c788809fce81e65451d4c0c7b11c28	event detection in a smart home environment using viterbi filtering and graph cuts in a 3d voxel occupancy grid		In this paper we present a system for detecting unusual events in smart home environments. A primary application of this is to prolong independent living for elderly people at their homes. We show how to effectively combine information from multiple heterogeneous sensors which are typically present in a smart home scenario. Data fusion is done in a 3D voxel occupancy grid. Graph Cuts are used to accurately reconstruct people in the scene. Additionally we present a joint multi object Viterbi tracking framework, which allows tracking of all people, and simultaneously detecting critical events such as fallen persons.	cut (graph theory);home automation;image sensor;modal logic;prometheus;sensor;viterbi algorithm;voxel	Martin Hofmann;Moritz Kaiser;Nicolas H. Lehment;Gerhard Rigoll	2011			embedded system;computer vision;computer science;pattern recognition	HCI	40.53526582901309	-43.6680160790629	164253
687894a7e4c41e8751f7e0adaf7ce22c958136d6	segmentation with invisible keying signal	electrical capacitance tomography image segmentation cameras layout computer science read only memory contamination lighting hip physics;electrical capacitance tomography;computer vision image segmentation object detection;color cues;image segmentation;chroma keying;single chip sensor;hip;invisible signal;layout;virtual studios;computer vision;chip;physics;objects segmentation;automatic keying;single chip sensor invisible keying signal chroma keying objects segmentation color cues virtual studios automatic keying invisible signal catadioptric camera;lighting;contamination;computer science;read only memory;invisible keying signal;cameras;object detection;catadioptric camera	Croma keyingis the process of segmenting objects from images and video using color cues. A blue (or green) screen placed behind an object during recording is used in special effects and in virtual studios. The blue color is later replaced by a different background. Blue screen is an example of chroma keyingwhere the keying signal is chroma difference. A new method for automatic keying using invisible signal is presented. The advantages of the new approach over conventional chroma keying include: (i) Unlimited color range for foreground objects. (ii) No foreground contamination by background color. (iii) Better performance in non uniform illumination. (iv) Features for generating refraction and reflection of dynamic objects. The method can be used in real-time and no user assistance is required. New design of Catadioptric camera and a single chip sensor for keying is also presented.	chroma subsampling;color;key (cryptography);lisp;real-time computing;real-time locating system;virtual studio;web colors	Moshe Ben-Ezra	2000		10.1109/CVPR.2000.855795	chip;layout;computer vision;computer science;lighting;contamination;image segmentation;read-only memory;computer graphics (images)	Graphics	46.35844002682005	-43.60119668690569	166026
9dc6c6cfaf164e4568c1527e423f0cba3a81c5e3	xtrack: a flexible real-time 3d scanner for home computing applications	home computing;software;optical scanners;computer vision;streaming media;image representation;feature extraction;stereo image processing;notebook xtrack flexible real time 3d scanner home computing 3d object representation computer vision medical imaging system entertainment system filmmaking 3d movies gaming microsoft kinect music generic object photogrammetry stereovision holography time of flight camera interferometry structured light surface movement 3d information extraction algorithm projector webcam;stereo image processing computer vision feature extraction home computing image representation optical scanners solid modelling;hardware equations real time systems software streaming media;solid modelling;hardware;real time systems	Obtaining realistic three-dimensional (3D) representations of objects is one of the key research topics in Computer Vision. The applications of such researches, in fact, are multifaceted and range from serious scenarios, such as medical imaging systems, to leisure ones, including entertainment systems which support filmmaking (e.g., 3D movies such as Avatar), gaming (e.g., Microsoft Kinect) and music (e.g., House of Cards video by Radiohead), citing here only a few. Several techniques have been employed in the past three decades to acquire 3D information from generic objects: photogrammetry, stereovision, holography, time-of-flight cameras, interferometry and structured light, among others. Many of such techniques are capable of providing 3D information with high fidelity, and are also apt to track any surface movements with great precision and great performances, but often require the use of specialized and proprietary hardware and software. We here present a system that prevents from falling into such a nuisance, as it is based on existing 3D information extraction algorithms and solely requires off-the-shelf devices: a projector, a webcam and a notebook. In addition, the proposed system avoids the fall of any complexity on users' shoulders, as all settings and tuning procedures are implemented in software.	3d computer graphics;3d scanner;algorithm;computer vision;holography;information extraction;kinect;medical imaging;performance;photogrammetry;proprietary hardware;real-time clock;real-time transcription;stereopsis;structured light;video projector;webcam	Matteo Cocon;Gustavo Marfia;Marco Roccetti	2012	2012 21st International Conference on Computer Communications and Networks (ICCCN)	10.1109/ICCCN.2012.6289209	computer vision;simulation;feature extraction;computer science;operating system;computer graphics (images)	Visualization	42.99835465890582	-38.589083975648435	166029
4d2c36c0f88b18a06ccb4edccbfb383945a13d1e	human tracking by ip ptz camera control in the context of video surveillance	video surveillance;camera control;ip ptz camera;human tracking;feature based;low frame rate tracking;target localization;people tracking;fuzzy tracking;target detection;motion detection;fuzzy classifier	In this paper, a fuzzy feature-based method for online people tracking using an IP PTZ camera is proposed. It involves five steps: 1) target modeling, 2) track initialization, 3) blob extraction, 4) target localization using a fuzzy classifier, and 5) IP PTZ camera control. It selects the most similar target among candidate blobs found in the image using skin and motion detection. Results show that the proposed method has a good target detection precision (> 89%), low track fragmentation, and the target is almost always localized within 1/6th of the image diagonal from the image center. In addition, results suggest that our tracking method can cope with occlusion and large motion of the target.	closed-circuit television;pan–tilt–zoom camera	Parisa Darvish Zadeh Varcheie;Guillaume-Alexandre Bilodeau	2009		10.1007/978-3-642-02611-9_65	computer vision;simulation;tracking system;video tracking;computer graphics (images)	Vision	45.65458416132518	-44.55847128447426	166365
9ebf97c53a2a97de5315985e3c9422564bb47697	video sequence matching via decision tree path following	video sequence matching;subgraph isomorphism detection;decision tree path;video sequence;decision tree;vision	This paper presents an algorithm for resolution of a sequence of incrementally changing iconic queries, against a known database of model graphs. The algorithm is based on a representation using graphs and subgraph isomorphism detection.	decision tree;tree (data structure)	Kim Shearer;Svetha Venkatesh;Horst Bunke	2001	Pattern Recognition Letters		vision;combinatorics;computer science;machine learning;decision tree;pattern recognition;subgraph isomorphism problem;mathematics;induced subgraph isomorphism problem;maximum common subgraph isomorphism problem	Vision	39.89729710110037	-40.37837134779972	166427
56c13a4a9c435d3f1737c68b2d14db08820313f6	real-time simulation of visual defects with gaze-contingent display	signs and symptoms;image processing;macular degeneration;real time;real time simulation;system performance;gaze contingent display;early diagnosis;low vision;public awareness;variable resolution image;eye disease;foveated imaging;head eye tracking system;eye tracking;virtual environment;visual field;gaze direction;system simulation;visual fields	Effective management and treatment of glaucoma and other visual diseases depend on early diagnosis. However, early symptoms of glaucoma often go unnoticed until a significant portion of the visual field is lost. The ability to simulate the visual consequences of the disease offers potential benefits for patients and clinical education as well as for public awareness of its signs and symptoms. Experiments using simulated visual field defects could identify changes in behaviour, for example during driving, that one uses to compensate at the early stages of the disease's development. Furthermore, by understanding how visual field defects affect performance of visual tasks, we can help develop new strategies to cope with other devastating diseases such as macular degeneration. A Gaze-Contingent Display (GCD) system was developed to simulate an arbitrary visual field in a virtual environment. The system can estimate real-time gaze direction and eye position in earth-fixed coordinates during relatively large head movement, and thus it can be used in immersive projection based VE systems like the CAVE#8482;. Arbitrary visual fields are simulated via OpenGL and Shading Language capabilities and techniques that are supported by the GPU, thus enabling fast performance in real time. In order to simulate realistic visual defects, the system performs multiple image processing operations including change in acuity, brightness, color, glare and image distortion. The final component of the system simulates different virtual scenes that the participant can navigate through and explore. As a result, this system creates an experimental environment to study the effects of low vision on everyday tasks such as driving and navigation.	contingency (philosophy);distortion;graphics processing unit;image processing;opengl;real-time locating system;real-time transcription;shading language;simulation;virtual reality	Margarita Vinnikov;Robert S. Allison;Dominik Swierad	2008		10.1145/1344471.1344504	computer vision;simulation;computer science;human visual system model;computer graphics (images)	HCI	40.42585232762743	-41.81679180220272	166834
49730ba4234cb402ca3dea9522fdd350ab2a5f78	neural-based quality measurement of fingerprint images in contactless biometric systems	vision system;image capture;fingertip ridge pattern;nist;neural nets;distance 0 2 m;ccd based system;frame extraction;real time;feature extraction biometrics entropy nist estimation cameras pixel;biometrics;neural based quality measurement;fingertip valley pattern;image classification;neural classification;ccd image sensors;computer vision;estimation;image acquisition;computational complexity;feature extraction;pixel;image sequence;user fingerprint images;distance 0 2 m neural based quality measurement contactless biometric systems user fingerprint images vision system ccd based system fingertip ridge pattern fingertip valley pattern illumination problem image capture image acquisition frame extraction biometric information image sequence neural classification computational complexity reduction;illumination problem;entropy;contactless biometric systems;quality measures;point of view;biometric information;computational complexity reduction;cameras;fingerprint identification;neural nets ccd image sensors computer vision feature extraction fingerprint identification image classification image sequences;image sequences	Traditional fingerprint biometric systems capture the user fingerprint images by a contact-based sensor. Differently, contactless systems aim to capture the fingerprint images by an approach based on a vision system without the need of any contact of the user with the sensor. The user finger is placed in front of a special CCD-based system that captures the pattern of ridges and valleys of the fingertips. This approach is less constrained by the point of view of the user, but it requires much more capability of the system to deal with the focus of the moving target, the illumination problems and the complexity of the background in the captured image. During the acquisition procedure, the quality of each frame must be carefully evaluated in order to extract only the correct frames with valuable biometric information from the sequence. In this paper, we present a neural-based approach for the quality estimation of the contactless fingertips images. The application of the neural classification models allowed for a relevant reduction of the computational complexity permitting the application in real-time. Experimental results show that the proposed method has an adequate accuracy, and it can capture fingerprints at a distance up to 0.2 meters.	biometrics;charge-coupled device;computational complexity theory;contactless smart card;fingerprint;fingerprint recognition;mathematical optimization;point of view (computer hardware company);real-time clock;sensor;statistical classification	Ruggero Donida Labati;Vincenzo Piuri;Fabio Scotti	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596694	fingerprint;computer vision;entropy;estimation;contextual image classification;speech recognition;nist;machine vision;feature extraction;computer science;machine learning;computational complexity theory;artificial neural network;pixel;biometrics;computer graphics (images)	Robotics	44.48421682455453	-43.724486398555754	166860
5cd99b570be2297868b8013b23993eaade0c8962	vehicle ground-truth database for the vertical-view ft. hood imagery	vehicle detection performance vehicle ground truth database vertical view ft hood imagery manual image annotation;neural networks;image databases;vehicle detection;bayesian methods;land vehicles image databases pixel vehicle detection road vehicles spatial databases internet training data bayesian methods neural networks;vertical view ft hood imagery;training data;internet;pixel;manual image annotation;spatial databases;land vehicles;ground truth;vehicles;vehicle ground truth database;vehicle detection performance;visual databases vehicles object detection;object detection;road vehicles;visual databases	This paper reports the work on building the ground-truth databases for the vehicles in the vertical-view Ft. Hood (VVFH) image data set. We briefly describe the protocols followed in manual annotation of the images, how the ground-truth information is inferred from the annotated image data, and the major entities in the ground-truth database. The vehicle detection performance of a few algorithms is evaluated using the data set. The entire data set of images and ground-truth is available on the Internet.	aerial photography;algorithm;database;entity;ground truth;hood method;internet;performance evaluation;receiver operating characteristic;single version of the truth	Gang Liu;Robert M. Haralick	2000		10.1109/ICPR.2000.905348	computer vision;training set;the internet;ground truth;bayesian probability;computer science;machine learning;data mining;artificial neural network;pixel	Vision	40.67633513809588	-41.85586989174858	167080
5fb65867fe69aa9ce315160a1942563d588aaa6c	application of yolo deep learning model for real time abandoned baggage detection		We proposed an abandoned-baggage detection system that the baggage was left in public places for security reasons, i.e., subway stations. The proposed system applied the YOLO deep learning model for object detection, and presented a GUI for supporting a parameter setting. With this GUI, the detection system will be invariant to lighting and camera position.		Tossaporn Santad;Piyarat Silapasupphakornwong;Worawat Choensawat;Kingkarn Sookhanaphibarn	2018	2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2018.8574819	computer vision;deep learning;object detection;invariant (mathematics);computer science;artificial intelligence	Robotics	41.800685228039576	-43.94860867046891	167978
6c3941fb74416e75e1612e3f1887d1d7e9c430e2	handheld person verification system using face image		This paper presents our newly developed wireless system for person verification  based  on  the  face  verification  technology.  In  our  system,  a handheld device cooperating with a client-server wireless network is used. Due to the limitation of the handheld device's computing power, most computations will be distributed to a remote control server. There are two main problems for implementing  the  system.  One  is  the  variations  in  lighting  and  background conditions.  The  other  is  that  the  camera  on  the  handheld  device  can  not  be calibrated  with  the  system  in  advance.    For  calibration  on  line,  we  use  a three-point localization scheme for extracting appropriate face region according to the information of eyes and mouth. Furthermore, statistic based illumination normalization is used in preprocessing to decrease illumination influence under variant  lighting  conditions.  Experiment  shows  the  proposed  system  provides users a more  flexible and feasible way  to  interact with  the verification system through handheld device.	client–server model;computation;mobile device;preprocessor;remote control;server (computing)	Yao-Hong Tsai;Raymond Fu;Li-Wu Huang;Ching-Chun Huang;Cheng-Yi Liu	2003			artificial intelligence;computer vision;computer science;mobile device	Mobile	45.755408380970366	-43.29685373378231	168449
efbe30ecbacf77e287c670d8cec1fe9c7dfd534f	where are we after five editions?: robot vision challenge, a competition that evaluates solutions for the visual place classification problem	robot sensing systems;visual place classification problem;object recognition;robot vision image classification;competition;info eu repo semantics article;visualization;visualization robot sensing systems benchmark testing object recognition;benchmark testing;visual place classification problem robot vision;robot vision challenge	This article describes the Robot Vision challenge, a competition that evaluates solutions for the visual place classification problem. Since its origin, this challenge has been proposed as a common benchmark where worldwide proposals are measured using a common overall score. Each new edition of the competition introduced novelties, both for the type of input data and subobjectives of the challenge. All the techniques used by the participants have been gathered up and published to make it accessible for future developments. The legacy of the Robot Vision challenge includes data sets, benchmarking techniques, and a wide experience in the place classification research that is reflected in this article.	baseline (configuration management);benchmark (computing);computer vision;machine vision;outline of object recognition;robotics;testbed	Jesus Martínez-Gómez;Barbara Caputo;Miguel Cazorla;Henrik I. Christensen;Marco Fornoni;Ismael García-Varea;Andrzej Pronobis	2015	IEEE Robotics & Automation Magazine	10.1109/MRA.2015.2460931	benchmark;computer vision;simulation;competition;visualization;computer science;engineering;artificial intelligence;cognitive neuroscience of visual object recognition;mobile robot navigation	Robotics	45.70280179159184	-38.68606929585422	169313
53ee5798cc8224db919a957594ebd72fdac60e2f	head detection using kinect camera and its application to fall detection	video signal processing;head cameras detection algorithms humans feature extraction approximation algorithms radiation detectors;video signal processing object detection;body centroid head detection algorithm kinect camera depth video head positions head contour human outer contour hog human recognition histogram of oriented gradient human articulation back bending fall detection algorithm;object detection	This article proposes a head detection algorithm for depth video provided by a Kinect camera and its application to fall detection. The proposed algorithm first detects possible head positions and then based on these positions, recognizes people by detecting the head and the shoulders. Searching for head positions is rapid because we only look for the head contour on the human outer contour. The human recognition is a modification of HOG (Histogram of Oriented Gradient) for the head and the shoulders. Compared with the original HOG, our algorithm is more robust to human articulation and back bending. The fall detection algorithm is based on the speed of the head and the body centroid and their distance to the ground. By using both the body centroid and the head, our algorithm is less affected by the centroid fluctuation. Besides, we also present a simple but effective method to verify the distance from the ground to the head and the centroid.	algorithm;biconnected component;effective method;gradient;kinect;quantum fluctuation;sensor	Anh-Tuan Nghiem;Edouard Auvinet;Jean Meunier	2012	2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)	10.1109/ISSPA.2012.6310538	computer vision;speech recognition;computer science;computer graphics (images)	Robotics	45.6073401909718	-44.757481575236355	169571
29a10f176b3a91c04e86a99bddb46090612838f8	efficient object tracking in waas data streams	remote sensing image;systeme temps reel;moving object;teledetection;8920d;aplicacion militar;application militaire;image processing;0705p;sensors;surveillance;0130c;data stream;real time;trevor r h;imagerie;traitement image;computer vision;algorithme;detection and tracking algorithms;detection objet;imagery;research evaluation;remote sensing;object tracking;poursuite cible;moving object detection;military application;algorithms;defense and security;imagineria;near real time;target tracking;computer science efficient object tracking in waas data streams rochester institute of technology roxanne canosa clarke;4230v;waas;hyperspectral;imaging systems;object detection;real time systems	Wide area airborne surveillance (WAAS) systems are a new class of remote sensing imagers which have many military and civilian applications. These systems are characterized by long loiter times (extended imaging time over fixed target areas) and large footprint target areas. These characteristics complicate moving object detection and tracking due to the large image size and high number of moving objects. This thesis evaluates existing object detection and tracking algorithms with WAAS data and provides enhancements to the processing chain which decrease processing time and increase tracking accuracy. Decreases in processing time are needed to perform real-time or near real-time tracking either on the WAAS sensor platform or in ground station processing centers. Increased tracking accuracy benefits real-time users and forensic (off-line) users. The original contribution of this thesis increases tracking efficiency and accuracy by breaking a WAAS scene into hierarchical areas of interest (AOIs) and through the use of hyperspectral cueing	airborne ranger;algorithm;image resolution;object detection;online and offline;real-time clock;real-time computing;real-time transcription;wide area augmentation system	Trevor R. H. Clarke;Roxanne L. Canosa	2011		10.1117/12.872355	computer vision;simulation;image processing;sensor;hyperspectral imaging;video tracking;wide area augmentation system;remote sensing	Mobile	44.05475854157163	-42.71247840356695	169674
7b34277166c448c61ff4343a580b2eb7b7003c7b	vehicle detection based on multi-feature clues and dempster-shafer fusion theory		On-road vehicle detection and rear-end crash prevention are demanding subjects in both academia and automotive industry. The paper focuses on monocular vision-based vehicle detection under challenging lighting conditions, being still an open topic in the area of driver assistance systems. The paper proposes an effective vehicle detection method based on multiple features analysis and Dempster-Shafer-based fusion theory. We also utilize a new idea of Adaptive Global Haar-like (AGHaar) features as a promising method for feature classification and vehicle detection in both daylight and night conditions. Validation tests and experimental results show superior detection results for day, night, rainy, and challenging conditions compared to state-of-the-art solutions.	daylight;haar wavelet;statistical classification	Mahdi Rezaei;Mutsuhiro Terauchi	2013		10.1007/978-3-642-53842-1_6	dempster–shafer theory;fusion;advanced driver assistance systems;computer vision;collision detection;artificial intelligence;monocular vision;computer science;sensor fusion;automotive industry;crash	AI	41.66650466157953	-44.87691098804808	169718
69e7f4c14dd9e4261969ed1e0800625b98fd416d	3d indoor environment modeling and detection of moving object and scene understanding		Obtaining large-scale outdoor city environment is a mature technique, supporting many applications, such as search, navigation, etc. The indoor environment with same complexity often contains high density duplicate objects (e.g. table, chair, display, etc.). In this paper, special structure of indoor environment was applied to accelerate home video camera to conduct 3D collection and identification of indoor environment. There are two stages of this method: (i) Learning stage, gain three-dimensional model of objects which occurs frequently and variation pattern with only few scanning capture, (ii) identification stage, determine the objects which had been seen before but in different gestures and positions from the single scanning of a new field, which greatly accelerate identification process.		Bin Shao;Zhimin Yan	2018	T. Edutainment	10.1007/978-3-662-56689-3_4	image processing;computer vision;video camera;gesture;rough set;artificial intelligence;city environment;computer science	Robotics	40.42653355684553	-43.15138565530732	170828
6cff8c33b51fd228136d5516af6b6908b993173a	extended faster r-cnn for long distance human detection: finding pedestrians in uav images		Recently, using consumer Unmanned Aerial Vehicles(UAV) for aerial photography has became a trend. However, the images captured from the UAV raise a challenge to the existing pedestrian detection algorithms, because the humans in the image are too blur and too low-resolution resulted from the long distance between the UAV and pedestrians. The problem of detecting long distance humans in an image has always been over-looked, so even the performance of the state-of-the-art detection algorithms are not satisfactory when used on UAV pedestrian detection. In this paper, we extend Faster R-CNN algorithm by proposing an improved Region Proposal Network(RPN) and utilizing object context information to improve the detection performance. The experimental results show that the extended algorithm improves the performance of detecting pedestrians captured by UAV.	aerial photography;algorithm;gaussian blur;pedestrian detection;sensor;unmanned aerial vehicle	Hui Yuan Fu;Qing Wen;Deng Kui Zhang;Ling Fei Li	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326306	artificial intelligence;computer vision;object detection;feature extraction;computer science;image resolution;aerial photography;pedestrian detection	Robotics	41.27315112456275	-44.78006732227054	170962
9f6d6f17d0f083c9ed3767de8c20c61120fa2a45	a low-cost embedded platform for car's surrounding vision system	eye;sensors;distortion;embedded systems;lenses;video;vision;cameras	ABSTRACT The design and the implementation of a flexible and low-cost embedded system for cars surrounding vision is presented. The target of the proposed multi-camera vision system is to prov ide the driver a better view of the objects that surround the vehicle during maneuvering. Fish-eye lenses are used to achieve a larger field of view (FOV) but, on the other hand, introduce radial distortion of the images projected on the sensors. Using low-cost cameras there could be also some alignment issues. Since these complicati ons are noticeable and dangerous, a real- time algorithm for their correction and for the merging of 4 cameras video showed in a single view is presented. Keywords: Car surrounding vision, fish-eye camera, video auto motive assistance systems, r eal-time image processing, distortion correction, blind zones, embedded system. 1. INTRODUCTION The use of on-board cameras for vehicles 11-13 , in particular automotive applications is growing rapidly, probably because among all the car safety technologies that are spreading, those that stimulate the sight are considered the most immediate and reliable from the drivers. For this reason, a lot of manufacturers are trying to develop solutions always more advanced in this direction, offering technologies for the obstacles detection, brake assist and some other products to improve vision. However, proposed systems are expensive and equipped only on high-end vehicles. Among all the dangerous situations that can be encountered while driving, the most frequent is the one determined by the blind spots	embedded system	Sergio Saponara;Giacomo Fontanelli;Luca Fanucci;Emilio Franchi	2014		10.1117/12.2052878	vision;computer vision;simulation;video;distortion;telecommunications;sensor;lens;optics;physics;computer graphics (images)	Mobile	45.092133922168365	-41.219593590038286	171051
4ed1f9e599bccfa375c4ecf57135e3dc98222035	automated visual inspection system of automobile doors and windows using the adaptive feature extraction	inspection automobiles machine vision lighting feature extraction acoustic noise assembly systems costs optical reflection automotive engineering;automated visual inspection;automatic optical inspection;adaptive signal processing;machine vision;feature extraction;adaptive signal processing automobile industry automatic optical inspection feature extraction;assembly line automated visual inspection system automobile doors automobile windows adaptive feature extraction;automobile industry	We applied a machine vision technique to the assembly line of automobile. The volume of a car is very large. The overall machine vision environment is poor. The position and orientation of target is changed continuously. There are so many unexpected cases happening in inspection. Our system overcame the various poor conditions and achieved 99% of accuracy in the assembly line, using adaptive feature extraction method.	feature extraction;microsoft windows;visual inspection	Yun Koo Chung;Ki Hong Kim	1998		10.1109/KES.1998.725985	embedded system;computer vision;automated x-ray inspection;engineering;engineering drawing;automated optical inspection	ML	44.94946554434884	-42.76453480429708	171587
2e41593d5dbee75969dbe99ad0ebbe0759d18ad1	nobel chile japaleño sorting using structured laser and neural networks classifiers	vision ordenador;automatic system;sorting;industrie alimentaire;neural network classifier;tria;industria alimenticia;computer vision;food industry;sistema automatico;triage;systeme automatique;vision ordinateur;reseau neuronal;red neuronal;neural network	Jalapefio chili is grown extensively in Mexico, as it is one of the main vegetables consumed by the population, having also a high demand for exportation. Chili classification is fundamental before arriving to the processing plants, grocery stores and supermarkets. A CCD camera imaged the product which travelled through the conveyor belt, but it was very slow, so a laser scanning system was used to obtain the chili length in order to sort it by sizes. A brief study of the main chili features was carried out, before training a random backpropagation neural network classifier. It was noted that the best topology required to know only the chili width and length sorting up to five different sizes with accuracies over 94%.	artificial neural network;backpropagation;charge-coupled device;digital camera;sorting	Federico Hahn;Rafael Mota	1997		10.1007/3-540-63508-4_163	computer vision;food industry;simulation;computer science;sorting;artificial intelligence;artificial neural network	ML	40.74204336876337	-39.93573268740133	171874
a43122344da85598cf578e744b4159922a67906a	fall detection based on body part tracking using a depth camera	pedestrian safety;poison control;joints equations cameras trajectory head training three dimensional displays;injury prevention;training;safety literature;joints;traffic safety;injury control;home safety;injury research;safety abstracts;trajectory;human factors;three dimensional displays;occupational safety;safety;safety research;accident prevention;3d trajectory fall detection method body part tracking depth camera tracked key joints human body rgb input pose invariant randomization decision tree algorithm key joint extraction support vector machine classifier fall motion;violence prevention;head;bicycle safety;video surveillance decision trees feature extraction geriatrics medical image processing object tracking patient monitoring support vector machines;poisoning prevention;falls;ergonomics;suicide prevention;cameras	The elderly population is increasing rapidly all over the world. One major risk for elderly people is fall accidents, especially for those living alone. In this paper, we propose a robust fall detection approach by analyzing the tracked key joints of the human body using a single depth camera. Compared to the rivals that rely on the RGB inputs, the proposed scheme is independent of illumination of the lights and can work even in a dark room. In our scheme, a pose-invariant randomized decision tree algorithm is proposed for the key joint extraction, which requires low computational cost during the training and test. Then, the support vector machine classifier is employed to determine whether a fall motion occurs, whose input is the 3-D trajectory of the head joint. The experimental results demonstrate that the proposed fall detection method is more accurate and robust compared with the state-of-the-art methods.	a dark room;algorithmic efficiency;articular system;body dysmorphic disorders;computation;computational complexity theory;decision tree;extraction;kinesiology;light;list of algorithms;motion capture;population;randomized algorithm;remote digital terminal;support vector machine;telling untruths;trunk structure	Zhen-Peng Bian;Junhui Hou;Lap-Pui Chau;Nadia Magnenat-Thalmann	2015	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2014.2319372	computer vision;simulation;suicide prevention;human factors and ergonomics;trajectory;injury prevention;head;computer security	Vision	39.83268930461437	-44.18857836510853	173662
e8637fedd59a865c9591a2f9b67db8609017a9ee	models for gaze tracking systems	signal image and speech processing;articulo artikulua;biometrics;gaze tracking;info eu repo semantics article;pattern recognition;image processing and computer vision	One of the most confusing aspects that one meets when introducing oneself into gaze tracking technology is the wide variety, in terms of hardware equipment, of available systems that provide solutions to the same matter, that is, determining the point the subject is looking at. The calibration process permits generally adjusting nonintrusive trackers based on quite different hardware and image features to the subject. The negative aspect of this simple procedure is that it permits the system to work properly but at the expense of a lack of control over the intrinsic behavior of the tracker. The objective of the presented article is to overcome this obstacle to explore more deeply the elements of a video-oculographic system, that is, eye, camera, lighting, and so forth, from a purely mathematical and geometrical point of view. The main contribution is to find out the minimum number of hardware elements and image features that are needed to determine the point the subject is looking at. A model has been constructed based on pupil contour and multiple lighting, and successfully tested with real subjects. On the other hand, theoretical aspects of videooculographic systems have been thoroughly reviewed in order to build a theoretical basis for further studies.	eye tracking	Arantxa Villanueva;Rafael Cabeza	2007	EURASIP J. Image and Video Processing	10.1155/2007/23570	computer vision;simulation;computer science;archaeology;pattern recognition;biometrics	Vision	43.714140680722956	-39.71998929456936	174219
49eeb148582444149ecc28ca73120412fa9f92d6	early fire detection method in video for vessels	fuzzy clustering;dominant flame color lookup table;lookup table;dangerous flame detection;fuzzy c means clustering;dangerous smoke detection	New generation vessels are equipped with fire detecting sensors; however, fire may not immediately be detected if it is far away from the sensors. The fire process therefore cannot be recorded. A video-based fire alarm system is developed to overcome the drawbacks of traditional fire detection equipment. This paper presents a video-based flame and smoke detection method for vessels. For flame detection, the dominant flame color lookup table (DFCLT) is created by using the fuzzy c-means clustering algorithm. The changed video frames are automatically selected and the changed regions deduced from these frames. An elementary, medium, or emergency flame alarm is then triggered by comparing the pixels of changed regions with the DFCLT. The changed video frames are automatically selected for smoke detection. The changed regions are deduced from these frames. If the shape of the changed region conforms to the characteristic which the top area is wider than the bottom area, a dangerous smoke alarm is sounded. The experimental results show that the proposed fire detection approach can detect dangerous flames and smoke, effectively and efficiently.		Shuenn-Jyi Wang;Dah-Lih Jeng;Meng-Tsai Tsai	2009	Journal of Systems and Software	10.1016/j.jss.2008.09.025	simulation;lookup table;fuzzy clustering;telecommunications;computer science;engineering;forensic engineering	Embedded	43.43636032540066	-43.77563543850851	174999
239d2cc391e51bd6c65944bb3f3b48f1f3681877	carina dataset: an emerging-country urban scenario benchmark for road detection systems	sensors;roads;benchmark testing;cameras;radar;autonomous vehicles	Road traffic crashes are the leading cause of death among young people between 10 and 24 years old. In recent years, both academia and industry have been devoted towards the development of Driver Assistance Systems (DAS) and Autonomous Vehicles (AV) to decrease the number of road accidents. Detection of the road surface is a key capability for both path planning and object detection on Autonomous Vehicles. Current road datasets and benchmarks only depict European and North American scenarios, while emerging countries have higher projected consumer acceptance of AV and DAS technologies. This paper presents a selected Brazilian urban scenario dataset and road detection benchmark consisting of annotated RADAR, LIDAR and camera data. It also proposes a novel evaluation metric based on the intersection of polygons. The main goal of this manuscript is to provide challenging scenarios for road detection algorithm evaluation and the resulting dataset is publicly available at www.lrm.icmc.usp.br/dataset.	algorithm;autonomous car;autonomous robot;benchmark (computing);global positioning system;motion planning;object detection;radar	Patrick Yuri Shinzato;Tiago C. dos Santos;Luis Alberto Rosero;Daniela A. Ridel;Carlos M. Massera;Francisco A. R. Alencar;Marcos Paulo Batista;Alberto Yukinobu Hata;Fernando Santos Osório;Denis Fernando Wolf	2016	2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2016.7795529	simulation;geography;transport engineering;cartography	Robotics	42.2687775986877	-41.63879265427826	175874
9db27fe149af6e292893bf1278c00c3cd4f5aea4	optimized hog descriptor for on road cars detection	optimized hog;computing time;motion parameters;on road;car detection	The need for road security is increasingly highlighted, especially with the increase in road accidents around the world. One of the major causes is the vehicle speed and the drowsiness of drivers. Recently computer vision has become a well-developed research area especially in term of objects detection and tracking. Then, the exploitation of these developments for the road safety profile is strongly encouraged. Here we present our contribution to detect moving vehicles using optimized HOG process based on shape and motion parameters fusion. Indeed we prove in this paper that HOG descriptor combined with motion parameters is a very suitable car detector which reaches in record time a satisfactory recognition rate in dynamic outside area and bypasses several popular works without using sophisticated and expensive architectures such as GPU and FPGA. Also the performances of enhancement on road car detector are evaluated by the comparison of estimated cars positions with a ground truth manually extracted ones from a challenging database integrating a cars numbers and directions variations. The experimental study proves that estimated values are very close to the ground truth ones.	computer vision;device driver;experiment;field-programmable gate array;graphics processing unit;ground truth;performance	Nabila Mansouri;Yousra Ben Jemaa;Eric Watelain	2016		10.1145/2967413.2967415	computer vision;simulation;engineering;cartography	Vision	41.55774159269738	-42.85609116924068	176660
5cc7548d0f8c38cdc6f38541f1891e35320c1418	automated updating of camera parameters for user-friendly driver monitoring system	component technology;advanced safety system;safety systems;accuracy;monitoring system;monitoring;pixel;safety systems cameras monitoring road vehicles;cameras computerized monitoring condition monitoring calibration vehicle safety face detection eyes vehicle driving coordinate measuring machines proposals;driver monitoring;driver circuits;transmission line matrix methods;camera calibration;calibration;cameras;camera calibration driver monitoring advanced safety system;road vehicles;automated updating driver monitoring system component technology automated camera calibration camera parameters in vehicle driver monitoring camera system planar symmetry	This paper points to a component technology of driver monitoring system which has not been explored yet, namely, automated camera calibration. It proposes that the calibration should not be left to be done manually, as is the case now, but should be automated. In particular, this paper describes an algorithm to automatically update the camera parameters of the in-vehicle driver monitoring camera system using a planar symmetry, and discusses the accuracy and effectiveness of automated updating.	algorithm;camera resectioning;usability	Tatsunori Hirata;Mikio Danno;Masahiro Miyaji;Haruki Kawanaka;Md. Shoaib Bhuiyan;Koji Oguri	2009	2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES)	10.1109/ICVES.2009.5400234	smart camera;embedded system;computer vision;camera auto-calibration;simulation;engineering	Robotics	44.11610623491532	-42.142207407727895	176696
62905ab18e6eb1cd68730af6be06b864bfd8eff2	1 ms auto pan-tilt - video shooting technology for objects in motion based on saccade mirror with background subtraction	saccade mirror;visual servo;background subtraction;high speed;tracking	In this paper, the authors propose a state-of-the-art video shooting system for some randomly and quickly moving objects, which is based on our precise centering technology ‘1 ms Auto Pan-Tilt.’ It can capture the certain moving target as if it stopped and were fixed at the center of the image. In a previous study, we proposed and developed a handmade special device to control a camera gaze promptly, yet, it was not applicable for a real situation. This time, we remodeled the device as a unit with plural camera ports, which realized obtaining high-resolution movies. Moreover, with quantifying the system mathematically, we proposed a robust algorithm for a real use. Videos obtained by the new system help us grasp the detail movement of the target object more clearly, which is highly demanded in the realms of the measurement, astronomy, broadcasting service, and so on.	background subtraction	Kohei Okumura;Keiko Yokoyama;Hiromasa Oku;Masatoshi Ishikawa	2015	Advanced Robotics	10.1080/01691864.2015.1011299	computer vision;simulation;background subtraction;computer science;tracking;computer graphics (images)	Robotics	45.57603693034782	-42.31392598653506	177941
66d97a3d87bf0b64a4619ef13941b8cc8acc0838	noise resistant morphological algorithm of moving forklift truck detection on noisy image data	local descriptors;устойчивость алгоритмов к шумам;обработка изображений;keypoints detection;видеоанализ;noisy environment;binary morphology;motion history image;video based object detection	In this paper the authors focus on the specific problem of machine vision, namely, the video-based detection of the moving forklift truck. It is shown that the detection quality of the state-of-the-art local descriptors SURF, SIFT, etc. is not satisfactory if the resolution is low and the illumination is changed dramatically. The authors propose a novel algorithm to detect the presence of a cargo on the forklift truck on the basis of the mathematical morphological operators. At first, the movement direction is estimated with the updating motion history image method and the front part of the moving object is obtained. Next, contours are detected and the morphological operations in front of the moving object are used to compute several geometric features of an empty forklift. In the experimental study, it has been shown that the proposed method has 40% lower false positive rate and 27% lower false negative rate in comparison with conventional matching of local descriptors. Moreover, this algorithm is 7-35 times faster.	algorithm	Vladimir O. Chernousov;Andrey V. Savchenko	2014	IJCSSA	10.4018/IJCSSA.2014070103	computer vision;simulation;engineering;pattern recognition	Vision	41.870538296195036	-44.08918421465801	178223
3a1ce6d3e9ea7ff6d6ab7f3e978820b3435f6f30	car speed estimation method for uav images	spatial image coordinate shift car speed estimation method uav images unmanned aerial vehicles acquisition systems image acquisition image couple comparison;velocity measurement autonomous aerial vehicles image processing remote sensing road vehicles;unmanned aerial vehicles uav car detection car speed estimation scale invariant features transform sift;vehicles estimation feature extraction roads spatial resolution object recognition	Unmanned Aerial Vehicles (UAVs) exhibit an interesting operational flexibility considering that they can be used when and where it is necessary. These cutting-edge acquisition systems are able to fly very close to the ground allowing the acquisition of images in which objects are described by a very high level of detail. In this work, a method to detect moving objects and estimate their speed is presented. The method starts with the registration of two successive images belonging to a sequence acquired by means of a UAV. Then, the method proceeds with the identification of moving objects thanks to an opportune comparison of the couple of images. In the last step, to each moving object a speed estimate is assigned by exploiting the corresponding shift of spatial image coordinates. Experimental results have been conducted on a real UAV sequence of images. They show that the proposed method allows providing interesting estimation performances.	aerial photography;high-level programming language;level of detail;performance;unmanned aerial vehicle	Thomas Moranduzzo;Farid Melgani	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6947604	computer vision;simulation;remote sensing	Robotics	43.504437511290696	-41.9249821324262	178691
2b7f9f8a0c38158d8cd34c6e87b8b81557f60c4f	an image-based fluid surface pattern model		This work aims at generating a model of the ocean surface and its motion from one or more video cameras. The idea is to model wave patterns from video as a first step towards a larger system of photogrammetric monitoring of marine conditions for use in offshore oil drilling platforms. The first part of the proposed approach consists in reducing the dimensionality of sensor data made up of the many pixels of each frame of the input video streams. This enables finding a concise number of most relevant parameters to model the temporal dataset, yielding an efficient data-driven model of the evolution of the observed surface. The second part proposes stochastic modeling to better capture the patterns embedded in the data. One can then draw samples from the final model, which are expected to simulate the behavior of previously observed flow, in order to determine conditions that match new observations. In this paper we focus on proposing and discussing the overall approach and on comparing two different techniques for dimensionality reduction in the first stage: principal component analysis and diffusion maps. Work is underway on the second stage of constructing better stochastic models of fluid surface motion as proposed here.	diffusion map;dimensionality reduction;embedded system;experiment;machine learning;observable;photogrammetry;pixel;principal component analysis;sampling (signal processing);simulation;stochastic modelling (insurance);stochastic process;streaming media	Mauro de Amorim;Ricardo Fabbri;Lucia Maria dos Santos Pinto;Francisco Duarte Moura Neto	2013	CoRR		computer vision;simulation;computer science;machine learning;statistics	Vision	40.213271635046695	-40.84088246553466	178842
77492c7f711e4a09e7c82e32b0a43c9ff607b956	implementation of lane detection algorithm for self-driving vehicles using tensor flow		Recently, systems for detecting and tracking moving objects from video are gaining research interest in the field of image processing, owing to their applications in fields such as security, observation, and military, and considerable research is being conducted to develop high-accuracy and high-speed processing systems. In particular, as interest in autonomous driving has increased rapidly, various algorithms for lane keeping assistance devices have been developed. This study proposes a lane detection algorithm by comparing color-based lane detection algorithms and using a lane detection algorithm based on representative line extraction. Edge extraction and Gaussian filters are applied for lane detection and a Median filter is applied for image noise reduction. The detection accuracy is improved by extracting the region of interest for the image based on four pointers. Finally, a Hough transform is applied to improve the accuracy of straight line detection, and an algorithm to extract representative lines is applied to increase the detection rate in shadow regions and dark areas. Experimental results show that the proposed algorithm can detect lanes with high accuracy. The application of this algorithm to vehicle black boxes or autonomous driving will help prevent lane departure and reduce accident rates.		Hyunhee Park	2018		10.1007/978-3-319-93554-6_42	computer science;image processing;image noise;line (geometry);tensor;median filter;region of interest;algorithm;hough transform;shadow	Vision	41.811722051936826	-43.8125987009726	179492
242353b40d61bc1e5486136317d44e0bef41647e	assistive device for the blind based on object recognition: an application to identify currency bills	object recognition;real time;currency reader;assistive device;low vision;blindness;image analysis	We have developed a real-time portable object recognition system based on a bio-inspired image analysis software to increase blind people autonomy by localizing and identifying surrounding objects. A working prototype of this system has been tested on the issue of currency bill recognition encountered by most of the blind people. Seven blind persons were involved in an experiment which demonstrated that the usability of the system was good enough for such a device to be used daily in real-life situations.	assistive technology;british informatics olympiad;image analysis;outline of object recognition;portable object (computing);principle of good enough;prototype;real life;real-time computing;real-time transcription;usability	Rémi Parlouar;Florian Dramas;Marc J.-M. Macé;Christophe Jouffrais	2009		10.1145/1639642.1639688	computer vision;image analysis;simulation;computer science;artificial intelligence;cognitive neuroscience of visual object recognition	HCI	46.14897219265479	-38.680806688432064	179534
faa6a0c6a417f135772b82dd8320445cf0bc82b0	a novel hierarchical framework for object-based visual attention	moving object;hierarchical structure;saliency map;focus of attention;group process;visual attention	This paper proposes an artificial visual attention model which builds a saliency map associated to the sensed scene using a novel perception-based grouping process. This grouping mechanism is performed by a hierarchical irregular structure, and it takes into account colour contrast, edge and depth information. The resulting saliency map is composed by different parts or `pre-attentive objects' which correspond to units of visual information that can be bound into a coherent and stable object. Besides, the ability to handle dynamic scenarios is included in the proposed model by introducing a tracking mechanism of moving objects, which is also performed using the same hierarchical structure. This allows to conduct the whole attention mechanism in the same structure, reducing the computational time. Experimental results show that the performance of the proposed model is compatible with the existing models of visual attention whereas the object-based nature of the proposed approach renders advantages of precise localization of the focus of attention and proper representation of the shapes of the attended `pre-attentive objects'.	object-based language	Rebeca Marfil;Antonio Bandera;Juan Antonio Rodríguez;Francisco Sandoval Hernández	2008		10.1007/978-3-642-00582-4_3	psychology;computer vision;artificial intelligence;communication	ML	42.788972640357606	-40.046641658998304	180486
c7fd77bb9b17df62fc83944be8492f150cb72ed8	optimization algorithm and implementation of pedestrian detection	histograms of oriented gradient;pedestrian detection;svm;tri-linear interpolation;window's merging	Pedestrian detection is widely used in automotive assisting driving system. The algorithm based on Histograms of Oriented Gradient (HOG for short) feature is the main one in the current pedestrian detection. This paper uses tri-linear interpolation method to extract the image HOG features, and gives the optimization algorithm based on look-up table to reduce the amount of calculation in extracting HOG feature. And then classifies them by RBF and linear SVM to explore its speed and accuracy. At the end of the paper, an effective method is given to merge windows that contain detected pedestrians. Experiments on INRIA and MIT databases show that the detecting accuracy and speed of this method is relatively high.	algorithm;pedestrian detection	Meihua Xu;Huaimeng Zheng;Tao Wang	2013		10.1007/978-3-642-38715-9_51	merge (version control);interpolation;support vector machine;computer science;algorithm;effective method;histogram;pedestrian detection	Logic	41.02995084193983	-43.43594618964353	182294
0c74ee2e1eb13d32e1870730a44a20859d4afed3	deformable tissue parameterized by properties of real biological tissue	biological tissues;elastic properties;biological tissue;biomechanics;soft tissue modeling;orthopaedics;medical computing;surgery;soft tissue;digital simulation	Realistic mechanical models of biological soft tissues are a key issue to allow the implementation of reliable systems to aid on orthopedic diagnosis and surgery planning. We are working to develop a computerized soft tissues model for bio-tissues based on a mass-spring-like approach. In this work we present several experiments towards the parameterization of our model from the elastic properties of real materials.	british informatics olympiad;experiment	Anderson Maciel;Ronan Boulic;Daniel Thalmann	2003		10.1007/3-540-45015-7_8	biomechanics;soft tissue	AI	39.632918247566174	-38.488260940115225	182406
cdf45bbc5cf6ced37c31e9a9557ea990fa77eadb	simple two-dimensional object tracking based on a graph algorithm		The visual observation and tracking of cells and other micrometer-sized objects has many different biomedical applications. The automation of those tasks based on computer methods helps in the evaluation of such measurements. In this work, we present a general purpose algorithm that excels at evaluating deterministic behavior of micrometer-sized objects. Our concrete application is the tracking of fast moving objects over large distances along deterministic trajectories in a microscopic video. Thereby, we are able to determine characteristic properties of the objects. For this purpose, we use a set of basic algorithms, including blob recognition, feature-based shape recognition and a graph algorithm, and combined them in a novel way. An evaluation of the algorithms performance shows a high accuracy in the recognition of objects as well as of complete trajectories. Moreover, a direct comparison to a similar algorithm shows superior recognition rates.	algorithm;list of algorithms	Alexandra Heidsieck	2014	CoRR		computer vision;computer science;theoretical computer science;machine learning	Vision	42.108976286651874	-39.65518474300453	183617
79167bdaa68b8f13acc616dd8740a925d15bee97	unattended object intelligent analyzer for consumer video surveillance	railways;national security;background modeling;video surveillance;image processing;intelligent analyzer;surveillance system;real time;public safety;airports;consumer video surveillance;public safety application unattended object intelligent analyzer image processing technology object detection shopping malls airports railways station intelligent vision based analyzer semantic analysis monocular surveillance video consumer camera cluttered environment potential security breach terrorist explosive attack background subtraction algorithm periodic background model shadow removal quick lighting change adaptation object classification capability dynamic object stationary object decision making real time consumer video camera surveillance system;human behavior;visualization;multiple background model consumer video surveillance intelligent analyzer unattended object;video cameras;visual cues;pixel;multiple background model;object extraction;background subtraction;pixel video surveillance adaptation models visualization real time systems object detection;explosives;object classification;adaptation models;video surveillance airports explosives national security object detection railways video cameras;unattended object;object detection;semantic analysis;real time systems	Consumer video camera surveillance with the continuous advancements of image processing technologies is emerging for consumer world of applications. Technology for detecting objects left unattended in consumer world such as shopping malls, airports, railways stations has resulted in successful commercialization, worldwide sales and the winning of international awards. However, as a consumer video application the need is now greater than ever for a surveillance system that is robustly and effectively automated. In this paper, we propose an intelligent vision based analyzer for semantic analysis of objects left unattended relation with human behaviors from a monocular surveillance video, captured by a consumer camera through cluttered environments. Our analyzer employs visual cues to robustly and efficiently detect unattended objects which are usually considered as potential security breach in public safety from terrorist explosive attacks. The proposed system consists of three processing steps: (i) object extraction, involving a new background subtraction algorithm based on combination of periodic background models with shadow removal and quick lighting change adaptation,(ii) extracted objects classification as stationary or dynamic objects, and (iii) classified objects investigation by using running average about the static foreground masks to calculate a confidence score for the decision making about event (either unattended or very still person). We show attractive experimental results, highlighting the system efficiency and classification capability by using our real-time consumer video surveillance system for public safety application in big cities.	algorithm;background subtraction;closed-circuit television;computation;image processing;multi-function printer;object detection;outline of object recognition;real-time clock;robustness (computer science);sensor;stationary process	Thi Thi Zin;Pyke Tin;Hiromitsu Hama;Takashi Toriu	2011	IEEE Transactions on Consumer Electronics	10.1109/TCE.2011.5955191	computer vision;simulation;explosive material;visualization;background subtraction;sensory cue;image processing;computer science;national security;human behavior;computer security;pixel	Vision	43.62038870631997	-43.71478216553322	184951
3e80e32dcc0ec391b9709ed917e2eb8ddb094ea5	pedestrian tracking using single camera with new extended kalman filter	surveillance;photography;extended kalman filter;cameras;tracking	Purpose – The purpose of this paper is to present research in the area of the signal processing and application into pedestrian tracking in the video scene.Design/methodology/approach – The paper describes the design of a new extended Kalman filter (EKF) in the high‐dimensional space (HDS) and studies of mean square error and variance analysis of error. A design algorithm is implemented in MATLAB software and tested. The data set includes many hours of captured films.Findings – This paper includes a new derivation of the EKF and its implementation into the video scene.Practical implications – The proposed algorithm can be used to track each video application.Originality/value – The Kalman filter in the HDS is presented for the first time. Also, the application of the proposed method is applied in pedestrian tracking and counting.	extended kalman filter	Hadi Sadoghi Yazdi;Seyyed Ebrahim Hosseini	2008	Int. J. Intelligent Computing and Cybernetics	10.1108/17563780810893464	computer vision;invariant extended kalman filter;simulation;fast kalman filter;computer science;photography;tracking;extended kalman filter;moving horizon estimation;computer graphics (images)	Robotics	44.25213671073572	-41.68784160884896	186455
a21f07bd85d04b544c09d126c38265ed5e7e4a7f	driver drowsiness warning system using visual information for both diurnal and nocturnal illumination conditions	signal image and speech processing;drowsiness;pedestrian safety;poison control;sensors;injury prevention;safety literature;real time information;traffic safety;injury control;nighttime crashes;home safety;injury research;safety abstracts;quantum information technology spintronics;human factors;eye movements;fatigue physiological condition;occupational safety;safety;daytime crashes;driver monitoring;safety research;accident prevention;violence prevention;face;bicycle safety;detection and identification systems;sleep disorders;poisoning prevention;falls;periods of the day;ergonomics;suicide prevention;driver support systems	Every year, traffic accidents due to human errors cause increasing amounts of deaths and injuries globally. To help reduce the amount of fatalities, in the paper presented here, a new module for Advanced Driver Assistance System (ADAS) which deals with automatic driver drowsiness detection based on visual information and Artificial Intelligence is presented. The aim of this system is to locate, track, and analyze both the drivers face and eyes to compute a drowsiness index, where this real-time system works under varying light conditions (diurnal and nocturnal driving). Examples of different images of drivers taken in a real vehicle are shown to validate the algorithms used.	algorithm;architecture design and assessment system;artificial intelligence;real-time clock;real-time computing	Marco Javier Flores;Jose M. Armingol;Arturo de la Escalera	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/438205	face;real-time data;simulation;suicide prevention;sensor;human factors and ergonomics;injury prevention;computer security;eye movement	AI	40.351200199214766	-44.7288949885254	188045
16193c4e5c3c41db07cbd5855941895db1cabbcb	behaviour recognition using the event calculus	event calculus;human behaviour;satisfiability;temporal constraints;short period;symbolic representation	We present a system for recognising human behaviour given a symbolic representation of surveillance videos. The input of our system is a set of timestamped short-term behaviours, that is, behaviours taking place in a short period of time — walking, running, standing still, etc — detected on video frames. The output of our system is a set of recognised long-term behaviours — fighting, meeting, leaving an object, collapsing, walking, etc — which are pre-defined temporal combinations of short-term behaviours. The definition of a long-term behaviour, including the temporal constraints on the short-term behaviours that, if satisfied, lead to the recognition of the long-term behaviour, is expressed in the Event Calculus. We present experimental results concerning videos with several humans and objects, temporally overlapping and repetitive behaviours.	event calculus;frame (video);trusted timestamping	Alexander Artikis;Georgios Paliouras	2009		10.1007/978-1-4419-0221-4_55	computer science;artificial intelligence;event calculus;human behavior;algorithm;satisfiability	Vision	45.818399454536525	-40.40046382919157	188168
43a054c6b3ae2ef818b31c4da10e8728f8d5f56f	an abandoned object detection system based on dual background segmentation	video surveillance approximation theory closed circuit television image resolution image segmentation object detection video cameras;tracking video surveillance left baggage detection background segmentation;video surveillance;cctv cameras;background segmentation;image segmentation;image resolution;occlusion;approximation algorithms;object detection cameras layout video surveillance mathematical model subtraction techniques data mining image converters image segmentation matrix converters;qvga resolution;closed circuit television camera;left baggage detection;data mining;manganese;approximation theory;closed circuit television;streaming media;video cameras;dual time background subtraction algorithm;pixel;approximate median model;background subtraction;occlusion abandoned object detection system dual background segmentation benchmark datasets mathematical model qvga resolution cctv cameras dual time background subtraction algorithm approximate median model closed circuit television camera video surveillance;mathematical model;benchmark datasets;dual background segmentation;abandoned object detection system;algorithm design and analysis;tracking;object detection	An abandoned object detection system is presented and evaluated using benchmark datasets. The detection is based on a simple mathematical model and works efficiently at QVGA resolution at which most CCTV cameras operate. The pre-processing involves a dual-time background subtraction algorithm which dynamically updates two sets of background, one after a very short interval (less than half a second) and the other after a relatively longer duration. The framework of the proposed algorithm is based on the Approximate Median model. An algorithm for tracking of abandoned objects even under occlusion is also proposed. Results show that the system is robust to variations in lighting conditions and the number of people in the scene. In addition, the system is simple and computationally less intensive as it avoids the use of expensive filters while achieving better detection results.		Anish Rajendra Singh;S. Sawan;Madasu Hanmandlu;Vamsi Krishna Madasu;Brian C. Lovell	2009	2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2009.74	computer vision;simulation;image resolution;background subtraction;computer science;manganese;mathematical model;tracking;image segmentation;approximation algorithm;pixel;approximation theory;computer graphics (images)	Vision	45.200199227633064	-44.83514934524596	188795
e3ef4e6f0453e970fee2131994c2ce291f4d90a0	radio tomographic imaging based body pose sensing for fall detection		Fall is a common daily activity and a leading cause of death among the older adults. It reveals growing demands to use some non-invasive methods to detect the pose of older people and give a timely and efficient alert, especially in some place with high fall-risk. This article presents a radio tomographic imaging (RTI) based approach for fall detection. A wireless network organized by a group of radio-frequency sensors is used for human pose sensing in the vertical direction. The human body would cause the statistical shadowing losses on the passing links between pairs of nodes in the network. Then an attenuation image of body pose can be obtained by using the received signal strength measurements. The non-negative total variation minimization is used to reconstruct the gray image of body. The fall detection is cast as an image recognition problem. This is a new approach based on the use of RTI to enable the building of a fall detection system. Experimental studies are conducted to validate the proposed method.	computer vision;control theory;holography;illumination (image);integrated circuit;left 4 dead 2;radio frequency;run-time infrastructure (simulation);scalability;sensor;sensor web;tomography;wet floor effect	Tong Liu;Jun Liu;Xiaomu Luo	2014	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-014-0243-x	computer vision;simulation	Robotics	42.180850457745926	-38.79032304095735	190563
d5045791a2b155798d4b19690fff72a18bd697b2	a particle filter-based approach for tracking undersea narrow telecommunication cables	autonomous underwater vehicle;tracking system;autonomous underwater vehicle auv;cable tracking;video tracking;multi dimensional;particle filter;visual features;growth form;ground truth;remotely operated vehicle;underwater vision;pose estimation	The surveillance and inspection of underwater installations such as telecommunication cables are currently carried out by trained operators who, from the surface, guide a remotely operated vehicle (ROV) with cameras mounted over it. This manual visual control is, however, a very tedious job that tends to fail if the operator loses concentration. This paper describes a tracking system for underwater narrow telecommunication cables, the main objective of which is to allow an autonomous underwater vehicle to video-document the whole length of a cable. The approach is based on particle filters (PFs) because of their natural ability to model multi-dimensional multi-modal PDFs, which allows handling in a more appropriate way the ambiguities that naturally arise from undersea environments. In effect, despite the special visual features that artificial objects present, which allow distinguishing them in natural scenarios such as the seabed, distracting background such as rocks or algae growing on top and nearby cables, complicate the detection and tracking and give rise to ambiguities when rocks or marine growth form shapes and textures that resemble the cable. Apart from the different models that a PF requires, the paper also describes a set of added features, which successfully compensate some large errors in the cable pose estimation when the non-enhanced tracker is applied. Extensive experimental results over a test set of more than 10,000 frames, for which a ground truth has been manually generated, have shown the usefulness of the solution proposed. Besides, results for a set of six video sequences accounting for almost 150,000 frames and around one hour and a half of successful continuous video tracking are also discussed. All those images come from inspection runs captured by ROVs navigating over real telecommunication undersea cables.	autonomous robot;ground truth;modal logic;particle filter;portable document format;remotely operated vehicle;test set;tracking system;video tracking	Alberto Ortiz;Javier Antich;Gabriel Oliver	2009	Machine Vision and Applications	10.1007/s00138-009-0199-6	computer vision;simulation;pose;particle filter;remotely operated vehicle;tracking system;telecommunications;ground truth;computer science;video tracking	Vision	43.13892328366948	-41.63859687914678	190716
3da7dcda5d912029c2c5f89193aae79a098b862e	detection of basic behaviors in logged data in robocup small size league	satisfiability;time series data	This paper describes a method that extracts the basic behaviors of robots such as kicking and passing from the history data of the positions and velocities of the robots and the ball in RoboCup Small Size League (SSL). In this paper, as a first step, we propose an offline method that extracts the basic behaviors of robots from the logged data which is a record of the positions and velocities of the robots and the ball as the time series data. First, paying attention to the ball movement, we extract the line segments in the ball trajectory which satisfy our proposed conditions. These segments arise from the kicking actions. Then we classify the extracted line segments into the detailed kicking actions by analysing the intention of the kicked robot. We also propose algorithms that detect and classify the covering actions. Experimental results show that 98% of the kicking actions are correctly detected and more than 80% of the detected kicking actions are correctly classified, and that 90% of the covering actions are also correctly classified.		Kenichi Asano;Kazuhito Murakami;Tadashi Naruse	2008		10.1007/978-3-642-02921-9_38	simulation;computer science;artificial intelligence;time series;satisfiability	AI	45.7920459788645	-40.29526229913994	192681
d92f64f24341bc7bf0b88e9b2dec548081e63e6e	a method of real-time image correction for multi-aircrafts cooperative detection		Aiming at the problem of the geometric distortion of the real-time image under the condition of multi- aircrafts cooperative detection, an effective method for the geometric correction is proposed. First multi-aircrafts cooperative terminal guidance model is established, especially the relationship between terminal detection angle and the initial position. Second the detecting angle model combined with the actual parameters, is analyzed, then put forward a real-time image geometric correction method; Simulation of the detection angle geometric distortion is applied to real-time image. Last using normalized product correlation algorithm for a large number of matching simulation, then get matching threshold, a real-time image geometric correction experiment is carried out. The simulation results show that the proposed method is effective,which provides theoretical basis for the reformation of reference image and the further improvement of the algorithm.	real-time transcription	Ge Fu;Xiao-gang Yang;Xiao-pei Tang;Ai-gang Zhao;Nai-xin Qi	2016		10.1007/978-3-319-40259-8_25	computer science;simulation;computer vision;normalization (statistics);terminal guidance;distortion;effective method;correlation;artificial intelligence	Robotics	42.966427964939164	-42.319539947150595	195881
6ea7c0a6cefbf57dfd1911d77e582415c5650aee	a fluoroscopy-based system for three-dimensional kinematic analysis of artificial knee implants and clinical application	three dimensional			Takaharu Yamazaki;Tetsu Watanabe;Yoshikazu Nakajima;Kazuomi Sugamoto;Tetsuya Tomita;Yoshinobu Sato;Hideki Yoshikawa;Shinichi Tamura	2004			three-dimensional space	Robotics	39.84169023174504	-38.85595263001348	195976
0bce9061e3e33efd818f91dd8a75fe5bec818a56	challenges in head pose estimation of drivers in naturalistic recordings using existing tools		Head pose is an important feature to understand the driver's behavior and their level of attention. While current head pose estimation (HPE) algorithms are suitable for many applications under controlled conditions, the performance drops in driving environments where images commonly have varying illumination, occlusions, and extreme head rotations. It is important to understand the limitations of current HPE algorithms to create computer vision solutions that target in-vehicle applications. This paper analyzes the HPE algorithms OpenFace, IntraFace and ZFace, with images recorded under natural driving environment where the goal is to find consistent conditions that affect the performance of current HPE systems. The key feature of the recordings is the use of a headband with AprilTags, which are used to estimate reference head pose angles. We study the effect of different factors including head pose angles, illumination in the frames and occlusions due to glasses. We identify the range of yaw and pitch rotations for which these HPE algorithms provide reliable estimations. While the HPE algorithms work reasonably well for normal/rimless glasses, occlusion due to thicker glasses is a major problem. We identified frames where all the HPE algorithms failed to provide an estimate. We are releasing the data to serve as a benchmark for future HPE algorithms in naturalistic driving conditions.	3d pose estimation;algorithm;benchmark (computing);computer vision;hidden surface determination;java caps;yaws	Sumit Jha;Carlos Busso	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317870	artificial intelligence;computer vision;pose;engineering	Robotics	45.187793464791866	-43.22060027511574	197019
2d23924b63f3527ab914682ca8ac3242dbf3bbbf	correction of selection bias in traffic data by bayesian network data fusion		Bayesian Data Fusion (BDF) is a well-established method in decision-level fusion to increase the quality of measured data of several equal or different sensors, e.g. [7], [13]. Although the method is powerful, the results of the fusion process are only (1) as good as the sensors are; (2) as good as the a priori knowledge about the sensors is and (3) as good as the a priori knowledge about the underlying process is. For instance, in case of vehicle classification for traffic surveillance by several more or less accurate sensors (item 1), accurate relative frequencies of correct and wrong classifications (phantom detections, incorrect classified vehicles) are required to achieve beneficial fusion results (item 2). This statement is supplemented by an adequate characterisation and quantification of the underlying unknown traffic process (item 3). For an adequate traffic management, there is a particular need for highly accurate traffic data, measured by accurate and reliable sensors, yielding a high degree of acceptance and credibility concerning the significance of the measured traffic parameters. There are a lot of different sensor technologies with different physical functional principles, different performance, problems and thus, differing operational areas [18], [19]. Two currently important coexisting sensor technologies are for instance the inductive loop detectors and video sensors. Loop detectors measure the traffic process temporally, while video sensors enable temporal and wide area measurements, yielding more comprehensive data about the underlying traffic process than loop detectors. Both sensors provide a data quality in accordance with their physical functional principle and in accordance with the influences of the affecting surrounding environment. For instance, an inductive loop detector works properly under fluid traffic conditions, whereas the measurements are not accurate, if there is stop-and-go traffic. Furthermore, vehicle detection and classification may be problematic in case of overtaking procedures, when the loops are overrun only partly, [11], [12]. That means an inductive loop detector is a sensor, which is influenced by the traffic process itself. In contrast to loop detectors, it is a well-known fact, that the most currently employed video sensors usually work poorly under bad weather conditions (e.g. heavy rain, fog, etc.), changing illuminations (e.g. reflections on the road surface) and traffic process dependent problems (e.g. occlusions among the vehicles on the road). Although new methods have recently been developed to overcome the addressed problems [11], the detection errors of currently used video sensors increase to more than 1000%, if the weather and illumination conditions are bad [6]. In contrast, they perform much better (they can reach even the same accuracy as an inductive loop detector), if the conditions for an optimal operation are maintained.	backward differentiation formula;bayesian network;data quality;imaging phantom;reflection (computer graphics);selection bias;sensor;temporal logic	Marek Junghans;Hans-Joachim Jentschel	2008	J. Adv. Inf. Fusion		machine learning;mathematics;artificial intelligence;selection bias;sensor fusion;bayesian network	Mobile	41.647093539316806	-41.11741002811093	197023
08ab813ebcfc5ce7dd11f1810257247f8f403ecd	robust human detection and tracking in intelligent environments by information fusion of color and infrared video	robust human detection;robust tracking;human computer interaction;thermal infrared video segmentation;image segmentation;support vector machines;neural nets;video signal processing;thermal infrared spectra robust human detection robust human tracking intelligent environments information fusion ambient intelligence systems human centered ambient intelligent system data acquisition robust tracking human behavior interpretation monitored environments thermal infrared video segmentation color video segmentation traditional tracking problems support vector machines artificial neural networks tracking level color infrared spectra;video signal processing data acquisition human computer interaction image colour analysis image segmentation infrared detectors neural nets object detection object tracking sensor fusion support vector machines;human behavior interpretation;color infrared spectra;tracking level;thermal infrared spectra;humans image color analysis cameras image segmentation robustness proposals optical imaging;artificial neural networks;optical imaging;human centered ambient intelligent system;intelligent environments;ambient intelligence systems;image color analysis;image colour analysis;color video segmentation;traditional tracking problems;object tracking;robustness;humans;information fusion;sensor fusion;robust human tracking;proposals;monitored environments;data acquisition;infrared detectors;cameras;object detection	This paper is related to ambient intelligence systems capable of locating and tracking humans. These are the first steps of a human-centered ambient intelligent system, ranging from data acquisition to robust tracking, for the purpose of interpreting human behaviors in monitored environments. The first objective is to improve human detection through the fusion of thermal-infrared and color video segmentation. On the level following to segmentation, the traditional tracking problems (e.g. occlusions, crossings, etc.) are faced. Finally, the use of several classifiers such as support-vector machines and artificial neural networks are proposed to enhance the tracking level. The work proposes a combination of both color and thermal infrared spectra in human tracking.	ambient intelligence;artificial intelligence;artificial neural network;data acquisition;intelligent environment;support vector machine	Juan Serrano-Cuerda;María T. López;Antonio Fernández-Caballero	2011	2011 Seventh International Conference on Intelligent Environments	10.1109/IE.2011.21	computer vision;simulation;tracking system;engineering;video tracking;computer graphics (images)	Robotics	43.15936626505731	-43.774075873513794	197152
da27dc5e3c3c8fd2e900261dcd57f32ca8ba74cf	an algorithm for machine detection and tracking of moving objects in television images	vision system;real time;cognitive modeling with heuristics;environmental conditions;detection and tracking of moving objects;computer vision;machine learning;automatic detection;learning strategy;human motion;heuristics;multiple object tracking;motion detection;object identification	An important area of application of computer vision is the detection of object motion. We are currently developing a computer vision system to automatically detect and track human motion across the international border between the United States and Mexico. Because of the wide range of environmental conditions, this application represents a stringent test of computer vision algorithms for motion detection and object identification. The desired output of this vision system is both accurate statistical data as to the frequency of illegal border crossings and accurate, real-time locations for individuals. Fundamental requirements are that the system work in real time under varying environmental conditions with minimally expensive hardware. Such a system would have applications to a wide range of multiple object tracking problems.	algorithm;computer vision;kinesiology;real-time clock;requirement;television	Andrew Bernat;Stephen Riter	1989		10.1145/75427.1030308	computer vision;structure from motion;simulation;object-class detection;machine vision;tracking system;computer science;viola–jones object detection framework;heuristics;3d single-object recognition;computer graphics (images)	Vision	43.5866129833975	-42.84541309599461	197811
f796c56b7ff05ea5412522efe30dfd7be05aa4e4	background subtraction using compressed low-resolution images		Image processing and recognition are an important part of the modern society, with applications in fields such as advanced artificial intelligence, smart assistants, and security surveillance. The essential first step involved in almost all the visual tasks is background subtraction with a static camera. Ensuring that this critical step is performed in the most efficient manner would therefore improve all aspects related to objects recognition and tracking, behavior comprehension, etc.. Although background subtraction method has been applied for many years, its application suffers from real-time requirement. In this letter, we present a novel approach in implementing the background subtraction. The proposed method uses compressed, low-resolution grayscale image for the background subtraction. These low-resolution grayscale images were found to preserve the salient information very well. To verify the feasibility of our methodology, two prevalent methods, ViBe and GMM, are used in the experiment. The results of the proposed methodology confirm the effectiveness of our approach.	artificial general intelligence;artificial intelligence;background subtraction;digital image processing;google map maker;grayscale;motion detector;real-time clock;vibe	Min Chen;Andy Song;Anna Eusebi;Jing Zhang	2018	CoRR		grayscale;image processing;artificial intelligence;pattern recognition;computer science;comprehension;background subtraction	AI	41.01285953500855	-44.80080936876728	198715
7442d16828ec0be5c8859f2365c53830817549f0	automatic reading of domestic electric meter: an intelligent device based on image processing and zigbee/ethernet communication	automatic meter reading;image processing;zigbee;bp neural network;dsp	In undeveloped areas around the world, many traditional meters need to be upgraded. Compared with replacing the mounted meters with high-cost modern ones, it is a better choice to upgrade them with new technologies. In this paper, an automatic reading system of the traditional household meter is designed on the basis of image processing and advanced DSP system. To identify the meter reading accurately, a regional average method is proposed to implement the image-scaling in order to avoid the distortion. In the image-filtering process, we raise an average-product method which is verified to attain good effects. For image segmentation, a new union thresholding method, based on the grayscale transformation, is proposed to enhance the adaptability of uneven luminance. Iterative rejection is applied to decrease the errors during character localization. Then, a training sample library with 1,400 characters is designed and collected for the training of the BP neural network. For data transmission, NAT technology is introduced to build data connection between the remote server and the data collectors working in the local area network. According to the field test, the proposed system can obtain a recognition rate of 99.7 % under normal environment, with the identification period below 2 s, while the resulting data can be transferred reliably through 1–3 walls in ordinary buildings.	artificial neural network;digital signal processing;distortion;grayscale;image processing;image scaling;image segmentation;network address translation;rejection sampling;server (computing);thresholding (image processing)	Yunzhou Zhang;Shanbao Yang;Xiaolin Su;Enyi Shi;Handuo Zhang	2013	Journal of Real-Time Image Processing	10.1007/s11554-013-0361-2	embedded system;computer vision;real-time computing;computer hardware;image processing;computer science;digital signal processing;automatic meter reading	Graphics	43.664642983623494	-40.99592037765387	198943
68f09c86b77c844bde4f2e9c97a2a98c76f6992a	artificial neural network modeling for estimating the depth of penetration and weld bead width from the infra red thermal image of the weld pool during a-tig welding	image features;image processing;real time;weld bead width;infra red;hot spot;thermal imaging;depth of penetration;image processing techniques;thermal processing;neural network model;a tig welding;artificial neural network;infra red thermal images	It is necessary to estimate the weld bead width and depth of penetration using suitable sensors during welding to monitor weld quality. Infra red sensing is the natural choice for monitoring welding processes as welding is inherently a thermal processing method. An attempt has been made to estimate the bead width and depth of penetration from the infra red thermal image of the weld pool using artificial neural network models. Real time infra red images were captured using IR camera during A-TIG welding. The image features such as length and width of the hot spot, peak temperature and other features are extracted using image processing techniques. These parameters along with their respective current values are used as inputs while the measured bead width and depth of penetration are used as output of the neural network models. Accurate ANN models predicting weld bead width and depth of penetration have been developed.		S. Chokkalingham;N. Chandrasekhar;M. Vasudevan	2010		10.1007/978-3-642-17298-4_28	infrared;computer science;machine learning;hot spot;feature;artificial neural network	ML	41.17818828645886	-40.21937803044685	199289
9ec26e533adb4be88870c7f4019073c6a2f5fa49	short to mid-range night fire detection		Computer vision methods used for night-time fire detection are limited. Existing works are for detection of distant night fires recorded from watch towers. In this paper, detection of short to mid-range night fires from video cameras are aimed. Flames in short distance flicker, grow and move more rapidly compared to ones in long distance. Features obtained by taking advantage of these distinctions let us detect fire over 90% accuracy on average in videos containing deceptive light sources like common city lights and headlights of vehicles.	computer vision;flicker (screen)	Ahmet Kerim Aggirman;Kasim Tasdemir	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960275	artificial intelligence;computer vision;flicker;feature extraction;computer science;fire detection	Vision	44.95465594097398	-41.577961766968976	199598
