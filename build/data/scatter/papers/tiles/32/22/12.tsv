id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
053ffb90f8ede89a845c491783ed5f8f076dd2ac	transform-coded pel-recursive video compression	rate distortion;codecs;decoding;video compression transform coding video coding pel recursive compression;decoding transforms video compression prediction algorithms codecs encoding rate distortion;video compression;prediction algorithms;transforms;encoding	We propose an algorithm that accomplishes transform-coded, spatiotemporal, pel-recursive video compression. Traditional pel-recursive coders obtain sophisticated spatio-temporal predictions for the current pixel based on previously decoded data. The resulting per-pixel prediction errors are encoded independently so that the decoder can use previously-encoded pixels in the prediction of the current. It is well-known that pel-recursive coders significantly under-perform modern hybrid coders which use comparatively very simple predictors but transform code the prediction errors. Our algorithm combines the accurate predictors of pel-recursive techniques with transform coders so that the strengths of both approaches can be taken advantage of. In the proposed work, the decoder transform decodes the residuals for a block and then acts like a simple pel-recursive decoder for the pixels of that block. We show that the proposed algorithm can be seen as the use of a pel-recursion enabling transform at the encoder, which generates and encodes the correct transform coefficients to avoid error propagation at the decoder. A straightforward implementation of our work (implemented as an extension of HEVC that preserves independent decodability of INTER blocks) shows compression improvements over the baseline HEVC.	algorithm;baseline (configuration management);coefficient;data compression;encoder;high efficiency video coding;pixel;propagation of uncertainty;recursion;software propagation	Jana Ehmann;Onur G. Guleryuz;Sehoon Yea	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532787	video compression picture types;data compression;computer vision;data compression ratio;codec;prediction;computer science;theoretical computer science;mathematics;move-to-front transform;encoding;statistics;sum of absolute transformed differences;multiview video coding	Visualization	45.44444916347923	-17.29384872075468	188884
39b348d4deee12bdbdc6a65f5e93a8bac6b2c320	video time encoding machines	silicon;desciframiento;modelizacion;codificacion de video;algorithms artificial intelligence nerve net neural networks computer pattern recognition automated software design video recording visual cortex visual perception;cascade circuit;entrada salida;streaming;neurone impulsionnel;video streaming;filter bank;integrated circuit;decodage;decoding;banc filtre;incendie;input circuit;neurona pulsante;integrate and fire;neural circuit architectures;faithful representation;single input multi output;circuito entrada;calcul ensembliste;cine;circuit cascade;input output;modelisation;video coding;transmission en continu;neurons encoding silicon integrated circuit modeling streaming media decoding retina;visual receptive fields;calculo conjunto;spiking neurons;senal video;signal video;visual receptive fields faithful representation neural circuit architectures spiking neurons time encoding;codage video;streaming media;spiking neuron;retina;movies;banco filtro;integrated circuit modeling;security key;video recording;time encoding;incendio;aparato visual;video signal;artificial intelligence;algorithms;appareil visuel;nerve net;visual perception;pattern recognition automated;spike train;receptive field;set computation;neurons;transmision fluyente;cinema;reseau neuronal;neural networks computer;software design;cle securite;spike sequence video time encoding machines time encoding time decoding visual stimuli synthetic video streams natural video streams single input multi output neural circuits neural circuits;fires;encoding;circuit entree;modeling;visual system;visual cortex;red neuronal;circuito cascada;neural network;entree sortie;llave seguridad	We investigate architectures for time encoding and time decoding of visual stimuli such as natural and synthetic video streams (movies, animation). The architecture for time encoding is akin to models of the early visual system. It consists of a bank of filters in cascade with single-input multi-output neural circuits. Neuron firing is based on either a threshold-and-fire or an integrate-and-fire spiking mechanism with feedback. We show that analog information is represented by the neural circuits as projections on a set of band-limited functions determined by the spike sequence. Under Nyquist-type and frame conditions, the encoded signal can be recovered from these projections with arbitrary precision. For the video time encoding machine architecture, we demonstrate that band-limited video streams of finite energy can be faithfully recovered from the spike trains and provide a stable algorithm for perfect recovery. The key condition for recovery calls for the number of neurons in the population to be above a threshold value.	action potential;algorithm;analog;animation;arbitrary-precision arithmetic;architecture as topic;bandlimiting;biological neuron model;cascade device component;feedback;movies;neurons;numerous;projections and predictions;streaming media;synthetic intelligence;the spike (1997)	Aurel A. Lazar;Eftychios A. Pnevmatikakis	2011	IEEE Transactions on Neural Networks	10.1109/TNN.2010.2103323	input/output;computer vision;faithful representation;systems modeling;visual system;movie theater;visual perception;computer science;software design;theoretical computer science;integrated circuit;machine learning;filter bank;silicon;receptive field;artificial neural network;encoding	ML	42.98864715637192	-23.760543069857224	189008
64108852add4fd369a5f1de378fea9d69cef989e	content-adpative h.264 rate control for live screencasting	screen codec rate control h 264 screencast;image motion analysis;video streaming data compression graphical user interfaces image motion analysis video coding;video streaming;data compression;video coding;graphical user interfaces;bit rate classification algorithms motion pictures video codecs encoding graphical user interfaces;subjective quality assessment score content adaptive h 264 rate control algorithm live screen casting screen content encoding screen content streaming signal characteristics slow motion phase fast motion phase spatial frame area update pattern temporal frame area update pattern gui operations frame rate adaptive cqp fra cqp frame rate crf vbv rc algorithm	Live screencasting involves encoding and streaming the screen content of a PC in real-time. Most existing H.264 rate control (RC) algorithms are designed for natural scenes and do not perform well with the quite different signal characteristics of screencasting. This paper proposes a content-adaptive H.264 RC scheme which classifies screen content as “slow-motion” phase or “fast-motion” phase on the fly, based on the temporal and spatial frame-area update pattern of recent frames. Then, for frames in “slow-motion” phase, which usually result from a presenter's GUI operations, a new RC algorithm named Frame Rate Adaptive-CQP (FRA-CQP) is applied, which puts priority on the quality of individual frame rather than the frame rate. For frames in “fast-motion” phase, which usually result from playing a movie during the presentation, the classical CRF+VBV RC algorithm is applied. Evaluation results show this adaptive RC scheme can regularly achieves higher subjective quality assessment score (0.7 to 1.6 points on a 1-5 scale) than existing algorithms while meeting the RC objective.	graphical user interface;h.264/mpeg-4 avc;on the fly;oracle flashback;rc algorithm;real-time transcription;screencast;windows live	Yi Lin;Weikai Xie;Lei Jin;Ruimin Shen	2012	2012 Visual Communications and Image Processing	10.1109/VCIP.2012.6410797	data compression;real-time computing;telecommunications;computer science;graphical user interface;multimedia;statistics;computer graphics (images)	AI	44.962351612175	-20.703386409601393	189098
354b27e01e4edec290a3a8633d1c2b27a1def8d4	comments on “fast motion estimation based on content property for low-complexity h.265/hevc encoder”	encoding;video coding;motion estimation;predictive models;complexity theory	In this paper, we provide comments on the recent paper by Pan et al. that proposed an initial search point-based motion estimation skipping (ISP-MES) method. We found some discrepancies of the proposed method and its experimental results, especially in setting the ISPs. In this paper, we clarify these issues and, as a result, provide an enhanced results from the work of Pan et al.	encoder;high efficiency video coding;manufacturing execution system;motion estimation	Sang-hyo Park;Euee Seon Jang	2017	IEEE Transactions on Broadcasting	10.1109/TBC.2017.2711146	encoder;computer science;theoretical computer science;motion estimation	Robotics	46.323093526942785	-18.35918934097657	189709
e85d1f05e6aaae870340a130da9b3ae034c6abbf	a temporal mode selection in the mpeg-2 encoder scheme	rate distortion;standards;psnr;transform coding;psnr encoding transform coding optimization rate distortion standards digital multimedia broadcasting;digital multimedia broadcasting;optimization;lts1;encoding	This paper deals with the mode decision in an MPEG-2 framework. An algorithm for mode decision is introduced. This algorithm is based on a rate-distortion criterion and takes into account the temporal dependency of the frames. This approach can allow a quality gain of more than one dB compared to the Test Model 5 (TM5) mode decision algorithm.	algorithm;distortion;encoder;mpeg-2	Laurent Piron	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		electronic engineering;telecommunications;computer science;theoretical computer science	Robotics	46.37221402712591	-18.040752454591757	190004
66e2f7182c5ed7b4b90bec4857ad18a689e64c09	light reference video quality estimation with embedding of compact features	feature subspace embedding;regression;qoe estimation;light references	One key issue in the content delivery network service is efficient and effective video quality assessment. Yet the legacy method obtaining the results via detailed calculation based on the original sequence or subjective experiments which lacks efficiency. In this work, we developed a compact feature based solution that allows for on-demand quality assessment with limited light references. Compact feature subspace embedding and SVM classifiers are trained to derive Quality of Experience (QoE) labels from the combination of the original and corresponding reconstructed compact features. Overhead in computation complexity and communication cost is minimum. Simulation results demonstrated the robustness and universality of the proposed solution.	coefficient;computation;content delivery network;discrete cosine transform;experiment;linear discriminant analysis;simulation;thumbnail;universality probability;video	Wenjie Zhu;Zhu Li;Yiling Xu	2016	2016 IEEE International Symposium on Multimedia (ISM)	10.1109/ISM.2016.0034	computer vision;regression;computer science;theoretical computer science;machine learning	EDA	45.44606679854343	-22.44153608305212	190157
f3d1f7efbb1ed8fd1480458063a65a5690703426	bit-depth scalable lossless coding for high dynamic range images	signal image and speech processing;t technology general;quantum information technology spintronics;tk electrical engineering electronics nuclear engineering	In this paper, we propose a bit-depth scalable lossless coding method for high dynamic range (HDR) images based on a reversible logarithmic mapping. HDR images are generally expressed as floating-point data, such as in the OpenEXR or RGBE formats. Our bit-depth scalable coding approach outputs base layer data and enhancement layer data. It can reconstruct the low dynamic range (LDR) image from the base layer data, and reconstructs the HDR image by adding the enhancement layer data. Most previous two-layer methods have focused on the lossy coding of HDR images. Unfortunately, the extension of previous lossy methods to lossless coding does not significantly compress the enhancement layer data. This is because the bit depth becomes very large, especially for HDR images in floating-point data format. To tackle this problem, we apply a reversible logarithmic mapping to the input HDR data. Moreover, we introduce a format conversion to avoid any degradation in the quality of the reconstructed LDR image. The proposed method is effective for both OpenEXR and RGBE formats. Through a series of experiments, we confirm that the proposed method decreases the volume of compressed data while maintaining the visual quality of the reconstructed LDR images.	data compression;elegant degradation;experiment;high dynamic range;high-dynamic-range imaging;high-dynamic-range rendering;ldraw;lossless compression;lossy compression;openexr;rgbe image format;scalability	Masahiro Iwahashi;Taichi Yoshida;Norrima Mokhtar;Hitoshi Kiya	2015	EURASIP J. Adv. Sig. Proc.	10.1186/s13634-015-0209-y	telecommunications;computer science;theoretical computer science	Graphics	43.56673778119493	-17.818373622274336	190329
7761044bdbf9b938ff1365017d8a92dc9469424e	sample adaptive offset in avs2 video standard	non consecutive offset bands sao avs2 shifted structure category dependent offset;non consecutive offset bands;shifted structure;sao;lcu adaptive offset avs2 video standard next generation video coding standard audio video coding standard china sample adaptive offset sao shifted structure largest coding unit;avs2;category dependent offset;video coding audio coding;standards encoding video coding decoding indexes artificial intelligence media	AVS2 video standard is the next-generation video coding standard under the development of Audio Video coding Standard (AVS) workgroup of China. In this paper, the design of Sample Adaptive Offset (SAO) in AVS2 is presented. Considering the implementation issues, a shifted structure in which the SAO parameter region is shifted from the Largest Coding Unit (LCU) to the upper-left is adopted to make the SAO parameter region consistent with the processing region in implementation. Moreover, the category dependent offset is introduced in the edge type based on the statistical results to improve the offset coding and non-consecutive offset bands are adopted in the band type to optimize offset bands. The test results show that SAO achieves on average 0.3% to 1.4% luma coding gain in AVS2 common test conditions.	coding gain;coding tree unit;data compression;graphics display resolution;high efficiency video coding;lookahead carry unit;sword art online: progressive;traffic collision avoidance system;video coding format	Jie Chen;Sunil Lee;Elena Alshina;Yinji Piao	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051506	sub-band coding;real-time computing;speech recognition;telecommunications;computer science;coding tree unit	Vision	45.502556138722824	-19.123937086029684	191510
4e592631b4f6533b7f97579fe01d7e79e77a0b25	automatic region-of-interest detection and prioritisation for visually optimised coding of low bit rate videos	video coding fourier transforms mobile computing mobile handsets object detection;video coding;eye fixation data automatic region of interest detection automatic region of interest prioritisation visually optimised coding low bit rate videos video consumption mobile devices video coding strategy diverse communication networks video services sustainable quality visually optimised video adaptation region of interest based scalability roi based video coding phase spectrum quaternion of fourier transform method pqft roi prioritisation technique human roi;fourier transforms;mobile handsets;mobile computing;videos humans encoding bit rate video coding quaternions fourier transforms;object detection	The increasing popularity of video consumption from mobile devices requires an effective video coding strategy. To overcome diverse communication networks, video services often need to maintain sustainable quality when the available bandwidth is limited. One of the strategy for a visually-optimised video adaptation is by implementing a region-of-interest (ROI) based scalability, whereby important regions can be encoded at a higher quality while maintaining sufficient quality for the rest of the frame. The result is an improved perceived quality at the same bit rate as normal encoding, which is particularly obvious at the range of lower bit rate. However, because of the difficulties of predicting region-of-interest (ROI) accurately, there is a limited research and development of ROI-based video coding for general videos. In this paper, the phase spectrum quaternion of Fourier Transform (PQFT) method is adopted to determine the ROI. To improve the results of ROI detection, the saliency map from the PQFT is augmented with maps created from high level knowledge of factors that are known to attract human attention. Hence, maps that locate faces and emphasise the centre of the screen are used in combination with the saliency map to determine the ROI. The contribution of this paper lies on the automatic ROI detection technique for coding a low bit rate videos which include the ROI prioritisation technique to give different level of encoding qualities for multiple ROIs, and the evaluation of the proposed automatic ROI detection that is shown to have a close performance to human ROI, based on the eye fixation data.	data compression;eb-eye;experiment;face detection;frame (video);high-level programming language;map;mobile device;region of interest;scalability;screenshot;spectral density;telecommunications network	Ivan Himawan;Wei Song;Dian Tjondronegoro	2013	2013 IEEE Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2013.6475002	fourier transform;computer vision;computer science;multimedia;mobile computing;multiview video coding;computer graphics (images)	Vision	45.64754742182768	-23.17410042275138	191649
696bdd5c7c18f2d5a306e766d5277ece1bd4a0bf	rate scalable video compression based on flexible block wavelet coding technique	flexible block wavelet coding;scalable video;prediction error;psnr;data compression;motion compensation;chaos;performance;video compression;entropy coder;transform coding;entropy coder rate scalable video compression flexible block wavelet coding fbwc delta frame compression video compression video codec performance motion compensation temporal redundancy predicted error frames zerotree coding mechanism;zerotree coding mechanism;video codec;computer applications;motion compensated;temporal redundancy;wavelet transforms;variable rate codes;video coding;fbwc;wavelet transform;redundancy;entropy codes;scientific computing;video codecs;video compression wavelet transforms scalability motion compensation chaos video codecs wavelet domain psnr scientific computing computer applications;scalability;rate scalable video compression;wavelet domain;predicted error frames;entropy codes data compression video coding video codecs variable rate codes wavelet transforms transform coding motion compensation redundancy;delta frame compression	Summary form only given. We present a new wavelet coding technique, flexible block wavelet coding (FBWC). It is specially designed for delta frame compression. FBWC not only makes delta frame compression more efficient but also keeps the rate scalability and other scalabilities of wavelet based scalable video compression techniques. Based on this new delta frame compression technique, we have implemented a highly efficient rate scalable video codec. Its overall performance surpasses the video codec based on SPIHT and is comparable to that of H.263. Using FBWC, delta frames are compressed in three steps: First, we use motion compensation to eliminate the temporal redundancy between adjacent frames and the predicted error frames (PEF) are generated. Next, the PEF are transformed into the wavelet domain using the flexible block wavelet transform (FBWT). Finally, the zerotree coding mechanism and entropy coder are used to actually compress the FBWT transformed wavelet coefficients to the target data rate.	data compression;wavelet transform	Ming Wei;Hongyang Chao	2002		10.1109/DCC.2002.1000021	video compression picture types;data compression;reference frame;computer vision;speech recognition;second-generation wavelet transform;computer science;theoretical computer science;mathematics;wavelet packet decomposition;stationary wavelet transform;context-adaptive binary arithmetic coding;discrete wavelet transform;motion compensation;statistics;multiview video coding;wavelet transform	EDA	45.77968455642597	-17.045426582260813	192424
bc2c36a1f7795edbab3becd6b9ca3de9e89b6270	improvement of the tile format for stereoscopic video	data compression;video signal processing;video signal processing data compression stereo image processing television broadcasting three dimensional television;tiling artifact reduction tile format improvement stereoscopic video frame packing arrangement 3d tv broadcasting services left eye image right eye image stereoscopic image pair single composition frame service provider production infrastructure distribution infrastructure frame compatible formats video quality 3d display 2d display tile format composition method;three dimensional television;television broadcasting;video compression stereoscopic video video format 3d video;stereo image processing;tiles psnr image edge detection image coding stereo image processing three dimensional displays video sequences	Tile format is a new frame packing arrangement for the first generation of 3D TV broadcasting services. It puts the left- and right-eye images of each stereoscopic image pair into a single composition frame that allows the service provider to reuse existing production and distribution infrastructure for offering 3D TV services. Different from frame compatible formats, the tile format composition does not down-sample the left- and right-eye images and, thus, provides a higher video quality for both 3D and 2D display. The problem with the tile format is that it creates artificial edges in the right-eye image and introduces visible tiling artifacts around the artificial edges after compression. This paper presents a simple modification to the tile format composition method that significantly reduces the tiling artifacts. Experimental results show that local PSNRs around artificial edges are improved by over 1dB.	3d television;dvb 3d-tv;quadruple-precision floating-point format;ringing artifacts;set packing;stereoscopy;tiling array;tiling window manager	Philip Blanchfield;Demin Wang	2013	2013 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2013.6621705	video compression picture types;computer vision;video;image processing;computer science;video capture;video tracking;multimedia;video processing;smacker video;multiview video coding;computer graphics (images)	Robotics	43.10579382970658	-20.80368729046096	192430
68a5cc5d0a7fddc18099f54b43a7b5758eb26e8f	impact analysis of baseband quantizer on coding efficiency for hdr video	baseband;test sequences impact analysis hdr video high dynamic range video baseband signal video codecs baseband quantization;video coding quantisation signal;quantization signal;pipelines;dynamic range;video codecs;baseband quantization signal encoding dynamic range video codecs pipelines;transform coding bitdepth hevc h 265 high dynamic range hdr quantization reshaping;encoding	Digitally acquired high dynamic range (HDR) video baseband signal can take 10-12 bits per color channel. It is economically important to be able to reuse the legacy 8 or 10-bit video codecs to efficiently compress the HDR video. Linear or nonlinear mapping on the intensity can be applied to the baseband signal to reduce the dynamic range before the signal is sent to the codec, and we refer to this range reduction step as a baseband quantization. We show analytically and verify using test sequences that the use of the baseband quantizer lowers the coding efficiency. Experiments show that as the baseband quantizer is strengthened by 1.6 bits, the drop of PSNR at a high bitrate is up to 1.60 dB. Our result suggests that in order to achieve high coding efficiency, information reduction of videos in terms of quantization error should be introduced in the video codec instead of on the baseband signal.	algorithmic efficiency;baseband;channel (digital image);color depth;high-dynamic-range rendering;high-dynamic-range video;nonlinear system;peak signal-to-noise ratio;quantization (signal processing)	Chau-Wai Wong;Guan-Ming Su;Min Wu	2016	IEEE Signal Processing Letters	10.1109/LSP.2016.2597175	computer vision;dynamic range;real-time computing;telecommunications;computer science;baseband;pipeline transport;encoding	Vision	44.17988145611489	-17.779924338848694	192890
d9cc0005abc916f4f72c23059ffe1b423052651c	on the standardization of the jpeg xt image compression	transform coding image coding additives proposals encoding iso standards;data compression;video coding data compression iso standards standardisation;iso standards;coding technology jpeg xt image compression standardization iso iec 10918 1 standard;standardisation;video coding	The ISO recently started a standardization initiative on a forwards-compatible extension of its popular JPEG (ISO/IEC 10918-1) standard. This new standard aims at carefully extending the feature set of the existing technology while preserving the established tool chain built around its 20 years old predecessor. In this work, an overview on JPEG XT, its goals and the underlying technology will be given, and two of the proposed coding technologies for JPEG XT are compared side by side.	forward compatibility;image compression;jpeg;jpeg 2000;toolchain;utility functions on indivisible goods	Thomas Richter	2013	2013 Picture Coding Symposium (PCS)	10.1109/PCS.2013.6737677	data compression;telecommunications;computer science;jpeg;jpeg 2000;mathematics;multimedia;jpeg file interchange format;h.262/mpeg-2 part 2;mpeg-4;standardization;statistics	Logic	42.949665226155396	-17.787975143794448	193171
c0de10f9ac0bafb8aeb7515f06d697146881b6d4	multi-resolution lossless image compression for progressive transmission and multiple decoding using an enhanced edge adaptive hierarchical interpolation		In a multi-resolution image encoding system, the image is encoded into a single file as a layer of bit streams, and then it is transmitted layer by layer progressively to reduce the transmission time across a low bandwidth connection. This encoding scheme is also suitable for multiple decoders, each with different capabilities ranging from a handheld device to a PC. In our previous work, we proposed an edge adaptive hierarchical interpolation algorithm for multi-resolution image coding system. In this paper, we enhanced its compression efficiency by adding three major components. First, its prediction accuracy is improved using context adaptive error modeling as a feedback. Second, the conditional probability of prediction errors is sharpened by removing the sign redundancy among local prediction errors by applying sign flipping. Third, the conditional probability is sharpened further by reducing the number of distinct error symbols using error remapping function. Experimental results on benchmark data sets reveal that the enhanced algorithm achieves a better compression bit rate than our previous algorithm and other algorithms. It is shown that compression bit rate is much better for images that are rich in directional edges and textures. The enhanced algorithm also shows better rate-distortion performance and visual quality at the intermediate stages of progressive image transmission.		Yenewondim Biadgie;Minsung Kim;Kyung-Ah Sohn	2017	TIIS	10.3837/tiis.2017.12.018	distributed computing;interpolation;redundancy (engineering);transmission time;computer science;conditional probability;encoding (memory);algorithm;decoding methods;lossless compression;bandwidth (signal processing)	Vision	44.63116195543362	-17.288528896325225	194251
43f7083f5674514ea0de38d77cf5c48706e34a36	content-adaptive spatial scalability for scalable video coding	nonlinear filters;high resolution;video streaming;scalable video coding;image resolution;cass;svc;h 264 avc;non linear image warping h 264 avc scalable video coding spatial scalability content adaptation;non linear texture prediction;code standards;encoding static var compensators scalability bit rate maximum likelihood detection nonlinear filters spatial resolution;adaptive codes;bit rate;image texture;video streams;avc;video coding;non linear image warping content adaptive spatial scalability scalable video coding svc h 264 avc cass video streams video resolution non linear distortion non linear texture prediction;content adaptive spatial scalability;content distribution;video resolution;non linear distortion;prediction accuracy;maximum likelihood detection;h 264;content adaptation;static var compensators;scalability;non linear image warping;video streaming adaptive codes code standards image resolution image texture video coding;encoding;scalable coding;image warping;aspect ratio;spatial scalability;spatial resolution	This paper presents an enhancement of the SVC extension of the H.264/AVC standard by content-adaptive spatial scalability (CASS). CASS introduces a novel functionality which is important for high quality content distribution. The video streams (spatial layers), which are used as input to the encoder, are created by content-adaptive and art-directable retargeting of existing high resolution video. Video is retargeted to resolutions and aspect ratios which are mainly dictated by target display devices. Thereby no content is cut off, but visually important content is preserved at the expense of a non-linear distortion of visually unimportant areas. The non-linear dependencies between such video streams are efficiently exploited by CASS for scalable coding. This is achieved by integrating warping-based non-linear texture prediction and warp coding into the SVC framework. The results indicate high prediction accuracy of non-linear predictors and high compression efficiency with limited increase in bit rate and complexity compared to the standard SVC for the case of INTRA only coding.	data compression;digital distribution;display resolution;distortion;encoder;h.264/mpeg-4 avc;image resolution;image warping;nonlinear system;retargeting;scalability;scalable video coding;streaming media	Yongzhe Wang;Nikolce Stefanoski;Xiangzhong Fang;Aljoscha Smolic	2010	28th Picture Coding Symposium	10.1109/PCS.2010.5702471	scalable video coding;computer vision;real-time computing;image resolution;computer science;coding tree unit;multimedia;context-adaptive binary arithmetic coding;multiview video coding	Vision	44.47150604751627	-18.545363907776473	194513
1940240463934a0c491f815dc5701d255c24ca82	bandwidth-aware video encoding with adaptive image scaling	image quality;wireless networks;bandwidth;quantization;fluctuations;mpeg 4;psnr;image resolution;degradation;mpeg 2;rate distortion theory;encoding;wireless network	We proposed an adaptive MPEG-2/4 video encoding scheme suited for a dynamic networking environment. Conventional approaches use large quantization scales to reduce the transmission bit rate in low bandwidth situations. However, the larger quantization scale degrades the image quality severely. We believe not only that adaptive quantization should be used for bit allocation, but also that the image resolution should be jointly adjustable. Our goal is thus to investigate how image subsampling should be used in bandwidth fluctuations (e.g., 3/4G wireless networks). For this purpose, a rate-distortion model considering image subsampling is studied and implemented in the proposed encoder. Experiments show that, by choosing the right subsampling factor, a better image quality with considerably lower transmission bit rate can be obtained. A maximum gain in PSNR of 2 dB is observed with more than 50% reduction in quantization size.	2.5d;chroma subsampling;data compression;distortion;encoder;experiment;image quality;image resolution;image scaling;line code;network traffic control;peak signal-to-noise ratio;quantization (signal processing)	A. S. Abraham;Ju Wang;Jonathan C. L. Liu	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		image quality;computer vision;image resolution;rate–distortion theory;computer science;theoretical computer science;wireless network;video post-processing;statistics;computer network	Vision	43.83387602724161	-17.738086215151327	194761
4430fcbec65116805fc0d552bc6eeb3a4c48fc94	an mpeg-7 extension for describing visual impairments	audio visual systems;delivery system;audio quality description;quality analysis mpeg 7 defect;mpeg 7 extension;image restoration;transform coding;materials;audiovisual media;audiovisual production;standardisation;video coding;visualization;video coding audio visual systems meta data;quality analysis;mpeg 7 standard image restoration satellite broadcasting production systems multimedia communication image analysis documentation content management noise level visualization;defect;audiovisual metadata standards;multimedia communication;visual impairments;audiovisual preservation;production;visual impairment;audio quality description mpeg 7 extension visual impairments audiovisual essence audiovisual production audiovisual preservation audiovisual media audiovisual metadata standards;meta data;quality measures;mpeg 7;noise;audiovisual essence	Analysing the condition of audiovisual essence is an important step in audiovisual production and preservation. Standardised impairment description of audiovisual media is a pre-requisite for system interoperability between content digitisation, documentation, management, restoration, production and delivery systems.We analyse existing capabilities for describing impairment in audiovisual metadata standards. Because of its unique detailed spatiotemporally structured description capabilities we have selected MPEG-7 as the basis for visual impairment description. Following the approach for audio quality description, we define a general description scheme for visual impairments, which allows representing defect events and statistical quality measures. For certain defects, more specialised descriptors are proposed. In addition, we define a comprehensive classification scheme for visual impairments.	cellular automaton;circuit restoration;documentation;interoperability;mpeg-7;software bug	Werner Bailer;Peter Schallauer	2008	2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services	10.1109/WIAMIS.2008.22	image restoration;computer vision;transform coding;speech recognition;visualization;computer science;noise;multimedia;metadata;standardization	Vision	42.549709879677415	-21.73903931581147	195099
b8aea1978343b152461651db9a596925f3499311	block merging for quadtree-based partitioning in hevc	data compression;motion compensation;quadtree partition block merging direct mode high efficiency video coding hevc motion compensation;iso standards;single motion parameter set hevc standard high efficiency video coding standard itu t video coding experts group iso iec moving picture experts group video compression capability quadtree based block partitioning motion compensated prediction block merging algorithm;video coding;iec standards;video coding data compression iec standards iso standards motion compensation quadtrees;quadtrees;merging encoding video coding motion compensation algorithm design and analysis standards	The joint development of the upcoming High Efficiency Video Coding (HEVC) standard by ITU-T Video Coding Experts Group and ISO/IEC Moving Picture Experts Group marks a new step in video compression capability. In technical terms, HEVC is a hybrid video-coding approach using quadtree-based block partitioning together with motion-compensated prediction. Even though a high degree of adaptability is achieved by quadtree-based block partitioning, this approach has certain intrinsic drawbacks, which may result in redundant sets of motion parameters being transmitted. Previous work has shown that those redundancies can effectively be removed by merging the leafs of a particular quadtree structure. Following this concept, a block merging algorithm for HEVC is now proposed. This algorithm generates a single motion parameter set for a whole region of contiguous motion-compensated blocks. In this paper, we describe the various components of the proposed block merging algorithm and, using experimental evidence, demonstrate their benefits in terms of coding efficiency.	algorithm;algorithmic efficiency;blu-ray;data compression;direct mode;h.264/mpeg-4 avc;high efficiency video coding;moving picture experts group;quadtree;simulation;vc dimension	Philipp Helle;Simon Oudin;Benjamin Bross;Detlev Marpe;M. Oguz Bici;Kemal Ugur;Joël Jung;Gordon Clare;Thomas Wiegand	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2012.2223051	data compression;computer vision;real-time computing;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;mathematics;block-matching algorithm;context-adaptive binary arithmetic coding;motion compensation;h.262/mpeg-2 part 2;h.261;algorithm;statistics;multiview video coding	EDA	46.2302349597168	-18.86823555535539	196076
147eced33d112f4ebea4b6a70ed48448df2ca87c	multiresolution graph fourier transform for compression of piecewise smooth images	image coding;decoding;image coding discrete cosine transforms decoding image edge detection transform coding laplace equations;transform coding;piecewise smooth images image compression graph fourier transform;laplace equations;image edge detection;discrete cosine transforms;piecewise smooth image compression multiresolution graph fourier transform h 264 peak signal to noise ratio bit rate multiresolution gft scheme depth maps computer graphics images decoding eigen decomposition arithmetic edge coding hr boundaries interpolation lr gft low resolution pixel block high resolution pixel block encoder low pass filter computational complexity minimum graph cuts spectral clustering graph optimization techniques discrete cosine transform signal transform coefficients signal representation cost graph signal processing signal characteristics pws image compression;low pass filters arithmetic codes computational complexity data compression eigenvalues and eigenfunctions fourier transforms graph theory image coding image representation image resolution interpolation	Piecewise smooth (PWS) images (e.g., depth maps or animation images) contain unique signal characteristics such as sharp object boundaries and slowly varying interior surfaces. Leveraging on recent advances in graph signal processing, in this paper, we propose to compress the PWS images using suitable graph Fourier transforms (GFTs) to minimize the total signal representation cost of each pixel block, considering both the sparsity of the signal's transform coefficients and the compactness of transform description. Unlike fixed transforms, such as the discrete cosine transform, we can adapt GFT to a particular class of pixel blocks. In particular, we select one among a defined search space of GFTs to minimize total representation cost via our proposed algorithms, leveraging on graph optimization techniques, such as spectral clustering and minimum graph cuts. Furthermore, for practical implementation of GFT, we introduce two techniques to reduce computation complexity. First, at the encoder, we low-pass filter and downsample a high-resolution (HR) pixel block to obtain a low-resolution (LR) one, so that a LR-GFT can be employed. At the decoder, upsampling and interpolation are performed adaptively along HR boundaries coded using arithmetic edge coding, so that sharp object boundaries can be well preserved. Second, instead of computing GFT from a graph in real-time via eigen-decomposition, the most popular LR-GFTs are pre-computed and stored in a table for lookup during encoding and decoding. Using depth maps and computer-graphics images as examples of the PWS images, experimental results show that our proposed multiresolution-GFT scheme outperforms H.264 intra by 6.8 dB on average in peak signal-to-noise ratio at the same bit rate.	algorithm;animation;biologic preservation;block (programming);cluster analysis;coefficient;compresses (device);compression;computation (action);computer graphics;cut (graph theory);decimation (signal processing);decoder device component;depth map;discrete cosine transform;eigen (c++ library);encoder device component;graph - visual representation;h.264/mpeg-4 avc;high-resolution scheme;image compression;image resolution;incised wound;interpolation imputation technique;lr parser;lookup table;low-pass filter;mathematical optimization;microsoft personal web server;myocytes, smooth muscle;numerous;partial wave spectroscopic microscopy;peak signal-to-noise ratio;pixel;precomputation;real-time clock;s transform;sampling - surgical action;signal processing;sparse matrix;spectral clustering;upsampling;statistical cluster	Wei Hu;Gene Cheung;Antonio Ortega;Oscar C. Au	2015	IEEE Transactions on Image Processing	10.1109/TIP.2014.2378055	mathematical optimization;discrete mathematics;transform coding;lapped transform;theoretical computer science;discrete cosine transform;mathematics;statistics	Vision	42.789269764627335	-17.09373278514997	196200
6ba33cf5524b3ca4a66b28af97d5b203bf5b9a1b	contour image sequence compression through motion analysis and hybrid coding method	motion analysis;image coding;image processing;sign language;line drawings;computer programming;object oriented;signal processing;image sequence;compression ratio;video signals;parameter estimation	This paper presents a motion analysis algorithm (MAA) and a hybrid coding method for contour image sequence compression. The contour image sequence consists of objects moving and rotating in a 3-D world with occlusion, shape, and size variations from frame to frame. The MAA separates the moving image sequence into several object-oriented subsequences (OOSs). In each OOS, the component is either stationary or moves smoothly, and the motion parameters can be easily estimated. The first and last frames of OOS are key frames, and the others are in-between frames. The key frames are unpredictable, and the entire frames need to be encoded. The in-between frames are compensable, and they are encoded by the motion parameter coding. The hybrid coder uses vectorgraph coding to remove spatial redundancy of the key frames and motion parameter coding to reduce the temporal redundancy of the OOSs. The motion parameters are encoded as combinations of 2-D translation, 2-D rotation, and scaling. There are many applications for contour image sequence compression. The cartoon image sequence (a sequence of line drawing sketches) and the high-frame-rate videophone for sign language transmission are good examples. Experiments show that our method encodes the contour image sequence at a very high compression ratio without losing intelligibility.		Chung-Lin Huang	1992	Multidim. Syst. Sign. Process.	10.1007/BF01942045	computer vision;speech recognition;sign language;image processing;computer science;signal processing;compression ratio;motion estimation;computer programming;mathematics;motion compensation	Vision	42.01362848156605	-19.033145430790327	196506
b2eded346f050112d6071499d562d89c0f376f83	an overview of emerging technologies for high efficiency 3d video coding		3D video coding is one of the most popular research area in multimedia. This paper reviews the recent progress of the coding technologies for multiview video (MVV) and free view-point video (FVV) which is represented by MVV and depth maps. We first discuss the traditional multiview video coding (MVC) framework with different prediction structures. The rate-distortion performance and the view switching delay of the three main coding prediction structures are analyzed. We further introduce the joint coding technologies for MVV and depth maps and evaluate the rate-distortion performance of them. The scalable 3D video coding technologies are reviewed by the quality and view scalability, respectively. Finally, we summarize the bit allocation work of 3D video coding. This paper also points out some future research problems in high efficiency 3D video coding such as the view switching latency optimization in coding structure and bit allocation.	asp.net mvc;behavior model;data compression;depth map;digital video;distortion;mathematical optimization;model–view–controller;multiview video coding;requirement;scalability;stereoscopic video coding;streaming media	Qifei Wang	2015	CoRR		computer vision;computer science;theoretical computer science;multimedia;h.261;multiview video coding	AI	44.064076869989215	-19.643906937281862	197363
929b1850d5dd1067982a82d864f808985cc220ef	enhancing ltw image encoder with perceptual coding and gpu-optimized 2d-dwt transform	signal image and speech processing;quantum information technology spintronics	When optimizing a wavelet image coder, the two main targets are to (1) improve its rate-distortion (R/D) performance and (2) reduce the coding times. In general, the encoding engine is mainly responsible for achieving R/D performance. It is usually more complex than the decoding part. A large number of works about R/D or complexity optimizations can be found, but only a few tackle the problem of increasing R/D performance while reducing the computational cost at the same time, like Kakadu, an optimized version of JPEG2000. In this work we propose an optimization of the E_LTW encoder with the aim to increase its R/D performance through perceptual encoding techniques and reduce the encoding time by means of a graphics processing unit-optimized version of the two-dimensional discrete wavelet transform. The results show that in both performance dimensions, our enhanced encoder achieves good results compared with Kakadu and SPIHT encoders, achieving speedups of 6 times with respect to the original E_LTW encoder.	algorithmic efficiency;beta encoder;cuda;coefficient;computation;computer graphics;discrete wavelet transform;distortion;encoder;graphics processing unit;human visual system model;image resolution;jpeg 2000;kakadu;mathematical optimization;psychoacoustics;set partitioning in hierarchical trees	Miguel Martínez-Rach;Otoniel López;Vicente Galiano Ibarra;Héctor Migallón Gomis;Jesús Llor;Manuel P. Malumbres	2013	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2013-141	real-time computing;simulation;telecommunications;computer science;theoretical computer science	Robotics	44.39086433757033	-17.311219470284364	197773
080e80934ab82d89003eabcbb04cf4657b422a4f	visually lossless compression of stereo images	visually lossless coding;image coding;stereo images consumer grade 3d displays stereo pair left eye image right eye image efficient data compression technique visibility thresholds 3d displays active shutter glasses visually lossless compression method monochrome stereoscopic 3d images;data compression;crosstalk;stereo image processing data compression image coding;stereoscopic images;human visual system;stereo image processing;jpeg2000;human visual system stereoscopic images visually lossless coding jpeg2000 crosstalk;image coding three dimensional displays stereo image processing transform coding bit rate	With consumer-grade 3D displays becoming widely available, stereoscopic 3D imaging has received increased attention over the last few years. Since a left eye and a right eye image are contained in a stereo pair, stereoscopic imaging requires double the amount of data compared to 2D images. Therefore, efficient data compression techniques are especially critical in these applications. In this paper, we propose a method to determine visibility thresholds (VTs) for 3D displays with active shutter glasses. These VTs are then used in a novel visually lossless compression method for monochrome stereoscopic 3D images.	3d reconstruction;active shutter 3d system;data compression;lossless compression;monochrome;movie projector;stereo display;stereoscopic video game;stereoscopy	Hsin-Chang Feng;Michael W. Marcellin;Ali Bilgin	2013	2013 Data Compression Conference	10.1109/DCC.2013.71	data compression;lossy compression;computer vision;crosstalk;computer science;jpeg 2000;human visual system model;statistics;computer graphics (images)	Vision	43.1210932088453	-20.676421138959245	198566
f41d7bae215accbc69302a6a4cf00f6affedab86	object-based analysis-synthesis coding based on the source model of moving rigid 3d objects	videophone;parameter coding;analysis synthesis coding;shape coding;mpeg 4;image analysis;3d motion estimation;object based image coding	The topic of investigation was object-based analysis synthesis coding (OBASC) using the source model of 'moving rigid 3D objects' for the encoding of moving images at very low data rates. According to the coding concept, each moving object of an image is described and encoded by three parameter sets defining its motion, shape and surface color. The parameter sets of each object are obtained by image analysis. They are coded using an object-dependent parameter coding. Using the coded parameter sets, an image can be synthesized by model-based image synthesis. In comparison to block-based hybrid coding, OBASC requires the additional transmission of shape parameters. The transmission of shape information avoids the mosquito and block artifacts of a block-based coder. Furthermore, important areas such as facial area.s can be reconstructed with a significant image quality improvement. OBASC based on the source models of'moving flexible 2D objects' and of'moving rigid 3D objects' gives almost identical image quality for the same data rate. Therefore the use of more advanced source models like flexible 3D objects or 3D face models is expected to further improve image quality.	facial recognition system;image analysis;image quality;object-based language;open-source software;rendering (computer graphics);uncompressed video	Jörn Ostermann	1994	Sig. Proc.: Image Comm.	10.1016/0923-5965(94)90012-4	computer vision;image analysis;computer science;multimedia;mpeg-4;computer graphics (images)	Vision	42.1785750579754	-19.159238221267717	199180
514d3055031c872d81c92aeda7db06ac223f3b7e	fast mode selection algorithm based on derived layer	fast mode selection algorithm h 264 svc coding standard fast svc algorithm macroblock mode selection coding speed improvement coding cost multilayers video network complex heterogeneous network environment video transmission problems characteristic performance video compression user requirements video streaming rate video resolutions video quality svc coding efficiency scalable video coding derived layer;encoding static var compensators standards cost function correlation prediction algorithms streaming media;video streaming data compression image resolution video coding	Scalable Video Coding (SVC), provides different resolutions, different video quality and different video streaming rate after once compression according to various requirements of users. The characteristic performance can solve a series of video transmission problems encountered in the current complex and heterogeneous network environment conveniently and effectively, and provide a highly efficient solution for the new video network. Because of problems such as the SVC coding efficiency in multilayer and the coding cost, the research on SVC is mainly focused on how to improve the coding speed of the algorithm (fast SVC algorithm). For the macroblock mode selection in H.264/SVC, the paper selects the fast algorithm based on the macroblock in derived layer.	algorithmic efficiency;computation;loss function;macroblock;requirement;scalable video coding;selection algorithm;streaming media	Yingyi Liang;Zhenyu He;Yi Li	2014	Proceedings 2014 IEEE International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)	10.1109/SPAC.2014.6982678	video compression picture types;data compression;scalable video coding;real-time computing;uncompressed video;computer science;coding tree unit;block-matching algorithm;multimedia;context-adaptive binary arithmetic coding;motion compensation;h.261;computer network;multiview video coding	Robotics	45.62923508393048	-19.03885175656263	199645
4749e7adc459ecefa5d63a2911a372b6bd8cdb6a	low-complexity lossless video coding via adaptive spatio-temporal prediction	lossless video application low complexity lossless video coding backward adaptive spatio temporal prediction video sequence digital cinema medical imaging professional video processing color video compression spectral redundancy intra frame spatial predictor adaptive optimal weighting residual error entropy coding context based arithmetic encoder higher compression gain;arithmetic codes video coding spatiotemporal phenomena image sequences biomedical imaging entropy codes;entropy coding;adaptive optimization;video processing;biomedical imaging;low complexity;video coding;arithmetic codes;medical image;entropy codes;spatiotemporal phenomena;video coding video compression video sequences motion pictures biomedical imaging redundancy entropy context arithmetic delay;image sequences	Lossless coding of video sequences is becoming increasingly attractive for applications as diverse as digital cinema, medical imaging and professional video processing. We present a new low-delay, low-complexity algorithm for lossless color video compression. The proposed technique adaptively exploits temporal, spatial and spectral redundancy. Key features of the coder are a backward-adaptive temporal predictor, an intra-frame spatial predictor and adaptive optimal weighting of both predictive components. The residual error is entropy coded by a context-based arithmetic encoder. The proposed lossless video encoder delivers significantly higher compression gains than traditional approaches at approximately the same complexity and delay, enabling efficient storage and communications for emerging lossless video applications.	data compression;lossless compression	Elias S. G. Carotti;Juan Carlos De Martin;Angelo Raffaele Meo	2003		10.1109/ICIP.2003.1246650	video compression picture types;data compression;scalable video coding;medical imaging;adaptive optimization;computer vision;uncompressed video;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;video tracking;coding tree unit;tunstall coding;block-matching algorithm;multimedia;video processing;smacker video;adaptive coding;context-adaptive binary arithmetic coding;motion compensation;h.261;video denoising;multiview video coding	Vision	44.41488607861174	-18.192688386179757	199953
