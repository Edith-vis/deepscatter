id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
ae47eeab17b0eb85b6e3fa82704637118d05ed22	representing and integrating bibliographic information into the semantic web: a comparison of four conceptual models	linked data;frbr object oriented frbroo;bibliographic framework bibframe;functional requirements for bibliographic records frbr;europeana data model edm;conceptual models;semantic web;interoperability	Integration of library data into the Semantic Web environment is a key issue for libraries and is approached on the basis of interoperability between conceptual models. Several data models exist for the representation and publication of library data in the Semantic Web and therefore inter-domain and intra-domain interoperability issues emerge as a growing number of web data are generated. Achieving interoperability for different representations of the same or related entities between the library and other cultural heritage institutions shall enhance rich bibliographic data reusability and support the development of new data-driven information services. This paper aims to investigate common ground and convergences between four conceptual models, namely Functional Requirements for Bibliographic Records (FRBR), FRBR Object-Oriented (FRBRoo), Bibliographic Framework (BIBFRAME) and Europeana Data Model (EDM), enabling semantically-richer interoperability by studying the representation of monographs, as well as of content relationships (derivative and equivalent bibliographic relationships) and of whole-part relationships between them.	aggregate data;bibframe;bibliographic record;data model;don woods (programmer);endeavour (supercomputer);entity;europeana;frbroo;functional requirements for bibliographic records;information science;inter-domain;interoperability;library (computing);linked data;quixote;reference model;remote database access;requirement;semantic web;test case;turing completeness;worldcat	Sofia Zapounidou;Michalis Sfakakis;Christos Papatheodorou	2017	J. Information Science	10.1177/0165551516650410	semantic web;semantic interoperability;data modeling;information retrieval;data mining;computer science;interoperability;frbroo;bibframe;functional requirements for bibliographic records;data model	Web+IR	-38.48659797469041	6.603464641367044	159167
44f8a6c998cd9b0f5284a697dc5fd92747487427	a data model for information extraction from the web	query language;web documents;information extraction;automatic generation;data model;formal method;world wide web	The large amount of data available from the World Wide Web and the presence of heterogeneous infor mation sources require customized tools for extracting and summarizing relevant information Information Extraction IE from the Web is the process of extracting data from a set of Web pages in order to ll a given data base schema With respect to other extraction tasks IE from the Web presents two particular features i the input documents are semi structured in the sense that they are neither raw data full text nor strictly typed relational tables but a structure is provided by HTML or XML tags ii the information sources are heterogenous since the same kind of information is often represented in di erent ways by di erent sources A Web Information Extraction system receives two kinds of input i a data base schema that repre sents the information the user wants to retrieve ii a set of information sources Web sites containing the data to be extracted Such a system aims at populating the data base with data extracted from the sources and must provide mechanisms for dealing with semi structured documents and for accessing heterogeneous information sources An important design element for these systems is the representation techniques used for modelling the DB schema and the information sources Indeed this issue permits a classi cation of approaches to Web Information Extraction	data model;database;html;information extraction;population;semiconductor industry;type system;web page;world wide web;xml	Luca Iocchi	1999			web service;web mining;static web page;web development;web modeling;web query classification;data web;web mapping;web standards;computer science;semantic web;web navigation;web page;semantic web stack;database;web intelligence;web search query;web 2.0;world wide web;information extraction;information retrieval;web server	DB	-34.725712468641255	5.448475906194316	159177
ae71bb6b34ea93c836a75a85582cb1c5d3caf9f0	iafa templates in use as internet metadata	computer programming	Recently there has been a growing need for a metadata standard for the Internet. The files that are available on ftp and WWW sites can be difficult to search if they are enclosed in a container format (e.g. tar). and bibliographical data can be deeply embedded in documentation. This paper describes how IAFA Templates have been used in a real archive to store the metadata of lots of different types of documents and software and to derive WWW, gopher and text indices from them.		Dave J. Beckett	1996	World Wide Web Journal		the internet;world wide web;documentation;metadata standard;metadata;software;computer science;database;metadata repository;file transfer protocol;computer programming	ML	-35.12512779435818	7.927169922496228	159216
3650f9bb12a490f5276dd4fef02d0d270b439087	automatic conversion of scientific data into canonical format	xml data structures data visualisation document handling scientific information systems web services;relational data model;data manipulation;document handling;data visualization automatic scientific data conversion canonical format raw data representation database schemas web services automatic xml description relational data model converter compiler data manipulation;particle measurements;canonical form;scientific data;raw data representation;semantics;bridges;layout;web service;data model;converter compiler;data visualisation;data semantics;data structures;automatic scientific data conversion;web services;data visualization;xml;relational data model raw scientific data canonical format service oriented architecture;semantics layout xml data models bridges web services particle measurements;automatic xml description;data layout;database schemas;service oriented architecture;canonical format;raw scientific data;scientific information systems;data models	The purpose of the automatic conversion of scientific data into canonical format is to provide a link between raw representation of data and database schemas. On the basis of these concepts Web services are developed for automatic XML description. They describe structurally and semantically scientific data and the conversion of the raw data layout description into canonical form based on data semantics. The canonical form looks like a relational data model where efficient methods for storage and retrieval of scientific data have been used. This paper describes the implementation of the converter compiler of raw scientific data layout description into canonical format. Using canonical XML form we have achieved better standardization of the input for data manipulation and visualization routines and minimization of their number.	canonical xml;compiler;database schema;mac address;relational database management system;relational model;sql;semantic data model;web service	Mariana Goranova;Bogdan Shishedjiev;Juliana Georgieva;Velin Achev	2011	2011 IEEE EUROCON - International Conference on Computer as a Tool	10.1109/EUROCON.2011.5929171	web service;canonical s-expressions;computer science;data mining;canonical model;database;information retrieval;data visualization	DB	-34.49008002137865	7.099628472204822	159382
c5a94663e278a0012305c0fe30f48b110d808a0c	clever search: a wordnet based wrapper for internet search engines	lexical semantics;search engine;web search engine;artificial intelligent	This paper presents an approach to enhance search engines with information about word senses available in WordNet. The approach exploits information about the conceptual relations within the lexical-semantic net. In the wrapper for search engines presented, WordNet information is used to specify a user's request or to classify the results of a publicly available web search engine, like Google, Yahoo, etc. In diesem Beitrag wird ein Ansatz vorgestellt, der auf der Grundlage der verfügbaren Informationen in WordNet die Ergebnisse von herkömmlichen Suchmaschinen verbessert. Es werden hierzu die konzeptuellen Relationen des lexikalischen-semantischen Netzes genutzt. Der beschriebene Suchmaschinen-aufsatz nutzt WordNet-Informationen um Nutzeranfragen zu spezifizieren und um die gefundenen Webseiten der herkömmlichen Suchmaschinen (Google, Yahoo etc.) zu klassifizieren und zu gruppieren.	semantic network;unified model;web search engine;wordnet	Peter M. Kruse;Andre Naujoks;Dietmar F. Rösner;Manuela Kunze	2005	CoRR		search engine indexing;lexical semantics;database search engine;metasearch engine;web search engine;semantic search;search engine optimization;computer science;spamdexing;phrase search;concept search;database;search analytics;world wide web;information retrieval;search engine	AI	-33.870537584833784	6.149104037363703	159638
10a0373219eefbf36506fa0b067193ec1db547cd	standard protocol for exchange of health-checkups based on sgml: the healthcheckups data markup language (hdml)			handheld device markup language;standard generalized markup language	Hiroki Sugimori;Shoichiro Hara;Katsumi Yoshida;Katsuhiko Furumi;Ikuo Tofukuji;Takeshi Kubodera;Takashi Yoda;Kawai Masaki	2001		10.3233/978-1-60750-928-8-708	programming language;markup language;pcdata;database;sgml;computer science	DB	-33.883291957810584	7.837685399224845	159871
4e6720477306522fcbe7338e2ff3c99771c67217	tailor-made native xml storage structures		Automatically choosing suitable native storage structures for XML documents arriving at the XML DBMS is a challenging task for its storage manager. While some of the critical parameters require pre-specification, others can be determined by pre-analysis or sampling of the incoming document or by just making experience-driven “educated guesses”. In this paper, we discuss approaches to achieve an adaptive behavior of the storage manager to provide tailor-made native XML storage structures to the extent possible.	adaptive behavior;sampling (signal processing);xml	Karsten Schmidt;Theo Härder	2007			well-formed document;database;data mining;xml framework;xml encryption;xml schema;xml validation;xml database;computer science;simple api for xml;efficient xml interchange	DB	-33.71661695973883	5.639828842956276	160331
eb2cdfac47b56a28239db08f5500c8d5fb17ba25	the semantic web: trends and challenges	semantic web	The Semantic Web changes the way we deal with data, because assumptions about the nature of the data that we deal with differ substantially from the ones in established database approaches. Semantic Web data is (i) provided by different people in an ad-hoc manner, (ii) distributed, (iii) semi-structured, (iv) (more or less) typed, (v) supposed to be used serendipitously. In fact, these are highly relevant assumptions and challenges, since they are frequently encountered in all kind of data-centric challenges also in cases where Semantic Web standards are not in use. However, they are only partially accounted for in existing programming approaches for Semantic Web data including (i) semantic search, (ii) graph programming, and (iii) traditional database programming approaches. The main hypothesis of this talk is that we have not yet developed the right kind of programming paradigms to deal with the proper nature of Semantic Web data, because none of the mentioned approaches fully considers its characteristics. Thus, we want to outline empirical investigations of Semantic Web data and recent developments towards Semantic Web programming that target the reduction of the impedance mismatches between data engineering and programming ap-	characteristic impedance;database;emoticon;hoc (programming language);impedance matching;information engineering;programming paradigm;semantic web;semantic search;semiconductor industry;web standards	Valentina Presutti;Claudia d’Amato;Fabien L. Gandon;Mathieu d’Aquin;Steffen Staab;Anna Tordai;Gerhard Goos;Juris Hartmanis;Jan van Leeuwen;David Hutchison	2014		10.1007/978-3-319-07443-6	semantic computing;web modeling;data web;semantic search;semantic grid;web standards;computer science;artificial intelligence;semantic web;web navigation;social semantic web;semantic web stack;web intelligence;semantic technology;world wide web;owl-s;website parse template;information retrieval;semantic analytics	Web+IR	-38.911616928316896	6.1412339941200065	160759
db83594793cffea8b6f77302e23c2e52ee899a98	intelligent multimedia layout: a reference architecture for the constraint-based spatial layout of multimedia presentations	automated layout;constraint processing techniques;informal communication;intelligent multimedia presentation systems;multimedia presentation;reference architecture;standardization	With increases in the complexity and dynamics of multimedia information communicated by current applications, there arises a corresponding need towards a standard technology for intelligent multimedia interfaces. In this article, we address those components of an intelligent multimedia presentation system (IMMPS) which deal with the design and the realization of spatial Layout. We treat multimedia layout as a multidimensional constraint problem and propose a reference architecture for a general-purpose intelligent multimedia layout manager (IMMLM) that is based on a dedicated constraint solver kernel.	general-purpose markup language;layout manager;reference architecture;solver	Winfried Graf	1997	Computer Standards & Interfaces	10.1016/S0920-5489(97)00016-0	reference architecture;real-time computing;computer science;theoretical computer science;database;multimedia;standardization	DB	-35.83897929173408	10.051762269963538	161320
c33a33f40ffa918fab77388603277497eed60fe4	html to the max: a manifesto for adding sgml intelligence to the world-wide web	langage description donnee;navegacion informacion;information loss;information science;data description language;navigation information;information browsing;document reproduction;lenguaje descripcion dato;sgml;metalangage;html;internet;metalanguage;sgml on the world wide web;reproduccion documento;world wide web;style sheets;reproduction document;science information;markup language;document type definition;www;metalenguaje	HTML demonstrates that SGML markup is useful for networked information. How can it be made even more useful ? One way is to extend the tag set from HTML to HTML2, etc. We argue here for a more radical approach : full SGML awareness in WWW. We believe the difficulties are small ; the cost affordable, and the advantages overwhelming. SGML is a metalanguage for defining markup languages ; HTML is just one instance of this infinite family. At present, documents in other SGML document types must be translated into HTML for display by a Mosaic client-sometimes this imposes unacceptable information loss. WWW browsers could handle other SGML document types without translation by launching a general-purpose SGML browser to view them, as they now launch graphics viewers ; a better solution overall would be to build SGML display into the WWW browsers themselves. Either way, display of an SGML document would be controlled by a style sheet using a small number of display primitives (bold, line break, etc.) to specify the rendition of each element type. For well-known document type definitions (DTDs) like HTML, style sheets could be distributed with the browser, or built in. For other DTDs, the browser would fetch a style sheet from the server. Using style sheets, browser software can also make it easy to customize document display. DTDs and style sheets can be designed to accommodate extensions, ensuring that authors can make small extensions to the tag set with no change whatsoever in the target browsers and virtually no performance penalty.	html;standard generalized markup language;world wide web	C. M. Sperberg-McQueen;Robert F. Goldstein	1995	Computer Networks and ISDN Systems	10.1016/0169-7552(95)00100-0	numeric character reference;the internet;processing instruction;html;information science;metalanguage;computer science;document type definition;document type declaration;database;multimedia;markup language;style sheet;programming language;world wide web;sgml;sgml entity	ECom	-36.098999194394764	10.873986522950059	161509
e2b6ea611749a2b833c77e765322abd91db418d0	data quality in web information systems	data uncertainty;data provenance;poor data quality;web data;data quality problem;web information systems;large-scale web information systems;use data;data quality;world wide web;database system;closed world assumption;management information system;web based applications	Evaluation of data quality in web information systems provides support for a correct interpretation of the contents of web pages. Data quality dimensions proposed in the literature need to be adapted and extended to represent the characteristics of data in web pages, and in particular their dynamic aspects. The present paper proposes and discusses a model and a methodological framework to support data quality in web information systems.	data quality;information system;web page	Barbara Pernici;Monica Scannapieco	2002	J. Data Semantics	10.1007/978-3-540-39733-5_3	web service;web application security;web mining;static web page;web development;web modeling;the internet;data web;web analytics;web mapping;data quality;web design;web standards;computer science;semantic web;web navigation;web page;data mining;semantic web stack;web intelligence;web 2.0;world wide web;information retrieval;information system;web server	DB	-38.69247617899402	5.388305773717671	162177
5d050e22e7bace62f1d13deceedf932af47e028a	matching of ontologies with xml schemas using a generic metamodel	semantic annotation;xml schema;modeling language;schema matching;model management;matching model;meta model	Schema matching is the task of automatically computing correspondences between schema elements. A multitude of schema matching approaches exists for various scenarios using syntactic, semantic, or instance information. The schema matching problem is aggravated by the fact that models to be matched are often represented in different modeling languages, e.g. OWL, XML Schema, or SQL DDL. Consequently, besides being able to match models in the same metamodel, a schema matching tool must be able to compute reasonable results when matching models in heterogeneous modeling languages. Therefore, we developed a matching component as a part of our model management system GeRoMeSuite which is based on our generic metamodel GeRoMe. As GeRoMe provides a unified representation of models, the matcher is able to match models represented in different languages with each other. In this paper, we will show in particular the results for matching XML Schemas with OWL ontologies as it is often required for the semantic annotation of existing XML data sources. GeRoMeSuite allows for flexible configuration of the matching system; various matching algorithms for element and structure level matching are provided and can be combined freely using different ways of aggregation and filtering in order to define new matching strategies. This makes the matcher highly configurable and extensible. We evaluated our system with several pairs of XML Schemas and OWL ontologies and compared the performance with results from other systems. The results are considerably better which shows that a matching system based on a generic metamodel is favorable for heterogeneous matching	algorithm;extensibility;graph (abstract data type);holism;metamodeling;modeling language;ontology (information science);ontology alignment;prototype;sql;sorting;test case;tree traversal;unification (computer science);usability;web ontology language;xml database;xml namespace;xml schema	Christoph Quix;David Kensche;Zhenni Li	2007		10.1007/978-3-540-76848-7_71	xml validation;computer science;document structure description;data mining;xml schema;database;document schema definition languages;programming language;xml schema editor	Web+IR	-36.07442155118652	6.733677273994349	162361
6ae59716865bc0fe1ee03aea3eb95e9271f8e96b	web database applications with php and mysql - building effective database-driven web sites: covers pear, php 5, and mysql 4.1 (2. ed.)	web databases		mysql;pear;php	Hugh E. Williams;David Lane	2004			computer science;data mining;database;web application development;world wide web	DB	-33.71397578406222	7.516701472301591	162528
07f151ac9e38d6a4bf977f50bfcd588a09a315ff	memento: time travel for the web	software agent;proof of concept;content management system	The Web is ephemeral. Many resources have representations that change over time, and many of those representations are lost forever. A lucky few manage to reappear as archived resources that carry their own URIs. For example, some content management systems maintain version pages that reflect a frozen prior state of their changing resources. Archives recurrently crawl the web to obtain the actual representation of resources, and subsequently make those available via special-purpose archived resources. In both cases, the archival copies have URIs that are protocolwise disconnected from the URI of the resource of which they represent a prior state. Indeed, the lack of temporal capabilities in the most common Web protocol, HTTP, prevents getting to an archived resource on the basis of the URI of its original. This turns accessing archived resources into a significant discovery challenge for both human and software agents, which typically involves following a multitude of links from the original to the archival resource, or of searching archives for the original URI. This paper proposes the protocol-based Memento solution to address this problem, and describes a proof-of-concept experiment that includes major servers of archival content, including Wikipedia and the Internet Archive. The Memento solution is based on existing HTTP capabilities applied in a novel way to add the temporal dimension. The result is a framework in which archived resources can seamlessly be reached via the URI of their original: protocol-based time travel for the Web.	archive;content management system;hypertext transfer protocol;memento pattern;software agent;surface web;uniform resource identifier;wikipedia;world wide web	Herbert Van de Sompel;Michael L. Nelson;Robert Sanderson;Lyudmila Balakireva;Scott Ainsworth;Harihar Shankar	2009	CoRR		simulation;computer science;software agent;data mining;proof of concept;world wide web;information retrieval	AI	-40.33350440963809	9.19824342964173	162603
2f43abe059978640adea641679dd0fe168d1b30e	schema.org: evolution of structured data on the web		Big data makes common schemas even more necessary.	big data;data model;data science;entity;intelligent agent;schema.org;vocabulary;web search engine;world wide web	Ramanathan V. Guha;Dan Brickley;Steve Macbeth	2016	Commun. ACM	10.1145/2844544	data web	DB	-39.612376569798734	6.359829292580817	162951
673136902481bdcda586cab984e9e8f1d03df69a	semantic web research anno 2006: main streams, popular fallacies, current status and future challenges	reseau social;multiagent system;cooperation;web semantique;intelligence artificielle;cooperacion;semantic web technology;social network;web semantica;semantic web;artificial intelligence;inteligencia artificial;sistema multiagente;red social;systeme multiagent	In this topical paper we try to give an analysis and overview of the current state of Semantic Web research. We point to different interpretations of the Semantic Web as the reason underlying many controversies, we list (and debunk) four false objections which are often raised against the Semantic Web effort. We discuss the current status of the Semantic Web work by reviewing the current answers to four central research questions that need to be answered, and by surveying the uptake of Semantic Web technology in different application areas. Finally, we try to identify the main challenges facing the Semantic Web community. 1 Which Semantic Web? It has already been pointed out by Marshall and Shipman in [1] that the term “Semantic Web” is used to describe a variety of different goals and methods. They distinguish (1) the Semantic Web as a universal library for human access; (2) as the habitat for automated agents and web-services ; and (3) as a method for federating a variety of databases and knowledge bases. And although we in no way share their rather pessimistic analysis of the possibilities for each of these three scenario’s (founded as they are on rather strawman versions of each of them), we do agree that it is important to unravel the different ambitions that underly the “Semantic Web” term. In the current Semantic Web work, we distinguish two main goals. These goals are often unspoken, but the differences between them often account for many debates on design choices, on the applicability of various techniques, and on the feasibility of applications. 1 in the sense of: “of current interest”, “concerning contemporary topics of limited validity” 2 although Marshall and Shipman do not actually use the term web-services Interpretation 1: The Semantic Web as the Web of Data In the first interpretation (close to Marshall and Shipman’s third option), the main aim of the Semantic Web is to enable the integration of structured and semi-structured data-sources over the Web. The main recipe is to expose datasets on the web in RDF format, to use RDF Schema to express the intended semantics of these data-sets, in order to enable the integration and unexpected re-use of these data-sets. A typical use-cases for this version of the Semantic Web is the combination of geo-data with a set of consumer ratings for restaurants in order to provide an enriched information source. Interpretation 2: The Semantic Web as an enrichment of the current Web In the second interpretation, the aim of the Semantic Web is to improve the current World Wide Web. Typical use-cases here are improved search engines, dynamic personalisation of web-sites, and semantic enrichment of existing webpages. The source of the required semantic meta-data in this version of the Semantic Web is mostly claimed to come from automatic sources: concept-extraction, named-entity recognition, automatic classification, etc. More recently, the insight is gaining ground that the required semantic markup can also be produced by social mechanisms of in communities that provide large-scale human-produced markup. Of course there are overlaps between these two versions of the Semantic Web: they both rely on the use of semantic markup, typically in the form of metadata described by ontology-like schemata. But perhaps more noticeable are the significant differences: different goals, different sources of semantics, different use-cases, different technologies. 2 Four popular fallacies The Semantic Web is subject to a stream of strongly and often polemically voiced criticisms . Unfortunately, not all of these are equally well informed. A closer analysis reveals that many of these polemics attribute a number of false assumptions or claims to the Semantic Web programme. In this section we aim to identify and debunk these fallacies. Fallacy 1: The Semantic Web tries to enforce meaning from the top This fallacy claims that the Semantic Web, enforces meaning on users through its standards OWL and RDF(S). The repost to this fallacy is easy. The only 3 e.g. http://www.shirky.com/writings/semantic syllogism.html and http://www.csdl.tamu.edu/∼marshall/mc-semantic-web.html meaning that OWL and RDF(S) enforce is the meaning of the connectives in a language that users can use to express their own meaning. The users are free to to choose their own vocabulary, and to assign their own meaning to terms in this vocabulary, to describe whatever domain of their choice. OWL and RDF(S) are entirely neutral in this. The situation is comparable to HTML: HTML does not enforce the lay-out of web-pages “from the top”. All HTML enforces is the language that people can use to describe their own lay-out. And HTML has shown that such an agreement on the use of a standardised language (be it HTML for the lay-out of web-pages, or RDF(S) and OWL for their meaning) is a necessary ingredient for world-wide interoperability. Fallacy 2: The Semantic Web requires everybody to subscribe to a single predefined meaning for the terms they use. Of course, the meaning of terms cannot be predefined for global use. Of course, meaning is fluid and contextual. The motto of the Semantic Web is not the enforcement of a single ontology. It’s motto is rather “let a thousand ontologies blossom”. That is exactly the reason why the construction of mappings between ontologies is such a core topic in the Semantic Web community (see [2,3,4] for some surveys). And such mappings are expected to be partial, imperfect and context-dependent. Fallacy 3: The Semantic Web will require users to understand the complicated details of formalised knowledge representation. Indeed some of the core technology of the Semantic Web relies on intricate details of formalised knowledge representation. The semantics of RDF Schema and OWL, and the layering of the subspecies of OWL are difficult formal matters. The design of good ontologies is a specialised are of Knowledge Engineering. But for most of the users of (current and future) Semantic Web applications, such details will be entirely “under the hood”, just as the intricacies of CSS and (X)HTML are under the hood of the current Web. Navigation or personalisation engines can be powered by underlying ontologies, expressed in RDF Schema or OWL, without the user ever being confronted with the ontology, let alone its representation language. Fallacy 4: The Semantic Web people will require the manual markup of all existing web-pages It’s hard enough for most web-site owners to maintain the human-readable content of their site. They will certainly not maintain a second parallel version in which they will have to write a machine-accessible version of the same information in RDF or OWL. If this were the case, that would indeed spell bad news for the Semantic Web. Instead, Semantic Web applications rely on large-scale automation for the extraction of such semantic markup from the sources themselves. This will often be very lightweight semantics, but for many applications, that has shown to be enough. Notice that this fallacy mostly affects interpretation 2 of the Semantic Web (previous section), since massive markup in the “Web of data” is much easier: the data is already available in (semi-)structured formats, and is often already organised by database schema’s that can provide the required semantic interpretation.	anno 1602;cascading style sheets;context-sensitive language;database schema;gene ontology term enrichment;hood method;human-readable medium;information source;interoperability;knowledge engineering;knowledge representation and reasoning;logical connective;markup language;marshalling (computer science);named-entity recognition;ontology (information science);personalization;rdf schema;resource description framework;semantic html;semantic web;semantic interpretation;semi-structured data;semiconductor industry;vocabulary;web ontology language;web application;web search engine;web service;world wide web;xhtml	Frank van Harmelen	2006		10.1007/11839354_1	semantic computing;semantic search;semantic grid;web standards;computer science;knowledge management;artificial intelligence;semantic web;social semantic web;data mining;semantic web stack;database;web intelligence;world wide web;owl-s;computer security;cooperation;semantic analytics;social network	Web+IR	-40.670491038651114	8.421845618240443	163462
c85971829f0bbf9f4471f4a1ab17ba5ce197a77f	preserving information content in rdf using bounded homomorphisms	preserving information content;bounded homomorphisms;particular instance;present paper;migrating data;rdf triple;information content;non-distortive manner;rdf homomorphisms;linked data;rdf graph;rdf vocabulary	The topic of study in the present paper is the class of RDF homomorphisms that substitute one predicate for another throughout a set of RDF triples, on the condition that the predicate in question is not also a subject or object. These maps turn out to be suitable for reasoning about similarities in information content between two or more RDF graphs. As such they are very useful e.g. for migrating data from one RDF vocabulary to another. In this paper we address a particular instance of this problem and try to provide an answer to the question of when we are licensed to say that data is being transformed, reused or merged in a non-distortive manner. We place this problem in the context of RDF and Linked Data, and study the problem in relation to SPARQL construct queries.	computable function;distortion;graph theory;interlock (engineering);linked data;map;modulo operation;paging;representational state transfer;resource description framework;sparql;self-information;simulation;time complexity;vocabulary;while	Audun Stolpe;Martin G. Skjæveland	2012		10.1007/978-3-642-30284-8_12	rdf/xml;sparql;linked data;data mining;database;rdf query language;blank node;information retrieval;rdf schema	ECom	-35.20777815146114	6.513012324759301	163491
546ca0aff2fb7c044e0bd3b727440cf91290be92	acquire: agent-based complex query and information retrieval engine	distributed active archive center;distributed data;agent based;optimization technique;information retrieval;mobile agents;data processing;data format;interface agent;ease of use;softbot;mobile agent;distributed data sources;data retrieval;database query;time constraint	The heterogeneous, distributive and voluminous nature of many government and corporate data sources impose severe constraints on meeting the diverse requirements of users who analyze the data. Additionally, communication bandwidth limitations, time constraints, and multiplicity of data formats impose further restrictions on users of these distributed data sources. What is required is a reliable, robust, and efficient data retrieval technique that can access data from distributed data sources while maintaining the autonomy of individual sources. In this paper, we present an Agent-based Complex QUerying and Information Retrieval Engine (ACQUIRE) for large, heterogeneous, and distributed data sources. ACQUIRE acts as a softbot or interface agent by presenting users with the appearance of a single, unified, homogenous data source, against which users can pose high-level declarative queries. ACQUIRE translates each such user query into a set of sub-queries by employing a combination of planning and traditional database query optimization techniques. For each sub-query, ACQUIRE then spawns a corresponding mobile agent, which retrieves data from the appropriate data source. These mobile agents carry with them data-processing code that can be executed at the remote site, thus reducing the size of data returned by the agent. When all mobile agents have returned, ACQUIRE filters and merges the retrieved data and presents the results to the user. Validation experiments on simulated NASA Distributed Active Archive Centers (DAACs) have demonstrated that complex queries can be effectively decomposed and retrieved by this approach, resulting in the twin benefits of improved ease of use and significantly reduced query retrieval times.	agent-based model;archive;autonomy;data retrieval;database;experiment;high- and low-level;information retrieval;intelligent user interface;mathematical optimization;mobile agent;query optimization;requirement;usability	Subrata Kumar Das;Kurt Shuster;Curt Wu	2002		10.1145/544862.544891	usability;data processing;computer science;data mining;mobile agent;database;world wide web;data retrieval	DB	-35.571208529072685	9.486146958529408	163518
53b81e61fa4e1698b8618e86a809a6422edaf484	design features for the social web: the architecture of deme	content management;application framework;social web;object oriented programming;object oriented;polymorphism;content management system;web technology;open source	We characterize the “social Web” and argue for several features that are desirable for users of socially oriented web applications. We describe the architecture of Deme, a web content management system (WCMS) and extensible framework, and show how it implements these desired features. We then compare Deme on our desiderata with other web technologies: traditional HTML, previous open source WCMSs (illustrated by Drupal), commercial Web 2.0 applications, and open-source, object-oriented web application frameworks. The analysis suggests that a WCMS can be well suited to building social websites if it makes more of the features of object-oriented programming, such as polymorphism, and class inheritance, available to nonprogrammers in an accessible vocabulary.	drupal;html;open-source software;vocabulary;web 2.0;web application;web content management system;web framework	Todd R. Davies;Mike D. Mintz	2009	CoRR		web service;web application security;web development;web modeling;data web;web mapping;web design;web standards;computer science;web navigation;social semantic web;web page;semantic web stack;database;multimedia;web intelligence;web engineering;object-oriented programming;web 2.0;world wide web;mashup	Web+IR	-40.7096388696249	10.956360439264717	163574
1d77573d61f2ee107f25331a3dbe410d3d744419	assessing and refining mappings to rdf to improve dataset quality	rdfunit;technology and engineering;linked data mapping;rml;r2rml;sparql;data quality	rdf dataset quality assessment is currently performed primarily after data is published. However, there is neither a systematic way to incorporate its results into the dataset nor the assessment to the publishing workflow. Adjustments are manually—but rarely—applied. Nevertheless, the root of the violations which often derive from the mappings that specify how the rdf dataset will be generated, is not identified. We suggest an incremental, iterative and uniform validation workflow for rdf datasets stemming originally from semi-structured data (e.g., csv, xml, json). In this work, we focus on assessing and improving their mappings. We incorporate i) a test-driven approach for assessing the mappings instead of the rdf dataset itself, as mappings reflect how the dataset will be formed when generated; and ii) perform semi-automatic mapping refinements based on the results of the quality assessment. The proposed workflow is applied to different cases, e.g., large, crowdsourced datasets as dbpedia, or newly generated, as iLastic. Our evaluation indicates the efficiency of our workflow, as it improves significantly the overall quality of an rdf dataset in the observed cases.	computation;computational complexity theory;crowdsourcing;dbpedia;data quality;data structure alignment;iterative method;json;linked data;semi-structured data;semiconductor industry;software propagation;software release life cycle;stemming;user interface;xml	Anastasia Dimou;Dimitris Kontokostas;Markus Freudenberg;Ruben Verborgh;Jens Lehmann;Erik Mannens;Sebastian Hellmann;Rik Van de Walle	2015		10.1007/978-3-319-25010-6_8	data quality;computer science;sparql;data mining;database;information retrieval;rdf schema	DB	-36.16848694149111	4.425660393815172	163773
5a21b7aba792cf24a73e2a0bcac0d532f9ccb581	towards distributed ecore models		Models are the cornerstone of Model-Driven Engineering (MDE). Their size is constantly growing, becoming one of the main problems when it comes to manipulating them, via model-to-model transformations, model-to-text transformations or simply parsing them. In this paper we propose a way of distributing Ecore models representing them as JSON and URLs as identifiers, since HTTP is one of the most successful distributed protocols ever created. An implementation of distributed Ecore models using a RESTful-like service is also presented and and is publicly available.	hypertext transfer protocol;identifier;json;model-driven engineering;model-driven integration;parsing	Jesús M. Perera Aracil;Diego Sevilla Ruiz	2016	2016 4th International Conference on Model-Driven Engineering and Software Development (MODELSWARD)	10.5220/0005685002090216	xml;computer science;theoretical computer science;parsing;database;data mining;json;identifier	SE	-39.852965917439015	10.629333337770412	164909
9108d1b08157cc8ac4149878e5546bda1c84e898	specification of a general linguistic annotation framework and its use in a real context	computacion informatica;filologias;integration architecture;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;arquitectura para la integracion;annotation model;grupo a;ciencias sociales;grupo b;modelo de anotacion;tei p4	In this paper we present AWA, a general architecture for representing the linguistic information produced by diverse linguistic processors. Our aim is to establish a coherent and flexible representation scheme that will be the basis for the exchange of information. We use TEI-P4 conformant feature structures as a representation schema for linguistic analyses. A consistent underlying data model, which captures the structure and relations contained in the information to be manipulated, has been identified and implemented by a set of classes following the object-oriented paradigm. As an example of the usefulness of the model, we will show the usage of the framework in a real context: two corpora have been annotated by means of an application which aim is to exploit and manipulate the data created by the linguistic processors developed so far.	browsing;central processing unit;coherence (physics);data model;graphical user interface;markup language;parsing;programming paradigm;relax ng;text encoding initiative;text corpus;tokenization (data security);word-sense disambiguation;xml	Xabier Artola;Arantza Díaz de Ilarraza;Aitor Sologaistoa;Aitor Soroa	2007	Procesamiento del Lenguaje Natural		computer science;artificial intelligence;algorithm	NLP	-36.622202467485614	7.666419894746707	164917
f16d51d2aae68de6bfb6897eaa94572f40b6bab8	database schema matching using machine learning with feature selection	attribute dictionary;database schema matching;schema matching;automatch project;corresponding database instance;feature selection;optimal matching;knowledge base;machine learning;schema integration;database application;semantically related database schema;new schema	Schema matching, the problem of finding mappings between the attributes of two semantically related database schemas, is an important aspect of many database applications such as schema integration, data warehousing, and electronic commerce. Unfortunately, schema matching remains largely a manual, labor-intensive process. Furthermore, the effort required is typically linear in the number of schemas to be matched; the next pair of schemas to match is not any easier than the previous pair. In this paper we describe a system, called Automatch, that uses machine learning techniques to automate schema matching. Based primarily on Bayesian learning, the system acquires probabilistic knowledge from examples that have been provided by domain experts. This knowledge is stored in a knowledge base called the attribute dictionary. When presented with a pair of new schemas that need to be matched (and their corresponding database instances), Automatch uses the attribute dictionary to find an optimal matching. We also report initial results from the Automatch project.	database schema;feature selection;machine learning	Jacob Berlin;Amihai Motro	2013		10.1007/978-3-642-36926-1_25	schema migration;knowledge base;semi-structured model;computer science;conceptual schema;artificial intelligence;machine learning;star schema;pattern recognition;data mining;database;feature selection;database schema	AI	-35.80510364023709	6.303201160918037	164994
2e543a5fdfbad79829075f315c027096d96a4a50	semantic matching across heterogeneous data sources	correspondance ontologie;data integrity;ontology mapping;integration information;semantic integration;semantics;semantica;semantique;information integration;integracion informacion;semantic matching;correspondencia ontologia;heterogeneous data sources	Discovering semantic correspondences is the key to semantic integration of data sources and ultimately to data integration across disparate databases.	database;semantic integration;semantic matching	Huimin Zhao	2007	Commun. ACM	10.1145/1188913.1188916	semantic similarity;semantic computing;semantic integration;computer science;data mining;database;semantics;semantic equivalence;ontology-based data integration;semantic technology;information retrieval;data mapping	DB	-38.124702672813896	6.5116668000344005	165383
416b1080bccc9e82b6c86f6e8ac5ef91b8829740	spreadsheet programming for collecting, exploring and publishing web data	data mining;data visualization;xml;uniform resource locators data visualization data mining xml joining processes;joining processes;data collection spreadsheet programming web data collection web data exploration web data publishing web services json data javascript object notation data data format hierarchical json data json documents data retrieval;uniform resource locators;web services document handling information retrieval java spreadsheet programs	The Internet has all kinds of data available. Many companies provide Web services that let people access their data programmatically. These data are used every day by thousands of businesses and individuals to create custom applications or to gain insights about specific topics1. The increasing use of Web services has led to the increasing amount of JSON (JavaScript Object Notation) data, the predominant data format used by current Web services. JSON data is hierarchical and can contain multiple levels of structure. Currently, a person needs to write code to extract and compute data from JSON documents. To retrieve data from Web services and to create custom applications that use the collected data require additional programming efforts.	internet;json;javascript;spreadsheet;web service	Kerry Shih-Ping Chang	2015	2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)	10.1109/VLHCC.2015.7357229	data exchange;web service;ajax;web mining;semi-structured data;web modeling;data web;content security policy;web standards;computer science;web api;ws-policy;web page;data mining;database;web intelligence;web 2.0;world wide web;json-ld;data mapping	Visualization	-39.147356164422234	10.087641935512812	165547
e462606065ccedfbfab33419863ac859554cbafd	process based metadata for semantic desktop search		An off-highway vehicle includes a body having an engine compartment in which is positioned an internal combustion engine. The grill for the engine compartment is hinged to the body to permit ready access to the engine compartment. The radiator system for the engine includes a plurality of similar cooling modules which are interconnected together and to the engine to circulate coolant with respect to the engine. The number of cooling modules may be readily varied to vary the cooling capacity of the radiator system.	semantic desktop	Florian Gerecke	2008			radiator (engine cooling);semantic computing;metadata;semantic grid;semantic search;information retrieval;internal combustion engine;metadata repository;database;semantic desktop;computer science	NLP	-40.493893190245004	6.563806560942494	165691
795ab007a44fe546e4e67655e39e448f51729e8f	semantic discovery of the user intended query in a selectable target query language	description logics reasoners;query language;filtering;generators;marine animals;query processing;search engines;ontologies artificial intelligence;web search engine;query languages;semantic web ontologies artificial intelligence query languages query processing search engines;semantic web;ontologies;description logic;description logics reasoners semantic web ontology based query generation;rendering computer graphics;ontology based query generation;database languages;database languages ontologies logic intelligent agent web search search engines vehicles semantic web filters filtering;ontology based query generation semantic discovery user intended query selectable target query language web search engines formal queries description logics user keywords semantic web	The syntactic approach of most of web search engines still has the drawback of not considering the semantics of the keywords entered by the user. So, users usually have to browse many hits looking for the information they want. In this paper, we present a system that, given a set of keywords with well defined semantics, automatically generates a set of formal queries, in the query language of the user's choice, which attempt to capture what the user had in mind when she or he wrote those keywords. The system uses ontologies and a Description Logics reasoner to perform a semantic enrichment of user keywords to improve the discovering of possible user queries and to reject semantically inconsistent queries.	browsing;description logic;gene ontology term enrichment;ontology (information science);query language;semantic reasoner;web search engine	Carlos Bobed;Raquel Trillo Lado;Eduardo Mena;Jordi Bernad	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.362	web search engine;computer science;artificial intelligence;database;world wide web;information retrieval;query language	DB	-35.28250499447719	5.85817414169172	166744
0f309648223747d613d23f67c6023a31cba5ad0e	a vocabulary for the modelling of image search microservices		In order to take advantage of the services that are available on the Web, several approaches that allow describing services have been proposed. With them, developers can publish service descriptions, allowing services to be automatically executed and composed. However, in most cases, the service description task is not carried out, partly because it is a time-consuming task. This has caused initiatives such as WSMO lite, SA-REST, hRESTS or Microservices, that try to reduce complexity in services, to appear. Also, an increasing number of web applications have followed the Linked Data initiative and publish information that is machine processable thanks to Semantic Web technologies such as RDF. However, sometimes direct access to information requires the usage of search forms and, in other cases, spidering techniques such as focused crawling in order to aggregate and filter data. Automatic execution of search services would improve access to information in the web by enabling agents to automatically aggregate, filter and directly access	aggregate data;focused crawler;freedom of information laws by country;linked data;microservices;random access;resource description framework;semantic web;vocabulary;wsmo;web application;web crawler;world wide web	José Ignacio Fernández-Villamor;Carlos Angel Iglesias;Mercedes Garijo	2010			natural language processing;speech recognition	Web+IR	-40.98174466499579	8.247659926358626	167375
08381927558761d2c241c731a998eef3495804e5	semantics-preserving mappings between xml schemas in p2p data integration systems	xml schema;p2p data integration system;semantics-preserving mapping	We discuss the problem of deciding whether there exists a semantics-preserving mapping between two XML schemas or parts of schemas specified by tree patterns. To define semantics of XML data we interpret its schema in an OWL ontology through a semantic annotation (see eg. [1]) and use this annotation to derive complex XML-to-OWL dependencies. There are two steps in our algorithm: (1) schema matching - identification of such maximal parts π1 and π2 of XML schemas, respectively S1 and S2, that π1 is semantically subsumed by π2 (i.e. π1 (sqsubseteq) π2); (2) schema mapping - generating a specification describing how data structured according to π1 are to be transformed to data satisfying π2. The existence of matching between subschemas π1 and π2 is a necessary condition for the existence of semantics-preserving mapping between them.	peer-to-peer	Tadeusz Pankowski	2011		10.1007/978-3-642-25126-9_7	xml validation;relax ng;xml schema;geography markup language;computer science;document structure description;data mining;xml schema;database;xml schema editor;information retrieval	DB	-35.732396312862726	7.366777839572126	167737
d4227b505366d5b0e5074b5ea1d622641722a450	querying and reasoning with rdf(s)/owl in xquery	semantic web	In this paper we investigate how to use the XQuery language for querying and reasoning with RDF(S)/OWL-style ontologies. Our proposal allows the handling of RDF(S)/OWL triples by means of a XQuery library for the Semantic Web, and it encodes RDF(S)/OWL reasoning by means of XQuery functions. We have tested and implemented the approach.	algorithm;conjunctive query;description logic;existential quantification;formal system;international joint conference on automated reasoning;lecture notes in computer science;owl-s;ontology (information science);rewriting;rule interchange format;semantic web;semantic web rule language;semantic reasoner;sinclair ql;springer (tank);web ontology language;world wide web;xquery	Jesús Manuel Almendros-Jiménez	2011		10.1007/978-3-642-20291-9_51	rdf/xml;cwm;semantic web rule language;computer science;sparql;semantic web;database;world wide web;information retrieval;rdf schema	AI	-37.796819930759625	6.92573124461472	169027
bc76592b1a81658994c16a8cb35a0edfa07cd52b	distributed reasoning based on problem solver markup language (psml): a demonstration through extended owl	hypermedia markup languages;hierarchical structure;horn clauses;inference mechanisms;ontologies artificial intelligence;expressive power;internet;web ontology language;world wide web;hypermedia markup languages internet ontologies artificial intelligence inference mechanisms problem solving horn clauses;markup languages owl portals semantic web computer science web sites engines web services problem solving logic;distributed web environment distributed reasoning problem solver markup language web ontology language owl world wide web problem solving systems distributed web inference engines multiargument relation horn clauses hierarchical structure;markup language;problem solving	Since the World Wide Web is enlarging its scale, users cannot find and utilize information easily. Hence problem-solving systems in the Web environment are required. The core of such systems is the problem solver markup language (PSML) and PSML-based distributed Web inference engines. In this paper, we demonstrate a possible implementation of certain distributed reasoning capabilities as required in the future PSML. In particular, our proposed implementation, called /spl beta/-PSML, is based on the combination of OWL (Web ontology language) with Horn clauses. From the viewpoint of expressive power, the proposed /spl beta/-PSML can represent multi-argument relation that is an extension of the OWL capability, and models domains with a rich hierarchical structure for Horn clauses. Furthermore, we discuss how to extend the /spl beta/-PSML for solving problems in a large-scale distributed Web environment.	expressive power (computer science);f-logic;horn clause;inference engine;markup language;ontology (information science);portals;problem solving;prolog;reasoning system;semantic web;solver;web ontology language;web intelligence;web service;world wide web	Yila Su;Lei Zheng;Ning Zhong;Chunnian Liu;Jiming Liu	2005	2005 IEEE International Conference on e-Technology, e-Commerce and e-Service	10.1109/EEE.2005.59	ruleml;natural language processing;xhtml;web modeling;semantic web rule language;data web;html;html5;collaborative application markup language;web standards;computer science;semantic web;social semantic web;semantic web stack;database;web intelligence;programming language;owl-s	AI	-38.5894457256804	8.068073213959796	169401
27e9ec8709fd08ecba433ea593cc4959c4fce990	supporting adaptable granularity of changes for massive-scale collaborative editing	internet;groupware	Since the Web 2.0 era, the Internet is a huge content editing place in which users contribute to the content they browse. Users do not just edit the content but they collaborate on this content. Such shared content can be edited by thousands of people. However, current consistency maintenance algorithms seem not to be adapted to massive collaborative updating. Shared data is usually fragmented into smaller atomic elements that can only be added or removed. Coarse-grained data leads to the possibility of conflicting updates while fine-grained data requires more metadata. In this paper we offer a solution for handling an adaptable granularity for shared data that overcomes the limitations of fixed-grained data approaches. Our approach defines data at a coarse granularity when it is created and refines its granularity only for facing possible conflicting updates on this data. We exhibit three implementations of our algorithm and compare their performances with other algorithms in various scenarios.	algorithm;browsing;concurrency (computer science);conflict-free replicated data type;cut, copy, and paste;internet;linearizability;optimistic replication;overhead (computing);performance;point of view (computer hardware company);simulation;substring;text editor;unique identifier;web 2.0;world wide web	Luc André;Stéphane Martin;Gérald Oster;Claudia-Lavinia Ignat	2013	9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing		the internet;computer science;operating system;database;distributed computing;law;world wide web;collaborative software	HPC	-40.740961325315595	10.23508670309989	170163
21fedec573b64807f0efc2305a72f99598c4b11c	a novel three-level architecture for large data warehouses	sistema 3 niveles;semi automatic and semantic methodologies;methode semi automatique;architecture systeme;metadata;semiautomatic method;three level system;interscheme property derivation;cryptanalyse;metodo semi automatico;stockage donnee;sistema n niveles;data warehouse design;almacen dato;cryptanalysis;criptoanalisis;data storage;systeme n niveaux;scheme abstraction;multilevel system;metadonnee;almacenamiento datos;arquitectura sistema;metadatos;entrepot donnee;data warehouses;data warehouse;system architecture;scheme integration;systeme 3 niveaux;large data	Classical architectures proposed so far for data warehouses show some drawbacks when adopted to work over large numbers of heterogeneous operational sources. In this paper we propose a variant of a three-level architecture for data warehouses that overcomes these drawbacks. However, in the application context under consideration, having a suitable architecture may be not enough for the design purposes. Indeed, data warehouse design in very large operational environments can be a quite hard problem to attack with traditional manual methodologies. In this paper, automatic techniques are also illustrated that are capable to produce the data warehouse design according to the proposed architecture, with a limited human intervention.		Luigi Palopoli;Luigi Pontieri;Giorgio Terracina;Domenico Ursino	2002	Journal of Systems Architecture	10.1016/S1383-7621(02)00056-5	cryptanalysis;computer science;data warehouse;computer data storage;data mining;database;metadata;algorithm;data architecture	Arch	-34.716885001611296	10.902598449298129	170468
f754c8e6856ec383214b1235e0b927d831ec8664	an object-oriented framework for reconciliation and extraction in heterogeneous data federations	data reconciliation;integrable system;heterogeneous data;object oriented framework;data model;semantic heterogeneity;data extraction;object oriented;oriente objet;information system;orientado objeto;systeme information;sistema informacion	Two major problems in constructing database federations concern achieving and maintaining consistency and a uniform representation of the data on the global level of the federation. The process of creation of uniform representations of data is known as data extraction, whereas data reconciliation is concerned with resolving data inconsistencies. Our approach to constructing a global, integrated system from a collection of (semantically) heterogeneous component databases is based on the concept of exact view. We will show that a global database constructed by exact views integrates component schemas without loss of constraint information. We shall describe a semantic framework for specification of database federations based on the UML/OCL data model. In particular, we will show that we can represent exact views by so-called derived classes in UML/OCL, providing the means to resolve in a combined setting data extraction and reconciliation problems on the global level of the federation.		Herman Balsters;Bert O. de Brock	2004		10.1007/978-3-540-30198-1_5	data modeling;integrable system;data model;computer science;theoretical computer science;data mining;database;programming language;object-oriented programming;information system	NLP	-33.90280334419127	11.18419101355779	170666
09bb5b0ac930a5aca1383cce4cb66e6c96b942fd	temporal query answering in a fuzzy world		Ontologies play a central role in semantic applications: by providing semantics to the given data, they support the integration and automated processing of knowledge. Systems for ontology-based data access do however not take into account both the fuzzy and the temporal nature of the knowledge, which is often inherent in realworld data. In this paper, we propose an approach for temporal query answering over fuzzy data w.r.t. ontologies.	data access;fuzzy logic;ontology (information science);semantic web	Veronika Thost;Erik Zenker	2015			information retrieval;query expansion;fuzzy logic;web search query;query language;ontology (information science);semantics;data access;query optimization;computer science	AI	-37.74464904561195	6.137903903621634	171196
4535dde45912b736014b0c45a44e94849b010936	constructing geographic digital libraries using a hypermedia framework	geographic data;metadata management;digital library;digital libraries;storage management;data model;object oriented database management system;hypermedia applications;design methodologies;object oriented databases;object oriented database;design methodology	This paper presents an object oriented hypermedia database framework for designing and building digital libraries, which are treated as enhanced hypermedia applications. It is based on combining and extending results from two domains: the navigation characteristics of hypertext systems, and the view mechanism and the persistent storage management facility of an object oriented database management system. As a result, users can alternate between two integrated types of interaction modes. The hypertext dimension of the framework allows navigation via static and dynamic hyperlinks; the object oriented database support enables querying by content and metadata management. The framework includes still a digital library design methodology that guides the implementation of digital libraries over the OODBMS. This proposal integrates hypermedia and DL concepts to a database environment, being instantiated on the realm of geographic data.	data model (gis);database;digital library;hyperlink;hypermedia;hypertext;library (computing);microsoft windows;oohdm;parameter (computer programming);persistence (computer science);view (sql)	Marcos André Gonçalves;Claudia Bauzer Medeiros	1999	Multimedia Tools and Applications	10.1023/A:1009622326327	digital library;intelligent database;design methods;data model;computer science;data mining;database;world wide web;database design	DB	-36.18981324016263	9.890249870502739	171915
5b610d23e488a3e80a57a58a44cb0fa5312e9fa6	an ontology-driven process for unification of xml instances	data integrity;semantic integration;data representation;xml;xml document;domain ontology;ontology;data integration	XML documents have been widely used for data representation and data interchange, specially on the Web. However, the great number of XML data sources on the Web drawn from the same domain with heterogeneous representations becomes very complex the semantic integration of XML instances for querying purposes, for example. This paper presents an automatic process for the unification of XML documents representing data instances on a same domain. It receives a set of XML instances as input and generates a unique representation for them. Different from related work, the process considers a domain ontology as a basis for nomenclature and structure uniformization of the resulting XML instance. The process core is a set of unification operators that are invoked to provide the necessary adjustments in the instances in order to fit well with the domain ontology schema.	data (computing);ontology (information science);semantic integration;unification (computer science);world wide web;xml database	Carlos Alberto Souza;Ronaldo dos Santos Mello	2008		10.1145/1666091.1666131	well-formed document;xml catalog;xml validation;binary xml;xml encryption;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;ontology;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-35.930415589403154	7.743038933363382	172145
e0ba9434318adc59de388a19ba8aa415baffd58f	social semantic rule sharing and querying in wellness communities	time of day;seasonality;knowledge base	In this paper we describe the Web 3.0 case study WellnessRules, where ontology-structured rules (including facts) about wellness opportunities are created by participants in rule languages such as Prolog and N3, and translated for interchange within a wellness community using RuleML/XML. The wellness rules are centered around participants, as profiles, encoding knowledge about their activities, nutrition, etc. conditional on the season, the time-of-day, the weather, etc. This distributed knowledge base extends fact-only FOAF profiles with a vocabulary and rules about wellness group networking. The communication between participants is organized through Rule Responder, permitting translator-based reuse of wellness profiles and their distributed querying across engines. WellnessRules interoperates between rules and queries in the relational (Datalog) paradigm of the pure-Prolog subset of POSL and in the frame (F-logic) paradigm of N3. These derivation rule languages are implemented in the engines OO jDREW and Euler, and connected via Rule Responder to support wellness communities. An evaluation of Rule Responder instantiated for WellnessRules found acceptable Web response times.		Harold Boley;Taylor Michael Osmun;Benjamin Larry Craig	2009		10.1007/978-3-642-10871-6_24	knowledge base;computer science;artificial intelligence;data mining;database;distributed computing;world wide web;seasonality	AI	-37.085104662225504	9.058103071460698	172414
5fb41c1a112087b33673b5ce9d1c78dc8a6fef9f	uncertainty reasoning for linked data	linked data;uncertainty reasoning;linked open data;semantic web	Linked open data offers a set of design patterns and conventi ons for sharing data across the semantic web. In this position paper we e numerate some key uncertainty representation issues which apply to linked da ta and suggest approaches to tackling them. We suggest the need for vocabula ries to enable representation of link certainty, to handle ambiguous or imprec ise values and to express sets of assumptions based on named graph combinators.	combinatory logic;design pattern;linked data;named graph;semantic web;xilinx ise	Dave Reynolds	2009			computer science;linked data;data mining;database;information retrieval	AI	-37.86400174376059	6.330858808254818	172730
817f0ab2366ec865711413ba15212ece5ee19672	conversion of ebook documents based on mapping relations	livre electronique;document structure;documento electronico;data conversion;estructura de documento;standards;metadata;structure document;conversion donnee;document electronique;semantic information;electronic book;conversion datos;relacion;norma;metadonnee;libro electronico;metadatos;relation;norme;electronic document	An electronic book means a digital form of a paper book. Currently, to promote an eBook market, many countries have established eBook content standards. But, the publication of different caused exchanging problems due to mismatch of content forms. Therefore, to exchange eBook conforming each standard, the content has to be converted according to document structure and its semantic information. But existing conversion methods are almost based on syntax information. Even, using semantic information they are not reflected eBook characteristics. So, to precise and correct eBook conversion, we analyze each standard and define mapping relations considering semantic information and eBook characteristics. To generalize the mapping relations, we classify mapping relations into ten conversion classes, and provide conversion scripts examples for each class. With defined mapping relations and conversion classes, we write up conversion scripts for EBKS to OEB PS/JepaX, and experiment with them. We believe defined conversion classes can be applied to normal document conversions.		Seung-Kyu Ko;Myoung-Soo Kang;Won-Sung Sohn;Soon-Bum Lim;Yoon-Chul Choy	2002		10.1007/3-540-45747-X_3	data conversion;computer science;relation;document structure description;data mining;database;metadata;world wide web	NLP	-35.8695901749096	7.63543706105963	173734
176197ea89dfb8a33959d7b03aeb61bbb2a5c9bc	the architecture of harmonise: an ontology-based platform for semantic interoperability	semantic interoperability		semantic interoperability	Michele Missikoff;Francesco Taglino	2003			database;semantic interoperability;architecture;interoperability;ontology;semantic web stack;upper ontology;computer science	Vision	-40.089851825728644	6.97218258357047	173872
ff34464ce41ad841c6d8d90c8e40800de965edaa	cross: an owl wrapper for reasoning on relational databases	relational database;semantic web;open source;knowledge base	One of the challenges of the Semantic Web is to integrate the huge amount of information already available on the standard Web, usually stored in relational databases. In this paper, we propose a formalization of a logic model of relational databases, and a transformation of that model into OWL, a Semantic Web language. This transformation is implemented in Cross, as an open-source prototype. We prove a relation between the notion of legal database state and the consistency of the corresponding OWL knowledge base. We then show how that transformation can prove useful to enhance databases, and integrate them in the Semantic Web.	algorithm;column (database);foreign key;gigabyte;knowledge base;open database connectivity;open-source software;prototype;relational database;semantic web;turing completeness;web ontology language;world wide web	Pierre-Antoine Champin;Geert-Jan Houben;Philippe Thiran	2007		10.1007/978-3-540-75563-0_34	semantic data model;f-logic;knowledge base;relational model/tasmania;web modeling;relational model;semantic web rule language;data web;relational calculus;web standards;relational database;computer science;semantic web;social semantic web;data mining;semantic web stack;database;owl-s;information retrieval	DB	-37.785388238937685	6.984806800443939	174466
94fd46f01863637ef5a2737bad622288546d9ee5	towards an industrial strength sql/xml infrastructure	query processing;sql;relational databases xml sql query processing;data processing;design and implementation;xml;xml logic data processing relational databases kernel memory engines data mining industrial relations query processing;generating function;relational databases;critical infrastructure;xmltable function xml data processing model sql xmlquery	XML has become an attractive data processing model for applications. SQL/XML is a SQL standard that integrates XML with SQL. It introduces the XML datatype as a native SQL datatype and defines XML generation functions in the SQL/XML 2003 standard. The goal for the next version of SQL/XML is integrating XQuery with SQL by supporting XQuery embedded inside SQL functions such as the XMLQuery and XMLTable functions. Starting with the 9i database release, Oracle has supported the XML datatype and various operations on XML instances. In this paper, we present the design and implementation strategies of the SQL/XML standard in Oracle XMLDB. We explore the various critical infrastructures needed in the SQL database kernel to support an efficient native XML datatype implementation and the design approaches for efficient generation, query and update of the XML instances. Furthermore, we also illustrate extensions to SQL/XML that makes Oracle XMLDB a truly industrial strength platform for XML processing.	embedded system;sql;sql/xml;xml database;xquery	Muralidhar Krishnaprasad;Zhen Hua Liu;Anand Manikutty;James W. Warner;Vikas Arora	2005	21st International Conference on Data Engineering (ICDE'05)	10.1109/ICDE.2005.144	xml validation;xml encryption;xml base;pl/sql;sql;generating function;xml;sql injection;stored procedure;data processing;streaming xml;relational database;computer science;query by example;document structure description;xml framework;critical infrastructure;data mining;xml database;xml schema;database;sql/psm;language integrated query;xml signature;xml schema editor;information retrieval;null;efficient xml interchange	DB	-34.63341156927014	8.314721211581384	174543
e59e6665d19f6610d8cea0eb4fc25650f2317e12	odq: a fluid office document query language	query language;information retrieval;fluid office document;common document model	Fluid office documents, as semi-structured data often represented by Extensible Markup Language (XML) are important parts of Big Data. These office documents have different formats, and their matching Application Programming Interfaces (APIs) depend on developing platform and versions, which causes difficulty in custom development and information retrieval from them. To solve this problem, we have been developing an office document query (ODQ) language which provides a uniform method to retrieve content from documents with different formats and versions. ODQ builds common document model ontology to conceal the format details of documents and provides a uniform operation interface to handle office documents with different formats. The results show that ODQ has advantages in format independence, and can facilitate users in developing documents processing systems with good interoperability.	application programming interface;big data;information retrieval;interoperability;language-independent specification;markup language;procedural programming;prototype;query language;semi-structured data;semiconductor industry;xml	Xuhong Liu;Ning Li;Yunmei Shi;Xia Hou	2015	Information	10.3390/info6020275	computer science;database;world wide web;information retrieval;query language	Web+IR	-36.39470540305093	8.390747481596852	174776
b0ab3f08ec6fc31dac407bb08a605b071afa0ade	using and abusing xml	libraries;software;xml file format xml tag;style keywords xml best practices interoperability schemas;xml schema;schemas;gain;best practice;keywords xml;style;xml file format;best practices;stability analysis;xml;xml document;interoperability;economics;xml humans libraries iso standards global positioning system mice poles and towers satellites computer graphics robustness;xml tag;programming	XML is an extremely nifty format. Computers can easily parse XML data, yet humans can also understand it. By adopting XML, we can take advantage of the scores of tools that work on arbitrary XML documents. Common tasks like editing, validation, transformations, and queries become just a matter of selecting and applying the right tool.	parsing;xml	Diomidis Spinellis	2008	IEEE Software	10.1109/MS.2008.55	xml catalog;xml validation;binary xml;xml encryption;simple api for xml;xml;relax ng;streaming xml;computer science;document structure description;xml framework;xml database;xml schema;database;xml signature;management;world wide web;xml schema editor;cxml;information retrieval;efficient xml interchange;best practice;sgml	DB	-34.6488802612349	7.846851871028032	174821
1b22f99e3060de78297f164917308418c3fe27ee	dolar: virtualizing heterogeneous information spaces to support their expansion	virtual objects;information space expansion;service reuse and roi	Users expect applications to successfully cope with the exp ansion of information as necessitated by the continuous inclusion of novel types of content. Given that s uch content may originate from “not-seen thus far” data collections and/or data sources, the challenging issue is to achieve the return of investment on existing services, adapting to new information without cha nging existing business-logic implementation. To address this need, we introduce DOLAR, a service-neutral fr amework which virtualizes the information space to avoid invasive, time-consuming and expensive sour ce-code extensions that frequently break applications. Specifically, DOLAR automates the introduct ion of new business-logic objects in terms of the proposed virtual “content objects”. Such user-specifie d virtual objects align to storage artifacts and help realize uniform “store-to-user” data-flows atop heterogen eous sources, while offering the reverse “user-tostore” flows with identical effectiveness and ease of use. In addition, the suggested virtual object composition schemes help decouple business-logic from any content orig in, storage and/or structural details, allowing applications to support novel types of items without modify ing their service provisions. We expect that content-rich applications will benefit from our approach an d demonstrate how DOLAR has assisted in the cost-effective development and gradual expansion of a prod uction-quality digital library. Experimentation shows that our approach imposes minimal overheads and DOLAR -based applications scale as well as any underlying datastore(s). Copyright c © 0000 John Wiley & Sons, Ltd.	align (company);business logic;digital library;exptime;john d. wiley;object composition;usability;year 10,000 problem	Kostas Saidis;Yannis Smaragdakis;Alex Delis	2011	Softw., Pract. Exper.	10.1002/spe.1050	simulation;computer science;engineering;theoretical computer science;operating system;data mining	Web+IR	-40.47597421701173	10.00698415119618	175241
05745975661a7446974e0e697a7e23c2c949078c	an improved semantic information searching scheme based multi-agent system and an innovative similarity measure	web languages;semantic annotation;multi agent system;metadata;semantic information search;semantic web;wordnet;inference engine;similarity measure;ontology	The key task for the web, namely, web searches, is evolving towards some novel form of semantic web search. In fact, most information retrieval systems are based on static vectors representations. Two major difficulties when a researcher uses current information retrieval systems are how to filter out irrelevant documents, and how to discover latest or more significant documents. Recently a very promising approach to semantic web search is based on combining standard web pages and search queries with ontological background knowledge. In this perspective we will describe a model to hold a document’s noise, and incompleteness. For this, we merge a syntactic keyword search with purely semantic search based domain ontology and a multi-agent system to solve such distributed problems. Then we perform a ranking algorithm on returned documents, and we propose a new semantic similarity measure between concepts based on the WordNet taxonomy structure.	agent-based model;artificial intelligence;case-based reasoning;information retrieval;multi-agent system;ontology (information science);programming paradigm;relevance;search algorithm;semantic web;semantic query;semantic search;semantic similarity;shoelace formula;similarity measure;web content;web page;web resource;web search engine;web search query;wordnet	Djamel Nessah;Okba Kazar	2013	IJMSO	10.1504/IJMSO.2013.058411	wordnet;semantic similarity;semantic computing;semantic integration;semantic web rule language;data web;explicit semantic analysis;semantic search;semantic grid;image retrieval;computer science;artificial intelligence;semantic web;concept search;ontology;social semantic web;data mining;semantic web stack;database;semantic equivalence;semantic technology;web search query;metadata;world wide web;owl-s;information retrieval;semantic analytics;inference engine;search engine	Web+IR	-40.68059339978696	7.25061503301696	175819
8686b318ca7398823502880377a7823f720bb24e	scalable association rule mining with predication on semantic representations of data	frequent closed pattern;association rules;class association rules;semantic web;sparql;semantic association	Finding semantic associations from a vast amount of heterogeneous data is an important and useful task in various applications. We present a framework to extract semantic association patterns directly from a very large graph dataset without the extra step of converting graph data into transaction data. The proposed algorithm SAG (Semantic Association Generator) utilizes the principle of minimum description length to unearth general relevant associations and demonstrates that SPARQL commands can be used to perform data mining tasks.	algorithm;association rule learning;data mining;extensibility;minimum description length;sparql;sql access group;scalability;semantic web;structure mining;transaction data;usability;world wide web	Li-Shiang Tsay;Sreenivas R. Sukumar;Larry W. Roberts	2015	2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2015.7407080	semantic similarity;semantic computing;named graph;semantic grid;computer science;sparql;social semantic web;data mining;database;semantic equivalence;information retrieval	AI	-35.13100711678884	4.712226935537623	177011
023769a5cae9b7851c4559fe8ff7657703817b0b	an enhanced model for searching in semantic portals	fuzzy reasoning;information retrieval;fuzzy description logic;semantic web technology;information sharing;semantic information;web portal;semantic portal;next generation;semantic search	Semantic Portal is the next generation of web portals that are powered by Semantic Web technologies for improved information sharing and exchange for a community of users. Current methods of searching in Semantic Portals are limited to keyword-based search using information retrieval (IR) techniques, ontology-based formal query and reasoning, or a simple combination of the two. In this paper, we propose an enhanced model that tightly integrates IR with formal query and reasoning to fully utilize both textual and semantic information for searching in Semantic Portals. The model extends the search capabilities of existing methods and can answer more complex search requests. The ideas in a fuzzy description logic (DL) IR model and a formal DL query method are employed and combined in our model. Based on the model, a semantic search service is implemented and evaluated. The evaluation shows very large improvements over existing methods.	description logic;information retrieval;portals;semantic web;semantic search;web search engine	Lei Zhang;Yong Yu;Jian Zhou;Chenxi Lin;Yin Yang	2005		10.1145/1060745.1060812	semantic data model;semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic web rule language;explicit semantic analysis;semantic search;semantic grid;computer science;semantic web;concept search;social semantic web;semantic web stack;semantic compression;database;semantic technology;web search query;world wide web;information retrieval;semantic analytics	Web+IR	-40.498510578513134	7.501897941354393	177098
e9b7d96347e4ea38fc3cdd6537ad4ab8ad73bb78	styrian diversity visualisation: visualising statistical open data with a leanweb app and data server	h 5 2 information interfaces and presentation;d 2 12 software engineering;interoperability;data mapping;user interfaces	Statistical open data is usually provided only in the form of spreadsheets or CSV files. The developers of open data apps must either restrict themselves to managable bite-sized chunks of data, which can be consumed (read, parsed, and held in memory) in one go, or must install and maintain their own data server which the app can query on demand. The Styrian Diversity Visualisation (in German “Steirische Vielfalt Visualisiert” or SVV) project demonstrates the use of a dedicated data server (triple store) to host large amounts of statistical open data. The SVV web app queries the data server dynamically using SPARQL queries to obtain exactly the data required at that particular time, greatly simplifying its internal logic. There is no need to parse and store entire data sets in memory.	parsing;sparql;server (computing);spreadsheet;triplestore;web application	Keith Andrews;Thomas Traunmüller;Thomas Wolkinger;Eva Goldgruber;Robert Gutounig;Julian Ausserhofer	2016		10.2312/eurp.20161150	human–computer interaction;computer science;data mining;world wide web	DB	-39.66262211929735	10.651492516666426	178103
934d436e80c1cca1fd17cb0da6b9fc0db744ffec	linkable geographic ontologies	relational data;management system;linked data;information retrieval;conceptual model;data model;geo ontologies;geographic knowledge base;knowledge management system;geographic information retrieval;knowledge base	The performance of some tasks in Information Retrieval is strongly related to the extent and quality of the geographic knowledge about named places. This paper presents a conceptualization of the geographic knowledge, the Geo-Net vocabulary, and a tool for building large knowledge bases of named places, the GKB management system, developed in the GREASE-II project. The Geo-Net vocabulary is a conceptual model for describing geographic places, including their names, types, relationships and footprints. It uses URIs and the RDF data model to expose, share and connect pieces of geographic knowledge each other and to related data on the Web. The GKB system is a multi-paradigm knowledge management system that enables the development of geographic ontologies with the Geo-Net vocabulary. This paper also presents a geographic ontology of Portugal, Geo-Net-PT 02, created with the Geo-Net vocabulary and the GKB system.	conceptualization (information science);data model;information retrieval;knowledge management;neo geo;ontology (information science);programming paradigm;resource description framework;uniform resource identifier;vocabulary;world wide web	Francisco J. López-Pellicer;Mário J. Silva;Marcirio Silveira Chaves	2010		10.1145/1722080.1722082	local information systems;knowledge base;data model;relational database;computer science;conceptual model;linked data;data mining;management system;database;world wide web;information retrieval	Web+IR	-40.551211519762134	5.146261781677936	178414
c66aba2fe2127790e1539f343bd7733f2d5cce54	"""advanced features of integrated db/dc system """"xdm"""""""		As the applications of DEVDC systems expand, DE/DC systems are growing very rapidly in terms of size and functionality. Distributed database and composite sub-system management facilities are developed in XOM, the strategic DB/DC system program product of Hitachi,Ltd., to meet recent users’ requirement. As a result of these functional extensions of XDM, it is possible to construct a large-scaled distributed DBIDC system with high performance and reliability.	distributed database;extensible device metadata;query language;relational database;sql;systems management;xml object model	Kazuo Masai;Shoji Yamamoto;Hiromichi Ishikawa;Takaashi Sumiyoshi;Mitsuo Miyazaki;Masato Saito;Toshikazu Hiramoto;Shigeru Yoneda	1989			database;computer science;data mining	HPC	-35.62973121402708	9.399914989679976	178461
194afc4de78a2d3f2d598f5bd8d9d6634c9852b7	an ontological approach to design real-time applications		These recent years, we have seen the emergence of several studies allowing a linkage between databases and ontologies to facilitate the design of databases. However, these studies do not address the needs of the advanced applications, because they do not integrate mechanisms that consider the temporal characteristic. In addition, the complexity of real-time applications is ever increasing which makes their design processes very difficult. For that purpose, a new vision will allow to fill in the limits of these studies and to suggest a method to conceptualize this kind of databases (real-time databases). This vision needs to define a new ontology to be integrated into these databases. This suggested ontology should be used to represent a domain in an explicit way without any ambiguity, to describe the most general categories and relations, and to define the temporal information and to resolve the semantic conflicts. In this paper, our solution is to advance an ontological modeling for real-time applications.	database;emergence;linkage (software);ontology (information science);real-time clock;real-time computing;real-time locating system;real-time transcription;replay attack;temporal logic	Wided Ben Abid;Mohamed Mhiri;Emna Bouazizi;Ahlem Rhayem	2016	Journal on Data Semantics	10.1007/s13740-016-0068-1	computer science;knowledge management;theoretical computer science;data mining;database	DB	-38.660474369653556	4.580974856403918	179053
21859e7d22228b5922e1574df7065e6c91fec6dd	correlation-based refinement of rules with numerical attributes	004 informatik	Learning rules is a common way of extracting useful information from knowledge or data bases. Many of such data sets contain numerical attributes. However, approaches like Inductive Logic Programming (ILP) or association rule mining are optimized for data with categorical values, and considering numerical attributes is expensive. In this paper, we present an extension to the top-down ILP algorithm, which enables an efficient discovery of datalog rules from data with both numerical and categorical attributes. Our approach comprises a preprocessing phase for computing the correlations between numerical and categorical attributes, as well as an extension to the ILP refinement step, which enables us to detect interesting candidate rules and to suggest refinements with relevant attribute combinations. We report on experiments with U.S. Census data, Freebase and DBpedia, and show that our approach helps to efficiently discover rules with numerical intervals.	algorithm;association rule learning;dbpedia;database;datalog;discretization;experiment;freebase;heuristic;inductive logic programming;inductive reasoning;linked data;numerical analysis;preprocessor;refinement (computing);top-down and bottom-up design;vergence	André Melo;Martin Theobald;Johanna Völker	2014			computer science;machine learning;data mining;algorithm	ML	-35.5708884102084	5.072419729379746	179698
b76eef384aea9def190a76c8d39262bafb859a9c	determining relevant product information sources	resource selection;i heterogeneous product information sources federated enterprise search semantic web techniques;information sources;information retrieval;i;semantics;distributed information retrieval;semantics taxonomy ontologies indexes semantic web;enterprise search;semantic web information retrieval;indexes;federated enterprise search;design and implementation;semantic web techniques;ontologies distributed information retrieval federated information systems resource selection;federated information systems;taxonomy;semantic web;ontologies;information need;heterogeneous product information sources	This position paper describes the challenges related to federated enterprise search over heterogeneous product information sources. It focuses on the aspect of finding relevant information sources w.r.t. the user's information need and identifies core research questions with regards to the design and implementation of a potential solution, based on Semantic Web techniques.	information needs;semantic web	Matthias Wauer	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.266	database index;information needs;computer science;ontology;artificial intelligence;semantic web;data mining;database;semantics;information retrieval;taxonomy	DB	-38.544358034032946	5.7916987721445725	180479
906260b11670eaedead8ca2865b8c0d3aaa33d8d	a framework for semantic discovery on the web of things		This paper addresses the problem of discovering WoT agents (servients) that can interact in a mediated or peer-to-peer fashion in a larger system. We develop a framework that relies on the W3C Thing Description (TD) and Semantic Sensor Network (SSN) ontologies, which provide semantics for the physical world entities WoT servients are associated with. We formulate the problem of WoT discovery as an abductive reasoning problem over knowledge bases expressed in terms of TD and SSN concepts, where new semantic relationships between existing systems lead to the creation of other, larger systems. We then address the specific case of EL knowledge bases, a fragment of Description Logic. We leverage the mathematical framework of Abductive Logic Programming to provide a concrete algorithm for abduction that terminates in polynomial time. We illustrate the feasability of our approach on an experimental industrial workstation, equipped with micro-controllers capable of storing and exchanging RDF data in binary format (μRDF store with EXI4JSON serialization).	abductive logic programming;abductive reasoning;algorithm;binary file;dmz (computing);description logic;efficient xml interchange;entity;ontology (information science);peer-to-peer;resource description framework;semantic sensor web;serialization;time complexity;web of things;workstation;world wide web	Victor Charpenay;Sebastian Käbisch;Harald Kosch	2018		10.3233/978-1-61499-894-5-147		AI	-37.05416810070233	9.395910779545643	180596
badd1e6c10ac1e451752c82fc32764d9f0c2b7a7	on semantic matching of multilingual attributes in relational systems	query processing;multilingual databases;supercomputer education research centre;computer science automation formerly school of automation;semantic information retrieval;semantic information;ontology based;semantic matching	In an increasingly multilingual digital world, it is critical that information management tools, such as web search engines, e-Commerce portals and e-Governance applications, support the simultaneous use of multiple natural languages. An essential pre-requisite is that the underlying database engines (typically relational), provide the functionality for processing multilingual data seamlessly across languages. As a part of our Mira [1] research initiative focusing on functionality and performance aspects of supporting multilingualism in relational database systems, we propose SemEQUAL, a semantic operator for matching text attribute data across languages based on meaning. For example, to automatically match the English noun mathematics, with mathématiques in French or  (meaning mathematics) in Tamil.	database engine;e-commerce;e-governance;information management;natural language;portals;relational database;semantic matching;web search engine	A. Kumaran;Jayant R. Haritsa	2004		10.1145/1031171.1031217	natural language processing;semantic computing;computer science;data mining;semantic web stack;database;world wide web;information retrieval	DB	-39.85442347611185	7.038378906793591	181447
9c0cc8d06dd2580dbefd966480814b8c1aa56107	exploiting fast classification of snomed ct for query and integration of health data.	relational data;controlled vocabulary;tool support;snomed ct	By constructing local extensions to SNOMED we aim to enrich existing medical and related data stores, simplify the expression of complex queries, and establish a foundation for semantic integration of data from multiple sources. Specifically, a local extension can be constructed from the controlled vocabulary(ies) used in the medical data. In combination with SNOMED, this local extension makes explicit the implicit semantics of the terms in the controlled vocabulary. By using SNOMED as a base ontology we can exploit the existing knowledge encoded in it and simplify the task of reifying the implicit semantics of the controlled vocabulary. Queries can now be formulated using the relationships encoded in the extended SNOMED rather than embedding them ad-hoc into the query itself. Additionally, SNOMED can then act as a common point of integration, providing a shared set of concepts for querying across multiple data sets. Key to practical construction of a local extension to SNOMED is appropriate tool support including the ability to compute subsumption relationships very quickly. Our implementation of the polynomial algorithm for EL+ in Java is able to classify SNOMED in under 1 minute.	algorithm;central processing unit;controlled vocabulary;data store;hoc (programming language);java;multi-core processor;overhead (computing);polynomial;semantic integration;statistical classification;subsumption architecture;systematized nomenclature of medicine;thread (computing);usability;weatherstar	Michael Lawley	2008			controlled vocabulary;medicine;relational database;computer science;snomed ct;data mining;database;information retrieval	AI	-36.09439517957431	4.991850108505758	181774
5b8c45e9c8ded5f8adb7df7de1cadc6c1c9f6674	new framework for semantic search engine	search engines semantics resource description framework ontologies database languages;user interfaces data structures pattern matching query languages query processing search engines semantic web;search engines;component;semantics;resource description framework;search framework;semantic web querying semantic search engine machine readable format data matching user friendly interface query language processor result optimiser result ranking data structures;semantic web syntax based search semantic search;semantic web;ontologies;semantic search;search framework component;syntax based search;database languages	The semantic web is a technology to save data in a machine-readable format that facilitates machines to intelligently match that data with related data based on meanings. Whilst this approach is being adopted and implemented by some large organisations there is a need for an effective semantic search engine to maximise the full potential of that semantic web. A major difficulty is that the search experience is dependent on a number of elements including a user-friendly interface, a strong query language processor, a result optimiser, result ranking and the use of appropriate data structures to store data. Apart from the technical aspects related to implementation, a strategy to prioritise these elements is needed to optimize and enhance the search experience over the semantic web. The purpose of this work is to investigate some relevant issues on querying the semantic web in a context of semantic search engines, and propose a framework that facilitates an effective search over the semantic web.	data structure;human-readable medium;internet;natural language processing;query language;semantic web;semantic search;usability;web search engine;while	Arooj Fatima;Cristina Luca;George Wilson	2014	2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation	10.1109/UKSim.2014.114	search engine indexing;semantic similarity;semantic computing;query expansion;web query classification;semantic integration;semantic web rule language;data web;semantic search;semantic grid;computer science;semantic web;concept search;social semantic web;semantic web stack;semantic compression;database;semantic equivalence;semantic technology;web search query;world wide web;information retrieval;semantic analytics;search engine	Web+IR	-34.840097597616385	4.6032240262135105	182116
e05eb9ab333cd19c98ead549d33d35b996ccfe6b	understanding personal data as a space - learning from dataspaces to create linked personal data	qa75 electronic computers computer science	In this paper we argue that the space of personal data is a dataspace as defined by Franklin et al. We define a personal dataspace, as the space of all personal data belonging to a user, and we describe the logical components of the dataspace. We describe a Personal Dataspace Support Platform (PDSP) as a set of services to provide a unified view over the user’s data, and to enable new and more complex workflows over it. We show the differences from a DSSP to a PDSP, and how the latter can be realized using Web protocols and Linked APIs.	dataspaces;desktop computer;franklin electronic publishers;personally identifiable information;semantic desktop;setun;world wide web	Laura Dragan;Markus Luczak-Rösch;Nigel Shadbolt	2014			computer science;theoretical computer science;data mining;database	DB	-40.04741700416484	9.294845833133559	182151
00088986c30f3b32724562d84b4e9be87262bc21	reasoning support for semantic web ontology family languages using alloy	alloy;swrl;owl;fol;semantic web	Semantic Web (SW), commonly regarded as the next generation of the Web, is an emerging vision of the new Web from the Knowledge Represen tatio and the Web communities. To realize this vision, a series of techniq ues has been proposed. Semantic Web Ontology Langauge (OWL) and its extension Sema ntic Web rule Language (SWRL) and Semantic Web Logic Language (SWRL-FOL) are some of the most important outputs from the SW activities. However t h existing reasoning and consistency checking tools for those languages are p rimitive. This paper proposes using the existing formal modelling tool, in parti cular Alloy, to provide an automatic reasoning service for the Semantic Web ontolog y family languages (OWL/SWRL/SWRL-FOL). keywords: Semantic Web, Alloy, OWL, SWRL, FOL. 1Primary contact: Hai H. Wang, School of Computer Science, Th University of Manchester, M13 9JPL, United Kingdom Email: hai.wang@cs.manchester.ac.uk Tele phone: +44 161 275 0686 Fax: +44 161 275 6204	alloy (specification language);automated reasoning;computer science;email;fax;first-order logic;semantic web rule language;shattered world;television;ues (cipher);web ontology language;world wide web	Hai H. Wang;Jin Song Dong;Jing Sun;Jun Sun	2006	Multiagent and Grid Systems		knowledge representation and reasoning;semantic computing;web modeling;description logic;semantic web rule language;data web;semantic search;ontology inference layer;semantic grid;web standards;computer science;ontology;artificial intelligence;semantic web;social semantic web;semantic web stack;database;web intelligence;world wide web;owl-s;website parse template;information retrieval;semantic analytics	Web+IR	-39.80700757275393	7.002448188480378	182926
56cea527a1f173c9a313c9e3a8afc670d37b76d6	a string representation of ldap search filters		The Lightweight Directory Access Protocol (LDAP) [1] defines a network representation of a search filter transmitted to an LDAP server. Some applications may find it useful to have a common way of representing these search filters in a human-readable form. This document defines a human-readable string format for representing LDAP search filters.	lightweight directory access protocol	Tim Howes	1996	RFC	10.17487/RFC1960	computer science;metadirectory;theoretical computer science;database;world wide web	Vision	-34.53192464354783	7.340107681364611	182998
d12238a36dc579c8fbe9bad0565701ac5cc24de6	validating rdf data		RDF and Linked Data have broad applicability across many fields, from aircraft manufacturing to zoology. Requirements for detecting bad data differ across communities, fields, and tasks, but nearly all involve some form of data validation. This book introduces data validation and describes its practical use in day-to-day data exchange. The Semantic Web offers a bold, new take on how to organize, distribute, index, and share data. Using Web addresses (URIs) as identifiers for data elements enables the construction of distributed databases on a global scale. Like the Web, the Semantic Web is heralded as an information revolution, and also like the Web, it is encumbered by data quality issues. The quality of Semantic Web data is compromised by the lack of resources for data curation, for maintenance, and for developing globally applicable data models. At the enterprise scale, these problems have conventional solutions. Master data management provides an enterprise-wide vocabulary, while constraint languages capture and enforce data structures. Filling a need long recognized by Semantic Web users, shapes languages provide models and vocabularies for expressing such structural constraints. This book describes two technologies for RDF validation: Shape Expressions (ShEx) and Shapes Constraint Language (SHACL), the rationales for their designs, a comparison of the two, and some example applications.	data curation;data model;data quality;data structure;data validation;digital curation;distributed database;identifier;information revolution;linked data;master data management;requirement;resource description framework;semantic web;sensor;vocabulary;world wide web	José Emilio Labra Gayo;Eric Prud'hommeaux;Iovka Boneva;Dimitris Kontokostas	2017		10.2200/S00786ED1V01Y201707WBE016	web modeling;semantic analytics;data mapping;data mining;linked data;social semantic web;semantic web;data web;semantic web stack;computer science	Web+IR	-38.626202829431485	5.456458161767185	183079
a7956e99d523d23e025dee3c252458ef25804818	exploiting the rich document structures and network topology of legal information systems	conceptual information retrieval;legal information systems;network topologies	The move toward on-line publishing of legal documents in the public domain has made extra information available by way of mark-up for electronic publishing. Legal Concepts that were once implicit in documents are now explicit and further inferences can now be drawn through computation however the quality and extent of this information varies. This paper discusses the application of web techniques to the legal system for the ranking of related legal concepts. We demonstrate how these techniques can be adapted and applied to the legal system and how strategies can be used to optimize the best combination of techniques depending on the quality of the source information. Courts and legislatures create and distribute laws through documents that serve as the primary sources of law within legal systems. With the increase in on-line publishing, these documents are being marked-up with greater quantities of information particularly with links to other related sources within the legal system.	computation;information systems;information system;network topology;online and offline;primary source;quantities of information;software propagation	John Ellis Murphy;Robert Steele;Rita Shen	2008			computer science;knowledge management;data mining;management;world wide web;computer security;information retrieval;network topology	Web+IR	-39.94457424916387	5.03627774895514	183128
30a598d1dc8da465cea69682d6820a559f9a231b	a front-end approach for user query generation and information retrieval in the semanticlife framework	front end;data storage;information retrieval;semantic web	Formulating unambiguous queries in the Semantic Web applications is a challenging task for users. This paper presents a new approach in guiding users to generate clear requests based on their common nature of querying for information. The approach known as the “front-end approach” gives users an overview about the system data through a “virtual data component” which stores the extracted metadata of the data storage sources in the form of an ontology. This approach reduces the ambiguities in users’ requests at very early stage; and allows the query refinement process to easily to fulfill users’s demands. Furthermore, the approach provide a powerful query engine, called “context-based querying”, that recommends the appropriate query patterns according to the user’s querying context. These features help the user in generating clear query more easier.	benchmark (computing);computer data storage;information retrieval;real-time business intelligence;refinement (computing);semantic web;user interface	Hanh Huu Hoang;Tho Manh Nguyen;Amin Andjomshoaa;A Min Tjoa	2006			database;ranking (information retrieval);query expansion;web search query;data mining;web query classification;computer science;information retrieval;query language;sargable;front and back ends;query optimization	Web+IR	-35.32040018093801	4.36505546655515	183174
2ded054565542411fe4dc45b9af343a174110f9b	devisa - concepts and architecture of a data mining models scoring and management web system	web system;data mining	In this paper we describe DeVisa, a Web system for scoring and management of data mining models. The system has been designed to provide unified access to different prediction models using standard technologies based on XML. The prediction models are serialized in PMML format and managed using a native XML database system. The system provides functions such as scoring, model comparison, model selection or sequencing through a web service interface. DeVisa also defines a specialized PMML query language named PMQL used for specifying client requests and interaction with PMML repository. The paper analyzes the system’s architecture and functionality and discusses its use as a tool for researchers.	algorithm;application programming interface;association rule learning;data mining;delta-sigma modulation;download;first-order logic;java;knowledge base;lookup table;maximal set;model selection;naive bayes classifier;ontology (information science);predictive model markup language;query language;replay attack;ruleml;upload;web service;weka;xml database	Diana Gorea	2008			web mining;web modeling;data web;computer science;data science;data mining;web intelligence;world wide web	Web+IR	-39.3017641834435	6.552079325478457	183220
fa0cbdbb21d55ac8008003b9aa97f798af01771f	the evolution of web documents: the ascent of xml	web documents			Dan Connolly;Rohit Khare;Adam Rifkin	1997	World Wide Web Journal		xml base;web development;site map;data web;web standards;computer science;semantic web;web page;semantic web stack;web 2.0;world wide web;xml schema editor;efficient xml interchange	Web+IR	-39.51875289239812	7.359110905336756	184127
6bdff7a87f99f19207901e39920c0335eff08ce9	personnalisation de bases de données multidimensionnelles		This paper deals with decision support systems res ting on multidimensional modelling of data. Moreover, we intend to offer a se t of concepts and mechanisms for personalized multidimensional database specificatio ns. This personalization consists in associating weights to different components of a mul tidimensional schema. Personalization specifications are specified through the use of a l anguage based on the principle of Event Condition Action. This personalisation determines mu ltidimensional data display as well as their analyses (with the use of drilling or rotating operations). MOTS-CLÉS : modélisation et analyses multidimensionnelles, pe rsonnalisation, règles ECA	decision support system;event condition action;online analytical processing;personalization	Franck Ravat;Olivier Teste;Gilles Zurfluh	2007			data mining;database;personalization;online analytical processing;event condition action;decision support system;schema (psychology);computer science	DB	-35.54117760727372	10.72792824684146	184375
6e0cd9af16cf27b87eff52545bd79ae3b0602fc2	a generic data quality framework applied to the product data for naval vessels	data quality	The data quality framework presented here is a set of tools and methods providing services for the data quality assessment along the presented data quality dimensions. Thin adapters connect the framework to the underlying data sources such as Part 21 [21], xml or relational databases. In the other end, domain specific business rules defined by UML/OCL, Schematron or java/C# extends the framework. Client applications can provide rich graphics to visualize views on the data quality dimensions such as completeness, validation and overlap. If present, 3D models and external reference data libraries can be linked to the data to verify part locations and general lexical meaning of attributes. The semiotic information quality framework suggested by Price and Shanks in [15] is used to categorize some of the data quality dimensions described here. .	3d modeling;categorization;data quality;domain model;graphics;information quality;java;library (computing);object constraint language;relational database;schematron;semiotic information theory;thin plate spline;unified modeling language;xml;xslt	Jørgen Stang;Tore Christiansen;David Skogan;Atle Kvalheim;Tor Arne Irgens	2008			data mining;data quality;computer science	DB	-35.970138917788375	8.487544439201253	184809
dccdd7c09b1b48be5ad389fb86152851bf918636	xml programming with sql/xml and xquery	query language;extensible markup language;relational data;structured query language;relational database system;data retrieval	Most business data are stored in relational database systems, and SQL (Structured Query Language) is used for data retrieval and manipulation. With XML (Extensible Markup Language) rapidly becoming the de facto standard for retrieving and exchanging data, new functionality is expected from traditional databases. Existing SQL applications will evolve to retrieve relational data as XML data using database or SQL extensions for XML. New XML data will be stored, searched, and manipulated in the database as a “first class” citizen along with existing relational data. Furthermore, new applications will emerge that solely operate in terms of XML. These new XML applications operate on the same database using an XML query language, XQuery. In this paper, we describe an integrated database architecture that enables SQL applications with XML extensions as well as XQuery applications to operate on the same data. The architecture allows for a seamless flow from relational data to XML and back.	data model;data retrieval;first-class function;hierarchical database model;hierarchical storage management;hoc (programming language);markup language;query language;relational database;relational model;sql;sql/xml;seamless3d;server (computing);system integration;xml;xquery	John E. Funderburk;Susan Malaika;Berthold Reinwald	2002	IBM Systems Journal	10.1147/sj.414.0642	data exchange;xml validation;xml encryption;data definition language;sql;relational database management system;xml;streaming xml;relational database;computer science;query by example;document structure description;database model;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;data retrieval;information retrieval;object-relational impedance mismatch;query language;efficient xml interchange;sgml	DB	-33.93908515865586	8.386247806195408	184852
8a7b8580b19c2c5652fdcbe720066c81751f1285	using owl ontology for reasoning about schema mappings in data exchange systems			web ontology language	Tadeusz Pankowski	2012		10.3233/978-1-61499-105-2-430	owl-s;web ontology language;schema (psychology);ontology (information science);data mining;database;logical schema;suggested upper merged ontology;data exchange;computer science	DB	-38.42121452466248	6.90001852228151	185349
6e63d371e80e58a454f0f73f96043806a223191e	semantic query-based generation of customized 3d scenes	semantic queries;3d content;semantic web;3d web	The paper presents a method of building generalized content representations - 3D meta-scenes, and on-demand generation of customized final 3D scenes with the use of semantic queries. The presented method enables generation of 3D scenes by the use of selection, projection, composition and extension operations, performed in the semantic domain, providing a powerful mechanism for content customization. The use of semantics simplifies customization of 3D scenes, which usually consist of complex components with complex dependencies between them. The resulting scenes can be presented using existing 3D web browsers.	3d computer graphics;semantic query;world wide web	Krzysztof Walczak;Jakub Flotynski	2015		10.1145/2775292.2775311	semantic computing;semantic search;computer science;social semantic web;semantic web stack;database;world wide web;information retrieval;semantic analytics	Graphics	-40.53146987970823	9.694650032794184	185471
b15f827ff0d968515fb6d253305fecc66fc71055	the deepthought core architecture framework	information extraction;customer relationship management;information retrieval;research purpose;business intelligence	The research performed in the DeepThought project aims at demonstrating the potential of deep linguistic processing if combined with shallow methods for robustness. Classical information retrieval is extended by high precision concept indexing and relation detection. On the basis of this approach, the feasibility of three ambitious applications will be demonstrated, namely: precise information extraction for business intelligence; email response management for customer relationship management; creativity support for document production and collective brainstorming. Common to these applications, and the basis for their development is the XMLbased, RMRS-enabled core architecture framework that will be described in detail in this paper. The framework is not limited to the applications envisaged in the DeepThought project, but can also be employed e.g. to generate and make use of XML standoff annotation of documents and linguistic corpora, and in general for a wide range of NLP-based applications and research purposes.	architecture framework;customer relationship management;deep linguistic processing;email;information extraction;information retrieval;intel core (microarchitecture);natural language processing;precision and recall;text corpus;xml	Ulrich Callmeier;Andreas Eisele;Ulrich Schäfer;Melanie Siegel	2004			computer science;knowledge management;data mining;database;customer intelligence;business intelligence;business relationship management;information extraction;information framework;business architecture;human–computer information retrieval	Web+IR	-40.60924211388984	6.241442118410289	185887
e4579296310f65f184a333462fb0f092e9e883bf	ontology driven semantic profiling and retrieval in medical information systems	information sources;information retrieval system;information retrieval;semantics;data model;user profile;knowledge representation and data models;e health information systems;medical information systems;medical information system;semantic web;health information system;interoperability;semantic contextualization and profiling;ontology	We propose the application of a novel sub-ontology extraction methodology for achieving interoperability and improving the semantic validity of information retrieval in the medical information systems (MIS) domain. The system offers advanced profiling of a user’s field of specialization by exploiting the concept of sub-ontology extraction, i.e., each sub-ontology may subsequently represent a particular user profile. Semantic profiling of a user’s field of specialization or interest is necessary functionality in any medical domain information retrieval system; this is because the (structural and semantic) extent of information eywords: emantic contextualization and profiling edical information systems -Health information systems	information retrieval;information system;interoperability;ontology learning;partial template specialization;profiling (computer programming);user profile	Mehul Bhatt;J. Wenny Rahayu;Sury Prakash Soni;Carlo Wouters	2009	J. Web Sem.	10.1016/j.websem.2009.05.004	idef1x;semantic interoperability;interoperability;semantic computing;semantic integration;explicit semantic analysis;relevance;cognitive models of information retrieval;data model;computer science;information filtering system;semantic web;ontology;data mining;semantic web stack;database;semantics;information retrieval;human–computer information retrieval	Web+IR	-39.01453917771877	6.281933902113977	186408
8fd8fe0f15588301d02f4e61e381e4a53b31a10d	updating xml schemas and associated documents through exup	xml schema;xml schemas;exup system;web;semantics;processing schema modification;xsupdate language;xml engines java semantics organizations conferences graphical user interfaces;graphical user interfaces;engines;associated documents;xml;graphic user interface;organizations;conferences;java;document adaptation statements xml schemas exup system xsupdate language web associated documents processing schema modification;document adaptation statements	Data on the Web mostly are in XML format and the need often arises to update their structure, commonly described by an XML Schema. When a schema is modified the effects of the modification on documents need to be faced. XSUpdate is a language that allows to easily identify parts of an XML Schema, apply a modification primitive on them and finally define an adaptation for associated documents, while Eχup is the corresponding engine for processing schema modification and document adaptation statements. Purpose of this demonstration is to provide an overview of the facilities of the XSUpdate language and of the Eχup system.	database schema;world wide web;xml schema	Federico Cavalieri;Giovanna Guerrini;Marco Mesiti	2011	2011 IEEE 27th International Conference on Data Engineering	10.1109/ICDE.2011.5767951	data exchange;xml validation;xml encryption;xml;relax ng;xml schema;streaming xml;computer science;document type definition;xs3p;document definition markup language;document structure description;xml framework;xml database;graphical user interface;xml schema;database;semantics;document schema definition languages;schematron;xml signature;world wide web;xml schema editor;cxml;information retrieval;efficient xml interchange	DB	-34.33346453247476	7.972885262039816	186618
4d005fd3ab5705380734a961b478ac7b9ddce5fc	towards licenses compatibility and composition in the web of data	linked data;lincences;deontic logic;reasonning;semantic web	The absence of clarity concerning the licensing terms does not encourage the reuse of the data by the consumers, thus preventing further publication and consumption of datasets, at the expense of the growth of the Web of Data itself. In this paper, we propose a general framework to attach the licensing terms to the data queried on the Web of Data. In particular, our framework addresses the following issues: (i) the various license schemas are collected and aligned taking as reference the Creative Commons schema, (ii) the compatibility of the licensing terms concerning the data affected by the query is verified, and (iii) if compatible, the licenses are combined into a composite license. The framework returns the composite license as licensing term about the data resulting from the query.	algorithm;align (company);cc system;data web;heuristic (computer science);named graph;normative social influence;semantic web;social proof;software license;vocabulary;world wide web	Serena Villata;Fabien L. Gandon	2012			computer science;semantic web;linked data;data mining;deontic logic;database;world wide web	Web+IR	-39.448424263716774	8.892141804545487	186654
ff30252e47c49855444577fd92352896b0e854cc	reasoning the fma ontologies with trowl		In this paper, we briefly introduce the TrOWL ontology reasoning infrastructure and share our experience of using TrOWL to reason with various versions of the Foundational Model of Anatomy Ontology (FMA), which are among the most challenging ontologies for Description Logic reasoners.	approximation algorithm;debugging;description logic;fma instruction set;foundational model of anatomy;metamodeling;microsoft outlook for mac;ontology (information science)	Jeff Z. Pan;Yuan Ren;Nophadol Jekjantuk;Jhonatan Garcia	2013			ontology;foundational model of anatomy;description logic;natural language processing;ontology (information science);artificial intelligence;computer science	AI	-37.98703372549946	8.249741352560903	187316
f726e1ea7d7bd7813f693e78c6aef12fa093d69b	bisimulation-based consistency checking on syndrome feng-shi-re-bi in rheumatoid arthritis	bisimulation;text mining	Checking the consistency of knowledge items between textbook and clinical practice is very important for traditional Chinese medicine (TCM). According to the textbook of internal medicine of TCM, rheumatoid arthritis (RA) is a disease with four main syndromes (also called pattern) and Feng-Shi-Re-Bi is a minor one and the associated literature data are not rich. Bisimulation is an equivalence relationship/method in formal methods which can be used to compare the consistency of system's description and its behaviors. In TCM, knowledge items from textbook can be taken as system's description, and knowledge items from clinical practice can be taken as the behaviors of the system. For RA's Feng-Shi-Re-Bi syndrome, employing bisimulation method, the consistency of knowledge items between textbook and clinical practice have been checked. As a result, on Feng-Shi-Re-Bi syndrome, most knowledge items in textbook can be simulated by the clinical practice e.g., syndrome, symptom, herbal formula, herbal medicine. What's more, in clinical practice, there are also variations based on basic rules of traditional Chinese medicine. In brief, bisimulation is a proper method for consistency checking between TCM textbook and clinical practice.	algorithm;anycast;bisimulation;blog;control system;entity–relationship model;lina;lu decomposition;management system;maximal set;multi-agent system;petri net;q-learning;ray tracing (graphics);remote computer;routing;simulation;software as a service;test case;web 2.0;web service;x image extension;x86;yang	Guang Zheng;Kai Cui;Junping Zhan;Zekun Ning;Miao Jiang;Cheng Lu;Aiping Lu	2014	JSW		text mining;computer science;bisimulation;programming language	AI	-39.08254519738139	6.930233472633776	187712
f11aff6f13b67963a0bf5eb2463340cf763bad78	w-ace: a logic language for intelligent internet programming	client server systems logic programming languages internet knowledge based systems constraint handling;knowledge based system;client server systems;logic programming internet competitive intelligence world wide web web sites delay protocols intelligent agent knowledge based systems energy management;internet;communication protocol;constraint handling;world wide web;internet application;logic programs;logic programming languages;knowledge based systems;concurrent applications w ace logic language intelligent internet programming world wide web intelligent applications communication protocols low level communication mechanisms logic programming system knowledge based systems software agents constraint based management passive views active views;knowledge base	The development of the World Wide Web (WWW) has been considerably delayed due to the excessive complexity of developing advanced and intelligent applications for the Internet. An average application may require the use of different languages, an in-depth understanding of various communication protocols and low-level communication mechanisms, etc. We propose a logic programming system, called W-ACE, extended with various features to support natural and efficient development of Internet tools. The nature of the constructs introduced makes it particularly suitable to support intelligent Internet applications (knowledge-based systems, agents, etc.). W-ACE covers various issues in supporting knowledge-based handling of the World Wide Web, allowing structured and constraint-based management of WWW information, passive and active views of WWW, as well as a powerful support for concurrent applications. Various examples of complex intelligent applications are presented, to underline the simplicity and the power of the proposed ideas.	ace	Enrico Pontelli;Gopal Gupta	1997		10.1109/TAI.1997.632229	communications protocol;the internet;intelligent decision support system;computer science;artificial intelligence;theoretical computer science;knowledge-based systems;database;distributed computing;programming paradigm;inductive programming;fifth-generation programming language;programming language;logic programming	AI	-37.98257137148299	9.142475696379247	188244
bb4d0d346faacfb144d38a4a6b44e74453a4dca1	a web-based filtering engine for understanding event specifications in large conceptual schemas	filtering;large schemas;importance;conference lecture;event types	A complete conceptual schema must include all relevant general static and dynamic aspects of an information system. Event types describe a nonempty set of allowed changes in the population of entity or relationship types in the domain of the conceptual schema. The conceptual schemas of many real-world information systems that include the specification of event types are too large to be easily managed or understood. There are many information system development activities in which people need to understand the effect of a set of events. We present an information filtering tool in which a user focuses on one or more event types of interest for her task at hand, and the tool automatically filters the schema in order to obtain a reduced conceptual schema that illustrates all the elements affected by the given events.	conceptual schema;information filtering system;information system	Antonio Villegas;Antoni Olivé;Maria-Ribera Sancho	2012		10.1007/978-3-642-33999-8_45	filter;computer science;three schema approach;conceptual schema;data mining;database;information retrieval	DB	-36.93166888002355	9.279466192995626	190106
ea5f31f6bba8e4ed4d42a7a3c97049b8a8d9ac37	viquel: a spatial query language for presentation-oriented documents	context free grammars information extraction wrapping qualitative spatial reasoning;grammar;hypermedia markup languages;viquel;wrapping;query language;web pages;information extraction;qualitative spatial reasoning;query languages hypermedia markup languages;context free grammars;presentation oriented document;pdf document spatial query language viquel presentation oriented document web page information acquiring spatial arrangement html;spatial query language;data mining;information acquiring;pdf document;query languages;spatial arrangement;expressive power;html;visualization;context free grammar;grammar visualization html web pages data mining database languages wrapping;web page;database languages	In last years the huge relevance of accessing and acquiring information made available by Web pages and business documents has grown much further. Thus, wrapping information from documents in HTML and PDF formats is receiving increasing interest. In this paper we present a textual query language, named ViQueL, that allows for querying information in both Web and PDF documents on the base of its spatial arrangement. The proposed language is founded on spatial grammars, i.e. context free grammars extended by spatial constructs. The main feature of ViQueL is that it make possible to identify and extract relevant information from HTML and PDF documents on the base of their visual appearance by using easy-to-write queries. Despite a considerable expressive power, combined complexity of ViQueL is in P-Time. Moreover, experiments show that ViQueL is reasonably efficient for real life extraction tasks.	cyk algorithm;context-free grammar;experiment;formal grammar;html;portable document format;query language;real life;relevance;spatial query;spatial–temporal reasoning;web page;whole earth 'lectronic link;wrapping (graphics)	Ermelinda Oro;Francesco Riccetti;Massimo Ruffolo	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.121	natural language processing;computer science;database;programming language;world wide web;information extraction;query language	DB	-34.160885735142095	5.600714933080856	190183
37a2e3feb33559a7cd45bea4716ae086e59ff6aa	mining associations in text in the presence of background knowledge	query language;query processing;graphical interface;information retrieval;background knowledge;artificial intelligence;algorithms;data base management;programming languages;knowledge base;knowledge discovery;mathematics computers information science management law miscellaneous	This paper describes the FACT system for knowledge discovery from text. It discovers associations − patterns of co-occurrence − amongst keywords labeling the items in a collection of textual documents. In addition, FACT is able to use background knowledge about the keywords labeling the documents in its discovery process. FACT takes a query-centered view of knowledge discovery, in which a discovery request is viewed as a query over the implicit set of possible results supported by a collection of documents, and where background knowledge is used to specify constraints on the desired results of this query process. Execution of a knowledge-discovery query is structured so that these background-knowledge constraints can be exploited in the search for possible results. Finally, rather than requiring a user to specify an explicit query expression in the knowledge-discovery query language, FACT presents the user with a simple-to-use graphical interface to the query language, with the language providing a well -defined semantics for the discovery actions performed by a user through the i nterface.	graphical user interface;query language;zero-knowledge proof	Ronen Feldman;Haym Hirsh	1996			sargable;knowledge base;query optimization;query expansion;web query classification;ranking;computer science;artificial intelligence;query by example;machine learning;data mining;graphical user interface;database;rdf query language;knowledge extraction;web search query;information retrieval;query language;object query language	DB	-34.479672776349794	5.202116919245565	190219
fb6ba61dbf0b28f3462be102d4d65fca8e4dfe99	storhm: a protocol adapter for mapping soap based web services to restful http format	migration tool;wsdl;information technology;soap;uri;rest;web servcices;hypermedia;thesis;configuration wizard;web services;restful http;uniform interface;semantic web;protocol adapter	A protocol adapter ideally suited to enable enterprises to gradually transition from SOAP Web Services to RESTful HTTP Web Services without impacting existing clients is presented in this paper. The inherent advantage of such a transition is the visibility of RESTful HTTP messages to Web intermediaries such as caches. In contrast, SOAP messages are opaque, which disables Web intermediaries. While both approaches can use HyperText Transfer Protocol (HTTP) for message transfer, the paradigms contrast sharply. SOAP uses an interface specific approach whereas RESTful HTTP uses a Uniform Interface approach. SOAP marks up its payload with eXtensible Markup Language (XML) whereas in certain situations RESTful HTTP requires no XML. We present the disadvantages of the SOAP approach and outline how the RESTful HTTP approach solves these issues. We present results showing opaque SOAP messages transformed into transparent RESTful HTTP messages. We present StoRHm (SOAP to RESTful HTTP mapping), a protocol adapter which maps SOAP messages to RESTful HTTP format.		Sean Kennedy;Robert Stewart;Paul Jacob;Owen Molloy	2011	Electronic Commerce Research	10.1007/s10660-011-9075-3	web service;uniform resource identifier;computer science;web api;semantic web;soap;multimedia;rest;internet privacy;law;information technology;world wide web;simple soap binding profile	Networks	-40.52276238845444	11.128195188362495	190437
f92a917f93cf9e391de51eadeca2e82d19293525	a capability matching and ontology reasoning method for high precision ogc web service discovery	capability matching;search engine;owl s;ontology reasoning;web service;link detection;distributed environment;geospatial information service;information service;ogc web services	(2011): A capability matching and ontology reasoning method for high precision OGC web service discovery, This article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims, proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material. Finding the right spatially aware web service in a heterogeneous distributed environment using criteria such as service type, version, time, space, and scale has become a challenge in the integration of geospatial information services. A new method for retrieving Open Geospatial Consortium (OGC) Web Service (OWS) that deals with this challenge using page crawling, link detection, service capability matching, and ontology reasoning, is described in this paper. Its major components are distributed OWS, the OWS search engine, the OWS ontology generator, the ontology-based OWS catalog service, and the ontology-based multi-protocol OWS client. Experimental results show that the execution time of this proposed method equals only 0.26 of that of Nutch's method. In addition, the precision is much higher. Moreover, this proposed method can carry out complex OWS reasoning-based queries. It is being used successfully for the Antarctica multi-protocol OWS portal of the Geo-Information Web Service Portal of the Polar.	apache nutch;consortium;primary source;run time (program lifecycle phase);service discovery;web search engine;web service	Nengcheng Chen;Zeqiang Chen;Chuli Hu;Liping Di	2011	Int. J. Digital Earth	10.1080/17538947.2011.553688	web service;computer science;data mining;database;world wide web;owl-s;search engine;distributed computing environment;web coverage service	Web+IR	-40.067203586099446	5.576166595051341	190449
74ea22ba8bc3c7c65456a1609f58d129593e62c3	osmpg2java - konvertierung von osm-datenbankelementen zu jts-objekten		In this work a tool that transforms database elements of the OpenStreetMap project into Java JTS objects is presented. The key aspect is to resolve the OSM (multi-)polygon relations, as those are not stored explicitly with their geometry in the database tables by default. The conversion tool called “osmpg2java” extracts all the database elements and for relations of type “multipolygon” it is able to assemble all required parts to build valid JTS objects. The generated (multi-)polygon geometries are stored into the database, allowing mathematical and geometric operations to be performed on the geometry field, either directly in the database or especially in Java itself.	java topology suite (jts);openstreetmap;table (database)	Doris Silbernagl;Nikolaus Krismer;Günther Specht	2016	AGIT Journal	10.14627/537622026		DB	-35.82017390825474	8.643477682903102	190501
b76cb1100d765e489b8d3c73ccc19d1fb89c7d3b	a comparison of rdf query languages	query language;red www;web semantique;interrogation base donnee;reseau web;interrogacion base datos;resource description framework;lenguaje interrogacion;internet;web semantica;specification rdf;semantic web;world wide web;modele donnee;langage interrogation;database query;data models	The purpose of this paper is to provide a rigorous comparison of six query languages for RDF. We outline and categorize features that any RDF query language should provide and compare the individual languages along these features. We describe several practical usage examples for RDF queries and conclude with a comparison of the expressiveness of the particular query languages. The use cases, sample data and queries for the respective languages are available on the web[6].	categorization;rdf query language;resource description framework	Peter Haase;Jeen Broekstra;Andreas Eberhart;Raphael Volz	2004		10.1007/978-3-540-30475-3_35	rdf/xml;data modeling;query optimization;web query classification;the internet;computer science;sparql;semantic web;rdf;database;rdf query language;web search query;world wide web;information retrieval;query language;rdf schema	DB	-33.73319065140291	6.91296867386729	191689
a01664f2cf99e025a2506ae203fb3ec4907e3fce	wysiwye: an algebra for expressing spatial and textual rules for information extraction		The visual layout of a webpage can provide valuable clues for certain types of Information Extraction (IE) tasks. In traditional rule based IE frameworks, these layout cues are mapped to rules that operate on the HTML source of the webpages. In contrast, we have developed a framework in which the rules can be specified directly at the layout level. This has many advantages, since the higher level of abstraction leads to simpler extraction rules that are largely independent of the source code of the page, and, therefore, more robust. It can also enable specification of new types of rules that are not otherwise possible. To the best of our knowledge, there is no general framework that allows declarative specification of information extraction rules based on spatial layout. Our framework is complementary to traditional text based rules framework and allows a seamless combination of spatial layout based rules with traditional text based rules. We describe the algebra that enables such a system and its efficient implementation using standard relational and text indexing features of a relational database. We demonstrate the simplicity and efficiency of this system for a task involving the extraction of software system requirements from software product pages.	html;information extraction;relational database;requirement;seamless3d;software system;system requirements;text-based (computing);web page	Vijil Chenthamarakshan;Prasad Deshpande;Raghu Krishnapuram;Ramakrishna Varadarajan;Knut Stolze	2012		10.1007/978-3-642-32281-5_41	computer science;artificial intelligence;machine learning;data mining;database;programming language;information retrieval	DB	-34.89741166641303	6.06900471447741	191938
9a1cfcb7df4fa456cc79896f9f5885f845c0d340	multilingual food and health ontology learning using semi-structured and structured web data sources	wikipedia;multilingual ontology;food and health;ontology learning;resource allocation data structures internet learning artificial intelligence ontologies artificial intelligence;culture relevant aspects multilingual food ontology learning health ontology learning semistructured web data sources structured web data sources open web based information sources wikipedia multilingual domain ontologies semantic manipulation multilingual online resources monolingual ontologies language agnostic ontology;multilingual ontology ontology learning wikipedia food and health	The availability of open Web-based information sources such as Wikipedia in semi-structured format made it possible to build or extend multilingual domain ontologies. This work is a part of a project where we are developing a framework for semantic manipulation of health and nutrition information. In this paper, we present ongoing work aiming to build such ontology utilizing Wikipedia and other multilingual online resources automatically. The constructed ontology consists of monolingual ontologies and language-agnostic ontology connecting them. The constructed ontology is built to capture the culture-relevant aspects for each language based on the available concepts for each language. The ontology then could be used in many applications such as cross-lingual information access and multilingual information extraction for the domains of Food and Health. An initial evaluation shows the effectiveness and correctness of the constructed ontology.	correctness (computer science);information access;information extraction;language-independent specification;ontology (information science);ontology learning;open web;semiconductor industry;wikipedia	Saeed Al-Bukhitan;Tarek Helmy	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.240	natural language processing;upper ontology;open biomedical ontologies;ontology alignment;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;brand;ontology-based data integration;world wide web;owl-s;information retrieval;process ontology;suggested upper merged ontology	AI	-40.11068777577682	6.256411800158431	192438
4db7ebd01e38785907b699041017b62e69e76b1c	on visualizing the semantic web in ms office	information resources;search engines;online front ends data visualisation office automation semantic networks inference mechanisms information resources;inference mechanisms;ontobroker;resource description framework;semantic web visualisation;web service;semantic networks;microsoft office;2 dimensional;damlvisio;online front ends;data visualisation;visualization semantic web object oriented modeling ontologies microstrip shape resource description framework web services search engines computer aided software engineering;visualization;computer aided software engineering;semantic net;shape;object name look up;visualization technique;semantic nets;web services;eccma ontologies;microsoft office rdfs based semantic web knowledge web services semantic nets microsoft visio semtalk object name look up wordnet ontobroker inference engine eccma ontologies 2d visualisation techniques damlvisio semantic web visualisation;semantic web;microstrip;microsoft visio;ontologies;wordnet;semtalk;inference engine;2d visualisation techniques;rdfs based semantic web knowledge web services;object oriented modeling;office automation	This article gives an impression how existing RDFS based Semantic Web knowledge web services can already now be integrated in the creation process of semantic nets in MS Visio with SemTalk. We are presenting two examples: looking up object names in WordNet and using Ontobroker as an inference engine on ECCMA [11] ontologies. This article shows how different 2-dimensional visualization techniques such as DAMLVisio [5] or simple Visio shapes can be applied to semantic web models in order to address a broad range of users with very different expectations on the notation.	inference engine;ms-dos;microsoft visio;ontology (information science);rdf schema;semantic web;semantic network;web service;wordnet	Christian Fillies;York Sure-Vetter	2002		10.1109/IV.2002.1028811	semantic computing;semantic web rule language;data web;web standards;computer science;semantic web;social semantic web;semantic web stack;database;world wide web;information retrieval;semantic analytics	Web+IR	-38.494483030618305	8.882783647882999	193321
03f787cefb73e7036b8de99e226b51952d32bcff	faset: a set theory model for faceted search	libraries;faceted classification;model faceted search set theory;web databases;user interface;search engines;search algorithm;set theory;faceted search;expressive power;efficient implementation;intelligent agent;model;ranking algorithm;relational databases;power system modeling;set theory intelligent agent user interfaces relational databases deductive databases libraries search engines service oriented architecture conferences power system modeling;service oriented architecture;data structure;user interfaces;conferences;deductive databases	Faceted classification is a technique originated and refined in the library science field, that recently gained a lot of attention for creating efficient search interfaces for web databases. Faceted search requires the definition of a formal representation model, a search algorithm and a responsive user interface. This paper proposes FaSet, a representation model and search algorithm supporting the implementation of faceted search engines. FaSet relies on set theory, and strikes a good balance between expressive power and ease of implementation on web architectures. The paper presents the formal definition of the model, search and ranking algorithms, and a relational mapping of data structures and algorithms that enables its efficient implementation.	data structure;database;faceted classification;library science;search algorithm;set theory;user interface;web search engine	Dario Bonino;Fulvio Corno;Laura Farinetti	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.81	beam search;data structure;computer science;artificial intelligence;data mining;database;user interface;world wide web;intelligent agent	DB	-37.939800403472155	8.880570117761978	194170
a34b8c8f9b87b9c48d15bc0f62a8a1aec865556c	observer: an approach for query processing in global information systems based on interoperation across pre-existing ontologies	query language;information systems;query processing;selected works;information retrieval;heterogeneous data;information content;global information systems;query processing information systems ontologies navigation database languages information retrieval vocabulary logic information filtering computer science;bepress;interontology relationships observer query processing global information systems interoperation existing ontologies heterogeneous data repositories global information infrastructure locations structure organization query languages semantics information retrieval techniques information content vocabularies scalable approach vocabulary sharing intensional descriptions description logics user queries semantics preserving translations;description logic	"""The huge number of autonomous and heterogeneous data repositories accessible on the """"global information infrastructure"""" makes it impossible for users to be aware of the locations structure/organization, query languages and semantics of the data in various repositories. There is a critical need to complement current browsing, navigational and information retrieval techniques with a strategy that focuses on information content and semantics. In any strategy that focuses on information content, the most critical problem is that of different vocabularies used to describe similar information across domains. We discuss a scalable approach for vocabulary sharing. The objects in the repositories are represented as intensional descriptions by preexisting ontologies expressed in Description Logics characterizing information in different domains. User queries are rewritten by using interontology relationships to obtain semantics preserving translations across the ontologies."""		Eduardo Mena;Vipul Kashyap;Amit P. Sheth;Arantza Illarramendi	1996		10.1109/COOPIS.1996.554955	query expansion;description logic;global information system;self-information;cognitive models of information retrieval;computer science;artificial intelligence;information filtering system;concept search;data mining;database;information quality;information retrieval;information system;query language	DB	-37.503463354463	5.806617609834522	194214
5075efdddf34422c52633e8267915656d04e7b2b	semantic caching for web based learning systems	modelizacion;teleenseignement;query processing;traitement requete;web semantique;interrogation base donnee;interrogacion base datos;semantics;cache memory;semantica;semantique;antememoria;modelisation;antememoire;web based learning;web semantica;semantic web;teleensenanza;tratamiento pregunta;remote teaching;modeling;database query	For overcoming the existing problems in Web based learning system when network blocking or disconnection happens, a new architecture of the Web based learning system based on semantic caching is proposed; also, the model and the query processing mechanism of semantic caching are described in detail. Furthermore, this paper proposed and proved 11 simplification rules, which can be used to optimize the query processing of semantic caching, and the optimized query-processing algorithm is thoroughly described.	cache (computing)	Xiao-Wei Hao;Tao Zhang;Lei Li	2005		10.1007/11528043_22	semantic computing;systems modeling;cpu cache;computer science;semantic web;social semantic web;data mining;database;semantics;world wide web	NLP	-36.81088576985826	11.16645241831481	194355
e10bd15e157d5df54ef1e01bde3dc4de67c2905c	authoring and annotation of desktop files in semouse		Coping with an increasing number of files is one of the challenges of current desktops. Adding semantic capabilities is one possible solution. Aligned with this proposal, this work introduces the notion of “knowledge folder” as a coarse set of documents bound together by a common ontology. The ontology plays the role of a clipboard which can be transparently accessed by the file editors to either export (i.e. annotation) or import (i.e. authoring) metadata within the knowledge folder. Traditional desktop operations are now re-interpreted and framed by this ontology: copy&paste becomes annotation&authoring, and folder digging becomes property traversal. However, a desktop setting requires seamless tooling for these ideas to get through. To this end, this work proposes the use of the mouse as the “semantic device”. Through the mouse, the user can classify, annotate, author, and locate a file as a resource of the underlying ontology. Moreover, being editor-independent, the mouse accounts for portability and maintainability to face the myriad of formats and editors which characterizes current desktops. The “semantic mouse” is implemented as a plug-in for Windows.	clipboard (computing);desktop computer;folder lock;microsoft windows;plug-in (computing);seamless3d	Oscar Díaz;Jon Iturrioz;Sergio Fernández Anzuola	2005			computer science;data mining;database;world wide web	Web+IR	-40.39691051702497	10.08884936929162	194582
038d112abc4dbb34cdeff1e21023bcc4055c57ac	experiences in computer assisted xml-based modelling	data visualisation;extensible markup language;xml database;database design;user interface;graphic user interface;document type definition;xml document	Understandingthestructureandfunctionalityof thedomainof interestand representingthemin an intuitive, conceptualform is crucial in any effort to manageinformation about the domain. We adopt the view that documents containinformationaboutthedomainof applicationin astructuralform. However, thestructurein documentsis oftenlooseandmodellingandmanagement toolsareseldomusedin connectionwith document. In anidealsituation,all theorganization’ s documentsarestoredin anelectronic form in thesamedatabase.In that casewe saythesedocumentsform theorganization’ s documentdatabase. ExtensibleMarkupLanguage(XML) is now awidely acceptedformatfor electronicdocuments.XML documentype definitionsareusedto requirea specificstructureof documents.In our opinion, creatingdocumenttypedefinitionscorrespondsto conceptual andlogical databasedesignin adatabasedesignprocess.Weconsiderthatthisdesigncan besupportedwith a suitablesetof tools thathelp thedesignerconcentrateon conceptual issuesinsteadof implementationissues. In thispaper , weintroduceasoftwarecalledMetaDataVisualisation(MDV) that(i) assiststheuserwith agraphicaluserinterfacein thecreationof hisspecific documentypes,(ii) createsadatabaseaccordingto thesedocumentypes, (iii) allows theuserto browsethedatabase, and(iv) usesnative XML presentationof thedatain orderto allow queriesor datato beexportedto otherXML basedsystems. Our hypothesisis that usingthe methodologypresentedin this paperwe gainXML databases thatareusefulandrelevant,andwith whichMDV works asauserinterface.	database;xml	Marko Niinimäki;Vesa Sivunen	2003			well-formed document;discrete mathematics;mathematics;streaming xml;database;xml catalog;simple api for xml;xml framework;xml validation;xml schema editor;efficient xml interchange	DB	-35.73896852035017	8.539918393659978	195133
ca7f2c713aa2c6ea689fbec7257132eae9ff6cc8	efficient image recovery using visual and semantic contents	semantic data image recovery semantic contents visual contents low level descriptors three level software architecture object relational database image queries mpeg 7 standard descriptors semantic metadata rdf triplets semantic queries;resource description framework semantics image color analysis animals silicon compounds silicon noise measurement;semantic image retrieval object relational database;software architecture;software architecture image retrieval meta data relational databases;meta data;relational databases;image retrieval	Images recovery by combining low level descriptors and semantic content is a big challenge. In this work we present a three level software architecture implemented in an Object-Relational database that allows the image queries using both descriptions. For the physical level (low-level) MPEG-7 standard descriptors are employed and for the semantic metadata RDF triplets are used. The tests performed using combined low-level and semantic queries gave very good results in terms of images recovered. Since the semantic data can involve a huge number of RDF triplets, we are proposed and indexation structure for the triplets which shows a very good performance compared with other approaches.	high- and low-level;las vegas algorithm;linear algebra;lo que tú quieras oír;mpeg-7;object-relational database;plan 9 from bell labs;power-on reset;relational database;resource description framework;sql;software architecture;unique name assumption	Carlos E. Alvez;Aldo R. Vecchietti	2012	2012 XXXVIII Conferencia Latinoamericana En Informatica (CLEI)	10.1109/CLEI.2012.6427147	semantic data model;semantic similarity;semantic computing;semantic grid;image retrieval;computer science;data mining;semantic web stack;database;semantic equivalence;semantic technology;information retrieval;semantic analytics	Vision	-39.08269472355882	7.178195374987791	195160
3710ff1ddfe5c5aeb2d21bb82fb4213575b93300	geographic data access in an ontology-based peer data management system	query reformulation;obda;pdms;query answering	In distributed data environments, ontologies have been used as a support for data management. For instance, ontologies may be used to describe the semantics of data at different sources, helping to overcome problems of data heterogeneity and semantic interoperability. Generally, the task of accessing data by means of conceptual ontologies has been called Ontology-based Data Access (OBDA). A typical scenario for OBDA instantiation is a Peer Data Management System (PDMS) where queries submitted at a peer are answered with data residing at that peer and with data acquired from neighbor peers through the use of mappings. In this work, we apply the principles underlying an ODBA in the light of a PDMS, using geographic databases as data sources. When dealing with geospatial data, specific problems regarding query answering and data visualization may occur. To help matters, we propose an approach named easeGO, which provides access to a geographic database using an ontology. We also present a tool, which allows users to formulate queries using the peer ontology as well as visual elements and spatial operators. We present the principles underlying our approach, examples illustrating how it works and some obtained experimental results.	data access;management system	Rafael Figueiredo;Daniela Pitta;Ana Carolina Salgado;Damires Souza	2013	JIDM		computer science;data mining;database;ontology-based data integration;information retrieval	DB	-36.80110185940504	5.685113188157373	195266
4a2065b57f99e6ddd756089391665c35a35124f1	minerva: a scalable owl ontology storage and inference system	estensibilidad;distributed system;base relacional dato;reponse temporelle;regle inference;entreprise;representacion conocimientos;ontologie;haute performance;systeme reparti;universite;storage system;management system;systeme grande taille;ingenierie connaissances;web semantique;empresa;knowledge management;interrogation base donnee;service web;logica descripcion;interrogacion base datos;practical reasoning;large scale system;relational database;web service;vista materializada;materialized view;inference rule;large scale;sistema repartido;time response;web semantica;systeme memoire;firm;inferencia;representation connaissance;semantic web;alto rendimiento;base de donnees relationnelle;ontologia;university;extensibilite;scalability;information system;description logic;knowledge representation;respuesta temporal;sistema memoria;universidad;high performance;high efficiency;ontology;database query;systeme information;inference;sistema gran escala;servicio web;regla inferencia;logique description;sistema informacion;vue materialisee;knowledge engineering	With the increasing use of ontologies in Semantic Web and enterprise knowledge management, it is critical to develop scalable and efficient ontology management systems. In this paper, we present Minerva, a storage and inference system for large-scale OWL ontologies on top of relational databases. It aims to meet scalability requirements of real applications and provide practical reasoning capability as well as high query performance. The method combines Description Logic reasoners for the TBox inference with logic rules for the ABox inference. Furthermore, it customizes the database schema based on inference requirements. User queries are answered by directly retrieving materialized results from the back-end database. The effective integration of ontology inference and storage is expected to improve reasoning efficiency, while querying without runtime inference guarantees satisfactory response time. Extensive experiments on University Ontology Benchmark show the high efficiency and scalability of Minerva system.	abox;algorithm;benchmark (computing);database schema;description logic;experiment;inference engine;jing;knowledge management;lu decomposition;logic programming;norm (social);ontology (information science);relational database;relational database management system;requirement;response time (technology);scalability;semantic web;semantic reasoner;subsumption architecture;tbox;web ontology language;x image extension;yang	Jian Zhou;Li Ma;Qiaoling Liu;Lei Zhang;Yong Yu;Yue Pan	2006		10.1007/11836025_42	materialized view;web service;practical reason;description logic;scalability;ontology inference layer;relational database;computer science;semantic reasoner;ontology;artificial intelligence;semantic web;knowledge engineering;ontology;data mining;management system;database;information system;rule of inference	AI	-36.86755664368753	10.994601840315347	195464
10f3f71e8970f5b704441449419dfbe6f5d7b0df	completing queries: rewriting of incomplete web queries under schema constraints	query language;rewrite rule;query evaluation;semantic web;query rewriting;data transfer	Web queries have been and will remain an essential tool for accessing, processing, and, ultimately, reasoning with data on the Web. With the vast data size on the Web and Semantic Web, reducing costs of data transfer and query evaluation for Web queries is crucial. To reduce costs, it is necessary to narrow the data candidates to query, simplify complex queries and reduce intermediate results. This article describes a static approach to optimization of web queries. We introduce a set of rules which achieves the desired optimization by schema and type based query rewriting. The approach consists in using schema information for removing incompleteness (as expressed by ‘descendant’ constructs and disjunctions) from queries. The approach is presented on the query language Xcerpt, though applicable to other query languages like XQuery. The approach is an application of rules in many aspects—query rules are optimized using rewriting rules based on schema or type information specified in grammar rules.	algorithm;database schema;experiment;heuristic;mathematical optimization;query language;rewriting;sql;semantic web;semiconductor industry;world wide web;xquery	Sacha Berger;François Bry;Tim Furche;Andreas J. Häusler	2007		10.1007/978-3-540-72982-2_26	query optimization;query expansion;web query classification;computer science;artificial intelligence;semantic web;data mining;database;rdf query language;web search query;rewrite engine;information retrieval;query language;spatial query	DB	-34.85717440064545	5.488090915673447	195514
45e43fe15b7a25b035167d9cea87fb196253e580	query processing with description logic ontologies over object-wrapped databases	biology computing;bioinformatics query processing description logic ontologies object wrapped databases global schema declarative description vocabulary equivalent calculus expression expressive alcqi description logic query expression ontology definition global as view approach direct semantic optimization;object oriented databases query processing biology computing scientific information systems process algebra rewriting systems;query processing;rewriting systems;global as view;object oriented databases;description logic;process algebra;query processing logic ontologies proposals bioinformatics calculus spatial databases object oriented modeling algebra computer science;scientific information systems	This paper presents an approach to answering queries over an ontology modelled using a description logic. The ontology acts as a global schema, providing a declarative description of the concepts of the domain, the instances of which are stored in (potentially many) object-wrapped sources. Queries are expressed using terms from the rich vocabulary of the ontology, and are translated into an equivalent calculus expression, which references only the objects available in the source databases. The query is then optimized on the basis of information from the ontology and the source databases. Distinctive features of the approach include: the use of the expressive ALCQI description logic, which supports both ontology definition and query expression; the adoption of a global-as-view approach to relating the ontology to the sources; and the use of the ontology to direct semantic optimization of queries phrased over specific sources. The approach is being developed in, and is illustrated using examples from, bioinfo rmatics.	bioinformatics;database;description logic;interpreter (computing);mathematical optimization;ontology (information science);rich internet application;vocabulary	Martin Peim;Enrico Franconi;Norman W. Paton;Carole A. Goble	2002		10.1109/SSDM.2002.1029703	upper ontology;f-logic;sargable;query optimization;process calculus;description logic;ontology inference layer;computer science;ontology;theoretical computer science;database;rdf query language;ontology language;ontology-based data integration;information retrieval;process ontology;query language	AI	-34.81494526640774	8.312031466092577	195739
30d6aaf998e17ce875976e49fd912a2b437b4982	semantic grid resource discovery based on skos ontology		In large-scale distributed environments like dynamic Grids, an efficient and scalable resource discovery mechanism is required. Hybrid techniques based on node clustering are considered among solutions that guarantee the efficiency and the scalability at the same time. However, lack of a good representation of nodes in these environments often results in an irrelevant grouping of nodes. Combining hybrid techniques with Semantic Web technologies would bring more benefits to resource discovery. This paper introduces a semantic clustering of nodes based on their domains of interest to form groups called federations. It presents a new process for constructing federations and a three-layered overlay network. The proposed process is based on a SKOS lightweight ontology that describes domains of applications in the Grid. We conduct extensive simulations to evaluate the performance of the proposed approach; the experimental results demonstrate its efficiency and its ability to scale up with the system size.		Nabila Chergui;Salim Chikhi;M. Tahar Kechadi	2017	IJGUC	10.1504/IJGUC.2017.10009365	distributed computing;grid computing;computer science;simple knowledge organization system;ontology-based data integration;semantic grid;semantic web;upper ontology;overlay network;lightweight ontology	HPC	-37.194394673396545	5.6415986648178285	196158
ab1399d256a00f0800862d803ac07bfa37db9078	enhanced semantic access to the protein engineering literature using ontologies populated by text mining	owl dl ontologies;research papers;description logics;text mining;information retrieval;protein mutations;automated reasoning;protein mutation;ontology population;research paper;semantic web;full text papers;description logic;ontological nlp;protein engineering;natural language processing;formal ontology;biomedical literature;bioinformatics	The biomedical literature is growing at an ever-increasing rate, which pronounces the need to support scientists with advanced, automated means of accessing knowledge. We investigate a novel approach employing description logics (DL)-based queries made to formal ontologies that have been created using the results of text mining full-text research papers. In this paradigm, an OWL-DL ontology becomes populated with instances detected through natural language processing (NLP). The generated ontology can be queried by biologists using DL reasoners or integrated into bioinformatics workflows for further automated analyses. We demonstrate the feasibility of this approach with a system targeting the protein mutation literature.	bioinformatics;description logic;mutation;natural language processing;ontology (information science);paper;population;programming paradigm;protein engineering;text mining	René Witte;Thomas Kappler;Christopher J. O. Baker	2007	International journal of bioinformatics research and applications	10.1504/IJBRA.2007.015009	natural language processing;text mining;description logic;computer science;bioinformatics;data mining;information retrieval	Web+IR	-38.97129271191276	4.231366589237525	196649
d8a58c8aac2d40be94ad7d4a10a8cdd57870dfd9	quest - querying specialized collections on the web	description systeme;interfase usuario;search engine;system description;architecture systeme;red www;metadata;user interface;information retrieval;digital library;hidden web;web accessibility;web invisible;systematique;prototipo;motor investigacion;col;sistematica;recherche information;frankurt core;taxonomy;metadonnee;world wide web;arquitectura sistema;interface utilisateur;resource availability;reseau www;descripcion sistema;gwu;metadatos;recuperacion informacion;quest meta moteur;moteur recherche;system architecture;direct search;web technology;prototype;multibase	Ensuring access to specialized web-collections in a fast ev olving web environment requires flexible techniques for orientation a nd querying. The adoption of meta search techniques for web-collections is hinde re by the enormous heterogeneity of the resources. In this paper we introduce Q UEST — a system for querying specialized collections on the web. One focus of QUEST is to unify search fields from different col lections by relating the search concepts to each other in a concept-taxonomy. To identify the most relevant collections according to a user query, we propose a n association-based strategy. Furthermore the Frankurt Core is introduced — a me tadata-scheme for describing web-collections as a whole. Its fields are fill d automatically by a metadata-collector component. Finally a prototype of QUE ST is presented, demonstrating the integration of the techniques in an overa ll architecture.	dublin core;embedded system;experiment;german research centre for artificial intelligence;prototype;routing;world wide web	Martin Heß;Christian Mönch;Oswald Drobnik	2000		10.1007/3-540-45268-0_11	digital library;computer science;web accessibility;data mining;database;prototype;user interface;metadata;world wide web;search engine;taxonomy	DB	-38.1351500078294	11.014791345665891	196664
30047b4b01122734ecbd574b2794241384cb9e0e	towards the evaluation of the larkc reasoner plug-ins		In this paper, we present an initial framework of evaluation and benchmarking of reasoners deployed within the LarKC platform, a platform for massive distributed incomplete reasoning that will remove the scalability barriers of currently existing reasoning systems for the Semantic Web. We discuss the evaluation methods, measures, benchmarks, and performance targets for the plug-ins to be developed for approximate reasoning with interleaved reasoning and selection. In this paper, we propose a specification language of gold standards for the evaluation and benchmarking, and discuss how it can be used for the evaluation of reasoner plug-ins within the LarKC platform. keywords: Reasoning, Evaluation, Benchmarking, Web Scale Reasoning	approximation algorithm;benchmark (computing);scalability;semantic web;semantic reasoner;specification language;usability	Zhisheng Huang	2010			computer science;knowledge management;data mining;database	AI	-37.6757894997116	6.400190015812479	196930
c9b66c84b1f6e0a381558769acd79b072795eb9a	user demand description and optimization in web data management	content management;databases;demand description;web services content management optimisation query processing search engines sql;optimisation;user needs;service system;complexity theory;query processing;user demand set optimization;search engines;sql;optimal method;data management;wavelength division multiplexing databases optimization metasearch data models redundancy complexity theory;data model;metasearch;redundancy;web services;meta search engine;web data management;optimization;data management system;web data service system user demand description optimization object deputy database web data management system sql sentences meta search engine optimization method redundant query cost;data models;user demand set optimization web data management demand description;wavelength division multiplexing;wavelength division multiplex	Based on object deputy database, newly proposed web data management system (WDMS) provides user with personal data spaces to flexibly manage their various web data. Limited to database capacity, WDMS should gather data that user need from Web according to user demand implied in their personal data spaces. However, user demand that expressed in SQL sentences in data spaces can't be comprehended and executed by meta-search engine. Confronted with the problem, this paper proposed a user demand description method which is helpful to formalize user demand expression and bridge the gap between user demand and web data sources. Based on user demand description, this paper also proposed user demand set optimization method to eliminate the subset and intersection relationship among user demand set. The experimental results demonstrate that user demand description and its optimization method can well express complex user demand and reduce redundant query cost. This work can be widely used in various kinds of personal Web data service system.	database;dataspaces;information needs;mathematical optimization;personally identifiable information;program optimization;sql;user experience;web search engine	Weijie Ou;Zhiyong Peng;Weixiang Zhai;Ran Chen	2011	2011 Eighth Web Information Systems and Applications Conference	10.1109/WISA.2011.25	user interface design;web service;data modeling;sql;user modeling;computer user satisfaction;metasearch engine;data model;content management;computer science;data mining;database;redundancy;world wide web;wavelength-division multiplexing;service system	DB	-34.94022754558902	4.736935152380114	198004
20dd934a841f775ed18e0d2f8e887f363cbb9099	querying network directories	query language;hierarchical structure;beleif assertion;data model;expressive power;semi structured data;inheritance and overriding;object oriented database;mls database;reasoning;external memory algorithms;deductive databases	Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated “queries” involve navigational access. In this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.	algorithm;book;computable function;computer data storage;data model;database;directory (computing);expressive power (computer science);hoc (programming language);input/output;internet;query language;real life;semi-structured data;semiconductor industry	H. V. Jagadish;Laks V. S. Lakshmanan;Tova Milo;Divesh Srivastava;Dimitra Vista	1999		10.1145/304182.304194	semi-structured data;directory service;data model;computer science;theoretical computer science;data mining;database;programming language;expressive power;reason;query language	DB	-34.06248080696322	8.519527578688807	198083
17283a630e51342ce579bb7ec775009ca1621fb3	describing and manipulating xml data	data management;extensible markup language	"""This paper presents a brief overview of data management using the Extensible Markup Language (XML). It presents the basics of XML and the DTDs used to constrain XML data, and describes metadata management using RDF. It also discusses how XML data is queried, referenced, and transformed using stylesheet language XSLT and referencing mechanisms XPath and XPointer. 1 Describing XML Data The Extensible Markup Language (XML) [BPSM98] models data as a tree of elements that contain character data and have attributes composed of name-value pairs. For example, here is an XML representation of catalog information for a book: <book> <title>The spy who came in from the cold</title> <author>John <lastname>Le Carre</lastname></author> <price currency=""""USD"""">5.59</price> <review><author>Ben</author>Perhaps one of the finest...</review> <review><author>Jerry</author>An intriguing tale of...</review> <bestseller authority=""""NY Times""""/>"""	markup language;resource description framework;xml;xpath;xslt	Sudarshan S. Chawathe	1999	IEEE Data Eng. Bull.		streaming xml;xml;document type definition;xpath;database;document definition markup language;collaborative application markup language;computer science;xml schema editor;sgml	DB	-34.16812356326963	7.491403307590987	198163
b84406a1407d492a5978dba21105fdb7b59bf1c4	deriving intensional descriptions for web services	views;web services;schema mapping	Many data providers make their data available through Web service APIs. In order to unleash the potential of these sources for intelligent applications, the data has to be combined across different APIs. However, due to the heterogeneity of schemas, the integration of different APIs remains a mainly manual task to date. In this paper, we model an API method as a view with binding patterns over a global RDF schema. We present an algorithm that can automatically infer the view definition of a method in the global schema. We also show how to compute transformation functions that can transform API call results into this schema. The key idea of our approach is to exploit the intersection of API call results with a knowledge base and with other call results. Our experiments on more than 50 real Web services show that we can automatically infer the schema with a precision of 81%-100%.	algorithm;application programming interface;database schema;experiment;intensional logic;kilobyte;knowledge base;rdf schema;web service;world wide web	Maria Koutraki;Dan Vodislav;Nicoleta Preda	2015		10.1145/2806416.2806447	web service;computer science;star schema;data mining;database;view;world wide web;information retrieval	DB	-36.27267871414964	5.127062999334745	198233
b8f2b7ff407bdc1bc49c04f5f8c2e77a7c989397	ontology driven websites/metamorphosis: a framework to specify and manage ontology driven websites	xml document;physical layer	Website development has always been an hard task: it consumes time and resources. What is new today is normally taken as granted tomorrow by users. This is to say that users always want more. Today they want up to date information and they want to access it according to their point of view or particular preferences. To cope with these demands, websites must be dynamic and must be able to reconfigure automatically their structure, content and appearance. This scenery has favored the creation of tools for automatic generation and management websites. In this paper we propose not a new tool of this kind but a new approach to the problem. In our approach we consider two layers. A physical layer that we call the resources layer, composed by databases, XML documents, directory subtrees, and the whole sort of files you can think of to represent your information. A metadata layer called the ontology layer, that provides a view to those resources. Our framework consists of several parts. In this paper the focus will be the navigation component. This component takes an ontology and uses it to navigate through the resources layer. We are using XML technology to implement the whole framework and this component is implemented through an XML transformation process.	directory (computing);html;information system;megabyte;ontology (information science);point of view (computer hardware company);relational database;relational model;tree (data structure);user interface;web page;world wide web;xml	José Carlos Ramalho;Giovani Rubert Librelotto;Pedro Rangel Henriques	2003				Web+IR	-36.72094137426641	8.83166857503344	198459
28d831180b0d3fed8269697bf2f1f203d3a10d1a	peer-to-peer overlays and data integration in a life science grid	drug targeting;p2p;data grid;grid computing;data integrity;self organization;access control;data access	Databases and Grid computing are a good match. With the service orientation of Grid computing, the complexity of maintaining and integrating databases can be kept away from the actual users. Data access and integration is performed via services, which also allow to employ an access control. While it is our perception that many proposed Grid applications rely on a centralized and static infrastructure, Peer-to-Peer (P2P) technologies might help to dynamically scale and enhance Grid applications. The focus does not lie on publicly available P2P networks here, but on the self-organizing capabilities of P2P networks in general. A P2P overlay could, e.g., be used to improve the distribution of queries in a data Grid. For studying the combination of these three technologies, Grid computing, databases, and P2P, in this paper, we use an existing application from the life sciences, drug target validation, as an example. In its current form, this system has several drawbacks. We believe that they can be alleviated by using a combination of the service-based architecture of Grid computing and P2P technologies for implementing the services. The work presented in this paper is in progress. We mainly focus on the description of the current system state, its problems and the proposed new architecture. For a better understanding, we also outline the main topics related to the work presented here.	access control;centralized computing;data access;database;grid computing;organizing (structure);peer-to-peer;self-organization	Curt Cramer;Andrea Schafferhans;Thomas Fuhrmann	2004			drmaa;grid computing;architecture;grid;peer-to-peer;semantic grid;data integration;data mining;distributed computing;data grid;computer science	HPC	-37.26590828976022	5.50307778728756	199204
1390a9b57993356257e3af746050788cab8d7201	an ontology construction approach for the domain of poultry science using protege		The information retrieval systems that are present nowadays are mainly based on full text matching of keywords or topic based classification. This matching of keywords often returns a large number of irrelevant information and this does not meet the user’s query requirement. In order to solve this problem and to enhance the search using semantic environment, a technique named ontology is implemented for the field of poultry in this paper. Ontology is an emerging technique in the current field of research in semantic environment. This paper constructs ontology using the tool named Protégé versioned 4.0 and this also generates Resource Description Framework (RDF) schemas and XML scripts for using poultry ontology in web.	case-based reasoning;information retrieval;norsk data;protégé;relevance;resource description framework;statistical classification;xml	P. Kalaivani;A. Anandaraj;K. Raja	2012	CoRR		upper ontology;systems engineering;engineering;knowledge management;ontology;data mining;process ontology	Web+IR	-40.31505906278243	5.896471471407836	199585
69b056a3a0ee848c5a9a31263f720fa9e7343cb3	toward g-owl: a graphical, polymorphic and typed syntax for building formal owl2 ontologies	graphical ontology;graphical ontological syntax;owl 2;visual modeling;graphical syntax;ontology	The Web Ontology Language (OWL-2) aims at offering a family of syntax such as RDF/XML, Manchester Turtle and others, for building ontologies. Ontology engineering is a complex task that requires skills that are rarely accessible to content experts. On the other hand, to model contents pertaining to a specific domain, graphical modeling is a technique that is often used to offer a knowledge representation tool to content experts that are not well acquainted with the process of formal ontology design. In this paper, we present the way in which the usage of polymorphism and symbol typing of graphical vocabulary have allowed us to design the G-OWL syntax, a graphical syntax that aims to graphically represent domain-specific knowledge using the OWL-2.	ampersand;formal ontology;graphical user interface;knowledge representation and reasoning;ontology (information science);ontology engineering;rdf/xml;vocabulary;web ontology language;xml	Michel Héon;Roger Nkambou;Christian Langheit	2016		10.1145/2872518.2889377	natural language processing;upper ontology;abstract syntax;computer science;ontology;ontology;database;programming language;web ontology language;process ontology;abstract syntax tree;syntax error	HCI	-40.55040824371437	4.371613505589379	199628
90e7fee2cab07820fe6987ed14864e084701f655	language standardization for the semantic web: the long way from oil to owl	ontologie;red www;ontology interchange language;reseau web;semantics;oil;semantica;semantique;internet;semantic web;world wide web;ontology;internal standard	The major objective for the development of OIL was to develop an international standard for web-based representation of ontological information. This papers reports some lessons learnt in this process.	semantic web;web ontology language	Dieter Fensel	2002		10.1007/3-540-36261-4_20	web development;web modeling;the internet;semantic web rule language;data web;html;ontology inference layer;web standards;computer science;ontology;artificial intelligence;semantic web;web navigation;ontology;social semantic web;internal standard;semantic web stack;database;semantics;web intelligence;web 2.0;world wide web;owl-s;information retrieval	Web+IR	-39.076233918530576	8.27108987203253	199913
