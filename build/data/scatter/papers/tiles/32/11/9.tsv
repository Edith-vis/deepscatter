id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
848657cc5407311ea866e54d3b1e6fac0f3f983f	movement analysis for interactive dance using motion capture data part i: real-time tracking of multiple people from unlabelled markers			motion capture;real-time transcription	Daniel Whiteley;Gang Qian;Thanassis Rikakis;Jodi James;Todd Ingalls;Siew Wong;Loren Olson	2005		10.5244/C.19.74	computer vision;multimedia	Vision	-39.54461629007858	-38.31034082016906	161872
d169bad6a03d9e15358aa19c2823c7737cf88eaa	indoor positioning for visually impaired people based on smartphones		Autonomous navigation is a critical factor for visually impaired people. Outdoors, positioning based on ubiquitous signals is available, contrary indoors no ubiquitous navigation solution does exist. Because of the implementation of screen-reader software into mobile devices, visually impaired people start using smartphones. This paper focuses on the abilities of an indoor positioning purely based on sensors already present in smartphones nowadays. Therefore, algorithms specifically designed for low-cost sensors are developed. The outcome of these algorithms, which process the accelerometer, gyroscope, magnetometer and barometer data and a WiFi fingerprinting, is integrated within a mathematical filter to get a final position and heading information.		Thomas Moder;Petra Hafner;Manfred Wieser	2014		10.1007/978-3-319-08596-8_68	embedded system;accelerometer;software;computer science;mobile device;gyroscope	HCI	-39.64396991331423	-43.62345322149189	163209
3075dd0ac0b22c8c3ad830a88690fa56bb1b0a82	a user-adaptive gesture recognition system applied to human-robot collaboration in factories	depth camera;human robot collaboration;user adaptation;assembly line;gesture recognition	Enabling Human-Robot collaboration (HRC) requires robot with the capacity to understand its environment and actions performed by persons interacting with it. In this paper we are dealing with industrial collaborative robots on assembly line in automotive factories. These robots have to work with operators on common tasks. We are working on technical gestures recognition to allow robot to understand which task is being executed by the operator, in order to synchronize its actions. We are using a depth-camera with a top view and we track hands positions of the worker. We use discrete HMMs to learn and recognize technical gestures. We are also interested in a system of gestures recognition which can adapt itself to the operator. Indeed, a same technical gesture seems very similar from an operator to another, but each operator has his/her own way to perform it. In this paper, we study an adaptation of the recognition system by modifying the learning database with a addition very small amount of gestures. Our research shows that by adding 2 sets of gestures to be recognized from the operator who is working with the robot, which represents less than 1% of the database, we can improve correct recognitions rate by ~3.5%. When we add 10 sets of gestures, 2.6% of the database, the improvement reaches 5.7%.	cobot;gesture recognition;human–robot interaction;line level;robot	Eva Coupeté;Fabien Moutarde;Sotiris Manitsaris	2016		10.1145/2948910.2948933	computer vision;simulation;engineering;gesture recognition;communication	Robotics	-36.502142867017746	-40.445701408697786	163835
12ddda3567a32c07231b6795e0efd73ddaff5f0b	construction of human-robot cooperating system based on structure/motion model				Fumi Seto;Yasuhisa Hirata;Kazuhiro Kosuge	2006			control engineering;human–robot interaction;computer science	Robotics	-35.744816270486005	-39.50001300594396	164665
03d78bc600c5145e095a72bf73aba609bd17dcd2	an integral image and text processing system for automatic generation of 3d sign-language animations	motion analysis;natural language interfaces;japanese sign language;image motion analysis;3d sign language animation generation;image processing;text processing image analysis image motion analysis information processing animation fingers shape dictionaries text analysis motion analysis;3d virtual reality;sign language;multimodal information processing system;text processing;virtual reality;handicapped aids image processing text processing 3d sign language animation generation multimodal information processing system japanese 3d virtual reality hand movements finger shapes line images dictionaries image analysis;text analysis;automatic generation;hand movements;handicapped aids;shape;finger shapes;dictionaries;animation;information processing;fingers;line images;image processing computer animation natural language interfaces handicapped aids virtual reality dictionaries;image analysis;computer animation;japanese	This paper presents a new multi-modal information processing system for generating Japanese sign-language animations in a 3D virtual-reality space. In order to create sign-motions composed of hand movements and finger shapes, the system analyzes illustrative pictures(1ine images) and texts given in sign-language dictionaries. In addition to the individual image analysis and text analysis mutual augmentation and integration of the analyses are tried to obtain more precise motional data.	dictionary;image analysis;information processing;information processor;modal logic;virtual reality	T. Ozawa	2001		10.1109/ISCAS.2001.921070	natural language processing;computer vision;japanese;image analysis;speech recognition;sign language;information processing;image processing;shape;computer science;virtual reality	NLP	-35.76989746321635	-43.45046847711781	165051
d29072f91cf2d8012133c8a420e4ccab535ee08e	resolving ambiguities in a grounded human-robot interaction	adjective noun term;robot sensing systems;natural language interfaces;trainable system;information theoretic approach;natural language interfaces human robot interaction information theory learning systems;grounded language model learning;noun;verbal interaction;grounded meaning acquisition;human robot interaction;spatial term;learning systems;visualization;shape;user intervention;feature extraction;natural language;grounded human robot interaction;eventual ambiguity resolving;humans;natural language description;natural languages human robot interaction layout learning systems robot sensing systems intelligent robots robotics and automation computer science feedback spatial resolution;information theoretic;context;information theoretic approach grounded human robot interaction trainable system eventual ambiguity resolving grounded language model learning user intervention grounded meaning acquisition spatial term adjective noun term natural language description verbal interaction;language model;information theory	In this paper we propose a trainable system that learns grounded language models from examples with a minimum of user intervention and without feedback. We have focused on the acquisition of grounded meanings of spatial and adjective/noun terms. The system has been used to understand and subsequently to generate appropriate natural language descriptions of real objects and to engage in verbal interactions with a human partner. We have also addressed the problem of resolving eventual ambiguities arising during verbal interaction through an information theoretic approach.	construction grammar;emoticon;human–robot interaction;information theory;language model;natural language processing;robot;stellar classification;word-sense disambiguation	Haris Dindo;Daniele Zambuto	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326333	natural language processing;human–robot interaction;noun;visualization;information theory;feature extraction;shape;computer science;artificial intelligence;natural language;language model	Robotics	-33.96564033613448	-41.563084944948436	165435
0e9475c8e1d3354152e9ee5b27ee651adc887072	real-time panoramic mapping and tracking on mobile phones	real time panoramic mapping;environment maps;outdoor augmented reality;real time tracking;real time;panoramic images;real time systems augmented reality image enhancement mobile handsets;mobile phone;drift free rotation tracking;image enhancement;offline browsing;mobile phone panorama creation tracking;mobile handsets;visual enhancements;mobile handsets cameras robustness application software intelligent sensors accelerometers simultaneous localization and mapping augmented reality virtual reality computer vision;panoramic image;mobile phones;augmented reality;outdoor augmented reality real time panoramic mapping mobile phones real time tracking drift free rotation tracking panoramic images visual enhancements offline browsing;real time systems	We present a novel method for the real-time creation and tracking of panoramic maps on mobile phones. The maps generated with this technique are visually appealing, very accurate and allow drift-free rotation tracking. This method runs on mobile phones at 30Hz and has applications in the creation of panoramic images for offline browsing, for visual enhancements through environment mapping and for outdoor Augmented Reality on mobile phones.	augmented reality;bing maps;desktop computer;global positioning system;map;mobile phone;offline reader;real-time clock;real-time transcription;reflection mapping;side effect (computer science)	Daniel Wagner;Alessandro Mulloni;Tobias Langlotz;Dieter Schmalstieg	2010	2010 IEEE Virtual Reality Conference (VR)	10.1109/VR.2010.5444786	computer vision;augmented reality;computer science;multimedia;computer graphics (images)	Visualization	-39.3452011043998	-39.78095645979002	166827
7244be425d0789e716b1e3b2cffb9431652a99fe	indoor waypoint navigation via magnetic anomalies	magnetic fields;magnetic field;audio annotation indoor waypoint navigation magnetic anomaly indoor navigation service blind person vision impaired person proximity based system multilateration system indoor magnetic signatures steel frame buildings location system infrastructure magnetic indoor navigation wayfarer magnetometer reading waypoint information smartphone based application;magnetic sensors;data collection;magnetometers navigation magnetic sensors manganese magnetic fields indexes;cellular phone equipment design equipment failure analysis geographic information systems humans magnetometry self help devices sensory aids vision disorders;magnetometers;smart phones handicapped aids indoor radio magnetometers mobile computing radionavigation;magnetic sensor;smart phones;maintenance cost;radionavigation;manganese;indexes;navigation;handicapped aids;magnetic anomalies;indexation;indoor radio;navigation system;mobile computing	"""A wide assortment of technologies have been proposed to construct indoor navigation services for the blind and vision impaired. Proximity-based systems and multilateration systems have been successfully demonstrated and employed. Despite the technical success of these technologies, broad adoption has been limited due to their significant infrastructure and maintenance costs. An alternative approach utilizing the indoor magnetic signatures inherent to steel-frame buildings solves the infrastructure cost problem; in effect the existing building is the location system infrastructure. Although magnetic indoor navigation does not require the installation of dedicated hardware, the dedication of resources to produce precise survey maps of magnetic anomalies represents a further barrier to adoption. In the present work an alternative leader-follower form of waypoint-navigation system has been developed that works without surveyed magnetic maps of a site. Instead the wayfarer's magnetometer readings are compared to a pre-recorded magnetic """"leader"""" trace containing magnetic data collected along a route and annotated with waypoint information. The goal of the navigation system is to correlate the follower's magnetometer data with the leader's to trigger audio cues at precise points along the route, thus providing location-based guidance to the user. The system should also provide early indications of off-route conditions. As part of the research effort a smartphone based application was created to record and annotate leader traces with audio and numeric data at waypoints of interest, and algorithms were developed to determine (1) when the follower reaches a waypoint and (2) when the follower goes off-route. A navigation system utilizing this technology would enable a low-cost indoor navigation system capable of replaying audio annotations at precise locations along pre-recorded routes."""	antivirus software;audio media;level of measurement;multilateration;navigation;numbers;reading (activity);smartphone;steel;tracing (software);waypoint;algorithm;magnetometers	Timothy H. Riehle;Shane M. Anderson;Patrick A. Lichter;John P. Condon;Suneel I. Sheikh;Daniel S. Hedin	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091315	computer vision;simulation;magnetic field;telecommunications;computer science;electrical engineering;mobile computing;computer security;physics;quantum mechanics	HCI	-39.3330156975049	-43.37664125518143	167098
209c1d5cc7a14cffa1ba0c3ece7c5056c4734659	combatting rendering latency	bob;telepresence;force;scientific visualization;rendering system;just in time;atomic force microscopy;augmented reality;interactive graphics;haptic;scanning tunneling microscopy;teleoperation;virtual worlds;head mounted display	Latency or lag in an interactive graphics system is the delay between user input and displayed output. We have found latency and the apparent bobbing and swimming of objects that it produces to be a serious problem for head-mounted display (HMD) and augmented reality applications. At UNC, we have been investigating a number of ways to reduce latency; we present two of these. Slats is an experimental rendering system for our Pixel-Planes 5 graphics machine guaranteeing a constant single NTSC field of latency. This guaranteed response is especially important for predictive tracking. Just-in-time pixels is an attempt to compensate for rendering latency by rendering the pixels in a scanned display based on their position in the scan.	augmented reality;graphics;head-mounted display;interrupt latency;just-in-time compilation;ntsc;pixel;rendering (computer graphics)	Marc Olano;Jonathan D. Cohen;Mark R. Mine;Gary Bishop	1995		10.1145/199404.199407	computer vision;augmented reality;teleoperation;scientific visualization;atomic force microscopy;simulation;rendering;computer science;artificial intelligence;optical head-mounted display;scanning tunneling microscope;haptic technology;real-time rendering;alternate frame rendering;force;computer graphics (images)	Graphics	-40.60438236325228	-39.72841998614423	168204
247acb0b639c03b9b94bb5959d17ffe0ce0366a0	continuous control of avatar gesture	nonverbal communication;computer mediated communications;virtual environments;three dimensional;expressive power;computer mediated communication;avatars;virtual environment;networked virtual environment;face to face	We are developing an application to give humans the ability to transmit nonverbal communication behaviors through an avatar: specifically gesture, the movements of the arms and hands that accompany speech when people speak face-to-face. In this application the user will have continuous control over the avatar animation. The avatar will be like a virtual puppet and the user will manipulate the avatar using not strings or rods but the controlled and skilled motions of their hand. The system tracks hand motions and then maps that motion to the joint motions of a three-dimensional articulated avatar. As part of this research we will try out different ways of tracking the user's hand. Eventually we plan to test the efficacy of this system by incorporating it into a networked virtual environment in which two or more people can interact through the virtual medium. Working with artists will enable us to design a system that is expressive and to better understand the expressive power of this system.	avatar (computing);coat of arms;expressive power (computer science);map;virtual reality	Francesca Barrientos	2000		10.1145/357744.357747	simulation;computer science;multimedia;communication	HCI	-39.23290804872458	-38.03797079468444	169973
a17c3bce6528b9f86fd1761261b7145cee9388e4	interactive hand/eye coordination between a human and a humanoid-a proposal	touch interactive hand eye coordination binocular computer vision system robotic humanoid sophisticated human skill transference perception spatial reasoning sensor guided manipulation person to person communications eye contact body language speech;robotic humanoid;robot sensing systems;spatial reasoning;legged locomotion;intelligent robots;robot vision man machine systems legged locomotion;speech;human robot interaction;educational robots;binocular computer vision system;computer vision;eye contact;touch;robot vision;humanoid robots;robot sensing systems robot kinematics humanoid robots intelligent sensors intelligent robots computer vision educational robots human robot interaction robot vision systems education;sophisticated human skill transference;interactive hand eye coordination;body language;person to person communications;perception;sensor guided manipulation;man machine systems;robot vision systems;intelligent sensors;robot kinematics	This research concerns the development of a binocular computer vision system for a robotic humanoid using interactive hand/eye coordination as a focus for skills teaching and capability demonstrations. The humanoid form aids natural transference of sophisticated human skills of perception, spatial reasoning, sensor guided manipulation and person-to-person communications (eye contact, body language, speech, touch etc.) to the robot and simultaneously stimulates considerations of human centred applications requiring such skills. This research builds on a humanoid project undertaken within the Intelligent Robotics Research Centre at Monash University and funded internally in 1999. At this early stage of the project much of the material presented in this paper is speculative and part of the planning for an extended piece of research over a number of years.	interactivity	Ray A. Jarvis	2000		10.1109/IROS.2000.895223	human–robot interaction;computer vision;body language;simulation;computer science;engineering;humanoid robot;speech;artificial intelligence;spatial intelligence;eye contact;perception;educational robotics;robot kinematics;intelligent sensor	Vision	-35.89081202804399	-41.02656080577982	170124
3580cf157b5c6a7f84c723d5c12b80a519097d30	a consensual and non-ambiguous set of gestures to interact with uav in infantrymen	consensual gestures;non ambiguous gestures;uav in infantrymen	In the context of using an Unmanned Aerial Vehicle (UAV) in hostile environments, gestures allow to free the operator of bulky control interfaces. Since a navigation plan is defined before the mission, only a few commands have to be activated during the mission. This allows a gestural symbolic interaction that maps commands to a set of gestures. Nevertheless, as gestures are not universal, this asks the question of choosing the proper gestures that are easy to learn memorize and perform. We propose a four step methodology for eliciting a gestural vocabulary, and apply it to this use case. The methodology consists of 4 steps: (1) collecting gestures through user creativity sessions, (2) extracting candidate gestures to build a catalogue, (3) electing the gesture vocabulary and (3) evaluating the non-ambiguity of it. We then discuss the relevance of the GV.	aerial photography;gesture recognition;map;relevance;unmanned aerial vehicle;vocabulary	Florent Taralle;Alexis Paljic;Sotiris Manitsaris;Jordane Grenier;Christophe Guettier	2015		10.1145/2702613.2702971	computer vision;speech recognition;computer science	Robotics	-34.33793252769946	-41.264988629536205	170560
3a4f1c76dbf54c126d51cffb6762f518847879ce	multimodality and dialogue act classification in the robohelper project		We describe the annotation of a multimodal corpus that includes pointing gestures and haptic actions (force exchanges). Haptic actions are rarely analyzed as fullfledged components of dialogue, but our data shows haptic actions are used to advance the state of the interaction. We report our experiments on recognizing Dialogue Acts in both offline and online modes. Our results show that multimodal features and the dialogue game aid in DA classification.	dialog system;entity–relationship model;experiment;haptic technology;multimodal interaction;online and offline;principle of maximum entropy	Lin Chen;Barbara Di Eugenio	2013			computer vision;computer science;multimedia;communication	NLP	-36.92838999539578	-44.93850254972862	171041
3c6d9bf75c94e5b6c2b318ebf2087b3359c0a329	spatial resolution for robot to detect objects	pragmatics;object recognition;service robots human robot interaction mobile robots object detection robot vision;color;vocabulary;robotic system;service robots;mobile robots;interactive method;human robot interaction;books;robots pragmatics object recognition color humans vocabulary books;robot vision;robots;elderly people;humans;spatial information object detection spatial resolution robotic system interactive method;spatial information;object detection;spatial resolution	In this paper, we report on our development of a robotic system that assists people in accomplishing simple tasks in daily life (e.g., retrieving objects for handicapped and elderly people). These tasks, inevitably involve detecting various kinds of objects. In particular, here, we present an interactive method to detect objects using spatial information. Our experimental results confirm the usefulness and efficiency of our system. We also show how the approach can be improved and highlight necessary directions for future research.	experiment;human–robot interaction;interaction design;robot;sensor;spatial reference system	Lu Cao;Yoshinori Kobayashi;Yoshinori Kuno	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5651340	robot;mobile robot;computer vision;simulation;image resolution;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;spatial analysis;pragmatics	Robotics	-39.7677970741563	-44.10463920393839	171215
f56e56be1996c0233508c65cb31bee64239c05d7	augmented reality via temporal psycho-visual modulation	immersive virtual scene augmented reality temporal psycho visual modulation pattern recognition virtual object ar tag digital screen human eye perception digital camera image forming method;digital cameras;media;three dimensional displays;augmented reality modulation digital cameras mobile handsets three dimensional displays media;mobile handsets;augmented reality;mobile device augmented reality temporal psycho visual modulation perception;visual perception augmented reality image processing screens display;modulation	Nowadays, augmented reality (AR) has entered our lives. An AR tag is usually used and stacked on the products for sake of being detected and recognized easily. With the recognized pattern, AR system thereby can superimpose the corresponding virtual object on the AR tag. The AR tag, however, distort the perception of products. In this study, we proposed a method that the AR tag could be embedded on a digital screen by applying the technique of temporal psycho visual modulation (TPVM). The new method utilizes the differences between the human-eye perception and digital camera image-forming to stack an invisible AR tag on digital screen and projector. The AR tag can be detected by digital devices but not yet by human eyes. Based on this concept of AR and TPVM, the related applications would be achieved, such as the immersive virtual scene created in entertainment, commercial and entertainment.	augmented reality;bmc remedy action request system;digital camera;distortion;embedded system;modulation;video projector	Xiaoyong Lu;Bin You;Pei-Yu Lin	2016	2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2016.7574703	computer vision;augmented reality;computer-mediated reality;media;telecommunications;computer science;multimedia;computer graphics (images);modulation	Visualization	-40.76401003442977	-40.33268115428266	172051
934ba5f01cdc6dfe96967f9f92a270f550296768	mobile robotic teleoperation using gesture-based human interfaces	gesture based navigation mobile robotic teleoperation gesture based human interface device touch screen accelerometer image processing cellphone gaming device mobile robot programming architecture user interface;robot sensing systems;educational robotics;image processing;touch screen;mobile robot;user interface;mobile robots;human robot interaction;natural interaction;educational robots;computer architecture;navigation;human interface;research and development;telerobotics;robot sensing systems navigation collision avoidance computer architecture mobile robots educational robots;collision avoidance;user interaction;gesture recognition;interactive devices;telerobotics gesture recognition human robot interaction interactive devices mobile robots	In this work a natural interaction framework for programming a mobile robot with gestures is developed using two low-cost human interface devices available on the market. The use of natural motion has been growing among user interface researches and developers for offering comfortable ways of interacting with the devices around us. Some examples can be seen in how touch screens, accelerometers and image processing have influenced our interactions with cellphones, gaming devices and computers, thus, allowing us to take advantage of our body ergonomics. The objective of this work is to integrate an intuitive tool for teleoperating a mobile robot through gestures as an alternative input for encouraging non-experts to relate with robotics and ease navigation tasks. For accomplishing the objective of this work, the mobile robots programming architecture is studied and integrated with those of the user interfaces for allowing the teleoperation of the robotics device. To check how the implemented framework impacts the user interaction, a navigation scenario with obstacles is used for validating the suitability of the gesture-based navigation.	computer;gesture recognition;human factors and ergonomics;human interface device;image processing;interaction;kinect;mobile phone;mobile robot;sensor;touchscreen;user interface	Alvaro Joffre Uribe;Silas F. R. Alves;João Maurício Rosário;Humberto Ferasoli Filho;Byron Perez-Gutierrez	2011	IX Latin American Robotics Symposium and IEEE Colombian Conference on Automatic Control, 2011 IEEE	10.1109/LARC.2011.6086812	embedded system;computer vision;simulation;engineering;mobile robot navigation	Robotics	-40.92229517154677	-44.555513700355775	173965
beb95cb086bd9706e8a97cb2cb5498f7826707d1	robot control on the basis of bio-electrical signals	objeto de conferencia;robotics;bio electrical signal;human machine interfaces;brain machine interface;ciencias informaticas;user interfaces	Abstr act. This article shows the experiences carried out in the context of human/robot communication, on the basis of brain bio-electrical signals, with the application of the available technologies and interfaces which have facilitated the reading of the user’s brain bio-electrical signals and their association to explicit commands that have allowed the control of biped and mobile robots through the adaptation of communication devices. Our work presents an engineering solution, with the application of technological bases, the development of a highand low-level communication framework, the description of experiments and the discussion of the results achieved in field tests.	british informatics olympiad;experiment;high- and low-level;mobile robot;robot control	Jorge Salvador Ierache;Gustavo Pereira;Iris Sattolo;Juan Irribaren;Nicolás Suiffet	2012		10.1007/978-3-642-37374-9_33	embedded system;human–computer interaction;engineering;artificial intelligence	Robotics	-35.46341610601437	-39.95950172379817	174060
4ba8f98a898d74640a714eeb4e0a6f3609a480c1	a mobile robot which can follow and lead human by detecting user location and behavior with wearable devices	wearable computers feature extraction gesture recognition intelligent sensors mobile robots;conferences consumer electronics;smart watch mobile robot user location detection behavior detection wearable device sensor avoidance direction gesture recognition;consumer electronics;conferences	In this paper, a mobile robot that can detect human's location and behavior for following and leading human is proposed with the use of low frequency, radio frequency communication, various sensors and wearable devices. To know the user's location, the strength of low frequency signal is mainly used. It can also predict sudden change of human behavior by various sensors such as ultrasonic sensor, gyro sensor and acceleration sensor. Also, we can control robot and indicate direction of avoidance by gesture recognition of smart watch which is one of wearable device. A mobile robot gives us convenience for human. Furthermore, it can apply to luggage bag, shopping cart, stroller and so on.	gyro;gesture recognition;mobile robot;radio frequency;sensor;smartwatch;wearable technology	Jae Geun Lee;Min-Su Kim;Tae-Min Hwang;Soon-Ju Kang	2016	2016 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2016.7430583	embedded system;computer vision;simulation;engineering	Robotics	-39.09296765611556	-44.27035625442954	174329
0c751c7641f5b421a8e8939531e48c05fe8fa717	robots as conversational mediators	human robot interaction;human robot human interaction;natural language processing	We propose a system in which a humanoid robot acts as a broker of a human-robot-human conversation, with one of the robot-human communication performed remotely. The robot plays a role of a \textit{mediator}, summarizing relevant information, translating across languages or point-of-views and even censoring some information.	censoring (statistics);humanoid robot	Ekaterina Kynev;Nicolas Monet;Christophe Legras;Matthias Gallé	2017		10.1145/3029798.3038403	human–robot interaction;computer vision;simulation;computer science;artificial intelligence;social robot;personal robot	Robotics	-34.89127530986342	-41.17009824980395	175024
e2d7f44f717cf521b6f066f8e086b956fbb2700d	towards safety in human robot interaction	human robot interaction	Many new application areas are emerging where robots will be required to co-operate with or assist humans. This means that they are sharing the same work-space or they are working in our ‘personal’ space. The fundamental resultant issue of working in co-located space is that of safety. In service and social robotics, a major focus has therefore to be safe and reliable operation. It has been identified by some researchers that safety might be expressed in two inter-related modes, namely: physical and behavioural. Physical Safety is quite well understood and refers to potential failures in such areas as torque control and sensor failure as well as exploiting such phenomena as passive compliance. Behavioural Safety is a relatively new research theme, which aims to exploit the bi-directional multi-modal communication channels employed by humans such as: gesture, body pose, facial expressions, back-channel utterances and speech. This further requires individual and shared cognitive models between robot and human to provide the basis for models of shared goals and shared intentions. The key hypothesis is that safe interaction between human and robot can be engineered physically and cognitively for joint physical tasks requiring co-operative manipulation of real world objects. The advancement of actuator technology, mechanical systems design, sensor systems, computational power, control technology and strong links with disciplines such as cognitive psychology has lead us to this assumption to be true in	human–robot interaction	Guido Herrmann;Chris Melhuish	2010	I. J. Social Robotics	10.1007/s12369-010-0061-z	human–robot interaction;mobile robot;computer science;artificial intelligence;social robot;robot control	Robotics	-35.269771622827236	-40.220362454454786	175084
cc178442b7cfbe4ce8f9d4cbc0eb725e7961818b	catching a real ball in virtual reality	virtual reality;visualization;trajectory;target tracking;rendering computer graphics;floors	We present a system enabling users to accurately catch a real ball while immersed in a virtual reality environment. We examine three visualizations: rendering a matching virtual ball, the predicted trajectory of the ball, and a target catching point lying on the predicted trajectory. In our demonstration system, we track the projectile motion of a ball as it is being tossed between users. Using Unscented Kalman Filtering, we generate predictive estimates of the ball's motion as it approaches the catcher. The predictive assistance visualizations effectively increases the user's senses but can also alter the user's strategy in catching.	kalman filter;virtual reality	Matthew K. X. J. Pan;Günter Niemeyer	2017	2017 IEEE Virtual Reality (VR)	10.1109/VR.2017.7892280	computer vision;simulation;visualization;computer science;artificial intelligence;trajectory;virtual reality;computer graphics (images)	Visualization	-38.86736897469367	-39.31862488263648	176556
55b3cb4ad2f82ca28ba7a17f40bfa73b733007d4	voice activated command and control with speech recognition over wifi	wireless networks;wireless network;command and control;speech recognition;ubiquitous computing;lego mindstorms;java;ubiquitous computing environment	This paper presents work conducted to date on the development of a voice activated command and control framework specifically for the control of remote devices in a ubiquitous computing environment. The prototype device is a Java controlled Lego Mindstorm robot. The research considers three different scenario configurations. A recognition grammar for command and control of the robot has been created and implemented in Java, in part in the recognition engine and in part on the robot. The physical topology involves Java at each node endpoint, that is, at the handheld PC (iPaq), the PC workstation, the Linux server and onboard the robot (including its Java based Lejos OS). Network communications is primarily WLAN with an element of IR where the robot is concerned. The speech recognition software used includes Sphinx4, Microsoft SAPI and the Java Speech API. We compare these speech technologies and present their benefits in the context of this research. For each given scenario we present and discuss the implementation challenges encountered and their corresponding solutions, including future plans to create additional grammars to extend the framework’s range of devices. © 2005 Elsevier B.V. All rights reserved.	application programming interface;command (computing);communication endpoint;extensibility;fits;handheld pc;handheld game console;ipaq;java;java virtual machine;lambda calculus;lego mindstorms;linux;microsoft speech api;mobile device;network topology;operating system;programming paradigm;prototype;remote control;resource bounded measure;response time (technology);robot;server (computing);speaker recognition;speech recognition;sphinx4;ubiquitous computing;webcam;workstation	Tony Ayres;Brian Nolan	2006	Sci. Comput. Program.	10.1016/j.scico.2005.07.007	embedded system;real-time computing;computer science;operating system;wireless network;strictfp;real time java;ubiquitous robot;programming language;java;ubiquitous computing	HCI	-37.91056604540577	-42.40858016789938	176565
00bb36b946568d674b7bd1575e60334ec2674ef9	eggnog: a continuous, multi-modal data set of naturally occurring gestures with ground truth labels		Abstract-People communicate through words and gestures,but current voice-based computer interfaces such as Siri exploitonly words. This is a shame: human-computer interfaces wouldbe natural if they incorporated gestures as well as words. Tosupport this goal, we present a new dataset of naturally occurringgestures made by people working collaboratively on blocks worldtasks. The dataset, called EGGNOG, contains over 8 hours ofRGB video, depth video, and Kinect v2 body position data of 40subjects. The data has been semi-automatically segmented into24,503 movements, each of which has been labeled accordingto (1) its physical motion and (2) the intent of the participant.We believe this dataset will stimulate research into natural andgestural human-computer interfaces	blocks world;gesture recognition;ground truth;kinect;modal logic;pattern recognition;semiconductor industry;siri;video	Isaac Wang;Mohtadi Ben Fraj;Pradyumna Narayana;Dhruva Patil;Gururaj Mulay	2017	2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)	10.1109/FG.2017.145	ground truth;speech recognition;gesture;computer science;shame	Vision	-34.80301429842652	-42.61940633702922	176698
a59701cd2c89ac288b07a478e2900a67f62870de	human interaction with motion planning algorithm	tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;tecnologias;grupo a	This paper presents an interactive motion planning system to compute free collision motion in a numerical model. The system is based on interaction between a user and a motion planning algorithm. On one hand the user moves the object with an interactive device and on the other hand a motion planning algorithm searches a solution in the configuration space. The interaction aims at improving the guidance of an operator during a robot motion task in a virtual environment with the help of an automatic path planning algorithm. Existing works use a two-step decomposition which limits the interaction between the user and the ongoing process. We propose a modification of a classic motion planning method, the Rapidly-exploring Random Tree to build an Interactive-RRT. This method is based on exchanging pseudo-forces between the algorithm and the user, and on data gathering (labels) from the virtual scene. Examples are shown to illustrate the Interactive motion planning system with different interactive devices (space mouse and haptic arm). We analyze the influence of the user’s dexterity to find a solution depending on various parameters of the algorithm and we show how we can adapt these parameters to a user.	arm architecture;approximation algorithm;automated planning and scheduling;business process;color;dexterity programming language;embedded system;haptic technology;integrated development environment;iteration;mathematical model;motion planning;multi-user;performance;rapidly-exploring random tree;real-time transcription;virtual reality	Michel Taïx;David Flavigné;Etienne Ferre	2012	Journal of Intelligent and Robotic Systems	10.1007/s10846-012-9659-8	computer vision;simulation;computer science;artificial intelligence	Robotics	-38.0099278977661	-38.34342793704057	176780
eb4e27bf459e6e9b569f5e836038266cf839e705	multimodal input in the car, today and tomorrow	gaze;automotive engineering;road vehicles driver information systems interactive systems road accidents road safety;road accidents;gesture;gaze multimodal input automotive speech voice gesture touch;speech;automotive;voice;input;touch;intelligent vehicles;driver circuits speech vehicles speech recognition automotive engineering intelligent vehicles;speech recognition;driver circuits;multimodal;vehicles;road safety;interactive systems;driver information systems;interaction system multimodal input in vehicle system functionality driver distraction minimization;road vehicles	With the increased functionality offered by in-vehicle systems, multimodal input is emerging as an effective means of interaction to minimize driver distraction. This article describes the current state of this technology for automotive applications, various ways to combine modalities, and outlooks toward the future.	multimodal interaction	Christian A. Müller;Garrett Weinberg	2011	IEEE MultiMedia	10.1109/MMUL.2011.14	simulation;speech recognition;computer science;speech;multimodal interaction;gesture;voice	Visualization	-38.359266754879535	-43.1197569877878	176891
f5a276ac7a3c532455173af0d7a46a493a02ab2f	programmable triangulation light curtains		A vehicle on a road or a robot in the field does not need a full-featured 3D depth sensor to detect potential collisions or monitor its blind spot. Instead, it needs to only monitor if any object comes within its near proximity which is an easier task than full depth scanning. We introduce a novel device that monitors the presence of objects on a virtual shell near the device, which we refer to as a light curtain. Light curtains offer a light-weight, resource-efficient and programmable approach to proximity awareness for obstacle avoidance and navigation. They also have additional benefits in terms of improving visibility in fog as well as flexibility in handling light fall-off. Our prototype for generating light curtains works by rapidly rotating a line sensor and a line laser, in synchrony. The device is capable of generating light curtains of various shapes with a range of 20-30m in sunlight (40m under cloudy skies and 50m indoors) and adapts dynamically to the demands of the task. We analyze properties of light curtains and various approaches to optimize their thickness as well as power requirements. We showcase the potential of light curtains using a range of real-world scenarios.	obstacle avoidance;prototype;requirement;structured-light 3d scanner;thickness (graph theory)	Jian Wang;Joseph R. Bartels;William Whittaker;Aswin C. Sankaranarayanan;Srinivasa G. Narasimhan	2018		10.1007/978-3-030-01219-9_2	computer vision;blind spot;sunlight;visibility;computer science;artificial intelligence;obstacle avoidance;proximity sensor;computational photography;triangulation (social science);laser	Vision	-40.53715586755825	-42.33077192576224	177562
e3b2c2169c2fb02645563af2638309992eb4873f	analysis of ultra hd technology development according to human visual system		Ultra High Definition technology is discussed around five related elements that affect video and image quality in the paper. By analyzing human visual system and combining international standards, ultra HD technology can be identified to improve the picture quality of television and enhance audience's immersion. Ultra HD technology is consistent with the historical law of TV technology development.	human visual system model;image quality;immersion (virtual reality)	Jiefeng Liu;Minyong Shi;Yongbin Wang;Zhiguo Hong;Ying Li;Xiaosen Chen	2018	2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2018.8466450	brightness;high dynamic range;image quality;immersion (virtual reality);computer vision;human visual system model;artificial intelligence;computer science;image resolution	Robotics	-39.970647040020715	-40.663637963829125	177623
31ac18d2d05ef00a4a796397effa6923eadd2908	natural demonstration of manipulation skills for multimodal interactive robots	correspondence problem;robot arm;learning by demonstration;service robot;multimodal interaction;visual tracking;object detection	This paper presents a novel approach to natural demonstration of manipulation skills for multimodal interactive robots. The main focus is on the natural demonstration of manipulation skills, especially grasping skills. In order to teach grasping skills to a multimodal interactive robot, a human instructor makes use of natural spoken language and grasping actions demonstrated to the robot. The proposed approach emphasizes on four different aspects of learning by demonstration: First, the dialog system for processing natural speech is considered. Second, an object detection and classification scheme for the robot is shown. Third, the correspondence problem is addressed by an algorithm for visual tracking of the demonstrator's hands in real time and the transformation of the tracking results into an approach trajectory for a robotic arm. The fourth aspect addresses the fine-tuning of the robot's hand configuration for each grasp. It introduces a criterion to evaluate a grasp for stability and possible reuse of a grasped object. The approach produces stable grasps and is applied and evaluated on a multimodal service robot.	interactivity;multimodal interaction;robot	Markus Hüser;Tim Baier-Löwenstein;Marina Svagusa;Jianwei Zhang	2007		10.1007/978-3-540-73281-5_97	computer vision;simulation;computer science;social robot;robot control;communication	Robotics	-33.69673829594862	-40.1632066211499	177962
561e2982cbc06eaa461e9c66f5a81b82ca764bf4	towards calm displays: matching ambient illumination in bedrooms		We present a system for making emissive computer displays (LCDs) look like they are reflective, i.e. not emitting light but instead reflecting ambient light, an effect that we call a “calm display”. We achieve this effect by using a light sensor and a one-time calibration process to drive an algorithm which controls the display's backlight intensity and gamma correction functionality to continually match the brightness and chromaticity of the ambient light. We present an experimental evaluation of our system, showing quantitatively that the color and brightness output by our system is perceptually close to that of a piece of paper under similar lighting conditions. We argue that calm displays can more easily fade into the background, and further that they are more suitable for environments such as bedrooms where glowing displays are often out-of-place. We validate these claims and more generally explore users’ perception of calm displays, through a field study of an LCD display deployed in participants’ bedrooms.	algorithm;backlight;block cipher mode of operation;calm technology;color;computer monitor;display device;field research;gamma correction;oled;real-time computing;real-time transcription;software deployment	Jan Kučera;James M Scott;Nicholas Chen;Patrick Olivier;Steve Hodges	2017	IMWUT	10.1145/3090081	chromaticity;backlight;color vision;gamma correction;brightness;photodetector;calm technology;liquid-crystal display;computer vision;computer graphics (images);geography;artificial intelligence	HCI	-40.09016761063138	-41.51985043896746	178148
b6525df381f2e6e8fda1c98bd62eb8d304173978	real-time auditory and visual multiple-speaker tracking for human-robot interaction	audio visual integration;active audition;human robot interaction;speaker tracking			Kazuhiro Nakadai;Ken-ichi Hidai;Hiroshi G. Okuno;Hiroshi Mizoguchi;Hiroaki Kitano	2002	JRM	10.20965/jrm.2002.p0479	computer vision;speech recognition;computer audition;communication	Robotics	-37.008072723983574	-42.8096747152917	178239
df250662e43c606faf3c7accf25bbace7c01969e	learing compliant motions by task-demonstration in virtual environments	supervised learning;institut fur dynamik der flugsysteme;virtual environment;haptic interface;force control	"""We are proposing a supervised learning approach in robot force control which enables robot programming by demonstration through an operator. The learning element of the controller is based on a neural network prestructured to represent knowledge in terms of rules. Tasks are demonstrated in a virtual environment. Visual feedback using sensor ball devices and graphic display of forces as well as visual and proprioceptive feedback using haptic interfaces are being considered. Experimental results are shown for the \Put Block in a Corner of a Box Problem""""."""	artificial neural network;haptic technology;programming by demonstration;supervised learning;virtual reality	Reinhard Koeppe;Gerd Hirzinger	1995		10.1007/BFb0035220	simulation;human–computer interaction;computer science;engineering;virtual machine;artificial intelligence;haptic technology;supervised learning;mechanical engineering	Robotics	-35.65444883799435	-38.97602307617315	178347
4c31433c3647f6e1cca335ac94b932a6d0b12c60	multi-user egocentric online system for unsupervised assistance on object usage		We present an online fully unsupervised approach for automatically extracting video guides of how objects are used from wearable gaze trackers worn by multiple users. Given egocentric video and eye gaze from multiple users performing tasks, the system discovers task-relevant objects and automatically extracts guidance videos on how these objects have been used. In the assistive mode, the paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user’s gaze. The approach is tested on a variety of daily tasks ranging from opening a door, to preparing coffee and operating a gym machine.	assistive technology;multi-user;prototype;real-time clock;real-time computing;unsupervised learning;wearable computer	Dima Damen;Osian Haines;Teesid Leelasawassuk;Andrew Calway;Walterio W. Mayol-Cuevas	2014		10.1007/978-3-319-16199-0_34	computer vision;simulation;multimedia	HCI	-38.68949483406774	-41.52750715356979	178675
6fab513ed1d758f2893c15d10140fa2c396e1913	augmented reality product display system on a 360-degree view inside store	google;three dimensional displays;solid modeling;mobile handsets;cg augmented reality product display system ar 360 degree view inside store electronic commerce web site google street view smart phone html5 javascript computer graphics;mobile handsets google augmented reality cameras three dimensional displays rendering computer graphics solid modeling;web sites augmented reality computer graphics electronic commerce hypermedia markup languages java smart phones;photo ar online stores panoramic photos inside of store mobile devices;augmented reality;rendering computer graphics;cameras	This research proposes a novel product display system for electronic commerce websites that employs a panoramic photo of the inside of a real store as the background. Using a tablet or smartphone as a controller, the user is able to freely move around the inside of the store while viewing the panoramic photo, similarly to Google Street View. Also, the user can select a product they are interested in and freely view the product from any angle by rotating the tablet or smartphone. Furthermore, that product can be displayed as if it were in the hands of the user by a photo-based augmented reality system (Photo AR), allowing comparison with other objects possessed by the user. In this report, the proposed system was actually implemented using HTML5/JavaScript and the results were evaluated.	augmented reality;e-commerce;google street view;html5;smartphone;tablet computer	Masaya Ohta;Shunsuke Nagano;Hiroki Otani;Katsumi Yamashita	2014	2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2014.7031164	augmented reality;simulation;engineering;multimedia;computer graphics (images)	Visualization	-40.0662134295874	-39.82002851995352	179931
0066c510e1560053728d4894c3c06be1e4b04927	real-time transputer simulation of the human peripheral hearing system	real time	This paper presents new work on the implementation of a contemporary model of the human peripheral hearing system which is designed to operate in real-time. This is achieved by means of transputer technology. Psychoacoustic research is providing new insights into the operation of the human peripheral hearing system. It is suggested that many of these could be employed in the analysis of speech and singing to understand further the nature of the acoustic cues presented to the brain by each ear which are the basis for human communication. Contemporary psychoacoustic experiments into the nature of the peripheral hearing mechanism suggest that the fixed filter bandwidths used in the speech spectrograph could be obscuring patterns in the acoustic speech signal which are essential cues in our perception of speech and singing naturalness. To achieve a more appropriate model of the hearing system more sophisticated filters are required. A realtime version will provide a basis for visual displays of acoustic cues to be implemented for those with speech disorders and those wishing to develop healthy and more effective voices. The development of a real-time version of the model is not possible by means of standard sequential processing techniques, since the human peripheral hearing system is itself an inherently parallel process. Increased processing speed has been achieved by implementation using a network of processors which are themselves designed to operate in parallel.	acoustic cryptanalysis;central processing unit;experiment;peripheral;psychoacoustics;real-time clock;real-time computing;real-time transcription;simulation;transputer	Chris Swan;David M. Howard;Andrew M. Tyrrell	1994	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(94)90084-1	embedded system;speech recognition;computer science	Robotics	-36.25898484194583	-43.10949785913038	182647
23ffb69b947500ff3794309d985aee05ab14e8ee	method for reading sensors and controlling actuators using audio interfaces of mobile devices	computers handheld;multimedia;sensors;transducers;actuators;equipment failure analysis;robotics;smartphones;signal processing computer assisted;equipment design;feedback sensory;feedback;cellular phone;sound spectrography;control;mechatronics;robot;mobile devices	This article presents a novel closed loop control architecture based on audio channels of several types of computing devices, such as mobile phones and tablet computers, but not restricted to them. The communication is based on an audio interface that relies on the exchange of audio tones, allowing sensors to be read and actuators to be controlled. As an application example, the presented technique is used to build a low cost mobile robot, but the system can also be used in a variety of mechatronics applications and sensor networks, where smartphones are the basic building blocks.	audio media;autonomous robot;bioconductor caaffy player out probe summary method;computation (action);computers;control system;decoder device component;digital signal processing;dual-tone multi-frequency signaling;educational robotics;fast fourier transform;financial cost;interface device component;internet;mp3;mechatronics;mobile device;mobile phone;mobile robot;obsolete - editstatus;personal digital assistant;quantity;remote control;robot (device);smartphone;software development kit;tablet dosage form;tablet computer;universal controls;sensor (device)	Rafael Vidal Aroca;Aquiles M. F. Burlamaqui;Luiz Marcos Garcia Gonçalves	2012		10.3390/s120201572	robot;control engineering;embedded system;electronic engineering;mechatronics;audio over ethernet;transducer;computer science;engineering;sensor;mobile device;feedback;robotics;scientific control;actuator	Mobile	-40.61843513751485	-44.479010063062304	182854
d48cea14c17f5ff0c93538f5c4aaed518182423d	gpu accelerated left/right hand-segmentation in first person vision		Wearable cameras allow users to record their daily activities from a user-centered (First Person Vision) perspective. Due to their favourable location, they frequently capture the hands of the user, and may thus represent a promising user-machine interaction tool for different applications. Existent First Person Vision, methods understand the hands as a background/foreground segmentation problem that ignores two important issues: (i) Each pixel is sequentially classified creating a long processing queue, (ii) Hands are not a single “skin-like” moving element but a pair of interacting entities (left-right hand). This paper proposes a GPU-accelerated implementation of a left right-hand segmentation algorithm. The GPU implementation exploits the nature of the pixel-by-pixel classification strategy. The left-right identification is carried out by following a competitive likelihood test based the position and the angle of the segmented pixels.	algorithm;embedded system;entity;graphics processing unit;interaction;pixel;unified framework;user-centered design	Alejandro Betancourt;Lucio Marcenaro;Emilia I. Barakova;Matthias Rauterberg;Carlo S. Regazzoni	2016		10.1007/978-3-319-46604-0_36	computer vision;artificial intelligence;computer science;pixel;machine learning;wearable computer;exploit;queue;segmentation	Vision	-37.78153072030055	-44.1314729127403	184613
aa43877ef9e3ffcd2ebb7ad30a03dd202a458962	integration framework for nasa nextgen volumetric cockpit situation display with haptic feedback	software;input device;information retrieval;haptic device;user study;user feedback;data mining;force;force feedback;national aeronautics and space administration integration framework nasa nextgen volumetric cockpit situation display haptic feedback force feedback information csd operational information retrieval visual displays mouse user input device user feedback modalities object selection route manipulation guidance command communication gui based software interface development haptic device integration testbed system novint falcon force feedback device;graphical user interfaces;aerospace computing;situated display;computer displays;aircraft force feedback force atmospheric modeling data mining software;haptic feedback;mouse controllers computers;user interface management systems;collision avoidance;atmospheric modeling;haptic interfaces;aircraft displays;user interface management systems aerospace computing aircraft displays computer displays force feedback graphical user interfaces haptic interfaces information retrieval mouse controllers computers;aircraft	In this paper, we present a framework for the integration of force feedback information in a NASA NextGen Volumetric Cockpit Situation Display (CSD). With the current CSD, the user retrieves operational information solely through visual displays and interacts with the CSD tools through using a mouse. The advanced capabilities of the CSD may require complex manipulation of information which may be difficult to perform with input devices found in today's cockpits. Performance with the CSD could benefit from a new user input device and enhanced user feedback modalities that can be operated safely, effectively, and intuitively in a cockpit environment. In this work, we investigate the addition of force feedback in two key CSD tasks: object selection and route manipulation. Different force feedback models were applied to communicate guidance commands, such as collision avoidance and target contact. We also discuss the development of a GUI-based software interface to allow the integration of a haptic device for the CSD. A preliminary user study was conducted on a testbed system using the Novint Falcon force-feedback device. A full experiment, assessing the effectiveness and usability of the feedback model in the CSD, will be performed in the next phase of our research.	cambridge structural database;falcon (video game series);graphical user interface;haptic technology;input device;novint technologies;testbed;usability testing	Jose Robles;Matthew Sguerri;R. Conrad Rorie;Kim-Phuong L. Vu;Thomas Z. Strybel;Panadda Marayong	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224842	embedded system;simulation;human–computer interaction;computer science;artificial intelligence;haptic technology	Robotics	-36.993943275861675	-41.393402583820624	184890
93a22652790f1829a267ebd731d456bf28f147a1	workshop summary: artificial intelligence methods for ambient intelligence		Ubiquitous computing aims for the realisation of environments that assist users autonomously and proactively in a non-distractive manner. Therefore smart environment infrastructures need to be able to identify users needs (intention recognition) and to plan an appropriate assisting strategy (strategy generation) without explicit user interaction. In our two-stage approach we address inferring the intention of a team of users during a meeting within a smart multiple display environment and the system decision process – what information to present on which display – on the strategy generation level.	ambient intelligence;artificial intelligence;smart environment;ubiquitous computing	Ralph Bergmann;Klaus-Dieter Althoff;Ulrich Furbach;Klaus Schmid	2007		10.1007/978-3-540-85379-4_1	intelligence assessment	AI	-34.77487046860111	-40.501429573496885	184930
75205d82267c97d1dd7ce7e95086ded2bc064923	realtime local navigation for the blind: detection of lateral doors and sound interface	doors;blind;navigation;visao computacional;vision;article	Worldwide there are about 285 million visually impaired persons, of which 39 million are blind and the others have low vision. Almost all systems designed to assist them are quite complex and expensive, but most blind persons do not have advanced technical assistance and they are rather poor. We are therefore developing a low-cost navigation aid which can be afforded by almost all blind persons: basically, the ultimate goal is to use only a mobile phone with a built-in camera. This aid complements the white cane, it is easily portable, and it is not a hindrance when walking with the cane. The system will have an easy and intuitive interface, yet providing assistance in local and global navigation in realtime. In this paper we present the progress concerning local navigation. Path and obstacle detection just beyond the reach of the cane is now supplemented by detection of doors in corridors. This is necessary for localization, i.e., for developing a better impression of the environment and for finding a specific room. A sophisticated sound interface can assist the user for centering on paths like sidewalks and corridors, alerting to looming obstacles for avoiding them.	canonical account;internationalization and localization;lateral computing;lateral thinking;mobile phone;real-time computing	M. Moreno;Somayeh Shahrabadi;J. José;J. M. Hans du Buf;João Fabrício Mota Rodrigues	2012		10.1016/j.procs.2012.10.009	vision;computer vision;navigation;simulation;computer science;operating system;doors	HCI	-39.96659009679842	-43.66091422496133	186120
c592e5cc78e963548a0fb30f45e834e3043887b7	ubi-fx: ubiquitous effects with multiple pan-tilt projector and camera units for entertainment	image processing;virtual reality;video game;visual effect;augmented reality	We describe our Ubi-FX system that projects visual effects onto the environment and the player's body to enhance a video game. We achieve our concept with compact pan-tilt units that contain a projector and stereo camera. The camera works to recognize both the player and background, and the projector projects visual effects onto them. By using pan-tilt motions, the system provides a wide variety of visual effects. Also, our pan-tilt unit is really simple and compact, so users can set multiple units and arrange their position and angle to project more complex visual effects. In this paper, we describe our Ubi-FX, focusing on implementation and applications through system demonstrations.	list of amd fx microprocessors;pc-fx;stereo camera;video projector;visual effects	Hiroaki Tobita;Hajime Hata	2015		10.1145/2757710.2757737	computer vision;computer science;multimedia;computer graphics (images)	HCI	-40.724838011718425	-39.15871180682479	186837
06892eec3279af1a662d5fe94970857f05c657f9	camera calibration of a nintendowii remote using pa-10 robotic arms	i 4 8 image processing and computer vision tracking;robot arm;categories and subject descriptors according to acm ccs i 3 4 computer graphics virtual device interfaces;i 3 7 computer graphics virtual reality;i 4 1 image processing and computer vision camera calibration;camera calibration;i 3 8 computer graphics applications	The Remote control unit for the Nintendo Wii computer games console features an infrared camera capable of detecting up to 4 infrared lights. As such, it is possible to calibrate this device using normal camera calibration techniques so that it can be used for tracking or human interfacing in a virtual environment. Camera calibration is an active area of research and there is a wealth of software available to perform the calibration e.g. ArToolKit and Matlab. Camera calibration typically requires at least five images which contain multiple grid points from which a matrix of camera parameters can be estimated. This paper proposes a method of calibrating the Wii Remote's IR camera by building up 24 calibration points from the 4 viewed infrared LEDs for a single viewpoint of the IR camera. This is done by moving the 4 LEDs in a known sequence using highly accurate PA-10 robotic arms. The camera calibration matrix parameters obtained from this method are presented.	camera resectioning;coat of arms;wii	Holly S. Grimes;Stuart Ferguson;Karen McMenemy	2008		10.2312/EGVE/EGVE08/041-048	smart camera;stereo camera;computer vision;camera auto-calibration;camera resectioning;simulation;computer science;computer graphics (images)	Robotics	-39.83564772892789	-39.30742526021152	186930
85cace98e07e190494a5d88eba5e5b11538dc3b1	hierarchical semantic mapping using convolutional neural networks for intelligent service robotics		The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab’s custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information.	artificial neural network;convolutional neural network;interaction;map;programming paradigm;robotics;semantic mapper;service robot	Ren C. Luo;Michael Chiou	2018	IEEE Access	10.1109/ACCESS.2018.2873597	task analysis;robot;mobile service;convolutional neural network;semantic mapping;semantics;human–computer interaction;distributed computing;robotics;artificial intelligence;computer science;service robot	Robotics	-33.966334693270895	-39.33188377086062	188081
cf19a4272ae69e93b924a381504d43fd3708055d	designing reactive emotion generation model for interactive robots	robot sensing systems;computational modeling mathematical model adaptation model equations robot sensing systems appraisal;gomy reactive emotion generation model interactive robots psychology humans emotions toy like robot;reactive emotion generation model;generic model;computer model;gomy;dynamic model;emotion recognition;psychology;toy like robot;human robot interaction;appraisal;computational modeling;adaptation model;humanoid robots;humans emotions;interactive robots;mathematical model;psychology emotion recognition humanoid robots human robot interaction	Design of reactive emotion generation mechanism for interactive robots is proposed in this paper. Sorts of reactive emotions and the framework to generate these emotions against stimuli were identified based on psychological researches on humans' emotions. Computational models of reactive emotion system were also proposed with dynamic models. Each model realized necessary characteristics for reactive process successfully. The developed model was implemented in a toy-like robot which is called ‘GOMY’. The robot can show emotional response reactively against stimulus change. The proposed reactive emotion model can be considered as the most basic foundation of general emotional process for interactive robots.	computation;computational model;computer simulation;experiment;humans;robot;robotics;vii	Hyoung-Rock Kim;Seong-Yong Koo;Dong-Soo Kwon	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5651005	human–robot interaction;simulation;computer science;humanoid robot;artificial intelligence;mathematical model;computational model	Robotics	-34.86187246963382	-39.23855235022344	188208
4772db987070fca1c36537dd19d48b55064383c4	modeling workflows for industrial robots considering human-robot-collaboration		The increasing number of variants results in decreasing lot sizes in the assembly and manufacturing domain. Considering traditional approaches of automation using industrial robots, it is very time-consuming to permanently adapt industrial robots for specific product variants in assembly lines. One possible approach of resolving this issue is the use of human-robot-collaboration for specific assembly steps. However, this also means that, in addition to the robot program, a work instruction must be created for the human worker. With current methods no workflow can be modeled which is universally applicable for a human or a robot. Therefore, this paper presents a new approach, called Human Robot Time and Motion (HRTM). The method provides generic basic elements which can be performed by a human worker or a robot. Additionally, collaborative elements allow to model synchronous tasks and a communication between human/human, human/robot or robot/robot. The HRTM approach is initially demonstrated by modeling a simple workflow which can be performed by a human worker or a robot. Finally, we model a collaborative workflow using LegoⓇ DuploⓇ bricks and perform it at a workplace with a collaborative robot.	cobot;human–robot interaction;industrial robot;time and motion study	O. Payza;René Lindorfer;Roman Froschauer	2018	2018 IEEE 16th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2018.8471999	automation;systems engineering;unified modeling language;robot;engineering;visualization;workflow;human–robot interaction	Robotics	-36.466616062499035	-40.408629664246234	188290
4b3e00c09275038a7fa71fd110ab083195a819df	cosmos: convenient image retrieval system of flowers for mobile computing situations.	personal digital assistant;digital camera;mobile computer;image retrieval;image similarity	There are many image retrieval systems today. These systems, however, are difficult to use in outdoors. In this paper, we propose an image retrieval system of flowers that can be used for mobile computing environment. When we use this system, first we photograph a flower with a digital camera, put the flower image in a personal digital assistant (PDA) and give some simple characteristics of flowers on the PDA. Then we send the flower image and the characteristics to an image retrieval server via Personal Handyphone System (PHS). In the server, images similar to the received image are retrieved and the results are returned to the PDA via PHS. We check the results on the PDA. For the purpose of comparing the images, we use characteristics of shape and color extracted from the images. The result of our experiment shows that the percentage of getting the objective image from the first to the third places was about 92%. The objective flower-data was obtained in satisfactory time.	color;cosmos;digital camera;experiment;image retrieval;mobile computing;personal handy-phone system;personal digital assistant;server (computing)	Makiko Noda;Hirotaka Sonobe;Saeko Takagi;Fujiichi Yoshimoto	2002			computer vision;visual word;image retrieval;computer science;digital image processing;multimedia;automatic image annotation;digital image;computer graphics (images)	Vision	-39.48741493675334	-40.06598549081798	188313
0b748441986a63944246821f06177d1dd8f51c76	learning models for following natural language directions in unknown environments	robot sensing systems;topology;grounding;voice commandable wheelchair learning models natural language route directions unknown environments natural language manipulation interpretation natural language navigation command interpretation spatial information utilization semantic information utilization latent world model belief space planner map distribution behavior distribution;semantics;natural languages;wheelchairs handicapped aids human robot interaction learning artificial intelligence mobile robots natural language processing path planning speech based user interfaces;natural languages semantics robot sensing systems grounding topology;article	Natural language offers an intuitive and flexible means for humans to communicate with the robots that we will increasingly work alongside in our homes and workplaces. Recent advancements have given rise to robots that are able to interpret natural language manipulation and navigation commands, but these methods require a prior map of the robot's environment. In this paper, we propose a novel learning framework that enables robots to successfully follow natural language route directions without any previous knowledge of the environment. The algorithm utilizes spatial and semantic information that the human conveys through the command to learn a distribution over the metric and semantic properties of spatially extended environments. Our method uses this distribution in place of the latent world model and interprets the natural language instruction as a distribution over the intended behavior. A novel belief space planner reasons directly over the map and behavior distributions to solve for a policy using imitation learning. We evaluate our framework on a voice-commandable wheelchair. The results demonstrate that by learning and performing inference over a latent environment model, the algorithm is able to successfully follow natural language route directions within novel, extended environments.	algorithm;autonomous robot;coherence (physics);experiment;graphical model;humans;human–robot interaction;natural language understanding;seamless3d;simulation;user error	Sachithra Hemachandra;Felix Duvallet;Thomas M. Howard;Nicholas Roy;Anthony Stentz;Matthew R. Walter	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139984	natural language processing;ground;universal networking language;question answering;natural language user interface;computer science;artificial intelligence;semantics;natural language	Robotics	-33.99414642759861	-41.60591453146936	188359
041805c93984847cae963cae459f661291d0617c	"""understanding interlimb coordination mechanism of hexapod locomotion via """"tegotae""""-based control"""				Masashi Goda;Sakiko Miyazawa;Susumu Itayama;Dai Owaki;Takeshi Kano;Akio Ishiguro	2016		10.1007/978-3-319-42417-0_44		Robotics	-35.707288685662434	-39.41746296040213	190235
bb3e67402b5545ebb0016cc9843520ba81a43c7c	chromagram: a real-time chroma key application for mobile devices	object recognition;i 4 8 image processing and computer vision;scene analysis	Chroma Key is a special-effects technique widely used by television and motion picture industries for image composition. This technique allows users to replace sections identified by a chosen colour in a multimedia stream (like a video or a photo) with another image or video stream. In this paper, we describe an easy-to-implement technique for the creation of an Android based application for mobile devices (like smartphones and tablets) that applies Chroma Key-based effects to the video stream coming from the device camera. We discuss the algorithm used to achieve the Chroma Key effect focusing on the computational performance and on the quality of its final result. Using a picture selected from the device gallery, this application makes possible the replacement of video stream background areas characterized by a chroma value with the chosen picture.	algorithm;android;chroma feature;computation;mobile device;real-time clock;smartphone;streaming media;television;video	Fabrizio Corda;Fabio Sorrentino;Riccardo Scateni	2014		10.2312/stag.20141240	computer vision;computer science;multimedia;computer graphics (images)	HCI	-39.69582589548695	-39.768171734851514	191474
936707cd6825befa75638540df73296ebda13b3c	gaze-based, context-aware robotic system for assisted reaching and grasping		Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multimodal system, consisting of different sensing, decision making and actuating modalities, to create intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user’s gaze, to decode their intentions and implement lower-level motion actions and achieve higher level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and action grammars-based implementation of sequences of action through the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68±0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system.	assistive technology;endeavour (supercomputer);high- and low-level;multimodal interaction;robot;smt placement equipment	Ali Shafti;Pavel Orlov;A. Aldo Faisal	2018	CoRR		engineering;gaze;modalities;simulation;smt placement equipment;robotics;artificial intelligence;rule-based machine translation	Robotics	-35.28779214470661	-40.79326847875863	192506
08cd222b992aa426e752538048b4747b85197292	digital 3d ukiyoe using the effect of motion parallax	3d image;three dimensional displays exhibitions history museums solid modelling;mirrors;art;history;exhibitions digital 3d ukiyoe motion parallax effect japanese traditional culture japanese traditional parallel perspective western geometric perspective layer model 3d partial model three dimensional scene three dimensional display systems user head position measurement;perspective;museums;three dimensional displays;motion parallax digital museum 3d image ukiyoe perspective;solid modeling;exhibitions;position measurement;digital museum;head;ukiyoe;motion parallax;solid modeling mirrors three dimensional displays educational institutions head art position measurement;solid modelling	In this research, digital 3D ukiyoe was created by using current digital technology for the purpose of evoking many people's interest in ukiyoe that is one of the Japanese traditional culture. In ukiyoe, since the methods of Japanese traditional parallel perspective and Western geometric perspective are used, the layer model and the 3D partial model were used to represent the three-dimensional scene. In addition, three-dimensional display systems that use the effect of motion parallax of the user by measuring the user's head position were developed. Several contents were created and they were exhibited in the exhibitions. From the result of the questionnaire asked in the exhibitions, effectiveness of the proposed system was evaluated.	3d film;digital 3d;digital electronics;parallax barrier	Tetsuro Ogi;Yoshisuke Tateyama;Hao Lu;Erika Ikeda	2012	2012 15th International Conference on Network-Based Information Systems	10.1109/NBiS.2012.92	parallax;stereoscopy;computer vision;perspective;exhibition;multimedia;solid modeling;head;computer graphics (images)	Robotics	-40.70430515589019	-39.97153466719581	192913
9dde314f117942805449d3c432e16f92bed5455c	a management of mutual belief for human-robot interaction	human robot interaction collaboration orbital robotics robot sensing systems collaborative work cognitive robotics grounding knowledge management service robots robot motion;service robots control engineering computing interactive systems man machine systems;service robots mutual belief management human robot interaction human robot collaborative task achievement interactive task achievement jido robotic platform;jido robotic platform;service robots;human robot interaction;mutual belief management;shared knowledge;control engineering computing;human robot collaborative task achievement;interactive systems;man machine systems;interactive task achievement	"""Human-robot collaborative task achievement requires the robot to reason not only about its current beliefs but also about the ones of its human partner. In this paper, we introduce a framework to manage shared knowledge for a robotic system dedicated to interactive task achievement with a human. In a first part, we define which beliefs should be taken into account ; we then explain a manner to achieve them using communication schemes. Several examples are presented to illustrate the purpose of beliefs management including a real experiment demonstrating a """"give object"""" task between the Jido robotic platform and a human."""	gesture recognition;human–robot interaction;information needs;robot;streaming media	Aurélie Clodic;Maxime Ransan;Rachid Alami;Vincent Montreuil	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414019	human–robot interaction;mobile robot;simulation;computer science;knowledge management;artificial intelligence;social robot;robot control	Robotics	-34.2098634252631	-39.55787723609837	192965
e8ff1095386f7efeb14d81b64bb98b1804c7d291	development of augmented in-vehicle navigation system for head-up display	projective transformation augmented reality in vehicle navigation head up display information fusion;information fusion augmented in vehicle navigation system head up display hud lane level road information;sensor fusion augmented reality driver information systems head up displays road traffic;vehicles navigation roads cameras turning visualization data mining	In this paper, we present an augmented in-vehicle navigation system for Head-Up Display (HUD). The proposed system superimposes turning directions and lane recommendations that correctly match the real world according to the variations in the driver's view. Therefore, the driver can keep his attention to specific situations and intuitively drive in the right direction. The proposed system extracts the lane-level road information, detects road lanes, decides the upcoming manoeuvres by an information fusion, and displays the augmented instructions to support safe driving practices.	automotive navigation system;global positioning system;head-up display	Changrak Yoon;Kyong-Ho Kim;Seung-Hae Baek;Soon-Yong Park	2014	2014 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2014.6983221	computer vision;augmented reality;simulation;engineering;computer graphics (images)	Robotics	-39.36062752185198	-42.611402684541716	194544
b4d324c47aa4ae7e27c24726554bfb63a3807339	the method of kinetic typography communication	databases;motion analysis;dynamic motion;communication system;image motion analysis;electronic mail;kinetic typography communication;grammars electronic mail online front ends word processing natural languages computer animation;kinetic communication system;prototypes;natural languages;kansei words;dynamic motion engine system;telephony;engineering system;online front ends;indexes;grammars;kinetic theory;postal services;kansei communication;engines;factor analysis;web browser;animation;tone of voice;feeling kinetic typography communication communication mail system kansei information dynamic motion grammar short private message dynamic motion factor analysis kansei words dynamic motion engine system kinetic communication system web browser kansei communication character animation methodology motion character tone of voice;kinetic theory motion analysis engines postal services prototypes telephony animation indexes image motion analysis databases;character animation methodology;character animation;computer animation;motion character;kinetics;feeling;word processing;short private message;kansei information;communication mail system;dynamic motion grammar	The purpose of this study proposes a communication mail system applied kinetic typography. Depends on our precedent researches, kinetic typography can show a kind of Kansei information with dynamic motion grammar. Kinetic typography could be useful to be expressed in a short private message for greeting or question. We grasped some elements of dynamic motion through factor analysis for Kansei words, and concemed system of dynamic motion engine for kinetic typography. We developed a kinetic communication system to apply the kinetic motion engine, and proposed a prototype operating on web browser. It equips the dynamic motion engine and the grammar, and can generate kinetic typography. Senders can choice several types of motion what they would like to express. And receivers can revive the kinetic typography in the mail on their own browser. The prototype of this system will give us a vision for Kansei communication with kinetic typography.	factor analysis;personal message;prototype;web typography	Yasufumi Uekita;Junji Sakamoto;Masahiko Furukata	2000		10.1109/ICSMC.2000.885030	anime;kinetic theory;nonverbal communication;database index;character animation;speech recognition;feeling;computer science;prototype;computer animation;multimedia;natural language;telephony;factor analysis;communications system;kinetics	HCI	-36.251224868583265	-44.02613852428345	195465
74155e20c4d4c42378f2ac208bb5cd55455a6108	meaningful conversation with a mobile robot	accessible location;specific location;mobile robot;dialogue system;current orientation;supply information;meaningful conversation;topological map;internal map	We describe an implementation integrating a spoken dialogue system with a mobile robot, which the user can direct to specific locations, ask for information about its status, and supply information about its environment. The robot uses an internal map for navigation, and communicates its current orientation and accessible locations to the dialogue system using a topological map as interface.	dialog system;facial recognition system;godot engine;goto;lexicon;mobile robot;natural language;open agent architecture;robot control;spoken dialog systems;vagueness	Johan Bos;Ewan Klein;Tetsushi Oka	2003			mobile robot;computer vision;computer science;social robot;multimedia;mobile robot navigation	Robotics	-34.289684353314335	-41.28941517543792	195777
0ceab2555088ce7ca49336f472f5902191661ff1	grounding language by continuous observation of instruction following		Grounded semantics is typically learnt from utterance-level meaning representations (e.g., successful database retrievals, denoted objects in images, moves in a game). We explore learning word and utterance meanings by continuous observation of the actions of an instruction follower (IF). While an instruction giver (IG) provided a verbal description of a configuration of objects, IF recreated it using a GUI. Aligning these GUI actions to subutterance chunks allows a simple maximum entropy model to associate them as chunk meaning better than just providing it with the utterance-final configuration. This shows that semantics useful for incremental (word-by-word) application, as required in natural dialogue, might also be better acquired from incremental settings.	graphical user interface;principle of maximum entropy	David Schlangen;Ting Han	2017			natural language processing;linguistics;communication	NLP	-33.95392659695411	-41.52133888878034	196554
ff46645c089f7d18f6324a010d0a6c4360f323aa	barcode application innovation for smartphones		Modern smartphones provide a high sophisticated camera and a high resolution display. Therefore, they can be used to enable various application scenarios for 1D and 2D barcode recognition as well as barcode presentation device. The device positioning against the barcode allows innovative interaction: e.g., additional product information or “rail information” on paper tickets. In the future, the challenges lie on applications that are “easy to use” with one-click: e.g., book auctions with one key press. Beside usability approaches, the technical challenges cover fast and robust algorithms under bad light conditions, different angel orientation and parallax calculation. 1 Motivation – new technology of smartphone cameras Over the recent years, smartphone cameras have reached the level of compact cameras. Manual macro switch or additional lenses were replaced by autofocus systems while the resolution increases. Vendor dependent APIs to access picture or video stream mode are available and the device processor allows real-time calculations which are essential for object detection even under bad light conditions (brightness, colors, reflexes). (a) Shop 1 Shop 2 Shop 3 Shop 6 Shop 4 Shop 5 Mall	1-click;algorithm;barcode;color;event (computing);image resolution;object detection;parallax;real-time clock;smartphone;streaming media;usability	Gerald Eichler;Karl-Heinz Lüke;Aykan Aydin;Roland Schwaiger	2009			common value auction;parallax;barcode;computer vision;autofocus;vendor;object detection;usability;macro;artificial intelligence;computer science	Mobile	-39.44787590556915	-40.471343074123595	198251
ab93f89f5e69942484b326ce967b3517a33e8827	ubisim: multiple sensors mounted smart house simulator development	multiple sensors;smart house;real time simulation;virtual environment	It is essential for smart house researchers to have large datasets from actual environments. However, not all researchers have sufficient budgets to build test beds. These researchers need a simulator that can synthesize realistic sensory datasets. To solve this problem, we propose the 'UbiSim' simulator for activity recognition research. UbiSim provides a 3D graphical user interface to enable spatial perception using multiple sensors, including those that detect motion, pressure, vibration, temperature, and contact, along with RFID tags and receivers. The sensors are designed to verify collisions in a virtual space as a means to operate with minimal computational costs. Our proposed methods were tested in a virtual environment. The results show that the smart house simulator achieves real-time performance.	activity recognition;computation;graphical user interface;radio-frequency identification;real-time clock;sensor;simulation;smart house;virtual reality	Wonsik Lee;Seoungjae Cho;Wei Song;Kyhyun Um;Kyungeun Cho	2013	2013 IEEE 11th International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2013.105	embedded system;real-time computing;simulation;engineering	Robotics	-37.43507215812275	-39.84999292494487	199152
29dbac19ffd7b8ed3ff4df667a70c095f14a761a	tracking for following and passing persons	human augmented mapping person following passing person tracking multiple target tracking human robot interaction;human computer interaction;target tracking navigation humans service robots cognitive robotics simultaneous localization and mapping paper technology testing labeling contracts;data collection;user study;computer and information science;human computer interaction target tracking man machine systems robot vision navigation;human robot interaction;multiple target tracking;navigation;robot vision;target tracking;data och informationsvetenskap;man machine systems;mapping human robot interaction tracking follow ing	This paper presents a multiple target tracking approach for following and passing persons in the context of human-robot interaction. The general purpose for the approach is the use in human augmented mapping. This concept is presented and it is described how navigation and person following are subsumed under it. Results from experiments under test conditions and from data collected during a user study are also provided.	experiment;gps navigation device;human–robot interaction;mobile robot;modal logic;population;tracking system;usability testing	Elin Anna Topp;Henrik I. Christensen	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1544961	human–robot interaction;computer vision;navigation;simulation;human–computer interaction;tracking system;computer science;engineering;artificial intelligence;mobile robot navigation;statistics;data collection	Robotics	-34.66086779755255	-41.89381718445049	199578
