id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
a9078615ab6d97037fc173ee2baff4b8333a7018	swsd: a p2p-based system for service discovery from a mobile terminal	semantic web service;distributed system;red sin hilo;intercambio informacion;ontologie;systeme reparti;informatique mobile;reseau sans fil;bolsa valores;par a par;web semantique;wireless network;service web;semantics;p2p;intelligence artificielle;web service;semantica;semantique;bourse valeurs;intergiciel publication souscription;stock exchange;sistema repartido;lenguaje descripcion;poste a poste;intergicial editor suscriptor;echange information;web semantica;information exchange;semantic web;artificial intelligence;ontologia;inteligencia artificial;service discovery;mobile computing;peer to peer;ontology;publish subscribe middleware;langage description;mobile terminal;servicio web;description language	This paper presents the architecture and implementation of the system named Semantic Web Services Discovery (SWSD), our proposed solution for service discovery from a mobile terminal. The key features of SWSD may be summarized as (1) semantic service description, based on DAML-S ontology for description of Web services, (2) service discovery, based on JXTA P2P technology, and, (3) mobile terminal support, based on JXTA for J2ME. The case study demonstrates a prototype implementation of the system for finding stock exchange information.	service discovery	Darije Ramljak;Maja Matijasevic	2005		10.1007/11553939_94	web service;stock exchange;information exchange;computer science;artificial intelligence;operating system;wireless network;semantic web;ontology;peer-to-peer;data mining;database;semantics;service discovery;mobile computing;world wide web	Mobile	-37.777277947166354	12.651267209818734	159099
a00fa5d46e812cf4fb1fa55ed173f3e9bf6e65cf	interoperability from electronic commerce to litigation using xml rules	legal knowledge;electronic commerce;xml document;artificial intelligence;non monotonic logic;knowledge fusion	We used the XML Rule system earlier described in [6] to simulate litigation arising from electronic commerce in a purchase order situation. This work is distinguished by using XML documents that comply with standards issued by standards-issuing organizations.	e-commerce;interoperability;simulation;xml	Zaw Z. Han;Tin T. Khine;Imtinan Ahmad;Sunil Shrestha;Laurence L. Leff	2003		10.1145/1047788.1047809	well-formed document;e-commerce;xml validation;binary xml;xml;xml schema;computer science;knowledge management;artificial intelligence;document type definition;non-monotonic logic;xml framework;data mining;xml schema;database;ebxml;xml schema editor;cxml;efficient xml interchange	DB	-39.91918215942353	12.882196959951536	159478
bdca27a5b542141f8325853a65d2501aa71aba1d	using an intranet for real-time production management: experiences and effects	production management information system;base donnee repartie;perspectiva;distributed database;red www;gestion production;user interface;heterogeneous computing;information retrieval;real time;base repartida dato;langage java;computer systems;perspective;production management;production process;production control;internet;distributed environment;recherche information;temps reel;tiempo real;workflow;world wide web;interface utilisateur;real time java;systeme informatique;reseau www;information system;intranet;user interfaces;java language;java	Newspaper production is a complex and suboptimised process, characterised by heterogenous computer systems in a distributed environment and with tight time schedules. Attempts to optimise this process have generally been on a local subsystem level, without an overall perspective. With the emerging IFRAtrack standard for exchanging tracking and scheduling information. new possibilities for global production management are emerging. This paper describes a three-tier intranet-based solution for a production management system in newspaper production. Tracking information is collected from subsystems via IFRAtrack messages. It is then stored in a production database and distributed to the client modules. Clients can either poll the server for updates, or they can subscribe to particular events. The user interface runs in any WWW browser, thus facilitating installation and maintenance, and ensuring that information is dispersed throughout the company. The entire system is written in Java and is built around ODBC-compliant databases, thus ensuring a high degree of portability and platform independence. The prototype described is being used in daily production since June 1997 at a middle-sized Swedish newspaper with good results. Interviews with personnel suggest several benefits: Employees have increased their awareness of the production process. Cooperation between traditionally separated departments has improved. The newspaper has gained a more thorough understanding of its production process and its bottlenecks.	bottleneck (software);interviews;intranet;java;multitier architecture;open database connectivity;polling (computer science);prototype;real-time transcription;scheduling (computing);server (computing);software portability;user interface;www	Vlad Ionesco	1998	Computer Networks	10.1016/S0169-7552(98)00107-X	real-time computing;telecommunications;computer science;database;programming language;user interface;world wide web;distributed database	OS	-36.349507398028976	16.388239586098628	162976
c86c708f4431d13810eabca2ee0608647a3189aa	semantically-guided workflow construction in taverna: the sadi and biomoby plug-ins	biomoby;sadi;web service;web services;semantic web;workflow;service discovery;data structure;taverna	In the Taverna workflow design and enactment tool, users often find it difficult to both manually discover a service or workflow fragment that executes a desired operation on a piece of data (both semantically and syntactically), and correctly connect that service into the workflow such that appropriate connections are made between input and output data elements. The BioMoby project, and its successor the SADI project, embed semantics into their data-structures in an attempt to make the purpose and functionality of a Web Service more computable, and thereby facilitate service discovery during workflow construction. In this article, we compare and contrast the functionality of the BioMoby and SADI plug-ins to Taverna, with a particular focus on how they attempt to simplify workflow synthesis by end-users. We then compare these functionalities with other workflow-like clients we (and others) have created for the BioMoby and SADI systems, discuss the limitations to manual workflow synthesis, and contrast these with the opportunities we have found for fully automated workflow synthesis using the semantics of SADI.	biomoby;computable function;input/output;sadi;service discovery;web service	David Withers;Edward A. Kawas;E. Luke McCarthy;Benjamin P. Vandervalk;Mark D. Wilkinson	2010		10.1007/978-3-642-16558-0_26	web service;data structure;computer science;data mining;database;world wide web;workflow management system;workflow engine;workflow technology	DB	-40.7691583620493	11.352348871343507	162977
a766c23fe71ddcf9df7e0007b12647528eef6b71	conzilla - a conceptual interface to the semantic web	representacion conocimientos;navegacion informacion;steganographie;information sources;formal model;paysage;ingenierie connaissances;navigation information;web semantique;information browsing;computer and information science;semantics;hombre;paisaje;semantica;semantique;collaborative tools;steganography;esteganografia;web semantica;human;representation connaissance;semantic web;landscape;knowledge representation;data och informationsvetenskap;homme;knowledge engineering	This paper has two foci that are intended to be complementary. First, it describes Conzilla as an incarnation of a concept browser. More specifically, as a technical solution for expressing context-maps, concepts, concept relations etc. Second, it introduces Conzilla as a fairly complete RDF editor which combines graphand form-based manipulation of RDF-graphs. Apart from these foci, the main requirements for the Conzilla design is: It should serve as a collaboration tool for more or less formalized modeling techniques, most notably UML-dialects. It should simplify the task of creating information according to various metadata standards. It should support customized presentations of existing information without requiring duplication or modification of information sources. These requirements are fullfilled by choosing a three layered approach for working with semantic web information in Conzilla, i.e. the information, presentation and style layers.	defense in depth (computing);map;requirement;semantic web;unified modeling language	Matthias Palmér;Ambjörn Naeve	2005		10.1007/11524564_9	computer science;knowledge management;artificial intelligence;semantic web;knowledge engineering;social semantic web;data mining;semantic web stack;database;semantics;landscape;steganography;world wide web	Web+IR	-37.15452801929029	13.476954522181948	163373
56fcdcf4bd2072ffdc955c784d6340a617a5d8d9	resolving schematic discrepancy in the integration of entity-relationship schemas	entity relationship model;base donnee;metadata;information source;source information;integration information;conceptual analysis;database;base dato;semantics;modelo entidad relacion;schema integration;modele entite relation;semantica;semantique;classification;analisis conceptual;semantic heterogeneity;information integration;heterogeneidad;integracion informacion;metadonnee;metadatos;information system;analyse conceptuelle;clasificacion;systeme information;fuente informacion;entity relationship;heterogeneity;heterogeneite;sistema informacion	In schema integration, schematic discrepancies occur when data in one database correspond to metadata in another. We define this kind of semantic heterogeneity in general using the paradigm of context that is the meta information relating to the source, classification, property etc of entities, relationships or attribute values in entity-relationship (ER) schemas. We present algorithms to resolve schematic discrepancies by transforming metadata into entities, keeping the information and constraints of original schemas. Although focusing on the resolution of schematic discrepancies, our technique works seamlessly with existing techniques resolving other semantic heterogeneities in schema integration.	algorithm;discrepancy function;entity;entity–relationship model;lossless compression;programming paradigm;relational database;schematic;semantic heterogeneity;semi-structured data;semiconductor industry;software industry;verification and validation;xml schema	Qi He;Tok Wang Ling	2004		10.1007/978-3-540-30464-7_20	entity–relationship model;computer science;data mining;database;semantics;information retrieval	DB	-35.2507616980477	11.864547244900603	163666
71e1aabb2d2bec9f4035c2b812bbbf9be7431701	a toolkit for reuse in conceptual modelling	developpement logiciel;modelizacion;distributed system;systeme reparti;red www;ingenieria logiciel;software engineering;modelisation;conceptual schema;sistema repartido;conceptual modelling;desarrollo logicial;software development;genie logiciel;world wide web;reseau www;information system;modeling;software reuse;systeme information;information system development;sistema informacion	This paper proposes a toolkit for applying Reuse in Conceptual Modelling. The main objective is to cope with the problems of complexity in the Conceptual Modelling activity. In a long-term perspective this proposition intends to settle the basis for a larger application of Reuse in Information System development. While research in Software Reuse has revealed that the application of Reuse in software development is extremely difficult, Conceptual Modelling appears as a more promising area because it manipulates simpler objects: conceptual schemas. The proposed toolkit provides reuse-oriented services to KHEOPS database design environment. These services include: quality validation of reusable components, component selection from the Repository, and new conceptual schema construction by customising and composing reusable ones. Reusable components consist of an Extended Entity-Relationship schema as well as other information like executable reuse guidelines.	code reuse;conceptual schema;database design;entity–relationship model;executable;information system;software development	Raúl Ruggia;Ana Paula Ambrosio	1997		10.1007/3-540-63107-0_13	conceptual model;systems modeling;computer science;three schema approach;conceptual schema;software development;software engineering;data mining;database;information system	SE	-35.27560778083614	13.12937044942401	164436
2ece5c9616e1056f0c33baa849e67fa7ea693958	psm: a model of collaborative agents for e-markets	agent intelligence;software agent;collaborative agents;new model;agents result form;necessary agent code;stationary repository;network node;agent version;electronic commerce;supply chain management;software agents;multi agent systems;polymorphism	In the paper a new model of software agents is proposed that are collaborative, polymorphic and highly mobile. The first two features of agents result form the existence of potentially many agent versions. The third feature is implied by the fact that only the necessary agent code is transmitted through the network, while the rest is transmitted only on demand. Moreover, the data which does not extend agent intelligence is not carried between network nodes by the agent, but sent directly to a stationary repository.	bootstrapping (statistics);image segmentation;java;mobile agent;programming language;software agent;stationary process;version control	Waldemar Wieczerzycki	2004	Proceedings. 15th International Workshop on Database and Expert Systems Applications, 2004.	10.1109/DEXA.2004.1333489	agent architecture;supply chain management;computer science;artificial intelligence;autonomous agent;software agent;multi-agent system;mobile agent;intelligent agent	AI	-40.43319684765882	16.438121642773254	165570
390eb4668632924ddb0854a4aa1b557d6e21b79d	situated support for choice of representation for a semantic web application	informatica;developpement logiciel;semiologia;langage modelisation;semiotics;web semantique;conceptual analysis;semantics;semiologie;semantica;semantique;analisis conceptual;semiotique;semiotica;real world application;modelling language;lenguaje modelizacion;desarrollo logicial;web semantica;software development;semiology;semantic web;analyse conceptuelle;directory service	As more and more companies are augmenting their data to include semantics, it is imperative that the choices made when choosing the modelling language are well founded in knowledge about the language and the domain in question. This work extends the Semiotic Quality Framework with computational and situated instruments. Furthermore, it demonstrates how the extended Semiotic Quality Framework can facilitate the choice of the most suited language for a real world application. The application is a directory services system, which currently is being moved into the realms of the Semantic Web.	computation;directory service;imperative programming;modeling language;realms;semantic web;semiotics;situated;web application	Sari Hakkarainen;Anders Kofod-Petersen;Carlos Buil Aranda	2005		10.1007/11568346_42	natural language processing;directory service;computer science;artificial intelligence;software development;semantic web;database;semantics;semiotics;semiology;programming language	AI	-36.960068098794174	12.901343427117634	165760
54e6b4d7c29a1e31002c568c2dcc451b4f0e3129	querying distributed data in a super-peer based architecture	distributed data;base donnee repartie;reseau pair;distributed database;information sources;data integrity;information source;source information;integration information;interrogation base donnee;base repartida dato;interrogacion base datos;p2p;pregunta documental;query optimization;question documentaire;large scale;user profile;igual a igual p2p;information integration;integracion informacion;query;information system;peer to peer;database query;systeme information;fuente informacion;sistema informacion	Data integration is a significant challenge: relevant data objects are split across multiple information sources, and often owned by different organizations. The sources represent, maintain, and export the information using a variety of formats, interfaces and semantics. This paper addresses the issue of querying distributed data in a large scale context. We present a p2p information mediation framework based on the notion of super-peers, providing a super-peernetwork. This makes it possible for a super-peer to reach every other peer (data source) in the system, thus realizing the concept of a integrated schema formed from all possible information sources. This is achieved by classifying data sources into domains and creating user profiles for query optimization purposes.	cluster manager;entity;laurent polynomial;matchware mediator;mathematical optimization;metamodeling;peer-to-peer;query optimization;software deployment;statistical classification;user profile;xml	Zohra Bellahsene;Mark Roantree	2004		10.1007/978-3-540-30075-5_29	query optimization;computer science;information integration;peer-to-peer;data integrity;data mining;database;world wide web;distributed database;information system	DB	-36.64761837025275	11.574266094984914	165826
04d078b44878442d8cb54493e53aacf475e77a18	resolving ontological heterogeneity in the kraft project	multiagent system;systeme intelligent;architecture systeme;integration information;sistema inteligente;telecommunication network;information integration;red telecomunicacion;integracion informacion;intelligent system;reseau telecommunication;arquitectura sistema;information system;system architecture;sistema multiagente;systeme information;systeme multiagent;sistema informacion	KRAFT is an agent architecture for the integration of heterogeneous information systems. The focus in KRAFT is on the integration of knowledge in the form of constraints. In this article we describe the architecture from an ontological perspective. We start by introducing the agent architecture and illustrate its application in the telecommunication-network design. We then describe how we assess the ontological heterogeneity in the domain, which problems the integration of constraint knowledge pose, and how we construct a shared ontology. Also, we describe the mapping ̧ functions that are used to translate information between the shared and the local ontologies. Finally, we look at the direction our research is taking hereafter.	agent architecture;constraint (mathematics);information retrieval;information system;network planning and design;ontology (information science);problem domain;pure data;server message block	Pepijn R. S. Visser;Martin D. Beer;Trevor J. M. Bench-Capon;Bernard M. Diaz;Michael J. R. Shave	1999		10.1007/3-540-48309-8_62	computer science;artificial intelligence;information integration;data mining;database;information system;telecommunications network;systems architecture	AI	-38.50646443693034	13.272901045185867	166211
a2d861eb02dd4630e1c5f6a637bc710bee6d4ba2	sensor web oriented web-based gis	access network;real time;data type;web service;web based gis;spatial distribution;web services;sensor web;distributed architecture	Web-Based GIS has brought a lot of convenience to the public. However, traditional Web-Based GIS cannot meet the needs in applications that request for many different data types and real-time updates. A Sensor Web is a computer accessible network of many, spatially distributed devices using sensors to monitor conditions at different locations. In this paper we demonstrate the concept of Sensor Web Oriented Web-Based GIS, which is a new type of Web-Based GIS using Sensor Web as data source.#R##N##R##N#We first describe the concept of Sensor Web Oriented Web-Based GIS after analyzing the disadvantages of traditional Web-Based GIS and Sensor Web. Then we demonstrate the distributed architecture for such a system. At last, a prototype application of Sensor Web Oriented Web-Based GIS is described.	geographic information system;sensor web	Shaoqing Shen;Xiao Cheng;Peng Gong	2008		10.1007/978-3-540-89903-7_9	web service;web application security;sensor web;distributed gis;enterprise gis;web development;web modeling;data web;web analytics;web mapping;web-based simulation;web design;computer science;web navigation;data mining;database;am/fm/gis;web intelligence;law;world wide web	NLP	-36.08457221579823	14.58542736647132	166428
bdf8474edaa2607c11a1cac80de855de35f06648	artificial intelligence and grids: workflow planning and beyond	workflow management;pegasus system artificial intelligence workflow planning system grid computing scientific application;planning artificial intelligence;artificial intelligence grid computing geophysics computing mesh generation biology computing earthquake engineering large scale systems distributed computing physics computing high energy physics instrumentation computing;scientific information systems planning artificial intelligence workflow management software grid computing;scientific workflows;artificial intelligent;ai applications;workflow management software;grid computing;ai planning;scientific information systems	A key challenge for grid computing is creating large-scale, end-to-end scientific applications that draw from pools of specialized scientific components to derive elaborate new results. We develop Pegasus, an AI planning system which is integrated into the grid environment that takes a user's highly specified desired results, generates valid workflows that take into account available resources, and submits the workflows for execution on the grid. We also begin to extend it as a more distributed and knowledge-rich architecture.	artificial intelligence;computation;end-to-end principle;executable;grid computing;high- and low-level;online and offline;pegasus;problem solving;requirement;world wide web	Yolanda Gil;Ewa Deelman;Jim Blythe;Carl Kesselman;Hongsuda Tangmunarunkit	2004	IEEE Intelligent Systems	10.1109/MIS.2004.1265882	applications of artificial intelligence;automated planning and scheduling;computational science;workflow;artificial architecture;computer science;artificial intelligence;data science;theoretical computer science;utility computing;artificial intelligence, situated approach;workflow management system;workflow engine;grid computing	HPC	-36.60611614082109	17.652230072173	167525
5045c9d643da30908748a7088266e5b823edcc60	querying heterogeneous spatial databases: combining an ontology with similarity functions	developpement logiciel;ontologie;base donnee;architecture systeme;formal specification;integration information;interrogation base donnee;conceptual analysis;database;interrogacion base datos;base dato;base connaissance;conceptual model;analisis conceptual;similitude;spatial database;specification formelle;especificacion formal;information integration;filter;desarrollo logicial;base donnee spatiale;software development;similarity;integracion informacion;filtre;base conocimiento;ontologia;arquitectura sistema;base dato especial;similitud;information system;analyse conceptuelle;system architecture;similarity function;ontology;filtro;database query;systeme information;sistema informacion;knowledge base	This paper uses a knowledge-based approach to querying heterogeneous spatial databases based on an ontology and conceptual and attribute similarities. The ontology, which may be independent of the databases, expands and filters a user query. Then, queries are translated into a formal specification of entity classes, which are compared against definitions in databases. This process is carried out by determining the conceptual similarity between entities in a user ontology and by comparing these entities in the ontology with entities in the conceptual models of databases. In addition, the specification of a query is done not only by identifying entity classes but also by considering constraints based on attribute values. The paper describes the system architecture and presents a case study with data from a forestry information system.	entity;entity–relationship model;expect;formal specification;information system;knowledge-based systems;microsoft windows;ontology (information science);requirement;spatial database;systems architecture;usability;web ontology language	Mariella Gutiérrez;M. Andrea Rodríguez	2004		10.1007/978-3-540-30466-1_15	upper ontology;knowledge base;similarity;bibliographic ontology;filter;computer science;ontology;conceptual model;information integration;software development;similitude;ontology;data mining;formal specification;database;ontology-based data integration;spatial database;information retrieval;process ontology;information system;suggested upper merged ontology	DB	-36.149623931228085	11.916334209566347	167943
415905010c7965c90c60f1a7368321b247090afe	metaprogramming for relational databases	base relacional dato;entity relationship model;evaluation performance;base donnee;performance evaluation;sql;integration information;reutilizacion;evaluacion prestacion;interrogation base donnee;conceptual analysis;database;interrogacion base datos;base dato;abstraction;modelo entidad relacion;relational database;modele entite relation;abstraccion;analisis conceptual;reuse;langage dedie;information integration;structure and function;integracion informacion;domain specific language;base donnee relationnelle;metaprogrammation;information system;analyse conceptuelle;metaprogramming;metaprogramacion;database query;systeme information;domain specificity;reutilisation;lenguaje dedicado;sistema informacion	For systems that share enough structural and functional commonalities, reuse in schema development and data manipulation can be achieved by defining problem-oriented languages. Such languages are often called domainspecific, because they introduce powerful abstractions meaningful only within the domain of observed systems. In order to use domain-specific languages for database applications, a mapping to SQL is required. In this paper, we deal with metaprogramming concepts required for easy definition of such mappings. Using an example domain-specific language, we provide an evaluation of mapping performance.	abstraction layer;data manipulation language;domain-specific language;emoticon;metaprogramming;mined;nintendo ds and 3ds storage devices;object lifetime;overhead (computing);relational database;run time (program lifecycle phase);sql;schema evolution;software product line;stored procedure;subroutine;version control	Jernej Kovse;Christian Weber;Theo Härder	2004		10.1007/978-3-540-30464-7_49	metaprogramming;sql;entity–relationship model;relational database;computer science;domain-specific language;information integration;data mining;reuse;database;abstraction;metacompiler;information system;algorithm	PL	-35.01097917682504	11.706277095262546	168202
e95647e0d3ebae2683059e3c2c4a3bc10580374a	mining roles with semantic meanings	security and protection;database application;information systems;top down;role based access control;data mining;database management;identity management;rbac;access controls;theoretical foundation;role engineering;role mining;formal concept analysis	With the growing adoption of role-based access control (RBAC) in commercial security and identity management products, how to facilitate the process of migrating a non-RBAC system to an RBAC system has become a problem with significant business impact. Researchers have proposed to use data mining techniques to discover roles to complement the costly top-down approaches for RBAC system construction. A key problem that has not been adequately addressed by existing role mining approaches is how to discover roles with semantic meanings. In this paper, we study the problem in two settings with different information availability. When the only information is user-permission relation, we propose to discover roles whose semantic meaning is based on formal concept lattices. We argue that the theory of formal concept analysis provides a solid theoretical foundation for mining roles from userpermission relation. When user-attribute information is also available, we propose to create roles that can be explained by expressions of user-attributes. Since an expression of attributes describes a real-world concept, the corresponding role represents a real-world concept as well. Furthermore, the algorithms we proposed balance the semantic guarantee of roles with system complexity. Our experimental results demonstrate the effectiveness of our approaches.	algorithm;complexity;data mining;experiment;formal concept analysis;identity management;role-based access control;top-down and bottom-up design	Ian Molloy;Hong Chen;Tiancheng Li;Qihua Wang;Ninghui Li;Elisa Bertino;Seraphin B. Calo;Jorge Lobo	2008		10.1145/1377836.1377840	computer science;knowledge management;role-based access control;data mining;database;computer security	Security	-40.20408239361062	14.445952370479738	168424
19bc66b8e5d263c45e61a656c4a6a2709c130735	the image object content architecture	objet;architecture systeme;image processing;concepcion sistema;procesamiento imagen;object;traitement image;system design;arquitectura sistema;system architecture;objeto;conception systeme	Technical advances to image processing and the availability of the resulting technologies at reasonable cost have helped to promote the use of images in office, engineering, and scientific environments. As evidence of this use, a wide variety of applications and products designed for image processing have been introduced into the market in recent years. In order to encompass different applications and products in a single image processing system and to allow image data to be exchanged and interpreted consistently throughout the system, IBM has introduced the Image Object Content Architecture (IOCA). This paper discusses requirements for the architecture, concepts of the architecture, use of the architecture in the different data stream environments used by image processing systems, and the IOCA function sets that have been defined for interchange within Systems Application Architecture™ environments.		Yuji Hakeda	1990	IBM Systems Journal	10.1147/sj.293.0333	image processing;computer science;electrical engineering;object;programming language;systems architecture;systems design	Robotics	-34.33301103207201	17.710053032390825	169208
6faeeccbf20f031357a1a4453df6187d1b136d55	a web service composition modeling and evaluation method used petri net	modelizacion;modeling technique;red www;red petri;web semantique;evaluation method;reseau web;service web;service process;web service;proceso servicio;web service composition;modelisation;web application design;internet;processus service;object oriented;web semantica;semantic web;oriente objet;world wide web;petri net;modeling;orientado objeto;reseau petri;servicio web	The emergence of Web services opens a new way of Web application design and development. It has led to more interest into Web service composition, which is an active area of research. The formidable problem of efficient and effective composition of existing Web services is the subject of much current attention. The study of modeling is one of the most important parts and a key layer of Web service composition. Therefore, there is a need for modeling techniques and tools for reliable Web services composition. In this paper, we propose a method used an Advanced Object-Oriented Petri Net (AOOPN) to model and evaluation the process of Web services composition. This method is expressive enough to capture the semantics of complex Web services combinations.	emergence;petri net;service composability principle;web application;web service	Xiaoning Feng;Qun Liu;Zhuo Wang	2006		10.1007/11610496_125	web service;web application security;web development;web modeling;the internet;data web;systems modeling;web analytics;web-based simulation;web design;web standards;computer science;ws-policy;semantic web;social semantic web;ws-addressing;database;multimedia;web intelligence;object-oriented programming;ws-i basic profile;law;world wide web;petri net	Web+IR	-39.099993913197245	15.062334090375776	170214
0933cfb5d9c63528a91cda6f8c2ccb8b0b27741d	learning to attach semantic metadata to web services	bayes estimation;distributed system;marco;cluster algorithm;analyse amas;fiabilidad;reliability;learning algorithm;systeme reparti;data integrity;red www;metadata;ingenierie connaissances;integration information;naive bayes;reseau web;standard;service web;semantics;intelligence artificielle;algorithme apprentissage;web service;semantica;semantique;classification;semantic metadata;estimacion bayes;information integration;sistema repartido;cluster analysis;internet;machine learning;bayesian learning;fiabilite;integracion informacion;inferencia;metadonnee;artificial intelligence;world wide web;analisis cluster;metadatos;inteligencia artificial;etalon;algoritmo aprendizaje;clasificacion;langage html;inference;html language;lenguaje html;estimation bayes;knowledge engineering	Emerging Web standards promise a network of heterogeneous yet interoperable Web Services. Web Services would greatly simplify the development of many kinds of data integration and knowledge management applications. Unfortunately, this vision requires that services describe themselves with large amounts of semantic metadata “glue”. We explore a variety of machine learning techniques to semiautomatically create such metadata. We make three contributions. First, we describe a Bayesian learning and inference algorithm for classifying HTML forms into semantic categories, as well as assigning semantic labels to the form’s fields. These techniques are important as legacy HTML interfaces are migrated to Web Services. Second, we describe the application of the Naive Bayes and SVM algorithms to the task of Web Service classification. We show that an ensemble approach that treats Web Services as structured objects is more accurate than an unstructured approach. Finally, we describe a clustering algorithm that automatically discovers the semantic categories of Web Services. All of our algorithms are evaluated using large collections of real HTML forms and Web Services.	algorithm;cluster analysis;form (html);html;information retrieval;interoperability;knowledge management;machine learning;naive bayes classifier;semiconductor industry;web service;web standards;world wide web	Andreas Heß;Nicholas Kushmerick	2003		10.1007/978-3-540-39718-2_17	web service;fabry–pérot interferometer;web development;web modeling;the internet;naive bayes classifier;data web;web mapping;html;web design;biological classification;semantic grid;web standards;computer science;artificial intelligence;information integration;ws-policy;machine learning;semantic web;knowledge engineering;social semantic web;data integrity;data mining;reliability;ws-addressing;semantic web stack;database;semantics;cluster analysis;web intelligence;ws-i basic profile;bayesian inference;web 2.0;metadata;world wide web;website parse template	Web+IR	-38.01971778375788	12.077582339408014	172023
55bb057808e020e10fedd4267621e18eb818c2fb	managing xml data with evolving schema	xml schema;user needs;xml database;intelligent systems	XML databases evolve during their lifetime to address new requirements and reflect changes in the real world. The structure of XML data defined by either DTD or XML schema, undergoes changes to accommodate changing user needs. In this paper, we focus on the problem of DTD change operations which will help users to perform necessary changes to the schema. We propose a set of three high-level operators as an extension to existing set of primitive DTD change operators [3]. These operators are supported by algorithms to generate XSLT scripts which transform instances of current DTD to conform with the changed DTD.	algorithm;high- and low-level;requirement;schema evolution;xml database;xml schema;xslt	B. V. N. Prashant;P. Sreenivasa Kumar	2006			xml validation;xml encryption;simple api for xml;xml;relax ng;intelligent decision support system;xml schema;geography markup language;streaming xml;computer science;xs3p;document structure description;xml framework;data mining;xml database;xml schema;database;schematron;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange	DB	-34.736197891049066	11.269169702599905	172739
92b766493a7dc876d9d234a6e1a803edf2ef8968	dynamic ontologies and semantic web rules as bigraphical reactive systems		We show how a subpart of OWL ontologies and Semantic Web Rule Language (SWRL) rules can be represented as bigraphs and bigraphical reaction rules. While OWL allows for defining a static ontology and SWRL allows for inferring and adding more information to an ontology, we show that the bigraphical representation of ontologies and rules naturally allows for more general dynamic changes and modification of ontologies. We describe how the representation is implemented in BigRED, a recently developed Eclipse-based bigraphical editing tool allowing to simulate and perform state exploration in the defined systems using the BigMC bigraph model checker which is integrated in the BigRED editor. We discuss the potential uses of the approach for model-driven design and analysis of context-aware systems, and propose a first naive method to deal with the frame and ramification problems in the bigraphical ontology.	ontology (information science);semantic web	Wusheng Wang;Thomas T. Hildebrandt	2013		10.1007/978-3-319-08260-8_8	social semantic web;database;world wide web	Web+IR	-40.199445474283536	12.961774933840847	172886
1501e4c7815d52979d07b357bacbd26ba10028e6	an agent based semi-informed protocol for resource discovery in grids	distributed system;replication;multiagent system;systeme reparti;resource discovery;multi agent system;metadata;agent based;resource allocation;resource management;interrogation base donnee;interrogacion base datos;semantics;intelligence artificielle;semantica;semantique;classification;replicacion;grid;gestion recursos;sistema repartido;diffusion information;rejilla;information dissemination;metadonnee;grille;gestion ressources;artificial intelligence;simulation analysis;metadatos;asignacion recurso;inteligencia artificial;difusion informacion;information system;allocation ressource;sistema multiagente;database query;clasificacion;systeme information;systeme multiagent;sistema informacion	A Grid information system should rely upon two basic features: the replication and dissemination of information about Grid resources, and an intelligent logical distribution of such information among Grid hosts. This paper examines an approach based on multi agent systems to build an information systems in which metadata related to Grid resources is disseminated and logically organized according to a semantic classification of resources. Agents collect resources belonging to the same class in a restricted region of the Grid, so decreasing the system entropy. A semi-informed resource discovery protocol exploits the agents’ work: query messages issued by clients are driven towards “representative peers” which maintain information about a large number of resources having the required characteristics. Simulation analysis proves that the combined use of the resource mapping protocol (ARMAP) and the resource discovery protocol (ARDIP) allows users to find many useful results in a small amount of time.	communications protocol;entropy (information theory);grid computing;information system;multi-agent system;organizing (structure);programming paradigm;self-organization;self-replicating machine;semiconductor industry;simulation;web service	Agostino Forestiero;Carlo Mastroianni;Giandomenico Spezzano	2006		10.1007/11758549_139	replication;biological classification;semantic grid;resource allocation;computer science;artificial intelligence;resource management;data mining;database;semantics;grid;metadata;world wide web;information system	HPC	-38.71597565380734	14.1945166557168	173110
b739dea364d2cfa2fa88d58e0aebb9e84216d4d8	validation and interactivity of web api documentation	hypermedia markup languages;protocols;api;http;vocabulary;web api;resource description framework;knowledge representation languages;html;internet;application program interfaces;xml;validation;context;documentation	Many Web APIs (by which we mean ones using HTTP as the application protocol) do not publish a machine-readable API description (in a language such as WADL or WSDL) but only provide human-readable documentation, usually in HTML. This documentation may be machine-generated, or it may be hand-edited in which case there is the possibility of errors being introduced into the API description. In this paper we present a Web Interface Language (WIfL) vocabulary for API documentation, which is intended to be embedded in HTML using RDFa annotations. We present the semantics of WIfL, including a formal presentation of inheritance and validation. We discuss our WIfL tools, which include a dynamically generated console for interacting with an API's reference implementation, and a validator which can check an API for internal consistency.	application programming interface;documentation;embedded system;html;human-readable medium;hypertext transfer protocol;interaction;interactivity;java api for restful web services (jax-rs);rdfa;reference implementation;validator;vocabulary;web api;web application description language;web services description language;web crawler;xslt	Peter J. Danielsen;Alan Jeffrey	2013	2013 IEEE 20th International Conference on Web Services	10.1109/ICWS.2013.76	xml;html;documentation;computer science;web api;database;programming language;world wide web	SE	-40.392749308204365	11.435463396339287	173293
cc6825d9adec844f65692e3ae5a5dcbe1889a66c	a cooperative system environment for design, construction and maintenance of bridges	modelizacion;distributed system;database system;groupware;multiagent system;base donnee;systeme reparti;productivite;systeme cooperatif;visualizacion;maintenance;reutilizacion;data reuse;database;base dato;rfid tag;product model;productividad;reuse;identificacion sistema;modelisation;visualization;sistema repartido;construction system;cooperative systems;system identification;visualisation;identification radiofrequence;ingeniero consejero;mantenimiento;radio frequency identification;consultant;sistema construccion;ingenierie simultanee;productivity;ingenieur conseil;ingenieria simultanea;sistema multiagente;collecticiel;modeling;visual system;identification systeme;systeme construction;reutilisation;concurrent engineering;systeme multiagent	This paper proposes a new cooperative system environment for design, construction and maintenance of civil structures, especially bridges. We identified seven tools required to improve the productivity of design, construction and maintenance of bridges. They are product models, visualization system, communication tool, task systems with agents, database system, radio frequency identification (RFID) tags, and data reuse facility. These tools are to be integrated and used by owners, design consultants, contractors, manufacturers, etc., who can exchange and share structural, construction and inspection data, information and knowledge with this system.		Nobuyoshi Yabuki;Tomoaki Shitani;Hiroki Machinaka	2005		10.1007/11555223_22	radio-frequency identification;simulation;visualization;computer science;database	Robotics	-38.70417493968607	16.966720540632025	174094
5bfad949d9d7ad5b4399fdead186dc214f3cb475	towards a classification of web service feature interactions	modelizacion;commerce electronique;comercio electronico;e commerce;distributed computing;service web;satisfaccion;web service;classification;orientado servicio;satisfaction;qualite service;feature interaction;web service composition;service utilisateur;modelisation;calculo repartido;oriente service;servicio usuario;user service;modeling;user satisfaction;calcul reparti;clasificacion;service quality;electronic trade;servicio web;service oriented;calidad servicio	Web services promise to allow businesses to adapt rapidly to changes in the business environment, and the needs of different customers. The rapid introduction of new web services into a dynamic business environment can lead to undesirable interactions that negatively impact service quality and user satisfaction. In previous work, we have shown how to model such interactions between web services as feature interactions, and reason about undesirable side-effects of web service composition. In this paper we present the results of subsequent research on a classification of feature interactions among web services. Such a classification is beneficial as we can then search for ways of detecting and resolving each class of feature interaction in a generic manner. To illustrate the interactions we use a fictitious e-commerce scenario.	e-commerce;e-services;feature interaction problem;sensor;service composability principle;web service	Michael Weiss;Babak Esfandiari;Yun Luo	2005		10.1007/11596141_9	e-commerce;web service;web modeling;web query classification;systems modeling;biological classification;computer science;ws-policy;social semantic web;database;distributed computing;law;world wide web;service quality	Web+IR	-39.322784229076376	15.523287431732625	174166
fde55d9af7992638a77776ca2d7b0efecdb5f391	agent-supported information retrieval for tracking and tracing	software;commerce electronique;description systeme;system description;architecture systeme;comercio electronico;logistique;logiciel;information retrieval;advantage;transporte;agente;agent;transport;internet;logistics;logicial;arquitectura sistema;ventaja;descripcion sistema;system architecture;avantage;edifact;electronic trade;logistica	In recent years interorganizational communication and coordination have relied increasingly on the exchange of EDIFACT-messages. This is especially true for the logistics sector. In order for the sender to keep control over the forwarder's proper delivery process, status messages prove that certain milestones have been achieved. Taking into account that only a very small number of shipments face problems leading to a delay in delivery, this principle of pushed information causes a large amount of unnecessary EDIFACT-messages. In addition, many for-warders already allow their customers to request status information via WWW. A software agent based concept is presented to cope with the problem described, taking advantage of the WWW-services provided. The system called ECTL-Monitor (Electronic Commerce Transport Logistics) was developed in cooperation with a large company sending more than 10 million shipments per year. The main idea of the concept is to leave data of the transport process on the forwarders' computer systems and to access these remote system when necessary. On demand software agents collect status information of all forwarders involved in the transportation chain, which very often includes up to four diierent logistics service companies.	agent-based model;computer;e-commerce;edifact;information retrieval;logistics;software agent;track and trace;www	Dominik Deschner;Oliver Hofmann;Stefan Reinheimer;Freimut Bodendorf	1998		10.1007/BFb0053674	logistics;transport;the internet;simulation;advantage;computer science;artificial intelligence;operating system;edifact;database;distributed computing;law;computer security;systems architecture	AI	-38.966483049093235	16.358998386653578	174322
f9232e182cf8d6e89d1bd94873ada2575a16ee68	application of web service in web mining	fouille web;distributed computing;service web;web service;data mining;semistructured data;semi structured data;dato semi estructurado;fouille donnee;calculo repartido;web mining;busca dato;calcul reparti;servicio web;donnee semistructuree	To solve the problems we now encounter in web mining, We first propose a new distributed computing strategy——web service. It suggests building a web mining system based on web service, which can share and manage semi-structured data from heterogeneous platforms. Moreover, the system can integrate the mining services and algorithms, improve the efficiency of web mining, and make the mining results easier to access. We also conduct an experiment on selecting useful words to simulate the realization of the web mining system on the Microsoft.NET platform, which demonstrates the importance of Web service in Web mining.	algorithm;distributed computing;semi-structured data;semiconductor industry;simulation;web mining;web service	Beibei Li;Jiajin Le	2004		10.1007/978-3-540-30497-5_152	web service;web application security;web mining;semi-structured data;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;web navigation;social semantic web;web page;data mining;database;web intelligence;web 2.0;world wide web;web coverage service	Web+IR	-36.893263561076786	12.10252022928908	174640
003203f513b959f47b4d70f2bfd0b93e536629d1	agent-based intelligent clinical information system for persistent lifelong electronic medical record	clinical data;multiagent system;systeme intelligent;hospital;medical record;aplicacion medical;informacion electronica;agent based;executive function;integration information;sistema inteligente;service web;web service;systeme information gestion;information electronique;hopital;information integration;dossier medical;data privacy;agent intelligent;system integration;information management;historial clinico;integracion informacion;intelligent system;intelligent agent;management information systems;clinical information system;electronic information;medical application;functionality;agente inteligente;fonctionnalite;systeme gestion base donnee;information system;electronic medical record;sistema multiagente;legacy system;sistema gestion base datos;database management system;funcionalidad;systeme information;systeme multiagent;application medicale;sistema informacion	Hospital systems are heterogeneous and the clinical information scattered. The development of a clinical information system to integrate the information among hospitals and make legacy systems cooperate is thus difficult. For efficient clinical information management and system integration, clinical data, patient data privacy and integration of records need to be well organized and conform to relevant standards. In this paper we develop an agent-based intelligent clinical information system for persistent lifelong electronic medical record. Functional entities are divided according to tasks and implemented as collaborating agents. These agents reside on a multiagent platform which provides communication and invocation of execution functionality. We adopt HL7 standards for both clinical data and web services, in order to provide a userenvironment via the internet.	agent-based model;entity;health level 7;information management;information privacy;information system;internet;legacy system;system integration;web service	Il Kon Kim;Ji Hyun Yun	2003		10.1007/978-3-540-39896-7_17	web service;system of record;computer science;artificial intelligence;information integration;data mining;information management;world wide web;computer security;legacy system;intelligent agent;information system;medical record;system integration	DB	-38.733062767451045	14.576977747490085	178107
5f8939f7085dbc02fd9d2a2b43cc552055980268	service-oriented data and process models for personalization and collaboration in e-business	modelizacion;commerce electronique;confiance;modeling technique;psychologie sociale;comercio electronico;service system;tourisme;finance;negociation;service orientation;trust management;customization;service web;personnalisation;recommandation;service process;web service;orientado servicio;proceso servicio;data model;modelisation;confidence;tourism;processus service;confianza;negociacion;bargaining;psicologia social;personalizacion;recomendacion;recommendation;social psychology;modele donnee;oriente service;process model;turismo;business value;modeling;electronic trade;finanzas;servicio web;service oriented;data models	Providing personalized and collaborative services on the web is critical for creating customer and business values in many e-business domains such as e-shop, e-marketplace, e-news, e-learning, e-finance, and e-tourism. The goal of this paper is to propose a generic service-oriented framework and modeling techniques for facilitating the development of a powerful personalized and collaborative e-service system that is adaptable for use in various e-business applications. A unified data model as well as integrated process models for supporting advanced e-service requirements including search, recommendation, customization, collaboration, negotiation, and trust management are presented and discussed with examples.	electronic business;personalization;service-oriented architecture;service-oriented device architecture	Chien-Chih Yu	2006		10.1007/11823865_8	web service;data modeling;systems modeling;data model;computer science;knowledge management;business value;process modeling;database;confidence;tourism;law;world wide web;negotiation;service system	ML	-39.168235501401625	15.27603834430152	178375
46eb220261a4db19fcf50036853e00bf1b6ebb2b	using annotations in the naked objects framework to explore data requirements	agile methods;agile methods of software development;conceptual model;data requirements exploration;requirements engineering;requirements specification;requirement engineering;software development;data abstraction;data abstractions;requirement specification;object relational;conceptual data design	The creation of conceptual data design that appropriately represents specific application domain is one of the main challenges in requirements engineering. An initiative to help designers is the Naked Objects framework, where it is possible to interact with conceptual model in a limited way. The interactions are restricted to entity creations and single object-relations. We created an extension of the Naked Objects framework using annotations to allow manipulation of higher level abstractions as specialization and object-relationship. These abstractions allow better interactions between the domain specialist and designers. The use of our approach to explore and validate data requirements has several benefits: 1) It reduces conceptual specification problems (like poorly data requirements identification); 2) It narrows the distance among domain and design specialists; 3) It allows the simultaneous exploration of the conceptual data design and the system requirements.	application domain;data modeling;entity;interaction;naked objects;norm (social);partial template specialization;requirement;requirements engineering;system requirements	Marcos E. B. Broinizi;João Eduardo Ferreira;Alfredo Goldman	2008		10.1145/1363686.1363838	data modeling;requirements analysis;software requirements specification;conceptual model;data model;computer science;conceptual model;software development;requirement;system requirements specification;domain model;agile software development;database;requirements engineering;programming language;management;non-functional requirement	SE	-34.479740920128094	13.307064276541611	178879
dfef4a45f8ed9fc80f86043845887c2a766a569c	managing identities via interactions between ontologies	ontologie;red www;metadata;xml language;reseau web;customization;personnalisation;identity management;comportement utilisateur;metadonnee;personalizacion;world wide web;ontologia;metadatos;user behavior;computational efficiency;ontology;langage xml;lenguaje xml;comportamiento usuario	In this paper, we describe how an identity management system can be based on user ontologies in order to deal with complex attributes that are needed to model user interests or relationships. The problem of computing efficient bindings between ontology based metadata and XML based standards like SAML is also discussed.	interaction;ontology (information science)	Paolo Ceravolo	2003		10.1007/978-3-540-39962-9_74	xml;computer science;ontology;data mining;database;metadata;world wide web;identity management	Vision	-37.82487549649548	12.211396569133072	179026
fed530c71b7aa1b7ecf46cee8da327f7f77f1726	evaluation of rdf(s) and daml+oil import/export services within ontology platforms	informatica;representacion conocimientos;ontologie;red www;import;reseau web;semantics;systematique;semantica;semantique;importation;importacion;internet;sistematica;ontology evaluation;taxonomy;semantic web;world wide web;ontologia;functionality;fonctionnalite;knowledge representation;tool evaluation;representation connaissances;ontology;funcionalidad	Both ontology content and ontology building tools evaluations play an important role before using ontologies in Semantic Web applications. In this paper we try to assess ontology evaluation functionalities of the following ontology platforms: OilEd, OntoEdit, Protégé-2000, and WebODE. The goal of this paper is to analyze whether such ontology platforms prevent the ontologist from making knowledge representation mistakes in concept taxonomies during RDF(S) and DAML+OIL ontology import, during ontology building and during ontology export to RDF(S) and DAML+OIL. Our study reveals that most of these ontology platforms only detect a few mistakes in concept taxonomies when importing RDF(S) and DAML+OIL ontologies. It also reveals that most of these ontology platforms only detect some mistakes in concept taxonomies during building ontologies. Our study also reveals that these platforms do not detect any taxonomic mistake when exporting ontologies to such languages.	daml+oil;knowledge representation and reasoning;ontology (information science);ontology engineering;semantic web;taxonomy (general)	Asunción Gómez-Pérez;Mari Carmen Suárez-Figueroa	2004		10.1007/978-3-540-24694-7_12	upper ontology;ontology alignment;the internet;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;artificial intelligence;semantic web;ontology;data mining;database;semantics;ontology-based data integration;web ontology language;world wide web;owl-s;process ontology;taxonomy;suggested upper merged ontology	Web+IR	-37.97649804499311	12.393396504699197	179956
b70dba3778b73f66faec8e585c1bbd16a926ab47	event indexing systems for efficient selection and analysis of hera data	computer aided analysis;gestion informacion;interaction electron proton;analyse assistee;etude theorique;data management;software systems;data analysis;object oriented database management system;design and implementation;electron proton interactions;information management;indexation;gestion base donnee;analyse donnee;gestion information;theoretical study;data base management	The design and implementation of two software systems introduced to improve the efficiency of offline analysis of event data taken with the ZEUS Detector at the HERA electron-proton collider at DESY are presented. Two different approaches were made, one using a set of event directories and the other using a tag database based on a commercial object-oriented database management system. These are described and compared. Both systems provide quick direct access to individual collision events in a sequential data store of several terabytes, and they both considerably improve the event analysis efficiency. In particular the tag database provides a very flexible selection mechanism and can dramatically reduce the computing time needed to extract small subsamples from the total event sample. Gains as large as a factor 20 have been obtained. now at Fermi National Accelerator Laboratory, Batavia, USA on leave at Stanford Linear Accelerator Laboratory, Stanford, USA now at NIKHEF, Amsterdam, The Netherlands	data store;database;electron;online and offline;random access;software system;tag system;terabyte	L. A. T. Bauerdick;Adrian Fox-Murphy;Tobias Haas;Stefan Stonjek;Enrico Tassi	2001	CoRR	10.1016/S0010-4655(01)00162-X	data management;computer science;data mining;database;information management;data analysis;world wide web;software system	DB	-33.86412593091803	15.22941904114336	181671
93afb9b6d915f0b41b635a587e79d9c14c0862f5	agent warehouse: a new paradigm for mobile agent deployment	mobile agent deployment;agent warehouse;information science;web agents;mobile agents;information filtering;collaboration;software agents;multi agent systems;mobile web;internet;agent execution;mobile agents internet web server bandwidth collaboration software agents information science mobile computing information processing information filtering;information processing;bandwidth;offline autonomous negotiation;web server;mobile agent;mobile computing;web server host agent warehouse mobile agent deployment web agents internet bandwidth reduction multi agent systems offline autonomous negotiation agent execution;internet mobile agents multi agent systems;bandwidth reduction;web server host	"""This paper describes a novel concept of agent warehouse. Non-mobile Web agents typically operate from their users' computer and make request for data possibly from a very far location. In addition, much of this data will be irrelevant to the user, thus aggravating the bandwidth scarcity problem of the Internet. With current mobile agent paradigm, many of these problems such as bandwidth reduction and off-line autonomous negotiation are solved. However, this paradigm does have some significant limitations in its common deployment scenarios; system resource consumption, server collaboration, and accumulated agent size along the travelling path, etc. are some typical ones. These limitations are becoming more important when multiple visits to the same server host are required: updating time of host information is nondeterministic and the decision of negotiation is also not simultaneous. In this paper, the intermediate """"proxy-like"""" agent warehouse is proposed to address these issues. The agent warehouse locates near the data sources and supports agent execution, thus allowing agents to operate much closer to these data sources and minimising the effect of discarded search results. In addition, it is able to provide more resources than a normal Web server host does as it is dedicated to cater for agents. More importantly, even if the remote site does not support agent execution, the agent will still be able to complete its task through the warehouse. This changes the typical approach of how agents can be deployed by providing a more generic, flexible system environment for agents to execute."""	mobile agent;software deployment	Chi-Hung Chi;John Sim;Kwok-Yan Lam	2002		10.1109/TAI.2002.1180838	the internet;mobile web;information processing;information science;computer science;artificial intelligence;software agent;mobile agent;database;distributed computing;mobile computing;world wide web;bandwidth;web server;collaboration	Robotics	-37.334682202317815	15.96167262018419	182002
8ad44e0d388c1c8340b13ddb30abb29e1eb8417a	ontology learning from text: a soft computing paradigm	modelizacion;lenguaje natural;lenguaje documental;ontologie;text;web pages;red www;availability;disponibilidad;soft computing;web semantique;langage naturel;reseau web;semantics;structure sandwich;information access;texte;calculo flexible;semantica;semantique;080707 organisation of information and knowledge resources;modelisation;sandwich structure;langage documentaire;ontology learning;internet;web semantica;natural language;calcul souple;acces information;semantic web;information language;world wide web;ontologia;acceso informacion;reseau neuronal;estructura sandwich;multilinguisme;texto;modeling;disponibilite;ontology;red neuronal;multilingualism;domain specificity;multilinguismo;neural network	Text-based information accounts for more than 80% of today's Web content. They consist of Web pages written in different natural languages. As the semantic Web aims at turning the current Web into a machineunderstandable knowledge repository, availability of multilingual ontology thus becomes an issue at the core of a multilingual semantic Web. However, multilingual ontology is too complex and resource intensive to be constructed manually. In this paper, we propose a three-layer model built on top of a soft computing framework to automatically acquire a multilingual ontology from domain specific parallel texts. The objective is to enable semantic smart information access regardless of language over the Semantic Web.	ontology learning;programming paradigm;soft computing	Rowena Chau;Kate Smith-Miles;Chung-Hsing Yeh	2006		10.1007/11893295_34	natural language processing;upper ontology;availability;semantic computing;web development;web modeling;the internet;data web;systems modeling;bibliographic ontology;ontology inference layer;semantic grid;web standards;computer science;ontology;artificial intelligence;semantic web;ontology;social semantic web;web page;semantic web stack;database;semantics;soft computing;web intelligence;ontology-based data integration;natural language;programming language;world wide web;owl-s;website parse template;semantic analytics;artificial neural network;algorithm	NLP	-37.88301085547499	12.080977436418486	182222
5e5fbe22fddd5d807b8c42e41a167596952e4bc2	reengineering object-oriented fuzzy spatiotemporal data into xml		With the rapid development of the Internet, XML has become the defacto standard for integrating and exchanging data. Since more and more business data are stored in object-oriented database, we study the methodology of modeling fuzzy spatiotemporal data and transforming fuzzy spatiotemporal data from object-oriented databases to XML as well. In order to allow for better and platform independent sharing of fuzzy spatiotemporal data stored in an object-oriented format, we devise a fuzzy spatiotemporal data model in object-oriented database to capture the semantics of spatiotemporal features. In particular, XML schema best describes the existing fuzzy object-oriented schema, and we investigate the transforming rules of spatiotemporal data from fuzzy object-oriented database to XML. Furthermore, an instance demonstrates the validation of our approach. Such approach of transformation can provide a significant consolidation of the interoperability of fuzzy spatiotemporal data from object-oriented databases to XML.	code refactoring;data model;interoperability;semiconductor consolidation;spatiotemporal database;xml schema	Luyi Bai;Zhiyi Jia;Jiemin Liu	2018	IEEE Access	10.1109/ACCESS.2018.2809858	the internet;data mining;fuzzy logic;interoperability;data model;xml;data modeling;xml schema;object-oriented programming;distributed computing;computer science	DB	-34.49153477631936	11.248097381006914	182251
9691e6ce20daa1550491eab8b88e01923edb653c	service publishing and discovering model in a web services oriented peer-to-peer system	estensibilidad;developpement logiciel;modelizacion;fiabilidad;reliability;red www;par a par;reseau web;service web;web service;peer to peer system;orientado servicio;intergiciel publication souscription;modelisation;poste a poste;intergicial editor suscriptor;desarrollo logicial;fiabilite;software development;architecture basee modele;world wide web;oriente service;extensibilite;scalability;service oriented architecture;peer to peer;modeling;publish subscribe middleware;model driven architecture;servicio web;service oriented;arquitectura basada modelo	To enhance the reliability and scalability of the service oriented architecture, this paper introduces a Web Services Oriented Peer-to-peer (WSOP) architecture with a combination of centralized and decentralized characteristics, and gives a framework of service publishing and discovery model based on WSOP architecture.	centralized computing;peer-to-peer;scalability;service-oriented architecture;web service	Ruixuan Li;Zhi Zhang;Wei Song;Feng Ke;Zhengding Lu	2005		10.1007/11531371_78	enterprise architecture framework;web service;reference architecture;space-based architecture;scalability;simulation;systems modeling;computer science;applications architecture;service delivery framework;software development;service-oriented architecture;reliability;database;service;solution architecture;law;world wide web	Web+IR	-39.065719492922135	15.255462168474411	183660
272f2eaf8b7bc528f37ebf9b1be6f67b839cbf92	supporting user-defined activity spaces	activity space;schema definition;hypertext model;schema integration;object oriented systems;semantic net;object oriented system;specific activity;structural transformation;intelligent system;meta model	Activity spaces are usually task-specific and only common to a group of people who work together in a certain application domain. It is desirable to enable users to define and modify activity spaces according to their needs. However, many users are unable to use a pre-defined activity space correctly or incapable of formally defining an activity space. This work tries to solve these problems 1) by developing a flexible hypertext meta-model which can represent activity space semantics, 2) developing an example-based definition tool for users to create task-specific activity spaces, 3) providing intelligent aid in using these activity spaces, and 4) providing a flexible space for adopting existing and emergent patterns. A system (COWFISH) with the above components has been implemented and tested at GMD-IPSI. Examples and initial applications have shown that using the system users can easily define the schemata of many activity spaces and hyperdocuments. They can also create new activity spaces with stepwise structure transformation and through reusing existing activity spaces. The system then uses the schema knowledge to maintain the semantic consistency of the activity space instances and to provide users with context-sensitive examples, choices, and explanations.	activity diagram;activity recognition;application domain;context-sensitive grammar;emergence;general material designation;hypertext;metamodeling;spaces;stepwise regression	Weigang Wang;Jörg M. Haake	1997		10.1145/267437.267450	metamodeling;semi-structured model;logical schema;computer science;knowledge management;conceptual schema;specific activity;database;programming language	HCI	-34.468835262826076	12.535496514055925	184548
bcbb4bfe89d9cafbe96125c0826610a582711d42	tool support for the design and management of spatial context models	context aware application;personalization in databases and information systems;spatial context;context information;advanced database applications;spatial data;tool support;data modeling and database design;information space;data model;graphic user interface;information system;database design;xml and databases	A central task in the development of context-aware applications is the modeling and management of complex context information. In this paper, we present the NexusEditor, which eases this task by providing a graphical user interface to design schemas for spatial context models, interactively create queries, send them to a server and visualize the results. One main contribution is to show how schema awareness can improve such a tool: the NexusEditor dynamically parses the underlying data model and provides additional syntactic checks, semantic checks, and short-cuts based on the schema information. Furthermore, the tool helps to design new schema definitions based on the existing ones, which is crucial for an iterative and user-centric development of context-aware applications. Finally, it provides interfaces to existing information spaces and visualization tools for spatial data like GoogleEarth.		Nazario Cipriani;Matthias Wieland;Matthias Großmann;Daniela Nicklas	2009		10.1007/978-3-642-03973-7_7	idef1x;information schema;data model;computer science;spatial contextual awareness;data mining;graphical user interface;database;spatial analysis;context model;database schema;information retrieval;information system;database design	HCI	-35.18754551705379	13.511052961972865	186531
a9b97ce8504a55f42d475ac3e18ce3b51ae049aa	expressive security policy rules using layered conceptual graphs	distributed environment;conceptual graph;security policy	A method must be provided to support the analysis of security policy rules interdependencies in a (possibly distributed) environment. We propose a Conceptual Graphs based language that will allow us to represent the structure of information and to employ reasoning for consistency checking. We motivate our choice of language by the gained expressivity, the potential for depicting policy associations rigourously and by associated reasoning capabilities. We explain our approach in the context of security requirements for medical systems. We evaluate our work theoretically, by means of an example of a real world policy rule.	conceptual graph	Madalina Croitoru;Liang Xiao;David Dupplaw;Paul H. Lewis	2007		10.1007/978-1-84800-094-0_18	computer security model;conceptual graph;computer science;security policy;theoretical computer science;data mining;database;distributed computing environment	Logic	-40.256651423120815	14.636382748636587	186882
d051137cbce5ac26e94df401cde709412944cdcb	the research of data communication on distributed medical system realized with .net technology	distributed web service net remoting platform crossing;net remoting data communication distributed medical system internet distributed computing platform heterogeneous information communication web service;web services distributed databases internet medical information systems;heterogeneous databases;distributed computing;hospitals;web service;data communication;heterogeneous information;servers;internet;heterogeneous information communication;medical information systems;web services;xml;platform crossing;distributed databases;data communication web services distributed databases biomedical engineering data engineering hospitals xml simple object access protocol application software distributed computing;distributed;net remoting;simple object access protocol;distributed computing platform;distributed medical system	Nowadays, more and more application systems are based on Internet or the distributed computing platform of salability and extensibility offering by the Internet. However, the development of the distributed medical system, which was in different region with crossing-platform, enhanced the difficulty of data communication in the medical information. And the original distributed technologies had a lot of disadvantages which can not solve the present problem of heterogeneous information communication. In this paper, two kinds of distributed technologies that based on .NET Remoting and Web Service are compared and analyzed. The .NET Remoting is utilized to provide internal user with the nimble and effective service, at the same time the distributed heterogeneous of Web Service is utilized to provide server port for exterior. And then the information inquiry among distributed heterogeneous database of medical system is completed, then it complies with the development requirement of message communication in the modern networks.	.net remoting;distributed computing;extensibility;heterogeneous database system;internet;server (computing);web service	Qi Zhao;Yanliao Tan;Lili Xing	2009	2009 First International Workshop on Database Technology and Applications	10.1109/DBTA.2009.179	web service;computer science;database;distributed computing;distributed design patterns;.net remoting;law;world wide web;distributed database	HPC	-36.48058613763969	14.342809393899595	187143
1b6a26a5982ebc2a7e93083fdcc08a0800bfaf9d	information fusion for intelligent agent-based information gathering	evaluation performance;multiagent system;performance evaluation;red www;agent based;evaluacion prestacion;reseau web;data fusion;information gathering;reseau collecte;agent intelligent;fusion donnee;intelligent agent;world wide web;information fusion;agente inteligente;information system;sistema multiagente;fusion datos;red recoleccion;systeme information;systeme multiagent;gathering system;sistema informacion	This paper discusses the problem of information fusion for agent-based information gathering systems. The framework of information gathering in multi-agent environments is presented firstly, and then a cooperative fusion algorithm is presented for unstructured documents. The performance is also made by the traditional methods precision and recall, and it shows that the fusion algorithm is efficient.	intelligent agent	Yuefeng Li	2001		10.1007/3-540-45490-X_55	simulation;computer science;artificial intelligence;data mining;sensor fusion;world wide web;intelligent agent;information system	Robotics	-38.465574681908805	14.353615063339937	187801
9dac9dd8b618c70fa38f8fe8c025d11abbbef5ba	validating semistructured data using owl	database system;owl;ontologie;base donnee;red www;semantica formal;web semantique;reseau web;database;service web;base dato;formal semantics;web service;automated reasoning;data model;semantique formelle;semistructured data;raisonnement automatique;formal verification;internet;dato semi estructurado;web ontology language;web semantica;consistency checking;semantic web;world wide web;coherence;ontologia;modele donnee;coherencia;systeme gestion base donnee;analisis semantico;analyse semantique;sistema gestion base datos;database management system;ontology;servicio web;semantic analysis;data models;razonamiento automatico;donnee semistructuree	Semistructured data has become prevalent in both web applications and database systems. This rapid growth in use makes the design of good semistructured data essential. Formal semantics and automated reasoning tools enable us to reveal the inconsistencies in a semistructured data model and its instances. The Object Relationship Attribute model for Semistructured data (ORASS) is a graphical notation for designing and representing semistructured data. This paper presents a methodology of encoding the semantics of ORA-SS in the Web Ontology Language (OWL) and automatically validating the semistructured data design using the OWL reasoning tool RACER. Our methodology provides automated consistency checking of an ORA-SS data model at both the schema and instance levels.	algorithm;alloy analyzer;automated reasoning;data model;data modeling;database;graphical user interface;modeling language;ontology (information science);openraster;ora lassila;semantic web;semantic reasoner;synergy;verification and validation;web ontology language;web application	Yuan-Fang Li;Jing Sun;Gillian Dobbie;Jun Sun;Hai H. Wang	2006		10.1007/11775300_44	computer science;artificial intelligence;ontology;data mining;database;web ontology language;world wide web	DB	-37.05375850123934	12.901466462624834	188048
39266e0bfef8dbf67330178ea82e96fedafa7192	coo-bdi: extending the bdi model with cooperativity	multiagent system;declarative langage;langage declaratif;cooperative agents;by product;partage ressource;sous produit;subproducto;resource sharing;particion recursos;procedural knowledge;sistema multiagente;lenguaje declarativo;systeme multiagent	We define Coo-BDI, an extension of the BDI architecture with the notion of cooperativity. Agents can cooperate by exchanging and sharing plans in a quite flexible way. As a main result Coo-BDI promotes adaptivity and sharing of resources; as a by-product, it provides a better support for dealing with agents which do not possess their own procedural knowledge for processing a given event.		Davide Ancona;Viviana Mascardi	2003		10.1007/978-3-540-25932-9_7	shared resource;simulation;computer science;knowledge management;artificial intelligence;procedural knowledge	AI	-39.75523416942517	16.816512278311087	188092
27f5e25a6e02d3a50d00e20f215bcf481bfd7abc	an optimized mpeg-21 bsdl framework for the adaptation of scalable bitstreams	scalable video coding;mpeg 21 bsdl;technology and engineering;performance analysis;xml document;content adaptation;h 264 mpeg 4 avc;context related attributes;bitstream syntax descriptions	A format-agnostic framework for content adaptation allows reaching a maximum number of users in heterogeneous multimedia environments. Such a framework typically relies on the use of scalable bitstreams. In this paper, we investigate the use of bitstreams compliant with the scalable extension of the H.264/MPEG-4 AVC standard in a format-independent framework for content adaptation. These bitstreams are scalable along the temporal, spatial, and SNR axis. To adapt these bitstreams, a format-independent adaptation engine is employed, driven by the MPEG-21 Bitstream Syntax Description Language (BSDL). MPEG-21 BSDL is a specification that allows generating high-level XML descriptions of the structure of a scalable bitstream. As such, the complexity of the adaptation of scalable bitstreams can be moved to the XML domain. Unfortunately, the current version of MPEG-21 BSDL cannot be used to describe the structure of large video bitstreams because the bitstream parsing process is characterized by an increasing memory consumption and a decreasing description generation speed. Therefore, in this paper, we describe a number of extensions to the MPEG-21 BSDL specification that make it possible to optimize the processing of bitstreams. Moreover, we also introduce a number of additional extensions necessary to describe the structure of scalable H.264/AVC bitstreams. Our performance analysis demonstrates that our extensions enable the bitstream parsing process to translate the structure of the scalable bitstreams into an XML document multiple times faster. Further, a constant and low memory consumption is obtained during the bitstream parsing process.		Davy De Schrijver;Wesley De Neve;Koen De Wolf;Robbie De Sutter;Rik Van de Walle	2007	J. Visual Communication and Image Representation	10.1016/j.jvcir.2007.02.003	scalable video coding;real-time computing;xml;computer science;theoretical computer science;database	Vision	-40.51634668646823	12.61781705119945	188268
e77e1c3d2147870e009a359248ef528658eeac36	managing information quality in e-science: a case study in proteomics	developpement logiciel;modelizacion;quality assurance;ciencia informacion;decomposition domaine;ontologie;domain decomposition;information science;reutilizacion;conceptual analysis;descomposicion dominio;metric;analisis conceptual;reuse;modelisation;aseguracion calidad;desarrollo logicial;software development;information quality;preferencia;ontologia;metrico;preference;analyse conceptuelle;science information;quality model;modeling;assurance qualite;ontology;metrique;reutilisation;qa76 computer software	We describe a new approach to managing information quality (IQ) in an e-Science context, by allowing scientists to define the quality characteristics that are of importance in their particular domain. These preferences are specified and classified in relation to a formal IQ ontology, intended to support the discovery and reuse of scientists’ quality descriptors and metrics. In this paper, we present a motivating scenario from the biological sub-domain of proteomics, and use it to illustrate how the generic quality model we have developed can be expanded incrementally without making unreasonable demands on the domain expert who maintains it.	bridging (networking);data quality;e-science;embedded system;extensibility;information quality;iterative and incremental development;namecoin;proteomics;scenario (computing);sensitivity and specificity;subject-matter expert	Paolo Missier;Alun D. Preece;Suzanne M. Embury;Binling Jin;R. Mark Greenwood;David Stead;Al Brown	2005		10.1007/11568346_45	quality assurance;systems modeling;metric;information science;computer science;artificial intelligence;software development;ontology;data mining;reuse;domain decomposition methods;information quality	Comp.	-38.04975680555579	14.234226726415756	189514
ffccc3ec866cf5c9090e178521e589e57f45a260	formalizing mappings for owl spatiotemporal ontologies	distributed application;distributed system;topology;intercambio informacion;representacion conocimientos;ontologie;base donnee;sistema experto;systeme reparti;correspondance ontologie;red www;ontology mapping;information source;source information;base donnee temporelle;web semantique;topologie;reseau web;database;service web;logica descripcion;base dato;semantics;intelligence artificielle;information access;web service;semantica;semantique;spatial database;topologia;semantic heterogeneity;semantic information;access to information;sistema repartido;internet;heterogeneidad;echange information;web semantica;information exchange;base donnee spatiale;algorithme reparti;representation connaissance;acces information;semantic web;artificial intelligence;world wide web;ontologia;spatial data structures;algoritmo repartido;acceso informacion;temporal databases;base dato especial;inteligencia artificial;systeme expert;description logic;ontology web language;knowledge representation;distributed algorithm;ontology;correspondencia ontologia;fuente informacion;heterogeneity;servicio web;heterogeneite;logique description;expert system;structure donnee spatiale	Ontology mappings provide a common layer which allows distributed applications to share and to exchange semantic information. Providing mechanized ways for mapping ontologies is a challenging issue and main problems to be faced are related to structural and semantic heterogeneity. The complexity of these problems increases in the presence of spatiotemporal information such as geometry and topological intrinsic characteristics. Our proposal is intended for spatiotemporal ontologies and focuses on providing an integrated access to information sources using local ontologies. Our approach is set to build a system that guides users to derive meaningful mappings and to reason about them. To achieve this we use a description logic extended to spatiotemporal concrete domain. The ontology of each source is normalized in a common extended Ontology Web Language (OWL) which enables a natural correspondence with the spatiotemporal description logic formalism.	minimal mappings;ontology (information science)	Nacéra Bennacer	2006		10.1007/11827405_36	web service;distributed algorithm;description logic;the internet;semantic integration;information exchange;computer science;ontology;artificial intelligence;heterogeneity;semantic web;ontology;data mining;database;semantics;temporal database;world wide web;expert system;spatial database;process ontology	Vision	-37.55113390157677	12.32888012258563	189695
77131bf95a12ebd75256b83fec425d0864366b5c	agent-based coordination of regional information services	distributed system;adaptacion;information communication;user agent;systeme reparti;information sources;service information;congres international;agent based;information source;source information;implementation;congreso internacional;user adaptation;international conference;service utilisateur;prototipo;ejecucion;heterogeneous information;communication information;sistema repartido;agent intelligent;adaptation;intelligent agent;comunicacion informacion;servicio informacion;coordinacion;agente inteligente;information service;servicio usuario;user service;prototype;geographic distribution;fuente informacion;user model;coordination	This paper proposes an agent-based framework for coordinating heterogeneous regional information services. It is necessary to deal with distributed and heterogeneous information services since regional information sources are geographically distributed and the characteristics of regions vary the required regional information according to regions. It is also necessary to integrate information user-adaptively since users of regional information services have specific characteristics, such as knowledge about the region. In this paper, we propose an agent-based framework that consists of server and user agents. Some server agents wrap distributed regional information servers to provide flexible communication. Some server agents provide certain mediation services, such as ontology translation, to enable the coordination of heterogeneous information servers. Each user agent integrates information received from other agents according to a user model to enable the provision of useradapted information. A prototype system, called the GeoLinkAgent system, has been implemented based on the framework.	agent-based model;prototype;server (computing);user agent;user modeling	Jun-ichi Akahani;Kaoru Hiramatsu;Yoshikazu Furukawa;Kiyoshi Kogure	2001		10.1007/3-540-45636-8_22	user agent;user modeling;computer science;artificial intelligence;data mining;prototype;implementation;world wide web;computer security;intelligent agent;adaptation	AI	-38.83575131150171	14.086818518700639	191186
642022323a06265b01a005fa039d1acc87792acd	designing a knowledge representation interface for cognitive agents	generic interface design;agent programming framework;knowledge representation technology	The design of cognitive agents involves a knowledge representation KR to formally represent and manipulate information relevant for that agent. In practice, agent programming frameworks are dedicated to a specific KR, limiting the use of other possible ones. In this paper we address the issue of creating a flexible choice for agent programmers regarding the technology they want to use. We propose a generic interface, that provides an easy choice of KR for cognitive agents. Our proposal is governed by a number of design principles, an analysis of functional requirements that cognitive agents pose towards a KR, and the identification of various features provided by KR technologies that the interface should capture. We provide two use-cases of the interface by describing its implementation for Prolog and OWL with rules.	knowledge representation and reasoning	Timea Bagosi;Joachim de Greeff;Koen V. Hindriks;Mark A. Neerincx	2015		10.1007/978-3-319-26184-3_3	computer science;knowledge management;artificial intelligence	HCI	-40.46424829311425	14.820398195184314	191749
ecafbd17b9587030c3e74bae764b2c7d0747cd65	reverse engineering user interfaces for interactive database conceptual analysis	database engineering;tool support;user interface;conceptual analysis;requirements elicitation;requirements engineering;conceptual schema;visual representation;human computer interfaces reverse engineering;information systems engineering;requirement engineering;user requirements;information system;database design;user involvement;human computer interface;reverse engineering	The first step of most database design methodologies consists in eliciting part of the user requirements from various sources such as user interviews and corporate documents. These requirements formalize into a conceptual schema of the application domain, that has proved to be difficult to validate, especially since the visual representation of the ER model has shown understandability limitations from the end-users standpoint. In contrast, we claim that prototypical user interfaces can be used as a two-way channel to efficiently express, capture and validate data requirements. Considering these interfaces as a possibly populated physical view on the database to be developed, reverse engineering techniques can be applied to derive their underlying conceptual schema. We present an interactive tool-supported approach to derive data requirements from user interfaces. This approach, based on an intensive user involvement, addresses a significant subset of data requirements, especially when combined with other requirement elicitation techniques.	application domain;conceptual schema;database design;entity–relationship model;population;requirement;reverse engineering;user interface;user requirements document	Ravi Ramdoyal;Anthony Cleve;Jean-Luc Hainaut	2010		10.1007/978-3-642-13094-6_27	user interface design;computer science;systems engineering;conceptual schema;user requirements document;requirement;requirements elicitation;data mining;database;requirements engineering;user interface;management;information system;database design;reverse engineering	DB	-34.784106781319466	13.460163860973108	193118
d7d05a941b0ba4c48032d6a34bdc336d2443fe5d	a personal knowledge assistant for knowledge storing, integrating, and querying	modelizacion;lenguaje natural;interfaz grafica;text;base donnee;informatique mobile;red www;graphical interface;lenguaje uml;frase;langage naturel;interrogation base donnee;reseau web;database;interrogacion base datos;base dato;base connaissance;langage modelisation unifie;texte;data mining;user assistance;modelisation;sentence;assistance utilisateur;fouille donnee;natural language;unified modelling language;decouverte connaissance;knowledge discovery process;asistencia usuario;unified modeling language;handheld device;world wide web;descubrimiento conocimiento;base conocimiento;phrase;information system;mobile computing;texto;modeling;interface graphique;busca dato;database query;systeme information;knowledge modeling;sistema informacion;knowledge base;knowledge discovery	Using the underlying Unified Modeling Language (UML) for knowledge modeling, we discuss how to create a non-graphical interface to UML models and show how this interface can be used to capture knowledge from a sample domain specified in a natural language. We demonstrate the techniques of transforming a natural language text into standard sentences consisting of three tuples: known information-relationship-unknown information. We also discuss how to integrate the standard sentences with existing knowledge through a guided knowledge-discovery process in which more precise information is requested and added to the diagram in a controlled manner. Based on this knowledge-processing methodology, a software prototype was developed. Using such software, existing PDAs or specialized hardware can allow the student to process the knowledge. These handheld devices can store, process, and retrieve knowledge from Knowledge Databases. We refer to such devices as Personal Knowledge Assistants (PKA).		Bogdan D. Czejdo;John Biguenet;Jonathan Biguenet;J. Czejdo	2004		10.1007/978-3-540-30481-4_14	natural language processing;unified modeling language;knowledge base;computer science;artificial intelligence;operating system;applications of uml;knowledge-based systems;open knowledge base connectivity;data mining;database;knowledge extraction;mobile computing;domain knowledge	AI	-37.0721771954672	12.04306396897836	193721
2381a6266dd5f29c0a4fb93fae501e2474004ac8	mapping moving landscapes by mining mountains of logs: novel techniques for dependency model generation	distributed system;model generation;datorsystem;computer systems;large scale;data mining algorithm;natural language processing	Problem diagnosis for distributed systems is usually difficult. Thus, an automated support is needed to identify root causes of encountered problems such as performance lags or inadequate functioning quickly. The many tools and techniques existing today that perform this task rely usually on some dependency model of the system. However, in complex and fast evolving environments it is practically unfeasible to keep such a model up-to-date manually and it has to be created in an automatic manner. For high level objects this is in itself a challenging and less studied task. In this paper, we propose three different approaches to discover dependencies by mining system logs. Our work is inspired by a recently developed data mining algorithm and techniques for collocation extraction from the natural language processing field. We evaluate the techniques in a case study for Geneva University Hospitals (HUG) and perform large-scale experiments on production data. Results show that all techniques are capable of finding useful dependency information with reasonable precision in a real-world environment.	algorithm;collocation extraction;data mining;distributed computing;experiment;high-level programming language;natural language processing	Mirko Steinle;Karl Aberer;Sarunas Girdzijauskas;Christian Lovis	2006			simulation;computer science;data science;data mining;database	DB	-36.40879208946791	17.91223493412694	194540
6376a4e18338c6edd64ac8bcc974c0de2befc427	electronic institutions as a framework for agents' negotiation and mutual commitment	commerce electronique;multicriteria analysis;business to business;tratamiento transaccion;comercio electronico;negociation;intelligence artificielle;digital enterprise;negociacion;bargaining;artificial intelligence;analisis multicriterio;inteligencia artificial;analyse multicritere;entreprise numerique;transaction processing;electronic institution;electronic trade;traitement transaction	Electronic transactions are of increasing use due to its openness and continuous availability. The rapid growth of inform ation and communication technologies has helped the expansion of these elec tronic transactions, however, issues related to security and trust are y et limiting its action space, mainly in what concerns business to business activi ty. This paper introduces an Electronic Institution framework to help in electro nic transactions management making available norms and rules as well as monitor i g business participants’ behaviour in specific electronic business transacti ons. Virtual Organisation (VO) life cycle has been used as a complex scenario enc mpassing electronic transactions and where Electronic Institution helps in both formation and operation phase. A flexible negotiation process tha t includes multi-attribute and learning capabilities as well as distributed depend encies resolution is here proposed for VO formation. “Phased commitment” is a nother concept here introduced for VO operation monitoring through the El ctronic Institution.	algorithm;commitment scheme;continuous availability;electrical engineering;electronic business;electronic funds transfer;entity;online and offline;openness;personally identifiable information;phased array;reinforcement learning;requirement;virtual organization (grid computing)	Ana Paula Rocha;Eugénio C. Oliveira	2001		10.1007/3-540-45329-6_24	transaction processing;computer science;artificial intelligence;database;operations research;computer security;negotiation	DB	-39.540190616479755	16.467827184603617	194758
8d0c996ed065cd0231f5a5d357dcfe4b3db4e59a	design and realization of distinctive data interface based on sap hr system	databases;bims;peripheral interfaces;peripheral interfaces human resource management;sap hr;data synchronization;data interface;satellite broadcasting;synchronization databases humans satellite broadcasting personnel xml;personnel;synchronization;xml;human resource management distinctive data interface sap hr system personal information department information peripheral system hr data sap sfu bims method;humans;data synchronization sap hr bims sfu data interface;human resource management;sfu;data transfer	The paper analyzes the situation while the SAP HR system is running independently, its personal information, department information, and the relationship between them need to be referenced by multiple peripheral systems. So the target systems need to synchronize the HR data with SAP periodically. Currently, most databases are accessed directly by ODBC/JDBC, which are much more expensive, except oriented databases. To resolve these problems, the paper introduces a new method , using SAP-SFU-BIMS to be the data interface between SAP HR system and peripheral systems. The result shows that the method can improve the performance while reducing the number of interfaces between SAP system and peripheral systems. It provides a good reference for the design of data transfer interface.	jdbc;open database connectivity;peripheral;personally identifiable information	Wei Zhou;Xiao-Qian Zhu;Li-Xuan Ye	2011	2011 Second International Conference on Innovations in Bio-inspired Computing and Applications	10.1109/IBICA.2011.73	embedded system;real-time computing;engineering;database	Robotics	-35.45196881596735	15.573120012577927	195987
5c3248f9a12271ef316495ac596ede1f4a94d71c	ontology translation approaches for interoperability: a case study with protégé-2000 and webode	informatica;pragmatics;ontologie;interoperabilite;interoperabilidad;ingenierie connaissances;pragmatica linguistca;web semantique;semantics;semantica;semantique;pragmatique;web semantica;semantic web;preservation;ontologia;interoperability;preservacion;ontology;knowledge engineering	We describe four ontology translation approaches that can be used to exchange ontologies between ontology tools and/or ontology languages. These approaches are analysed with regard to two main features: how they preserve the ontology semantics after the translation process (aka semantic or consequence preservation) and how they allow final users and ontology-based applications to understand the resulting ontology in the target format (aka pragmatic preservation). These approaches are illustrated with practical examples that show how they can be applied to achieve interoperability between the ontology tools Protégé-2000 and WebODE.	apache axis;approximation algorithm;case preservation;code refactoring;interoperability;ontology (information science);ontology components	Óscar Corcho;Asunción Gómez-Pérez	2004		10.1007/978-3-540-30202-5_3	upper ontology;interoperability;ontology alignment;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;artificial intelligence;semantic web;knowledge engineering;ontology;data mining;database;semantics;ontology-based data integration;world wide web;preservation;process ontology;pragmatics;suggested upper merged ontology	Web+IR	-38.685268115992486	12.178641954754735	196228
e839eb3154dfd59baad678a3c2be433797bd56fb	mobile agents and the sara digital library	parallel computing;multi agent system;software libraries;digital library;earth;digital libraries;mobile agents;prototypes;distributed computing;heterogeneous remote sites;multi agent systems mobile agents sara digital library remote sensing data earth heterogeneous remote sites distributed computing autonomous data processing information discovery synthetic aperture radar atlas parallel computing prototype system;data processing;sara digital library;synthetic aperture radar atlas;information discovery;prototype system;parallel and distributed computing;computer architecture;geographic information systems multi agent systems distributed programming digital libraries remote sensing synthetic aperture radar visual databases;mobile agents software libraries remote sensing earth distributed computing data processing synthetic aperture radar parallel processing computer architecture prototypes;multi agent systems;remote sensing data;geographic information systems;distributed programming;remote sensing;mobile agent;autonomous data processing;parallel processing;visual databases;synthetic aperture radar	Remote-sensing data about the Earth's environment is being created at an ever-increasing rate and distributed among heterogeneous remote sites. Traditional models of distributed computing are inadequate to support such complex applications, which generally involve a large quantity of data. We explore an approach based on mobile agent techniques for autonomous data processing and information discovery on the Synthetic Aperture Radar Atlas (SARA) digital library which consists of distributed multi-agency archives of multi-spectral remote-sensing imagery of the Earth. Our goal is to enable automatic and dynamic configuration of distributed parallel computing resources and to efficiently support on-demand processing of such a remote-sensing archive. The design, architecture and implementation of a prototype system that applies this approach is reported.	digital library;mobile agent	Yanyan Yang;Omer F. Rana;Christos Georgousopoulos;David W. Walker;Roy Williams	2000		10.1109/ADL.2000.848371	computer vision;computer science;distributed design patterns;world wide web;remote sensing	Logic	-36.15772366789882	17.21081549591658	197218
a0c382a4bd25e9a613369b897f8dfb232885416f	nexuseditor: a schema-aware graphical user interface for managing spatial context models	context aware application;spatial context;schema awareness;stationary objects;graphical user interfaces context modeling conference management data visualization wireless sensor networks data models intelligent sensors switches satellites xml;googleearth;information space;nexuseditor;mobile object;data model;context model;wireless communication;information spaces;digital information;data visualisation;computational modeling;schema awareness domain specific editor context aware systems;graphical user interfaces;context aware applications;spatial relation;domain specific editor;data visualization;mobile communication;graphic user interface;mobile computing data visualisation graphical user interfaces;result visualization;context aware systems;mobile computing;spatial context model management;context modeling;schema aware graphical user interface;wireless sensor networks;domain specificity;mobile objects;googleearth nexuseditor schema aware graphical user interface spatial context model management context aware applications mobile objects stationary objects digital information result visualization information spaces;data models	To support context-aware applications, it is beneficial to maintain shared context models that contain different types of information, like mobile objects, stationary objects, or spatially related digital information. This demonstration is about the NexusEditor, a graphical user interface to maintain spatial context models, interactively create queries, send them to a server and visualize the results. The contribution here is to show how schema awareness can improve such a tool: the NexusEditor dynamically parses the underlying data model and provides additional syntactic and semantic checks and short-cuts based on the schema information. Also, it supports export to existing information spaces like GoogleEarth.	data model;digital data;google earth;graphical user interface;interactivity;server (computing);stationary process	Daniela Nicklas;Carsten Neumann	2008	The Ninth International Conference on Mobile Data Management (mdm 2008)	10.1109/MDM.2008.39	computer science;theoretical computer science;operating system;graphical user interface;database;context model;world wide web;data visualization	DB	-35.39119841548145	13.988777258170966	197222
bf987b7b7ba00d4d4507da31b5d216710270cbd4	a rule based resources management for collaborative grid environments	fabricacion asistida por computador;sistema experto;systeme cooperatif;collaborative awareness management;implementation;cooperation;rule based;reseau ordinateur;resource manager;resource management;collaboration;distributed computing;awareness management;computer network;collaborative grid;gestion recursos;fabrication assistee;cooperative systems;computer aided manufacturing;red informatica;gestion ressources;calculo repartido;resources management;systeme expert;implementacion;collaborative environments;grid computing;calcul reparti;cooperative work management;expert system	Something that is still missing, but strongly needed, in collaborative grid environments is a stable, flexible and dynamic resource management. This management should optimise collaboration and cooperation among several resources keeping resources constraints, preconditions and rules. This paper presents how to achieve these objectives, by means of a Collaborative Awareness Management (CAM) model. CAM optimises resources collaboration, promotes resources cooperation and responds to the specific demanded circumstances. This paper also describes how this model works in some specific examples and scenarios, emphasising on how the WS-CAM RulesBased Management Application has been designed, implemented, and validated to accomplish these purposes.	digital rights management;grid computing;ontology (information science);precondition;throughput	Pilar Herrero;José Luis Bosque;Manuel Salvadores;María S. Pérez-Hernández	2008	IJIPT	10.1504/IJIPT.2008.019294	simulation;computer science;knowledge management;resource management;implementation;cooperation;grid computing;collaboration	HPC	-39.53701664449117	16.71384644911321	198365
b5c877e73f54735ec25e3a1aa783181bd621b8a3	xml as a basis for interoperability in real time distributed systems	information technology;interoperability;distributed system;process control;application software;distributed environment;message passing;xml;real time;open systems;electronic data interchange;real time systems;xml document;aerospace industry;xml schema;data exchange	Many real time applications consist of components that can situate in a centralized/distributed environment. Typically due to different system requirements these components could be developed in different technologies which require a bridge for communication. With the increasing popularity of XML, it has become an alternative solution for data exchange in the real time application domain. XML has become the most important mechanism for data exchange between heterogeneous data sources. This paper presents a methodology for real time application data in XML. By defining a solid XML schema for the real time domain attributes, applications from diverse platforms can be based on the data definition and create instances of XML documents to exchange data.	application domain;centralized computing;data definition language;distributed computing;interoperability;rtml;requirement;situated cognition;system requirements;xml schema	Polly M. S. Poon;Tharam S. Dillon;Elizabeth Chang	2004	Second IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems, 2004. Proceedings.	10.1109/WSTFEUS.2004.10013	data exchange;xml validation;binary xml;xml encryption;service interface for real time information;xml schema;streaming xml;computer science;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange	DB	-40.57211594644355	12.643233638278875	198613
1657c79e2a3979c0508ad7e8b0f7b0adbf79086c	a conceptual markup language that supports interoperability between business rule modeling systems	domain model;ml language;lenguaje natural;commerce electronique;representacion conocimientos;ontologie;modele entreprise;comercio electronico;red www;interoperabilite;model system;interoperabilidad;langage ml;xml language;langage naturel;reseau web;web service;modelo empresa;business model;data semantics;conceptual schema;internet;natural language;world wide web;interoperability;knowledge representation;representation connaissances;markup language;ontology;business rules;langage xml;lenguaje xml;first order logic;electronic trade	"""The Internet creates a strong demand for standardized exchange not only of data itself but especially of data semantics, as this same internet increasingly becomes the carrier of e-business activity (e.g. using web services). One way to achieve this is in the form of communicating """"rich"""" conceptual schemas. In this paper we adopt the well-known CM technique of ORM, which has a rich complement of business rule specification, and develop ORM-ML, an XML-based markup language for ORM. Clearly domain modeling of this kind will be closely related to work on so-called ontologies and we will briefly discuss the analogies and differences, introducing methodological patterns for designing distributed business models. Since ORM schemas are typically saved as graphical files, we designed a textual representation as a marked-up document in ORM-ML so we can save these ORM schemas in a more machine exchangeable way that suits networked environments. Moreover, we can now write style sheets to convert such schemas into another syntax, e.g. pseudo natural language, a given rule engine’s language, first order logic."""	business rules engine;electronic business;first-order logic;graphical user interface;internet;interoperability;markup language;natural language;ontology (information science);semantic data model;style sheet (web development);web service;whole earth 'lectronic link;xml	Jan Demey;Mustafa Jarrar;Robert Meersman	2002		10.1007/3-540-36124-3_2	natural language processing;web service;business model;interoperability;the internet;xml;computer science;conceptual schema;artificial intelligence;operating system;domain model;ontology;first-order logic;database;markup language;natural language;programming language;business rule;law;world wide web;computer security;algorithm	DB	-38.19154933663496	11.537279112547992	199056
1b3b940fe8286aec8ade7d298121659770890b5a	semantic data integration in a newspaper content management system	content management;sistema interactivo;distributed system;information structure;ontologie;sistema experto;systeme reparti;relation equivalence;base de connaissances;structure information;integration information;universe of discourse;estructura informacion;semantics;information space;gestion contenido;logical programming;semantica;semantique;systeme conversationnel;heterogeneous information;information integration;equivalence relation;sistema repartido;internet;programmation logique;interactive system;integracion informacion;gestion contenu;content management system;base conocimiento;ontologia;systeme expert;description logic;user interaction;programacion logica;relacion equivalencia;semantic data integration;ontology;knowledge base;expert system	A newspaper content management system has to deal with a very heterogeneous information space as the experience in the Diari Segre newspaper has shown us. The greatest problem is to harmonise the different ways the involved users (journalist, archivists...) structure the newspaper information space, i.e. news, topics, headlines, etc. Our approach is based on ontology and differentiated universes of discourse (UoD). Users interact with the system and, from this interaction, integration rules are derived. These rules are based on Description Logic ontological relations for subsumption and equivalence. They relate the different UoD and produce a shared conceptualisation of the newspaper information domain.	content management system;description logic;domain of discourse;subsumption architecture;turing completeness	Alberto Abelló;Roberto Barraza Garcia;Rosa Gil;Marta Oliva;Ferran Perdrix	2006		10.1007/11915034_19	knowledge base;description logic;the internet;content management;computer science;artificial intelligence;information integration;ontology;data mining;domain of discourse;semantics;equivalence relation;expert system;algorithm	DB	-38.1225195737805	12.277072360189598	199237
ed06ceeca8142cdbf1d6e413053999d75ceaeeaa	schema mapping evolution through composition and inversion	building block;information integration;schema mapping;schema evolution	Mappings between different representations of data are the essential building blocks for many information integration tasks. A schema mapping is a high-level specification of the relationship between two schemas, and represents a useful abstraction that specifies how the data from a source format can be transformed into a target format. The development of schema mappings is laborious and time-consuming, even in the presence of tools that facilitate this development. At the same time, schema evolution inevitably causes the invalidation of the existing schema mappings (since their schemas change). Providing tools and methods that can facilitate the adaptation and reuse of the existing schema mappings in the context of the new schemas is an important research problem. In this chapter, we show how two fundamental operators on schema mappings, namely composition and inversion, can be used to address the mapping adaptation problem in the context of schema evolution. We illustrate the applicability of the two operators in various concrete schema evolution scenarios, and we survey the most important developments on the semantics, algorithms and implementation of composition and inversion. We also discuss the main research questions that still remain to be addressed.	algorithm;high- and low-level;schema (genetic algorithms);schema evolution;xml schema	Ronald Fagin;Phokion G. Kolaitis;Lucian Popa;Wang Chiew Tan	2011		10.1007/978-3-642-16518-4_7	schema migration;information schema;schema;semi-structured model;logical schema;computer science;three schema approach;conceptual schema;document structure description;star schema;data mining;database;document schema definition languages;database schema;algorithm	DB	-33.78747147708678	11.542885185992107	199288
9eeb17e8d47afdd786bb7586ec26f180c5a4af8f	oopus-designer - user-friendly master data maintenance through intuitive and interactive visualization	interactive visualization	Valid and consistent master data are pre-requisite for efficient working Enterprise Resource Planning (ERP) and Production Planning and Control (PPC) systems. Unfortunately users are often confused by a large number of forms or transactions in these systems. Confusing interfaces lead to faulty master data. In this paper we introduce a tool that provides intuitive and interactive visualization for the master data administration of a PPC system.	data validation;database;erp;enterprise resource planning;interactive visualization;master data management;usability	Wilhelm Dangelmaier;Benjamin Klöpper;Björn Kruse;Daniel Brüggemann;Tobias Rust	2007			data mining;compressed air;engineering drawing;computer science;visualization;visual analytics;user friendly;master data;offset (computer science);interactive visualization;information visualization	DB	-34.138547833307285	16.173563562203377	199473
