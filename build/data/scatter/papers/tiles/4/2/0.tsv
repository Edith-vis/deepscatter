id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
57ee5aff27c41d16a057ca59d59f6df8d4b65ee0	texture feature based automated seeded region growing in abdominal mri segmentation	abdomen magnetic resonance imaging image segmentation biomedical imaging image texture analysis medical diagnostic imaging biomedical engineering image analysis biomedical computing feature extraction;image segmentation;cooccurrence texture feature;biomedical imaging;abdominal mri;texture features;feature space;image texture;seeded region growing texture feature;semivariogram texture feature feature extraction automated seeded region growing abdominal mri image segmentation cooccurrence texture feature;mr imaging;seeded region growing;semivariogram texture feature;biomedical engineering;medical image processing biomedical mri feature extraction image segmentation image texture;feature extraction;medical image processing;region of interest;magnetic resonance imaging;abdomen;image analysis;image texture analysis;biomedical computing;automated seeded region growing;medical diagnostic imaging;texture feature;biomedical mri	A new texture feature based seeded region growing algorithm is proposed for the automated segmentation of organs in Abdominal MR image. Co-occurrence texture feature and semi-variogram texture feature are extracted from the image and the seeded region growing algorithm is run on these feature spaces. With a given Region of Interest(ROI), a seed point is automatically picked up based on three homogeneity criteria. A threshold is then obtained by taking a lower value just before the one causing 'explosion'. This algorithm is tested on 12 series of 3D abdominal MR images.	algorithm;region growing;semiconductor industry	Jie Wu;W. F. Skip Poehlman;Michael D. Noseworthy;Markad V. Kamath	2008	2008 International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2008.352	image texture;computer vision;image analysis;feature vector;feature extraction;computer science;magnetic resonance imaging;machine learning;pattern recognition;region growing;image segmentation;region of interest	Vision	38.33889959323062	-75.16817599384228	4290
ca58d313ed704627b86f01379dc31b6ac15ed0d3	address location on envelopes	binary image;digitizing;structure arborescente;localization;transformacion;localizacion;segmentation;imagen nivel gris;numerisation;algorithme;algorithm;algorritmo;reconnaissance caractere;localisation;estructura arborescente;feature extraction;tree structure;estructura datos;image niveau gris;image binaire;adresse postale;imagen binaria;numerizacion;inclusion;structure donnee;transformation;extraction caracteristique;grey level image;data structure;character recognition;segmentacion;reconocimiento caracter	"""-ln automatic postal address reading systems, the inability of the system to locate the address block correctly contributes significantly to the error rate. Envelopes often contain much extraneous printed information in addition to the address. The ideal approach to address location would be to read all this information and identify the address field by its semantic content; but it would be computationally expensive to implement a full-envelope character reading capability. This paper describes an alternative approach, which segments the envelope into regions of consistent print style and chooses a region most likely to be the address on grounds of position, size, print style, etc. without attempting to read the characters. Postal address reading Address location I, I N T R O D U C T I O N The U.S. Postal Service processes nearly 100 billion letter-size maiipieces each year. To provide speedy service and reduce the cost of mail sorting, it has installed 252 Optical Character Readers (OCR's) at various locations since 1980. About 60~o of the metered mail sent through these OCR's is read correctly while the other 40~ is rejected due to problems such as hand-printed address, partially obscured address, extraneous printing (advertising messages, etc.), colored background, varying type fonts, etc. Since hand-addressed mail represents only about 15~ of all letter mail, a significant fraction of unreadable mail has machine-printed addresses. It has been recognized that in the case of machine-printed addresses, the inability of the OCR to locate the address block correctly contributes significantly to the high reject rate. ~1~ Currently, the problem of address block location is partially dealt with by locating an area of high edge density on the envelope surface, a~ or by optically detecting an area of different retlectivity than the background. ~3J Character recognition procedures are then confined to this area only. Another approach limits the scanning of the envelope surface to the last few dark lines, which presumably contain all or part of the address. It is fairly easy to collect mail samples with addresses located in such a way that none of the above approaches would succeed, t4~ The ideal approach to mail sorting would be to read * The research described in this paper was sponsored by the U.S. Postal Service under Contract 104230-84-D-069. ~"""" To whom correspondence should be addressed. all the printing on the envelope and thus to identify the address field by its semantic content. But it does not appear to be practical to implement a full character reading system over the entire envelope in real time. Instead, our approach was to segment the envelope surface into regions of consistent print style and to choose a region most likely to be the address on grounds of geometrical information, print style, etc. without attempting to recognize the characters. This approach is strongly supported by an experiment in which an envelope is digitized, reduced to a binary image and every dark component (whether character or not) is replaced by its bounding rectangle with sides parallel to the edges of the envelope. The resulting image, now totally unreadable, is presented to a set of subjects. All the subjects are able to correctly identify the address block, the stamp, and the return address. Even though the experiment was not designed to include a statistically meaningful sample of mail, the result indicates how knowledge about patterns on the envelopes can aid in mail sorting without the need for recognizing characters. An outline of our approach is as follows: the image is segmented into connected light and dark regions separated by zero-crossings of a digital Laplacian operator. Properties of the dark connected components are computed and tabulated. These properties are then used to cluster the components into groups which should represent regions of uniform print style. Finally, an inference scheme is used to select the group that is most likely to be the address. The steps in the approach are described in detail in the following sections of this paper and examples of its application to digitized envelope images are given. Further details and a larger set of examples can be found in Refs (5) and (6)."""	analysis of algorithms;basic stamp;binary image;connected component (graph theory);location-based service;minimum bounding rectangle;postal;printing;return statement;scheme;sensor;sorting	Pen-Shu Yeh;Sergio Antoy;Anne Litcher;Azriel Rosenfeld	1987	Pattern Recognition	10.1016/0031-3203(87)90055-0	transformation;internationalization and localization;data structure;binary image;feature extraction;computer science;artificial intelligence;tree structure;segmentation;algorithm;inclusion	Graphics	47.72955792748574	-62.61268525927583	4304
5616e176d737c331a8680c184d3f1d7aefd5be7e	an affine-invariant tool for retrieving images from homogeneous databases	dynamic program;homogeneous database;shape deformation;invariants;multi dimensional;local structure;shape;affine transformation;affine;indexation;environmental change;content based image retrieval;environmental factor	In this paper, we examine the complexities involved in retrieving images from a database comprised of objects of very similar appearance. Such an operation requires a process that can discriminate among images at a very fine level, such as distinguishing among various species of fish. Furthermore, incidental environmental factors such as change in viewpoints and slight, nonessential shape deformation must be excluded from the similarity criteria. To this end, we propose a new method for content-based image retrieval and indexing, one that is well suited for discriminating among objects within the same class in a way that is insensitive to incidental environmental changes. The scheme comprises a global alignment and a local matching process. Affine transform is used to model the different viewpoints associated with positioning the camera, while multi-dimensional indexing techniques are used to make the global alignment scheme efficient. A local matching process based on dynamic programming allows the optimal matching of local structures using cost metrics that may ignore nonessential local shape deformation. Results show the method's ability to cancel out visual distortions caused by a changing viewpoint, and its tolerance to noise, occlusion, and slight deformations of the object.	content-based image retrieval;database;distortion;dynamic programming;fish (cryptography);hidden surface determination;optimal matching	Ronald-Bryan O. Alferez;Yuan-Fang Wang;Long Jiao	2004	Multimedia Tools and Applications	10.1023/B:MTAP.0000046385.62974.be	computer vision;affine transformation;affine shape adaptation	Vision	43.6016269538109	-55.131396490269545	4325
fa5a38845625390166a18309ce1dfaf017756412	orthogonal cross cylinder using segmentation based environment modeling	environment maps;image segmentation;image processing;computer graphics;procesamiento imagen;traitement image;fixed point;segmentation image;crossed cylinder;navigation system;point of view;grafico computadora;infographie;cylindre croise	Orthogonal Cross Cylinder (OCC) mapping and segmentation based modeling methods have been implemented for constructing the image-based navigation system in this paper. The OCC mapping method eliminates the singularity effect caused in the environment maps and shows an almost even amount of area for the environment occupied by a single texel. A full-view image from a fixed point-of-view can be obtained with OCC mapping although it becomes difficult to express another image when the point-of-view has been changed. The OCC map is segmented according to the objects that form the environment and the depth value is set by the characteristics of the classified objects for the segmentation-based modeling. This method can easily be implemented on an environment map and makes the environment modeling easier through extracting the depth value by the image segmentation.	cylinder-head-sector;fixed point (mathematics);glossary of computer graphics;image segmentation;map;optimistic concurrency control;reflection mapping;technological singularity;texel (graphics)	Seung Taek Ryoo;Kyung-hyun Yoon	2002		10.1007/3-540-46080-2_15	image texture;computer vision;range segmentation;image processing;computer science;segmentation-based object categorization;fixed point;image segmentation;computer graphics;scale-space segmentation;computer graphics (images)	Robotics	51.26915800103886	-58.28566351000037	4327
41a8e5530d3654b57cd250933463fcc204c6e3fe	prediction of organ space colorectal surgical site infections using prognostic bayesian networks			bayesian network	Yangmei Zhou;David W. Larson;Elizabeth B. Habermann;James M. Naessens;Hongfang Liu;Sunghwan Sohn	2017			bayesian network;computer science;artificial intelligence;pattern recognition	ML	7.72804043052341	-77.59364453605525	4342
1f9aa9ab6224e3b51de2524f6fd6b0e927ea8bec	a monitoring system for home-based physiotherapy exercises		This paper describes a robust, low-cost, vision based monitoring system for home-based physical therapy exercises. Our system contains two different modules. The first module achieves exercise recognition by building representations of motion patterns, stance knowledge, and object usage information in gray-level and depth video sequences and then combines these representations in a generative Bayesian network. The second module estimates the repetition count in an exercise session by a novel approach. We created a dataset that contains 240 exercise sessions and tested our system on this dataset. At the end, we achieved very favourable recognition rates and encouraging results on the estimation of repetition counts.	bayesian network;kinect;session (computer science)	Ilktan Ar;Yusuf Sinan Akgül	2012		10.1007/978-1-4471-4594-3_50	distributed computing;computer science;generative grammar;machine learning;bayesian network;artificial intelligence;physical therapy exercises	Vision	4.307974379025756	-85.32544223654709	4359
5a499ffbadf29def54a507a36a44c2978c818bab	detecting bilateral symmetry in perspective	detectors;mirrors;perspective projection;vehicle detection;psychology;computer vision;computer vision object detection image analysis detectors psychology image reconstruction vehicle detection laboratories robustness mirrors;image reconstruction;datavetenskap datalogi;robustness;image analysis;global symmetries;computer science;object detection	A method is presented for efficiently detecting bilateral symmetry on planar surfaces under perspective projection. The method is able to detect local or global symmetries, locate symmetric surfaces in complex backgrounds, and detect multiple incidences of symmetry. Symmetry is simultaneously evaluated across all locations, scales, orientations and under perspective skew. Feature descriptors robust to local affine distortion are used to match pairs of symmetric features. Feature quadruplets are then formed from these symmetric feature pairs. Each quadruplet hypothesises a locally planar 3D symmetry that can be extracted under perspective distortion. The method is posed independently of a specific feature detector or descriptor. Results are presented demonstrating the efficacy of the method for detecting bilateral symmetry under perspective distortion. Our unoptimised Matlab implementation, running on a standard PC, requires of the order of 20 seconds to process images with 1,000 feature points.	3d projection;algorithm;apache axis;bilateral filter;computation;distortion;feature model;image noise;matlab;sensor;xslt/muenchian grouping	Hugo Cornelius;Gareth Loy	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.63	iterative reconstruction;computer vision;detector;feature detection;perspective;image analysis;computer science;machine learning;mathematics;geometry;feature;robustness	Vision	45.238724514805966	-54.6442040930974	4369
1fac83a3eeff02561456b0ebe08995a87158a8c7	effect of complementary visual words versus complementary features on clustering for effective content-based image search				Zahid Mehmood;Muhammad Rashid;Amjad Rehman;Tanzila Saba;Hassan Dawood;Hussain Dawood	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-171137		Vision	30.841539612432417	-57.808827504873925	4374
44d22e70e0db883e1bbca76f382002d30003b33d	modeling the electrical field created by mass neural activity	simulation;ecog;erd;gamma oscillations	Gamma oscillations of large scale electrical activity are used in electrophysiological studies as markers for neural activity and functional processes in the cortex, yet the nature of this mass neural phenomenon and its relation to the evoked response potentials (ERP) are still not well understood. Many studies associated the gamma oscillations with oscillators around the 40 Hz frequency, yet recent studies have shown that gamma frequencies may be part of a broadband phenomenon ranging from 30 Hz up to 250 Hz. In this study we have examined the possibility that a simple model, based on available neurophysiological parameters, involving an increase in asynchronous (Poisson distributed) neural firing may be sufficient to generate the observed gamma power increases. Our simulation shows a roughly linear increase in gamma power as a function of the aggregated firing rate of the neural population, while the influence of the synchronization level within the neurons on the gamma power is limited. Our model supports the viewpoint that the broadband gamma response is mainly driven by the summed, asynchronous, activity of the neural population. We show that the time frequency spectrogram of the stimulus response can be reconstructed by combining two different phenomena-the broadband gamma power increase due to local processing and the more spatially distributed event related desynchronization (ERD). Our model thus raises the possibility that the broadband gamma response is closely linked to the aggregate population firing rate of the recorded neurons.	acoustic evoked brain stem potentials;action potential;aggregate data;asynchronous i/o;erp;electrocorticography;entity–relationship model;frequency band;hertz (hz);neural ensemble;neural oscillation;population;projections and predictions;published comment;simulation;spectrogram;thalamic structure	Eran Privman;Rafael Malach;Yehezkel Yeshurun	2013	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2013.01.004	entity–relationship model;computer science;artificial intelligence	ML	19.72729995910325	-74.72061303977065	4380
e1778ca695e430ab7ef21b9bb26107b4fdce0ed7	a study of detection of trip and fall using doppler sensor on embedded computer	androids;senior citizens;android os;legged locomotion;information terminal;assisted living;doppler effect;trip and fall;humanoid robots;monitoring;accidents;embedded computer;doppler effect senior citizens legged locomotion accidents androids humanoid robots monitoring;fast fourier transform trip detection fall detection doppler sensor embedded computer japan domestic accidents elderly persons accidental death privacy issues movement detection movement identification frequency analysis;android os care of elderly person doppler sensor trip and fall embedded computer information terminal;care of elderly person;fast fourier transforms;doppler measurement;fast fourier transforms accidents assisted living doppler measurement;doppler sensor	A recent social problem in Japan is domestic accidents involving elderly persons, who are increasingly living alone. Domestic accidents are unpredictable emergent situations, yet the constant care of elderly person by non-relatives or caregivers is difficult due to associated labor costs, privacy problems, and burdens placed on elderly persons themselves. This paper targets trip and fall, the leading cause of accidental death in elderly persons, and examines a detection method for such accidents. To address privacy issues, we use a Doppler sensor that can detect movement without the use of personally identifying data. We also investigate the movement detection and identification by frequency analysis of sensor output using fast Fourier transform. In addition, we design and implement a prototype terminal to detect trip and fall by using a Doppler sensor and an embedded computer.	care-of address;embedded system;emergence;error-tolerant design;fast fourier transform;frequency analysis;privacy;prototype;sensor	Masaru Uegami;Takeshi Iwamoto;Michito Matsumoto	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378294	fast fourier transform;simulation;doppler effect;telecommunications;computer science;humanoid robot;artificial intelligence;computer security	Robotics	6.46054205381292	-87.75288003712352	4387
b8c2d6ef9682eb7b0ed6ca2fa8bcdc024d37aa43	noninvasive internal bleeding detection method by measuring blood flow under ultrasound cross-section image	manipulators;algorithms blood flow velocity hemorrhage humans image enhancement;phantoms;hemorrhaging biomedical imaging blood flow fluid flow measurement volume measurement blood;ultrasonic imaging;medical robotics;medical image processing;ultrasonic imaging biomedical ultrasonics blood flow measurement blood vessels haemorheology manipulators medical image processing medical robotics phantoms ultrasonic devices;ultrasound probe basis 1 noninvasive internal bleeding detection method ultrasound cross section image ultrasound image processing robotic system noninvasive modality ultrasound imaging device measurement error constructed blood flow measurement algorithm phantom artery model manipulator;ultrasonic devices;biomedical ultrasonics;blood flow measurement;blood vessels;haemorheology	The purpose of this paper is to propose noninvasive internal bleeding detection method by using ultrasound (US) image processing under US cross-section image. In this study, we have developed a robotic system for detecting internal bleeding based on the blood flow measured by using a noninvasive modality like an US imaging device. Some problems related to the measurement error, however, still need to be addressed. In this paper, we focused on US image processing under US cross-section image, and constructed blood flow measurement algorithm under US cross-section image for internal bleeding detection. We conducted preliminary blood flow measurement experiments using a phantom containing artery model and a manipulator equipped with a US probe (BASIS-1). The results present the experimental validation of the proposed method.	algorithm;experiment;image processing;imaging device;internal hemorrhage;modality (human–computer interaction);phantom reference;phantoms, imaging;robot;sensor;blood flow measurement	Keiichiro Ito;Koichi Tsuruta;Shigeki Sugano;Hiroyasu Iwata	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346643	radiology;medicine;pathology;biological engineering	Robotics	41.264218054522914	-85.96882041072486	4389
19ad96b651025d52e96cdaf2cb08da7bfb836db6	expand training set for face detection by ga re-sampling	image sampling;detectors;image databases;snow;application software;data collection;face database;statistical methods;statistical method;testing;validation set;computer vision;image sampling face recognition genetic algorithms visual databases statistical analysis;learning systems;face recognition;statistical analysis;test set;next generation;sparse network of winnow;face detection testing snow genetic algorithms detectors computer vision application software learning systems image databases computer science;genetic algorithm;genetic algorithms;adaboost based face detector;computer science;face detection;genetic algorithm resampling;test set face detection genetic algorithm resampling face recognition face database sparse network of winnow adaboost based face detector statistical methods validation set;generalization capability;visual databases	Data collection for both training and testing a classifier is a tedious but essential step towards face detection and recognition. All of the statistical methods suffer from this problem. This paper presents a genetic algorithm (GA)-based method to swell face database through re-sampling from existing faces. The basic idea is that a face is composed of a limited components set, and the GA can simulate the procedure of heredity. This simulation can also cover the variations of faces in different lighting conditions, poses, accessories, and quality conditions. All the collected face samples are aligned and randomly divided into three sub-sets: training, validating, and testing set. The training set is then used to train a sparse network of winnow (SNoW). In addition, it is also used as the initial population of the GA. After each generation, we use the initial generation and the solutions with high fitness values to re-train the SNoW, and the newly-trained SNoW is used to evaluate the individuals of next generation and also tested on validation set and test set. To verify the generalization capability of the proposed method, we also use the expanded database to train an AdaBoost-based face detector and test it on the MIT+CMU frontal face test set. The experimental results show that the data collection can be speeded up efficiently by the proposed methods.	adaboost;face detection;genetic algorithm;randomness;sampling (signal processing);simulation;software release life cycle;sparse matrix;test set	Jie Chen;Xilin Chen;Wen Gao	2004	Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.	10.1109/AFGR.2004.1301511	facial recognition system;computer vision;genetic algorithm;computer science;machine learning;pattern recognition	Vision	26.1454308119284	-58.702078424096364	4391
16086f4378a7439c2bfd7e41fc1f7ffd443b3df0	democratic diffusion aggregation for image retrieval	vectors content based retrieval feature extraction image coding image fusion image retrieval matrix algebra multimedia systems;query fusion democratic diffusion aggregation dda image retrieval;diffusion processes;query fusion;visualization;image representation;feature extraction;initial top ranked image vectors content based image retrieval multimedia field large scale image search local features image feature encoding sum aggregation democratic diffusion aggregation method embedded vectors kernel matrix weighting coefficients local descriptors query fusion strategy;optimization;image representation image retrieval visualization feature extraction context diffusion processes optimization;context;democratic diffusion aggregation dda;image retrieval	Content-based image retrieval is an important research topic in the multimedia field. In large-scale image search using local features, image features are encoded and aggregated into a compact vector to avoid indexing each feature individually. In the aggregation step, sum-aggregation is wildly used in many existing works and demonstrates promising performance. However, it is based on a strong and implicit assumption that the local descriptors of an image are identically and independently distributed in descriptor space and image plane. To address this problem, we propose a new aggregation method named democratic diffusion aggregation (DDA) with weak spatial context embedded. The main idea of our aggregation method is to re-weight the embedded vectors before sum-aggregation by considering the relevance among local descriptors. Different from previous work, by conducting a diffusion process on the improved kernel matrix, we calculate the weighting coefficients more efficiently without any iterative optimization. Besides considering the relevance of local descriptors from different images, we also discuss an efficient query fusion strategy which uses the initial top-ranked image vectors to enhance the retrieval performance. Experimental results show that our aggregation method exhibits much higher efficiency (about × 14 faster) and better retrieval accuracy compared with previous methods, and the query fusion strategy consistently improves the retrieval quality.	coefficient;content-based image retrieval;embedded system;image plane;iterative method;mathematical optimization;relevance	Zhanning Gao;Jianru Xue;Wengang Zhou;Shanmin Pang;Qi Tian	2016	IEEE Transactions on Multimedia	10.1109/TMM.2016.2568748	image texture;computer vision;feature detection;visual word;visualization;feature extraction;image retrieval;computer science;pattern recognition;automatic image annotation;information retrieval	Vision	38.468858666323115	-61.21833250061031	4401
5663b101dc21c26bda07a4092ad6d23af454fb2f	contour-based multisensor image registration with rigid transformation	sensor phenomena and characterization;image processing;contour;centroid;chain code;optical noise;edge detection;image matching;image registration image sensors satellites mutual information data mining optical noise image processing joining processes parameter estimation sensor phenomena and characterization;long axis;image fusion;image sensors;data mining;rigid transformation;long axis image registration contour rigid transformation chain code invariant moments centroid;multisensor image registration;feature extraction;image registration;satellites;joining processes;image registration edge detection feature extraction image fusion image matching;mutual information;parameter estimation;geometrical deformation;contour matching;edge detection multisensor image registration geometrical deformation rigid transformation feature extraction contour matching;invariant moments	This paper presents a contour-based multisensor image registration algorithm. The characteristic of this approach is that the registration parameters are calculated according to the centroids and the long axes of matched contour pairs in the images to be registered It overcomes the difficulties of control point detection and correspondence in feature- based registration techniques. The geometrical deformation between the reference and sensed images is assumed to follow a rigid transformation. Salient contours are extracted from the reference and sensed images, respectively. After contour matching, open contour matches are changed to closed contour matches by linking the two endpoints of each open contour together with a line section. Registration parameters are then estimated according to the centroids and the angles of long axes of closed contour matches. Experiments using real data show that the proposed algorithm works well in multisensor image registration.	algorithm;contour line;control point (mathematics);emoticon;experiment;image moment;image registration;point set registration	Zhenhua Li;Henry Leung	2007	2007 10th International Conference on Information Fusion	10.1109/ICIF.2007.4408109	computer vision;machine learning;pattern recognition;mathematics	Vision	50.93682503370425	-52.98235536597643	4417
a4a50b4a8b423b1e75f6a493656d8a43a7cfef43	prediction of single neuron spiking activity using an optimized nonlinear dynamic model	optimisation;physiological models brain diseases neurophysiology optimisation parameter estimation patient treatment;brain;neurons mathematical model computational modeling biological system modeling optimization adaptation models predictive models;action potentials humans models neurological models theoretical neurons nonlinear dynamics;spike distance measurement single neuron spiking activity optimized nonlinear dynamic model brain disease treatment neural spiking phenomenon mathematical models experimental neuronal responses parameter estimation interspike intervals single neuron model experimental spike train biological neuron model parameters gradient descent method;diseases;patient treatment;neurophysiology;parameter estimation;physiological models	The increasing need of knowledge in the treatment of brain diseases has driven a huge interest in understanding the phenomenon of neural spiking. Researchers have successfully been able to create mathematical models which, with specific parameters, are able to reproduce the experimental neuronal responses. The spiking activity is characterized using spike trains and it is essential to develop methods for parameter estimation that rely solely on the spike times or interspike intervals (ISI). In this paper we describe a new technique for optimization of a single neuron model using an experimental spike train from a biological neuron. We are able to fit model parameters using the gradient descent method. The optimized model is then used to predict the activity of the biological neuron and the performance is quantified using a spike distance measure.	action potential;biological neuron model;brain diseases;cardiomyoplasty;estimation theory;gradient descent;mathematical model;mathematical optimization;mathematics;nonlinear system;population parameter;spike glycoprotein, coronavirus;spiking neural network;the spike (1997)	Anish Mitra;Andre Manitius;Tim Sauer	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346482	neuroscience;computer science;artificial intelligence;machine learning;estimation theory;neurophysiology;statistics	ML	16.6038222303118	-69.2648171848507	4428
3d62c6ccf420fcabb0ddb88f949df09d3cae8f52	immuco: a database of gene co-expression in immune cells	animals;mice;databases genetic;internet;期刊论文;immune system;transcriptome;humans;gene expression profiling;oligonucleotide array sequence analysis	Current gene co-expression databases and correlation networks do not support cell-specific analysis. Gene co-expression and expression correlation are subtly different phenomena, although both are likely to be functionally significant. Here, we report a new database, ImmuCo (http://immuco.bjmu.edu.cn), which is a cell-specific database that contains information about gene co-expression in immune cells, identifying co-expression and correlation between any two genes. The strength of co-expression of queried genes is indicated by signal values and detection calls, whereas expression correlation and strength are reflected by Pearson correlation coefficients. A scatter plot of the signal values is provided to directly illustrate the extent of co-expression and correlation. In addition, the database allows the analysis of cell-specific gene expression profile across multiple experimental conditions and can generate a list of genes that are highly correlated with the queried genes. Currently, the database covers 18 human cell groups and 10 mouse cell groups, including 20,283 human genes and 20,963 mouse genes. More than 8.6 × 10(8) and 7.4 × 10(8) probe set combinations are provided for querying each human and mouse cell group, respectively. Sample applications support the distinctive advantages of the database.	coefficient;database;gene co-expression network;gene expression profiling;microarray analysis	Pingzhang Wang;Huiying Qi;Shibin Song;Shuang Li;Ningyu Huang;Wenling Han;Dalong Ma	2015		10.1093/nar/gku980	biology;the internet;immune system;transcriptome;bioinformatics;gene expression profiling;genetics	Comp.	3.453012635506104	-58.27146968432107	4441
276b64f2eab4e55caadf3837b9fe20c89f539c03	regulondb (version 5.0): escherichia coli k-12 transcriptional regulatory network, operon organization, and growth conditions	transcription genetic;software;escherichia coli;escherichia coli k12;databases genetic;internet;regulon;gene expression regulation bacterial;genome bacterial;operon;user computer interface;transcriptional regulatory network	RegulonDB is the internationally recognized reference database of Escherichia coli K-12 offering curated knowledge of the regulatory network and operon organization. It is currently the largest electronically-encoded database of the regulatory network of any free-living organism. We present here the recently launched RegulonDB version 5.0 radically different in content, interface design and capabilities. Continuous curation of original scientific literature provides the evidence behind every single object and feature. This knowledge is complemented with comprehensive computational predictions across the complete genome. Literature-based and predicted data are clearly distinguished in the database. Starting with this version, RegulonDB public releases are synchronized with those of EcoCyc since our curation supports both databases. The complex biology of regulation is simplified in a navigation scheme based on three major streams: genes, operons and regulons. Regulatory knowledge is directly available in every navigation step. Displays combine graphic and textual information and are organized allowing different levels of detail and biological context. This knowledge is the backbone of an integrated system for the graphic display of the network, graphic and tabular microarray comparisons with curated and predicted objects, as well as predictions across bacterial genomes, and predicted networks of functionally related gene products. Access RegulonDB at http://regulondb.ccg.unam.mx.	bibliographic database;databases;digital curation;ecocyc;gene regulatory network;genome;genome, bacterial;internet backbone;largest;level of detail;machine-readable medium;microarray;navigation;operon;organism;regulon;regulondb;scientific literature;table (information);transcription, genetic;vertebral column	Heladia Salgado;Socorro Gama-Castro;Martín Peralta-Gil;Edgar Díaz-Peredo;Fabiola Sánchez-Solano;Alberto Santos-Zavaleta;Irma Martínez-Flores;Verónica Jiménez-Jacinto;César Bonavides-Martínez;Juan Segura-Salazar;Agustino Martínez-Antonio;Julio Collado-Vides	2006	Nucleic Acids Research	10.1093/nar/gkj156	biology;the internet;bioinformatics;operon;escherichia coli;genetics;regulon	Comp.	-1.5437905676697348	-59.39309515847415	4443
ad275d0797f3e21b46052ace2017d859123335c2	estimation of foot plantar center of pressure trajectories with low-cost instrumented insoles using an individual-specific nonlinear model	fall risk assessment;falls in the elderly;foot plantar center of pressure;low-cost instrumented insoles;postural control	Postural control is a complex skill based on the interaction of dynamic sensorimotor processes, and can be challenging for people with deficits in sensory functions. The foot plantar center of pressure (COP) has often been used for quantitative assessment of postural control. Previously, the foot plantar COP was mainly measured by force plates or complicated and expensive insole-based measurement systems. Although some low-cost instrumented insoles have been developed, their ability to accurately estimate the foot plantar COP trajectory was not robust. In this study, a novel individual-specific nonlinear model was proposed to estimate the foot plantar COP trajectories with an instrumented insole based on low-cost force sensitive resistors (FSRs). The model coefficients were determined by a least square error approximation algorithm. Model validation was carried out by comparing the estimated COP data with the reference data in a variety of postural control assessment tasks. We also compared our data with the COP trajectories estimated by the previously well accepted weighted mean approach. Comparing with the reference measurements, the average root mean square errors of the COP trajectories of both feet were 2.23 mm (±0.64) (left foot) and 2.72 mm (±0.83) (right foot) along the medial-lateral direction, and 9.17 mm (±1.98) (left foot) and 11.19 mm (±2.98) (right foot) along the anterior-posterior direction. The results are superior to those reported in previous relevant studies, and demonstrate that our proposed approach can be used for accurate foot plantar COP trajectory estimation. This study could provide an inexpensive solution to fall risk assessment in home settings or community healthcare center for the elderly. It has the potential to help prevent future falls in the elderly.	accidental falls;approximation algorithm;coefficient;esthesia;foot;hereditary angioedema type iii;lateral thinking;least-squares analysis;mean squared error;medial graph;nonlinear system;plantar - anatomical location;population parameter;risk assessment;system of measurement;tooth root structure	Xinyao Hu;Jun Zhao;Dongsheng Peng;Zhenglong Sun;Xingda Qu	2018		10.3390/s18020421	physical medicine and rehabilitation;center of pressure (fluid mechanics);electronic engineering;engineering;sensory functions;trajectory;nonlinear system;reference data (financial markets);foot (unit);force platform	HCI	13.317103032354025	-85.23912890760029	4448
c6d7f94b539cf026c433809618e0097fde260376	sar image registration using multiscale image patch features with sparse representation	speckle;reliability;stationary wavelet transform swt image patch multiscale feature synthetic aperture radar sar image registration sparse representation;feature extraction synthetic aperture radar image registration correlation speckle reliability transforms;feature extraction;image registration;transforms;correlation;synthetic aperture radar	In this paper, we propose a new image registration method for synthetic aperture radar (SAR) image with multiscale image patch features, in which the sparse representation technique is exploited. Considering the influence of speckle noise on feature extraction, in the proposed method, a spatial correlation strategy based on stationary wavelet transform is adopted to select the reliable feature points from the initial scale invariant feature transform keypoints in the reference image. By introducing multiscale image patch, a new feature descriptor is further designed to describe the attribute domain of feature points for higher discrimination. The corresponding points in the sensed image are established based on the minimum discrepancy criterion calculated by the sparse representation technique. Moreover, the local geometric consistency among a feature point and its nearest neighbor points is employed to remove the mismatches from the tentative matches. Twenty-two pairs of SAR images acquired under various conditions are utilized to validate the effectiveness of the proposed method. Compared with the traditional SAR image registration methods, the results show that the proposed method is competent to improve the registration performance substantially.	aperture (software);attribute domain;discrepancy function;entity name part qualifier - adopted;feature extraction;image registration;scale-invariant feature transform;single linkage cluster analysis;sparse approximation;sparse matrix;stationary process;stationary wavelet transform;synthetic data;visual descriptor;registration - actclass	Jianwei Fan;Yan Wu;Ming Li;Wenkai Liang;Qiang Zhang	2017	IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing	10.1109/JSTARS.2016.2628911	speckle pattern;computer vision;feature detection;synthetic aperture radar;feature extraction;computer science;image registration;kanade–lucas–tomasi feature tracker;pattern recognition;reliability;mathematics;correlation;top-hat transform;feature;remote sensing	Vision	41.808377552142936	-56.066846544845866	4459
ba8c20bd4cc0d29e3f1b5142a8fb0a7a98bf22a1	modified local discriminant bases algorithm and its application in analysis of human knee joint vibration signals	wavelet packets dissimilarity measures linear discriminant analysis local discriminant bases vibroarthrographic signals;vibrations;dissimilarity measure;signal analysis algorithm design and analysis humans knee senior citizens pathology wavelet analysis wavelet packets signal processing spatial databases;signal analysis;algorithms artificial intelligence diagnosis computer assisted discriminant analysis humans joint diseases knee joint pattern recognition automated reproducibility of results retrospective studies sensitivity and specificity sound spectrography vibration;biomechanics;linear discriminate analysis;wavelet packet;feature extraction;wavelet packet decomposition;signal classification;feature extraction vibrations biomechanics medical signal processing signal classification;nonstationary signal analysis modified local discriminant bases algorithm human knee joint vibration signals knee joint disorders vibroarthrography swinging movement wavelet packet decompositions highly discriminatory basis functions signal classification feature extraction linear discriminant analysis based classifier;knee joint;classification accuracy;medical signal processing	Knee joint disorders are common in the elderly population, athletes, and outdoor sports enthusiasts. These disorders are often painful and incapacitating. Vibration signals [vibroarthrographic (VAG)] are emitted at the knee joint during the swinging movement of the knee. These VAG signals contain information that can be used to characterize certain pathological aspects of the knee joint. In this paper, we present a noninvasive method for screening knee joint disorders using the VAG signals. The proposed approach uses wavelet packet decompositions and a modified local discriminant bases algorithm to analyze the VAG signals and to identify the highly discriminatory basis functions. We demonstrate the effectiveness of using a combination of multiple dissimilarity measures to arrive at the optimal set of discriminatory basis functions, thereby maximizing the classification accuracy. A database of 89 VAG signals containing 51 normal and 38 abnormal samples were used in this study. The features extracted from the coefficients of the selected basis functions were analyzed and classified using a linear-discriminant-analysis-based classifier. A classification accuracy as high as 80% was achieved using this true nonstationary signal analysis approach.	arthropathy;articular system;auscultation;base;basis function;cellular automaton;classification;coefficient;color gradient;dimensionality reduction;eighty nine;extraction;feature extraction;knee joint;limited stage (cancer stage);linear discriminant analysis;network packet;numerical analysis;orthopedics;patients;signal processing;sports;temporomandibular joint disorders;wavelet;algorithm	Karthikeyan Umapathy;Sridhar Krishnan	2006	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2005.869787	speech recognition;feature extraction;computer science;engineering;biomechanics;machine learning;vibration;signal processing;pattern recognition;wavelet packet decomposition;physiology;physics	ML	16.41595795102395	-90.77146438566392	4474
7640151ff8b94dc50aa8566a1434c603d8470ead	multi-scale analysis of imaging features and its use in the study of copd exacerbation susceptible phenotypes	pulmonary disease;sensitivity and specificity;radiology;technology;nuclear medicine medical imaging;radiographic image enhancement;theory methods;lung;science technology;genetic predisposition to disease;life sciences biomedicine;radiographic image interpretation;x ray computed;reproducibility of results;pattern recognition;chronic obstructive;obstructive pulmonary disease;artificial intelligence;algorithms;humans;computer assisted;computer science;automated;tomography	We propose a novel framework for exploring patterns of respiratory pathophysiology from paired breath-hold CT scans. This is designed to enable analysis of large datasets with the view of determining relationships between functional measures, disease state and the likelihood of disease progression. The framework is based on the local distribution of image features at various anatomical scales. Principal Component Analysis is used to visualise and quantify the multi-scale anatomical variation of features, whilst the distribution subspace can be exploited within a classification setting. This framework enables hypothesis testing related to the different phenotypes implicated in Chronic Obstructive Pulmonary Disease (COPD). We illustrate the potential of our method on initial results from a subset of patients from the COPDGene study, who are exacerbation susceptible and non-susceptible.	ct pulmonary angiogram;ct scan;chronic obstructive airway disease;color gradient;kidney failure, chronic;lung diseases, obstructive;lung diseases;numerous;patients;phenotype;principal component analysis;progressive disease;subgroup;while	Felix J. S. Bragman;Jamie McClelland;Marc Modat;Sébastien Ourselin;John R. Hurst;David J. Hawkes	2014	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-10443-0_53	radiology;medicine;pathology;computer science;tomography;surgery;technology	Vision	29.867559828865705	-78.94927128853395	4477
ae9c4aa74f241796fe753acbf50063975b831a15	the core and pan-genome of the vibrionaceae	genomics;core proteome;proteomics evolution biological genomics microorganisms proteins;pan proteome;comparative genomics;evolution biological;marine environment;orthologous protein pan proteome vibrionaceae marine environment pathogens evolutionary trajectories comparative genomics;proteins;orthologous proteins comparative genomics vibrionaceae core proteome;vibrionaceae;orthologous protein;orthologous proteins;proteomics;evolutionary trajectories;microorganisms;capacitive sensors bioinformatics pathogens genomics systems biology biology computing humans diseases organisms marine animals;pathogens	Species of the family Vibrionaceae are ubiquitous in marine environments and the family contains numerous important pathogens of humans and marine species. In order to find the core, accessory and pan-proteome of this family, we deduced the complete set of orthologs for eleven strains from this family. The core proteome consists of 1,882 groups of orthologs, which is 28% of the 6,629 orthologous groups in this family. The composition of the proteomes are reflective of the differing evolutionary trajectories followed by different strains to similar phenotypes.	homology (biology);sequence homology	Timothy G. Lilburn;Hong Cai;Jianying Gu	2009	2009 International Joint Conference on Bioinformatics, Systems Biology and Intelligent Computing	10.1109/IJCBS.2009.60	biology;genomics;bioinformatics;microorganism;proteomics;comparative genomics;genetics	Robotics	2.655898482884037	-62.187825910986966	4481
501fc9364578cd51bee4cf8c95e49662fc4dcccd	max-pooling convolutional neural network for chinese digital gesture recognition		A pattern recognition approach is proposed for the Chinese digital gesture. We shot a group of digital gesture videos by a monocular camera. Then, the video was converted into frame format and turned into the gray image. We selected the gray image as our own dataset. The dataset was divided into six gesture classes and other meaningless gestures. We use the neural network (NN) combining convolution and Max-Pooling (MPCNN) for classification of digital gestures. The MPCNN presents some differences on the data preprocessing, the activation function and the network structure. The accuracy and the robustness have been verified by the simulation experiments with the dataset. The result shows that the MPCNN classifies six gesture classes with 99.98 % accuracy using the Max-Pooling, the Relu activation function, and the binarization processing.	convolutional neural network;gesture recognition	Qian Zhao;Yawei Li;Mengyu Zhu;Yuliang Yang;Ling Xiao;Chunyu Xu;Lin Li	2015		10.1007/978-3-319-38771-0_8	speech recognition;machine learning;pattern recognition;time delay neural network;convolutional neural network;neocognitron	AI	32.77394448894647	-65.34504882697443	4493
24e386f73769680eba6c6ddc5aace8c65e12e717	detection of architectural distortion in mammograms using geometrical properties of thinned edge structures	architectural distortion;classification;sensitivity;pectoral muscle;structures;specificity;cuckoo search;mammogram;breast cancer;geometrical properties of edge	The proposed method detects the most commonly missed breast cancer symptom, Architectural Distortion. The basis of the method lies in the analysis of geometrical properties of abnormal patterns that correspond to Architectural Distortion in mammograms. Pre-processing methods are employed for the elimination of Pectoral Muscle (PM) region from the mammogram and to localize possible centers of Architectural Distortion. Regions that are candidates to contain centroids of Architectural Distortion are identified using a modification of the isotropic SUSAN filter. Edge features are computed in these regions using Phase Congruency, which are thinned using Gradient Magnitude Maximization. From these thinned edges, relevant edge structures are retained based on three geometric properties namely eccentricity to retain near linear structures, perpendicular distance from each such structure to the centroid of the edges and quadrant support membership of these edge structures. Features for classification are generated from these three properties; a feed-forward neural network, trained using a combination of backpropagation and a metaheuristic algorithm based on Cuckoo search, is employed for classifying the suspicious regions identified by the modified filter for Architectural Distortion, as normal or malignant. Experimental analyses were carried out on mammograms obtained from the standard databases MIAS and DDSM as well as on images obtained from Lakeshore Hospital in Kochi, India. The classification step yielded a sensitivity of 89%, 89.8.7% and 97.6% and specificity of 90.9, 85 and 96.7% on 60 images from MIAS, 100 images from DDSM database and 100 images from Lakeshore Hospital respectively	artificial neural network;backpropagation;cuckoo search;database;distance (graph theory);distortion;expectation–maximization algorithm;feedforward neural network;gradient;metaheuristic;phase congruency;sensitivity and specificity	Rekha Lakshmanan;P. T. ShijiT.;Suma Mariam Jacob;Thara Pratab;Chinchu Thomas;Vinu Thomas	2017	Intelligent Automation & Soft Computing	10.1080/10798587.2017.1257544	computer vision;sensitivity;biological classification;computer science;breast cancer;structures;control theory;cuckoo search	Robotics	35.593742662577476	-75.81534483467632	4497
4fc647a52c88a1e19f1d20b702c4f97b1ff34de1	hit: linking herbal active ingredients to targets	drug discovery;plant preparations;data mining;active ingredient;proteins;databases factual;drugs chinese herbal	The information of protein targets and small molecule has been highly valued by biomedical and pharmaceutical research. Several protein target databases are available online for FDA-approved drugs as well as the promising precursors that have largely facilitated the mechanistic study and subsequent research for drug discovery. However, those related resources regarding to herbal active ingredients, although being unusually valued as a precious resource for new drug development, is rarely found. In this article, a comprehensive and fully curated database for Herb Ingredients' Targets (HIT, http://lifecenter.sgst.cn/hit/) has been constructed to complement above resources. Those herbal ingredients with protein target information were carefully curated. The molecular target information involves those proteins being directly/indirectly activated/inhibited, protein binders and enzymes whose substrates or products are those compounds. Those up/down regulated genes are also included under the treatment of individual ingredients. In addition, the experimental condition, observed bioactivity and various references are provided as well for user's reference. Derived from more than 3250 literatures, it currently contains 5208 entries about 1301 known protein targets (221 of them are described as direct targets) affected by 586 herbal compounds from more than 1300 reputable Chinese herbs, overlapping with 280 therapeutic targets from Therapeutic Targets Database (TTD), and 445 protein targets from DrugBank corresponding to 1488 drug agents. The database can be queried via keyword search or similarity search. Crosslinks have been made to TTD, DrugBank, KEGG, PDB, Uniprot, Pfam, NCBI, TCM-ID and other databases.	complement system proteins;cross link;dosage forms;drug discovery;drugbank;greater than;hit (internet);kegg;keyword;literature;ncbi taxonomy;pfam;protein data bank;protein targeting;search algorithm;similarity search;therapeutic targets database;thrombocytopenia;traditional chinese medicine;uniprot	Hao Ye;Li Ye;Hong Kang;Duanfeng Zhang;Lin Tao;Kailin Tang;Xueping Liu;Ruixin Zhu;Qi Liu;Y. Z. Chen;Yixue Li;Zhi-Wei Cao	2011		10.1093/nar/gkq1165	pharmacology;bioinformatics;drugbank;active ingredient;drug discovery	Comp.	-0.4664815680864486	-61.7460001352273	4504
35509ad94173a327337aaac0d63d1e527dbf1234	pattern recognition of big nutritional data in rct	multi-validation criterion;rct data;new multi-clustering approach;pattern recognition;big data;comprehensive clustering;nutritional data;big nutritional data;nih-funded rct;nutritional dataset;clustering accuracy;k means;simulation;heterogeneity	As technology develops and research environment improves, large volume of data is collected for analyses. Unfortunately, these data are collected but not fully used or untouched. Particularly, such big data from health and medical studies pose significant challenges to the methodological field. This paper presents a new multi-clustering approach for pattern recognition of big data in a randomized controlled trial (RCT) with multi-validation criteria. Specifically, a nutritional dataset was used to demonstrate our approach, which was generated from an NIH-funded RCT for patients with metabolic syndrome. The proposed approach includes a suite of emerging and popular clustering methods: probability-based Gaussian Mixture Model (GMM), Hidden Markov Random Fields(HMRFs), Self-Organizing Map (SOM)-based neural networks, K-means and Agglomerative Hierarchical method. Using our RCT data and multi-validation criteria, our approach identified a most sufficient set of nutritional variables and detected distinct dietary change patterns with a universal agreement among the proposed multi-methods. The trajectory patterns were then generated using the method with the most clustering accuracy which was cross-validated via simulation. These patterns generated new and finer results for outcomes of the RCT. While our approach demonstrated a more accurate and comprehensive clustering only for nutritional data in RCT, it can be generalized to big data in other research fields.	pattern recognition	Zhongjing Wang;Hua Fang;Honggang Wang;Gin-Fei Olendzki;Chonggang Wang;Yunsheng Ma	2013			computer science;data science;machine learning;data mining	Vision	5.171839299350846	-75.28876679478982	4505
6b601b4b2a9a89739d14f9d14c85efa3d24ccff4	structured representation using latent variable models	elektroteknik och elektronik;electrical engineering electronic engineering information engineering;datalogi;computer science	Over the past two centuries the industrial revolution automated a great part of work that involved human muscles. Recently, since the beginning of the 21st century, the focus has shifted towards automating work that is involving our brain to further improve our lives. This is accomplished by establishing human-level intelligence through machines, which lead to the growth of the field of artificial intelligence. Machine learning is a core component of artificial intelligence. While artificial intelligence focuses on constructing an entire intelligence system, machine learning focuses on the learning ability and the ability to further use the learned knowledge for different tasks. This thesis targets the field of machine learning, especially structured representation learning, which is key for various machine learning approaches. Humans sense the environment, extract information and make action decisions based on abstracted information. Similarly, machines receive data, abstract information from data through models and make decisions about the unknown through inference. Thus, models provide a mechanism for machines to abstract information. This commonly involves learning useful representations which are desirably compact, interpretable and useful for different tasks. In this thesis, the contribution relates to the design of efficient representation models with latent variables. To make the models useful, efficient inference algorithms are derived to fit the models to data. We apply our models to various applications from different domains, namely E-health, robotics, text mining, computer vision and recommendation systems. The main contribution of this thesis relates to advancing latent variable models and deriving associated inference schemes for representation learning. This is pursued in three different directions. Firstly, through supervised models, where better representations can be learned knowing the tasks, corresponding to situated knowledge of humans. Secondly, through structured representation models, with which different structures, such as factorized ones, are used for latent variable models to form more efficient representations. Finally, through non-parametric models, where the representation is determined completely by the data. Specifically, we propose several new models combining supervised learning and factorized representation as well as a further model combining non-parametric modeling and supervised approaches. Evaluations show that these new models provide generally more efficient representations and a higher degree of interpretability. Moreover, this thesis contributes by applying these proposed models in different practical scenarios, demonstrating that these models can provide efficient latent representations. Experimental results show that our models improve the performance for classical tasks, such as image classification and annotations, robotic scene and action understanding. Most notably, one of our models is applied to a novel problem in E-health, namely diagnostic prediction using discomfort drawings. Experimental investigation show here that our model can achieve significant results in automatic diagnosing and provides profound understanding of typical symptoms. This motivates novel decision support systems for healthcare personnel.	algorithm;artificial intelligence;computer vision;decision support system;humans;java annotation;latent variable;machine learning;recommender system;robot;robotics;situated;supervised learning;text mining	Cheng Zhang	2016			computer science;engineering;mechanical engineering	AI	1.6869527679048326	-73.91105320946949	4522
38696d5573cfc5c573d2bf23ee0238d5913ebc86	noise smoothing for structural vibration test signals using an improved wavelet thresholding technique	health research;uk clinical guidelines;biological patents;europe pubmed central;sigmoid function;wavelet thresholding;citation search;uk phd theses thesis;vibration testing;denoise;wavelet transform wt;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	In structural vibration tests, one of the main factors which disturb the reliability and accuracy of the results are the noise signals encountered. To overcome this deficiency, this paper presents a discrete wavelet transform (DWT) approach to denoise the measured signals. The denoising performance of DWT is discussed by several processing parameters, including the type of wavelet, decomposition level, thresholding method, and threshold selection rules. To overcome the disadvantages of the traditional hard- and soft-thresholding methods, an improved thresholding technique called the sigmoid function-based thresholding scheme is presented. The procedure is validated by using four benchmarks signals with three degrees of degradation as well as a real measured signal obtained from a three-story reinforced concrete scale model shaking table experiment. The performance of the proposed method is evaluated by computing the signal-to-noise ratio (SNR) and the root-mean-square error (RMSE) after denoising. Results reveal that the proposed method offers superior performance than the traditional methods no matter whether the signals have heavy or light noises embedded.	angular defect;benchmark (computing);catastrophic interference;coefficient;computation (action);computer hardware;discrete wavelet transform;elegant degradation;embedded system;embedding;experiment;interference (communication);mean squared error;natural science disciplines;noise reduction;noise-induced hearing loss;nonlinear system;numerical method;particle filter;plant roots;rule (guideline);selection rule;sensor;sigmoid colon;sigmoid function;signal-to-noise ratio;smoothing (statistical technique);thresholding (image processing);tremor	Tinghua Yi;Hong-Nan Li;Xiao-Yan Zhao	2012		10.3390/s120811205	speech recognition;telecommunications;computer science;bioinformatics;engineering;electrical engineering;balanced histogram thresholding;data mining;sigmoid function;statistics	AI	19.954361652026638	-90.21773712591772	4533
ffab1a591ca4cec9d2d6442b483d54958ded951a	melanoma cell colony expansion parameters revealed by approximate bayesian computation	simulation and modeling;melanomas;cell motility;distribution curves;monte carlo method;cell proliferation;algorithms;melanoma cells	In vitro studies and mathematical models are now being widely used to study the underlying mechanisms driving the expansion of cell colonies. This can improve our understanding of cancer formation and progression. Although much progress has been made in terms of developing and analysing mathematical models, far less progress has been made in terms of understanding how to estimate model parameters using experimental in vitro image-based data. To address this issue, a new approximate Bayesian computation (ABC) algorithm is proposed to estimate key parameters governing the expansion of melanoma cell (MM127) colonies, including cell diffusivity, D, cell proliferation rate, λ, and cell-to-cell adhesion, q, in two experimental scenarios, namely with and without a chemical treatment to suppress cell proliferation. Even when little prior biological knowledge about the parameters is assumed, all parameters are precisely inferred with a small posterior coefficient of variation, approximately 2-12%. The ABC analyses reveal that the posterior distributions of D and q depend on the experimental elapsed time, whereas the posterior distribution of λ does not. The posterior mean values of D and q are in the ranges 226-268 µm2h-1, 311-351 µm2h-1 and 0.23-0.39, 0.32-0.61 for the experimental periods of 0-24 h and 24-48 h, respectively. Furthermore, we found that the posterior distribution of q also depends on the initial cell density, whereas the posterior distributions of D and λ do not. The ABC approach also enables information from the two experiments to be combined, resulting in greater precision for all estimates of D and λ.	approximation algorithm;assumed;bayesian analysis;cell adhesion;cell proliferation;coefficient;color gradient;computation;estimated;experiment;inference;mathematical model;mathematics;neoplasms	Brenda N. Vo;Christopher C. Drovandi;Anthony N. Pettitt;Graeme J. Pettet	2015		10.1371/journal.pcbi.1004635	biology;bioinformatics;cell growth;motility;statistics;monte carlo method	ML	9.232225062068881	-66.05442698245164	4538
c630114c9d2e5cbe4bafc9448f253108834cf227	analysis of phase shifts in clinical eeg evoked by ect	phase shift;time series;power spectrum;major depression;electroencephalography;neural network model;eeg frequency bands;neurodynamics;electroconvulsive therapy	We propose a new strategy for studying the phase shifts of electroencephalography (EEG) after electroconvulsive therapy (ECT) of patients with major depression. We divide each ECT EEG time series into four phases and calculate the power spectrum and coherence of left and right prefrontal EEGs for each phase. Previously, we have qualitatively demonstrated certain ECT EEG dynamical patterns by using a neo-cortical neural network model. Now we quantitatively analyze the dynamical phase shifts of the ECT EEG data. Our results are suggestive for a deeper understanding of the ECT EEG patterns and for building more realistic cortical models.	electroconvulsive therapy;electroencephalography	Yuqiao Gu;Björn Wahlund;Hans Liljenström;Dietrich von Rosen;Hualou Liang	2005	Neurocomputing	10.1016/j.neucom.2004.11.004	eeg-fmri;electroencephalography;time series;phase;spectral density;artificial neural network;statistics	AI	19.71844185119757	-76.69635498505265	4550
f9aa7f69910fdda09820cda9ac561f9928a48776	fusion of community structures in multiplex networks by label constraints		We develop a Belief Propagation algorithm for community detection problem in multiplex networks, which more accurately represents many real-world systems. Previous works have established that real world multiplex networks exhibit redundant structures/communities, and that community detection performance improves by aggregating (fusing) redundant layers which are generated from the same Stochastic Block Model (SBM). We introduce a probability model for generic multiplex networks, aiming to fuse community structure across layers, without assuming or seeking the same SBM generative model for different layers. Numerical experiment shows that our model finds out consistent communities between layers and yields a significant detectability improvement over the single layer architecture. Our model also achieves a comparable performance to a reference model where we assume consistent communities in prior. Finally we compare our method with multilayer modularity optimization in heterogeneous networks, and show that our method detects correct community labels more reliably.	adjacency matrix;algorithm;apollonian network;belief propagation;cobham's thesis;community;experiment;fuse device component;generative model;generic drugs;genetic heterogeneity;inference;mathematical optimization;message passing;multilayer perceptron;multiplexing;node - plant part;numerical method;reference model;software propagation;stochastic block model;super bit mapping;world-system;anatomical layer	Y. Huang;Ashkan Panahi;Hamid Krim	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8552943	belief propagation;multiplex;reference model;generative model;architecture;stochastic block model;heterogeneous network;machine learning;artificial intelligence;community structure;computer science	ML	15.42958985857292	-52.97721601508114	4555
4b3d8715d93c1643bfe570777a7bbdd46c952f75	sift and surf performance evaluation against various image deformations on benchmark dataset	image features;detectors;histograms;feature detection;scene classification;surf;performance evaluation;image matching;viewpoint;training;image classification;testing;scale;kd trees sift surf viewpoint scale;speeded up robust features;sift;transforms image classification image matching;scale invariant feature transform;feature extraction;kd trees;transforms;kd tree;feature extraction training detectors testing noise histograms buildings;image matching sift surf performance evaluation image deformation benchmark dataset scene classification indoor environment outdoor environment vision robotics image transformation feature detection algorithm scale invariant feature transform speeded up robust features;buildings;noise	Scene classification in indoor and outdoor environments is a fundamental problem to the vision and robotics community. Scene classification benefits from image features which are invariant to image transformations such as rotation, illumination, scale, viewpoint, noise etc. Selecting suitable features that exhibit such invariances plays a key part in classification performance. This paper summarizes the performance of two robust feature detection algorithms namely Scale Invariant Feature Transform (SIFT) and Speeded up Robust Features (SURF) on several classification datasets. In this paper, we have proposed three shorter SIFT descriptors. Results show that the proposed 64D and 96D SIFT descriptors perform as well as traditional 128D SIFT descriptors for image matching at a significantly reduced computational cost. SURF has also been observed to give good classification results on different datasets.	algorithm;benchmark (computing);computational complexity theory;feature detection (computer vision);feature detection (web development);image registration;performance evaluation;robotics;scale-invariant feature transform;speeded up robust features	Nabeel Younus Khan;Brendan McCane;Geoff Wyvill	2011	2011 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2011.90	computer vision;computer science;machine learning;pattern recognition;k-d tree;scale-invariant feature transform;mathematics	Vision	40.0410159233862	-54.85397607833771	4569
9095f26f1d92664b5a685d30199066561012a884	reference layer artefact subtraction (rlas): a novel method of minimizing eeg artefacts during simultaneous fmri		Large artefacts compromise EEG data quality during simultaneous fMRI. These artefact voltages pose heavy demands on the bandwidth and dynamic range of EEG amplifiers and mean that even small fractional variations in the artefact voltages give rise to significant residual artefacts after average artefact subtraction. Any intrinsic reduction in the magnitude of the artefacts would be highly advantageous, allowing data with a higher bandwidth to be acquired without amplifier saturation, as well as reducing the residual artefacts that can easily swamp signals from brain activity measured using current methods. Since these problems currently limit the utility of simultaneous EEG-fMRI, new approaches for reducing the magnitude and variability of the artefacts are required. One such approach is the use of an EEG cap that incorporates electrodes embedded in a reference layer that has similar conductivity to tissue and is electrically isolated from the scalp. With this arrangement, the artefact voltages produced on the reference layer leads by time-varying field gradients, cardiac pulsation and subject movement are similar to those induced in the scalp leads, but neuronal signals are not detected in the reference layer. Taking the difference of the voltages in the reference and scalp channels will therefore reduce the artefacts, without affecting sensitivity to neuronal signals. Here, we test this approach by using a simple experimental realisation of the reference layer to investigate the artefacts induced on the leads attached to the reference layer and scalp and to evaluate the degree of artefact attenuation that can be achieved via reference layer artefact subtraction (RLAS). Through a series of experiments on phantoms and human subjects, we show that RLAS significantly reduces the gradient (GA), pulse (PA) and motion (MA) artefacts, while allowing accurate recording of neuronal signals. The results indicate that RLAS generally outperforms AAS when motion is present in the removal of the GA and PA, while the combination of AAS and RLAS always produces higher artefact attenuation than AAS. Additionally, we demonstrate that RLAS greatly attenuates the unpredictable and highly variable MAs that are very hard to remove using post-processing methods.	amino acids;amplifier;bandwidth (signal processing);data quality;distortion;dynamic range;electricity;electroencephalography;embedded system;embedding;experiment;gradient;morphologic artifacts;myocytes, cardiac;phantoms, imaging;software release life cycle;spatial variability;thin layer chromatography;video post-processing;doxorubicin/mitomycin protocol;electrode;fmri	Muhammad E. H. Chowdhury;Karen J. Mullinger;Paul Glover;Richard Bowtell	2014	NeuroImage	10.1016/j.neuroimage.2013.08.039	computer science	ML	25.095247152642006	-83.47330150819147	4574
541c288a0c25e99faedf33251cf05b34456fce63	objective measure of sleepiness and sleep latency via bispectrum analysis of eeg	sleepiness;management system;obstructive sleep apnea;sleep latency;bispectrum;work environment;sleep disorder;single channel;nonlinear transformation;indexation;movement disorder;electroencephalography	Chronic sleepiness is a common symptom in the sleep disorders, such as, Obstructive Sleep Apnea, Periodic leg movement disorder, narcolepsy, etc. It affects 8% of the adult population and is associated with significant morbidity and increased risk to individual and society. MSLT and MWT are the existing tests for measuring sleepiness. Sleep Latency (SL) is the main measures of sleepiness computed in these tests. These are the laboratory-based tests and require services of an expert sleep technician. There are no tests available to detect inadvertent sleep onset in real time and which can be performed in any professional work environment to measure sleepiness level. In this article, we propose a fully automated, objective sleepiness analysis technique based on the single channel of EEG. The method uses a one-dimensional slice of the EEG Bispectrum representing a nonlinear transformation of the underlying EEG generator to compute a novel index called Sleepiness Index. The SL is then computed from the SI. Working on the patient’s database of 42 subjects we computed SI and estimated SL. A strong significant correlation (r ≥ 0.70, s < 0.001) was found between technician scored SL and that computed via SI. The proposed technology holds promise in the automation of the MSLT and MWT tests. It can also be developed into a sleep management system, wherein the SI is incorporated into a sleepiness index alert unit to alarm the user when sleepiness level crosses the predetermined threshold.	alert:type:point in time:^patient:nominal;bispectrum;chronic lymphocytic leukemia;electroencephalography;international system of units;morbidity - disease rate;movement disorders;narcolepsy;nonlinear system;onset (audio);patients;sl (complexity);score;sleep apnea syndromes;sleep disorders;sleep polysomnography domain;somnolence;wake-sleep algorithm;multiple sleep latency test (mslt)	Vinayak Swarnkar;Udantha R. Abeyratne;Craig Hukins	2010	Medical & Biological Engineering & Computing	10.1007/s11517-010-0715-x	psychology;bispectrum;simulation;sleep disorder;electroencephalography;management system;mathematics;anesthesia;forensic engineering;statistics	AI	16.1476445021148	-86.00259554447324	4592
ae18f1f6ce1ccac535ac64807d980d1d79d069d9	the dna rodent: a portable hand held dna sequence reader	dna sequence			Edmond J. Breen;Lois H. Browne;Len Glue;Keith L. Williams	1988	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/4.1.217	biology;dna sequencing;molecular biology;bioinformatics;genetics	Theory	1.1832924229804749	-63.852094271251914	4594
d4c816e1cc3a82643b7f918abc921752f8b81cec	steganalysis of content-adaptive binary image data hiding	steganalysis;binary image;steganography;content adaptive data hiding;l shape pattern	Most state-of-the-art binary image data hiding methods concentrate the embedding changes on the centers of l-shape patterns. This embedding criterion, however, introduces an unbalanced modification on boundary structures. This paper proposes a steganalytic scheme to detect recently developed content-adaptive binary image data hiding by exploiting the embedding effect associated with the l-shape pattern-based embedding criterion. We first assess how changing l  -shape patterns affects the distribution of a special 4×34×3 sized pattern. Based on the assessment, 4 classes of patterns that model the distribution of two pixels oriented the direction of pattern changing are employed to define a 32-dimensional steganalytic feature set. Experimental results show that, despite of the low dimensionality, the proposed steganalytic features can effectively detect state-of-the-art binary image data hiding schemes, especially those pattern-tracing-based approaches.	binary image;steganalysis	Bingwen Feng;Jian Weng;Wei Lu;Bei Pei	2017	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.01.008	computer vision;steganalysis;binary image;computer science;pattern recognition;data mining;mathematics;steganography;statistics	Vision	36.28921665820903	-60.702119295985874	4601
e909c3b39155cc69515b0ac6d7c496ac5df096ec	baseline drift and physiological noise removal in high field fmri data using kernel pca	bold signals;functional magnetic resonance imaging;kernel principal component analysis;brain;frequency analysis;respiratory fluctuations baseline drift physiological noise removal high field fmri kernel pca blood oxygenation dependent signals bold signals functional magnetic resonance imaging principal component analysis frequency analysis brain anatomy denoising cardiac fluctuations;aliasing;aliasing drift cardiac rate respiration;cardiology;blood oxygen level dependent;discriminant analysis;brain anatomy;respiratory fluctuations;thermal noise;automatic detection;cardiac fluctuations;medical image processing;principal component analysis;functional magnetic resonance images;blood;blood oxygenation dependent signals;drift;brain structure;baseline drift;respiration;kernel pca;image denoising;denoising;pneumodynamics;high field fmri;cardiac rate;noise removal;principal component analysis biomedical mri blood brain cardiology image denoising medical image processing pneumodynamics;kernel principal component analysis magnetic noise brain fluctuations blood magnetic resonance imaging frequency magnetic analysis anatomy;physiological noise removal;biomedical mri;principal component	Baseline drift and physiological (cardiac and respiratory) fluctuations are among major sources contaminating blood oxygenation level dependent (BOLD) signals in high field functional magnetic resonance imaging (fMRI). Automatically detecting and removing them have been long-standing problems. We propose here a new method, utilizing kernel principal component analysis (KPCA) and frequency analysis, to detect and remove the noise from fMRI data. Differing from thermal noise, the main energy of baseline drift and physiological noise are characterized by the most significant kernel principal components that also contain information on brain structure. To maintain the details of brain anatomy, we filter the feature projections to the components that are found to contain significant baseline drift and physiological noise. This approach is different from most discriminant analysis-based denoising methods that remove insignificant or noisy components before the reconstruction. Experimental results show that the proposed method increases the BOLD contrast and the detection sensitivity of activated voxels.	baseline (configuration management);frequency analysis;johnson–nyquist noise;kernel (operating system);kernel principal component analysis;linear discriminant analysis;noise reduction;resonance;sensor;voxel	Xiaomu Song;Tongyou Ji;Alice M. Wyrwicz	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517641	computer vision;speech recognition;kernel principal component analysis;computer science;machine learning;linear discriminant analysis;principal component analysis	ML	20.152193864262	-88.10110661878875	4613
fd878df143eb3ea0593cd2b5556ac4db27cdfbf6	sex-specific aspects of left and right ventricular volume regulation in patients following tetralogy of fallot repair		Ejection fraction (EF) is applied as a clinically relevant metric to assess both left (LV) and right ventricular (RV) function. EF depends on the interplay between end-systolic volume (ESV) and end-diastolic volume (EDV). The role of the two constitutive components is of particular interest for the follow-up study of Fallot patients at risk for RV volume overload. The volume regulation graph (VRG) relates ESV to EDV and has been advanced as a central tool to describe LV and RV function. The method permits additional analysis of the impact of clinically relevant determinants such as sex and age. Following Fallot repair and using MRI we evaluated LV and RV volumes in 124 patients (50 females), who were not taking any medication. Volumes were indexed for body surface area (BSA). The VRG regression lines are similar for both sexes, also when stratified for age (i.e. younger or older than 18 years), and different for LV and RV. However, RV ESV is larger (P=0.039) for adult males, as is RV EDV (P=0.026) in boys, relative to their female counterparts. For LV ESV we also found larger volumes, but only in boys (P=0.023) compared to girls. Average EF (only for RV) is lower in adult men compared to women (P=0.012), and to boys (P=0.007). These findings are partly in contrast with common observations made in individuals without a history of cardiac disease, where (with BSA indexation) LV and RV volumes are similar in children, but consistently larger in adult males. These results highlight the age- and sex-specific volumetric aspects of remodeling following surgery in Fallot patients, and emphasize the pivotal role of ESV size in both RV and LV.	body surface area;cloud fraction;diastole;ejection fraction (procedure);end diastolic volume imaging;end systolic volume imaging;entity framework;fluid overload;heart diseases;index;kidney failure, chronic;large;license;logical volume management;patients;residual volume;rota vector;ventricular fibrillation	Peter L. M. Kerkhof;B. W. Yoo;Peter M. van de Ven;Neal Handly	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037071	tetralogy of fallot;indexation;cardiology;ejection fraction;volume overload;graph;internal medicine;medicine	Visualization	21.201054882542877	-81.16019235988966	4627
2cef78f01b161c7a7e924002ca3a707af5f47560	characterization of quorum sensing and quorum quenching soil bacteria isolated from malaysian tropical montane forest	n acylhomoserine lactone;tropical climate;quorum quenching;biosensing techniques;soil microbiology;mass spectrometry;bacterial physiological phenomena;quorum sensing;polymerase chain reaction;trees;bacillus;pseudomonas frederiksbergensis;chromatography liquid;n dodecanoyl l homoserine lactone;dna primers;malaysia;p coumaroylhomoserine lactone;arthrobacter;rapid resolution liquid chromatography rrlc;base sequence;article;r medicine;liquid chromatography mass spectrometry lc ms	We report the production and degradation of quorum sensing N-acyl-homoserine lactones by bacteria isolated from Malaysian montane forest soil. Phylogenetic analysis indicated that these isolates clustered closely to the genera of Arthrobacter, Bacillus and Pseudomonas. Quorum quenching activity was detected in six isolates of these three genera by using a series of bioassays and rapid resolution liquid chromatography analysis. Biosensor screening and high resolution liquid chromatography-mass spectrometry analysis revealed the production of N-dodecanoyl-L-homoserine lactone (C12-HSL) by Pseudomonas frederiksbergensis (isolate BT9). In addition to degradation of a wide range of N-acyl-homoserine lactones, Arthrobacter and Pseudomonas spp. also degraded p-coumaroyl-homoserine lactone. To the best of our knowledge, this is the first documentation of Arthrobacter and Pseudomonas spp. capable of degrading p-coumaroyl-homoserine lactone and the production of C12-HSL by P. frederiksbergensis.	arthrobacter;biological assay;documentation;elegant degradation;european soil database;genera;l-methionine biosynthetic process from homoserine via o-acetyl-l-homoserine and cystathionine;lactones;leukoencephalitis, acute hemorrhagic;liquid chromatography mass spectrometry;p (complexity);pseudomonas frederiksbergensis;quorum quenching;random forest;sensitivity and specificity;sensor;tencent qq;virtual screening;acyl-coa dehydrogenase;quorum sensing	Teik-Min Chong;Chong-Lek Koh;Choon-Kook Sam;Yeun-Mun Choo;Wai-Fong Yin;Kok Gan Chan	2012		10.3390/s120404846	chromatography;quorum sensing;mass spectrometry;polymerase chain reaction;primer;soil microbiology	HCI	4.848607061466356	-63.77796547104374	4646
d3be8849ebecd40060644d63c4b3d4e2dc555315	a novel hybrid approach based on sub-pattern technique and whitened pca for face recognition	reconnaissance visage;traitement signal;methode globale locale;evaluation performance;metodo estadistico;analisis componente principal;performance evaluation;image processing;biometrie;evaluacion prestacion;modelo hibrido;biometrics;biometria;procesamiento imagen;statistical method;independent component analysis;modele hybride;traitement image;hybrid model;hybrid approach;automatic recognition;face recognition;independent component analysis ica;principal component analysis pca;methode statistique;sub pattern technique;feature extraction;signal processing;principal component analysis;analyse composante principale;pattern recognition;component analysis;analyse composante independante;global local method;reconnaissance forme;extraction caracteristique;reconocimiento patron;analisis componente independiente;metodo global local;procesamiento senal;reconocimiento automatico;reconnaissance automatique	Recently, in a task of face recognition, some researchers presented that ICA Architecture I involves a vertically centered PCA process (PCA I) and ICA Architecture II involves a whitened horizontally centered PCA process (PCA II). They also concluded that the performance of ICA strongly depends on its involved PCA process. This means that the computationally expensive ICA projection is unnecessary for further process and involved PCA process of ICA, whether PCA I or II, can be used directly for face recognition. But these approaches only consider the global information of face images. Some local information may be ignored. Therefore, in this paper, the Ac ce pte d m an us cri pt 2 sub-pattern technique was combined with PCA I and PCA II respectively for face recognition. In other words, two new different sub-pattern based whitened PCA approaches (which are called Sp-PCA I and Sp-PCA II respectively) were performed and compared with PCA I, PCA II, PCA, and sub-pattern based PCA (SpPCA). Then, we find that sub-pattern technique is useful to PCA I but not to PCA II and PCA. Simultaneously, we also discussed what causes this result in this paper. At last, by simultaneously considering global and local information of face images, we developed a novel hybrid approach which combines PCA II and Sp-PCA I for face recognition. The experimental results reveal that the proposed novel hybrid approach has better recognition performance than that obtained using other traditional methods.		Ping-Cheng Hsieh;Pi-Cheng Tung	2009	Pattern Recognition	10.1016/j.patcog.2008.09.024	independent component analysis;computer vision;speech recognition;image processing;feature extraction;computer science;artificial intelligence;machine learning;signal processing;biometrics;principal component analysis	Vision	44.39110019592874	-59.98069990136318	4677
bfdf4b48958195107d4a1b63398720fddedacbbe	iterative exact global histogram specification and ssim gradient ascent: a proof of convergence, step size and parameter selection	first order;parameter selection;pattern recognition;structural similarity	Alireza Avanaki user@yahoo.com (my ID is my last name) Abstract The SSIM-optimized exact global histogram specification (EGHS) is shown to converge in the sense that the first order approximation of the result’s quality (i.e., its structural similarity with input) does not decrease in an iteration, when the step size is small. Each iteration is composed of SSIM gradient ascent and basic EGHS with the specified target histogram. Selection of step size and other parameters is also discussed.	converge;gradient descent;histogram matching;iteration;order of approximation;structural similarity;times ascent	Alireza Nasiri Avanaki	2010	CoRR		mathematical optimization;combinatorics;computer science;histogram matching;structural similarity;pattern recognition;first-order logic;mathematics;statistics	ML	51.734174563511445	-70.42330596984667	4693
b25131bad927e35bf02ea350d6f21614f0807bb5	an extended image hashing concept: content-based fingerprinting using fjlt	signal image and speech processing;systems and data security;security science and technology;communications engineering networks	Dimension reduction techniques, such as singular value decomposition (SVD) and nonnegative matrix factorization (NMF), have been successfully applied in image hashing by retaining the essential features of the original image matrix. However, a concern of great importance in image hashing is that no single solution is optimal and robust against all types of attacks. The contribution of this paper is threefold. First, we introduce a recently proposed dimension reduction technique, referred as Fast JohnsonLindenstrauss Transform (FJLT), and propose the use of FJLT for image hashing. FJLT shares the low distortion characteristics of a random projection, but requires much lower computational complexity. Secondly, we incorporate Fourier-Mellin transform into FJLT hashing to improve its performance under rotation attacks. Thirdly, we propose a new concept, namely, content-based fingerprint, as an extension of image hashing by combining different hashes. Such a combined approach is capable of tackling all types of attacks and thus can yield a better overall performance in multimedia identification. To demonstrate the superior performance of the proposed schemes, receiver operating characteristics analysis over a large image database and a large class of distortions is performed and compared with the state-of-the-art image hashing using NMF.	computational complexity theory;dimensionality reduction;distortion;fingerprint;hash function;non-negative matrix factorization;random projection;singular value decomposition;zobrist hashing	Xudong Lv;Z. Jane Wang	2009	EURASIP J. Information Security	10.1155/2009/859859	dynamic perfect hashing;computer science;theoretical computer science;machine learning;data mining;locality preserving hashing;computer security	Vision	34.29372464431474	-59.37994516482951	4698
4ee7b09cc979f1af39870f09c95a738e6556ca31	network events on multiple space and time scales in cultured neural networks and in a stochastic rate model	animals;models neurological;cell culture techniques;neural networks;rats;eigenvalues;nonlinear systems;electrodes;hidden markov models;cells cultured;animals newborn;cerebral cortex;nerve net;neurons;action potentials;computational biology;computer simulation;bifurcation theory;synapses	Cortical networks, in-vitro as well as in-vivo, can spontaneously generate a variety of collective dynamical events such as network spikes, UP and DOWN states, global oscillations, and avalanches. Though each of them has been variously recognized in previous works as expression of the excitability of the cortical tissue and the associated nonlinear dynamics, a unified picture of the determinant factors (dynamical and architectural) is desirable and not yet available. Progress has also been partially hindered by the use of a variety of statistical measures to define the network events of interest. We propose here a common probabilistic definition of network events that, applied to the firing activity of cultured neural networks, highlights the co-occurrence of network spikes, power-law distributed avalanches, and exponentially distributed 'quasi-orbits', which offer a third type of collective behavior. A rate model, including synaptic excitation and inhibition with no imposed topology, synaptic short-term depression, and finite-size noise, accounts for all these different, coexisting phenomena. We find that their emergence is largely regulated by the proximity to an oscillatory instability of the dynamics, where the non-linear excitable behavior leads to a self-amplification of activity fluctuations over a wide range of scales in space and time. In this sense, the cultured network dynamics is compatible with an excitation-inhibition balance corresponding to a slightly sub-critical regime. Finally, we propose and test a method to infer the characteristic time of the fatigue process, from the observed time course of the network's firing rate. Unlike the model, possessing a single fatigue mechanism, the cultured network appears to show multiple time scales, signalling the possible coexistence of different fatigue mechanisms.	anatomy, regional;artificial neural network;avalanches;coexist (image);cultured neuronal network;depressive disorder;dynamical system;emergence;excitable medium;excitation;fatigue;global positioning system;instability;neural network simulation;neural networks;nonlinear system;ocular orbit;synaptic package manager;video-in video-out	Guido Gigante;Gustavo Deco;Shimon Marom;Paolo Del Giudice	2015		10.1371/journal.pcbi.1004547	computer simulation;neuroscience;eigenvalues and eigenvectors;synapse;electrode;artificial intelligence;bifurcation theory;cell culture;action potential	ML	17.88890227153836	-70.47290214747878	4701
7cf6f7da2e932da9a73b218c65f0b0264dd25479	producing radiologist-quality reports for interpretable artificial intelligence		Current approaches to explaining the decisions of deep learning systems for medical tasks have focused on visualising the elements that have contributed to each decision. We argue that such approaches are not enough to “open the black box” of medical decision making systems because they are missing a key component that has been used as a standard communication tool between doctors for centuries: language. We propose a model-agnostic interpretability method that involves training a simple recurrent neural network model to produce descriptive sentences to clarify the decision of deep learning classifiers. We test our method on the task of detecting hip fractures from frontal pelvic x-rays. This process requires minimal additional labelling despite producing text containing elements that the original deep learning classification model was not specifically trained to detect. The experimental results show that: 1) the sentences produced by our method consistently contain the desired information, 2) the generated sentences are preferred by doctors compared to current tools that create saliency maps, and 3) the combination of visualisations and generated text is better than either alone.	artificial intelligence;artificial neural network;black box;deep learning;machine learning;map;medical decision making;network model;radiology;recurrent neural network;sensor;text-based (computing)	William Gale;Luke Oakden-Rayner;Gustavo Carneiro;Andrew P. Bradley;Lyle J. Palmer	2018	CoRR		machine learning;computer science;deep learning;labelling;salience (neuroscience);interpretability;black box;recurrent neural network;artificial intelligence	AI	29.904056428866184	-74.03315455113643	4734
2aea042656686117fa9bfe0b7ac5055ad207c068	border noise removal and clean up based on retinex theory		Conversion from gray scale or color document image into binary image is the main step in most of Optical Character Recognition (OCR) systems and document analysis. After digitization, document images often suffer from poor contrast, noise, uniform lighting, and shadow. Also when a page of book is digitized using a scanner or a camera, a border noise, which is an unwanted text coming from the adjacent page, may appear. In this paper we present a simple and efficient document image clean up by border noise removal and enhancement based on retinex theory and global threshold. The proposed method produces high quality results compared to the previous works.		Marian Wagdy;Ibrahima Faye;Dayang Rohaya	2013		10.1007/978-981-4585-18-7_39	computer vision	EDA	38.55799324392138	-66.26899501499221	4741
5736760928c3384564ad52cd09c5cc3f49ce90b6	interactively co-segmentating topically related images with intelligent scribble guidance	minimisation;unsupervised learning;anotacion;minimization;cmu;scribbles;image segmentation;fonction energie;guidage;segmentacion de imagenes;user study;depth of field;recommandation;annotation;minimizacion;paralelisacion;apprentissage non supervise;guiado;co segmentation;profondeur champ;energy function;aprendizaje no supervisado;recommender system;parallelisation;segmentation image;parallelization;profundidad campo;funcion energia;recomendacion;guidance;recommendation;energy minimization;interactive segmentation	We present an algorithm for Interactive Co-segmentation of a foreground object from a group of related images. While previous works in co-segmentation have focussed on unsupervised co-segmentation, we use successful ideas from the interactive object-cutout literature. We develop an algorithm that allows users to decide what foreground is, and then guide the output of the co-segmentation algorithm towards it via scribbles. Interestingly, keeping a user in the loop leads to simpler and highly parallelizable energy functions, allowing us to work with significantly more images per group. However, unlike the interactive single-image counterpart, a user cannot be expected to exhaustively examine all cutouts (from tens of images) returned by the system to make corrections. Hence, we propose iCoseg, an automatic recommendation system that intelligently recommends where the user should scribble next. We introduce and make publicly available the largest co-segmentation dataset yet, the CMU-Cornell iCoseg dataset, with 38 groups, 643 images, and pixelwise hand-annotated groundtruth. Through machine experiments and real user studies with our developed interface, we show that iCoseg can intelligently recommend regions to scribble on, and users following these recommendations can achieve good quality cutouts with significantly lower time and effort than exhaustively examining all cutouts.	algorithm;autostereogram;chao (sonic);energy minimization;experiment;graphical user interface;interactivity;java;merge sort;pixel;recommender system;synthetic intelligence;unsupervised learning	Dhruv Batra;Adarsh Kowdle;Devi Parikh;Jiebo Luo;Tsuhan Chen	2010	International Journal of Computer Vision	10.1007/s11263-010-0415-x	unsupervised learning;computer vision;minimisation;simulation;computer science;machine learning;depth of field;image segmentation;energy minimization;recommender system;computer graphics (images)	Vision	53.33531528365081	-59.57684357071294	4743
4272c5dbd28138c4e493d798017da7a5d3fa670b	improved segmentation for footprint recognition of small mammals	footprints;conference item;thresholding;segment modification	In this paper we improve the automatic extraction of segments by resolving some of the issues for collected rat footprints, such as incomplete, fading, merged, or overlapping prints, or cuts due to the applied rectangular clipping process. First, binarization is by an adaptive method (proposed by Otsu) on the given input segment. Second, we remove small artefacts with a subsequent adaptive method. Third, merged regions are separated by a morphological method using an adaptive mask. Next, we find meaningful pads (central pad or toes) by analysing geometric relations defined by triangulation. Finally we reconstruct damaged footprints by using a convex-hull algorithm. We present experimental results of reconstructed footprints, and distributions of extracted features for improved segments. In the proposed technique, we automatically improve the quality and reliability of a scanned footprint image so as not to lose potential information for subsequent identification steps.	algorithm;convex hull;otsu's method;triangulation (geometry)	Bok-Suk Shin;Yihui Zheng;James C. Russell;Reinhard Klette	2012		10.1145/2425836.2425890	computer vision;thresholding;footprint;computer graphics (images)	Vision	43.1972642057366	-65.20282292942095	4751
f54900de2a4f3de6eeba3a5dac0cdb6b82d27880	visualization of anisotropic contact potentials within protein structures	analytical models;biology computing;protein model scoring;protein model scoring geometric contact potential protein structure prediction;amino acid;geometry;computational geometry;proteins visualization data visualization three dimensional displays geometry analytical models predictive models;three dimensional;energy function;data visualisation;protein structure;visualization;quality assessment;proteins;three dimensional displays;protein structure prediction;force field;structure prediction;data visualization;geometric contact potential;parameter space;predictive models;prediction model;contact geometry;proteins biology computing computational geometry data visualisation;improved energy function generation anisotropic residue dependent contact density potential visualization protein structure models local covalent geometry quality assessment quality refinement noncovalent geometry structure prediction synergistic modeling amino acid anisotropic contact potential extraction anisotropic contact potential analysis map projection contact geometry analysis plugin cgap cmview interactive protein modeling process geometric orientation propensities;analytical model	The use of local covalent geometry for quality assessment and refinement of protein structure models is a well-established methodology. The question arises whether information on non-covalent geometry contained within resolved structures can be harnessed to improve structure prediction. Moreover, incorporation of different combinations of priors would pave the way towards multi-body potentials. Existing empirical force-fields do not facilitate an interactive exploration of the parameter space and an assignment of spatial propensities to contacts. Hence, we investigate the possibility of making such propensities available for synergistic modeling. We present an approach that facilitates the extraction and analysis of anisotropic contact potentials for a multitude of parameters describing an amino acid and the conditions within its microenvi-ronment. For this purpose, two novel visualization principles will be introduced. The first visualization illustrates anisotropic residue-dependent contact density potentials in the form of a map projection. A second visualization is overlaid onto this, showing similar local neighborhoods as abstract traces of residues contained within each individual neighborhood. The Contact Geometry Analysis Plugin (CGAP) (for CMView) we developed allows incorporation of geometric orientation propensities into the process of interactive protein modeling and can be used for the generation of improved energy functions. It further supports the analysis of model quality, as it directly illustrates model consistency with known spatial propensities which, in turn, enables users to detect possible structural errors.	3d reconstruction;docking (molecular);force field (chemistry);homology (biology);homology modeling;interactivity;k-nearest neighbors algorithm;map projection;microsoft outlook for mac;protein data bank;protein structure prediction;prototype;refinement (computing);sampling (signal processing);scoring functions for docking;synergy;tracing (software)	Corinna Vehlow;Bernhard Preim;Michael Lappe	2011	2011 IEEE Symposium on Biological Data Visualization (BioVis).	10.1109/BioVis.2011.6094045	simulation;computer science;bioinformatics;machine learning	Visualization	13.074415967271998	-58.723700082876	4755
64cd70dd2bb8e0cad53dbbfff5794d4bb0c9ef73	temporal recurrence hashing algorithm for mining commercials from multimedia streams	histograms;media streaming cryptography;duplicate detection;video streaming;brute force pairwise matching;super fast tv commercial mining;multimedia streaming;frame hashing;streaming media histograms computational efficiency tv multimedia communication accuracy robustness;audio streams;duplicate detection fingerprinting;video streams;accuracy;large scale;time 1 month;streaming media;fingerprinting;cryptography;temporal recurrence hashing algorithm;time 10 hr;multimedia communication;media streaming;robustness;tv;computational efficiency;second stage hashing algorithm;multimedia streams;time 1 month temporal recurrence hashing algorithm multimedia streams super fast tv commercial mining frame hashing brute force pairwise matching second stage hashing algorithm audio streams video streams time 10 hr	We propose a dual-stage algorithm for fully-unsupervised and super-fast TV commercial mining in this paper. The two stages involved in process include: 1) searching for recurring short segments, and 2) assembling these short segments into sets of long and complete commercial sequences. The first stage is achieved by frame hashing. Different from the related studies that depend on brute-force pairwise matching, we propose applying a second-stage hashing algorithm for the recurring segment assemblage, which is the key idea in this paper. A large-scale archive containing a 10-hour and a 1-month stream was used for the experimentation. The algorithm mined commercials from the 1-month stream in less than 50 minutes, which was ten times faster than that of related studies, with a 98.05% sequence-level and 97.39% frame-level accuracy. We demonstrate the performance consistency of the algorithm on both audio and video streams, and investigate the computational cost from both the theoretical and experimental viewpoints.	algorithm;archive;brute-force search;computational complexity theory;frame language;hash function;mined;recurrence relation;streaming media	Xiaomeng Wu;Shin'ichi Satoh	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946948	computer science;cryptography;theoretical computer science;mathematics;multimedia;world wide web;statistics;robustness	DB	38.51594482271577	-52.12251298511164	4830
213b8bb7c58ae29e16938dce85bfa6299d61d3c9	duality between the watershed by image foresting transform and the fuzzy connectedness segmentation approaches	minimisation;graph theory;graph based transform;image foresting transform;image segmentation;duality mathematics;similarities maximization;efficient algorithm;duality;image gradient values duality image foresting transform fuzzy connectedness segmentation image segmentation watershed approach graph based transform optimal forest computation similarities maximization dissimilarities minimization;fuzzy set theory;dissimilarities minimization;transforms duality mathematics fuzzy set theory graph theory image colour analysis image segmentation minimisation;image colour analysis;transforms;image segmentation robustness optimization methods fuzzy set theory image processing image analysis filtering theory floods surface morphology surface topography;fuzzy connectedness segmentation;watershed approach;image gradient values;optimal forest computation	This paper makes a rereading of two successful image segmentation approaches, the fuzzy connectedness (FC) and the watershed (WS) approaches, by analyzing both by means of the image foresting transform (IFT). This graph-based transform provides a sound framework for analyzing and implementing these methods. This paradigm allows to show the duality existing between the WS by IFT and the FC segmentation approaches. Both can be modeled by an optimal forest computation in a dual form (maximization of the similarities or minimization of the dissimilarities), the main difference being the input parameters: the weights associated to each arc of the graph representing the image. In the WS approach, such weights are based on the (possibly filtered) image gradient values whereas they are based on much more complex affinity values in the FC theory. An efficient algorithm for both FC and IFT-WS computation is proposed. Segmentation robustness issue is also discussed	affinity analysis;computation;computer graphics;disjoint-set data structure;expectation–maximization algorithm;fuzzy set;image gradient;image processing;image segmentation;preprocessor;processor affinity;programming paradigm;robustness (computer science);transcutaneous electrical nerve stimulation;watershed (image processing);zone of the enders: the 2nd runner	Romaric Audigier;Roberto de Alencar Lotufo	2006	2006 19th Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2006.14	mathematical optimization;combinatorics;discrete mathematics;mathematics;scale-space segmentation	Vision	44.901266587716364	-70.42836853016843	4839
38d8ff137ff753f04689e6b76119a44588e143f3	when 3d-aided 2d face recognition meets deep learning: an extended ur2d for pose-invariant face recognition		Most of the face recognition works focus on specific modules or demonstrate a research idea. This paper presents a pose-invariant 3D-aided 2D face recognition system (UR2D) that is robust to pose variations as large as 90◦ by leveraging deep learning technology. The architecture and the interface of UR2D are described, and each module is introduced in detail. Extensive experiments are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset and 3% on the IJB-A dataset on average in face identification tasks. UR2D also achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing the Rank-1 accuracy score from template matching. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques.	deep learning;experiment;facial recognition system;pattern recognition;template matching	Xiang Jun Xu;Pengfei Dou;Ha A. Le;Ioannis A. Kakadiaris	2017	CoRR		computer science;pattern recognition;artificial intelligence;face detection;face recognition grand challenge;deep learning;architecture;facial recognition system;template matching;software;three-dimensional face recognition	Vision	29.004740390373595	-53.35855907402622	4842
9e1aa5ee031daa3d6b8bff21e7a9ec3300df02f8	robust reconstruction of mrsi data using a sparse spectral model and high resolution mri priors	minimisation;ell _ 1 minimization;integrated approach;spectroscopy;fat leakage;constrained total variation optimization scheme;unified algorithm;spikes;high resolution;magnetic fields;high resolution mri;magnetic field;image resolution;phantoms;forward model;classical sequential data processing schemes;robustness magnetic resonance imaging image reconstruction data processing magnetic resonance spectroscopy high resolution imaging magnetic fields polynomials image resolution;fat water boundary;3d estimate;baseline components;prior information;data processing;high resolution imaging;total variation b_ 0 inhomogeneity compensation ell _ 1 minimization fat leakage field map magnetic resonance spectroscopic imaging mrsi sparsity;sparse linear combination;spectrum;sparse spectral model;indexing terms;polynomials;constrained model;phantom;human subjects;significant field inhomogeneity;spectral leakage artifact minimization;sparsity;magnetic field inhomogeneity map;algorithms biopolymers brain humans image enhancement image interpretation computer assisted magnetic resonance imaging magnetic resonance spectroscopy pattern recognition automated reproducibility of results sensitivity and specificity subtraction technique;magnetic resonance;image reconstruction;medical image processing;magnetic resonance spectroscopic imaging mrsi;magnetic resonance imaging;high field map variations;metabolite components;robustness;magnetic resonance spectroscopic imaging;total variation;field map;robust reconstruction;image postprocessing steps;spectral analysis biomedical mri image reconstruction medical image processing minimisation phantoms;b_ 0 inhomogeneity compensation;spectral quality;spectral analysis	We introduce a novel algorithm to address the challenges in magnetic resonance (MR) spectroscopic imaging. In contrast to classical sequential data processing schemes, the proposed method combines the reconstruction and postprocessing steps into a unified algorithm. This integrated approach enables us to inject a range of prior information into the data processing scheme, thus constraining the reconstructions. We use high resolution, 3-D estimate of the magnetic field inhomogeneity map to generate an accurate forward model, while a high resolution estimate of the fat/water boundary is used to minimize spectral leakage artifacts. We parameterize the spectrum at each voxel as a sparse linear combination of spikes and polynomials to capture the metabolite and baseline components, respectively. The constrained model makes the problem better conditioned in regions with significant field inhomogeneity, thus enabling the recovery even in regions with high field map variations. To exploit the high resolution MR information, we formulate the problem as an anatomically constrained total variation optimization scheme on a grid with the same spacing as the magnetic resonance imaging data. We analyze the performance of the proposed scheme using phantom and human subjects. Quantitative and qualitative comparisons indicate a significant improvement in spectral quality and lower leakage artifacts.	algorithm;baseline (configuration management);computational human phantom;distortion;image resolution;magnetic fields;magnetic resonance imaging;mathematical optimization;morphologic artifacts;phantom reference;phantoms, imaging;platelet glycoprotein 4, human;polynomial;protein truncation abnormality;sparse matrix;spectral leakage;voxel;magnetic resonance spectroscopic imaging;quantitative	Ramin Eslami;Mathews Jacob	2010	IEEE Transactions on Medical Imaging	10.1109/TMI.2010.2046673	computer vision;radiology;image resolution;medicine;magnetic field;computer science;magnetic resonance imaging;mathematics;nuclear magnetic resonance	ML	47.612382016641014	-82.118476201279	4855
557681441ce4f87a2bd8ecc225eda6e9c460dc79	information theory of dna sequencing		DNA sequencing is the basic workhorse of modern day biology and medicine. Shotgun sequencing is the dominant technique used: many randomly located short fragments called reads are extracted from the DNA sequence, and these reads are assembled to reconstruct the original sequence. A basic question is: given a sequencing technology and the statistics of the DNA sequence, what is the minimum number of reads required for reliable reconstruction? This number provides a fundamental limit to the performance of any assembly algorithm. By drawing an analogy between the DNA se-quencing problem and the classic communication problem, we formulate this question in terms of an information theoretic notion of sequencing capacity. This is the asymp-totic ratio of the length of the DNA sequence to the minimum number of reads required to reconstruct it reliably. We compute the sequencing capacity explicitly for a simple statistical model of the DNA sequence and the read process. Using this framework, we also study the impact of noise in the read process on the sequencing capacity.	algorithm;information theory;randomness;statistical model	Abolfazl Seyed Motahari;Guy Bresler;David Tse	2012	CoRR		paired-end tag;bioinformatics;k-mer;dna sequencing theory;sequence assembly;hybrid genome assembly;algorithm	Comp.	0.3275605796568831	-53.0338615073927	4869
32acfb717aa3359197241530de5435165f31014c	a hierarchical visual model for video object summarization	labeling semisupervised learning graphical models object detection streaming media internet books layout surveillance;modelizacion;video object;temporal continuity;histograms;hierarchical system;object recognition;clutter;deteccion de objetos;linguistique;semisupervised learning hierarchical visual model video object summarization frame level labeling window feature descriptor temporal continuity patch level model video clip;document analysis;patch level model;window feature descriptor;supervised learning;video signal processing;multiple instance learning;analisis forma;localization;systeme hierarchise;resumen;reconnaissance objet;prior knowledge;intelligence artificielle;localizacion;semi supervised learning;probabilistic graphical model;computer vision;hierarchical visual model;modelisation;reseau bayes;detection objet;sistema jerarquizado;visualization;analyse documentaire;video signal processing image sequences learning artificial intelligence object detection;computational modeling;linguistica;fouillis echo;localisation;senal video;signal video;visual modeling;red bayes;resume;feature extraction;confusion eco;bayes network;video signal;analisis documental;artificial intelligence;video object summarization topic model probabilistic graphical model multiple instance learning semi supervised learning object detection;reconocimiento de objetos;pattern analysis;inteligencia artificial;apprentissage supervise;learning artificial intelligence;aprendizaje supervisado;abstract;modeling;video object summarization;frame level labeling;topic model;semisupervised learning;analyse forme;object detection;image sequences;video clip;linguistics	We propose a novel method for removing irrelevant frames from a video given user-provided frame-level labeling for a very small number of frames. We first hypothesize a number of windows which possibly contain the object of interest, and then determine which window(s) truly contain the object of interest. Our method enjoys several favorable properties. First, compared to approaches where a single descriptor is used to describe a whole frame, each window's feature descriptor has the chance of genuinely describing the object of interest; hence it is less affected by background clutter. Second, by considering the temporal continuity of a video instead of treating frames as independent, we can hypothesize the location of the windows more accurately. Third, by infusing prior knowledge into the patch-level model, we can precisely follow the trajectory of the object of interest. This allows us to largely reduce the number of windows and hence reduce the chance of overfitting the data during learning. We demonstrate the effectiveness of the method by comparing it to several other semi-supervised learning approaches on challenging video clips.	arabic numeral 0;clutter;computer performance;curve fitting;data logger;estimated;experiment;frame (physical object);microsoft windows;mixture model;normal statistical distribution;ocean observatories initiative;overfitting;population parameter;relevance;sampling - surgical action;scott continuity;semi-supervised learning;semiconductor industry;supervised learning;tracking system;video clip;visual descriptor	David Liu;Gang Hua;Tsuhan Chen	2010	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2010.31	computer vision;systems modeling;visualization;internationalization and localization;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;video tracking;pattern recognition;bayesian network;histogram;clutter;hierarchical control system;topic model;supervised learning;computational model	Vision	46.52010220976024	-57.52796808755266	4870
730b9d38cfc94327814f9b448721a99a7039d55f	automatic hotspots detection for intracellular calcium analysis in fluorescence microscopic videos		In recent years, life-cell imaging techniques and their software applications have become powerful tools to investigate complex biological mechanisms such as calcium signalling. In this paper, we propose an automated framework to detect areas inside cells that show changes in their calcium concentration i.e. the regions of interests or hotspots, based on videos taken after loading living mouse cardiomyocytes with fluorescent calcium reporter dyes. The proposed system allows an objective and efficient analysis through the following four key stages: 1) Pre-processing to enhance video quality, 2) First level segmentation to detect candidate hotspots based on adaptive thresholding on the frame level, 3) Second-level segmentation to fuse and identify the best hotspots from the entire video by proposing the concept of calcium fluorescence hit-ratio, and 4) Extraction of the changes of calcium fluorescence over time per hotspot. From the extracted signals, different measurements are calculated such as maximum peak amplitude, area under the curve, peak frequency, and inter-spike interval of calcium changes. The system was tested using calcium imaging data collected from Heart muscle cells. The paper argues that the automated proposal offers biologists a tool to speed up the processing time and mitigate the consequences of inter-intra observer variability.		David Traore;Katja Rietdorf;Nasser Al-Jawad;Hisham Al-Assam	2017		10.1007/978-3-319-60964-5_75	fluorescence microscope;calcium in biology;video quality;analytical chemistry;calcium signaling;computer vision;calcium;calcium imaging;thresholding;artificial intelligence;computer science	SE	38.99989298185731	-73.41289140156451	4874
345146349956fa3856ad370a51eba4cf4cb1931c	electronical and data-processing components of a modular flow-cytometer for use in microbiological research	data processing			Simon Lange	1996			bioinformatics;modular design;biology;data processing	Logic	0.11794152379869487	-64.69047517642159	4877
19e779b3a12b85c74eb53939f11ba4b0122e311e	nucleosome positioning of intronless genes in the human genome	biological patents;biomedical journals;text mining;dna bioinformatics computational biology genomics rna in vitro ieee transactions;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;nucleosome occupancy epigenetics intron containing genes intronless genes;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Nucleosomes, the basic units of chromatin, are involved in transcription regulation and DNA replication. Intronless genes, which constitute 3 percent of the human genome, differ from intron-containing genes in evolution and function. Our analysis reveals that nucleosome positioning shows a distinct pattern in intronless and intron-containing genes. The nucleosome occupancy upstream of transcription start sites of intronless genes is lower than that of intron-containing genes. In contrast, high occupancy and well positioned nucleosomes are observed along the gene body of intronless genes, which is perfectly consistent with the barrier nucleosome model. Intronless genes have a significantly lower expression level than intron-containing genes and most of them are not expressed in CD4+ T cell lines and GM12878 cell lines, which results from their tissue specificity. However, the highly expressed genes are at the same expression level between the two types of genes. The highly expressed intronless genes require a higher density of RNA Pol II in an elongating state to compensate for the lack of introns. Additionally, 5’ and 3’ nucleosome depleted regions of highly expressed intronless genes are deeper than those of highly expressed intron-containing genes.	dna replication;introns;leukemia, b-cell;nucleosomes;sensitivity and specificity;transcription (software);transcription initiation site;transcriptional regulation;nucleosome location;nucleosome positioning;pol genes	Xiangfei Cheng;Yue Hou;Yumin Nie;Yiru Zhang;Huan Huang;Hongde Liu;Xiao Sun	2018	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2476811	biology;text mining;medical research;computer science;bioinformatics;genetics	Comp.	5.90229832082398	-63.43572190076424	4893
889c457489dbdf350e50b1b4319e3a037b98bfca	lie group modeling of nonlinear point set shape variability	natural images;statistical model;linear model;empirical model;lie group;vector field	  Linear statistical models of shape variability of identifiable point sets have previously been described and applied successfully  to the empirical modeling of appearance variability in natural images. One of the limitations of these linear models has been  demonstrated in the nonlinear “bending” shape variability of point sets where a length ratio is constant.      We point out that modeling point set variability with groups of transformations generated by linear vector fields constitute  an algebraic frame for modeling simple nonlinear point set variability suitable for the modeling of shape variability. As  an example, the very simple “bending” shape variability of three points in the complex plane is in this way generated by a  linear vector field described by a complex 3 × 3 matrix.      	heart rate variability	Niels Holm Olsen;Mads Nielsen	2000		10.1007/10722492_20	mathematical analysis;discrete mathematics;topology;mathematics	Vision	47.44135390873016	-52.94094065500857	4923
82508cb5283d53c677ce3b0b9ac355f3a311c835	determining anxiety in obsessive compulsive disorder through behavioural clustering and variations in repetition intensity	anxiety analysis;behaviour classification;compulsive behaviour;obsessive compulsive disorder;surf	BACKGROUND AND OBJECTIVES Over the last decade, the application of computer vision techniques to the analysis of behavioural patterns has seen a considerable increase in research interest. One such interesting and recent application is the visual behavioural analysis of mental disorders. Despite the very recent surge in interest in this area, relatively little has been done thus far to assist individuals living with Obsessive Compulsive Disorder. The work proposed herein represents a proof of concept system designed to demonstrate the efficacy of such an approach, from the computational perspective. The specific focus of this work lies in demonstrating a mechanism for clustering different kinds of Obsessive Compulsive Disorder behaviours and then comparing new behaviours to existing behaviours to determine the approximate level of anxiety represented by a compulsive behaviour.   METHODS The proposed system uses Temporal Motion Heat Maps, SURF descriptors, a visual bag of words model and SVM-based classification to categorise representations of various behaviours commonly seen in OCD. Moreover, we apply a set of statistical measures to the images in a given category in order to derive an approximate anxiety level for a given compulsive behaviour. This proof of concept is an essential step in the direction of integrating computational approaches into the treatment of psychiatric conditions such as Obsessive Compulsive Disorder, for more effective recovery.   RESULTS Results gleaned from experimental simulations indicate that the proposed system is capable of correctly classifying different types of simulated Obsessive Compulsive Disorder behaviour classes 75% of the time, with the misclassifications almost exclusively occurring when two behavioural clusters appear highly similar. Based on this information the proposed system is then able to assign an approximate behavioural anxiety level to the compulsive behaviours that meets the approval of a mental health professional.   CONCLUSIONS The proposed system demonstrates a good ability to categorise various types of simulated OCD behaviour, in addition to establishing an approximate anxiety level for a given compulsive behaviour. This research demonstrates strong potential for the use of such systems in assisting mental health professionals looking to better understand their patients' condition and treatment progress across time.	anxiety disorders;approximation algorithm;attention deficit hyperactivity disorder;bag-of-words model;behavioral pattern;bipolar disorder;categorization;class;classification;cluster analysis;computation;computer vision;mental disorders;obsessive-compulsive disorder;patients;simulation;support vector machine;mental health;statistical cluster	Conor Cameron;Ibrahim Khalil;David Castle	2018	Computer methods and programs in biomedicine	10.1016/j.cmpb.2018.03.019	statistics;proof of concept;mental health;bag-of-words model;compulsive behaviour;cognitive psychology;cluster analysis;computer science;anxiety;behavioural analysis	ML	26.64392561076737	-75.68325562538966	4929
82c89d545ef68a1e8396a30dd272e20b3364ad6f	online learning from local features for video-based face recognition	reconnaissance visage;traitement signal;learning process;on line processing;teleenseignement;ucsd;image processing;learning;video based face recognition;biometrie;localization;biometrics;database;biometria;procesamiento imagen;base dato;localizacion;online learning;traitement image;similitude;algorithme;aprendizaje;tratamiento en linea;algorithm;large scale;apprentissage;automatic recognition;face recognition;localisation;clustering;voting;local features;feature extraction;signal processing;e learning;signal classification;similarity;base de donnees;pattern recognition;classification signal;teleensenanza;voto;reconnaissance forme;similitud;extraction caracteristique;traitement en ligne;vote;classification automatique;reconocimiento patron;remote teaching;automatic classification;procesamiento senal;clasificacion automatica;false accept rate;reconocimiento automatico;reconnaissance automatique;algoritmo	This paper presents an online learning approach to video-based face recognition that does not make any assumptions about the pose, expressions or prior localization of facial landmarks. Learning is performed online while the subject is imaged and gives near realtime feedback on the learning status. Face images are automatically clustered based on the similarity of their local features. The learning process continues until the clusters have a required minimum number of faces and the distance of the farthest face from its cluster mean is below a threshold. A voting algorithm is employed to pick the representative features of each cluster. Local features are extracted from arbitrary keypoints on faces as opposed to pre-defined landmarks and the algorithm is inherently robust to large scale pose variations and occlusions. During recognition, video frames of a probe are sequentially matched to the clusters of all individuals in the gallery and its identity is decided on the basis of best temporally cohesive cluster matches. Online experiments (using live video) were performed on a database of 50 enrolled subjects and another 22 unseen impostors. The proposed algorithm achieved a recognition rate of 97.8% and a verification rate of 100% at a false accept rate of 0.0014. For comparison, experiments were also performed using the Honda/UCSD database and 99.5% recognition rate was achieved.	facial recognition system	Ajmal S. Mian	2011	Pattern Recognition	10.1016/j.patcog.2010.12.001	computer vision;speech recognition;similarity;internationalization and localization;voting;image processing;feature extraction;computer science;artificial intelligence;similitude;machine learning;signal processing;pattern recognition;cluster analysis;biometrics;electoral-vote.com	Vision	45.26464967955286	-58.58381059604615	4939
e84f569bdd6908b5c7fd014f505075e98952bb47	semantic subgroup explanations	data mining;semantic data mining;subgroup discovery;ontologies;microarray data	Subgroup discovery (SD) methods can be used to find interesting subsets of objects of a given class. While subgroup describing rules are themselves good explanations of the subgroups, domain ontologies can provide additional descriptions to data and alternative explanations of the constructed rules. Such explanations in terms of higher level ontology concepts have the potential of providing new insights into the domain of investigation. We show that this additional explanatory power can be ensured by using recently developed semantic SD methods. We present a new approach to explaining subgroups through ontologies and demonstrate its utility on a motivational use case and on a gene expression profiling use case where groups of patients, identified through SD in terms of gene expression, are further explained through concepts from the Gene Ontology and KEGG orthology. We qualitatively compare the methodology with the supporting factors technique for characterizing subgroups. The developed tools are implemented within a new browser-based data mining platform ClowdFlows.	cluster analysis;data mining;experiment;gene expression profiling;gene ontology;high- and low-level;internet access;kegg;microarray;ontology (information science);real life;sensor;web application	Anze Vavpetic;Vid Podpecan;Nada Lavrac	2013	Journal of Intelligent Information Systems	10.1007/s10844-013-0292-1	bioinformatics;data mining;database	ML	6.304558568538671	-55.13835350091374	4947
3c50199cb745b9da4ea6fdf39eb5962aebe3721b	apnea-hypopnea index estimation from spectral analysis of airflow recordings	female humans linear models male polysomnography sleep apnea syndromes;sleep apnea feature extraction estimation accuracy training correlation;medical disorders;sleep;electrocardiography;spectral analysis electrocardiography electroencephalography medical disorders medical signal processing physiological models regression analysis sleep;regression analysis;electroencephalography;spectral analysis;electroencephalogram apnea hypopnea index estimation spectral analysis airflow recording analysis sleep apnea hypopnea syndrome diagnosis af data multiple linear regression model training group mlr models frequency band spectral features correlation pearsons coefficient ani decision thresholds automated estimation ecg eeg electrocardiogram;physiological models;medical signal processing	This study focuses on the analysis of airflow (AF) recordings to help in sleep apnea-hypopnea syndrome (SAHS) diagnosis. The objective is to estimate the apnea-hypopnea index (AHI) by means of spectral features from AF data. Multiple linear regression (MLR) was used for this purpose. A training group is used to obtain two MLR models: the first one consisting of features obtained from the full PSDs (MLRfull) and the second one consisting of features from a new frequency band of interest (MLRband). Then a test group is used to validate the final model. The correlation of spectral features and MLR models with AHI was compared by means of Pearson's coefficient (ρ). MLRband reached the highest ρ (0.809). Four different AHI decision thresholds were used to evaluate MLRband ability to distinguish the severity of SAHS. The accuracy achieved was higher as the threshold increased (69.7%, 75.3%, 80.9%, 87.6%) These results suggest that the automated estimation of AHI through spectral features can provide useful knowledge about SAHS severity.	anisotropic filtering;coefficient;frequency band;learning to rank;linear iga bullous dermatosis;multiple congenital anomalies;sleep apnea syndromes;sleep apnea, obstructive;sleep mode;wolff-parkinson-white syndrome;airflow	Gonzalo C. Gutiérrez-Tobal;Roberto Hornero;Daniel Álvarez;J. Víctor Marcos;Carlos Gómez;Félix del Campo	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346706	neuroscience;speech recognition;medicine;electroencephalography;sleep;anesthesia;regression analysis;statistics	Vision	17.12162393344799	-87.77895041849801	4961
a2cd94d27029835a06fa808227e08c529bbe63a3	evolutionary rate variation and rna secondary structure prediction	hiv;evolution molecular;evolutionary history;base pairing;rna secondary structure;databases nucleic acid;nucleotides;evolutionary rate;hiv 1;limit set;random noise;single stranded;models genetic;rna;secondary structure;substitution rate;point mutation;nucleic acid conformation;algorithms;sequence alignment;rna viral;rna secondary structure prediction;rna ribosomal;kinetics;prediction;rna transfer;base pair;evolution	Predicting RNA secondary structure using evolutionary history can be carried out by using an alignment of related RNA sequences with conserved structure. Accurately determining evolutionary substitution rates for base pairs and single stranded nucleotides is a concern for methods based on this type of approach. Determining these rates can be hard to do reliably without a large and accurate initial alignment, which ideally also has structural annotation. Hence, one must often apply rates extracted from other RNA families with trusted alignments and structures. Here, we investigate this problem by applying rates derived from tRNA and rRNA to the prediction of the much more rapidly evolving 5'-region of HIV-1. We find that the HIV-1 prediction is in agreement with experimental data, even though the relative evolutionary rate between A and G is significantly increased, both in stem and loop regions. In addition we obtained an alignment of the 5' HIV-1 region that is more consistent with the structure than that currently in the database. We added randomized noise to the original values of the rates to investigate the stability of predictions to rate matrix deviations. We find that changes within a fairly large range still produce reliable predictions and conclude that using rates from a limited set of RNA sequences is valid over a broader range of sequences.		Bjarne Knudsen;Ebbe Sloth Andersen;Christian K. Damgaard;Jørgen Kjems;Jan Gorodkin	2004	Computational biology and chemistry	10.1016/j.compbiolchem.2004.04.001	biology;base pair;bioinformatics;genetics	Comp.	4.249580496657034	-61.31980506306068	4994
cbb1d6893dedf07bf54071f1c33d9fdc4b75b183	use of cell morphology as an early bio-sensor for viral infection	single cell cell morphology bio sensor viral infection;time 24 hour cell morphology early biosensor cellular morphology vesicular stomatitis virus infection vsv infection time lapse approach homologous cell single cell analysis spindle shape mature cell cell proliferation round shape cell viral infection model;diseases;biomedical optical imaging;microorganisms;cellular biophysics;optical microscopy;fluorescence spectroscopy;proteins sociology statistics morphology shape fluorescence genetics;optical microscopy biomedical optical imaging cellular biophysics diseases fluorescence spectroscopy microorganisms	This paper reports a correlation between cellular morphology and the ability of adapting Vesicular stomatitis virus (VSV) infection. A time-lapse approach was employed to track the individual difference between homologous cells in adopting viral infection. Our single-cell analysis indicates that upon viral infection, mature cells that are in spindle shape are less likely to be infected after 24 hour infection. On the other hand, cells undergoing proliferation, which are in rounder shape, tend to adopt much higher viral infection within the same amount of time. This fact suggests cellular morphology may to be an early bio-sensor for viral infection. The findings in this paper could potentially be applied to other viral infection models.	24-hour clock;british informatics olympiad;cell (microprocessor);galaxy morphological classification;hard disk drive;mathematical morphology	Xianting Ding;Ningxia Liu;K. Matsuo;Mingzhu Sun;Chih-Ming Ho;Xin Zhao	2013	The 8th Annual IEEE International Conference on Nano/Micro Engineered and Molecular Systems	10.1109/NEMS.2013.6559928	fluorescence spectroscopy;optical microscope;microorganism;physics	Visualization	4.799185717632678	-63.93130127554224	5001
02c5b80efe6ab373acc346ac609437e21d3f3d36	affective–associative two-process theory: a neurocomputational account of partial reinforcement extinction effects	affect;associative two-process theory;decision making;partial reinforcement;reinforcement learning	The partial reinforcement extinction effect (PREE) is an experimentally established phenomenon: behavioural response to a given stimulus is more persistent when previously inconsistently rewarded than when consistently rewarded. This phenomenon is, however, controversial in animal/human learning theory. Contradictory findings exist regarding when the PREE occurs. One body of research has found a within-subjects PREE, while another has found a within-subjects reversed PREE (RPREE). These opposing findings constitute what is considered the most important problem of PREE for theoreticians to explain. Here, we provide a neurocomputational account of the PREE, which helps to reconcile these seemingly contradictory findings of within-subjects experimental conditions. The performance of our model demonstrates how omission expectancy, learned according to low probability reward, comes to control response choice following discontinuation of reward presentation (extinction). We find that a PREE will occur when multiple responses become controlled by omission expectation in extinction, but not when only one omission-mediated response is available. Our model exploits the affective states of reward acquisition and reward omission expectancy in order to differentially classify stimuli and differentially mediate response choice. We demonstrate that stimulus–response (retrospective) and stimulus–expectation–response (prospective) routes are required to provide a necessary and sufficient explanation of the PREE versus RPREE data and that Omission representation is key for explaining the nonlinear nature of extinction data.	experiment;extinction, psychological;nonlinear system;prospective search	Robert Lowe;Alexander Almer;Erik Billing;Yulia Sandamirskaya;Christian Balkenius	2017		10.1007/s00422-017-0730-1	extinction;affect (psychology);reinforcement learning;social psychology;associative property;process theory;reinforcement;expectancy theory;psychology;phenomenon	ML	14.31129699754112	-75.49649619376325	5017
9ab52d2b7c6289816052e89aee083c1ce3115a65	electrical flow algorithms for total variation minimization	maximum flow;total variation minimization;convex programming;efficient algorithm;image restoration;pattern recognition;total variation	The total variation (TV) minimization framework is a very popular method for a wide variety of image restoration problems. This framework comes in two variants: anisotropic, where the “smoothness” of the denoised image is measured by L1-difference of neighboring pixel; and isotropic, where the measure of “smoothness” is based on computing localized L2-differences and thus is rotationally invariant. There was a lot of work on obtaining efficient algorithms for computingTV denoising. Most of this effort was focused on anisotropic variant as it was possible to exploit its connection to the maximum flow problem. In case of the isotropic variant, this connection no longer holds and the algorithms in this context rely on convex programming techniques, which results in much slower running time. In this paper we develop an approach to TV minimization that is based on computing electrical flows and builds upon the framework introduced in [CKM + 11]. This approach encompasses in a natural way both variants of TV minimization and obtains running times for both versions that are essentially the same. On an image with n pixels and m neighboring relations, our algorithm produces a solution that’s within 1 + ǫ of the optimum solution in time ˜	algorithm	Aleksander Madry;Gary L. Miller;Richard Peng	2011	CoRR		mathematical optimization;machine learning;pattern recognition;total variation denoising	EDA	52.675371598836186	-72.47051130668213	5031
b2d4e6287ca8a19c2eddb71add36ff97278f544c	stochastic inference with spiking neurons in the high-conductance state		The highly variable dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference but stand in apparent contrast to the deterministic response of neurons measured in vitro. Based on a propagation of the membrane autocorrelation across spike bursts, we provide an analytical derivation of the neural activation function that holds for a large parameter space, including the high-conductance state. On this basis, we show how an ensemble of leaky integrate-and-fire neurons with conductance-based synapses embedded in a spiking environment can attain the correct firing statistics for sampling from a well-defined target distribution. For recurrent networks, we examine convergence toward stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This points to a new computational role of high-conductance states and establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level.	activation function;autocorrelation;biological neuron model;computation;computer simulation;conductance (graph);convergence (action);embedded system;embedding;graphical model;graphical user interface;in vitro [publication type];inference;neurons;population parameter;recurrent neural network;sampling (signal processing);software propagation;spiking neural network;stationary process;stochastic process;synapses;tissue membrane;video-in video-out	Mihai A. Petrovici;Johannes Bill;Ilja Bytschok;Johannes Schemmel;Karlheinz Meier	2016	Physical review. E	10.1103/PhysRevE.94.042312	theoretical computer science;mathematics;statistics	ML	20.558680375453125	-72.88405287987035	5032
271dddbd07c9fd6a1b6b5579dd551b0c88cf0850	learning high-order mrf priors of color images	monochromators;markov random fields;curse of dimensionality;markov random field;conference paper;learning systems;mathematical models;image denoising;markov processes;keywords color image processing;fields of experts model;color image	In this paper, we use large neighborhood Markov random fields to learn rich prior models of color images. Our approach extends the monochromatic Fields of Experts model (Roth & Black, 2005a) to color images. In the Fields of Experts model, the curse of dimensionality due to very large clique sizes is circumvented by parameterizing the potential functions according to a product of experts. We introduce simplifications to the original approach by Roth and Black which allow us to cope with the increased clique size (typically 3x3x3 or 5x5x3 pixels) of color images. Experimental results are presented for image denoising which evidence improvements over state-of-the-art monochromatic image priors.	curse of dimensionality;grayscale;markov chain;markov random field;monochrome;noise reduction;pixel;product of experts;realms of the haunting	Julian J. McAuley;Tibério S. Caetano;Alexander J. Smola;Matthias O. Franz	2006		10.1145/1143844.1143922	computer vision;curse of dimensionality;color image;computer science;machine learning;pattern recognition;mathematical model;mathematics;markov process;statistics	ML	52.15343085777678	-68.7979397533916	5033
832a86d51a996be132619e04eeb80b051f0e1531	detailed somatotopy in primary motor and somatosensory cortex revealed by gaussian population receptive fields	high-field fmri;motor cortex;population receptive fields;sensorimotor integration;somatosensory cortex;somatotopy	The relevance of human primary motor cortex (M1) for motor actions has long been established. However, it is still unknown how motor actions are represented, and whether M1 contains an ordered somatotopy at the mesoscopic level. In the current study we show that a detailed within-limb somatotopy can be obtained in M1 during finger movements using Gaussian population Receptive Field (pRF) models. Similar organizations were also obtained for primary somatosensory cortex (S1), showing that individual finger representations are interconnected throughout sensorimotor cortex. The current study additionally estimates receptive field sizes of neuronal populations, showing differences between finger digit representations, between M1 and S1, and additionally between finger digit flexion and extension. Using the Gaussian pRF approach, the detailed somatotopic organization of M1 can be obtained including underlying characteristics, allowing for the in-depth investigation of cortical motor representation and sensorimotor integration.	cerebral cortex;digit structure;estimated;mesoscopic physics;movement;normal statistical distribution;population;receptive aphasia (finding);relevance;somatosensory cortex;primary motor cortex	Wouter Schellekens;Natalia Petridou;Nick F. Ramsey	2018	NeuroImage	10.1016/j.neuroimage.2018.06.062	cognitive psychology;somatosensory system;psychology;neuroscience;primary motor cortex;receptive field;cortex (botany);motor cortex;population;cognitive neuroscience;numerical digit	ML	19.129072186788868	-76.8817090080538	5038
6f15a093497b60f65038dc399b39ee973f713459	definition and performance evaluation of a robust svm based fall detection solution	support vector machines;feature extraction support vector machines robustness cameras protocols error analysis training;wavelet transform support vector machine based classifier svm fall detection solution home environment features extraction human body silhouette tracking projection histograms fourier transform;wavelet transforms;feature extraction;fourier transforms;wavelet transforms feature extraction fourier transforms support vector machines	We propose an automatic approach to detect falls in home environment. A Support Vector Machine based classifier is fed by a set of selected features extracted from human body silhouette tracking. The classifier is followed by filtering operations taking into account the temporal nature of a video. The features are based on height and width of human body bounding box, the user's trajectory with her/his orientation, Projection Histograms and moments of order 0, 1 and 2. We study several combinations of usual transformations of the features (Fourier Transform, Wavelet transform, first and second derivatives), and we show experimentally that it is possible to achieve high performance using a single camera.We evaluated the robustness of our method using a realistic dataset. Experiments show that the best tradeoff between classification performance and time processing result is obtained combining the original data with their first derivative. The global error rate is lower than 1%, and the recall, specificity and precision are high (respectively 0.98, 0.996 and 0.942). The resulting system can therefore be used in a real environment. Hence, we also evaluated the robustness of our system regarding location changes. We proposed a realistic and pragmatic protocol which enables performance to be improved by updating the training in the current location, with normal activities records.	experiment;minimum bounding box;performance evaluation;sensitivity and specificity;support vector machine;wavelet transform	Imen Charfi;Johel Mitéran;Julien Dubois;Mohamed Atri;Rached Tourki	2012	2012 Eighth International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2012.155	wavelet;margin classifier;fourier transform;support vector machine;computer vision;continuous wavelet transform;feature extraction;computer science;machine learning;pattern recognition;discrete wavelet transform;wavelet transform	Robotics	30.97833647299062	-59.72534132977397	5058
66acc3ff6f098e7f43e1f1ca7a9badc92e167f2b	an ensemble of k-nearest neighbours algorithm for detection of parkinson's disease	ensemble classification;rotation forest;learning algorithms;parkinson s disease;feature selection;k nearest neighbour	Parkinson's disease is a disease of the central nervous system that leads to severe difficulties in motor functions. Developing computational tools for recognition of Parkinson's disease at the early stages is very desirable for alleviating the symptoms. In this paper, we developed a discriminative model based on a selected feature subset and applied several classifier algorithms in the context of disease detection. All classifier performances from the point of both stand-alone and rotation-forest ensemble approach were evaluated on a Parkinson's disease data-set according to a blind testing protocol. The new method compared to hitherto methods outperforms the state-of-the-art in terms of both predictions of accuracy (98.46%) and area under receiver operating characteristic curve (0.99) scores applying rotation-forest ensemble k-nearest neighbour classifier algorithm.	k-nearest neighbors algorithm	Murat Gök	2015	Int. J. Systems Science	10.1080/00207721.2013.809613	computer science;artificial intelligence;machine learning;pattern recognition;feature selection	Logic	14.351995104594772	-90.24741821441415	5077
0d2c479df83784f90573dec1b30a75d273712fde	2d to 3d convertion based on edge defocus and segmentation	wavelet analysis;color segmentation 2d to 3d conversion depth map wavelet lipschitz exponent;estimation theory;edge enhancement;2d to 3d image conversion;image segmentation;edge detection;lipschitz regularity;indexing terms;depth estimation method;three dimensional;edge defocussing;wavelet transforms;color segmentation;edge segmentation;lipschitz regularity 2d to 3d image conversion edge defocussing edge segmentation depth estimation method two dimensional wavelet analysis;2d to 3d conversion;wavelet transforms edge detection estimation theory image segmentation;lipschitz exponent;two dimensional wavelet analysis;depth estimation;wavelet analysis focusing image analysis image segmentation frequency cameras image converters rendering computer graphics computer vision stereo vision;depth map;wavelet	This paper presents a depth estimation method which converts two-dimensional images into three-dimensional data. Based on two-dimensional wavelet analysis of Lipschitz regularity for defocus estimation on edges, this method can effectively eliminate the horizontal stripes in the depth map resulted from traditional one-dimensional wavelet based approaches. Besides, we also propose several techniques such as edge enhancement, color-based segmentation, and depth optimization to obtain a more reliable and smoother depth map. The experimental results demonstrate the effectiveness of our proposed techniques.	3d rendering;depth map;edge enhancement;interaction;mathematical optimization;stripes;wavelet	Ge Guo;Nan Zhang;Longshe Huo;Wen Gao	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518076	wavelet;computer vision;mathematical optimization;computer science;pattern recognition;mathematics;statistics	Robotics	48.998551747081045	-66.83876433723647	5084
d511087ddd8688cb63f8be99f9be51faaa74d89b	automatic optimization of depth electrode trajectory planning		This paper presents a fully automatic procedure for optimization of depth electrode implantation planning in epilepsy. To record intracranial EEG in some patients with intractable epilepsy, depth electrodes are implanted through holes in the skull. The proposed fully automatic procedure maximizes recording coverage of the target volume by estimating the EEG recorded from each contact, while minimizing the risk of approaching vessels and other critical structures. All structures, including the hippocampus and amygdala were automatically segmented. We retrospectively validated the procedure for mesial temporal lobe implantations in 11 hemispheres. The automatic trajectories recorded from a larger volume of interest than the original manually selected trajectories while better avoiding the segmented structures. The procedure is integrated into a neuronavigation system enabling the surgeon to visualize the selected trajectories from an ordered list and, if necessary, enables re-planning a trajectory in near real time.		Rina Zelmann;Silvain Bériault;Kelvin Mok;Claire Haegelen;Jeffery A. Hall;G. Bruce Pike;Andre Olivier;D. Louis Collins	2013		10.1007/978-3-319-05666-1_13	computer vision	Robotics	38.97274426102648	-84.08631093190358	5105
4a4a312d4aa1265afc8706102ed8588294db37cb	probflow: joint optical flow and uncertainty estimation		Optical flow estimation remains challenging due to untextured areas, motion boundaries, occlusions, and more. Thus, the estimated flow is not equally reliable across the image. To that end, post-hoc confidence measures have been introduced to assess the per-pixel reliability of the flow. We overcome the artificial separation of optical flow and confidence estimation by introducing a method that jointly predicts optical flow and its underlying uncertainty. Starting from common energy-based formulations, we rely on the corresponding posterior distribution of the flow given the images. We derive a variational inference scheme based on mean field, which incorporates best practices from energy minimization. An uncertainty measure is obtained along the flow at every pixel as the (marginal) entropy of the variational distribution. We demonstrate the flexibility of our probabilistic approach by applying it to two different energies and on two benchmarks. We not only obtain flow results that are competitive with the underlying energy minimization approach, but also a reliable uncertainty measure that significantly outperforms existing post-hoc approaches.	benchmark (computing);best practice;calculus of variations;energy minimization;hoc (programming language);marginal model;optical flow;pixel;variational principle	Anne S. Wannenwetsch;Margret Keuper;Stefan Roth	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.133	measurement uncertainty;artificial intelligence;pattern recognition;probabilistic logic;mean field theory;adaptive optics;inference;energy minimization;computer science;posterior probability;optical flow	Vision	51.62682272317844	-72.59205710204508	5120
901a70616aa68cd8a8b4ccf48a6c3ced7a702d44	a neural net model of the neuropsychology of spelling processes	neural net;pattern matching;liveness;deadlock;minimal;petri net;strongly connected;trap;complete;neural network	The neural network (or 'brain-style') approach to computing is useful for developing systems to perform tasks humans traditionally do well and that computers do not. Neural network systems are particularly suited for pattern matching and categorization applications such as recognizing visual or auditory data.	artificial neural network;categorization;computer;pattern matching	Brent Auernheimer;Alison Butler	1989		10.1145/75427.1030255	complete;nervous system network models;computer science;artificial intelligence;theoretical computer science;deadlock;machine learning;pattern matching;time delay neural network;programming language;trap;petri net;strongly connected component;artificial neural network;liveness	ML	-3.9881697290119713	-74.82106019467379	5128
5cedafac2f92b3ff364e2417b50fb7a7641d9d45	a novel approach of facial expression recognition based on shearlet transform		A novel facial expression recognition method based on discrete shearlet transform is proposed in this paper, which is a new image multiscale geometric analysis method. In addition to multi-resolution and time-frequency localization owned by wavelet transform, the shearlet transform also has anisotropy and directionality. Firstly, normalization and equalization are applied to all test and training images. Then the facial expression features are extracted based on discrete shearlet transform, and Vector Support Machine is used to classify the seven expressions (happiness, sadness, surprise, disgust, fear, anger and neural) of JAFFE database. Experimental results show that the recognition rate of the proposed method is improved comparing to several known methods.	multiscale geometric analysis;sadness;shearlet;wavelet transform	Yang Lu;Shigang Wang;Wenting Zhao;Yan Zhao;Jian Wei	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8308672	wavelet transform;support vector machine;normalization (statistics);expression (mathematics);multiscale geometric analysis;facial expression;shearlet;mathematics;artificial intelligence;pattern recognition	Vision	33.80598376557429	-59.87687992739901	5131
6acaed1b80565e18971c034a45f938d439d7a70f	incorporation of inhaled insulin into the fda accepted university of virginia/padova type 1 diabetes simulator	postprandial glucose dynamics type 1 diabetes simulator artificial pancreas control algorithm design control algorithm testing subcutaneous insulin administration postprandial glucose control pharmacokinetic model technosphere insulin t1dm simulator;insulin sugar data models diabetes plasmas mathematical model absorption;sugar biochemistry diseases dosimetry drug delivery systems	The University of Virginia/Padova Type 1 Diabetes (T1DM) Simulator has been extensively used in artificial pancreas research mostly for testing and design of control algorithms. However, it also offers the possibility of testing new insulin analogs and alternative routes of delivery given that subcutaneous insulin administration present significant delays & variability. Inhaled insulin appears an important candidate to improve post-prandial glucose control given its rapid appearance in plasma. In this contribution, we present the results of incorporating a pharmacokinetic model of inhaled Technosphere® Insulin (TI) into the T1DM simulator. In particular, we successfully reproduced in silico the post-prandial glucose control observed in T1DM subjects treated with TI given at meal time, and the post-prandial glucose dynamics in response to different timing of TI dose.	algorithm;analog;diabetes mellitus;diabetes mellitus, non-insulin-dependent;glucose;heart rate variability;inspiration function;insulin lispro;pancreas, artificial;plasma active;simulators;ti-nspire series;united states food and drug administration	Roberto Visentin;Thomas Klabunde;Marshall Grant;Chiara Dalla Man;Claudio Cobelli	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319085	endocrinology;engineering;biological engineering;diabetes mellitus	Robotics	9.812836048717818	-71.36208475260614	5132
42b9c844fdbff3e8f05af4f0ea6b44a67f24fd82	a novel two stage evaluation methodology for word segmentation techniques	art;distance computation stage;handwriting recognition;image segmentation;word segmentation technique;computational intelligence;image classification;text analysis;telecommunication computing;euclidean distance;gap classification stage;word segmentation;accuracy;evaluation methodology;image segmentation document image processing handwritten character recognition image classification;detection rate;distance metric;document image processing;robustness;historical typewritten document set;image analysis;evaluation;informatics;text recognition;two stage evaluation methodology word segmentation technique handwritten character recognition distance computation stage gap classification stage historical typewritten document set;character recognition;handwritten character recognition;two stage evaluation methodology;character recognition image segmentation text analysis informatics telecommunication computing robustness computational intelligence laboratories image analysis art;evaluation framework;word segmentation evaluation	Word segmentation is a critical stage towards word and character recognition as well as word spotting and mainly concerns two basic aspects, distance computation and gap classification. In this paper, we propose a robust evaluation methodology that treats the distance computation and the gap classification stages independently. The detection rate calculated for every distance metric corresponds to the maximum detection rate that we could have achieved if we had a perfect classifier for the gap classification stage. The proposed evaluation framework has been applied to several state-of-the-art techniques using a handwritten as well as a historical typewritten document set. The best combination of distance metric computation and gap classification state-of-the-art techniques is proposed.	computation;optical character recognition;text segmentation	Georgios Louloudis;Nikolaos Stamatopoulos;Basilios Gatos	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.219	text segmentation;computer vision;contextual image classification;image analysis;speech recognition;metric;word error rate;computer science;evaluation;machine learning;computational intelligence;pattern recognition;euclidean distance;accuracy and precision;handwriting recognition;image segmentation;informatics;robustness	Vision	32.111803718980134	-65.97030341291696	5138
dc877a2c93370f03d308d8d3d85ba2d5e8452492	two-phase deep convolutional neural network for reducing class skewness in histopathological images based breast cancer detection	mitosis count;histopathology;class imbalance;deep learning;convolutional neural networks;breast cancer	Different types of breast cancer are affecting lives of women across the world. Common types include Ductal carcinoma in situ (DCIS), Invasive ductal carcinoma (IDC), Tubular carcinoma, Medullary carcinoma, and Invasive lobular carcinoma (ILC). While detecting cancer, one important factor is mitotic count - showing how rapidly the cells are dividing. But the class imbalance problem, due to the small number of mitotic nuclei in comparison to the overwhelming number of non-mitotic nuclei, affects the performance of classification models. This work presents a two-phase model to mitigate the class biasness issue while classifying mitotic and non-mitotic nuclei in breast cancer histopathology images through a deep convolutional neural network (CNN). First, nuclei are segmented out using blue ratio and global binary thresholding. In Phase-1 a CNN is then trained on the segmented out 80×80 pixel patches based on a standard dataset. Hard non-mitotic examples are identified and augmented; mitotic examples are oversampled by rotation and flipping; whereas non-mitotic examples are undersampled by blue ratio histogram based k-means clustering. Based on this information from Phase-1, the dataset is modified for Phase-2 in order to reduce the effects of class imbalance. The proposed CNN architecture and data balancing technique yielded an F-measure of 0.79, and outperformed all the methods relying on specific handcrafted features, as well as those using a combination of handcrafted and CNN-generated features.		Noorul Wahab;Asifullah Khan;Yeon Soo Lee	2017	Computers in biology and medicine	10.1016/j.compbiomed.2017.04.012	pathology;computer science;bioinformatics;breast cancer;machine learning;deep learning;histopathology	ML	33.471170259346714	-75.8619133620545	5143
8b927391ce8396c8d8afac19ff14027dcc018544	characterizing pairwise contact patterns in human contact networks	pairwise contact process;human contact networks;markov modulated poisson process	We use a counting process representation of the pairwise contact process to analyze pairwise contact patterns. Studying two real-world traces, we find that the pairwise contact patterns have three characteristics. First, human contact patterns are influenced by daily and weekly cycles of activity. Second short time intervals with intensive contact event (bursts) are separated by long periods with few contact events. Third, the pairwise contact process exhibits long range dependence. We introduce a Markov modulated Poisson process (MMPP) as a flexible model for pairwise contact process exhibiting both regular structure and irregular bursts of activity. Using standard statistical techniques, we demonstrate that the proposed model is consistent with the empirical data. Our work has significant implication for mobility modeling and performance analysis in human contact networks.		Lin-Tao Yang;Hao Jiang;Sai Wang;Lin Wang;Yuan Fang	2012	Ad Hoc Networks	10.1016/j.adhoc.2011.09.008	simulation	Robotics	21.411693241683583	-74.99316688228834	5200
4929d90eba3382c34f6a8e7484cdb9c6687891dd	a content-based retrieval of mammographic masses using the curvelet descriptor	databases;computer aided diagnosis;mammography;content based image retrieval	Computer-aided diagnosis (CAD) that uses content based image retrieval (CBIR) strategies has became an important research area. This paper presents a retrieval strategy that automatically recovers mammography masses from a virtual repository of mammographies. Unlike other approaches, we do not attempt to segment masses but instead we characterize the regions previously selected by an expert. These regions are firstly curvelet transformed and further characterized by approximating the marginal curvelet subband distribution with a generalized gaussian density (GGD). The content based retrieval strategy searches similar regions in a database using the Kullback-Leibler divergence as the similarity measure between distributions. The effectiveness of the proposed descriptor was assessed by comparing the automatically assigned label with a ground truth available in the DDSM database.1 A total of 380 masses with different shapes, sizes and margins were used for evaluation, resulting in a mean average precision rate of 89.3% and recall rate of 75.2% for the retrieval task.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	curvelet	Fabián Narváez;Gloria Díaz;Francisco Gómez;Eduardo Romero	2012		10.1117/12.911680	computer vision;computer science;data mining;information retrieval	Vision	33.19057807449258	-71.73300628384705	5203
25a38c61e6760add5071b37b11d50faa97ae9f78	investigation on the cutaneous/proprioceptive contribution to the force sensation induced by electrical stimulation above tendon		A method to present force sensation based on electrical stimulation to the tendon has been suggested, and the occurrence of the sensation was considered due to the contribution of proprioceptors such as Golgi tendon organs. However, there was no clear evidence about the contributing receptors and because the method uses percutaneous electrical stimulation, there are other candidates, the cutaneous receptors. In this paper, we conducted experiments to determine whether the force sensation generated by this method is due to cutaneous sensation or proprioception, by changing the effective depth of electrical stimulation with electrodes spacing. As a result, it was shown that when the electrical stimulation could reach to deep tissue receptors, the force sensation was felt clearer, suggesting possible contribution of the proprioceptor.	experiment;functional electrical stimulation	Akifumi Takahashi;Kenta Tanabe;Hiroyuki Kajimoto	2018		10.1145/3281505.3283402	cutaneous receptor;neuroscience;computer vision;computer science;tendon;proprioception;sensation;golgi tendon organ;golgi apparatus;artificial intelligence;stimulation;receptor	Robotics	24.788397661121365	-82.91737302268052	5214
6ede97306544fcff05c2f27982286b26b5a53f2a	texture analysis based on gaussian mixture modeling	image texture expectation maximisation algorithm gaussian processes image recognition;analytical models;image recognition;gaussian mixture;gaussian processes;gaussian mixtures;texture modeling;image texture analysis parameter estimation image analysis maximum likelihood estimation image texture gaussian distribution artificial intelligence computer science electronic mail euclidean distance;euclidean distance;data mining;image texture;manganese;statistical properties;texture analysis;gaussian mixture model;computational modeling;maximum likelihood estimate;image color analysis;model parameters texture analysis gaussian mixture modeling image textures modeling digital image processing expectation maximization algorithm data set weighted normalized euclidean distance measure;pixel;expectation maximization algorithm;mixture of gaussians;texture discrimination;algorithm design and analysis;texture discrimination texture analysis gaussian mixtures expectation maximization algorithm texture modeling;expectation maximisation algorithm	Gaussian mixture modeling is a recent approach in texture analysis and is used to model image textures. Texture is modeled using a mixture of Gaussian distributions, which capture the local statistical properties of the texture. The mixture parameters are estimated using Expectation Maximization algorithm. This algorithm finds the maximum likelihood estimate of the parameters of an underlying distribution from a given data set when data is incomplete. The paper presents a method of identifying changes as well as new patterns in the image using the Gaussian mixture model parameters. Model parameters of the original image texture are computed. Unexpected patterns in the image are discriminated by using weighted normalized Euclidean distance measure derived from the model parameters.	bio-inspired computing;euclidean distance;expectation–maximization algorithm;image texture;microsoft windows;mixture model	T. Sobha;S. Remya	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393701	computer science;machine learning;pattern recognition;mixture model;mathematics;statistics	Vision	49.74580635686596	-68.9786085950829	5215
efa84629386e717f41b1ea49fbe2eebb358e94bf	using functional pca for cardiac motion exploration	motion analysis;spaces of continuous functions;image motion analysis;image processing;cardiology;bounded domain functional principal component analysis mri cardiac motion analysis multivariate data analysis karhunen loeve decomposition image processing functional data curves variability multifunctional data continuous functions;medical image processing principal component analysis biomedical mri cardiology image motion analysis;multivariate data analysis;karhunen loeve;medical image processing;principal component analysis;bounded domain;functional data;principal component analysis random variables graphics image processing thyristors hilbert space information analysis space charge meteorology symmetric matrices;biomedical mri	Principal componenr analysis (PCAJ 114, 61 is a main rool in mulrivariare darn analysis. Irs paradigms are also used in rhe Karhunen-Lueve decomposirion 151. a srandard rool in image processing. Exrensions of PCA ro rhe framework of funcrional dara have been pmposed. The onalysis pmvided by rhe funcrional PCA seems ro be a powerjid rool roJindprincipal sources of variability in curves or images, bur i r fails in pmviding us wirh easy inrerprerarions in rhe case of mulrifuncrional dara. Guide lines aiming ar spor informarion fmm the ourpurs of PCA applied IO funcrionals wirh values in spare of conrinuous funcrions upon a bounded domain are proposed. An applicarion ro cardiac morion analysis illuswares rhe complextry of the mulrifuncrional framework and rhe results provided byfuncrional	ar (unix);heart rate variability;image processing	Denis Clot	2002		10.1109/ICDM.2002.1183890	computer vision;sparse pca;image processing;computer science;machine learning;pattern recognition;mathematics;multivariate analysis;multiple correspondence analysis;karhunen–loève theorem;statistics;principal component analysis	ML	48.92615980473343	-73.36729649904227	5225
e53967831ee33b67533af79c50824279a854a2fc	a markovian-based approach for daily living activities recognition	hierarchical and hidden markov model;home by room activities language;smart home;elderly person;activities;scenarios;prediction	Recognizing activities of daily living plays an important role in healthcare. It is necessary to use an adapted model to simulate the human behavior in a domestic space to monitor the patient harmonically and to intervene in the necessary time. In this paper we tackle this problem using the hierarchical hidden Markov model for representing and recognizing complex indoor activities, we propose a new grammar “Home By Room Activities language” to facilitate the complexity of human scenarios and hold us account to the abnormal activities.	algorithm;artificial neural network;differential evolution;gene ontology term enrichment;hierarchical hidden markov model;home automation;matlab;markov chain;simulation;smart environment	Zaineb Liouane;Tayeb Lemlouma;Philippe Roose;Frédéric Weis;Hassani Messaoud	2016		10.5220/0005809502140219	simulation;prediction;artificial intelligence;statistics	HCI	2.285992670664244	-86.32689554015924	5235
13dd9e1f37068200386302b02d3aaa364405c939	a robust home alone faint detection based on wireless sensor networks		Target detection and tracking are one of the fundamental problems for wireless sensor networks and play an important role in the safety field. Faint detection is an important problem for the elderly people or patients or even pregnant women. It has wide application in current society. This paper proposed amethod to collect information about the behavior and position of faint event in the sensing environment. This method detects and tracks faint person by combining Kalman filter and Camshift tracking algorithm. Experiments showed that the method yields good detection and tracking performance in complex environments.		Zhenhai Wang;Bo Xu	2015	IJDSN	10.1155/2015/534980	computer vision;simulation	Mobile	6.094162314931841	-85.21235537855492	5261
49f276e1b8fd162ac3cd996becc63cab2b2535b7	trained 3d models for cnn based object recognition		We present a method for 3D object recognition in 2D images which uses 3D models as the only source of the training data. Our method is particularly useful when a 3D CAD object or a scan needs to be identified in a catalogue form a given query image; where we significantly cut down the overhead of manual labeling. We take virtual snapshots of the available 3D models by a computer graphics pipeline and fine-tune existing pretrained CNN models for our object categories. Experiments show that our method performs better than the existing local-feature based recognition system in terms of recognition recall.	3d modeling;3d single-object recognition;computer graphics;computer-aided design;experiment;graphics pipeline;object detection;outline of object recognition;overhead (computing);robotics;unbiased rendering;word lists by frequency	Kripasindhu Sarkar;Kiran Varanasi;Didier Stricker	2017		10.5220/0006272901300137	computer vision;machine learning	Vision	29.917825554858602	-52.47895722504193	5269
b1db74061a2d10242392a8ca525269b8475d1f16	a single tri-axial accelerometer-based real-time personal life log system capable of human activity recognition and exercise information generation	exercise information;personal life log;accelerometer;activity recognition	Recording a personal life log (PLL) of daily activities in a ubiquitous environment is an emerging application of information technology. In this work, we present a single tri-axial accelerometer-based PLL system capable of human activity recognition and exercise information generation. Our PLL system exhibits two main functions: activity recognition and exercise information generation. For activity recognition, the system first recognizes a state of daily activities based on the statistical and spectral features of the accelerometer signals. An activity within the recognized state is then recognized using a set of augmented features, including autoregressive coefficients, signal magnitude area, and tilt angle, via linear discriminant analysis and hierarchical artificial neural networks. Upon the recognition of each activity, the system further estimates exercise information that includes energy expenditure based on metabolic equivalents, stride length, step count, walking distance, and walking speed. Our PLL system operates in real-time, and the life log information it generates is archived in a daily log database. We have validated our PLL system for six daily activities (i.e., lying, standing, walking, going-upstairs, going-downstairs, and driving) via subject-independent and subject-dependent recognition on a total of twenty subjects, achieving an average recognition accuracy of 94.43 and 96.61%, respectively. Our results demonstrate the feasibility of a portable real-time PLL system that could be used for u-lifecare and u-healthcare services in the near future.	activity recognition;archive;artificial neural network;autoregressive model;coefficient;linear discriminant analysis;phase-locked loop;real-time clock;real-time transcription;signal magnitude area;triangular function	Myong-Woo Lee;Adil Mehmood Khan;Tae-Seong Kim	2011	Personal and Ubiquitous Computing	10.1007/s00779-011-0403-3	embedded system;simulation;computer science;operating system;accelerometer;activity recognition	HCI	11.038877000128583	-89.23381067451633	5386
e038dd1c63840fa572f3d257acce05c6f1f2f9b2	learning variability of image feature appearance using statistical methods	analisis imagen;robot movil;image features;variabilidad;vision ordenador;vision robot;analisis estadistico;image processing;map building;mobile robot;image matching;localization;procesamiento imagen;statistical method;localizacion;aprendizaje probabilidades;probabilistic approach;traitement image;computer vision;statistical learning;localisation;robot vision;statistical analysis;robot mobile;enfoque probabilista;approche probabiliste;analyse statistique;pattern recognition;apprentissage probabilites;image analysis;vision ordinateur;vision artificielle;reconnaissance forme;variability;reconocimiento patron;artificial vision;variabilite;analyse image;appariement image;moving robot;probability learning;vision artificial	Motivated by the problems of vision-based mobile robot map building and localization, in this work, we show that using statistical learning methods the performance of the standard descriptor based methodology for matching image features in a wide base line can be improved. First, we propose two kinds of descriptors for image features and two statistical learning methods. Later, we present a study of the performance of descriptors with and without the statistical learning methods. This work does not pretend to present an exhaustive description of the mentioned methods but to give a good idea the effectiveness of using statistical learning methods together with descriptors for matching image features in a wide base line.		Rodrigo Munguía;Antoni Grau-Saldes	2006		10.1007/11892755_74	mobile robot;computer vision;image analysis;simulation;internationalization and localization;image processing;computer science;artificial intelligence;feature	Robotics	47.2407194553249	-58.871696147162325	5394
3cb6f5f826aae73760d11faa7770b3e241e903f9	a framework for ecg morphology features recognition	cluster algorithm;template reduction ecg morphology features recognition diagnosis feature cardiologists professional medical textbooks ecg classification 1 nearest neighbor with dynamic time warping time series matching algorithm ecg segments template selection;ecg segments;classification algorithm;feature recognition;professional medical textbooks;cardiology;training;template selection;time series;time series cardiology electrocardiography medical signal processing signal classification;accuracy;morphology;1 nearest neighbor with dynamic time warping;electrocardiography;ecg morphology features recognition;time series analysis;diagnosis feature;morphology electrocardiography accuracy clustering algorithms classification algorithms training time series analysis;nearest neighbor;signal classification;classification algorithms;clustering algorithms;template reduction;dynamic time warping;medical signal processing;cardiologists;ecg classification;time series matching algorithm	ECG morphology features, as a kind of significant diagnosis feature, are widely used by experienced cardiologists and highlighted in professional medical textbooks. Fail to utilize it should be one of the most important reasons for the underperformance of automatic ECG classification. In this paper, a framework for ECG morphology features recognition is presented. 1-nearest-neighbor with dynamic time warping (1NN-DTW), a strong time series matching algorithm, is used to compare the ECG segments with the templates store in the system. Template selection and reduction is applied to accelerate the classifier and cut down the templates volume as well. With the help of new proposed template reduction algorithm, our system has an accuracy of 90.71% by using a small portion of the original template set.	algorithm;dynamic time warping;mathematical morphology;statistical classification;time series	Jia-wei Zhang;Xiaojuan Hu;Xia Liu;Jun Dong	2010	2010 IEEE 23rd International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2010.6042619	speech recognition;morphology;computer science;machine learning;time series;pattern recognition;statistics	Arch	16.266688803811174	-89.02694442499582	5399
3f1e4a0313e33eb9d2188b3b9450f6694b7a7e50	disambiguating visual motion through contextual feedback modulation	modelo dinamico;estimacion sesgada;image features;traitement signal;ajustamiento modelo;neurone;calcul neuronal;neural computation;tecnologia electronica telecomunicaciones;computacion informatica;competition;feedforward;time course;signal sampling;top down;62m20;dynamic model;grupo de excelencia;feedback modulation;aperture problem;natural images;boucle anticipation;activity pattern;visual motion;ajustement modele;neurona;ciclo anticipacion;prediction theory;ciencias basicas y experimentales;signal processing;model matching;modele dynamique;image sequence;modulation retroaction;modele appariement;secuencia imagen;area mt;reseau neuronal;tecnologias;theorie prediction;grupo a;procesamiento senal;28xx;biased estimation;estimation biaisee;red neuronal;computacion neuronal;neuron;matching model;sequence image;competencia;neural network	Motion of an extended boundary can be measured locally by neurons only orthogonal to its orientation (aperture problem) while this ambiguity is resolved for localized image features, such as corners or nonocclusion junctions. The integration of local motion signals sampled along the outline of a moving form reveals the object velocity. We propose a new model of V1-MT feedforward and feedback processing in which localized V1 motion signals are integrated along the feedforward path by model MT cells. Top-down feedback from MT cells in turn emphasizes model V1 motion activities of matching velocity by excitatory modulation and thus realizes an attentional gating mechanism. The model dynamics implement a guided filling-in process to disambiguate motion signals through biased on-center, off-surround competition. Our model makes predictions concerning the time course of cells in area MT and V1 and the disambiguation process of activity patterns in these areas and serves as a means to link physiological mechanisms with perceptual behavior. We further demonstrate that our model also successfully processes natural image sequences.	feedforward neural network;matching;modulation;open bite;sampling - surgical action;velocity (software development);word-sense disambiguation	Pierre Bayerl;Heiko Neumann	2004	Neural Computation	10.1162/0899766041732404	neuroscience;competition;motion perception;computer science;artificial intelligence;signal processing;top-down and bottom-up design;communication;feature;feed forward;artificial neural network;models of neural computation	Vision	21.25702428238573	-69.14635909289657	5412
767857a11fbc7a1493ec3a90dc8c9f32a321ed92	influence of rehearsal in an auditory memory model for audio feature estimation	feature estimation;dissonance;rehearsal;onset detection;auditory memory	Audio feature estimation involves measuring key characteristics from audio. This paper demonstrates the improvement of a feature estimation method using an auditory memory model with rehearsal included. The auditory memory model is achieved using onset detection to identify new audio components for insertion into the auditory memory, and an algorithm that then combines the characteristics of the current interval with those of the audio components in the auditory memory. The auditory memory model mimics the storing, retrieval and forgetting processes of the short-term memory, and in this work the rehearsal as well. The feature estimation using the auditory memory model has been successfully applied to the estimation of sensory dissonance, and the characteristics of the memory model has been shown to be of interest in music categorization. The rehearsal step in the memory model is an important step in the understanding of the model. In addition, the dissonance estimation correlation with human ratings is tested with or without including rehearsal in the auditory memory model.		Kristoffer Jensen	2013		10.1007/978-3-319-12976-1_35	computer vision;speech recognition;cognitive dissonance;echoic memory	ML	-2.260441069337272	-86.66569645369327	5430
5300d8919d833bdd615dcf3ac6063399f70283d4	building 3d sulcal models using local geometry	point location;statistical models;statistical model;cortex;journal;mr imaging;sulcal patterns;brain atlas;deformable template;iterative closest point algorithm	This paper presents a series of 3D statistical models of the cortical sulci. They are built from points located automatically over the sulcal fissures, and corresponded automatically using variants on the iterative closest point algorithm. The models are progressively improved by adding in more and more structural and configural information, and the final results are consistent with findings from other anatomical studies. The models can be used to locate and label anatomical features automatically in 3D MR images of the head, for analysis, visualisation, classification, and normalisation.		Angela Caunce;Christopher J. Taylor	2001	Medical image analysis	10.1016/S1361-8415(00)00033-5	statistical model;computer vision;machine learning;pattern recognition;mathematics;statistics	Vision	42.846315501417166	-77.90795946800942	5448
d048b4c17887cb4f2263bfd001ebcac4efc4123d	machine learning in the prediction of cardiac epicardial and mediastinal fat volumes	epicardial;quantification;adipose tissue;regression;cardiac fat segmentation;volume estimation;correlation;prediction;mediastinal	We propose a methodology to predict the cardiac epicardial and mediastinal fat volumes in computed tomography images using regression algorithms. The obtained results indicate that it is feasible to predict these fats with a high degree of correlation, thus alleviating the requirement for manual or automatic segmentation of both fat volumes. Instead, segmenting just one of them suffices, while the volume of the other may be predicted fairly precisely. The correlation coefficient obtained by the Rotation Forest algorithm using MLP Regressor for predicting the mediastinal fat based on the epicardial fat was 0.9876, with a relative absolute error of 14.4% and a root relative squared error of 15.7%. The best correlation coefficient obtained in the prediction of the epicardial fat based on the mediastinal was 0.9683 with a relative absolute error of 19.6% and a relative squared error of 24.9%. Moreover, we analysed the feasibility of using linear regressors, which provide an intuitive interpretation of the underlying approximations. In this case, the obtained correlation coefficient was 0.9534 for predicting the mediastinal fat based on the epicardial, with a relative absolute error of 31.6% and a root relative squared error of 30.1%. On the prediction of the epicardial fat based on the mediastinal fat, the correlation coefficient was 0.8531, with a relative absolute error of 50.43% and a root relative squared error of 52.06%. In summary, it is possible to speed up general medical analyses and some segmentation and quantification methods that are currently employed in the state-of-the-art by using this prediction approach, which consequently reduces costs and therefore enables preventive treatments that may lead to a reduction of health problems.	algorithm;approximation error;ct scan;coefficient;fat-restricted diet;fatty acid glycerol esters;file allocation table;machine learning;mediastinum;memory-level parallelism;platelet glycoprotein 4, human;quantitation;x-ray computed tomography;biologic segmentation	Érick Oliveira Rodrigues;V. H. A. Pinheiro;Panos Liatsis;Aura Conci	2017	Computers in biology and medicine	10.1016/j.compbiomed.2017.02.010	regression;prediction;pathology;adipose tissue;mathematics;nuclear medicine;correlation;surgery;statistics	ML	36.959977330882865	-82.09819750143853	5462
a77f98b8240bb9f49c9e0e9fd35768580f8aa5da	metamod: software for steady-state modelling and control analysis of metabolic pathways on the bbc microcomputer	metabolic pathway;steady state	METAMOD, a BBC microcomputer-based software package for steady-state modelling and control analysis of model metabolic pathways, is described, The package consists of two programs. METADEF allows the user to define the pathway in terms of reactions, rate equations and initial concentrations of metabolites. METACAL uses one of two algorithms to calculate the steady-state concentrations and fluxes. One algorithm uses the current ratio of production and consumption rates of variable metabolites to adjust iteratively their concentrations in such a way that they converge towards the steady state. The other algorithm solves the roots of the system equations by means of a quasi-Newtonian procedure. Control analysis allows the calculation of elasticity, control and response coefficients, by means of finite difference approximation. METAMOD is interactive and easy to use, and suitable for teaching and research purposes.		J. H. Hofmeyr;K. J. van der Merwe	1986	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/2.4.243	biology;metabolic pathway;simulation;computer science;steady state	SE	16.150835660951586	-59.86097122928167	5466
f5af284de1ed88b9c90d14cc88c5d33a6de9f7bf	parallel implementation of a track recognition system using hough transform	parallelisme;distributed system;algoritmo paralelo;systeme reparti;parallel algorithm;particle detector;image processing;scintillation camera;camara centelleografia;procesamiento imagen;high energy;physique haute energie;transformacion hough;traitement image;algorithme parallele;parallelism;sistema repartido;paralelismo;fisica alta energia;high energy physics;camera scintillation;traza particula;particle tracks;parallel machines;hough transformation;hough transform;detecteur particule;transformation hough;parallel implementation;reconstruction algorithm;detector particula;trace particule	The reconstruction of tracks left by particles in a scintillating fiber detector from a high energy experiment is discussed. The track reconstruction algorithm is based on the Hough transform and achieves an efficiency above 86%. The algorithm is implemented in a 16-node parallel machine using two parallelism approaches in order to speed up the application of the Hough transform, which is known to have large computational cost.		Augusto Cesar Heluy Dantas;José Manoel de Seixas;Felipe Maia Galvão França	2000		10.1007/3-540-44942-6_38	hough transform;computer vision;image processing;computer science;scale-invariant feature transform;computer graphics (images)	Vision	48.20667466318648	-57.74051012707352	5478
34cfa206f350e990bdae075a3907cba27d8570a8	lips contour detection and tracking using watershed region-based active contour model and modified $h_{\infty}$	lyapunov methods;lyapunov stability;watershed h_ infty filter active contour models acm lip contour detection and tracking lypunov stability theory;filtering;lip contour detection and tracking;lyapunov stability theory;image segmentation;lips feature points lips contour detection lips contour tracking watershed region based active contour model modified h filter watershed segmentation lyapunov stability theory;lypunov stability theory;modified h filter;watershed region based active contour model;lips contour detection;stability;lyapunov method;face recognition;active contour models acm;lips feature points;feature extraction;h2 filters;watershed segmentation;lips mathematical model feature extraction image segmentation filtering lyapunov methods educational institutions;mathematical model;lips;lips contour tracking;watershed;h_ infty filter;stability face recognition h 2 filters image segmentation lyapunov methods object detection;stability theory;h 2 filters;active contour model;object detection	In this paper, a region-based active contour model (ACM) with local information using watershed segmentation is proposed for lips contour detection. Compared to the ACM with global energy terms, the proposed system provides a more precise lips contour convergence under the circumstances where the lips are difficult to distinguish using global statistics. Furthermore, since the ACM is sensitive to the initial contour position, a modified H∞ based on Lyapunov stability theory is proposed to provide better tracking of the subsequent lips feature points as the ACM initialization. The integration of the proposed ACM and modified H∞ has revealed an improvement of the overall lips contour detection.	active contour model;algorithm;audio-visual speech recognition;contour line;lyapunov fractal;simulation;tracking system;watershed (image processing)	Siew Wen Chin;Kah Phooi Seng;Li-Minn Ang	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2180771	facial recognition system;computer vision;speech recognition;watershed;computer science;mathematics;statistics	Graphics	43.950975542643704	-65.21201537364414	5481
588041c603e5ce1cc8d3cfeae702a3439768ae0c	face recognition on partially occluded images using compressed sensing	compressed sensing;face recognition;partial occlusion	In this work we have built a face recognition system using a new method based on recent advances in compressed sensing theory. The authors propose a method for recognizing faces that is robust to certain types and levels of occlusion. They also present tests that allow to assess the incidence of the proposed method. Face detection and recognition are issues that are being widely studied due to the large number of applications they have. At present , we can find face recognition systems in social networking sites, photo management software and access control systems, to name a few. Face recognition presents several difficulties. The image of the human face can have large intra-subject variations (changes in the same individual) that make it difficult to develop a recognition system. Variations may arise, for example, from head position variation when taking the picture, differences in lighting, facial expression (laughter, anger, etc.), occlusion of parts of the face due to the use of accessories such as lenses, sunglasses and scarves, facial hair (mustache, beard, long hair, etc.), and changes in facial features due to aging. On the other hand, inter-subject variations (differences between individuals) can be very small between two people with similar traits, making correct identification difficult. Presently there are various methods of face recognition. However, when the image is occluded, those methods that extract global features (holistic features) (as Eigenfaces and Fisherfaces) cannot be applied. There are many approaches that deal with occlusion in face recognition. Among them we can mention the one proposed by Shermina and Vasudevan (2012) that propose block comparison and thresholding to detect occlusion in the query image and use Empirical Mode Decomposition (Huang et al., 1998) (EMD) to normalize facial expression. In Lang and Jing (2011) , Guillamet and Vitria (2002) and Lee and Seung (1999) the use of Non-negative Matrix Factor-ization is explored because the locality of this approach lends itself to deal with occlusions. In Chiang and Chen (2011) a occlusion resistant face recognition method is proposed in which the query image is first normalized to a common shape and then, its texture is reconstructed by using PCA for each specific person in the database , which in turn allows to identify the occluded pixels as the ones that are very different from the query image. While methods that use local features may not be affected by occlusion, (Wright et al., 2009) has shown …	access control;compressed sensing;control system;eigenface;entity–relationship model;experiment;face detection;facial recognition system;hidden surface determination;hilbert–huang transform;holism;incidence matrix;inter-protocol exploitation;jing;locality of reference;pixel;signal processing;thresholding (image processing)	Ariel Morelli Andrés;Sebastian Padovani;Mariano Tepper;Julio Jacobo-Berlles	2014	Pattern Recognition Letters	10.1016/j.patrec.2013.08.001	facial recognition system;computer vision;speech recognition;computer science;pattern recognition;three-dimensional face recognition;compressed sensing	Vision	32.75143670211037	-59.309874810711705	5494
3198ffd1c5f8a1db542fd00e87d4665d9da4fe49	a review of classification techniques of emg signals during isotonic and isometric contractions	isotonic contractions;isometric contractions;feature extractions;emg signals;classifications;probability density functions	In recent years, there has been major interest in the exposure to physical therapy during rehabilitation. Several publications have demonstrated its usefulness in clinical/medical and human machine interface (HMI) applications. An automated system will guide the user to perform the training during rehabilitation independently. Advances in engineering have extended electromyography (EMG) beyond the traditional diagnostic applications to also include applications in diverse areas such as movement analysis. This paper gives an overview of the numerous methods available to recognize motion patterns of EMG signals for both isotonic and isometric contractions. Various signal analysis methods are compared by illustrating their applicability in real-time settings. This paper will be of interest to researchers who would like to select the most appropriate methodology in classifying motion patterns, especially during different types of contractions. For feature extraction, the probability density function (PDF) of EMG signals will be the main interest of this study. Following that, a brief explanation of the different methods for pre-processing, feature extraction and classifying EMG signals will be compared in terms of their performance. The crux of this paper is to review the most recent developments and research studies related to the issues mentioned above.	assistive technology;classification;contraction mapping;electromyography;exercise, isometric;feature extraction;human–computer interaction;incontinentia pigmenti achromians;isometric contraction;isometric projection;isotonic regression;muscle;nondeterministic algorithm;pattern recognition;portable document format;preprocessor;real-time clock;review [publication type];signal processing;the quality of life;user interface	Nurhazimah Nazmi;Mohd Azizi Abdul Rahman;Shin-ichiroh Yamamoto;Siti Anom Ahmad;Hairi Zamzuri;Saiful Amri Mazlan	2016		10.3390/s16081304	probability density function;speech recognition;isometric exercise;engineering;artificial intelligence;isotonic;statistics	ML	14.967954659882388	-90.85490070746617	5501
23b5112f6b2e5f7515e0b15ea2d2e18300d02860	clustering-based color image segmentation using local maxima		Color﻿image﻿segmentation﻿has﻿contributed﻿significantly﻿to﻿image﻿analysis﻿and﻿retrieval﻿of﻿relevant﻿ images.﻿ Color﻿ image﻿ segmentation﻿ helps﻿ the﻿ end﻿ user﻿ subdivide﻿ user﻿ input﻿ images﻿ into﻿ unique﻿ homogenous﻿regions﻿of﻿similar﻿pixels,﻿based﻿on﻿pixel﻿property.﻿The﻿success﻿of﻿ image﻿analysis﻿ is﻿ largely﻿owing﻿to﻿the﻿reliability﻿of﻿segmentation.﻿The﻿automatic﻿segmentation﻿of﻿a﻿color﻿image﻿into﻿ accurate﻿regions﻿without﻿over-segmentation﻿is﻿a﻿tedious﻿task.﻿Our﻿paper﻿focuses﻿on﻿segmenting﻿color﻿ images﻿automatically﻿ into﻿multiple﻿ regions﻿accurately,﻿based﻿on﻿ the﻿ local﻿maxima﻿of﻿ the﻿GLCM﻿ texture﻿ property,﻿ with﻿ pixels﻿ spatially﻿ clustered﻿ into﻿ identical﻿ regions.﻿ A﻿ novel﻿ Clustering-based﻿ Image﻿Segmentation﻿using﻿Local﻿Maxima﻿(CBIS-LM)﻿method﻿is﻿presented.﻿Our﻿proposed﻿approach﻿ generates﻿reliable,﻿accurate﻿and﻿non-overlapping﻿multiple﻿regions﻿for﻿the﻿given﻿user﻿input﻿image.﻿The﻿ segmented﻿regions﻿can﻿be﻿automatically﻿annotated﻿with﻿distinct﻿labels﻿which,﻿in﻿turn,﻿help﻿retrieve﻿ relevant﻿images﻿based﻿on﻿image﻿semantics. KeywoRdS Color Image Segmentation, Fast K-Means, Fuzzy C-Means, Gray Level Co-Occurrence Matrix, Local Maxima, Median Centroid, Natural Images, Region Clustering	cluster analysis;co-occurrence matrix;color image;image segmentation;k-means clustering;maxima and minima	Kalaivani Anbarasan;S. Chitrakala	2018	IJIIT	10.4018/IJIIT.2018010103	data mining;scale-space segmentation;computer science;color histogram;cluster analysis;computer vision;color image;region growing;artificial intelligence;image segmentation;image texture;segmentation	Vision	42.32882705557962	-68.48385681281842	5545
8288bc5383cfcdb2c888faaebf170c7240b897f9	real-time affine invariant gesture recognition for led smart lighting control	human computer interaction;skin;light emitting diodes;gesture recognition;cameras	"""Gesture recognition has attracted extensive research interest in the field of human computer interaction. Realtime affine invariant gesture recognition is an important and challenging problem. This paper presents a robust affine view invariant gesture recognition system for realtime LED smart light control. As far as we know, this is the first time that gesture recognition has been applied for control LED smart light in realtime. Employing skin detection, hand blobs captured from a top view camera are first localized and aligned. Subsequently, SVM classifiers trained on HOG features and robust shape features are then utilized for gesture recognition. By accurately recognizing two types of gestures (“gesture 8"""" and a “5 finger gesture""""), a user is enabled to toggle lighting on/off efficiently and control light intensity on a continuous scale. In each case, gesture recognition is rotation- and translation-invariant. Extensive evaluations in an office setting demonstrate the effectiveness and robustness of the proposed gesture recognition algorithm. © (2015) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only."""	gesture recognition;lighting control system;real-time transcription;smart lighting	Xu Chen;Miao Liao;Xiao-Fan Feng	2015		10.1117/12.2077329	computer vision;gesture recognition;skin;sketch recognition;computer graphics (images);light-emitting diode	Vision	32.1014683540431	-58.77210415108792	5547
93addd2f257c06bbe44e6a7de186f27ec0ec1e7f	a non-label and enzyme-free sensitive detection method for thrombin based on simulation-assisted dna assembly	colorimetric;computation and simulation;fluorescence;isothermal cycling signal amplification;thrombin detection	Taking advantage of the high selectivity of aptamers and enzyme-free catalyzed hairpin assembly (CHA) amplification strategy, we herein describe a label-free and enzyme-free sensitive fluorescent and colorimetric strategy for thrombin detection in this paper. In the presence of target, the corresponding aptamer of the partial dsDNA probes will bind to the target and liberate the initiation strand, which is artfully designed as the “on” switch for hairpin assembly. Moreover, the displaced initiation strand partakes in a multi-cycle process and produces numerous G-quadruplexes, which have a remarkable enhancement in fluorescent/colorimetric signal from NMM (N-methyl-mesoporphyrin IX) and TMB (3,3′,5,5′-tetramethylbenzidine), respectively. The proposed amplification strategy for thrombin detection is of high sensitivity, down to 2.4 pM, and also achieves colorimetric signals that are able to be distinguished by naked eye. More importantly, the thermodynamics of interacting DNA strands used in our work, and the process of toehold strand displacement-driven assembly are simulated before biological testing, verifying the feasibility theoretically, and simplifying the subsequent actual experiments. Therefore, our approach and simulation have a certain potential application in biomarker detection and quantitatively monitor for disease diagnosis.	accessory;biological markers;conflict (psychology);dna;displacement mapping;enlargement procedure;experiment;instrumentation (attribute);interaction;manuscripts;methylmethacrylate;natural science disciplines;numerous;organism;reagents;selectivity (electronic);simulation;strand (programming language);transcription initiation;verification and validation;verifying specimen;aptamer	Yingying Zhang;Luhui Wang;Yanan Wang;Yafei Dong	2018		10.3390/s18072179	analytical chemistry;dna;engineering;thrombin;molecular biology;enzyme	EDA	10.940293788222538	-64.75233957924449	5563
6455c1f35825252af4dbe07cef65c1b639c33c53	detecting inconsistency in biological molecular databases using ontologies	tecnologia electronica telecomunicaciones;data integrity;heterogeneous databases;data preparation;biological molecular databases;data mining;integration;journal article;life sciences;measure;tecnologias;grupo a;biological database;inconsistency;ontology	The rapid growth of life science databases demands the fusion of knowledge from heterogeneous databases to answer complex biological questions. The discrepancies in nomenclature, various schemas and incompatible formats of biological databases, however, result in a significant lack of interoperability among databases. Therefore, data preparation is a key prerequisite for biological database mining. Integrating diverse biological molecular databases is an essential action to cope with the heterogeneity of biological databases and guarantee efficient data mining. However, the inconsistency in biological databases is a key issue for data integration. This paper proposes a framework to detect the inconsistency in biological databases using ontologies. A numeric estimate is provided to measure the inconsistency and identify those biological databases that are appropriate for further mining applications. This aids in enhancing the quality of databases and guaranteeing accurate and efficient mining of biological databases.	biological database;data mining;data pre-processing;experiment;interoperability;ontology (information science);semantic heterogeneity;sensor;structure mining;the australian	Qingfeng Chen;Yi-Ping Phoebe Chen;Chengqi Zhang	2007	Data Mining and Knowledge Discovery	10.1007/s10618-007-0071-0	database theory;biological database;measure;computer science;probabilistic database;ontology;data integrity;data mining;database;information retrieval	DB	-4.439985872487307	-63.78951513974732	5588
e643d2de7810bed263e21c29ea2a0f9bc6955761	symmetric stability of low level feature detectors	feature matching;journal article;feature detectors;bilateral symmetry	We investigate the capability of low level feature detectors to consistently define feature keypoints in an image and its horizontally reflected (mirrored) image. It is our assertion that this consistency is a useful attribute of a feature detector and should be considered in assessing the robustness of a feature detector. We test ten of the most popular detectors using a popular dataset of 8,677 images. Wedefine a set of errormeasurements to help us to understand the invariance in keypoint position, size and angle of orientation, and we use SIFT descriptors extracted from the keypoints to measure the consistency of extracted feature descriptors. We conclude that the FAST and CenSurE detectors are perfectly invariant to bilateral symmetry, Good Features to Track and the Harris Corner detector produce consistent keypoints that can be matched using feature descriptors, and others vary in their invariance. SIFT is the least invariant of all the detectors that we test.	algorithm;assertion (software development);bilateral filter;corner detection;feature (computer vision);feature detection (computer vision);feature detection (web development);feature model;feature vector;harris affine region detector;maximally stable extremal regions;scale-invariant feature transform;sensor;speeded up robust features	Craig Henderson;Ebroul Izquierdo	2016	Pattern Recognition Letters	10.1016/j.patrec.2016.03.027	computer vision;symmetry in biology;computer science;machine learning;pattern recognition;feature detection;mathematics;feature	Vision	40.88504090561366	-55.91211713975676	5589
57985ea70e62e89b77baca25739b14bed9d20cd6	broken symmetries in a location-invariant word recognition network	linear regression;word recognition;k means clustering	We studied the feedforward network proposed by Dandurand et al. (2010), which maps location-specific letter inputs to location-invariant word outputs, probing the hidden layer to determine the nature of the code. Hidden patterns for words were densely distributed, and K-means clustering on single letter patterns produced evidence that the network had formed semi-location-invariant letter representations during training. The possible confound with superseding bigram representations was ruled out, and linear regressions showed that any word pattern was well approximated by a linear combination of its constituent letter patterns. Emulating this code using overlapping holographic representations (Plate, 1995) uncovered a surprisingly acute and useful correspondence with the network, stemming from a broken symmetry in the connection weight matrix and related to the group-invariance theorem (Minsky & Papert, 1969). These results also explain how the network can reproduce relative and transposition priming effects found in humans.	approximation algorithm;arabic numeral 0;backpropagation;bigram;cdisc adas-cog - word recognition summary score;chou's invariance theorem;classification;cluster headache;cluster analysis;disease regression;distance;dworkin's game driver;emulator;feedforward neural network;holography;k-means clustering;map;maxima and minima;null value;numerator;ploidies;priming exercise;regression - mental defense mechanism;semiconductor industry;stemming;symmetry breaking;transposition table;visual word;weight;word association tests;statistical cluster	Thomas Hannagan;Frédéric Dandurand;Jonathan Grainger	2011	Neural Computation	10.1162/NECO_a_00064	speech recognition;word recognition;computer science;linear regression;machine learning;mathematics;algorithm;statistics;k-means clustering	ML	-2.7925009518107324	-73.2348859189231	5606
c5785ee65aaf5481096cbb51d604ef122f1231fd	in vivo mri assessment of knee cartilage in the medial meniscal tear model of osteoarthritis in rats	pilot study;longitudinal study;preclinical mri;surface registration;osteoarthritis;region of interest;cartilage segmentation;thickness statistics	We present a new approach for quantifying the degradation of knee cartilage in the medial meniscal tear (MMT) model of osteoarthritis in the rat. A statistical strategy was used to guide the selection of a region of interest (ROI) from the images obtained from a pilot study. We hypothesize that this strategy can be used to localize a region of cartilage most vulnerable to MMT-induced damage. In order to test this hypothesis, a longitudinal study was conducted in which knee cartilage thickness in a pre-selected ROI was monitored for three weeks and comparisons were made between MMT and control rats. We observed a significant decrease in cartilage thickness in MMT rats and a significant increase in cartilage thickness in sham-operated rats as early as one week post surgery when compared to pre-surgery measurements.		Zhiyong Xie;Serguei Liachenko;Ping-Chun Chiao;Santos Carvajal-Gonzalez;Susan Bove;Thomas Bocan	2010	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-15711-0_8	computer vision;radiology;medicine;computer science;anatomy;surgery;region of interest	Robotics	39.293008360784874	-82.1123807949584	5640
8be91e3b5ebd5ebf370ff6374d01a830901c7abb	psist: a scalable approach to indexing protein structures using suffix trees	sufijo;alignement;topology;industria farmaceutica;proteine;drug discovery;structure arborescente;suffix;analyse fonctionnelle;metodo arborescente;topologie;interrogation base donnee;biologia molecular;interrogacion base datos;bioinformatique;classification;geometric feature;industrie pharmaceutique;topologia;structure proteine;suffix tree;feature vector;protein structure;local structure;functional analysis;indexing;estructura arborescente;protein structure indexing;molecular biology;indexation;tree structure;protein classification;alineamiento;indizacion;tree structured method;proteina;methode arborescente;bioinformatica;suffixe;function prediction;pharmaceutical industry;classification accuracy;protein;database query;clasificacion;external suffix trees;alignment;structural similarity;local alignment;bioinformatics;biologie moleculaire;analisis funcional	6 Approaches for indexing proteins, and for fast and scalable searching for struc7 tures similar to a query structure have important applications such as protein struc8 ture and function prediction, protein classification and drug discovery. In this paper, 9 we develop a new method for extracting local structural (or geometric) features from 10 protein structures. These feature vectors are in turn converted into a set of symbols, 11 which are then indexed using a suffix tree. For a given query, the suffix tree index 12 can be used effectively to retrieve the maximal matches, which are then chained to 13 obtain the local alignments. Finally, similar proteins are retrieved by their align14 ment score against the query. Our results show classification accuracy up to 50% 15 and 92.9% at the topology and class level according to the CATH classification. 16 These results outperform the best previous methods. We also show that PSIST is 17 highly scalable due to the external suffix tree indexing approach it uses; it is able 18 to index about 70,500 domains from SCOP in under an hour. 19	cath;co-ment;feature vector;maximal set;query language;scalability;scop;suffix tree	Feng Gao;Mohammed J. Zaki	2008	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2007.07.008	functional analysis;generalized suffix tree;search engine indexing;protein structure;feature vector;biological classification;computer science;bioinformatics;structural similarity;smith–waterman algorithm;data mining;mathematics;compressed suffix array;tree structure;drug discovery;algorithm	Comp.	-3.739620793684214	-53.140596314005656	5644
c9b61e9d563fdf47f36fbf98fff997f89841c0a8	edge-supressed color clustering for image thresholding	traitement automatise;iterative method;remote control;karhunen loeve transformation;estimation mouvement;motion control;image segmentation;image processing;threshold detection;forme onde;indexation automatique;etude experimentale;edge detection;estimacion movimiento;algoritmo recursivo;procesamiento imagen;motion estimation;telecommande;segmentation;iterative algorithm;traitement image;image clustering;experimental result;deteccion contorno;metodo iterativo;commande mouvement;video indexing;detection contour;detection seuil;control movimiento;deteccion umbral;forma onda;algorithme recursif;internet;senal video;signal video;methode iterative;tratamiento automatizado;poursuite cible;automatic indexing;resultado experimental;visual control;video signal;controle visuel;k means algorithm;recursive algorithm;waveform;control remoto;classification automatique;transformation karhunen loeve;video;target tracking;resultat experimental;automatic classification;imagen color;clasificacion automatica;transformacion karhunen loeve;estudio experimental;motion detection;acquisition tracking and pointing;segmentacion;image couleur;natural scenes;control visual;indizacion automatica;color image;automated processing;principal component	This paper discusses the development of an iterative algorithm for fully automatic (gross or fine) segmentation of color images. The basic idea here is to automate segmentation for on-line operations. This is needed for such critical applications as internet communication, video indexing, target tracking, visual guidance, remote control, and motion detection. The method is composed of an edge-suppressed clustering (learning) and principal component thresholding (classification) step. In the learning phase, image clusters are well formed in the (R,G,B) space by considering only the non-edge points. The unknown number (N) of mutually exclusive image segments is learned in an unsupervised operation mode developed based on the cluster fidelity measure and K-means algorithm. The classification phase is a correlation-based segmentation strategy that operates in the K-L transform domain using the Otsu thresholding principal. It is demonstrated experimentally that the method is effective and efficient for color images of natural scenes with irregular textures and objects of varying sizes and dimension.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	cluster analysis;thresholding (image processing)	Mehmet Celenk;Maarten Uijt de Haag	2000		10.1117/12.379385	computer vision;speech recognition;balanced histogram thresholding;mathematics;region growing;thresholding;image segmentation;cartography	Robotics	48.662517948907336	-57.870903274546954	5685
2fa4fcdbc4c2c474fdefb3550d769fa524284259	learning self-organized topology-preserving complex speech features at primary auditory cortex	neural coding;primary auditory cortex;computer model;neural code;independent component analysis;inner hair cell;complex speech features;frequency modulated;feature extraction;auditory cortex;topology preserving self organization;self organization;topology preservation;basilar membrane	By applying independent component analysis (ICA) algorithm to auditory signals a computational model was developed for the speech feature extraction at the primary auditory cortex. Unlike the other ICA-based features with simple frequency selectivity at the basilar membrane and inner hair cells the learnt features represent complex signal characteristics at the primary auditory cortex such as onset/offset and frequency modulation in time. Also, the topology is preserved with the help of neighborhood coupling during the self-organization. The extracted complex features demonstrated good performance for the robust discrimination of speech phonemes. r 2004 Elsevier B.V. All rights reserved.	algorithm;computation;computational model;feature extraction;independent computing architecture;independent component analysis;information theory;modulation;neuroinformatics;offset (computer science);onset (audio);selectivity (electronic);self-organization;signal processing	Taesu Kim;Soo-Young Lee	2005	Neurocomputing	10.1016/j.neucom.2004.10.076	computer simulation;speech recognition;computer science;neurocomputational speech processing;machine learning;computational auditory scene analysis;neural coding	ML	-3.6923924113543176	-91.39132783236039	5705
bb6cf6288fe7cd563ed2f758fecbf71b79719457	3d high-speed multifocal multiphoton microscopy in muscle and wavelet analysis of fast elementary calcium release events	ultrafast optics;wavelet analysis;3d segmentation;mammalian skeletal muscle fibres;image segmentation;undisturbed calcium homeostasis;detection algorithms;three dimensions;multiphoton microscopy;signal analysis;image sequence analysis;fast elementary calcium release events;undisturbed calcium homeostasis 3d high speed multifocal multiphoton microscopy mammalian skeletal muscle fibres wavelet analysis fast elementary calcium release events automated ecre denoising automated ecre detection wavelet transform hard threshold wavelet denoising low pass filtering image sequence 3d segmentation morphological dynamics;3d high speed multifocal multiphoton microscopy;high speed optical techniques;wavelet transforms bioelectric phenomena biomedical optical imaging biomembrane transport calcium high speed optical techniques image denoising image segmentation image sequences medical image processing multiphoton processes muscle optical microscopy;microscopy;low pass filter;microscopy muscles wavelet analysis calcium image analysis noise reduction time series analysis algorithm design and analysis detection algorithms signal analysis;time series;calcium;area of interest;biomembranes;morphological dynamics;automated ecre denoising;wavelet transforms;temporal resolution;calcium homeostasis;wavelet transform;time series analysis;medical image processing;low pass filtering;noise reduction;hard threshold wavelet denoising;image sequence;detection algorithm;biomedical image processing;multiphoton processes;image analysis;image denoising;biomedical optical imaging;bioelectric phenomena;automated ecre detection;signal to noise ratio;biomembrane transport;skeletal muscle;calcium imaging;high speed;algorithm design and analysis;optical microscopy;wavelet denoising;muscle;muscles;image sequences	Recent approaches to record and analyze skeletal muscle elementary calcium release events (ECRE) in three dimensions (XYT) were restricted to short time series and small areas of interest. Here, we present a combined method for XYT analysis of ECRE using high-speed multifocal multiphoton calcium imaging and a wavelet based automated denoising and detection algorithm. Images were analyzed with the `a trous' implementation of the wavelet transform. `Hard-threshold' wavelet denoising resulted in markedly improved signal-to-noise ratios compared to low-pass filtering. Automated detection of ECRE was performed across wavelet scales and single events were tracked within an image sequence using 3D segmentation. With this combined approach, the morphological dynamics of fast ECRE in mammalian skeletal muscle fibres could be faithfully monitored with high spatial and temporal resolution. Our method may provide an improved tool for the facilitated detection of ECRE in muscle in three dimensions even under conditions of undisturbed calcium homeostasis	algorithm;homeostasis;low-pass filter;noise reduction;signal-to-noise ratio;time series;wavelet transform	Frederic von Wegner;Martin Both;Rainer H. A. Fink;Oliver Friedrich	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1625126	computer vision;image analysis;computer science;microscopy;time series;mathematics;optics;wavelet transform	Visualization	40.16730896967365	-74.510117659497	5711
80d13913d4db963e12400a8f089369eee512f389	real-time stereo correspondence using a truncated separable laplacian kernel approximation on graphics hardware	optimisation;2d laplacian kernel;stereo image processing approximation theory laplace equations optimisation;middlebury stereo database;approximation theory;laplace equations;real time stereo correspondence;graphics hardware;local stereo methods;stereo image processing;geometric proximity;laplace equations kernel graphics hardware stereo vision throughput costs approximation algorithms optimization methods spatial databases;global optimization;nvidia geforce 7900 real time stereo correspondence truncated separable laplacian kernel approximation graphics hardware graphics processing unit 2d laplacian kernel geometric proximity truncated support windows local stereo methods global optimization middlebury stereo database;nvidia geforce 7900;graphics processing unit;truncated support windows;truncated separable laplacian kernel approximation	We present a novel real-time stereo algorithm that achieves both good quality results and very high disparity estimation throughput on the graphics processing unit (GPU). As the key idea of this paper, a truncated separable approximation to an isotropic Laplacian kernel is proposed. This truncated 2D Laplacian kernel variant combines the advantages of large support windows and shiftable windows, while support-weights on geometric proximity can still be appropriately applied to each pixel in truncated support windows. Our method outperforms previous GPU-based local stereo methods and even some methods using global optimization on the benchmark Middlebury stereo database. Because of its separable and regular property, the proposed kernel can be very efficiently implemented on CPUs. Our optimized implementation completely running on an Nvidia GeForce 7900 graphics card achieves over 668 million disparity estimations per second (Mde/s) including all the overhead, about 2.3 to 13.4 times faster than the existing GPU-based solutions.	algorithm;approximation;benchmark (computing);binocular disparity;central processing unit;computer graphics;geforce 7 series;global optimization;graphics hardware;graphics processing unit;kernel (operating system);mathematical optimization;microsoft windows;norm (social);overhead (computing);pixel;real-time clock;throughput;video card	Jiangbo Lu;Sammy Rogmans;Gauthier Lafruit;Francky Catthoor	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4285058	computer vision;mathematical optimization;mathematics;graphics hardware;global optimization;approximation theory;computer graphics (images)	Vision	53.20402217244756	-72.68126683445165	5733
ee45cc825ebf2eec46097271840793ca59ba39ff	simulation of heart rate variability data with methods of wavelet transform	rithmogram;heart rate variability hrv;rr interval;electrocardiography;synthetic hrv data;heart rate;respiratory sinus arrhythmia;tachogram;mayer waves;wavelet transformation	In the paper is described an algorithm for generating of realistic Heart Rate Variability (HRV) records. In the mathematical model are used two Gaussian distributions and wavelet transforms for generating a typical heart rate time series. The Respiratory Sinus Arrhythmia (RSA) and Mayer waves are incorporated in the proposed model. The obtained results show that the algorithm could be applied to clinical statistics based on HRV signals recorded by the patients.	algorithm;heart rate variability;mathematical model;simulation;time series;trigonometric tables;wavelet transform	Galya Georgieva-Tsaneva;Mitko Gospodinov;Evgeniya Gospodinova	2012		10.1145/2383276.2383321	speech recognition;vagal tone	AI	18.95587994631925	-87.1379978703131	5735
79e10448c0dc22aada78f29a307c8d46fcc712ac	configurable automatic detection and registration of fiducial frames for device-to-image registration in mri-guided prostate interventions	sensitivity and specificity;surgery computer assisted;male;image enhancement;magnetic resonance imaging;reproducibility of results;phantoms imaging;pattern recognition automated;humans;prostate;fiducial markers	We propose a novel automatic fiducial frame detection and registration method for device-to-image registration in MRI-guided prostate interventions. The proposed method does not require any manual selection of markers, and can be applied to a variety of fiducial frames, which consist of multiple cylindrical MR-visible markers placed in different orientations. The key idea is that automatic extraction of linear features using a line filter is more robust than that of bright spots by thresholding; by applying a line set registration algorithm to the detected markers, the frame can be registered to the MRI. The method was capable of registering the fiducial frame to the MRI with an accuracy of 1.00 +/- 0.73 mm and 1.41 +/- 1.06 degrees in a phantom study, and was sufficiently robust to detect the fiducial frame in 98% of images acquired in clinical cases despite the existence of anatomical structures in the field of view.	anatomic structures;exanthema;fiducial marker;frame (physical object);image registration;phantom reference;phantoms, imaging;thresholding (image processing);algorithm;registration - actclass	Junichi Tokuda;Sang-Eun Song;Kemal Tuncali;Clare M. Tempany;Nobuhiko Hata	2013	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-40760-4_45	computer vision;radiology;fiducial marker;medicine;pathology;computer science;magnetic resonance imaging;medical physics	Vision	41.11186960672643	-81.81078741578465	5739
732b730b0b3411e954f575a29022f7d1da89d47b	gibbs random fields, fuzzy clustering, and the unsupervised segmentation of textured images	analisis imagen;vision ordenador;analisis textura;fuzzy dustering;champ aleatoire markov;unsupervised segmentation;segmentation;markov random field;computer vision;decision bayesienne;fuzzy clustering;texture analysis;groupage flou;image analysis;vision ordinateur;bayesian decision;analyse image;analyse texture;segmentacion;random field	Abstract   In this paper we present an unsupervised segmentation strategy for textured images, based on a hierarchical model in terms of discrete Markov Random Fields. The textures are modeled as Gaussian Gibbs Fields, while the image partition is modeled as a Markov Mesh Random Field. The segmentation is achieved in two phases: the first one consists of evaluating, from disjoint blocks which are classified as homogeneous, the model parameters for each texture present in the image. This unsupervised learning phase uses a fuzzy clustering procedure, applied to the features extracted from every pixel block, to determine the number of textures in the image and to roughly locate the corresponding regions. The second phase consists of the fine segmentation of the image, using Bayesian local decisions based on the previously obtained model parameters. The originality of the proposed approach lies in the three following aspects: (1) the Gibbs distribution corresponding to each texture type is expressed in terms of its  canonical  potential. This formulation leads to a compact formulation of the global field energy, in terms of the marginal probabilities over pixel cliques. A similar expression is also introduced in the partition model. Such formulations lead to the decomposition of the segmentation problem into a set of local statistical decisions; (2) the segmentation strategy consists of an unsupervised estimation, in which the model parameters are evaluated directly from the observation, by means of a fuzzy clustering technique; (3) no arbitrary assumption is made concerning the number of textures present. Rather, the fuzzy clustering procedure used to estimate the model parameters is applied in a hierarchical manner, searching for a cluster configuration of maximum plausibility.	fuzzy clustering	Hong Hai Nguyen;Paul Cohen	1993	CVGIP: Graphical Model and Image Processing	10.1006/cgip.1993.1001	computer vision;random field;image analysis;fuzzy clustering;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation;segmentation	ML	44.939161134570675	-66.95065550687765	5741
621a5d8b79507a2845c4e211eec66f9df0024ab8	comparison of regularization methods for human cardiac diffusion tensor mri	clinical application;cardiac imaging;diffusion weighted image regularization;regularization method;positive definite;diffusion weighted images;respiratory motion;error propagation;image quality;87 59;synthetic data;signal to noise ratio;diffusion tensor mri;tractography;tensor field regularization;diffusion tensor;diffusion weighted	Diffusion tensor MRI (DT-MRI) is an imaging technique that is gaining importance in clinical applications. However, there is very little work concerning the human heart. When applying DT-MRI to in vivo human hearts, the data have to be acquired rapidly to minimize artefacts due to cardiac and respiratory motion and to improve patient comfort, often at the expense of image quality. This results in diffusion weighted (DW) images corrupted by noise, which can have a significant impact on the shape and orientation of tensors and leads to diffusion tensor (DT) datasets that are not suitable for fibre tracking. This paper compares regularization approaches that operate either on diffusion weighted images or on diffusion tensors. Experiments on synthetic data show that, for high signal-to-noise ratio (SNR), the methods operating on DW images produce the best results; they substantially reduce noise error propagation throughout the diffusion calculations. However, when the SNR is low, Rician Cholesky and Log-Euclidean DT regularization methods handle the bias introduced by Rician noise and ensure symmetry and positive definiteness of the tensors. Results based on a set of sixteen ex vivo human hearts show that the different regularization methods tend to provide equivalent results.		Carole Frindel;Marc C. Robini;Pierre Croisille;Yue Min Zhu	2009	Medical image analysis	10.1016/j.media.2009.01.002	image quality;diffusion mri;mathematical optimization;radiology;propagation of uncertainty;tractography;mathematics;geometry;positive-definite matrix;signal-to-noise ratio;statistics;synthetic data	Vision	45.74935454520352	-82.58361166124408	5746
78d30fad5fe6e8c076c8d165c686ee2da6d94d84	illustrations segmentation in digitized documents using local correlation features	svms;autocorrelation	In this paper we propose an approach for Document Layout Analysis based on local correlation features. We identify and extract illustrations in digitized documents by learning the discriminative patterns of textual and pictorial regions. The proposal has been demonstrated to be effective on historical datasets and to outperform the state-of-the-art in presence of challenging documents with a large variety of pictorial elements. c © 2014 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Scientific Committee of IRCDL 2014.	document layout analysis;image	Dalia Coppi;Costantino Grana;Rita Cucchiara	2014		10.1016/j.procs.2014.10.014	computer vision;computer science;multimedia;information retrieval	NLP	36.03764979794536	-65.13500968540089	5761
2145c4b84ec9f11ae4e680bedb62ea2921e7421b	nasonet, modeling the spread of nasopharyngeal cancer with networks of probabilistic events in discrete time	bayesian network;dynamical processes;probabilistic temporal reasoning;discrete time;temporal noisy gates;cancer diagnosis and prognosis;cancer diagnosis;temporal reasoning;causality;bayesian networks	The spread of cancer is a non-deterministic dynamic process. As a consequence, the design of an assistant system for the diagnosis and prognosis of the extent of a cancer should be based on a representation method that deals with both uncertainty and time. The ultimate goal is to know the stage of development of a cancer in a patient before selecting the appropriate treatment. A network of probabilistic events in discrete time (NPEDT) is a type of Bayesian network for temporal reasoning that models the causal mechanisms associated with the time evolution of a process. This paper describes NasoNet, a system that applies NPEDTs to the diagnosis and prognosis of nasopharyngeal cancer. We have made use of temporal noisy gates to model the dynamic causal interactions that take place in the domain. The methodology we describe is general enough to be applied to any other type of cancer.	bayesian network;causal filter;forecast of outcome;inference;interaction;knowledge acquisition;knowledge representation and reasoning;nasopharyngeal carcinoma;nasopharynx;neoplasms;numerical analysis;occur (action);patients;reasoning	Severino F. Galán;Francisco Aguado;Francisco Javier Díez;José Mira Mira	2002	Artificial intelligence in medicine	10.1016/S0933-3657(02)00027-1	computer science;artificial intelligence;machine learning;bayesian network;statistics	AI	2.320140885826209	-76.06460288797906	5765
f9119a9eeb56b726def0d17311e267802b98de84	structural sensivity for large-scale line-pattern recognition	analisis sensibilidad;object recognition;image processing;analisis forma;procesamiento imagen;reconnaissance objet;traitement image;large scale;sensitivity analysis;pattern recognition;analyse sensibilite;hausdorff distance;pattern analysis;reconnaissance forme;reconocimiento patron;analyse forme	This paper provides a detailed sensitivity analysis for the problem of recognising line patterns from large structural libraries. The analysis focuses on the characterization of two diierent recognition strategies. The rst is histogram-based while the second uses feature-sets. In the former case comparison is based on the Bhattacharyya distance between histograms, while in the latter case the feature-sets are compared using a probabilistic variant of the Hausdorr distance. We study the two algorithms under line-dropout, line fragmentation, line addition and line end-point position errors. The analysis reveals that while the histogram-based method is most sensitive to the addition of line segments and end-point position errors, the set-based method is most sensitive to line dropout.	algorithm;dropout (neural networks);feature model;fragmentation (computing);library (computing);pattern recognition	Benoit Huet;Edwin R. Hancock	1999		10.1007/3-540-48762-X_88	hausdorff distance;computer vision;image processing;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;sensitivity analysis	Robotics	47.67037307311932	-59.70193602732188	5769
1729462f52aa3ff1c3400f2e2fe42660c35c8c8b	shape -based human actions recognition in videos	fourier descriptor;principal component analysis;human activity recognition	The paper presents a system for human action recognition using contour based shape representation. With the rapid progress of computing and communication technology smart user computer interfaces are becoming most widespread. A major goal is to go further than traditional human computer interaction (like mouse or keyboard) and to find more natural means of interaction with computers, including the application of computer games and surveillance. The objective of this work is to achieve representation eigenspace for modeling and classifying actions performed by individuals. Eigenspace is the subspace for each type of action. A representation eigenspace approach based on the Principal Component Analysis (PCA) algorithm is used to train the classifier. Behaviors are classified with respect to a predefined set of learning actions. The key points of this approach include the mode silhouettes are extracted from video, the kind of shape descriptor used, the development of the new eigenspace and the kind of classification used. Performance of the system is expressed in terms of percentage of right or wrong classifications.	algorithm;experiment;feature extraction;fourier analysis;human computer;human–computer interaction;pc game;principal component analysis;similarity learning;test data	Nitish Amraji;Lin Mu;Mariofanna G. Milanova	2011		10.1007/978-3-642-21602-2_58	computer vision;computer science;machine learning;pattern recognition;principal component analysis	Vision	30.330866870319493	-59.68496946321605	5772
62aea991cdd46e6d89dbb5c22d24d5349223e9c4	on the geometries of conic section representation of noisy object boundaries	bepress selected works;object shapes;conic section representation;noisy object boundaries;geometric feature;digital image;noisy object boundaries conic section representation object shapes	This paper studies some geometrical properties of conic sections and the utilization of these properties for the generation of conic section representations of object boundaries in digital images. Several geometrical features of the conic sections, such as the chord, the characteristic point, the guiding triangles, and their appearances under the tessellation and noise corruption of the digital images are discussed. The study leads to a noniterative algorithm that takes advantage of these features in the process of formulating the conic section parameters and generating the approximations of object boundaries from the given sequences of edge pixels in the images. The results can be optimized with respect to certain different criteria of the fittings.		Qiuming Zhu	1999	J. Visual Communication and Image Representation	10.1006/jvci.1999.0419	computer vision;mathematical optimization;computer science;mathematics;geometry;digital image	Vision	49.27535241538955	-61.60026454790964	5778
2f8da24aaf1ec60f7206ecd95aab3ae99cc657a2	coffee transcriptome visualization based on functional relationships among gene annotations		Simplified visualization and conformation of gene networks is one of the current bioinformatics challenges when thousands of gene models are being described in an organism genome. Bioinformatics tools such as BLAST and Interproscan build connections between sequences and potential biological functions through the search, alignment and annotation based on heuristic comparisons that make use of previous knowledge obtained from other sequences. This work describes the search procedure for functional relationships among a set of selected annotations, chosen by the quality of the sequence comparison as defined by the coverage, the identity and the length of the query, when coffee transcriptome sequences were compared against the reference databases UNIREF 100, Interpro, PDB and PFAM. Term descriptors for molecular biology and biochemistry were used along the wordnet dictionary in order to construct a Resource Description Framework (RDF) that enabled the finding of associations between annotations. Sequence-annotation relationships were graphically represented through a total of 6845 oriented vectors. A large gene network connecting transcripts by way of relational concepts was created with over 700 non-redundant annotations, that remain to be validated with biological activity data such as microarrays and RNAseq. This tool development facilitates the visualization of complex and abundant transcripotome data, opens the possibility to complement genomic information for data mining purposes and generates new knowledge in metabolic pathways analysis.		Luis Fernando Castillo;Oscar Gómez-Ramírez;Narmer Galeano-Vanegas;Luis Bertel-Paternina;Gustavo A. Isaza;Álvaro Gaitán-Bustamante	2012		10.1007/978-3-642-28839-5_32	bioinformatics;data mining	Visualization	-0.5133269677637308	-60.18474413706033	5800
e6c58e6b50f2abf233ab8648716ca87cbea23886	multi-classification of parkinson's disease via sparse low-rank learning		Neuroimaging techniques have been widely applied to various neurodegenerative disease analysis to reveal the intricate brain structure. The high dimensional neuroimaging features and limited sample size are the main challenges for the diagnosis task due to the unbalanced input data. To handle it, a sparse low-rank learning framework is proposed, which unveils the underlying relationships between input data and output targets by building a matrix-regularized feature network. Then we obtain the feature weight from the network based on local clustering coefficients. By discarding the irrelevant features and preserving the discriminative structured features, our proposed method can select the most relevant features and identify different stages of Parkinson's disease (PD) from normal controls. Extensive experimental results evaluated on the Parkinson's progression markers initiative (PPMI) dataset demonstrate that the proposed method achieves promising classification performance and outperforms the conventional algorithms. Furthermore, it can detect potential brain regions related to PD for future medical analysis.	algorithm;brain;cluster analysis;clustering coefficient;color gradient;neurodegenerative disorders;neuroimaging;parkinson disease;parkinsonian disorders;relevance;silo (dataset);sparse matrix;unbalanced circuit;statistical cluster	Haijun Lei;Yujia Zhao;Z Huang;Feng Zhou;Zhongwei Huang;Baiying Lei	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546091	support vector machine;discriminative model;sparse matrix;sample size determination;artificial intelligence;cluster analysis;feature extraction;parkinson's disease;pattern recognition;computer science	Vision	24.774729630224723	-78.04750218424945	5811
ad39efdb45b9b855844442991c0588eeaa61fbd0	landmark-based fisher vector representation for video-based face verification	focs landmark based fisher vector representation unconstrained video based face verification image quality video to video face verification dense multiscale sift features extraction encoding facial landmark detection face similarity joint bayesian metric learning unconstrained video face dataseis multiple biometric grand challenge mbgc face and ocular challenge series;face feature extraction measurement face recognition lighting encoding training;fisher vector face verification facial landmarks;video coding bayes methods biometrics access control face recognition image representation learning artificial intelligence	Unconstrained video-based face verification is a challenging problem because of dramatic variations in pose, illumination, and image quality of each face in a video. In this paper, we propose a landmark-based Fisher vector representation for video-to-video face verification. The proposed representation encodes dense multi-scale SIFT features extracted from patches centered at detected facial landmarks, and face similarity is computed with the distance measure learned from joint Bayesian metric learning. Experimental results demonstrate that our approach achieves significantly better performance than other competitive video-based face verification algorithms on two challenging unconstrained video face dataseis, Multiple Biometric Grand Challenge (MBGC) and Face and Ocular Challenge Series (FOCS).	algorithm;biometrics;illumination (image);image quality;multiple biometric grand challenge;symposium on foundations of computer science	Jun-Cheng Chen;Vishal M. Patel;Rama Chellappa	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351294	computer vision;face detection;speech recognition;object-class detection;computer science;pattern recognition;three-dimensional face recognition	Vision	34.11631364995627	-55.559460842044146	5833
8244a442da03938055938602e41772070ad1f9a0	a study of the rao-blackwellised particle filter for efficient and accurate vision-based slam	estensibilidad;modelizacion;vision ordenador;image processing;odometer;mesure distance;real time;localization;slam;mesure position;procesamiento imagen;localizacion;robotics;mixture proposal;feature matching;traitement image;medicion posicion;odometre;computer vision;rao blackwellised particle filter;modelisation;estructura segun moviemento;detector proximidad;posture;distance measurement;localisation;temps reel;simultaneous localization and mapping;postura;position measurement;robotica;tiempo real;vision ordinateur;real time implementation;robotique;extensibilite;scalability;rao blackwellised particle filters;modeling;vision;structure from motion;proximity detector;odometro;structuration d apres le mouvement;pose estimation;detecteur proximite	With recent advances in real-time implementations of filters for solving the simultaneous localization and mapping (SLAM) problem in the range-sensing domain, attention has shifted to implementing SLAM solutions using vision-based sensing. This paper presents and analyses different models of the Rao-Blackwellised particle filter (RBPF) for vision-based SLAM within a comprehensive application architecture. The main contributions of our work are the introduction of a new robot motion model utilizing structure from motion (SFM) methods and a novel mixture proposal distribution that combines local and global pose estimation. In addition, we compare these under a wide variety of operating modalities, including monocular sensing and the standard odometry-based methods. We also present a detailed study of the RBPF for SLAM, addressing issues in achieving real-time, robust and numerically reliable filter behavior. Finally, we present experimental results illustrating the improved accuracy of our proposed models and the efficiency and scalability of our implementation.	applications architecture;approximation;computation;experiment;map;motion estimation;numerical analysis;odometry;particle filter;pixel;real-time clock;real-time computing;real-time transcription;scalability;scale-invariant feature transform;simultaneous localization and mapping;structure from motion	Robert Sim;Pantelis Elinas;James J. Little	2006	International Journal of Computer Vision	10.1007/s11263-006-0021-0	vision;computer vision;structure from motion;scalability;simulation;systems modeling;pose;internationalization and localization;image processing;computer science;robotics;odometer;simultaneous localization and mapping	Robotics	47.93271602960599	-57.508989785938425	5874
07dc0e209ba0e83251c54508cbc9a506f92060d3	statistical-mechanical analysis of inverse digital-halftoning	theoretical framework;statistical mechanics;threshold constant;gibbs sampler;bayers matrices;bayes methods;inverse halftoning;markov random field;markov chain monte carlo;image colour analysis;mean square error;posterior marginal estimate;grayscale images;statistical mechanical analysis;digital image;markov processes;markov chain monte carlo simulations;monte carlo methods bayes methods image colour analysis markov processes;grayscale images statistical mechanical analysis statistical performance inverse digital halftoning problems posterior marginal estimate markov random fields model digital images threshold constant bayers matrices gibbs sampler markov chain monte carlo simulations hyper parameter dependence mean square error inverse halftoning;digital images;inverse digital halftoning problems;monte carlo methods;performance analysis gray scale markov random fields bayesian methods pixel digital images optical noise intelligent systems system analysis and design application software;markov random fields model;hyper parameter dependence;dynamic properties;statistical performance	We propose a theoretical framework to investigate statistical performance of inverse digital-halftoning problems. In the context of the maximizer of the posterior marginal (MPM) estimate corresponding to the Markov random fields (MRFs) model in which each pixel takes discrete values such as 1, ..., Q, we formulate the problem of inverse digital-halftoning in which digital images are generated by the threshold constant and the so-called Bayers' matrices. To construct the Gibbs sampler for the MRFs, we carry out Markov chain Monte Carlo (MCMC) simulations and investigate hyper-parameter dependence of the performance in terms of the mean-square error. By using the statistical-mechanical analysis, we also investigate averaged case performance of the inverse-halftoning for the corresponding analytically tractable class of the MRFs models. Both equilibrium and dynamical properties of the MPM estimation of the original grayscale images are revealed.	algorithm;binary image;cobham's thesis;digital image;gibbs sampling;grayscale;marginal model;markov chain monte carlo;markov random field;material point method;mean squared error;monte carlo method;pixel;sampling (signal processing);simulation	Jun-ichi Inoue;Yohei Saika;Masato Okada	2007	Seventh International Conference on Intelligent Systems Design and Applications (ISDA 2007)	10.1109/ISDA.2007.43	econometrics;mathematical optimization;statistical mechanics;computer science;mathematics;digital image;statistics	Robotics	51.47786077354721	-69.10019759899049	5875
12315e202fc930628561bb549f17a634711a1c3a	multi-modal volume registration by maximization of mutual information	relative position;image processing;computed tomography;pet imaging;positron emission tomography;mr imaging;medical image;magnetic resonance;stochastic approximation;multi modality volume registration;mutual information;information theoretic;information theory	A new information-theoretic approach is presented for finding the registration of volumetric medical images of differing modalities. Registration is achieved by adjustment of the relative position and orientation until the mutual information between the images is maximized. In our derivation of the registration procedure, few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and can foreseeably be used with a wide variety of imaging devices. This approach works directly with image data; no pre-processing or segmentation is required. This technique is, however, more flexible and robust than other intensity-based techniques like correlation. Additionally, it has an efficient implementation that is based on stochastic approximation. Experiments are presented that demonstrate the approach registering magnetic resonance (MR) images with computed tomography (CT) images, and with positron-emission tomography (PET) images. Surgical applications of the registration method are described.	application domain;ct scan;expectation–maximization algorithm;hoc (programming language);information theory;modal logic;muscle rigidity;mutual information;patients;polyethylene terephthalate;positrons;preprocessor;resonance;semantics (computer science);stochastic approximation;x-ray computed tomography;biologic segmentation;registration - actclass	William M. Wells;Paul A. Viola;Hideki Atsumi;Shin Nakajima;Ron Kikinis	1996	Medical image analysis	10.1016/S1361-8415(01)80004-9	stochastic approximation;computer vision;radiology;pet-ct;image processing;information theory;image registration;magnetic resonance imaging;mathematics;mutual information;nuclear medicine;statistics;medical physics	Vision	44.8867143618419	-78.54383046181428	5908
0abb9315b4905e7950ca7548abef24666cbdc7f7	block based texture analysis for iris classification and matching	databases;pigment spots;anatomical structure;experimental analysis;image segmentation;iris feature extraction data mining fingerprint recognition large scale systems biometrics frequency anatomical structure pigmentation spatial databases;biometrics access control;image matching;biometrics;iris recognition;large scale biometric systems;pigmentation;data mining;probes;statistical analysis biometrics access control feature extraction image matching iris recognition;iris classification;iris matching;large scale;texture analysis;statistical analysis;fingerprint recognition;feature extraction;pixel;spatial databases;iris texture;block based texture analysis;iris;frequency;high frequency;pigment spots block based texture analysis iris classification iris matching large scale biometric systems statistical features iris texture anatomical structures;large scale systems;statistical features;anatomical structures	The goal of this paper is to analyze the texture of irides and determine if they can be quantitatively measured and assigned into multiple categories. Such an exercise would ensure that irides, like fingerprints, can be partitioned into multiple classes thereby allowing for faster retrieval of identities in large scale biometric systems. In order to facilitate this, a set of 68 statistical features is extracted from the iris texture. These features correspond to the high frequency information associated with anatomical structures in the iris such as crypts, furrows and pigment spots. The statistical features extracted from different blocks in the iris are fused at the feature level and decision level. Experimental analysis using the UPOL database indicates the efficacy of the proposed scheme in (a) clustering iris texture, and (b) assigning an input iris to the correct cluster based on its textural content. The feasibility of using blocks of iris to perform partial iris matching is also investigated	biometrics;cluster analysis;emoticon;experiment;feature extraction;feature vector;fingerprint;flexos;matching (graph theory);pigment	Arun Ross;Manisha Sam Sunder	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5543234	computer vision;speech recognition;feature extraction;computer science;pigment;frequency;high frequency;pattern recognition;iris recognition;image segmentation;fingerprint recognition;pixel;biometrics;experimental analysis of behavior	Vision	35.9527444087748	-72.31515417700469	5928
65229255a8d47c8f39f965696d1291d085cb8418	optimal echo-time selection for mrsi acquisition	echo times;chemicals;multiple k space frames;phantoms biomedical mri biomedical optical imaging image reconstruction medical image processing;data collecting processing;phantoms;ethanol;spatial reconstruction;data collection;scattering;optimal echo time selection;sequential backward selection;chemical shift;spatial reconstruction mrsi acquisition optimal echo time selection magnetic resonance spectroscopic imaging motion artifacts data collecting processing sequential backward selection echo planar imaging acquisition multiple echo time values multiple k space frames phantom;phantom;chemical shift magnetic resonance spectroscopic imaging echo times echo planar imaging sequential backward selection;mrsi acquisition;motion artifacts;image reconstruction imaging ethanol methanol chemicals electron tubes scattering;image reconstruction;medical image processing;imaging;echo planar imaging acquisition;echo planar imaging;multiple echo time values;methanol;magnetic resonance spectroscopic imaging;motion artifact;biomedical optical imaging;electron tubes;biomedical mri	Long acquisition time is one of the major challenges of magnetic resonance spectroscopic imaging (MRSI). Patient discomfort, motion artifacts and cost will increase when the acquisition time lengthens. In order to accelerate the data collecting process, we propose to apply sequential backward selection (SBS) to optimal echo-time selection and then use these selected echo-time values in echo-planar imaging (EPI) acquisition. For multi-echo EPI, our method allows multiple echo-time values to be selected simultaneously. As a result, in each excitation, multiple k-space frames can be acquired and the acquisition time reduces further. Phantom experiments demonstrate the feasibility of the proposed method. We compare the reconstructed spatial and spectral results from this new technique, standard MRSI, equal echo-time selection and random echo-time selection. Comparisons show that the proposed method can achieve similar results to standard MRSI while greatly reducing the data quantity requirements.	experiment;requirement;resonance;smart battery system	Wenting Deng;Stanley J. Reeves;Donald B. Twieg	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872727	iterative reconstruction;medical imaging;computer vision;chemical industry;radiology;medicine;chemical shift;ethanol fuel;magnetic resonance spectroscopic imaging;methanol fuel;nuclear magnetic resonance;nuclear medicine;scattering;data collection	Visualization	46.6013682553755	-83.03000574965357	5929
4356da713da8ab85133b4b29f9db330ef4a66c44	a curvature sensitive filter and its application in microfossil image characterisation	noise	A new class of oriented, curvature sensitive filters are introduced. These filters provide a low-level detection facility for noisy curves without a prior edge extraction stage. The application of these filters to the detection of Carboniferous Foraminifers (a type of microfossil found in plane rock sections) is described. A symbolic representation of the detected curves is stored in a database which is then queried to recover the required structures. We show that the curves identified by the filter correspond to salient features of the microfossil evidence in the image.		John P. Oakley;Richard T. Shann	1992		10.5244/C.6.41	discrete mathematics;topology;geometry	Vision	49.10184246291938	-62.87110631095647	5931
0f3b74f57b02366878d226395e76c44bfcfc3e10	trip database: a manually curated database of protein–protein interactions for mammalian trp channels	animals;transient receptor potential channels;protein protein interaction;humans;user computer interface;protein interaction mapping;mammals;databases protein	Transient receptor potential (TRP) channels are a superfamily of Ca(2+)-permeable cation channels that translate cellular stimuli into electrochemical signals. Aberrant activity of TRP channels has been implicated in a variety of human diseases, such as neurological disorders, cardiovascular disease and cancer. To facilitate the understanding of the molecular network by which TRP channels are associated with biological and disease processes, we have developed the TRIP (TRansient receptor potential channel-Interacting Protein) Database (http://www.trpchannel.org), a manually curated database that aims to offer comprehensive information on protein-protein interactions (PPIs) of mammalian TRP channels. The TRIP Database was created by systematically curating 277 peer-reviewed literature; the current version documents 490 PPI pairs, 28 TRP channels and 297 cellular proteins. The TRIP Database provides a detailed summary of PPI data that fit into four categories: screening, validation, characterization and functional consequence. Users can find in-depth information specified in the literature on relevant analytical methods and experimental resources, such as gene constructs and cell/tissue types. The TRIP Database has user-friendly web interfaces with helpful features, including a search engine, an interaction map and a function for cross-referencing useful external databases. Our TRIP Database will provide a valuable tool to assist in understanding the molecular regulatory network of TRP channels.	calcium;cardiovascular diseases;categories;cations;cross-reference;database;histocompatibility testing;mammals;pixel density;proton pump inhibitors;superfamily;usability;user interface;web search engine;nervous system disorder;protein protein interaction;search a word	Young-Cheul Shin;Soo-Yong Shin;Insuk So;Dongseop Kwon;Ju-Hong Jeon	2011		10.1093/nar/gkq814	protein–protein interaction;biology;bioinformatics;transient receptor potential channel	Comp.	-0.13336227241056892	-61.884594832136294	5934
ccb27d726c0b799e800a63eac18a33bf262852aa	conditional random fields meet deep neural networks for semantic segmentation: combining probabilistic graphical models with deep learning for structured prediction		Semantic segmentation is the task of labeling every pixel in an image with a predefined object category. It has numerous applications in scenarios where the detailed understanding of an image is required, such as in autonomous vehicles and medical diagnosis. This problem has traditionally been solved with probabilistic models known as conditional random fields (CRFs) due to their ability to model the relationships between the pixels being predicted. However, deep neural networks (DNNs) recently have been shown to excel at a wide range of computer vision problems due to their ability to automatically learn rich feature representations from data, as opposed to traditional handcrafted features. The idea of combining CRFs and DNNs have achieved state-of-the-art results in a number of domains. We review the literature on combining the modeling power of CRFs with the representation-learning ability of DNNs, ranging from early work that combines these two techniques as independent stages of a common pipeline to recent approaches that embed inference of probabilistic models directly in the neural network itself. Finally, we summarize future research directions.	artificial neural network;autonomous robot;computer vision;conditional random field;deep learning;graphical model;machine learning;pixel;structured prediction	Anurag Arnab;Shuai Zheng;Sadeep Jayasumana;Bernardino Romera-Paredes;Måns Larsson;Alexander Kirillov;Bogdan Savchynskyy;Carsten Rother;Fredrik Kahl;Philip H. S. Torr	2018	IEEE Signal Processing Magazine	10.1109/MSP.2017.2762355	computer science;theoretical computer science;artificial neural network;machine learning;probabilistic logic;deep learning;image segmentation;conditional random field;ranging;graphical model;artificial intelligence;structured prediction	AI	24.063089770595024	-53.84705924900041	5946
6f1ba86764e4c6289c644909749168f2ce47de28	a computational approach for the estimation of heart failure patients status using saliva biomarkers		The aim of this work is to present a computational approach for the estimation of the severity of heart failure (HF) in terms of New York Heart Association (NYHA) class and the characterization of the status of the HF patients, during hospitalization, as acute, progressive or stable. The proposed method employs feature selection and classification techniques. However, it is differentiated from the methods reported in the literature since it exploits information that biomarkers fetch. The method is evaluated on a dataset of 29 patients, through a 10-fold-cross-validation approach. The accuracy is 94 and 77% for the estimation of HF severity and the status of HF patients during hospitalization, respectively.	adverse event;biological markers;cross-validation (statistics);feature selection;heart failure;heart valve disease;helicon filter;hospitalization;patients;statistical classification;the new york times;saliva;triangulation	Evanthia E. Tripoliti;Theofilos G. Papadopoulos;Georgia S. Karanasiou;Fanis G. Kalatzis;Yorgos Goletsis;Aris Bechlioulis;Silvia Ghimenti;Tommaso Lomonaco;Francesca Bellagambi;Maria Giovanna Trivella;Roger Fuoco;Mario Marzilli;Maria Chiara Scali;Katerina K. Naka;Abdelhamid Errachid	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037648	computer science;medical emergency;feature selection;biomarker (medicine);heart failure	Visualization	8.979766780260947	-78.3538194446863	5954
a0f666f05bd38cc3f22dc64480b2182cd1696236	cardiopulmonary resuscitation quality parameters from motion capture data using differential evolution fitting of sinusoids		Cardiopulmonary resuscitation (CPR) is alongside electrical defibrillation the most crucial countermeasure for sudden cardiac arrest, which affects thousands of individuals every year. In this paper, we present a novel approach including sinusoid models that use skeletal motion data from an RGB-D (Kinect) sensor and the Differential Evolution (DE) optimization algorithm to dynamically fit sinusoidal curves to derive frequency and depth parameters for cardiopulmonary resuscitation training. It is intended to be part of a robust and easy-to-use feedback system for CPR training, allowing its use for unsupervised training. The accuracy of this DE-based approach is evaluated in comparison with data of 28 participants recorded by a state-of-the-art training mannequin. We optimized the DE algorithm hyperparameters and showed that with these optimized parameters the frequency of the CPR is recognized with a median error of ±2.9 compressions per minute compared to the reference training mannequin.		Christian Lins;Daniel Eckhoff;Andreas Klausen;Sandra Hellmers;Andreas Hein;Sebastian J. F. Fudickar	2018	CoRR		mathematics;machine learning;differential evolution;cardiopulmonary resuscitation;computer vision;hyperparameter;sudden cardiac arrest;motion capture;artificial intelligence;defibrillation	Vision	23.468209303115646	-90.41831034712759	5963
041ac60d6670abc6e5097ef2d87f7015be06ae40	mechanisms of information filtering in neural systems	coherence neurons frequency measurement information processing information rates shape mutual information;information rates;frequency measurement;shape;nonlinear dynamical systems neuroscience biological information theory mutual information;information processing;mutual information;coherence;neurons	Claude Shannon’s information theory has been applied to neural information transmission and information rates of up to hundreds of bits per second have been estimated for single spiking neurons. For Gaussian white-noise signals, one can meaningfully resolve the single number of information rate with respect to frequencies and pose the corresponding question: does a neuron encode information preferentially about slow, intermediate, or fast components of the stimulus? This question is answered in an approximate way by the spectral coherence function, a frequency-resolved measure of information transfer that is related to the lower bound on the mutual information rate. Integrator dynamics in conjunction with uncorrelated intrinsic noise yields an overall low-pass filter. In this paper, the features of the neural dynamics that lead to experimentally observed band-pass or high-pass filter shapes of the coherence are reviewed. The mechanisms at the cellular level such as resonating subthreshold dynamics and slow channel noise are discussed and furthermore the filter effects encountered at the population level such as heterogeneous synaptic short-term plasticity in conjunction with multiple stimuli are explained and a scenario in which the synchronous output of a population is regarded as the signal carrier. Finally, the frequency-resolved mutual information rate, which goes beyond the coherence, is discussed.	approximation algorithm;coherence (physics);data rate units;encode;experiment;information filtering system;information theory;low-pass filter;mutual information;neuron;noise (electronics);shannon (unit);synaptic package manager	Benjamin Lindner	2016	IEEE Transactions on Molecular, Biological and Multi-Scale Communications	10.1109/TMBMC.2016.2618863	coherence;information processing;shape;computer science;theoretical computer science;machine learning;mathematics;mutual information;statistics	ML	19.913805037119335	-72.23088709476849	5971
5d991651626f199e5e38853ad4294e1ce0aac2f9	robust fuzzy c-means in classifying breast tissue regions	fuzzy c mean;biological tissues;kernel;image segmentation;medical image processing biological tissues biomedical mri fuzzy set theory image classification image segmentation;automatic segmentation;kernel function;image classification;biomedical imaging;automatic segmentation system;breast;fuzzy set theory;magnetic resonance image;breast tissue region classification;objective function;functional imaging;mr imaging;breast magnetic resonance imaging data;medical image;clustering;hyper tangent function;medical image processing;magnetic resonance imaging;fuzzy c means;pattern recognition;clustering algorithms;hyper tangent functions;fuzzy segmentation algorithm;medical image segmentation;mr imaging fuzzy c means clustering kernel function hyper tangent function image segmentation;lagrangian multipliers;fuzzy segmentation;breast tissue image segmentation biomedical imaging kernel noise robustness spatial resolution magnetic noise magnetic resonance imaging lagrangian functions equations;lagrangian multipliers breast tissue region classification medical image segmentation mathematical algorithm automatic segmentation system fuzzy segmentation algorithm breast magnetic resonance imaging data kernel induced fuzzy c means hyper tangent functions;biomedical mri;kernel induced fuzzy c means;spatial resolution;mathematical algorithm	Limited spatial resolution, poor contrast, overlapping intensities, noise and intensity in homogeneities variation make the assignment of segmentation of medical images is greatly difficult. In recent days, mathematical algorithm supported automatic segmentation system plays an important role in segmentation of medical imaging. This paper presents an effective fuzzy segmentation algorithm for breast magnetic resonance imaging data. This paper obtains a new effective objective function of fuzzy c-means called Kernel induced Fuzzy C-Means based hyper tangent function based on Kernel functions, hyper tangent functions, and Lagrangian multipliers. Initially, this paper tries to derive new objective function by introducing kernel function and consequently it derives new effective equations for calculating memberships and updating prototypes, which are shown to be more robust than FCM. The performance of proposed method has been shown with random data and then the new approach is applied to real medical images. Experimental results on both basic FCM and proposed method have been done for the purpose of comparison of proposed method’s result.	algorithm;augmented lagrangian method;fuzzy cognitive map;lagrange multiplier;loss function;medical imaging;optimization problem;randomness;resonance	S. R. Kannan;S. Ramathilagam;A. Sathya	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.46	computer vision;mathematical optimization;computer science;magnetic resonance imaging;machine learning;mathematics;cluster analysis	Robotics	43.38536316570509	-72.98648462462646	5974
b29c94c8892062ddf8d5f690baf7e469bcacf2d1	personal recognition using hand shape and texture	traitement signal;evaluation performance;texture;ajustamiento modelo;selection problem;problema seleccion;shape biometrics digital cameras discrete cosine transforms hardware predictive models classification tree analysis decision trees support vector machines support vector machine classification;decision tree;performance evaluation;decision trees classification scheme;palmprint image recognition;support vector machines;naive bayes classification scheme;biometrics access control;algorithms artificial intelligence biometry computer security hand humans image enhancement image interpretation computer assisted information storage and retrieval pattern recognition automated user computer interface;bayes methods;biometrie;evaluacion prestacion;transformation cosinus discrete;image representations;naive bayes;maquina vector soporte;biometrics;biometria;digital camera;image classification;shape recognition;bayes procedures;ffn classification scheme personal recognition hand shape image palm texture bimodal biometric system feature level fusion palmprint image recognition digital camera feature extraction discrete cosine transform coefficients naive bayes classification scheme decision trees classification scheme k nn classification scheme svm classification scheme;hand shape image;arbol decision;indexing terms;discrete cosine transform;identificacion sistema;ajustement modele;feature subset selection and combination;palm texture;machine vecteur support;ffn classification scheme;automatic recognition;hand shape recognition;system identification;discrete cosine transforms;image representation;feature extraction;signal processing;model matching;signal classification;textura;palmprint recognition biometrics feature level fusion feature subset selection and combination hand shape recognition;feature subset selection;pattern recognition;classification signal;palmprint recognition;feature selection;access control	This paper proposes a new bimodal biometric system using feature-level fusion of hand shape and palm texture. The proposed combination is of significance since both the palmprint and hand-shape images are proposed to be extracted from the single hand image acquired from a digital camera. Several new hand-shape features that can be used to represent the hand shape and improve the performance are investigated. The new approach for palmprint recognition using discrete cosine transform coefficients, which can be directly obtained from the camera hardware, is demonstrated. None of the prior work on hand-shape or palmprint recognition has given any attention on the critical issue of feature selection. Our experimental results demonstrate that while majority of palmprint or hand-shape features are useful in predicting the subjects identity, only a small subset of these features are necessary in practice for building an accurate model for identification. The comparison and combination of proposed features is evaluated on the diverse classification schemes; naive Bayes (normal, estimated, multinomial), decision trees (C4.5, LMT), /spl kappa/-NN, SVM, and FFN. Although more work remains to be done, our results to date indicate that the combination of selected hand-shape and palmprint features constitutes a promising addition to the biometrics-based personal recognition systems.	authentication;biometrics;c4.5 algorithm;coefficient;computational complexity theory;decision trees;decision tree;digital camera;discrete cosine transform;extraction;feature selection;fetal fibronectin measurement;fingerprint;image acquired;image segmentation;logistic model tree;multinomial logistic regression;naive bayes classifier;perimeter;subgroup;trees (plant);universal quantification;biologic segmentation	Ajay Kumar;David Zhang	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.875214	support vector machine;computer vision;computer science;machine learning;decision tree;signal processing;pattern recognition;feature selection	Vision	43.82706026316943	-60.08049601418238	5983
d0ccbcb9997a63313f16d823f1388ac014199d41	a methodology for automated extraction of the optimal pathways from influence diagrams	diagnostic imaging;medical decision making;predictive value of tests;chest pain;influence diagram	The influence diagram (ID) is a powerful tool for modelling medical decision-making processes, like the optimal application of diagnostic imaging. In this area, where safety and efficacy are determined by a number of aspects varying in nature and importance, it is difficult for humans to relate all available pieces of evidence and consequences of choices. IDs are well suited to provide evidence-based diagnostic pathways. However, medical specialists cannot be expected to be familiar with IDs and their output can be difficult to interpret. To overcome these shortcomings, a methodology is developed to automatically extract the optimal pathways from an ID and represent these in a tree shaped flow diagram. It imposes a few general rules on the structure of the model, which determine the relation between decisions to perform imaging and the availability of test results. Extracting the optimal pathways requires postprocessing the results of an ID, leaving out the sub optimal choices and irrelevant scenarios. Predictive value of tests are vital information in medical protocols, they are at hand in the ID for each relevant scenario. The methodology is illustrated by the problem of diagnosing acute chest pain.	influence diagram;medical imaging;relevance	Alexander Benjamin Meijer	2007		10.1007/978-3-540-73599-1_48	medical imaging;simulation;influence diagram;artificial intelligence;predictive value of tests;data mining	ML	1.375527888170197	-76.2429656127276	5989
0523283e18aa7681eb678d8066626486c79d81e1	spike-frequency adaptation as a mechanism for dynamic coding in v1	neuromodulation;neural coding;neural code;primary visual cortex;activity pattern;flashed stimuli	We investigate the representation of visual stimuli and the short-term dynamics of activity within primary visual cortex in a ‘free-viewing’ scenario with ‘saccading eye movements’ modeled as a series of visual stimuli that are flashed onto the retina for the duration of a fixation period (200–       ). We assume that the entire activity pattern from the beginning of fixation until time t constitutes the neural code. Given a noisy (Poissonian) representation it follows that the signal-to-noise ratio increases with time, because more spikes become available for representation. Here, we show that for archiving an optimal stimulus representation in any increasing time-window beginning with stimulus onset, the processing strategy of the network should be dynamic in the sense that an initially high recurrent cortical competition between orientation selective cells attenuates with time, i.e. mediated by the instrinsic property of spike-frequency adaptation of pyramidal cells.	action potential	Lars Schwabe;Péter Adorján;Klaus Obermayer	2001	Neurocomputing	10.1016/S0925-2312(01)00436-2	computer vision;artificial intelligence;neural coding	HCI	18.26496992375933	-73.73050269157271	6002
501da13ddf28a192ac57dc4d2ea82bcab69b32b2	pairsdb atlas of protein sequence space	genes;spinal cord injuries;protein family;sequence similarity;protein sequence;consensus sequence;amino acid sequence;relational database;satisfiability;conserved sequence;internet;graphical displays;molecular biology;trans membrane;humans;sequence alignment;user computer interface;database search;basic local alignment search tool;sequence analysis protein;databases protein	Sequence similarity/database searching is a cornerstone of molecular biology. PairsDB is a database intended to make exploring protein sequences and their similarity relationships quick and easy. Behind PairsDB is a comprehensive collection of protein sequences and BLAST and PSI-BLAST alignments between them. Instead of running BLAST or PSI-BLAST individually on each request, results are retrieved instantaneously from a database of pre-computed alignments. Filtering options allow you to find a set of sequences satisfying a set of criteria-for example, all human proteins with solved structure and without transmembrane segments. PairsDB is continually updated and covers all sequences in Uniprot. The data is stored in a MySQL relational database. Data files will be made available for download at ftp://nic.funet.fi/pub/sci/molbio. PairsDB can also be accessed interactively at http://pairsdb.csc.fi. PairsDB data is a valuable platform to build various downstream automated analysis pipelines. For example, the graph of all-against-all similarity relationships is the starting point for clustering protein families, delineating domains, improving alignment accuracy by consistency measures, and defining orthologous genes. Moreover, query-anchored stacked sequence alignments, profiles and consensus sequences are useful in studies of sequence conservation patterns for clues about possible functional sites.	amino acid sequence;atlases;blast;cluster analysis;consensus sequence;conserved sequence;download;downstream (software development);graph - visual representation;interactivity;molecular biology;mysql;peptide sequence;pierre robin syndrome;pipeline (computing);precomputation;protein family;question (inquiry);relational database;sequence alignment;sequence homology;uniprot;statistical cluster	Andreas Heger;Eija Korpelainen;Taavi Hupponen;Kimmo Mattila;Vesa Ollikainen;Liisa Holm	2008		10.1093/nar/gkm879	consensus sequence;biology;database search engine;the internet;relational database;bioinformatics;simple modular architecture research tool;protein sequencing;gene;sequence alignment;peptide sequence;conserved sequence;protein family;satisfiability	Comp.	-1.5184623930426981	-58.9462467720091	6036
73183a3794c46a29b4e77bf5f7294c5ae01294d9	ridge-line density estimation in digital images	2d model ridge line density estimation digital images sinusoidal signals;image processing;edge detection;density estimation;digital image;edge detection image processing;digital images frequency estimation fingerprint recognition gabor filters filtering mathematics surface texture area measurement density measurement computational efficiency	This paper introduces a new efficient method for estimating the local ridge-line density in digital images. A mathematical characterization of the local frequency of sinusoidal signals is given, and a 2d-model is developed in order to approximate the ridge-line patterns. Experimental results obtained through a discrete implementation of the method are presented both in terms of accuracy and efficiency.	approximation algorithm;computation;computational complexity theory;digital image;discrete mathematics	Dario Maio;Davide Maltoni	1998		10.1109/ICPR.1998.711198	computer vision;density estimation;edge detection;image resolution;image processing;computer science;digital signal processing;digital image processing;pattern recognition;image formation;digital image	ML	51.54070041618837	-65.77567548178943	6040
ab15a22b787ab373db5aa6c0ed6b41475ffb7e38	image quality of dry-processed film	aide diagnostic;experiencia profesional;radiology;evaluation performance;professional experience;informatica biomedical;image numerique;biomedical data processing;pacs;experience professionnelle;performance evaluation;image processing;resolution spatiale;resolucion espacial;technology;evaluacion prestacion;radiologia;informatique biomedicale;dry film processor;procesamiento imagen;hombre;radiologie;dry radiographic film;qualite image;traitement image;image quality;technologie;imagen numerica;human;calidad imagen;digital image;dry processed film;diagnostic aid;homme;ayuda diagnostica;spatial resolution;tecnologia	Recently, a lot of dry film processors and its radiographic films have been favorably used in hospitals. We have evaluated the diagnostic capability and measured the image quality of these dry processed films, comparing it with those of conventional wet processed films. It is found that (1) dry processed film has almost the same diagnostic capability to that of wet processed film, and (2) dry film had strong absorption at 804 nm which brought a reddish color tone to dry film and gave radiologists problems in diagnosis. These situations have been improved through this study to some extent.	central processing unit;image quality;radiography;radiology;x-ray film	T. Okabe;Kazuaki Nakamura;T. Asano	2001	Computer methods and programs in biomedicine	10.1016/S0169-2607(01)00138-9	image quality;simulation;radiology;image processing;computer science;digital image;computer graphics (images);technology	Graphics	46.686473088917694	-79.83982605039256	6049
58f00f48912ce63d38368c05ce1a2c02a63a1f27	liver tumor detection in ct images by adaptive contrast enhancement and the em/mpm algorithm	histograms;contrast enhanced;shape constraints;liver;liver tumor segmentation;image segmentation;computed tomography;measurement;computer aided diagnosis;probability density function;cad;em mpm algorithm liver tumor segmentation ct images probability density function;tumors liver computed tomography shape image segmentation histograms measurement;focal tumor identification liver tumor detection ct images adaptive contrast enhancement em mpm algorithm computer aided diagnosis image segmentation low contrast images low level images intensity contrast probability density function estimation expectation maximization posterior marginal algorithm noise reduction shape constraint;image enhancement;shape;automatic detection;expectation maximization;medical image processing;computerised tomography;tumors;image denoising;em mpm algorithm;ct images;object detection cad computerised tomography expectation maximisation algorithm image denoising image enhancement image segmentation medical image processing;quantitative evaluation;object detection;liver tumor;expectation maximisation algorithm	Automatic tumor detection and segmentation is essential for the computer-aided diagnosis of live tumors in CT images. However, it is a challenging task in low-contrast images as the low-level images are too weak to detect. In this paper, we propose a new method for the automatic detection of liver tumors. We first adaptively enhance the intensity contrast of CT images by probability density function estimation. Then, to detect tumorous regions, we use the expectation maximization/maximization of the posterior marginal (EM/MPM) algorithm, which utilizes both the intensity and label information of the adjacent regions. Finally, a shape constraint is applied to reduce noise and identify focal tumors. Quantitative evaluation experiments show that our method can accurately and effectively detect tumors even in poor-contrast CT images.	ct scan;expectation–maximization algorithm;experiment;focal (programming language);high- and low-level;marginal model;material point method	Yu Masuda;Tomoko Tateyama;Wei Xiong;Jiayin Zhou;Makoto Wakamiya;Syuzo Kanasaki;Akira Furukawa;Yen-Wei Chen	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115708	computer vision;probability density function;expectation–maximization algorithm;shape;pattern recognition;cad;histogram;mathematics;image segmentation;computed tomography;measurement;statistics	Vision	40.33296415061129	-76.2983960675196	6050
82e52da05fc9dae38c4a5cc5f6340aed906c3914	an inflection point method for the determination of pulmonary transit time from contrast echocardiography	manuals echocardiography computational modeling fitting data models inspection biomedical measurement;random processes echocardiography lung medical signal processing;computational analysis inflection point method pulmonary transit time contrast echocardiography indicator dilution curves idc ptt low frequency noise recirculation artifacts signal truncation algorithm local density random walk model;signal processing time indicator dilution curves	Objective: Indicator-dilution curves (IDCs) for the estimation of pulmonary transit times (PTTs) can be generated noninvasively using contrast echocardiography. Currently, these IDCs are analyzed by manual inspection, which is not feasible in a clinical setting, or fit to a statistical model to derive parameters of interest. However, IDCs generated from patients are frequently subject to significant low-frequency noise and recirculation artifacts that obscure the first-pass signal and render model fitting impractical or inaccurate. Thus, the objective of this paper was to develop alternative computational methods to determine PTT using noisy clinical data in which the signal decay is not adequately visible. Methods: We report on a method that uses a model fit to the rise portion of the IDCs to determine the signal inflection point. Additionally, a signal truncation algorithm was developed that enables automated analysis of the IDCs. Results: We compare PTTs derived from our inflection point method to those obtained by manual inspection in 25 patients (R2 = 0.86) and to those obtained by mean transit time calculation following fitting to a local density random walk model (R2 = 0.80) in a subset of this cohort. Conclusion: Combined with a signal truncation algorithm, the inflection point method provides robust, automated determination of PTT from noisy IDCs containing recirculation artifacts. Significance: The inflection point method addresses the need for computational analysis of IDCs obtained from contrast echocardiograms that are not amenable to first-pass model fitting.	activated partial thromboplastin time measurement;addresses (publication format);curve fitting;echocardiography;indicator dilution techniques;morphologic artifacts;patients;protein truncation abnormality;pulmonary valve insufficiency;ringing artifacts;statistical model;subgroup;algorithm	Steven M. Boronyak;Ken Monahan;Evan L. Brittain;W. David Merryman	2015	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2015.2405764	electronic engineering;electrical engineering;mathematics	ML	50.13184730479197	-84.39687321041556	6061
400de569c8f2b7def35dee8fd84837b99a43d984	review of control strategies for lower limb prostheses		Each year thousands of people lose their lower limbs, mainly due to three causes: wars, accidents and vascular diseases. The development of prostheses is crucial to improve the quality of millions of people’s lives by restoring their mobility. Lower limb prostheses can be divided into three major groups: passive, semi-active or variable damping and powered or intelligent. This contribution provides a literature review of the principal control strategies used in lower limb prostheses, i.e., the controllers used in energetically powered transfemoral and transtibial prostheses. We present a comparison of the presented literature review and the future trends of this important field. It is concluded that the use of bio-inspired concepts and continuous control combined with the other control approaches can be crucial in the improvement of prosthesis controllers, enhancing the quality of amputee’s lives.		César Ferreira;Luís Paulo Reis;Cristina P. Santos	2015		10.1007/978-3-319-27149-1_17	simulation;prosthesis;biomedical engineering;computer science	Robotics	9.424749584327639	-81.96832293167627	6064
8263bc97954b9bc920b749d03223301bcfede99f	automatic descriptor-based co-registration of frame hyperspectral data	sensors;ransac;affine;uavs;earth observation;feature descriptors;automation	Frame hyperspectral sensors, in contrast to push-broom or line-scanning ones, produce hyperspectral datasets with, in general, better geometry but with unregistered spectral bands. Being acquired at different instances and due to platform motion and movements (UAVs, aircrafts, etc.), every spectral band is displaced and acquired with a different geometry. The automatic and accurate registration of hyperspectral datasets from frame sensors remains a challenge. Powerful local feature descriptors when computed over the spectrum fail to extract enough correspondences and successfully complete the registration procedure. To this end, we propose a generic and automated framework which decomposes the problem and enables the efficient computation of a sufficient amount of accurate correspondences over the given spectrum, without using any ancillary data (e.g., from GPS/IMU). First, the spectral bands are divided in spectral groups according to their wavelength. The spectral borders of each group are not strict and their formulation allows certain overlaps. The spectral variance and proximity determine the applicability of every spectral band to act as a reference during the registration procedure. The proposed decomposition allows the descriptor and the robust estimation process to deliver numerous inliers. The search space of possible solutions has been effectively narrowed by sorting and selecting the optimal spectral bands which under an unsupervised manner can quickly recover hypercube’s geometry. The developed approach has been qualitatively and quantitatively evaluated with six different datasets obtained by frame sensors onboard aerial platforms and UAVs. Experimental results appear promising. Remote Sens. 2014, 6 3410	aerial photography;computation;feature model;global positioning system;sensor;sorting;unmanned aerial vehicle;unsupervised learning	Maria Vakalopoulou;Konstantinos Karantzalos	2014	Remote Sensing	10.3390/rs6043409	earth observation;computer vision;ransac;simulation;sensor;automation;machine learning;affine transformation;mathematics;optics;remote sensing	Vision	42.15458640515309	-55.1084572730309	6074
9b0c8413c1173708503e9e45f09c7dd0491212cc	immature green citrus fruit detection using color and thermal images		Abstract Citrus fruit detection is one of the most important and challenging steps in citrus yield mapping. The distinct color differences between the ripe fruit and leaves allowed previously-described imaging-based methods to achieve good results. However, immature green citrus fruit detection, which aims to provide valuable information for citrus yield mapping at earlier stages is much more difficult because the fruit and leaf colors are very similar. This study combines color and thermal images for immature green fruit detections. Experiments identified optimal conditions for thermal imaging. A multimodal imaging platform was built to integrate color and thermal cameras. A novel image registration method was developed for combining color and thermal images and matching fruit in both images which achieved pixel-level accuracy. A new Color-Thermal Combined Probability (CTCP) algorithm was created to effectively fuse information from the color and thermal images to classify potential image regions into fruit and non-fruit classes. Algorithms were also developed to integrate image registration, information fusion and fruit classification and detection into a single step for real-time processing. An increase in recall rate from 78.1% when using only color images to 90.4% after fusing the color and thermal images was obtained at similar precision rates, and an increase in precision rate from 86.6% to 95.5% was obtained at similar recall rates. The fusion of the color and thermal images effectively improved immature green citrus fruit detection.	color	H. Gan;Won Suk Lee;Victor Alchanatis;Reza Ehsani;John K. Schueller	2018	Computers and Electronics in Agriculture	10.1016/j.compag.2018.07.011	computer vision;artificial intelligence;engineering;image registration;yield mapping	Vision	34.49958006862685	-70.63927855432605	6083
69911f72e45bd9d6379bdb5a034ce22a2ae25552	efficient convex optimization approach to 3d non-rigid mr-trus registration		In this study, we propose an efficient non-rigid MR-TRUS deformable registration method to improve the accuracy of targeting suspicious locations during a 3D ultrasound (US) guided prostate biopsy. The proposed deformable registration approach employs the multi-channel modality independent neighbourhood descriptor (MIND) as the local similarity feature across the two modalities of MR and TRUS, and a novel and efficient duality-based convex optimization based algorithmic scheme is introduced to extract the deformations which align the two MIND descriptors. The registration accuracy was evaluated using 10 patient images by measuring the TRE of manually identified corresponding intrinsic fiducials in the whole gland and peripheral zone, and performance metrics (DSC, MAD and MAXD) for the apex, mid-gland and base of the prostate were also calculated by comparing two manually segmented prostate surfaces in the registered 3D MR and TRUS images. Experimental results show that the proposed method yielded an overall mean TRE of 1.74 mm, which is favorably comparable to a clinical requirement for an error of less than 2.5 mm.		Yue Sun;Jing Yuan;Martin Rajchl;Wu Qiu;Cesare Romagnoli;Aaron Fenster	2013	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-40811-3_25	computer vision;mathematical optimization;mathematics	Vision	41.31795003741682	-79.25712391211304	6090
6c3cf99f87e51cb008984028a1b174f2cf123c6a	interindividual variability and intraindividual reliability of intermittent theta burst stimulation-induced neuroplasticity mechanisms in the healthy brain		We combined patterned TMS with EMG in several sessions of a within-subject design to assess and characterize intraindividual reliability and interindividual variability of TMS-induced neuroplasticity mechanisms in the healthy brain. Intermittent theta burst stimulation (iTBS) was applied over M1 to induce long-term potentiation-like mechanisms as assessed by changes in corticospinal excitability. Furthermore, we investigated the association between the observed iTBS effects and individual differences in prolonged measures of corticospinal excitability. Our results show that iTBS-induced measures of neuroplasticity suffer from high variability between individuals within a single assessment visit and from low reliability within individuals across two assessment visits. This indicates that both group and individual effects of iTBS on corticospinal excitability cannot be assumed to be reliable and therefore need to be interpreted with caution, at least when measured by changes in the amplitudes of motor-evoked potentials.	acoustic evoked brain stem potentials;assumed;corticospinal tracts;electromyographs;electromyography;evoked potentials;excited state;heart rate variability;long-term potentiation;neuronal plasticity;pulse-width modulation;relevance;respiratory burst;structure of ventral corticospinal tract;transcranial magnetic stimulation;chemosensitization/potentiation	Lukas Schilberg;Teresa Schuhmann;Alexander Thomas Sack	2017	Journal of Cognitive Neuroscience	10.1162/jocn_a_01100	psychology;neuroscience;developmental psychology;social psychology	HCI	18.035032775609686	-79.75385624075034	6105
54032bbd0b6b9629c523a17c286d4c5f46341c25	network analysis of possible anaphylaxis cases reported to the us vaccine adverse event reporting system after h1n1 influenza vaccine		The identification of signals from spontaneous reporting systems plays an important role in monitoring the safety of medical products. Network analysis (NA) allows the representation of complex interactions among the key elements of such systems. We developed a network for a subset of the US Vaccine Adverse Event Reporting System (VAERS) by representing the vaccines/adverse events (AEs) and their interconnections as the nodes and the edges, respectively; this subset we focused upon included possible anaphylaxis reports that were submitted for the H1N1 influenza vaccine. Subsequently, we calculated the main metrics that characterize the connectivity of the nodes and applied the island algorithm to identify the densest region in the network and, thus, identify potential safety signals. AEs associated with anaphylaxis formed a dense region in the 'anaphylaxis' network demonstrating the strength of NA techniques for pattern recognition. Additional validation and development of this approach is needed to improve future pharmacovigilance efforts.		Taxiarchis Botsis;Robert Ball	2011	Studies in health technology and informatics	10.3233/978-1-60750-806-9-564	anaphylaxis;adverse event reporting system;knowledge management;medical emergency;h1n1 influenza;virology;medicine	Web+IR	-2.099053727442909	-67.6399458110956	6106
208a1a4b989cb8356af091cde50f0710684b9277	gpnn: power studies and applications of a neural network method for detecting gene-gene interactions in studies of human disease	genetic program;human disease;eskitis institute for drug discovery;faculty of science environment engineering and technology;gene environment interaction;genetic testing;chromosome mapping;genetic effect;journal article;data model;computational biology bioinformatics;diagnosis computer assisted;data analysis;genetic predisposition to disease;pattern recognition;cx;algorithms;pattern recognition automated;humans;genetic epidemiology;320702;parkinson disease;neural networks computer;combinatorial libraries;protein interaction mapping;limit of detection;computer appl in life sciences;high power;multigene family;polymorphism single nucleotide;central nervous system;gene expression profiling;neural network;environmental factor;microarrays;bioinformatics	The identification and characterization of genes that influence the risk of common, complex multifactorial disease primarily through interactions with other genes and environmental factors remains a statistical and computational challenge in genetic epidemiology. We have previously introduced a genetic programming optimized neural network (GPNN) as a method for optimizing the architecture of a neural network to improve the identification of gene combinations associated with disease risk. The goal of this study was to evaluate the power of GPNN for identifying high-order gene-gene interactions. We were also interested in applying GPNN to a real data analysis in Parkinson's disease. We show that GPNN has high power to detect even relatively small genetic effects (2–3% heritability) in simulated data models involving two and three locus interactions. The limits of detection were reached under conditions with very small heritability (<1%) or when interactions involved more than three loci. We tested GPNN on a real dataset comprised of Parkinson's disease cases and controls and found a two locus interaction between the DLST gene and sex. These results indicate that GPNN may be a useful pattern recognition approach for detecting gene-gene and gene-environment interactions.	artificial neural network;biological neural networks;computation;data model;genetic programming;hereditary diseases;interaction;locus;neural network simulation;parkinson disease;parkinsonian disorders;pattern recognition;sensor;silo (dataset)	Alison A. Motsinger-Reif;Stephen L. Lee;George Mellick;Marylyn DeRiggi Ritchie	2005	BMC Bioinformatics	10.1186/1471-2105-7-39	biology;dna microarray;genetic epidemiology;detection limit;cx;gene–environment interaction;data model;biotechnology;computer science;bioinformatics;central nervous system;gene expression profiling;data analysis;genetics	ML	6.496236083700521	-54.003424993293	6121
77949cb04ac10b91151eb6c520588d528a608738	the anesthetic propofol shifts the frequency of maximum spectral power in eeg during general anesthesia: analytical insights from a linear model	biological patents;biomedical journals;text mining;europe pubmed central;neural fields;citation search;citation networks;power spectrum;research articles;abstracts;open access;life sciences;clinical guidelines;eeg;general anaesthesia;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search;propofol	The work introduces a linear neural population model that allows to derive analytically the power spectrum subjected to the concentration of the anesthetic propofol. The analytical study of the power spectrum of the systems activity gives conditions on how the frequency of maximum power in experimental electroencephalographic (EEG) changes dependent on the propofol concentration. In this context, we explain the anesthetic-induced power increase in neural activity by an oscillatory instability and derive conditions under which the power peak shifts to larger frequencies as observed experimentally in EEG. Moreover the work predicts that the power increase only occurs while the frequency of maximum power increases. Numerically simulations of the systems activity complement the analytical results.	complement system proteins;computer simulation;electroencephalography;experiment;instability;large;linear model;maximum power transfer theorem;neural ensemble;numerical integration;parkinsonism, experimental;population model;propofol;spectral density	Axel Hutt	2013		10.3389/fncom.2013.00002	text mining;computer science;bioinformatics;data mining;anesthesia;spectral density	EDA	20.157486331425975	-83.19365012838618	6146
853c4bcdb58e5b526398e58799c19bf0123e55ec	extraction and reconstruction of yarn's characteristic by using fractal geometry	fractal geometry		fractal	Hidefumi Nakagawa;Toshihiko Tanaka;Ken'ichi Ohta	1999			machine learning;artificial intelligence;discrete mathematics;computer science;yarn;fractal dimension on networks;fractal	Vision	43.53136575936715	-69.32794096277577	6157
aa01526766a76328332b80967d45a4660b86b57c	a fuzzy system for fetal heart rate assessment	systeme intelligent;obstetrique;systeme aide decision;cardiotocografia;sistema inteligente;obstetrico;sistema ayuda decision;cardiotocography;diagnostic prenatal;decision support system;intelligent system;sistema difuso;systeme flou;fetal heart rate;fuzzy system;obstetrics;cardiotocographie	The clinical interpretation of fetal heart rate traces is a difficult task that has led to the development computerised assessment systems. These systems are limited by their inability to represent uncertainty. This paper describes the first stage in the development of a fu2zy expert system for fetal heart rate assessment. A preliminary evaluation study comparing the initial fuzzy system with three clinicians and an existing crisp expert system is presented. The fuzzy system improved on the crisp system and achieved the highest overall performance. The use of fuzzy systems for analysis of fetal heart rate traces and similar time varying signals is shown to have potential benefit.	algorithm;biological system;expert system;feature extraction;fuzzy control system;tracing (software)	Jonathan F. Skinner;Jonathan M. Garibaldi;Emmanuel C. Ifeachor	1999		10.1007/3-540-48774-3_3	decision support system;computer science;artificial intelligence	HCI	4.489606448244002	-78.94924893138854	6181
9e5307c5a8daf7ca5a8780015ce8f294ec21c4eb	memristive cellular automata for modeling of epileptic brain activity		Cellular Automata (CA) is a nature-inspired and widespread computational model which is based on the collective and emergent parallel computing capability of units (cells) locally interconnected in an abstract brain-like structure. Each such unit, referred as CA cell, performs simplistic computations/processes. However, a network of such identical cells can exhibit nonlinear behavior and be used to model highly complex physical phenomena and processes and to solve problems that are highly complicated for conventional computers. Brain activity has always been considered one of the most complex physical processes and its modeling is of utter importance. This work combines the CA parallel computing capability with the nonlinear dynamics of the memristor, aiming to model brain activity during the epileptic seizures caused by the spreading of pathological dynamics from focal to healthy brain regions. A CA-based confrontation extended to include long-range interactions, combined with the recent notion of memristive electronics, is thus proposed as a modern and promising parallel approach to modeling of such complex physical phenomena. Simulation results show the efficiency of the proposed design and the appropriate reproduction of the spreading of an epileptic seizure.		Rafailia-Eleni Karamani;Josemar G DE Oliveira Filho;Vasileios G. Ntinas;Ioannis Vourkas;Georgios Ch. Sirakoulis;Antonio Rubio	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351805	brain activity and meditation;memristor;control theory;cellular automaton;theoretical computer science;computation;computer science;nonlinear system	Metrics	16.94372321092048	-68.52607885500944	6186
ae0547aa185edb86f9a59bd86e3b65c82b7347fd	a high dimensional qsar study on the aldose reductase inhibitory activity of some flavones: topological descriptors in modeling the activity	multiple linear regression;quantitative structure activity relationship;of hydroxyl;high dimensionality;partial least square;information content;molecular descriptor;aldose reductase	The quantitative structure-activity relationships (QSAR) of the Aldose Reductase (AR) inhibitory activity of 48 flavones were studied using Free-Wilson, Combinatorial Protocol in Multiple Linear Regression (CP-MLR), and Partial Least Squares (PLS) procedures. For the latter two procedures 152 Molconn-Z parameters and six indicators corresponding to the hydroxyls of flavones were used as molecular descriptors. Independently, all procedures suggested the significance of hydroxyls in modulating the activity of these compounds. The CP-MLR procedure identified 26 descriptors to model the activity. They suggested that structures rich in aromatic CH fragments, with a limited number of aliphatic fragments such as -CH2-, -CH<, and free hydroxyls at 7-, 3'-, and 4'-positions of the 2-arylbenzpyran-4-one core would be preferred for the activity. The PLS analysis agreed with the information content and the relative significance of the descriptors identified in the CP-MLR for modeling the activity. The study offers the scope to modulate the inhibitory activity of these compounds.	aromatics;cp/cms;cp/m;cerebral palsy;epilepsies, partial;flavones;learning to rank;linear iga bullous dermatosis;molecular descriptor;papillon-lefevre disease;partial least squares regression;propionibacterium acnes;quantitative structure-activity relationship;self-information;aldose	Yenamandra S. Prabhakar;Manish K. Gupta;Nobendu Roy;Yenamandra Venkateswarlu	2006	Journal of chemical information and modeling	10.1021/ci050060u	molecular descriptor;biochemistry;stereochemistry;chemistry;self-information;linear regression;machine learning;organic chemistry;computational chemistry;quantitative structure–activity relationship;statistics	ML	12.104161279657248	-58.22956545397479	6206
d33ce95ad0580fdad2f2b7592281f4dc456d46cd	multichannel generalization of the upper-lower edge detector using ordered weighted averaging operators		A large number of methods in the edge detection literature are only prepared to deal with monochannel images, which represent the value at each pixel by means of scalar values. This fact hinders their applicability to many fields in which multichannel are common, including remote sensing or medical imagery. Very often, multichannel images have to be turned into grayscale images on which edge detection can be performed, but this is coupled to a loss of information that can be unbearable in certain scenarios. In this work we propose a technique for multichannel edge feature fusion technique that can be combined with any edge detection method using scalar edge features. In this way, we can extend edge detection methods by considering an initial phase of monochannel feature extraction followed by a subsequent phase of multichannel feature fusion. For the information fusion we make use of Ordered Weighted Averaging (OWA) operators, which are able to vary the relevance of each of the features to be aggregated depending upon their value. As an example, our proposal is tested with the Upper-Lower Edge Detector, despite it can be further combined with a wide range of edge detectors. 7	edge detection;feature extraction;grayscale;medical imaging;ordered weighted averaging aggregation operator;pixel;relevance;sensor	Carlos Guerra;Aranzazu Jurio;Humberto Bustince;Carlos Lopez-Molina	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-131110	computer vision;mathematical optimization;machine learning;mathematics	Vision	40.51043777217595	-67.64471076651186	6221
b4a57f02af01ea260feaa6b2b4ede6c5e8e5ae78	recent advances in the applications of convolutional neural networks to medical image contour detection		The fast growing deep learning technologies have become the main solution of many machine learning problems for medical image analysis. Deep convolution neural networks (CNNs), as one of the most important branch of the deep learning family, have been widely investigated for various computer-aided diagnosis tasks including long-term problems and continuously emerging new problems. Image contour detection is a fundamental but challenging task that has been studied for more than four decades. Recently, we have witnessed the significantly improved performance of contour detection thanks to the development of CNNs. Beyond purusing performance in existing natural image benchmarks, contour detection plays a particularly important role in medical image analysis. Segmenting various objects from radiology images or pathology images requires accurate detection of contours. However, some problems, such as discontinuity and shape constraints, are insufficiently studied in CNNs. It is necessary to clarify the challenges to encourage further exploration. The performance of CNN based contour detection relies on the state-of-the-art CNN architectures. Careful investigation of their design principles and motivations is critical and beneficial to contour detection. In this paper, we first review recent development of medical image contour detection and point out the current confronting challenges and problems. We discuss the development of general CNNs and their applications in image contours (or edges) detection. We compare those methods in detail, clarify their strengthens and weaknesses. Then we review their recent applications in medical image analysis and point out limitations, with the goal to light some potential directions in medical image analysis. We expect the paper to cover comprehensive technical ingredients of advanced CNNs to enrich the study in the medical image domain. 1E-mail: zizhaozhang@ufl.edu Preprint submitted to arXiv August 26, 2018 ar X iv :1 70 8. 07 28 1v 1 [ cs .C V ] 2 4 A ug 2 01 7	angular defect;artificial neural network;computer vision;contour line;convolution;convolutional neural network;deep learning;image analysis;machine learning;medical image computing;medical imaging;radiology;reflections of signals on conducting lines	Zizhao Zhang;Fuyong Xing;Hai Su;Xiaoshuang Shi;Lin Yang	2017	CoRR		artificial intelligence;machine learning;convolutional neural network;artificial neural network;deep learning;computer science;design elements and principles;computer vision	ML	31.917057145574194	-74.47689859427459	6242
84ba1cc31a6cf42827c17f8b32ecb3f4e9b9984a	respiratory signal extraction for 4d ct imaging of the thorax from cone-beam ct projections	signal extraction;cone beam	Current methods of four-dimensional (4D) CT imaging of the thorax synchronise the acquisition with a respiratory signal to restrospectively sort acquired data. The quality of the 4D images relies on an accurate description of the position of the thorax in the respiratory cycle by the respiratory signal. Most of the methods used an external device for acquiring the respiratory signal. We propose to extract it directly from thorax cone-beam (CB) CT projections. This study implied two main steps: the simulation of a set of CBCT projections, and the extraction, selection and integration of motion information from the simulation output to obtain the respiratory signal. A real respiratory signal was used for simulating the CB acquisition of a breathing patient. We extracted from CB images a respiratory signal with 96.4% linear correlation with the reference signal, but we showed that other measures of the quality of the extracted respiratory signal were required.	ct scan;chest;cone beam computed tomography;cone-beam computed tomography;extraction;map projection;pet/ct scan;patients;peripheral;projections and predictions;simulation	Simon Rit;David Sarrut;Chantal Ginestet	2005	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/11566465_69	computer vision;pathology;computer science	Robotics	43.41244513894361	-83.67250865325055	6280
055d8fb6ee90c0fbcd52bfcdfa381372b34b7117	gated myocardial perfusion spect underestimates left ventricular volumes and shows high variability compared to cardiac magnetic resonance imaging -- a comparison of four different commercial automated software packages	software;sensitivity and specificity;female;left ventricular end diastolic volume;cardiac magnetic resonance imaging;imaging three dimensional;middle aged;male;tomography emission computed single photon;left ventricular;coronary artery disease;image enhancement;mr imaging;myocardial perfusion spect;image interpretation computer assisted;perfusion imaging;imaging radiology;magnetic resonance;adult;ventricular dysfunction left;end systolic volume;reproducibility of results;software package;stroke volume;algorithms;ejection fraction;pattern recognition automated;humans;magnetic resonance imaging cine;gated blood pool imaging;software validation;false negative reactions;aged;quantitative gated spect	BACKGROUND We sought to compare quantification of left ventricular volumes and ejection fraction by different gated myocardial perfusion SPECT (MPS) programs with each other and to magnetic resonance (MR) imaging.   METHODS N = 100 patients with known or suspected coronary artery disease were examined at rest with 99 mTc-tetrofosmin gated MPS and cardiac MR imaging. Left ventricular end-diastolic volume (EDV), end-systolic volume (ESV), stroke volume (SV) and ejection fraction (EF) were obtained by analysing gated MPS data with four different programs: Quantitative Gated SPECT (QGS), GE MyoMetrix, Emory Cardiac Toolbox (ECTb) and Exini heart.   RESULTS All programs showed a mean bias compared to MR imaging of approximately -30% for EDV (-22 to -34%, p < 0.001 for all), ESV (-12 to -37%, p < 0.001 for ECTb, p < 0.05 for Exini, p = ns for QGS and MyoMetrix) and SV (-21 to -41%, p < 0.001 for all). Mean bias +/- 2 SD for EF (% of EF) was -9 +/- 27% (p < 0.01), 6 +/- 29% (p = ns), 15 +/- 27% (p < 0.001) and 0 +/- 28% (p = ns) for QGS, ECTb, MyoMetrix, and Exini, respectively.   CONCLUSIONS Gated MPS, systematically underestimates left ventricular volumes by approximately 30% and shows a high variability, especially for ESV. For EF, accuracy was better, with a mean bias between -15 and 6% of EF. It may be of value to take this into consideration when determining absolute values of LV volumes and EF in a clinical setting.	arteriopathic disease;artificial cardiac pacemaker;cerebrovascular accident;cloud fraction;coronary artery disease;diastole;ejection fraction (procedure);end diastolic volume imaging;end systolic volume imaging;entity framework;gated blood-pool imaging;gated community;heart diseases;kidney failure, chronic;logical volume management;mps (format);magnetic resonance imaging;myocardial infarction;patients;perfusion scanning;quantitation;spatial variability;stroke volume;systemverilog;tetrofosmin	Fredrik Hedeer;John Palmer;Håkan Arheden;Martin Ugander	2010		10.1186/1471-2342-10-10	verification and validation;radiology;medicine;end-diastolic volume;stroke volume;magnetic resonance imaging;perfusion scanning;nuclear medicine;end-systolic volume;ejection fraction;cardiology	Arch	37.631134907083485	-81.86490840719887	6302
210843782ca2ad7c4bb95cd81794d64acd828c8b	dynamic channels in biomolecular systems: path analysis and visualization	i 3 5 computer graphics comput geom object modeling boundary representations j 3 computer applications life and medical sciences biology and genetics;molecular biophysics biology computing computational geometry data visualisation;biology computing;cavity resonators;j 3 computer applications life and medical sciences biology and genetics;j 3 computer applications life and medical sciences cavity resonators;skin;bacteriorhodopsin proton pump dynamic channels biomolecular systems path analysis path visualization protein dynamics dynamic structures internal cavities internal channels voronoi based algorithm molecular dynamics trajectory van der waals spheres cavity structure timeline visualization tools;computational geometry;i 3 5 computer graphics comput geom object modeling 8212;data visualisation;visualization;trajectory;proteins;boundary representations;biology and genetics;retina;molecular biophysics;data visualization;i 3 5 computer graphics comput geom object modeling boundary representations;cavity resonators proteins trajectory visualization skin data visualization retina	Analysis of protein dynamics suggests that internal cavities and channels can be rather dynamic structures. Here, we present a Voronoi-based algorithm to extract the geometry and the dynamics of cavities and channels from a molecular dynamics trajectory. The algorithm requires a pre-processing step in which the Voronoi diagram of the van der Waals spheres is used to calculate the cavity structure for each coordinate set of the trajectory. In the next step, we interactively compute dynamic channels by analyzing the time evolution of components of the cavity structure. Tracing of the cavity dynamics is supported by timeline visualization tools that allow the user to select specific components of the cavity structures for detailed exploration. All visualization methods are interactive and enable the user to animate the time-dependent molecular structure together with its cavity structure. To facilitate a comprehensive overview of the dynamics of a channel, we have also developed a visualization technique that renders a dynamic channel in a single image and color-codes time on its extension surface. We illustrate the usefullness of our tools by inspecting the structure and dynamics of internal cavities in the bacteriorhodopsin proton pump.	algorithm;autostereogram;code;interactivity;molecular dynamics;path analysis (statistics);preprocessor;rendering (computer graphics);timeline;voronoi diagram	Norbert Lindow;Daniel Baum;Ana-Nicoleta Bondar;Hans-Christian Hege	2012	2012 IEEE Symposium on Biological Data Visualization (BioVis)	10.1109/BioVis.2012.6378599	computer science;theoretical computer science;computer graphics (images)	Visualization	16.12462297985504	-62.85646407044025	6317
2912b19f58adee66233c465ad25e3a060543ffe2	color features for image fingerprinting	analisis imagen;image processing;recherche image;procesamiento imagen;analogie;metric;intelligence artificielle;traitement image;similitude;histogram;empreinte digitale;image generation;histogramme;similarity;analogy;analogia;fingerprint;artificial intelligence;metrico;image analysis;huella digital;inteligencia artificial;similitud;imagen color;histograma;analyse image;similarity measure;color quantization;image couleur;metrique;color image;image retrieval	Image fingerprinting systems aim to extract unique and robust image descriptors (in analogy to human fingerprints). They search for images that are not only perceptually similar but replicas of an image generated through mild image processing operations. In this paper, we examine the use of color descriptors based on a 24-color quantized palette for image fingerprinting. Comparisons are provided between different similarity measures methods as well as regarding the use of color-only and spatial chromatic histograms.	chromatic polynomial;fingerprint (computing);image processing;palette (computing);visual descriptor	Marios A. Gavrielides;Elena Sikudová;Dimitris Spachos;Ioannis Pitas	2006		10.1007/11752912_53	image texture;fingerprint;computer vision;feature detection;image analysis;color quantization;speech recognition;similarity;color image;analogy;binary image;metric;image processing;image retrieval;computer science;artificial intelligence;similitude;histogram;mathematics	Vision	43.22597773945291	-61.61739350699648	6322
db91512026db042f09146d03ed8515d970f386e8	identifying rice grains using image analysis and sparse-representation-based classification	cultivar identification;locality constraint;image processing;machine learning;machine vision;sparse coding	A microscope system was developed for acquiring high resolution grain images of 30 rice varieties.The morphological, textural, and color traits of the rice grains were quantified using image processing.Trait discrepancies among varieties were observed and explained.Sparse-representation-based classifier was developed to identify the varieties of the grains.The classification achieved an accuracy of 89.1% and a standard deviation of 7.0%. Rice (Oryza sativa L.) is a major staple food worldwide, and is traded extensively. The objective of this study is to distinguish the rice grains of 30 varieties nondestructively using image processing and sparse-representation-based classification (SRC). SRC uses over-complete bases to capture the representative traits of rice grains. In the experiments, rice grain images were acquired by microscopy. The morphological, color, and textural traits of the grain body, sterile lemmas, and brush were quantified. An SRC classifier was subsequently developed to identify the varieties of the grains using the traits as the inputs. The proposed approach could discriminate rice grain varieties with an accuracy of 89.1%.	image analysis;sparse matrix	Tzu-Yi Kuo;Chia-Lin Chung;Szu-Yu Chen;Heng-An Lin;Yan-Fu Kuo	2016	Computers and Electronics in Agriculture	10.1016/j.compag.2016.07.020	computer vision;botany;machine vision;image processing;computer science;machine learning;agronomy;neural coding	Vision	34.84028367135837	-70.50261284296516	6340
e39cdbc2c8332ee5610b402c7008130664d07256	computer-based assessment of left ventricular regional ejection fraction in patients after myocardial infarction	heart;gadolinium;magnetism;surveillance;computing systems;diseases and disorders	After myocardial infarction (MI), the left ventricle (LV) undergoes progressive remodeling which adversely affects heart function and may lead to development of heart failure. There is an escalating need to accurately depict the LV remodeling process for disease surveillance and monitoring of therapeutic efficacy. Current practice of using ejection fraction to quantitate LV function is less than ideal as it obscures regional variation and anomaly. Therefore, we sought to (i) develop a quantitative method to assess LV regional ejection fraction (REF) using a 16-segment method, and (ii) evaluate the effectiveness of REF in discriminating 10 patients 1-3 months after MI and 9 normal control (sex- and agematched) based on cardiac magnetic resonance (CMR) imaging. Late gadolinium enhancement (LGE) CMR scans were also acquired for the MI patients to assess scar extent. We observed that the REF at the basal, mid-cavity and apical regions for the patient group is significantly lower as compared to the control group (P < 0.001 using a 2-tail student t-test). In addition, we correlated the patient REF over these regions with their corresponding LGE score in terms of 4 categories – High LGE, Low LGE, Border and Remote. We observed that the median REF decreases with increasing severity of infarction. The results suggest that REF could potentially be used as a discriminator for MI and employed to measure myocardium homogeneity with respect to degree of infarction. The computational performance per data sample took approximately 25 sec, which demonstrates its clinical potential as a real-time cardiac assessment tool. © (2014) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Soo Kng Teo;Yi Su;Ru San Tan;Liang Zhong	2014		10.1117/12.2043241	magnetism;heart;physics;quantum mechanics;gadolinium	AI	36.74341036830395	-81.2156488602754	6362
2a5e7637241584324fdaad8736626e1f57771ca4	the role of magnetic assisted capsule endoscopy (mace) to aid visualisation in the upper gi tract	mace;oesophagus;magnetic;gastrointestinal;stomach;capsule endoscope	Examination of the upper gastrointestinal tract by a standard endoscope is often thought as a daunting experience to many who have undertaken or are about to undergo the procedure. The overall perceived size of the gastroscope, unpleasantness of stimulation of the gag reflex and the need often for sedation is discouraging to many. A method to visualise the upper gastrointestinal mucosa which negates the need for sedation, the associated expensive decontamination costs and the possibility of having a community based examination would be particularly welcoming to this endoscopy field. Since the first swallow of a capsule endoscope by a human volunteer in 1999, their usage for examining the small bowel has exponentially grown to that of over a million patients worldwide. More recently, innovation in this field have shown plausibility for its use to visualise the upper gastrointestinal tract, with the integration of magnets within the capsule the most promising method.	capsule endoscopes;decontamination;endoscope device component;gastrointestinal tract structure;gastroscopes;intestines;patients;plausibility structure;reflex action;sedation procedure;small intestinal wall tissue;tract (literature);upper gastrointestinal tract;urinary tract infection	Imdadur Rahman;Nadeem Ahmad Afzal;Praful Patel	2015	Computers in biology and medicine	10.1016/j.compbiomed.2015.03.014	gastroenterology;magnet;medicine;pathology;quantum mechanics;surgery	HCI	35.63122446302628	-84.64734408706023	6364
51e1debae9a455a9761b16c6ba648f6c816e2a47	automatic assessment of eye blinking patterns through statistical shape models	automatic assessment;statistical shape model	Several studies have related the alertness of an individual to their eye-blinking patterns. Accurate and automatic quantification of eye-blinks can be of much use in monitoring people at jobs that require high degree of alertness, such as that of a driver of a vehicle. This paper presents a non-intrusive system based on facial biometrics techniques, to accurately detect and quantify eye-blinks. Given a video sequence from a standard camera, the proposed procedure can output blink frequencies and durations, as well as the PERCLOS metric, which is the percentage of the time the eyes are at least 80% closed. The proposed algorithm was tested on 360 videos of the AV@CAR database, which amount to approximately 95,000 frames of 20 different people. Validation of the results against manual annotations yielded very high accuracy in the estimation of blink frequency with encouraging results in the estimation of PERCLOS (average error of 0.39%) and blink duration (average error within 2 frames).	statistical model	Federico Sukno;Sri-Kaushik Pavani;Constantine Butakoff;Alejandro F. Frangi	2009		10.1007/978-3-642-04667-4_4	computer vision;simulation;speech recognition;computer science	ML	15.366720689463042	-86.73495774902152	6373
52c715d0a01cca11ebc266510d27d608554d9d62	object recognition and segmentation in videos by connecting heterogeneous visual features	top down method;methode descendante;teletrafic;modelizacion;bottom up and top down features;object recognition;bottom up method;vision ordenador;clutter;local descriptors;video surveillance;bottom up;metodo ascendente;image segmentation;occlusion;localizacion objeto;surveillance;edge detection;top down;interest points;occultation;object location;pertinencia;oclusion;reconnaissance objet;methode ascendante;lorry;computer vision;deteccion contorno;modelisation;detection contour;object segmentation;teletrafico;object localization;fouillis echo;vigilancia;metodo descendente;confusion eco;pertinence;image sequence;segmentation image;teletraffic;visual features;pattern recognition;vision ordinateur;secuencia imagen;reconnaissance forme;relevance;object localization and segmentation;reconocimiento patron;camion;ocultacion;modeling;localisation objet;sequence image	We present an approach for model-free and instance-level object recognition and segmentation in cluttered scenes, based on heterogeneous visual features. The first contribution of this work addresses the description of the visual appearance of objects, by proposing the joint use of complementary features of different natures: on the one hand, a set of local descriptors based on interest points that have well-known interesting properties; on the other hand, a global descriptor based on a snake, providing a high-level description of the object shape. Our second contribution consists in efficiently structuring and connecting the visual features obtained, making possible the use of global descriptors without prior segmentation/detection. Our approach is compared to a classic one based on local descriptors only and is evaluated for video surveillance purposes over sequences involving 20 objects. We show that recognition is improved, and provides precise object segmentation, even with large occlusions. A real scenario of application to video surveillance of truck traffic validates the relevance of the approach.	outline of object recognition	Valérie Gouet-Brunet;Bruno Lameyre	2008	Computer Vision and Image Understanding	10.1016/j.cviu.2007.10.004	computer vision;computer science;top-down and bottom-up design;scale-space segmentation	Vision	47.427094731515076	-58.35095236483608	6385
ecc09ab9c61dc3a3a15f55332f63bccbf443f291	cross-domain deep face matching for real banking security systems.		Ensuring the security of transactions is currently one of the major challenges facing banking systems. The usage of face for biometric authentication of users is becoming adopted worldwide due its convenience and acceptability by people, and also given that, nowadays, almost all computers and mobile devices have built-in cameras. Such user authentication approach is attracting large investments from banking and financial institutions, especially in cross-domain scenarios, in which facial images from ID documents are compared with digital selfportraits (selfies) taken with the cameras of mobile devices, for the automated opening of new checking accounts or financial transactions authorization. In this work, besides of collecting a large cross-domain face database, with 27,002 real facial images of selfies and ID documents (13,501 subjects) captured from the systems of the major public Brazilian bank, we propose a novel approach for such cross-domain face matching based on deep features extracted by two well-referenced Convolutional Neural Networks (CNN). Results obtained on the large dataset collected, which we called FaceBank, with accuracy rates higher than 93%, demonstrate the robustness of the proposed approach to the cross-domain problem (comparing faces in IDs and selfies) and its feasible application in real banking security systems.	authentication;authorization;biometrics;canonical account;computer;convolutional neural network;database;digital camera;image processing;mobile device;mobile phone;radio frequency;robustness (computer science);selfie	Johnatan S. Oliveira;Gustavo B. Souza;Anderson Rocha;Flavio E. de Deus;Aparecido Nilceu Marana	2018	CoRR		convolutional neural network;machine learning;computer security;biometrics;robustness (computer science);artificial intelligence;checking accounts;authorization;computer science;mobile device;authentication;financial transaction	Security	29.147995212983588	-62.67381917910305	6392
faf2c964b9ee2afaf169b857193c5221ea98c0a9	evaluation of biocompatibility using human craniofacial bone cells				Kathleen Emma McDougall	2001			biocompatibility;craniofacial;bone cell;biomedical engineering;medicine;text mining	ML	10.427906300062412	-79.87854900640818	6405
1f201bc37f39b8e67c84021792062b70c883ed0e	a study of genomic modules and intermodular networks for understanding the operating system of life	operating system	Higher eukaryotes’ phenotypes–the unique properties of a system composed of several genes and their relations–are entirely different from a set of properties of individual genes. During evolution, higher organisms’ genes become multipurpose and their phenotypic implication become abstract. We hypothesize that an operating system of life (OSL) exists in “genome space,” encoding a variety of phenotypes and realizing them through chemical circuits in “material space.” Genes’ generalization and abstraction explain the necessity of “genomic modules,” each of which encodes a unitary phenotype as a subroutine in the OSL. A genetic network is a projection map of the OSL’s active modules on the interface between the two spaces. We searched for genomic modules and intermodular networks developed in differentiating cells with the density matrix that mathematically defines a state of the system composed of genes and their relations, and demonstrated the modules’ relevance to the phenotypes of differentiating cells.	density matrix;gene regulatory network;open shading language;operating system;relevance;subroutine	Hye Young Kim;Jin Hyuk Kim	2010			computer engineering;computer science;systems engineering	ML	3.1963367115986916	-66.42806122683007	6413
72cb3c33c2f4b44d46161ab6b6bca6f5e6cab8e6	vein pattern indexing using texture and hierarchical decomposition of delaunay triangulation		In biometric identification systems, the identity corresponding to the query image is determined by comparing it against all images in the database. This exhaustive matching process increases the response time and the number of false positives of the system; therefore, an effective mechanism is essential to select a small collection of candidates to which the actual matching process is applied. This paper presents an efficient indexing algorithm for vein pattern databases to improve the search speed and accuracy of identification. In this work, we generate a binary code for each image using texture information. A hierarchical decomposition of Delaunay triangulation based approach for minutiae is proposed and used with binary code to narrow down the search space of the database. Experiments are conducted on two vein pattern databases, and the results show that, while maintaining 100% Hit Rate, the proposed method achieves lower penetration rate than what existing methods achieve.	delaunay triangulation	Ilaiah Kavati;Munaga V. N. K. Prasad;Chakravarthy Bhagvati	2013		10.1007/978-3-642-40576-1_21	computer vision;engineering drawing	Vision	39.90404002689031	-58.43264405075625	6418
c502b9612216c2ecf0e11d39d583df29eb9479c1	fully automating graf's method for ddh diagnosis using deep convolutional neural networks			convolutional neural network	David Golan;Yoni Donner;Chris Mansi;Jacob L. Jaremko;Manoj Ramachandran	2016		10.1007/978-3-319-46976-8_14	computer science;artificial intelligence;data mining;database	NLP	31.5968433479569	-74.43828350767137	6446
6f7e70511244725f84a8d2c3f58a7b53d6e8f222	evaluation of pilot mental workload for simulator based training using heart rate variability	interpolation;marine vehicles navigation hafnium training educational institutions heart rate sea measurements;splines mathematics;ships;computer based training;evaluated reaction pilot mental workload simulator based training heart rate variability maneuvering simulator pilot trainee practices heart rate monitor lf hf heartbeat interval data spline interpolation mem maximum entropy method ordinal reference value;entropy;heart rate mental workload pilot simulator lf hf;splines mathematics computer based training digital simulation entropy interpolation ships;digital simulation	The purpose of this study is evaluation of mental workload when pilot trainee practices of maneuvering simulator and evaluation of validity of it as substitution for real ship from its tendency. The subjects had trained for three months who were equipped heart rate monitor. I calculated LF/HF to process the heartbeat interval data using spline interpolation and MEM (maximum entropy method) and defined result data as mental workload. In addition, I set the ordinal reference value and evaluated reaction above this numerical value. I summarized the number of times that mental workload and grouped for mental workload tendency to compare for the contents of the corresponding scenario of simulator. In consequence, I analyzed their timing of mental workload and poor contents. As a result, reactions were differ from each them and had no uniformity for the timing or contents. Additionally in paradoxical, we understood that it is possible to get mental tendency for each to do simulator practice.	circuit complexity;heart rate variability;numerical analysis;ordinal data;principle of maximum entropy;simulation;spline interpolation	Masahiro Tanaka;Koji Murai;Yuji Hayashi	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.783	entropy;real-time computing;simulation;interpolation;computer science;artificial intelligence;statistics	Robotics	6.910070268887352	-81.17358504487571	6455
c114269a285466665d3f152a2dce982a7c34a659	robust image segmentation by texture sensitive snake under low contrast environment	image segmentation;image processing;conference_paper;image processing applications;medical image analysis;texture analysis;medical image;image analysis;medical diagnosis	Robust image segmentation plays an important role in a wide range of daily applications, like visual surveillance system, computer-aided medical diagnosis, etc. Although commonly used image segmentation methods based on pixel intensity and texture can help finding the boundary of targets with sharp edges or distinguished textures, they may not be applied to images with poor quality and low contrast. Medical images, images captured from web cam and images taken under dim light are examples of images with low contrast and with heavy noise. To handle these types of images, we proposed a new segmentation method based on texture clustering and snake fitting. Experimental results show that targets in both artificial images and medical images, which are of low contrast and heavy noise, can be segmented from the background accurately. This segmentation method provides alternatives to the users so that they can keep using imaging device with low quality outputs while having good quality of image analysis result.	cluster analysis;image analysis;image segmentation;medical imaging;pixel;webcam	Shu-Fai Wong;Kwan-Yee Kenneth Wong	2004			image quality;image texture;computer vision;feature detection;image analysis;color image;image gradient;binary image;image processing;computer science;segmentation-based object categorization;digital image processing;medical diagnosis;multimedia;region growing;image segmentation;scale-space segmentation;automatic image annotation;computer graphics (images)	Vision	41.024063947369456	-68.76484524151911	6456
d6af94b4c536a8b11de13c8368f1014995a59174	exploring the complexity of pathway-drug relationships using latent dirichlet allocation	bayesian inference;biological pathways;functional genomics;latent dirichlet allocation lda;genome complexity	"""Analysis of cellular responses to diverse stimuli enables the exploration in the complexity of functional genomics. Typically, high-throughput microarray data allow us to identify genes that are differentially expressed under a phenomenon of interest. To extract the meanings from the long list of those differentially expressed genes, we present a new method """"pathway-based LDA"""" to determine pathways/gene sets that are perturbed after exposure to different chemicals. In this study, a pathway is defined as a group of functionally related genes. Specifically, we have implemented a probabilistic Latent Dirichlet Allocation (LDA) model to learn drug-pathway-gene relations by taking known gene-pathway memberships as prior knowledge. We applied the pathway-based LDA model and 236 known pathways in order to determine pathway responsiveness to gene expression data of 1169 drugs. Our method yielded a better predictive performance on pathway responsiveness to drug treatments than the existing methods. Moreover, the pathway-based LDA also revealed genes contributing the most in each pre-defined pathway through a probabilistic distribution of genes. In achieving that, our method could provide a useful estimator of the pathway complexity of a genome."""	anterior descending branch of left coronary artery;contribution;functional genomics;gene expression;gene regulatory network;high-throughput computing;latent dirichlet allocation;microarray;responsiveness;throughput	Naruemon Pratanwanich;Pietro Liò	2014	Computational biology and chemistry	10.1016/j.compbiolchem.2014.08.019	functional genomics;biology;computer science;bioinformatics;machine learning;biological pathway;data mining;mathematics;bayesian inference	Comp.	5.439884830395526	-55.79436181116438	6467
5220850a9db5659f35d043c65f4ce5504554c67d	a low-dimensional, time-resolved and adapting model neuron		A low-dimensional, time-resolved and adapting model neuron is formulated and evaluated. The model is an extension of the integrate-and-fire type of model with respect to adaptation and of a recent adapting firing-rate model with respect to time-resolution. It is obtained from detailed conductance-based models by a separation of fast and slow ionic processes of action potential generation. The model explicitly includes firing-rate regulation via the slow afterhyperpolarization phase of action potentials, which is controlled by calcium-sensitive potassium channels. It is demonstrated that the model closely reproduces the firing pattern and excitability behaviour of a detailed multicompartment conductance-based model of a neocortical pyramidal cell. The inclusion of adaptation in a model neuron is important for its capability to generate complex dynamics of networks of interconnected neurons. The time-resolution is required for studies of systems in which the temporal aspects of neural coding are important. The simplicity of the model facilitates analytical studies, insight into neurocomputational mechanisms and simulations of large-scale systems. The capability to generate complex network computations may also make the model useful in practical applications of artificial neural networks.	acclimatization;action potentials;action potential;artificial neural network;biological neuron model;calcium;complex dynamics;complex network;computation;conductance (graph);ionic;neural coding;potassium channel;pyramidal cells	Bo Cartling	1996	International journal of neural systems	10.1142/S012906579600021X	computer science;artificial intelligence;machine learning	ML	18.114379004334616	-70.51628633046795	6468
e64f6cfd279cff9f061c05b8920b8b35ca8af2fe	effects of horizontal field-of-view restriction on manoeuvring performance through complex structured environments	stimulus onset asynchrony;complex structure;human behaviour;boolean query;visualization;head mounted displays;field of view;visual query;dorsal stream;evaluation;visual field;spatial information;head mounted display	Field-of-view (FOV) restrictions are known to affect human behaviour and to degrade performance for a range of different tasks. A proposed cause for this performance impairment is the predominant activation of the ventral cortical stream as compared to the dorsal stream. This may compromise the ability to control heading as well as degrade the processing of spatial information [Patterson et al. 2006]. Furthermore, the peripheral visual field is important in maintaining postural equilibrium [Turano et al. 1993]. These are all significant factors when manoeuvring through complex structured environments. We discuss here two experiments investigating the influence of horizontal FOV-restriction on manoeuvring performance through real-world structured environments. The results can help determine requirements for the selection and development of FOV limiting devices such as Head-Mounted Displays (HMDs).	course (navigation);experiment;head-mounted display;peripheral;requirement	Sander E. M. Jansen;Alexander Toet;Nico J. Delleman	2008		10.1145/1394281.1394318	computer vision;simulation;visualization;boolean conjunctive query;field of view;computer science;optical head-mounted display;evaluation;generalized complex structure;spatial analysis;optics;communication;human behavior	AI	14.939480418651616	-76.24098581717034	6499
89b08858497aefd7d3c1d91765f37c7f5085f1f5	biological and bionic hands: natural neural coding and artificial perception	touch biological hands bionic hands natural neural coding artificial perception dexterous object manipulation sensory signals motor function restoration upper limb neuroprostheses somatosensory feedback tetraplegic patient amputee prosthetic limbs arbitrary sensations sensory information contact location intracortical microstimulation primary somatosensory cortex nonhuman primates rhesus macaques percept elicitation skin neuron stimulation receptive field contact pressure natural neural code local neuron activation animals pressure discrimination task mechanical stimuli native fingers prosthetic fingers contact event timing phasic icms object contact slowly varying pressure related feedback biomimetic feedback dexterity;touch physiological artificial limbs bioelectric phenomena brain dexterous manipulators neurophysiology skin	The first decade and a half of the twenty-first century brought about two major innovations in neuroprosthetics: the development of anthropomorphic robotic limbs that replicate much of the function of a native human arm and the refinement of algorithms that decode intended movements from brain activity. However, skilled manipulation of objects requires somatosensory feedback, for which vision is a poor substitute. For upper-limb neuroprostheses to be clinically viable, they must therefore provide for the restoration of touch and proprioception. In this review, I discuss efforts to elicit meaningful tactile sensations through stimulation of neurons in somatosensory cortex. I focus on biomimetic approaches to sensory restoration, which leverage our current understanding about how information about grasped objects is encoded in the brain of intact individuals. I argue that not only can sensory neuroscience inform the development of sensory neuroprostheses, but also that the converse is true: stimulating the brain offers an exceptional opportunity to causally interrogate neural circuits and test hypotheses about natural neural coding.	algorithm;biomimetics;circuit restoration;electroencephalography;neural coding;neuroprosthetics;refinement (computing);robot;self-replicating machine;sensory neuroscience	Sliman J. Bensmaia	2015		10.1109/WHC.2015.7177674	computer science;artificial intelligence	AI	17.535640854543306	-76.00128533798143	6507
f4dcc23035aee91c50901a4937e35ac2fc6d7ac0	protein structural class prediction via k-separated bigrams using position specific scoring matrix	k separated bigram;scop;artificial intelligence and image processing not elsewhere classified;t technology general;institute for integrated and intelligent systems;faculty of science environment engineering and technology;structural class prediction;journal article;transition probabilities;svm;bigram;artificial intelligence and image processing;080199;pssm	Protein structural class prediction (SCP) is as important task in identifying protein tertiary structure and protein functions. In this study, we propose a feature extraction technique to predict secondary structures. The technique utilizes bigram (of adjacent and k-separated amino acids) information derived from Position Specific Scoring Matrix (PSSM). The technique has shown promising results when evaluated on benchmarked Ding and Dubchak dataset.	bigram;feature extraction;position weight matrix	Harsh Saini;Gaurav Raicar;Alok Sharma;Sunil Pranit Lal;Abdollah Dehzangi;Rajeshkannan Ananthanarayanan;James G. Lyons;Neela Biswas;Kuldip K. Paliwal	2014	JACIII	10.20965/jaciii.2014.p0474	support vector machine;speech recognition;computer science;artificial intelligence;machine learning;bigram;structural classification of proteins database	NLP	10.27520791355672	-53.513729687800094	6510
a382f4fed64adf8af7cc70afd8dcc2f4586d9bcd	background adjustment for dna microarrays using a database of microarray experiments	background adjustment;saccharomyces cerevisiae;dna probes;databases nucleic acid;bias epidemiology;gene expression;high density oligonucleotide microarrays;likelihood functions;gene expression regulation;roc curve;humans;dna microarray;preprocessing;gene expression profiling;oligonucleotide array sequence analysis;organ specificity	DNA microarrays have become an indispensable technique in biomedical research. The raw measurements from microarrays undergo a number of preprocessing steps before the data are converted to the genomic level for further analysis. Background adjustment is an important step in preprocessing. Estimating background noise has been challenging because background levels vary a lot from probe to probe, yet there are limited observations on each probe. Most current methods have used the empirical Bayes approach to borrow information across probes on the same array. These approaches shrink the background estimate for either the entire sample or probes sharing similar sequence structures. In this article, we present a solution that is truly probe specific by using a database of large number of microarray experiments. Information is borrowed across samples and background noise is estimated for each probe individually. The ability to obtain probe specific background distributions allows us to extend the dynamic range of gene expression levels. We illustrate the improvement in detecting gene expression variation on two datasets: a Latin Square spike-in experiment from Affymetrix and an Estrogen Receptor experiment with biological replicates. An R package dbRMA implementing our method can be obtained from the authors.	affymetrix;bundle adjustment;dna microarray;dynamic range;estimated;estrogen receptors;estrogens;experiment;gene expression;preprocessor;sensor	Yunxia Sui;Xiaoyue Zhao;Terence P. Speed;Zhijin Wu	2009	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2009.0063	biology;hybridization probe;molecular biology;regulation of gene expression;gene expression;dna microarray;bioinformatics;gene expression profiling;genetics;preprocessor;receiver operating characteristic	Comp.	3.512665228304687	-53.81680825667858	6518
487089350fd0429754d3b0ae053dfceb7cf5b538	findsite: a combined evolution/structure-based approach to protein function prediction	evolution moleculaire;ligando;software;prediccion;criblage;virtual ligand screening;high resolution;protein function;proteine;ligand binding site prediction;ligands;screening;low resolution;low resolution protein structures;ligand binding;binding site;fonction structure;binding sites;fixation ligand;structure function;site fixation;protein structure;evolucion molecular;models molecular;protein function prediction;structure activity relationship;proteins;protein conformation;molecular evolution;ligand;protein structure prediction;cernido;algorithms;protein folding;proteina;molecular sequence data;function prediction;protein;fijacion ligando;sitio fijacion;prediction;funcion estructura;sequence analysis protein	A key challenge of the post-genomic era is the identification of the function(s) of all the molecules in a given organism. Here, we review the status of sequence and structure-based approaches to protein function inference and ligand screening that can provide functional insights for a significant fraction of the approximately 50% of ORFs of unassigned function in an average proteome. We then describe FINDSITE, a recently developed algorithm for ligand binding site prediction, ligand screening and molecular function prediction, which is based on binding site conservation across evolutionary distant proteins identified by threading. Importantly, FINDSITE gives comparable results when high-resolution experimental structures as well as predicted protein models are used.	image resolution;inference;ligands;open reading frames;open reading frame;protein function prediction;protein, organized by function;thread (computing);unassigned dosage form;algorithm;molecular_function	Jeffrey Skolnick;Michal Brylinski	2009	Briefings in bioinformatics	10.1093/bib/bbp017	biology;biochemistry;protein structure;molecular biology;image resolution;bioinformatics;binding site;ligand	Comp.	2.546975676744994	-59.70990810947882	6529
3be4bf3946b9a5f2f0876b25891bcac9ed755f31	utilizing visualization technology in medical education	databases;genomics;human computer interaction;compounds;computer aided instruction;biological pathways;web services bioinformatics biomedical education computer aided instruction data visualisation human computer interaction user interfaces;data visualisation;visualization;compounds databases genomics education bioinformatics graphics;web services;biomedical education;similar reaction sequences visualization technology medical education bioinformatics database kegg kyoto encyclopedia of genes and genomes user friendly interface interactive interface life sciences education field web service system exhaustive search biological pathway;human machine interaction visualization biological pathways;user interfaces;graphics;human machine interaction;bioinformatics	Although existing bioinformatics databases such as KEGG (Kyoto Encyclopedia of Genes and Genomes) provide a wealth of information, they generally lack a user-friendly and interactive interface. As a result, they are of only limited use in the life sciences education field. Accordingly, the present study proposes a web service system for exploring the contents of the KEGG database in an intuitive and interactive manner. The two-dimensional KEGG pathways are transformed to a three-dimensional format, thereby improving the readability of the entries and annotations. The system supports two basic functions, namely an exhaustive search for all possible reaction paths between two specified genes in a biological pathway, and the identification of similar reaction sequences in different biological pathways.	bioinformatics;brute-force search;database;gene regulatory network;kegg;list of biological databases;usability;user interface;web service	Zong-Xian Yin;Sin-Yan Li	2012	7th International Conference on Communications and Networking in China	10.1109/ChinaCom.2012.6417565	web service;genomics;visualization;human–computer interaction;computer science;bioinformatics;graphics;biological pathway;user interface;world wide web	Visualization	-3.4808440661520947	-60.204457587666234	6542
592aacb6c81b96d2e19c119801828ac871ec9c63	clonality inference from single tumor samples using low coverage sequence data		Inference of intra-tumor heterogeneity can provide valuable insight into cancer evolution. Somatic mutations detected by sequencing can help estimate the purity of a tumor sample and reconstruct its subclonal composition. While several methods have been developed to infer intra-tumor heterogeneity, the majority of these tools rely on variant allele frequencies as estimated via ultra-deep sequencing from multiple samples of the same tumor. In practice, obtaining sequencing data from a large number of samples per patient is only feasible in a few cancer types such as liquid tumors, or in rare cases involving solid tumors selected for research. We introduce CTPsingle, which aims to infer the subclonal composition using low-coverage sequencing data from a single tumor sample. We show that CTPsingle is able to infer the purity and the clonality of single-sample tumors with high accuracy even restricted to a coverage depth of (sim )30x.		Nilgun Donmez;Salem Malikic;Alexander W Wyatt;Martin E. Gleave;Colin Collins;Süleyman Cenk Sahinalp	2016		10.1007/978-3-319-31957-5_6	bioinformatics;data mining;statistics	Vision	3.4680207661080207	-53.85099615378081	6550
022027ce7475029f78fb3cdb758d947a8978ab5c	invariant visual object recognition: biologically plausible approaches	animals;models neurological;learning;invariant representations;qa76 electronic computers computer science computer software;neural inhibition;visual pathways;inferior temporal visual cortex;trace learning rule;nerve net;imagination;humans;neurons;photic stimulation;visual object recognition;action potentials;computer simulation;visual cortex;hmax;visnet;pattern recognition visual	Key properties of inferior temporal cortex neurons are described, and then, the biological plausibility of two leading approaches to invariant visual object recognition in the ventral visual system is assessed to investigate whether they account for these properties. Experiment 1 shows that VisNet performs object classification with random exemplars comparably to HMAX, except that the final layer C neurons of HMAX have a very non-sparse representation (unlike that in the brain) that provides little information in the single-neuron responses about the object class. Experiment 2 shows that VisNet forms invariant representations when trained with different views of each object, whereas HMAX performs poorly when assessed with a biologically plausible pattern association network, as HMAX has no mechanism to learn view invariance. Experiment 3 shows that VisNet neurons do not respond to scrambled images of faces, and thus encode shape information. HMAX neurons responded with similarly high rates to the unscrambled and scrambled faces, indicating that low-level features including texture may be relevant to HMAX performance. Experiment 4 shows that VisNet can learn to recognize objects even when the view provided by the object changes catastrophically as it transforms, whereas HMAX has no learning mechanism in its S–C hierarchy that provides for view-invariant learning. This highlights some requirements for the neurobiological mechanisms of high-level vision, and how some different approaches perform, in order to help understand the fundamental underlying principles of invariant visual object recognition in the ventral visual stream.	encode;face;high- and low-level;neuron;neurons;outline of object recognition;physical object;plausibility structure;requirement;sparse approximation;sparse matrix;temporal lobe	Leigh Robinson;Edmund T. Rolls	2015		10.1007/s00422-015-0658-2	psychology;computer simulation;computer vision;neuroscience;computer science;machine learning;imagination;communication	ML	20.76736388605113	-67.51606960445481	6551
9edef3c025f8eaebf02ecf36256977624b10135c	local phase-context for face recognition under varying conditions	fourier transform;face recognition	In this paper, we address the problem of face recognition of low-resolution images under varying light, illumination and blur using local texture based face representation. The main contribution is the texture representation using Phase-Context which is based on four-quadrant mask of the Fourier transform phase in local neighborhoods.#R##N##R##N#The contextual phase generates a more discriminative code filtering responses, and a more effective feature set than the Local Phase Quantization (LPQ) descriptor which is suffering from the influence of the noisy filter responses, the order relation breakdown of the generated codes, and the discretization effect of the quantization.#R##N##R##N#The experimental results on CMU-PIE, extended YALE-B and CAS-PEAL-R1 databases show that the Phase-Context methodology is more descriptive than LPQ, outperforming the widely used Local Binary Pattern (LBP), and Histogram of oriented Gradients (HOG).	facial recognition system	Mohamed Dahmane;Langis Gagnon	2014		10.1016/j.procs.2014.11.004	computer vision;local binary patterns;machine learning;pattern recognition;mathematics	Vision	35.37640386721727	-58.47670591600782	6568
950c2034ba71bd4141527585edbad1e67c914b08	an iterative reconstruction for poly-energetic x-ray computed tomography	poly energetic;iterative reconstruction;beam hardening	A beam-hardening effect is a common problem affecting the quantitative ability of X-ray computed tomography. We develop a statistical reconstruction for a poly-energetic model, which can effectively reduce beam-hardening effects. A phantom test is used to evaluate our approach in comparison with traditional correction methods. Unlike previous methods, our algorithm utilizes multiple energy-corresponding blank scans to estimate attenuation map for a particular energy spectrum. Therefore, our algorithm has an energy-selective reconstruction. In addition to the benefits of other iterative reconstructions, our algorithm has the advantage in no requirement for prior knowledge about object material, energy spectrum of source and energy sensitivity of the detector. The results showed an improvement in the coefficient of variation, uniformity and signal-to-noise ratio demonstrating better beam hardening correction in our approach.	ct scan;iterative method;iterative reconstruction;tomography	Ho-Shiang Chueh;Wen-Kai Tsai;Chih-Chieh Chang;Shu-Ming Chang;Kuan-Hao Su;Jyh-Cheng Chen	2007		10.1007/978-3-540-79490-5_7	iterative reconstruction;materials science;mathematical optimization;nuclear medicine;medical physics	Vision	47.16798451555725	-84.10662645220427	6597
cfad4641e5f0c03e9fe7793e0aa353467fca6b1a	automated template-based pet region of interest analyses in the aging brain	software;female;brain;male;aging;image processing computer assisted;positron emission tomography;data analysis;healthy subjects;brain mapping;data extraction;region of interest;magnetic resonance imaging;humans;aged	The definition of regions of interest for PET data analysis poses a number of complex problems. While studies have shown that regions drawn on a template can be appropriate for extracting data for normal healthy subjects, it is unclear how these results can be applied to different populations. In this study, we focused on the aging population and examined how different parameters in the template data-extraction process may affect the accuracy of the results. We first present an automated method for extracting PET counts using a region-of-interest approach within a template framework. Then, we discuss two studies in which we measure the effects of varying specific parameters in this process. In study 1 we examined three parameters that may influence this process: choice of template, region, and threshold. In study 2 we focused on the hippocampus. We considered 6 different templates, and examined how well the subject-specific hippocampal masks overlapped with each other and with the template hippocampal masks after normalization. While the data in the older cohort are more variable than the normal population, the results suggest that using an appropriate template and selecting the correct parameters for the template-based ROI method can provide template-extracted counts that are highly correlated to counts extracted using subject-specific ROIs.	clinical use template;extraction;masks;polyethylene terephthalate;population;region of interest	Felice T. Sun;Roberta A. Schriber;Joel M. Greenia;Jiawei He;Amy Gitcho;William J. Jagust	2007	NeuroImage	10.1016/j.neuroimage.2006.09.022	psychology;computer vision;radiology;medicine;pathology;magnetic resonance imaging;data analysis;brain mapping;region of interest	HCI	23.273434609261912	-80.56872167591942	6599
3c271a50a50de929d08f3fbbb95c53683fa09545	a unified model of gmrf and mog for image segmentation	image segmentation;gaussian processes;image segmentation gaussian processes markov random fields parameter estimation feature extraction probability density function density functional theory mathematical model equations image processing;texture segmentation;expectation maximisation algorithm image segmentation image texture gaussian processes markov processes random processes;gauss markov random field;image texture;unified model;em algorithm mixture of gaussian gauss markov random field image segmentation textured image;expectation maximization algorithms image segmentation texture segmentation mixture of gaussian models gauss markov random field;expectation maximization;random processes;mixture of gaussians;markov processes;parameter estimation;em algorithm;expectation maximisation algorithm	In texture segmentation, features must be firstly extracted in the mixture-of-Gaussian (MOG) models. In this paper, we combine MOG model with Gauss Markov random field (GMRF) model and get a unification model. This unified model takes interaction coefficients of neighbor pixels as parameters. We derivate a set of parameters estimation equations by expectation-maximization (EM) algorithms and apply them to a two-class texture segmentation problem. Experimental results show the efficiencies and strengths of the model.	coefficient;estimation theory;expectation–maximization algorithm;image segmentation;markov chain;markov random field;pixel;unification (computer science);unified model	Yu Peng;Tong Xing-Wei;Feng Ju-Fu	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530598	stochastic process;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mathematics;statistics	Robotics	50.6012115251065	-69.7418965624996	6605
b57e661a3d0da21d0379d789bc634cb61fafb098	in silico design of mhc class i high binding affinity peptides through motifs activation map	convolutional neural network;design new peptides with high binding affinity to mhc-i molecule;motifs activation map	Finding peptides with high binding affinity to Class I major histocompatibility complex (MHC-I) attracts intensive research, and it serves a crucial part of developing a better vaccine for precision medicine. Traditional methods cost highly for designing such peptides. The advancement of computational approaches reduces the cost of new drug discovery dramatically. Compared with flourishing computational drug discovery area, the immunology area lacks tools focused on in silico design for the peptides with high binding affinity. Attributed to the ever-expanding amount of MHC-peptides binding data, it enables the tremendous influx of deep learning techniques for modeling MHC-peptides binding. To leverage the availability of these data, it is of great significance to find MHC-peptides binding specificities. The binding motifs are one of the key components to decide the MHC-peptides combination, which generally refer to a combination of some certain amino acids at certain sites which highly contribute to the binding affinity. In this work, we propose the Motif Activation Mapping (MAM) network for MHC-I and peptides binding to extract motifs from peptides. Then, we substitute amino acid randomly according to the motifs for generating peptides with high affinity. We demonstrated the MAM network could extract motifs which are the features of peptides of highly binding affinities, as well as generate peptides with high-affinities; that is, 0.859 for HLA-A*0201, 0.75 for HLA-A*0206, 0.92 for HLA-B*2702, 0.9 for HLA-A*6802 and 0.839 for Mamu-A1*001:01. Besides, its binding prediction result reaches the state of the art. The experiment also reveals the network is appropriate for most MHC-I with transfer learning. We design the MAM network to extract the motifs from MHC-peptides binding through prediction, which are proved to generate the peptides with high binding affinity successfully. The new peptides preserve the motifs but vary in sequences.		Zhoujian Xiao;Yuwei Zhang;Runsheng Yu;Yin Chen;Xiaosen Jiang;Ziwei Wang;Shuaicheng Li	2018		10.1186/s12859-018-2517-3		Comp.	9.111517414999323	-56.545180192614886	6610
5a3a45b650ea2cf14bbe2a59f21f76829d2d1ae3	unsupervised learning of sensor topologies for improving activity recognition in smart environments	unsupervised learning;digital signal processing;meta learning;activities of daily life;machine learning;smart homes;activity recognition	There has been significant recent interest in sensing systems and ‘smart environments’, with a number of longitudinal studies in this area. Typically the goal of these studies is to develop methods to predict, at any one moment of time, the activity or activities that the resident(s) of the home are engaged in, which may in turn be used for determining normal or abnormal patterns of behaviour (e.g. in a health-care setting). Classification algorithms, such as Conditional Random Fields (CRFs), typically consider sensor activations as features but these are often treated as if they were independent, which in general they are not. Our hypothesis is that learning patterns based on combinations of sensors will be more powerful than single sensors alone. The exhaustive approach – to take all possible combinations of sensors and learn classifier weights for each combination – is clearly computationally prohibitive. We show that through the application of signal processing and informationtheoretic techniques we can learn about the sensor topology in the home (i.e. learn an adjacency matrix) which enables us to determine the combinations of sensors that will be useful for classification ahead of time. As a result we can achieve classification performance better than that of the exhaustive approach, whilst only incurring a small cost in terms of computational resources. We demonstrate our results on several datasets, showing that our method is robust in terms of variations in the layout and the number of residents in the house. Furthermore, we have incorporated the adjacency matrix into the CRF learning framework and have shown that it can improve performance over multiple baselines.	activity recognition;adjacency matrix;algorithm;computation;computational resource;conditional random field;robustness (computer science);sensor;signal processing;smart tv;smart environment;unsupervised learning;while	Niall Twomey;Tom Diethe;Ian Craddock;Peter A. Flach	2017	Neurocomputing	10.1016/j.neucom.2016.12.049	unsupervised learning;computer science;artificial intelligence;digital signal processing;machine learning;data mining;activity recognition	ML	4.0173791459833135	-84.51922452793782	6625
af84aa1697b3a574d5c81d3eb54f88ce1725cceb	the detection of dna-binding proteins by protein blotting	dna;chromatography gel;transcriptional repression;dna binding proteins;western blotting;electrophoresis polyacrylamide gel;filters;binding molecular function;dna helicases;histones;rna;nucleoproteins;carrier proteins;humans;rna viral;gel;iodine 125;hela cells;dna binding protein	"""A method, called """"protein blotting,"""" for the detection of DNA-binding proteins is described. Proteins are separated on an SDA-polyacrylamide gel. The gel is sandwiched between 2 nitrocellulose filters and the proteins allowed to diffuse out of the gel and onto the filters. The proteins are tightly bound to each filter, producing a replica of the original gel pattern. The replica is used to detect DNA-binding proteins, RNA-binding proteins or histone-binding proteins by incubation of the filter with [32P]DNA, [125I]RNA, or [125I] histone. Evidence is also presented that specific protein-DNA interactions may be detected by this technique; under appropriate conditions, the lac repressor binds only to DNA containing the lac operator. Strategies for the detection of specific protein-DNA interactions are discussed."""	dna binding site;histones;interaction;lac repressors;strand displacement amplification;transcription repressor/corepressor;western blotting;polyacrylamide gels	B. Bowen;J. Steinberg;U. K. Laemmli;Herschel J. R. Weintraub	1980	Nucleic acids research	10.1093/nar/8.1.1	biology;biochemistry;dna-binding protein;molecular biology;blot;genetics	Comp.	4.459599350831679	-63.80541499997593	6645
a8a481a69d3417eec48e592224a8025a3e465311	current and emerging technology for continuous glucose monitoring	mini invasive;implanted devices;non invasive;continuous glucose monitoring;article;glucose biosensor	Diabetes has become a leading cause of death worldwide. Although there is no cure for diabetes, blood glucose monitoring combined with appropriate medication can enhance treatment efficiency, alleviate the symptoms, as well as diminish the complications. For point-of-care purposes, continuous glucose monitoring (CGM) devices are considered to be the best candidates for diabetes therapy. This review focuses on current growth areas of CGM technologies, specifically focusing on subcutaneous implantable electrochemical glucose sensors. The superiority of CGM systems is introduced firstly, and then the strategies for fabrication of minimally-invasive and non-invasive CGM biosensors are discussed, respectively. Finally, we briefly outline the current status and future perspective for CGM systems.	cessation of life;diabetes mellitus;glucose metabolism disorders;hematological disease;implants;sensor (device)	Cheng Chen;Xueling Zhao;Zhan-Hong Li;Zhigang Zhu;Shao-Hong Qian;Andrew J. Flewitt	2017		10.3390/s17010182	biological engineering	HCI	9.745015987750122	-81.86784794453393	6646
574f7a279e09ec8deebaae4b607eb581ea9be788	hierarchical model based human motion tracking	hierarchical model based human motion tracking;cluster algorithm;boundary algorithm;image recognition;pattern clustering;clustering algorithm;video signal processing;biological system modeling;motion estimation;robert operator;x y coordinate;inner area;color model;data mining;self adaptation;image sequence based tracking;brightness;color block information;x y coordinate hierarchical model based human motion tracking image sequence based tracking color model color block information inner area boundary algorithm robert operator clustering algorithm self adaptation rough area;image colour analysis;feature extraction;human motion;image sequence;clustering algorithms;image recognition tracking motion estimation image sequences pattern clustering image colour analysis video signal processing feature extraction;rough area;artificial intelligence;visual perception;humans;hierarchical model;tracking;humans tracking biological system modeling brightness clustering algorithms artificial intelligence visual perception laboratories image sequences data mining;image sequences	86-571-7951853 panyh@sun.zju.edu.cn Abstract Image sequence based tracking is the pivotal technique of human motion. In the paper, we first propose an appropriate human model and color model. Second, two approaches are proposed aiming at two different levels of color-block information including the boundary and the inner-area: the extraction of boundary algorithm is based on Robert operator and clustering algorithm is based on self-adaptation. Third, we unite two regions, which are processed by different approaches. This step counteracts the ambiguous of information obtaining from each level. Finally, after the rough area of color-block is obtained, the boundary of color-block is determined by calculating the histogram of the X, Y coordinate of every point on the boundary of block. The experiment result is presented at the end of the paper.	algorithm;cluster analysis;hierarchical database model;kinesiology	Yueting Zhuang;Qiang Zhu;Yunhe Pan	2000		10.1109/ICIP.2000.899301	computer vision;computer science;machine learning;pattern recognition;cluster analysis	Robotics	40.92450817827022	-52.41316279798891	6660
1917a78aa99d3cf074a4ddddd916cbb0c5aee8c8	finite automata based compression of bi-level images	image coding;lossy compression;vector quantization finite automata based compression bi level images specification inference algorithm lossy compression system;specification;inference mechanisms;vector quantization;bi level images;finite automata based compression;inference mechanisms finite automata vector quantisation image coding;finite automata;vector quantizer;automata image coding inference algorithms image resolution decoding computer science vector quantization image recognition image quality gray scale;inference algorithm;vector quantisation;lossy compression system	We introduce generalized finite automata as a tool for specification of bi-level images. We describe an inference algorithm for generalized finite automata and a lossy compression system for bi-level images based on this algorithm and vector quantization.	automaton;finite-state machine	Karel Culik;Vladimir Valenta	1996		10.1109/DCC.1996.488333	data compression;lossy compression;discrete mathematics;quantum finite automata;computer science;theoretical computer science;machine learning;mathematics;lossless compression;fractal transform;finite-state machine;specification;vector quantization;algorithm;statistics	Logic	52.492686428621894	-67.18968736944754	6661
966a96be81c9e9ed01aa8c50870c0356e7308836	effect of glycosylation on an immunodominant region in the v1v2 variable domain of the hiv-1 envelope gp120 protein	biochemical simulations;peptides;hiv 1;disulfide bonds;basic biological sciences biological science;antigens;biological science;glycosylation;entropy;basic biological sciences;antibodies	Heavy glycosylation of the envelope (Env) surface subunit, gp120, is a key adaptation of HIV-1; however, the precise effects of glycosylation on the folding, conformation and dynamics of this protein are poorly understood. Here we explore the patterns of HIV-1 Env gp120 glycosylation, and particularly the enrichment in glycosylation sites proximal to the disulfide linkages at the base of the surface-exposed variable domains. To dissect the influence of glycans on the conformation these regions, we focused on an antigenic peptide fragment from a disulfide bridge-bounded region spanning the V1 and V2 hyper-variable domains of HIV-1 gp120. We used replica exchange molecular dynamics (MD) simulations to investigate how glycosylation influences its conformation and stability. Simulations were performed with and without N-linked glycosylation at two sites that are highly conserved across HIV-1 isolates (N156 and N160); both are contacts for recognition by V1V2-targeted broadly neutralizing antibodies against HIV-1. Glycosylation stabilized the pre-existing conformations of this peptide construct, reduced its propensity to adopt other secondary structures, and provided resistance against thermal unfolding. Simulations performed in the context of the Env trimer also indicated that glycosylation reduces flexibility of the V1V2 region, and provided insight into glycan-glycan interactions in this region. These stabilizing effects were influenced by a combination of factors, including the presence of a disulfide bond between the Cysteines at 131 and 157, which increased the formation of beta-strands. Together, these results provide a mechanism for conservation of disulfide linkage proximal glycosylation adjacent to the variable domains of gp120 and begin to explain how this could be exploited to enhance the immunogenicity of those regions. These studies suggest that glycopeptide immunogens can be designed to stabilize the most relevant Env conformations to focus the immune response on key neutralizing epitopes.	antigens;basal ganglia diseases;charge (electrical);clade;computation;computational biology;computer simulation;conserved sequence;cysteine;cytology;distance;disulfide linkage;efimov state;encapsulated postscript;entity name part qualifier - adopted;env;ephrin type-b receptor 1, human;epitopes;exhibits as topic;file spanning;frame (physical object);gene ontology term enrichment;glycopeptides;hiv envelope protein gp120;hiv-1;htlv-i antibodies;human immunodeficiency virus 2 (hiv-2);hyperactive behavior;interaction;internet backbone;like button;linkage (software);molecular dynamics;parallel tempering;peptide fragments;personnameuse - assigned;polysaccharides;primates;protein glycosylation;protomers;snapshot (computer storage);spatial variability;strand (programming language);subtype (attribute);tracer;unfolding (dsp implementation);v3 loop;vaccine design;vertebral column;xfig;free energy;genetic linkage;newton	Jeff Tian;Cesar A. López;Cynthia A. Derdeyn;Morris S. Jones;Abraham Pinter;Bette T. Korber;S. Gnanakaran	2016		10.1371/journal.pcbi.1005094	biology;biochemistry;entropy;molecular biology;bioinformatics;antigen;antibody;disulfide bond	HCI	7.564843505392175	-63.00960413017214	6666
f63e83b367a21fe8a3c4a11d83b60601fb7ae25f	smoothing of digital images using the concept of diffusion process	iterative method;quality label;image numerique;proceso difusion;marque qualite;algoritmo adaptativo;adaptive filtering;filtrado adaptable;processus diffusion;isotropic diffusion;anisotropic diffusion;topography index;metodo iterativo;diffusion isotrope;adaptive algorithm;algorithme adaptatif;methode iterative;smoothing;difusion isotropica;image quality;indexation;imagen numerica;alisamiento;marca calidad;adaptive smoothing;filtrage adaptatif;diffusion process;quality index;digital image;pepper;lissage	-An adaptive smoothing algorithm has been described which is capable of performing various tasks, such as removing salt and pepper noise, preserving roof edges, stretching (enhancing) step edges and reducing variations in low intensity varied regions. While iteration advances, it approximates both isotropic and anisotropic heat diffusion processes in performing these tasks. A region topography index has been defined for guiding the algorithm under different situations. Further, an image quality index is proposed which provides a criterion for automatic termination of the algorithm. This criterion can also be used with other iterative smoothing algorithms. The superiority of the method over some other similar techniques has been established for both synthetic and real images. Adaptive smoothing lsotropic/anisotropic diffusion Edge stretching Quality index. Topography index	algorithm;anisotropic diffusion;digital image;image quality;inverted index;iteration;salt (cryptography);salt-and-pepper noise;smoothing;synthetic intelligence;topography	Sambhunath Biswas;Nikhil R. Pal;Sankar K. Pal	1996	Pattern Recognition	10.1016/0031-3203(94)00093-X	image quality;adaptive filter;computer vision;mathematical optimization;simulation;computer science;artificial intelligence;diffusion process;mathematics;iterative method;anisotropic diffusion;digital image;smoothing	Vision	46.99034222741354	-64.27567703445446	6675
5e464178bce1516eb6b937a2c5f6be17f4355fd1	video segmentation using vector-valued diffusion and clustering	moving object;cluster algorithm;image region labeling video segmentation vector valued diffusion oversegmentation problem alleviation image simplification color clustering discrete three dimensional diffusion model agglomerative hierarchical clustering look up table moving object extraction;look up table;pattern clustering;image segmentation;image segmentation clustering algorithms resistors labeling filters smoothing methods equations table lookup mpeg 4 standard mathematical model;image sequences video coding image segmentation pattern clustering table lookup;video segmentation;three dimensional;statistical properties;video coding;number of clusters;table lookup;diffusion model;image sequences	In this paper, we propose a user-assisted video segmentation algorithm based on color information to alleviate oversegmentation problems. We perform intra-frame segmentation by image simplification, region labeling, and color clustering. In this paper, we also present a discrete three dimensional diffusion model for easy implementation. The statistical property of each labeled region is used to estimate the number of total clusters, and agglomerative hierarchical clustering is performed with the estimated number of clusters. Since the proposed clustering algorithm counts each region as a unit, it does not generate oversegmentation problems along region boundaries. For inter-frame segmentation, we employ a look-up table for foreground color clusters, track the foreground regions, and utilize those information to extract moving objects.	algorithm;cluster analysis;connected-component labeling;hierarchical clustering;intra-frame coding;level of detail;lookup table	Daehee Kim;Chung-Hyun Ahn;Yo-Sung Ho	2003		10.1109/ICIP.2003.1247131	correlation clustering;three-dimensional space;computer vision;lookup table;fuzzy clustering;computer science;machine learning;segmentation-based object categorization;pattern recognition;diffusion;mathematics;region growing;image segmentation;cluster analysis;single-linkage clustering;scale-space segmentation	Vision	47.82727769387089	-66.72522382439116	6678
4d9cc41a250af1284092a21dde250af7f901dfc5	fall detection without people: a simulation approach tackling video data scarcity		Abstract We propose an intelligent system to detect human fall events using a physics-based myoskeletal simulation, detecting falls by comparing the simulation with a fall velocity profile using the Hausdorff distance. Previous methods of fall detection are trained using recordings of acted falls which are limited in number, body variability and type of fall and can be unrepresentative of real falls. The paper demonstrates that the use of fall recordings are unnecessary for modelling the fall as the simulation engine can produce a variety of fall events customised to an individual’s physical characteristics using myoskeletal models of different morphology, without pre-knowledge of the falling behaviour. To validate this methodological approach, the simulation is customised by the person’s height, modelling a rigid fall type. This approach allows the detection to be tailored to cohorts in the population (such as the elderly or the infirm) that are not represented in existing fall datasets. The method has been evaluated on several publicly available datasets which show that our method outperforms the results of previously reported research in fall detection. Finally, our approach is demonstrated to be robust to occlusions that hide up to 50% of a fall, which increases the applicability of automatic fall detection in a real-world environment such as the home.	simulation	Georgios Mastorakis;Tim J. Ellis;Dimitrios Makris	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.06.019	machine learning;scarcity;data mining;hausdorff distance;artificial intelligence;population;computer science	Vision	2.029270847976817	-84.45189445256418	6717
03f2bd94ffc7e160449458ce9b0c706eba5b379c	classification of neuron sets from non-disease states using time series obtained through nonlinear analysis of the 3d dendritic structures		The nonlinear dynamic analysis of time series is a powerful tool which has extended its application to many branches of scientific research. Topological equivalence is one of the main concepts that sustain theoretically the nonlinear dynamics procedures that have been implemented to characterize the discrete time series. Based on this concept, in this work a novel way to analyze dendritic trees with high complexity is reported, using features obtained through splitting the 3D structure of the dendritic trees of traced neurons into time series. Digitally reconstructed neurons were separated into control and pathological sets, which are related to two categories of alterations caused by the reduced activity of the adult born neurons (ABNs) in the mouse olfactory bulb. In the first category, a viral vector encoding a small interfering RNA (siRNA) to knock-down sodium channel expression and a second category a naris occlusion (NO) method is applied to reduce the activity of ABNs that migrate to the olfactory bulb. Using the method proposed in this study the mean result of the correct classification was improved in 4.8 and 2.76% for the NO and siRNA sets respectively, while the maximum correct classification rates were improved in 9.53 and 2.5% respectively, when compared to methods based in the use of morphological features.	dendritic spine;neuron;time series	Leonardo Agustín Hernández-Pérez;José Daniel López-Cabrera;Rubén Orozco-Morales;Juan V. Lorenzo-Ginori	2018		10.1007/978-3-030-01132-1_2	discrete time and continuous time;equivalence (measure theory);encoding (memory);nonlinear system;small interfering rna;sodium channel;olfactory bulb;neuron;biological system;mathematics	ML	13.607164115793422	-69.72619448966239	6718
a4b021dc4379a00ae35ed0b7abc6a9c60d491c34	very low frequency modulation in qrs slopes and its relation with respiration and heart rate variability during hemodialysis	patient care;patient care electrocardiography medical signal processing;electrocardiography;frequency 0 01 hz to 0 03 hz very low frequency modulation qrs slopes heart rate variability vlf modulation ecg derived respiration hemodialysis patients ordinary coherence partial coherence;coherence heart rate variability frequency modulation electrocardiography frequency domain analysis;medical signal processing	In this work, we study the very low frequency (VLF) modulation (range 0.01-0.03 Hz) in QRS slopes, heart rate variability (HRV) and ECG-derived respiration in hemodialysis patients. First, the relation between QRS slopes and HRV in the VLF band is measured using ordinary coherence. Then, partial coherence is used to measure the former relationship once the effect related to respiration is removed. Ordinary coherence values above a statistical threshold revealed linear relationship between VLF modulation in QRS slopes and HRV in about 10% of analyzed segments, with mean ± SD values of 0.79 ± 0.07 for upward slope and 0.77 ± 0.06 for downward slope. For these segments, partial coherence values drop below the threshold for 64% of the cases for upward slope and 76% for downward slope, suggesting that the origin of the VLF modulation in QRS slopes is mainly driven by respiration or linearly related to it. In the rest of the cases, partial coherence values dropped with respect to ordinary coherence from 0.89 to 0.77 for upward slope and from 0.86 to 0.75 for downward slope, suggesting that other ANS effects non-linearly related to respiration also contribute to the VLF modulation in QRS slopes.	autonomic nervous system;bluetooth;cache coherence;coherence (physics);edrophonium;heart rate variability;hemodialysis;hertz (hz);modulation;patients;qrs axis:angle:point in time:heart:quantitative:ekg;slope	David Hernando;Alejandro Alcaine;Pablo Laguna;Esther Pueyo;Raquel Bailón	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610761	electronic engineering;telecommunications;electrical engineering	SE	17.304264179459032	-85.7586761662337	6728
20e5dbe9d6be01c7dfd5e8ce20de915609afd76b	efficient concept detection by fusing simple visual features	multimedia retrieval;concept detection;local binary pattern;feature extraction;semantic gap;visual features;high level feature extraction;video database;trecvid	Concept detection is one of the important tasks in video indexing due to its importance to bridging the semantic gap in multimedia retrieval. Many methods have been proposed for this task, however finding a method which can generalize well for a large number of concepts and is scalable for processing huge video databases is still challenging. In this paper, we introduce a general framework for efficient and scalable concept detection by fusing SVM classifiers trained by only simple visual features such as color moments, edge orientation histogram and local binary patterns. We evaluate the proposed framework for detecting a large number of concepts on various TRECVID datasets with hundreds of hours of video. Experimental results show that the proposed framework achieves good performance with a small computational cost.	algorithmic efficiency;bridging (networking);color moments;computation;database;edge enhancement;local binary patterns;scalability;sensor	Duy-Dinh Le;Shin'ichi Satoh	2009		10.1145/1529282.1529693	computer vision;local binary patterns;feature extraction;computer science;machine learning;pattern recognition;data mining;semantic gap	Vision	33.80971219245424	-55.806756203555494	6740
cb65a0c4275b0a5123235d1d8355d310daa3f682	a bibliometric analysis on tobacco regulation investigators	biological patents;biomedical journals;text mining;europe pubmed central;principle investigators;citation search;data mining and knowledge discovery;citation networks;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;bibliometric analysis;tobacco regulation science;full text;computer appl in life sciences;rest apis;orcids;europe pmc;fda;author topic modeling;biomedical research;bioinformatics;literature search	To facilitate the implementation of the Family Smoking Prevention and Tobacco Control Act of 2009, the Federal Drug Agency (FDA) Center for Tobacco Products (CTP) has identified research priorities under the umbrella of tobacco regulatory science (TRS). As a newly integrated field, the current boundaries and landscape of TRS research are in need of definition. In this work, we conducted a bibliometric study of TRS research by applying author topic modeling (ATM) on MEDLINE citations published by currently-funded TRS principle investigators (PIs). We compared topics generated with ATM on dataset collected with TRS PIs and topics generated with ATM on dataset collected with a TRS keyword list. It is found that all those topics show a good alignment with FDA’s funding protocols. More interestingly, we can see clear interactive relationships among PIs and between PIs and topics. Based on those interactions, we can discover how diverse each PI is, how productive they are, which topics are more popular and what main components each topic involves. Temporal trend analysis of key words shows the significant evaluation in four prime TRS areas. The results show that ATM can efficiently group articles into discriminative categories without any supervision. This indicates that we may incorporate ATM into author identification systems to infer the identity of an author of articles using topics generated by the model. It can also be useful to grantees and funding administrators in suggesting potential collaborators or identifying those that share common research interests for data harmonization or other purposes. The incorporation of temporal analysis can be employed to assess the change over time in TRS as new projects are funded and the extent to which new research reflects the funding priorities of the FDA.	atm turbo;bibliometrics;biological science disciplines;categories;inference;interaction;keyword;medline;protocols documentation;scientific publication;silo (dataset);topic model;citation;interest;pneumococcal purpura-producing principle	Dingcheng Li;Janet Okamoto;Hongfang Liu;Scott James Leischow	2015		10.1186/s13040-015-0043-7	text mining;medical research;computer science;bioinformatics;data science;data mining	ML	-2.181994551487181	-64.90293617596906	6742
1a77653d2f6b53fee3251652c0798225c5b55ac1	delayed feedback suppression of collective rhythmic activity in a neuronal ensemble	global coupling;rulkov model;delayed feedback control;feedback control;neuronal synchrony	We analyze the delayed feedback approach to suppression of collective synchrony in a population of globally or randomly coupled neurons. In particular, we consider the main factors of imperfection of the control scheme and their influence on the suppression efficiency. Next, with the help of a realistic model of synaptically coupled population of inhibitory and excitatory neurons we demonstrate the potential of the suppression scheme for neurophysiological applications.	neural ensemble;randomness;zero suppression	Michael D Rosenblum;Natalia Tukhlina;Arkady Pikovsky;Laura Cimponeriu	2006	I. J. Bifurcation and Chaos	10.1142/S0218127406015842	control theory;feedback;mathematics;physics	ML	17.452656727077407	-70.80872620712678	6750
19da059eba734b83afb9cd004812ecb027af1c7e	a web-based system for clinical decision support and knowledge maintenance for deterioration monitoring of hemato-oncological patients	sepsis;deterioration monitoring;sirs;hemato oncological patients;hematology;fever of unknown origin fuo;web based system;bacteremia;clinical decision support system;knowledge maintenance;diagnosis	We introduce a web-based clinical decision support system (CDSS) and knowledge maintenance based on rules and a set covering method focusing on the problem of detecting serious comorbidities in hemato-oncological patients who are at high risk of developing serious infections and life threatening complications. We experienced that diagnostic problems which are characterized by fuzzy, uncertain knowledge and overlapping signs, still reveal some kind of patterns that can be transferred into a computer-based decision model. We applied a multi-stage evaluation process to assess the system's diagnostic performance. Depending on how system behavior was compared to presumably correct judgment of a case the correctness rate for closed cases with all data available varied between 58% and 71%, the overall rate after critical review was 84%. However, the real time behavior of our approach which data becoming available as time passes still has to be evaluated and observational studies need to be conducted.		Andreas Wicht;Thomas Wetter;Ulrike Klein	2013	Computer methods and programs in biomedicine	10.1016/j.cmpb.2013.02.007	clinical decision support system;intensive care medicine;medicine;hematology;medical diagnosis;surgery	AI	4.68054336086695	-78.87696066200886	6760
9cb5c55cb7672b4bc340a96ee3ab6fdc4a811d7e	image recognition using weighted two-dimensional maximum margin criterion	image features;image recognition;small sample size;feature extraction;image recognition feature extraction;feature extraction techniques;two dimensional image matrix image recognition weighted two dimensional maximum margin criterion feature extraction techniques;weighted two dimensional maximum margin criterion;image recognition linear discriminant analysis principal component analysis feature extraction covariance matrix scattering educational technology robustness pixel laboratories;two dimensional image matrix	In image recognition, feature extraction techniques are widely used to enhance discriminatory performance. In this paper, a new method for image feature extraction, called weighted two-dimensional maximum margin criterion (W2DMMC), is proposed. Different from conventional maximum margin criterion (MMC), W2DMMC is directly based on two-dimensional image matrix rather than one-dimensional vector. And W2DMMC has an additional weighted parameter beta that further broadens the margin. W2DMMC completely circumvents the small sample size problem and is computationally efficient. As a connection to 2DLDA, we show that 2DLDA can be recovered from W2DMMC when imposing some constraints. The better performance of W2DMMC in terms of both recognition accuracy and training time is demonstrated by experiments on real data set.	algorithmic efficiency;computer vision;discriminant;experiment;feature (computer vision);feature extraction;image processing;memory management controller;nonlinear system	Haixian Wang;Sibao Chen;Zilan Hu	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.430	computer vision;feature detection;feature extraction;machine learning;pattern recognition;mathematics;feature	Robotics	33.87967532672034	-58.0610582230668	6768
3030c8dab6873512268486f1e53124803f2e4221	elucidation of conformational states, dynamics, and mechanism of binding in human κ-opioid receptor complexes		"""Opioid G protein-coupled receptors (GPCRs) have been implicated in modulating pain, addiction, psychotomimesis, mood and memory, among other functions. We have employed the recently reported crystal structure of the human κ-opioid receptor (κ-OR) and performed molecular dynamics (MD), free energy, and ab initio calculations to elucidate the binding mechanism in complexes with antagonist JDTic and agonist SalA. The two systems were modeled in water and in DPPC lipid bilayers, in order to investigate the effect of the membrane upon conformational dynamics. MD and Atoms in Molecules (AIM) ab initio calculations for the complexes in water showed that each ligand was stabilized inside the binding site of the receptor through hydrogen bond interactions that involved residues Asp138 (with JDTic) and Gln115, His291, Leu212 (with SalA). The static description offered by the crystal structure was overcome to reveal a structural rearrangement of the binding pocket, which facilitated additional interactions between JDTic and Glu209/Tyr139. The role of Glu209 was emphasized, since it belongs to an extracellular loop that covers the binding site of the receptor and is crucial for ligand entrapment. The above interactions were retained in membrane complexes (SalA forms additional hydrogen bonds with Tyr139/312), except the Tyr139 interaction, which is abolished in the JDTic complex. For the first time, we report that JDTic alternates between a """"V-shape"""" (stabilized via a water-mediated intramolecular interaction) and a more extended conformation, a feature that offers enough suppleness for effective binding. Moreover, MM-PBSA calculations showed that the more efficient JDTic binding to κ-OR compared to SalA (ΔGJDTic = -31.6 kcal mol(-1), ΔGSalA = -9.8 kcal mol(-1)) is attributed mostly to differences in electrostatic contributions. Importantly, our results are in qualitative agreement with the experiments (ΔGJDTic,exp = -14.4 kcal mol(-1), ΔGSalA,exp = -10.8 kcal mol(-1)). This study provides previously unattainable information on the dynamics of human κ-OR and insight on the rational design of drugs with improved pharmacological properties."""		Georgios Leonis;Aggelos Avramopoulos;Ramin Ekhteiari Salmas;Serdar Durdagi;Mine Yurtsever;Manthos G. Papadopoulos	2014	Journal of chemical information and modeling	10.1021/ci5002873	hydrogen bond;jdtic;opioid receptor;stereochemistry;binding site;agonist;receptor;g protein-coupled receptor;chemistry;atoms in molecules	Comp.	9.115105800467093	-62.92383701870088	6794
2cf6ffc1af27d64738f5cf58902c2a5d32bef0b1	multi-level assessment model for wellness service based on human mental stress level	mental stress;multi-level assessment;wellness;inspection middleware	In this paper, we measure human physiological changes from different body parts to quantify human mental stress level by using multimodal bio-sensors. By integrating these physiological responses, we generate bio-index and rule for the prediction of mental status, such as tension, normal, and relax. We also develop an inspection service middleware for analyzing health parameters such as electroencephalography (EEG), electrocardiography (ECG), oxygen saturation (SpO2), blood pressure (BP), and respiration rate (RR). In this service middleware, we use the multi-level assessment model for mental stress level that consists of three steps as follows; classification, reasoning, and decision making. The classification of datasets from bio-sensors is enabled by fuzzy logic and SVM algorithm. The reasoning uses the decision-tree model and random forest algorithm to classify the mental stress level from the health parameters. Finally, we propose a prediction model to make a decision for the wellness contents by using Expectation Maximization (EM).	british informatics olympiad;categorization;decision tree model;electroencephalography;expectation–maximization algorithm;fuzzy logic;middleware;multimodal interaction;random forest;rapid refresh;regular language description for xml;risk assessment;sensor	Yuchae Jung;Yongik Yoon	2016	Multimedia Tools and Applications	10.1007/s11042-016-3444-9	simulation;artificial intelligence;data mining	HCI	6.305317213604192	-86.5697199033513	6800
7dc4a71d8130af88720076ad46d568f339f7d5a8	chancroid transmission dynamics: a mathematical modeling approach	female;male;models biological;haemophilus ducreyi;basic reproduction number;chancroid;humans;computer simulation	Mathematical models have long been used to better understand disease transmission dynamics and how to effectively control them. Here, a chancroid infection model is presented and analyzed. The disease-free equilibrium is shown to be globally asymptotically stable when the reproduction number is less than unity. High levels of treatment are shown to reduce the reproduction number suggesting that treatment has the potential to control chancroid infections in any given community. This result is also supported by numerical simulations which show a decline in chancroid cases whenever the reproduction number is less than unity.	chancroids;epstein-barr virus infections;mathematical model;mathematics;numerical analysis;simulation;disease transmission	Claver P. Bhunu;Steady Mushayabasa	2011	Theory in Biosciences	10.1007/s12064-011-0132-1	computer simulation;basic reproduction number;biology;ecology	Metrics	8.459531799673032	-67.54836184971747	6804
f7705d292a035148b49bd78e1b4a98014e452a64	three-dimensional reconstruction and volume rendering of intravascular ultrasound slices imaged on a curved arterial path	ultrasound;volume rendering;three dimensional;three dimensional reconstruction;intravascular ultrasound;coordinate system;volume data	Past techniques for three-dimensional reconstruction of intravascular ultra­ sound have assumed that the ultrasound slices are parallel and that the vessel being imaged is straight. These assumptions result in distortions of vessel and lesion geometry. To prop­ erly reconstruct the volume data for a curved artery, the position and orientation of the transducer must be known or calculated. We use angiography to recover the geometry of the artery centerline, which is then used as a coordinate system to position the ultrasound slices. To estimate the registration of the slices, several landmark sites are selected by the physician and imaged over a complete heart cycle. Continuous pullbacks are then used to sample between the landmark sites, yielding a three-dimensional volume data set. Standard volume rendering techniques require data on a regular grid. We present new sampling and rendering techniques that handle the oriented ultrasound slices positioned along the curved artery.		Jed Lengyel;Donald P. Greenberg;Alan Yeung;Edwin Alderman;Richard Popp	1995		10.1007/978-3-540-49197-2_50	three-dimensional space;computer vision;coordinate system;ultrasound;mathematics;geometry;volume rendering	Visualization	40.56306596925037	-83.42168124419744	6817
f7e19ba66cb795c8be8ea8ef068dc8c8f5c13f38	computational challenges of tumor spheroid modeling	real life;tumor spheroid;tumor cells;solid tumor;numerical model;biological systems	The speed and the versatility of today's computers open up new opportunities to simulate complex biological systems. Here we review a computational approach recently proposed by us to model large tumor cell populations and spheroids, and we put forward general considerations that apply to any fine-grained numerical model of tumors. We discuss ways to bypass computational limitations and discuss our incremental approach, where each step is validated by experimental observations on a quantitative basis. We present a few results on the growth of tumor cells in closed and open environments and of tumor spheroids. This study suggests new ways to explore the initial growth phase of solid tumors and to optimize antitumor treatments.	biological system;computation;computer;computers;increment;mathematical model;neoplasms;numerical analysis;population;simulation;tumor cells, uncertain whether benign or malignant;bypass;solid tumor	Roberto Chignola;Alessio Del Fabbro;Marcello Farina;Edoardo Milotti	2011	Journal of bioinformatics and computational biology	10.1142/S0219720011005379	biology;simulation	ML	6.842298078473362	-69.0798127505545	6819
e9c0c589d9b66759675a71e0dd4784887f82173b	a new method for vehicle detection using mexicanhat wavelet and moment invariants	image segmentation;conferences signal processing;edge detection;automated highways;computer vision;wavelet transforms;wavelet transforms automated highways computer vision edge detection feature extraction image segmentation object detection;feature extraction;moment invariants vehicle detection adaptive thresholding;mexicanhat wavelet invariants vision based intelligent transportation systems nonvehicle images real vehicle images rotation invariance scale invariance translation invariance hu moments detection accuracy noise robustness edge sharpness wavelet modulus maximum method adaptive threholding rule feature extraction adaptive edge detection vehicle detection moment invariants;object detection	Considering the limits of power and processors in wireless intelligent terminals for vision-based intelligent transportation systems, a new strategy is proposed for vehicle detection in this paper. The contribution of the proposed strategy consists of the adaptive edge detection and feature extraction. In the step of the adaptive edge detection, an adaptive threholding rule is proposed and used for the wavelet modulus maximum method. The threshold rule keeps the edge sharpness and the robustness to noise to improve the detection accuracy. To reduce the computation time, the moment invariants based on Hu moments are calculated to reduce feature data and keep the feature with the invariance of translation, scale and rotation. The real vehicle and non-vehicle images are used to construct the database for experiments. The experimental results show that the average correct rate is about 85%-88% while the running time is less than 1 second. The proposed strategy keeps the balance of the accuracy and running time to a certain extent.	central processing unit;computation;edge detection;experiment;feature data;feature extraction;holographic principle;image moment;modulus of continuity;time complexity;wavelet	Qian Tian;Tengfei Zhong;Hong Li	2013	SiPS 2013 Proceedings	10.1109/SiPS.2013.6674521	computer vision;scale space;object-class detection;edge detection;feature extraction;computer science;machine learning;pattern recognition;mathematics;image segmentation;feature;wavelet transform	Robotics	40.70299563984856	-54.3552136839469	6853
144c38b556bf43a15770e94691ccd1b9255dcc79	approximate bisimulations for sodium channel dynamics	imw channel;modified imw model;sodium channel dynamic;overall imw model;imw sodium channel model;13-state probabilistic model;approximate bisimulations;hh-type sodium-channel approximation;two-state sodium channel model;hh-type channel;67-variable cardiac myocycte model	This paper shows that, in the context of the Iyer et al. 67variable cardiac myocycte model (IMW), it is possible to replace the detailed 13-state continuous-time MDP model of the sodium-channel dynamics, with a much simpler Hodgkin-Huxley (HH)-like two-state sodiumchannel model, while only incurring a bounded approximation error. The technical basis for this result is the construction of an approximate bisimulation between the HH and IMW channel models, both of which are input-controlled (voltage in this case) continuous-time Markov chains. The construction of the appropriate approximate bisimulation, as well as the overall result regarding the behavior of this modified IMW model, involves: (1) The identification of the voltage-dependent parameters of the m and h gates in the HH-type channel, via a two-step fitting process, carried out over more than 22,000 representative observational traces of the IMW channel. (2) Proving that the distance between observations of the two channels is bounded. (3) Exploring the sensitivity of the overall IMW model to the HH-type sodium-channel approximation. Our extensive simulation results experimentally validate our findings, for varying IMW-type input stimuli.	approximation algorithm;approximation error;bisimulation;experiment;hodgkin–huxley model;huxley: the dystopia;markov chain;modified huffman coding;norm (social);simulation;system equivalence;turing completeness	Abhishek Murthy;Md. Ariful Islam;Ezio Bartocci;Elizabeth Cherry;Flavio H. Fenton;James Glimm;Scott A. Smolka;Radu Grosu	2012		10.1007/978-3-642-33636-2_16	discrete mathematics;theoretical computer science;mathematics;algorithm	ML	10.344884850106046	-69.36066628812844	6882
e2444223b27f4adb8849dd11ff5b12fe302054aa	good features for reliable registration in multi-atlas segmentation	publikationer;konferensbidrag;artiklar;rapporter	This work presents a method for multi-organ segmentation in whole-body CT images based on a multi-atlas approach. A robust and efficient feature-based registration technique is developed which uses sparse organ specific features that are learnt based on their ability to register different organ types accurately. The best fitted feature points are used in RANSAC to estimate an affine transformation, followed by a thin plate spline refinement. This yields an accurate and reliable nonrigid transformation for each organ, which is independent of initialization and hence does not suffer from the local minima problem. Further, this is accomplished at a fraction of the time required by intensity-based methods. The technique is embedded into a standard multi-atlas framework using label transfer and fusion, followed by a random forest classifier which produces the data term for the final graph cut segmentation. For a majority of the classes our approach outperforms the competitors at the VISCERAL Anatomy Grand Challenge on segmentation at ISBI 2015.	asp.net ajax;ct scan;computational anatomy;embedded system;graph cuts in computer vision;maxima and minima;random forest;random sample consensus;refinement (computing);sparse matrix;thin plate spline	Fredrik Kahl;Jennifer Alvén;Olof Enqvist;Frida Fejne;Johannes Ulén;Johan Fredriksson;Matilda Landgren;Viktor Larsson	2015			computer vision;computer science;machine learning;pattern recognition;scale-space segmentation	Vision	42.33159693845769	-78.51774553397782	6891
0656138b161c8f009d53453ba51e36b4f6e1a153	co-parent selection for fast region merging in pyramidal image segmentation	pyramidal image segmentation;pattern recognition image segmentation;image segmentation;co parent candidates co parent selection pyramidal image segmentation pyramid structure co parent node;image segmentation artificial neural networks pixel;artificial neural networks;co parent candidates;co parent node;pixel;pattern recognition;receptive field;co parent selection;pyramid structure;region merging	The goal of image segmentation is to partition an image into regions that are internally homogeneous and heterogeneous with respect to other neighbouring regions. We build on the pyramid image segmentation work proposed by [3] and [9] by making a more efficient method by which children chose parents within the pyramid structure. Instead of considering only four immediate parents as in [3], in [9] each child node considers the neighbours of its candidate parent, and the candidate parents of its neighbouring nodes in the same level. In this paper, we also introduce the concept of a co-parent node for possible region merging at the end of each iteration. The new parents of the former children are co-parent candidates as if they are similar. The co-parent is chosen to be the one with the largest receptive field among candidate co-parents. Each child then additionally considers one more candidate, the co-parent of the previous parent. Other steps in the algorithm, and its overall layout, were also improved. The new algorithm is tested on a set of images. Our algorithm is fast (produces segmentations within seconds), results in the correct segmentation of elongated and large regions, very simple compared to plethora of existing algorithms, and appears competitive in segmentation quality with the best publicly available implementations. The major improvement over [9] is that it produces visually appealing results at earlier levels of pyramid segmentation, and not only at the top one.	algorithm;cluster analysis;image segmentation;iteration;tree (data structure)	Milos Stojmenovic;Andres Solis Montero;A. Nayak	2010	2010 2nd International Conference on Image Processing Theory, Tools and Applications	10.1109/IPTA.2010.5586811	computer vision;range segmentation;computer science;artificial intelligence;machine learning;segmentation-based object categorization;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;receptive field;artificial neural network;pixel	Vision	45.443417766754514	-68.67690812607562	6892
2a6694e7648eac5c572b64dfa2358a2ac17d6b7c	analysis of retinal vasculature using a multiresolution hermite model	optimisation;eye;robustness analysis;image segmentation;hermite function;branch point;bayes methods;feature modeling;kruskal m spanning tree;algorithms artificial intelligence fluorescein angiography humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval models cardiovascular pattern recognition automated reproducibility of results retinal vessels retinoscopy sensitivity and specificity signal processing computer assisted;blood vessel;stochastic linking algorithm;qa76 electronic computers computer science computer software;retinal images;qa75 electronic computers computer science;akaike information criteria aic;stochastic processes;expectation maximization;retina spatial resolution labeling image segmentation blood vessels biomedical imaging image resolution robustness image analysis information analysis;computational complexity;local features;image representation;medical image processing;tree structure;stochastic linking algorithm akaike information criteria aic expectation maximization em hermite modeling kruskal m spanning tree retinal images;akaike information criteria;tr photography;spanning tree;retinal vessel labelling retinal vasculature multiresolution hermite model vascular representation vascular segmentation two dimensional hermite function intensity model blood vessel profiles expectation maximization algorithm optimization local model parameter estimation information theoretic test bayesian stochastic inference;biomedical optical imaging;hermite modeling;stochastic processes bayes methods biomedical optical imaging blood vessels expectation maximisation algorithm eye image representation image segmentation medical image processing optimisation;information theoretic;retinal imaging;image modeling;expectation maximization em;r medicine;blood vessels;tk electrical engineering electronics nuclear engineering;spatial resolution;expectation maximisation algorithm	This paper presents a vascular representation and segmentation algorithm based on a multiresolution Hermite model (MHM). A two-dimensional Hermite function intensity model is developed which models blood vessel profiles in a quad-tree structure over a range of spatial resolutions. The use of a multiresolution representation simplifies the image modeling and allows for a robust analysis by combining information across scales. Estimation over scale also reduces the overall computational complexity. As well as using MHM for vessel labelling, the local image modeling can accurately represent vessel directions, widths, amplitudes, and branch points which readily enable the global topology to be inferred. An expectation-maximization (EM) type of optimization scheme is used to estimate local model parameters and an information theoretic test is then applied to select the most appropriate scale/feature model for each region of the image. In the final stage, Bayesian stochastic inference is employed for linking the local features to obtain a description of the global vascular structure. After a detailed description and analysis of MHM, experimental results on two standard retinal databases are given that demonstrate its comparative performance. These show MHM to perform comparably with other retinal vessel labelling methods	anatomy, regional;blast e-value;blood vessel tissue;blood supply aspects;computational complexity theory;database;expectation–maximization algorithm;feature model;hermite polynomials;inference;mathematical optimization;multiresolution analysis;structure of blood vessel of retina;tree structure;biologic segmentation	Li Wang;Abhir Bhalerao;Roland Wilson	2007	IEEE Transactions on Medical Imaging	10.1109/TMI.2006.889732	stochastic process;branch point;computer vision;mathematical optimization;akaike information criterion;image resolution;expectation–maximization algorithm;spanning tree;computer science;machine learning;mathematics;image segmentation;tree structure;computational complexity theory;statistics	Vision	43.981188773198774	-76.49613041426664	6895
264fdc7e22b760d65f20734bd5874aa040ae56c3	simple ambulatory gait monitoring system using a single imu for various daily-life gait activities	legged locomotion;sensors;foot;monitoring;estimation error;parameter estimation	The gait monitoring during daily-life is promising measure to predict mortality, functional decline and fall risk, and classification of gait activities has the possibility for further clinical usage. This paper introduces a simple ambulatory gait monitoring system using a single IMU attached on foot. The system consist of gait parameter estimation and simple gait activity classification algorithm based on characteristics of foot behavior in major gait activities of daily-life, such as leveled, ramp, and stair walk. Through experiment with five healthy subjects, the estimated gait parameters and the classification were verified by motion capture system and video recordings. The results showed that the proposed IMU-based gait monitoring is applicable with leveled, ramp, and stair walk, and its accuracy is acceptable comparing with existing complicated gait monitoring methods.	algorithm;estimation theory;gait analysis;motion capture;ramp simulation software for modelling reliability, availability and maintainability	Minsu Song;Jonghyun Kim	2016	2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)	10.1109/BHI.2016.7455926	effect of gait parameters on energetic cost;simulation;engineering;physical therapy;surgery	Robotics	10.772287073858555	-85.56579353964514	6924
fbab7073591a88ef63c285cb1e6438c094558146	lung image patch classification with automatic feature learning	biological tissues;feature extraction lungs vectors training neurons computed tomography;image classification;lung;feature extraction;medical image processing;computerised tomography;diseases;interstitial lung disease lung image patch classification automatic feature learning medical imaging applications image feature vectors image data intrinsic image features multiscale feature extractors autogenerated image features lung tissue;learning artificial intelligence;medical image processing biological tissues computerised tomography diseases feature extraction image classification learning artificial intelligence lung	Image patch classification is an important task in many different medical imaging applications. The classification performance is usually highly dependent on the effectiveness of image feature vectors. While many feature descriptors have been proposed over the past years, they can be quite complicated and domain-specific. Automatic feature learning from image data has thus emerged as a different trend recently, to capture the intrinsic image features without manual feature design. In this paper, we propose to create multi-scale feature extractors based on an unsupervised learning algorithm; and obtain the image feature vectors by convolving the feature extractors with the image patches. The auto-generated image features are data-adaptive and highly descriptive. A simple classification scheme is then used to classify the image patches. The proposed method is generic in nature and can be applied to different imaging domains. For evaluation, we perform image patch classification to differentiate various lung tissue patterns commonly seen in interstitial lung disease (ILD), and demonstrate promising results.	algorithm;comparison and contrast of classification schemes in linguistics and metadata;description;display resolution;extractors;feature (computer vision);feature extraction;feature learning;feature vector;generic drugs;high-resolution computed tomography;internet listing display;interstitial webpage;lung diseases, interstitial;lung diseases;medical imaging;numerous;raw image format;statistical classification;structure of parenchyma of lung;unsupervised learning;disease classification	Qing Li;Tom Weidong Cai;David Dagan Feng	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610939	computer vision;contextual image classification;feature detection;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;feature	Vision	32.407912213139	-74.70736929165624	6938
27b137c38d0017fe6bb14c03e73bb1a3dca8ed98	disentangling sequential effects of stimulus- and response-related conflict and stimulus-response repetition using brain potentials	potentiel evoque cognitif;human information processing;stimulus related conflict;response stimulus relation;conflict monitoring theory;event evoked potential;corteza cingular;systeme nerveux central;conflict;hombre;electrophysiology;event related potential;encefalo;sistema nervioso central;anterior cingulate cortex;encephale;conflicto;cognition;information processing;human;potencial evocado cognitivo;cognicion;relation stimulus reponse;electrofisiologia;conflict monitoring;encephalon;brain potentials;cingulate cortex;conflit;relacion estimulo respuesta;response related conflict;electrophysiologie;cortex cingulaire;central nervous system;stimulus response repetition;homme	Conflict monitoring theory holds that detection of conflicts in information processing by the anterior cingulate cortex (ACC) results in processing adaptation that minimizes subsequent conflict. Applying an Eriksen f lanker task with four stimuli mapped onto two responses, we investigated whether such modulation occurs only after response-related or also after stimulus-related conflict, focusing on the N2 component of the event-related potential. Contrasting with previous findings, both stimulus- and response-related conflict elicited enhancement of the N2, suggesting that the ACC is sensitive to conflict at both the stimulus and the response level. However, neither type of conflict resulted in reduced conflict effects on the following trial when stimulus-response (S-R) sequence effects were controlled by excluding identical S-R repetition trials. Identical S-R repetitions were associated with facilitated processing, thus demonstrating that inclusion of these trials in the analysis may mimic results predicted by the conflict adaptation hypothesis.	acclimatization;cerebral cortex;cingulate cortex;conflict (psychology);information processing;modulation;perseveration	Mike Wendt;Marcus Heldmann;Thomas F. Münte;Rainer H. Kluwe	2007	Journal of Cognitive Neuroscience	10.1162/jocn.2007.19.7.1104	psychology;event-related potential;electrophysiology;neuroscience;cognition;developmental psychology;information processing;central nervous system;communication;social psychology;cognitive science	ML	16.673009019654558	-77.53337225260985	6985
f6f3922dd5a939a3a8474a3157850d097f62a25d	the effect of test format on visual recognition memory performance				Nora Andermane;Jeffrey S. Bowers	2013			recognition memory;visual short-term memory;cognitive psychology;psychology	Vision	13.132610103496695	-75.52293720055332	6996
d24450d927d5f58ac1590e4d44d5c9aca70735a2	fascicle-selectivity of an intraneural stimulation electrode in the rabbit sciatic nerve	animals;impedance;action potentials animals differential threshold electric stimulation electrodes implanted equipment design equipment failure analysis nerve fibers rabbits sciatic nerve sensitivity and specificity;electrodes animals impedance electron tubes impedance measurement glass nerve fibers;animal experiments;chronic experiment intraneural stimulation electrode fascicle selectivity rabbit sciatic nerve peripheral nerve interface extraneural electrode intrafascicular electrode fiber proximity sciatic nerve tibial nerve peroneal nerve anaesthetized rabbits blunt glass tool epineurium neural prosthetic device biocompatible electrodes;nerve fibers;prosthetics;interfascicular electrode;blood vessel;glass;proof of concept;peripheral nerve;electrodes;peripheral nerves;impedance measurement;prosthetics biomedical electrodes neurophysiology;sciatic nerve;stimulation selectivity;biomedical electrodes;neurophysiology;electron tubes;nerve stimulation;stimulation selectivity animal experiments interfascicular electrode nerve stimulation peripheral nerves	The current literature contains extensive research on peripheral nerve interfaces, including both extraneural and intrafascicular electrodes. Interfascicular electrodes, which are in-between these two with respect to nerve fiber proximity have, however, received little interest. In this proof-of-concept study, an interfascicular electrode was designed to be implanted in the sciatic nerve and activate the tibial and peroneal nerves selectively of each other, and it was tested in acute experiments on nine anaesthetized rabbits. The electrode was inserted without difficulty between the fascicles using blunt glass tools, which could easily penetrate the epineurium but not the perineurium. Selective activation of all tibial and peroneal nerves in the nine animals was achieved with high selectivity (Ŝ = 0.98 ± 0.02). Interfascicular electrodes could provide an interesting addition to the bulk of peripheral nerve interfaces available for neural prosthetic devices. Since interfascicular electrodes can be inserted without fully freeing the nerve and have the advantage of not confining the nerve to a limited space, they could, e.g., be an alternative to extraneural electrodes in locations where such surgery is complicated due to blood vessels or fatty tissue. Further studies are, however, necessary to develop biocompatible electrodes and test their stability and safety in chronic experiments.	abducens nerve diseases;adipose tissue;blood vessel;blunt (object);bone structure of tibia;clinical act of insertion;epineurium;experiment;fascicle - nerve fibers;implantation procedure;implants;ion implantation;neuroprosthetics;neurotrophic electrode;perineurium;peripheral nerves;peripheral nerve interface;prosthesis;selectivity (electronic);structure of common peroneal nerve;structure of sciatic nerve	Thomas N. Nielsen;Cristian Sevcencu;Johannes J. Struijk	2012	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2011.2169671	neuroscience;computer science;engineering;electrical engineering;electrode;electrical impedance;glass;focused impedance measurement;proof of concept;neurophysiology;physics;anatomy;surgery	Visualization	25.848585473749292	-84.98852801099915	7003
0943f05662ddd7e233a6b3eba88db02e353a4937	patgpcr: a multitemplate approach for improving 3d structure prediction of transmembrane helices of g-protein-coupled receptors	software;lipids;receptors g protein coupled;protein structure secondary;imaging three dimensional;ligands;magnetic resonance spectroscopy;models molecular;期刊论文;protein binding;models statistical;algorithms;protein folding;humans;computer simulation	"""The structures of the seven transmembrane helices of G-protein-coupled receptors are critically involved in many aspects of these receptors, such as receptor stability, ligand docking, and molecular function. Most of the previous multitemplate approaches have built a """"super"""" template with very little merging of aligned fragments from different templates. Here, we present a parallelized multitemplate approach, patGPCR, to predict the 3D structures of transmembrane helices of G-protein-coupled receptors. patGPCR, which employs a bundle-packing related energy function that extends on the RosettaMem energy, parallelizes eight pipelines for transmembrane helix refinement and exchanges the optimized helix structures from multiple templates. We have investigated the performance of patGPCR on a test set containing eight determined G-protein-coupled receptors. The results indicate that patGPCR improves the TM RMSD of the predicted models by 33.64% on average against a single-template method. Compared with other homology approaches, the best models for five of the eight targets built by patGPCR had a lower TM RMSD than that obtained from SWISS-MODEL; patGPCR also showed lower average TM RMSD than single-template and multiple-template MODELLER."""	algorithm;alignment;boat dock;clinical use template;docking (molecular);homologous gene;homology (biology);image resolution;ligands;modeller;mathematical optimization;parallel computing;pipeline (computing);refinement (computing);swiss-model;scientific publication;set packing;template method pattern;test set;web site;molecular_function;receptor	Hongjie Wu;Qiang Lv;Lijun Quan;Peide Qian;Xiaoyan Xia	2013		10.1155/2013/486125	computer simulation;protein folding;plasma protein binding;computer science;bioinformatics;ligand;algorithm	Visualization	10.737655510645338	-59.78925319685313	7004
6126e8cf254f681a05c7a313ee4d4618078feff7	end-epi: an application for inferring phylogenetic and population dynamical processes from molecular sequences	software;arbre phylogenetique;evolucion biologica;evolution biologique;phylogeny;logiciel;computerized processing;tratamiento informatico;phylogenese;dinamica poblacion;arbol filogenetico;biological evolution;population dynamic;phylogenetic tree;graphical method;filogenesis;population dynamics;logicial;dynamique population;traitement informatique	MOTIVATION Phylogenetic trees constructed from molecular sequences contain information about the evolutionary or population dynamical processes that created them. Here we describe a computer package (End-Epi) that uses graphical methods to allow researchers to make inferences about these processes from their data. Statistical analyses can be performed to test the consistency of the data with various competing hypotheses.   AVAILABILITY End-Epi can be obtained by WWW from http://evolve.zoo.ox.ac.uk/ and by anonymous FTP from ftp://evolve.zoo.ox.ac.uk/packages/End-Epi10.hqx. This file contains the compiled application, the manual and a test tree.	compiler;dynamical system;file transfer protocol;list of graphical methods;phylogenesis;phylogenetic tree;phylogenetics;population;statistical model;trees (plant);www	Andrew Rambaut;P. H. Harvey;S. Nee	1997	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/13.3.303	biology;phylogenetic tree;bioinformatics;population dynamics;algorithm	Comp.	-3.1507180910627515	-55.60548038010056	7011
28f02f723f2ac2d3e88868f217d38adf0c35886e	the molecular pages of the mesotelencephalic dopamine consortium (dopanet)	software;animals;telencephalon;rats;databases bibliographic;ligands;systems biology;databases genetic;web service;computational biology bioinformatics;cooperative behavior;large scale;models genetic;internet;protein conformation;signal processing;system biology;public display;algorithms;receptors nicotinic;humans;neurons;combinatorial libraries;dopamine;protein transport;computer appl in life sciences;microarrays;bioinformatics	DopaNet http://www.dopanet.org is a Systems Biology initiative that aims to investigate precisely and quantitatively all the aspects of neurotransmission in a specific neuronal system, the mesotelencephalic dopamine system. The project should lead to large-scale models of molecular and cellular processes involved in neuronal signaling. A prerequisite is the proper storage of knowledge coming from the literature. DopaNet Molecular Pages are highly structured descriptions of quantitative parameters related to a specific molecular complex involved in neuronal signal processing. A Molecular Page is built by maintainers who are experts in the field, and responsible for the quality of the page content. Each piece of data is identified by a specific ontology code, annotated (method of acquisition, species, etc.) and linked to the relevant bibliography. The Molecular Pages are stored as XML files, and processed through the DopaNet Web Service, which provides functionalities to edit the Molecular Pages, to cross-link the Pages and generate the public display, and to search them. DopaNet Molecular Pages are one of the core resources of the DopaNet project but should be of widespread utility in the field of Systems Neurobiology.	bibliography;description;dopamine;ontology;page (document);signal processing;synaptic transmission;systems biology;web service;xml;neuronal signal transduction	Nicolas Le Novère;Marco Donizelli	2004	BMC Bioinformatics	10.1186/1471-2105-5-174	web service;biology;dopamine;computer science;bioinformatics;data science;genetics;systems biology;algorithm	Metrics	-1.7832454037014542	-61.46505481151472	7033
c8133c43a7373590049c5871556c585ce2e7358c	independent component analysis approach for the neurobiological recording data	artefacto;interfase usuario;rhythm;brain;analisis estadistico;realite virtuelle;systeme nerveux central;realidad virtual;electroencefalografia;user interface;movimiento ocular;virtual reality;hombre;independent component analysis;probabilistic approach;encefalo;electroencephalographie;artefact;artifacts;realite augmentee;sistema nervioso central;realidad aumentada;cerebro;statistical analysis;diagnostic panne;encephale;enfoque probabilista;approche probabiliste;cerveau;fault diagnostic;eye movement;contaminacion;analyse statistique;diagnostico pana;human;rythme;analyse composante independante;eeg;interface utilisateur;ritmo;encephalon;contamination;neurophysiology;electroencephalography;augmented reality;analisis componente independiente;second order statistics;mouvement oculaire;electroencephalogram;human brain;central nervous system;homme	Electroencephalograms (EEG) can provide a unique window on the human brain. Since contamination of EEG recording with artifacts (e.g., signals caused by muscular activity, eye movements, cardiac rhythm and power noise etc.) can decrease the efficiency of diagnosis procedure, here we apply a kind of fast independent component analysis (ICA) approach to analyze the real multi-variant EEG recorded signals. Besides, the comparison between ICA and a second order statistical algorithm are given in this paper. By the real measured data, our experimental results confirm the validity and usefulness of ICA algorithm.	independent component analysis	Minghai Yao;Jing Hu;Qing-long L. Gu	2006		10.1007/11941354_68	augmented reality;electroencephalography;computer science;artificial intelligence;rhythm;virtual reality;neurophysiology	SE	22.37417121446503	-92.32354439185966	7041
c1bc25c0b92e4a6d8d104c427e99a5399930cca3	volume regularization in explicit image registration used for breast cancer bed localization		A breast tumor bed localization is a challenging task for a supportive radiotherapy performed after an oncoplastic surgery. The tumor bed position can be determined by the segmented cancer contour propagation. We introduce a computationally efficient regularization method for the tumor volume contraction and a log-linear error function. The results are validated by an ability to reconstruct artificial but real-like deformation fields and a breast tumors relative volume reduction. We show that both the volume regularization and the log-linear error function improve the applied deformation field reconstruction and increase the relative tumor bed volume contraction.	algorithmic efficiency;finite volume method;image registration;log-linear model;matrix regularization;software propagation	Marek Wodzinski;Andrzej Skalski;Izabela Ciepiela;Tomasz Kuszewski;Piotr Kedzierawski	2018	2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)	10.1109/ISBI.2018.8363548	computer vision;oncoplastic surgery;breast cancer;cancer;computer science;pattern recognition;volume contraction;artificial intelligence;error function;radiation therapy;image registration;regularization (mathematics)	Vision	46.586919797026944	-81.71690311972786	7063
bf14f6cbcdfff780cd38e10ef43eddbdc75b9600	dtminer: identification of potential disease targets through biomedical literature mining		MOTIVATION Biomedical researchers often search through massive catalogues of literature to look for potential relationships between genes and diseases. Given the rapid growth of biomedical literature, automatic relation extraction, a crucial technology in biomedical literature mining, has shown great potential to support research of gene-related diseases. Existing work in this field has produced datasets that are limited both in scale and accuracy.   RESULTS In this study, we propose a reliable and efficient framework that takes large biomedical literature repositories as inputs, identifies credible relationships between diseases and genes, and presents possible genes related to a given disease and possible diseases related to a given gene. The framework incorporates name entity recognition (NER), which identifies occurrences of genes and diseases in texts, association detection whereby we extract and evaluate features from gene-disease pairs, and ranking algorithms that estimate how closely the pairs are related. The F1-score of the NER phase is 0.87, which is higher than existing studies. The association detection phase takes drastically less time than previous work while maintaining a comparable F1-score of 0.86. The end-to-end result achieves a 0.259 F1-score for the top 50 genes associated with a disease, which performs better than previous work. In addition, we released a web service for public use of the dataset.   AVAILABILITY AND IMPLEMENTATION The implementation of the proposed algorithms is publicly available at http://gdr-web.rwebox.com/public_html/index.php?page=download.php The web service is available at http://gdr-web.rwebox.com/public_html/index.php CONTACT: jenny.wei@astrazeneca.com or kzhu@cs.sjtu.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.	algorithm;base;bioinformatics;computation;cross-reference;dictionary [publication type];end-to-end principle;f1 score;geographic information systems;knowledge bases;large;libraries;library (computing);named-entity recognition;paper;relationship extraction;repository;silo (dataset);statistical classification;support vector machine;therapeutic procedure;time complexity;web search engine;web service;world wide web	Dong Xu;Meizhuo Zhang;Yanping Xie;Fan Wang;Ming Chen;Kenny Q. Zhu;Jia Wei	2016		10.1093/bioinformatics/btw503	computer science;disease;bioinformatics	Comp.	-2.7445440713031375	-64.7140155830173	7082
88b00934d0e3a96ed229566542ec55778295eee1	comparative study on morphological skeletonization and fuzzy medial axis transformation	medial axis transform;mathematical morphology;information science;data storage;comparative study;computing systems;superposition;article	In this paper, we make a comparative study on morphological skeletonization (MSK) and fuzzy medial axis transformation (FMAT). Methods have been proposed to construct convex FMAT from the morphological skeleton points and to translate FMAT to MSK, respectively. For the case of translating MSK to convex FMAT, the experimental results reveal that the combination of the proposed method and the redundant removal algorithm is very effective. Especially, the combined method is faster than the original method for constructing convex FMAT of smoothed images. © 2002 SPIE and IS&T. [DOI: 10.1117/1.1426075]	algorithm;apache axis;medial graph;morphological skeleton;smoothing	Sen-Ren Jan;Yuang-Cheh Hsueh	2002	J. Electronic Imaging	10.1117/1.1426075	superposition principle;computer vision;mathematical morphology;information science;computer science;theoretical computer science;comparative research;computer data storage;mathematics;algorithm	Robotics	48.59911498777685	-63.40403635651225	7089
05d577f8abba5555051d5f3e526b6c6d2de71bc8	progression analysis and stage discovery in continuous physiological processes using image computing	signal image and speech processing;biological patents;biomedical journals;text mining;europe pubmed central;systems biology;citation search;citation networks;computational biology bioinformatics;biomedical engineering;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	We propose an image computing-based method for quantitative analysis of continuous physiological processes that can be sensed by medical imaging and demonstrate its application to the analysis of morphological alterations of the bone structure, which correlate with the progression of osteoarthritis (OA). The purpose of the analysis is to quantitatively estimate OA progression in a fashion that can assist in understanding the pathophysiology of the disease. Ultimately, the texture analysis will be able to provide an alternative OA scoring method, which can potentially reflect the progression of the disease in a more direct fashion compared to the existing clinically utilized classification schemes based on radiology. This method can be useful not just for studying the nature of OA, but also for developing and testing the effect of drugs and treatments. While in this paper we demonstrate the application of the method to osteoarthritis, its generality makes it suitable for the analysis of other progressive clinical conditions that can be diagnosed and prognosed by using medical imaging.	color gradient;computation (action);degenerative polyarthritis;medical imaging;physiological processes;radiology;score;skeletal bone;skin physiological phenomena	Lior Shamir;Salim Rahimi;Nikita Orlov;Luigi Ferrucci;Ilya G. Goldberg	2010		10.1155/2010/107036	biology;text mining;medical research;computer science;bioinformatics;engineering;data science;data mining;biological engineering;systems biology	ML	34.24599857055599	-80.8452637941788	7091
3ba49e81f71225dc2227a92dfeb3e013ed2100a7	application to estimate haplotypes for multiallelic present-absent loci	computer program;expectation maximization algorithm;mathematical model;null allele	The article presents an algorithm and an application to estimate haplotype frequencies from genotype data for unrelated individuals. Presented approach can handle loci with multiple alleles as well as silent (null) alleles. The mathematical model and an expanded Expectation-Maximization algorithm is described. The computer program, called NullHap, available freely at http://staff.elka.pw.edu.pl/ ~rnowak2/nullhap implements presented ideas. Comparison with known software to estimate haplotypes: Arlequin, PHASE and Haplo-IHP proves the advantage presented method.	arlequin;computation;computer program;expectation–maximization algorithm;haplogroup;mathematical model;missing data	Robert D. Nowak	2008		10.1007/978-3-540-68168-7_40	econometrics;bioinformatics;statistics	Visualization	2.758030689027779	-52.4825831051709	7098
1b2471e99a045f663bae5936bb0fc06cf9f152ee	electromyography data processing impacts muscle synergies during gait for unimpaired children and children with cerebral palsy	amplitude scaling;cerebral palsy;electromyography;low pass filtering;muscle synergies;non-negative matrix factorization;walk-dmc	Muscle synergies calculated from electromyography (EMG) data identify weighted groups of muscles activated together during functional tasks. Research has shown that fewer synergies are required to describe EMG data of individuals with neurologic impairments. When considering potential clinical applications of synergies, understanding how EMG data processing impacts results and clinical interpretation is important. The aim of this study was to evaluate how EMG signal processing impacts synergy outputs during gait. We evaluated the impacts of two common processing steps for synergy analyses: low pass (LP) filtering and unit variance scaling. We evaluated EMG data collected during barefoot walking from five muscles of 113 children with cerebral palsy (CP) and 73 typically-developing (TD) children. We applied low pass (LP) filters to the EMG data with cutoff frequencies ranging from 4 to 40 Hz (reflecting the range reported in prior synergy research). We also evaluated the impact of normalizing EMG amplitude by unit variance. We found that the total variance accounted for (tVAF) by a given number of synergies was sensitive to LP filter choice and decreased in both TD and CP groups with increasing LP cutoff frequency (e.g. 9.3 percentage points change for one synergy between 4 and 40 Hz). This change in tVAF can alter the number of synergies selected for further analyses. Normalizing tVAF to a z-score (e.g., dynamic motor control index during walking, walk-DMC) reduced sensitivity to LP cutoff. Unit variance scaling caused comparatively small changes in tVAF. Synergy weights and activations were impacted less than tVAF by LP filter choice and unit variance normalization. These results demonstrate that EMG signal processing methods impact outputs of synergy analysis and z-score based measures can assist in reporting and comparing results across studies and clinical centers.	cerebral palsy;electromyography;hertz (hz);image scaling;impacted tooth;lp-type problem;low-pass filter;muscle;sample variance;signal processing;synergy;test scaling;weight;nervous system disorder;z-score	Benjamin R. Shuman;Michael H. Schwartz;Katherine M. Steele	2017		10.3389/fncom.2017.00050	artificial intelligence;machine learning;filter (signal processing);cutoff;physical medicine and rehabilitation;normalization (statistics);cerebral palsy;motor control;gait;computer science;physical therapy;electromyography;low-pass filter		16.525481437986258	-85.7610185035115	7101
b95299397b5dbdc3c0fe07e2297de244a548b5d7	separating components of attention and surprise		Cognitive processes involved in both allocation of attention during decision making as well as surprise when making mistakes trigger release of the neurotransmitter norepinephrine, which has been shown to be correlated with an increase in pupil dilation, in turn reflecting raised levels of arousal. Extending earlier experiments based on the Attention Network Test (ANT), separating the neural components of alertness and spatial re-orientation from the attention involved in more demanding conflict resolution tasks, we demonstrate that these signatures of attention are so robust that they may be retrieved even when applying low cost eye tracking in an everyday mobile computing context. Furthermore we find that the reaction of surprise elicited when committing mistakes in a decision task, which in the neuroimaging EEG literature have been referred to as a negativity feedback error correction signal, may likewise be retrieved solely based on an increase in pupil dilation.	cognition;dilation (morphology);electroencephalography;error detection and correction;experiment;eye tracking;mobile computing;negativity (quantum mechanics);type signature	Per Baekgaard;Michael Kai Petersen;Jakob Eg Larsen	2016	CoRR		computer science;pupillary response;alertness;surprise;multimedia;eye tracking;cognition;arousal;negativity effect;artificial intelligence;pattern recognition	AI	15.28534974364738	-77.65367180469896	7124
0d019f260d7b4fbb793ca67af1fd08900326cfe8	structure-based integrative computational and experimental approach for the optimization of drug design	modelo dinamico;integrated approach;optimisation;spectrometrie rmn;multiagent system;basse energie;medicament;proteine;teoria ergodica;optimizacion;low energy;baja energia;dynamic model;mass spectrometry;molecular dynamics;active measurement;optimum global;optimization method;intelligence artificielle;simulated annealing;global optimum;metodo optimizacion;approche deterministe;nmr spectroscopy;molecular dynamic simulation;residu;dynamique moleculaire;deterministic approach;molecular dynamics method;protein targeting;recuit simule;programacion lineal;drug design;distance geometry;nmr spectrometry;structure determination;modele dynamique;theorie ergodique;enfoque determinista;methode optimisation;linear programming;programmation lineaire;linear optimization;artificial intelligence;recocido simulado;global optimization;medicamento;optimization;proteina;methode dynamique moleculaire;inteligencia artificial;drug;dinamica molecular;espectrometria rmn;residuo;sistema multiagente;protein;residue;optimo global;ergodic theory;systeme multiagent;metodo dinamico molecular;in silico	We present an integrative approach for the optimization in the design of peptides which are candidates to become therapeutic agents. This approach is based on the structure of the peptide ligand when structural information on the protein target is not available. Our approach combines (i) NMR spectroscopy, (ii) structure determination by distance geometry, simulated annealing, and global optimization methods, restrained with NMR-derived or deduced restraints, (iii) molecular dynamics simulations, based on NMR low energy, averaged minimized, or ensemble of structures, (iv) in silico sequence selection using integer linear optimization, (v) fold specificity using deterministic global optimization, and (vi) peptide synthesis, mass spectrometry characterization, and activity measurements. The optimization of the design of the 13-residue cyclic peptide compstatin is presented as a paradigm for the application of our approach. The same principles can be applied for the design of small proteins with desired properties and function.	computation;deterministic global optimization;emoticon;integer programming;linear programming;mathematical optimization;molecular dynamics;programming paradigm;sensitivity and specificity;simulated annealing;simulation	Dimitrios Morikis;Christodoulos A. Floudas;John D. Lambris	2005		10.1007/11428848_88	ergodic theory;simulated annealing;linear programming;artificial intelligence;global optimum;residue;distance geometry;deterministic system;drug design;protein targeting	Comp.	11.016602882872178	-61.478500841295585	7144
2db515cd99c29699e735e72c2b8a456bcdef1b4d	improved quantification of csf bilirubin in the presence of hemoglobin using least squares curve-fitting	mathematical model algorithm design and analysis curve fitting calibration hemorrhaging equations fluids;fluids;subarachnoid hemorrhage;time window;algorithms artifacts bilirubin complex mixtures data interpretation statistical hemoglobins humans least squares analysis reproducibility of results sensitivity and specificity spectrophotometry ultraviolet;hemorrhaging;early diagnosis;least square;mathematical model;curve fitting;algorithm design;calibration;algorithm design and analysis;cerebrospinal fluid	Subarachnoid hemorrhage (SAH) is a dangerous neurological event with a very short time window for early diagnosis. Clinical diagnoses performed in a lab seek to quantify bilirubin in cerebrospinal fluid (CSF) as a biomarker for SAHs; however laboratory assays suffer from lengthy protocols, interference from hemoglobin, and the availability of expertise. Substantial improvements in the determination of bilirubin concentration in the presence of he moglobin in CSF are demonstrated in this work. Concentration estimates within 15% for bilirubin in the range of 0.2 to 1.6 mg /dl were determined for CSF samples containing fresh hemoglobin concentrations ranging from 0.05 to 0.25 g/dl. To demonstrate extensibility of the system with respect to more complete mock SAH samples, sample sets with one additional species of both hemoglobin and bilirubin, methemoglobin and alpha-bilirubin, respectively, were tested and yielded results within 25% of actual values, as measured by standard chemical assays of preparations prior to mixing.	bilirubin;biological markers;cerebrospinal fluid;curve fitting;estimated;extensibility;interference (communication);least squares;methemoglobin;mock object;per deciliter;protocols documentation;quantitation;s-adenosylhomocysteine;sleep apnea, obstructive;subarachnoid hemorrhage;alpha-mannosidosis	Joshua D. Butler;Blaine Booher;Peggy S. Bowman;Joseph F. Clark;Fred R. Beyette	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6089882	algorithm design;computer science;mathematics;surgery;statistics;fluid mechanics	Visualization	50.04917049351345	-84.37797119840857	7148
1433826d1bdeeeca463be27f47355d8489bcf234	robust photometric invariant region detection in multispectral images	spectrum;universiteitsbibliotheek;noise robustness;clustering;reflection model;multispectral images;polar angle representation;camera calibration;region detection;photometric invariance	Our aim is to detect photometric invariant regions in multispectral images robust against sensor noise. Therefore, different polar angle representations of a spectrum are examined for invariance using the dichromatic reflection model. These invariant representations take advantage of white balancing. Based on the camera sensitivity, a theoretical expression is obtained of the certainty associated with the polar angular representations under the influence of noise. The expression is employed by the segmentation technique to ensure robustness against sensor noise.	angularjs;color balance;image noise;multispectral image	Theo Gevers;Harro M. G. Stokman	2003	International Journal of Computer Vision	10.1023/A:1023095923133	multispectral image;spectrum;computer vision;camera resectioning;computer science;machine learning;cluster analysis	Vision	51.91319650807154	-61.20251484783049	7163
9332cf14bb1078a7a90ba32d054f2d81cf5b7502	a feasibility study of automatic multi-organ segmentation using probabilistic atlas		Thoracic and abdominal multi-organ segmentation has been a challenging problem due to the inter-subject variance of human thoraxes and abdomens as well as the complex 3D intra-subject variance among organs. In this paper, we present a preliminary method for automatically segmenting multiple organs using non-enhanced CT data. The method is based on a simple framework using generic tools and requires no organ-specific prior knowledge. Specifically, we constructed a grayscale CT volume along with a probabilistic atlas consisting of six thoracic and abdominal organs: lungs (left and right), liver, kidneys (left and right) and spleen. A non-rigid mapping between the grayscale CT volume and a new test volume provided the deformation information for mapping the probabilistic atlas to the test CT volume. The evaluation with the 20 VISCERAL non-enhanced CT dataset showed that the proposed method yielded an average Dice coefficient of over 95% for the lungs, over 90% for the liver, as well as around 80% and 70% for the spleen and the kidneys respectively.	ct scan;grayscale;sørensen–dice coefficient	Shuqing Chen;Jürgen Endres;Sabrina Dorn;Joscha Maier;Michael Lell;Marc Kachelriess;Andreas K. Maier	2017		10.1007/978-3-662-54345-0_50	sørensen–dice coefficient;computer vision;grayscale;probabilistic logic;artificial intelligence;atlas (anatomy);segmentation;computer science	Vision	41.985754411700626	-79.50479411585793	7179
c7102bb7212be3d52c2c7f2da2550b0466119ddc	fuzzy rule-based hand gesture recognition	monotonic gesture segment;objeto de conferencia;sign language;data glove;fuzzy rule base;finite automata;ciencias informaticas;hand gesture recognition	This paper introduces a fuzzy rule-based method for the recognition of hand gestures acquired from a data glove, with an application to the recognition of some sample hand gestures of LIBRAS, the Brazilian Sign Language. The method uses the set of angles of finger joints for the classification of hand configurations, and classifications of segments of hand gestures for recognizing gestures. The segmentation of gestures is based on the concept of monotonic gesture segment, sequences of hand configurations in which the variations of the angles of the finger joints have the same sign (non-increasing or non-decreasing). Each gesture is characterized by its list of monotonic segments. The set of all lists of segments of a given set of gestures determine a set of finite automata, which are able to recognize every such gesture.	automata theory;finite-state machine;fuzzy rule;fuzzy set;gesture recognition;logic programming;programming language;python;t-norm;wired glove	Benjamín R. C. Bedregal;Antônio Carlos da Rocha Costa;Graçaliz Pereira Dimuro	2006		10.1007/978-0-387-34747-9_30	speech recognition;computer science;artificial intelligence;gesture recognition;communication	AI	31.847599650054512	-68.12212096505216	7202
c61fc36b167a575684c5fdb4356d724fb07d0f2a	bossanova at imageclef 2012 flickr photo annotation task		We present the BossaNova scheme for the ImageCLEF 2012 Flickr Photo Annotation Task. BossaNova is a mid-level image representation, recently developed by our team, that enriches the Bag-of-Words representation, by keeping a histogram of distances between the descriptors found in the image and those in the codebook. Our scheme has the advantage of being conceptually simple, non-parametric, and easily adaptable. Compared to other schemes existing in the literature to add information to the Bag-of-Words model, it leads to much more compact representations. Furthermore, it complements well the cutting-edge Fisher Vector representations, showing even better results when employed in combination with them. In our participation, we submitted four purely visual runs. Our best result (MiAP = 34.37%) achieved the second rank by MiAP measure among the 28 purely visual submissions and the 18 teams.	ambiguous name resolution;bag-of-words model in computer vision;codebook;flickr;grammar-based code;high- and low-level;matroid rank	Sandra Eliza Fontes de Avila;Nicolas Thome;Matthieu Cord;Eduardo Valle;Arnaldo de Albuquerque Araújo	2012			computer vision;mathematics;multimedia;information retrieval	Vision	25.564899645453288	-57.147510042008314	7208
8b03afedd113dec48990f2c23de8ae21888c2435	gait analysis for patients with alzheimer's disease using a triaxial accelerometer	accelerometer gait analysis alzheimer s disease walking;instruments;walking;detection algorithms;legged locomotion;acceleration;alzheimer s disease;gait analysis;accelerometer;accelerometers;legged locomotion acceleration alzheimer s disease accelerometers detection algorithms educational institutions instruments	This paper presents an inertial-sensor-based wearable device and its associated stride detection algorithm to analyze gait information for patients with Alzheimer's disease (AD). The wearable gait analysis device is composed of a triaxial accelerometer, a microcontroller, and an RF wireless transmission module. To validate the effectiveness of the proposed device and algorithm, nine AD patients and three healthy controls were recruited to participate a gait analysis experiment. They were asked to mount the device on their foot and walk along a straight line of 40 meters at normal speed. The stride detection algorithm, consisting of procedures of data collection, signal preprocessing, and stride detection, has been developed for acquiring gait feature information from acceleration signals. The advantages of this wearable gait analysis device include the following: 1) It can be used anywhere without any external device, and 2) the stride detection algorithm can acquire gait feature information from acceleration signals automatically and effectively. Experimental results show that the AD patients exhibited a significantly shorter mean stride length and slower mean gait speed than those of the healthy controls. No significant differences in mean stride frequency and mean cadence were observed in the two groups. The variability in the percentage of the stance phase of the AD patients was slightly greater than that of the healthy controls. Based on the above results and discussions with physicians, we conclude that the proposed wearable gait analysis device is a promising tool for automatically analyzing gait information which can serve as indicators for early diagnosis of AD.	algorithm;color gradient;denavit–hartenberg parameters;gait analysis;microcontroller;peripheral;preprocessor;radio frequency;spatial variability;wearable computer;wearable technology	Pau-Choo Chung;Yu-Liang Hsu;Chun-Yao Wang;Chien-Wen Lin;Jeen-Shing Wang;Ming-Chyi Pai	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271484	embedded system;effect of gait parameters on energetic cost;simulation;computer science;engineering;operating system;accelerometer	Mobile	10.840537490758477	-86.3627627368815	7220
cd95be68499d19add9c30310877b13d8b82e9634	synaptic weighting for physiological responses in recurrent spiking neural networks	action potentials humans models theoretical nerve net synapses;topology;neurons computational modeling steady state topology biological neural networks physiology;neural nets;computer model;sensorimotor integration;neurophysiology medical computing neural nets;medical computing;mean field;spiking neural network;physiology;computational modeling;recurrent network;coordinate transformation;neurons;neurophysiology;synaptic weighting approach synaptic weighting physiological response recurrent spiking neural networks neuron spiking neuron response characteristics neuron steady state response;biological neural networks;physiological response;neural network;steady state	Recurrently connected neural networks have been used extensively in the literature to describe various neuro-physiological phenomena, such as coordinate transformations during sensorimotor integration. Due to the directed cycles that can exist in recurrent networks, there is no well-known way to a priori specify synaptic weights to elicit neuron spiking responses to stimuli based on available neurophysiology. Using a common mean field assumption in which synaptic inputs are uncorrelated for sufficiently large populations of neurons, we show that the connection topology and a neuron's response characteristics can be decoupled. This allows specification of neuron steady-state responses independent of the connection topology. We provide evidence from two case studies which serve to validate this synaptic weighting approach.	anatomy, regional;artificial neural network;neural network simulation;neuron;neurons;physiological phenomena;population;recurrent neural network;science of neurophysiology;specification;spiking neural network;steady state;synapse;synaptic package manager;synaptic weight;cell transformation	David J. Herzfeld;Scott A. Beardsley	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091039	neuroscience;computer science;artificial intelligence;mean field theory;coordinate system;machine learning;steady state;computational model;artificial neural network;physics;spiking neural network	ML	17.871284072646663	-69.30460453753075	7224
07bbf08d06ecfddfde17aeecfbec2efdfae21201	3-d tomographic reconstruction of the average propagator from mri data	tomography magnetic resonance imaging image reconstruction biomedical measurements particle measurements nuclear magnetic resonance materials science and technology biological system modeling fourier transforms attenuation;biomedical measurements;computed tomography;fourier transform;particle measurements;average propagator;medical image processing biomedical mri fourier transforms image reconstruction;biological system modeling;nuclear magnetic resonance;spectrum;diffusion weighted mri;attenuation;computed tomography principles;signal attenuation;diffusion spectrum imaging 3 d tomographic reconstruction average propagator diffusion weighted mri diffusion weighted nmr 3 d fourier transform signal attenuation computed tomography principles;tomographic reconstruction;materials science and technology;diffusion spectrum imaging;image reconstruction;medical image processing;magnetic resonance imaging;fourier transforms;biomedical image processing;a priori information;diffusion weighted nmr;microstructures;tomography;3 d fourier transform;diffusion model;biomedical mri;3 d tomographic reconstruction;diffusion weighted	"""The measurement of the 3-D """"average propagator"""", P(r) from diffusion-weighted (DW) NMR or MRI data has been a """"holy grail"""" in materials science and biomedicine, as P(r) provides detailed microstructural information, particularly about restriction, without assuming an underlying diffusion model. While Callaghan proposed a 3-D Fourier transform relationship between P(r) and the DW signal attenuation, E(q), using it to measure P(r) from E(q) data is not currently feasible biologically or clinically, owing to the staggering amount of DW data required. To address this problem, we propose that computed tomography principles can be applied to reconstruct P(r) from DW signals. Moreover, this reconstruction can be performed efficiently using many fewer DW E(q) data as compared to conventional 3-D q-space MRI or diffusion spectrum imaging (DSI) by employing a priori information about E(q) and P(r)"""	ct scan;dreamwidth;propagator;tomographic reconstruction;tomography	Valery Pickalov;Peter J. Basser	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1625015	fourier transform;radiology;medicine;magnetic resonance imaging;mathematics;tomography;nuclear magnetic resonance;nuclear medicine;medical physics	Arch	47.891907476940574	-82.83897782776835	7225
fe4155dcf15e1418a1ae8b6feb494cd9c1aa0477	molecular simulations to delineate functional conformational transitions in the hcv polymerase	conformations;free energy landscape;replication	Hepatitis C virus (HCV) is a global health concern for which there is no vaccine available. The HCV polymerase is responsible for the critical function of replicating the RNA genome of the virus. Transitions between at least two conformations (open and closed) are necessary to allow the enzyme to replicate RNA. In this study, molecular dynamic simulations were initiated from multiple crystal structures to understand the free energy landscape (FEL) explored by the enzyme as it interconverts between these conformations. Our studies reveal the location of distinct states within the FEL as well as the molecular interactions associated with these states. Specific hydrogen bonds appear to play a key role in modulating conformational transitions. This knowledge is essential to elucidate the role of these conformations in replication and may also be valuable in understanding the basis by which this enzyme is inhibited by small molecules. © 2016 Wiley Periodicals, Inc.	cab direct (database);crystal structure;dna replication;familial hemophagocytic lymphocytosis;hepatitis a;hepatitis c virus;hydrogen;interaction;john d. wiley;molecular dynamics;rna;self-replicating machine;simulation;free energy	Ester Sesmero;Jodian A. Brown;Ian F. Thorpe	2017	Journal of computational chemistry	10.1002/jcc.24662	chemistry;computational chemistry;polymerase;bioinformatics	Comp.	7.508784600881487	-62.5838883255929	7233
d079ee292062351c9f8a97e261de0986eb58d770	comparing active vision models	model selection;probabilistic approach;three dimensional;action selection;article letter to editor;behavioural approach;mutual information;object classification;high performance;active vision	Performance on visual tasks such as classification can be enhanced by employing active vision systems. Such systems do not passively receive observations, but have to some extent control over the observations they perceive. There are two general approaches to active vision. The first approach to active vision is a probabilistic approach, in which reducing uncertainty on a part of the world state is the central goal. This uncertainty is modelled by a belief state. The second approach to active vision is a behavioural approach, in which successful behaviour is the central goal. For both approaches, there have been considerable research efforts into designing and studying various active vision models. However, it is not clear how the different existing active vision models relate to each other, and what their relative advantages are. In this report, we identify three main types of active vision models in the probabilistic approach and describe them in a common formal framework. The first type of model selects actions on the basis of the mutual information between actions and classes, and is referred to as the Mutual Information model (MI). The second type of model learns an action policy on the basis of entropy loss in the belief state, the Entropy Loss model (EL). The third type of model bases its action selection on the mode of the belief state, the Mode of Belief state model (MB). In addition, we introduce a fourth type of active vision model that is based on the behavioural approach to active vision, the BeHavioural model (BH). Model BH is identical to EL, except that it learns an action policy that achieves a high performance rather than one that achieves entropy loss in the belief state. We compare the four active vision models empirically on a view-based three-dimensional object classification task. The experimental results give insight into the differences between the models. The overall result is that BH generally outperforms the models EL and MB of the probabilistic approach. In addition, within the probabilistic approach model MI has the best classification performance. Besides revealing performance differences between the active vision models, the experimental results also illustrate properties of the relation between the usefulness of active vision, the number of objects involved in the classification task, and the richness of the visual observations of the models.	action selection;active vision;information model;megabyte;mutual information;x86	Guido C. H. E. de Croon;Ida G. Sprinkhuizen-Kuyper;Eric O. Postma	2009	Image Vision Comput.	10.1016/j.imavis.2008.06.004	three-dimensional space;computer vision;action selection;active vision;computer science;artificial intelligence;machine learning;mutual information;model selection;statistics	AI	23.69047525903403	-67.33293114837863	7239
3ca3a7b3786e4c6af6ba6b54b5c5a4d7b583c4aa	pattern recognition display methods for the analysis of computed molecular properties	unsupervised learning;large data sets;exploratory analysis;computational chemistry;biological activity;pattern recognition	Pattern recognition methods, particularly the 'unsupervised learning' techniques, are well suited for the preliminary analysis of the large data sets produced by computer chemistry. The use of linear and non-linear display methods for such exploratory analysis are exemplified with the aid of two data sets of biologically active molecules. Advantages and disadvantages of these techniques are discussed.	nonlinear system;pattern recognition;unsupervised learning	Brian D. Hudson;David J. Livingstone;Elizabeth Rahr	1989	Journal of computer-aided molecular design	10.1007/BF01590995	unsupervised learning;chemistry;feature;computer science;data science;machine learning;biological activity;computational chemistry;data mining	Vision	12.589169879718746	-55.16520406463365	7258
1ea749ac42b1a981ba3feaf7ab531f3dee846227	a random set scoring model for prioritization of disease candidate genes using protein complexes and data-mining of generif, omim and pubmed records	genomics;disease;gene regulatory networks;genome wide association study;databases genetic;data mining;computational biology bioinformatics;proteins;likelihood functions;reproducibility of results;models statistical;algorithms;humans;combinatorial libraries;phenotype;protein interaction maps;computer appl in life sciences;pubmed;microarrays;bioinformatics	Prioritizing genetic variants is a challenge because disease susceptibility loci are often located in genes of unknown function or the relationship with the corresponding phenotype is unclear. A global data-mining exercise on the biomedical literature can establish the phenotypic profile of genes with respect to their connection to disease phenotypes. The importance of protein-protein interaction networks in the genetic heterogeneity of common diseases or complex traits is becoming increasingly recognized. Thus, the development of a network-based approach combined with phenotypic profiling would be useful for disease gene prioritization. We developed a random-set scoring model and implemented it to quantify phenotype relevance in a network-based disease gene-prioritization approach. We validated our approach based on different gene phenotypic profiles, which were generated from PubMed abstracts, OMIM, and GeneRIF records. We also investigated the validity of several vocabulary filters and different likelihood thresholds for predicted protein-protein interactions in terms of their effect on the network-based gene-prioritization approach, which relies on text-mining of the phenotype data. Our method demonstrated good precision and sensitivity compared with those of two alternative complex-based prioritization approaches. We then conducted a global ranking of all human genes according to their relevance to a range of human diseases. The resulting accurate ranking of known causal genes supported the reliability of our approach. Moreover, these data suggest many promising novel candidate genes for human disorders that have a complex mode of inheritance. We have implemented and validated a network-based approach to prioritize genes for human diseases based on their phenotypic profile. We have devised a powerful and transparent tool to identify and rank candidate genes. Our global gene prioritization provides a unique resource for the biological interpretation of data from genome-wide association studies, and will help in the understanding of how the associated genetic variants influence disease or quantitative phenotypes.	abstract summary;candidate disease gene;causal filter;data mining;disease susceptibility;generif;hereditary diseases;mode of inheritance;online mendelian inheritance in man;phenotype;pubmed;relevance;score;text mining;trait;vocabulary;protein protein interaction	Li Jiang;Stefan M. Edwards;Bo Thomsen;Christopher T. Workman;Bernt Guldbrandtsen;Peter Sørensen	2013		10.1186/1471-2105-15-315	genome-wide association study;biology;gene regulatory network;genomics;dna microarray;bioinformatics;phenotype;data mining;candidate gene;genetics	Comp.	3.786685061180231	-56.544065655610076	7269
8701bb3417d0f9b759301bd3904917a6c8b788fc	evaluation of respiratory properties by means of fractional order models	respiratory system;respiratory impedance;knn classifier;modeling;fractional order model	The goal of this paper is to model and analyze the properties of the respiratory system by means of fractional calculus. A linear fractional order system of commensurate order is obtained using the real and the imaginary parts of the measured respiratory impedance through an identification technique. In this context, the features used for the classification of some respiratory diseases are the identified parameters of the linear fractional order system of commensurate order. These features are then classified using the K-Nearest Neighbors (KNN) classifier. The proposed method has achieved an accuracy of 40% using only the first feature, however by using all the features the accuracy has increased up to 100%. The proposed classification technique is validated on 15 patients: healthy, asthma and chronic obstructive pulmonary disease (COPD).		I. Assadi;Abdelfatah Charef;Dana Copot;Robin De Keyser;Tahar Bensouici;Clara M. Ionescu	2017	Biomed. Signal Proc. and Control	10.1016/j.bspc.2017.02.006	mathematical optimization;speech recognition;systems modeling;machine learning;respiratory system;mathematics	Robotics	16.86697867948365	-90.04788170539301	7272
07183f004f76e27ee6beb204d350a40796d96bf8	a robust method to estimate the intracranial volume across mri field strengths (1.5t and 3t)	reverse normalization;atrophy;tissue probability maps;brain;intraclass correlation coefficient;statistical parametric map;montreal neurological institute;probability;estimation method;spm;population based study;segmentation;image processing computer assisted;healthy subjects;cerebral ventricles;voxel based morphometry;bias correction;semantic dementia;magnetic resonance imaging;reproducibility of results;mri;normalization;robust method;algorithms;intensity nonuniformity;humans;mild cognitive impairment;alzheimer disease;brain extraction;hippocampal volumes;human brain;images;intracranial volume;cohort studies;volumetry;aged;organ size;cerebrospinal fluid;cognition disorders;automation	As population-based studies may obtain images from scanners with different field strengths, a method to normalize regional brain volumes according to intracranial volume (ICV) independent of field strength is needed. We found systematic differences in ICV estimation, tested in a cohort of healthy subjects (n=5) that had been imaged using 1.5T and 3T scanners, and confirmed in two independent cohorts. This was related to systematic differences in the intensity of cerebrospinal fluid (CSF), with higher intensities for CSF located in the ventricles compared with CSF in the cisterns, at 3T versus 1.5T, which could not be removed with three different applied bias correction algorithms. We developed a method based on tissue probability maps in MNI (Montreal Neurological Institute) space and reverse normalization (reverse brain mask, RBM) and validated it against manual ICV measurements. We also compared it with alternative automated ICV estimation methods based on Statistical Parametric Mapping (SPM5) and Brain Extraction Tool (FSL). The proposed RBM method was equivalent to manual ICV normalization with a high intraclass correlation coefficient (ICC=0.99) and reliable across different field strengths. RBM achieved the best combination of precision and reliability in a group of healthy subjects, a group of patients with Alzheimer's disease (AD) and mild cognitive impairment (MCI) and can be used as a common normalization framework.	alzheimer's disease;cerebral ventricles;cerebrospinal fluid;coefficient;cognition disorders;fmrib software library (fsl);intracranial hypertension;map;mild cognitive disorder;normalize;patients;restricted boltzmann machine;algorithm	Shiva Keihaninejad;Rolf A. Heckemann;Gianlorenzo Fagiolo;Mark R. Symms;Joseph V. Hajnal;Alexander Hammers	2010		10.1016/j.neuroimage.2010.01.064	psychology;neuroscience;developmental psychology;radiology;medicine;pathology;cohort study;automation;magnetic resonance imaging;normalization;probability;intraclass correlation;segmentation;statistics;voxel-based morphometry	ML	34.78894803640903	-81.8576637329595	7288
18a15e8574a2f68db7571b9abacf9bfdc9a413bb	attention guided u-net for accurate iris segmentation		Abstract Iris segmentation is a critical step for improving the accuracy of iris recognition, as well as for medical concerns. Existing methods generally use whole eye images as input for network learning, which do not consider the geometric constrain that iris only occur in a specific area in the eye. As a result, such methods can be easily affected by irrelevant noisy pixels outside iris region. In order to address this problem, we propose the ATTention U-Net (ATT-UNet) which guides the model to learn more discriminative features for separating the iris and non-iris pixels. The ATT-UNet firstly regress a bounding box of the potential iris region and generated an attention mask. Then, the mask is used as a weighted function to merge with discriminative feature maps in the model, making segmentation model pay more attention to iris region. We implement our approach on UBIRIS.v2 and CASIA.IrisV4-distance, and achieve mean error rates of 0.76% and 0.38%, respectively. Experimental results show that our method achieves consistent improvement in both visible wavelength and near-infrared iris images with challenging scenery, and surpass other representative iris segmentation approaches.		Sheng Lian;Zhiming Luo;Zhun Zhong;Xiang Lin;Songzhi Su;Shaozi Li	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.10.001	artificial intelligence;discriminative model;pattern recognition;computer vision;mathematics;pixel;merge (version control);iris recognition;minimum bounding box;segmentation	Vision	27.621906906335894	-55.63953338768193	7289
17258be40c5684d36161f9a61be3982bcb87427f	triggering robot hand reflexes with human emg data using spiking neurons		The interaction of humans and robots (HRI) is of great relevance for the field of neurorobotics as it can provide insights on motor control and sensor processing mechanisms in humans that can be applied to robotics. We propose a spiking neural network (SNN) to trigger motion reflexes on a robotic hand based on human EMG data. The first part of the network takes EMG signals to measure muscle activity, then classify the data to detect which finger is active in the human hand. The second part triggers single finger reflexes using the classification output. The finger reflexes are modeled with motion primitives activated with an oscillator and mapped to the robot kinematic. We evaluated the SNN by having users wear a non-invasive EMG sensor, record a training dataset, and then flex different fingers, one at a time. The muscle activity was recorded using a Myo sensor with eight channels. EMG signals were successfully encoded into spikes as input for the SNN. The classification could detect the active finger to trigger motion generation of finger reflexes. The SNN was able to control a real Schunk SVH robotic hand. Being able to map myo-electric activity to functions of motor control for a task, can provide an interesting interface for robotic applications, and also to study brain functioning. SNN provide a challenging but interesting framework to interact with human data. In future work the approach will be extended to control a robot arm at the same time.		Juan Camilo Vasquez Tieck;Sandro Weber;Terrence C. Stewart;Arne Rönnau;Rüdiger Dillmann	2018		10.1007/978-3-030-01370-7_70	neurorobotics;humanoid robot;spiking neural network;robot;computer vision;artificial intelligence;motor control;robotic arm;human–robot interaction;robotics;computer science	ML	12.037463804386345	-93.49075232108545	7312
0ee91e66813266153c3767ce26f905e50369474c	integrative bayesian analysis of neuroimaging-genetic data through hierarchical dimension reduction	genetics brain modeling imaging computational modeling principal component analysis analytical models bayes methods;generalized principal component analysis;bayesian model averaging;imaging genetics;diffusion tensor imaging;imaging genetics bayesian model averaging diffusion tensor imaging generalized principal component analysis	Advances in neuromedicine have emerged from endeavors to elucidate the distinct genetic factors that influence the changes in brain structure that underlie various neurological conditions. We present a framework for examining the extent to which genetic factors impact imaging phenotypes described by voxel-wise measurements organized into collections of functionally relevant regions of interest (ROIs) that span the entire brain. Statistically, the integration of neuroimaging and genetic data is challenging. Because genetic variants are expected to impact different regions of the brain, an appropriate method of inference must simultaneously account for spatial dependence and model uncertainty. Our proposed framework combines feature extraction using generalized principal component analysis to account for inherent short- and long-range structural dependencies with Bayesian model averaging to effectuate variable selection in the presence of multiple genetic variants. The methods are demonstrated on a cocaine dependence study to identify ROIs associated with genetic factors that impact diffusion parameters.	brain;cocaine dependence;collections (publication);dimensionality reduction;ensemble learning;feature extraction;feature selection;inference;neuroimaging;principal component analysis;region of interest;span distance;spatial autocorrelation;voxel	Shabnam Azadeh;Brian P. Hobbs;Liangsuo Ma;David A. Nielsen;F. Gerard Moeller;Veerabhadran Baladandayuthapani	2016	2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2016.7493393	diffusion mri;econometrics;machine learning;imaging genetics;mathematics;statistics	Visualization	24.224706810914924	-77.29416504796085	7313
2f527932a09b619c1daa44a7bf01764522a83cc5	a highly accurate, optical flow-based algorithm for nonlinear spatial normalization of diffusion tensor images	biodiffusion;numerical analysis biodiffusion biomedical mri image registration image sequences medical image processing;会议论文;numerical analysis;optical imaging computer vision image motion analysis diffusion tensor imaging image resolution nonlinear optics tensile stress;medical image processing;image registration;dti data registration algorithm highly accurate optical flow based algorithm nonlinear spatial normalization diffusion tensor images voxel based analyses high dimensional spatial normalization 3d optical flow conventional optic flow theory intensity gradient consistency discontinuity preserving spatiotemporal smoothness hierarchical strategy euler lagrange numerical analysis simulated datasets real datasets;biomedical mri;image sequences	Spatial normalization plays a key role in voxel-based analyses of diffusion tensor images (DTI). We propose a highly accurate algorithm for high-dimensional spatial normalization of DTI data based on the technique of 3D optical flow. The theory of conventional optic flow assumes consistency of intensity and consistency of the gradient of intensity under a constraint of discontinuity-preserving spatio-temporal smoothness. By employing a hierarchical strategy ranging from coarse to fine scales of resolution and a method of Euler-Lagrange numerical analysis, our algorithm is capable of registering DTI data. Experiments using both simulated and real datasets demonstrated that the accuracy of our algorithm is better not only than that of those traditional optical flow algorithms or using affine alignment, but also better than the results using popular tools such as the statistical parametric mapping (SPM) software package. Moreover, our registration algorithm is fully automated, requiring a very limited number of parameters and no manual intervention.	algorithm;euler;euler–lagrange equation;gradient;nonlinear system;numerical analysis;optical flow;reflections of signals on conducting lines;super paper mario;voxel	Ying Wen;Bradley S. Peterson;Dongrong Xu	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706989	computer vision;mathematical optimization;numerical analysis;computer science;image registration;theoretical computer science;mathematics;spatial normalization	Vision	49.731631692708156	-80.04540239467822	7316
aac9324d3cb0673eeb7836b6714d1da9ffcaf344	the origin of informational replicators by serial dilution of a primordial soup		How can informational replicators (Zachar and Szathmáry 2010) such as template replicators, arise from noninformational autocatalysts (Szathmáry and Maynard Smith 1997; Szathmary 2000)? Variants of an informational replicator have a high probability of being autocatalytic, thus allowing potentially unlimited heritable variants to be replicated, for example, mutants of a DNA sequence have this property. Variants of non-informational replicators such as glycolaldehyde in the Formose cycle are not in general autocatalytic; therefore, there is little capacity for hereditary variation (Szathmáry 2006). This paper asks; what are the necessary and sufficient conditions for an increase in the probability that a variant of an autocatalyst will itself be capable of autocatalysis? Given some well-defined assumptions, serial dilution in a rich generative chemistry such as that found in the Miller experiment should result in the emergence of informational replicators, i.e. autocatalysts whose variants have a high probability of themselves being capable of autocatalysis.	emergence;maynard electronics;replicator (stargate)	Chrisantha Fernando	2010			biology;genetics	AI	4.877495414620298	-62.83501504940285	7320
533a03abd3467b18e6de4aa8e2efeb30b04f384d	regulatory coordination of clustered micrornas based on microrna-transcription factor regulatory network	simulation and modeling;systems biology;gene regulatory networks;physiological cellular and medical topics;databases genetic;transcription factors;computational biology bioinformatics;models genetic;genome human;algorithms;humans;micrornas;bioinformatics	MicroRNA (miRNA) is a class of small RNAs of ~22nt which play essential roles in many crucial biological processes and numerous human diseases at post-transcriptional level of gene expression. It has been revealed that miRNA genes tend to be clustered, and the miRNAs organized into one cluster are usually transcribed coordinately. This implies a coordinated regulation mode exerted by clustered miRNAs. However, how the clustered miRNAs coordinate their regulations on large scale gene expression is still unclear. We constructed the miRNA-transcription factor regulatory network that contains the interactions between transcription factors (TFs), miRNAs and non-TF protein-coding genes, and made a genome-wide study on the regulatory coordination of clustered miRNAs. We found that there are two types of miRNA clusters, i.e. homo-clusters that contain miRNAs of the same family and hetero-clusters that contain miRNAs of various families. In general, the homo-clustered as well as the hetero-clustered miRNAs both exhibit coordinated regulation since the miRNAs belonging to one cluster tend to be involved in the same network module, which performs a relatively isolated biological function. However, the homo-clustered miRNAs show a direct regulatory coordination that is realized by one-step regulation (i.e. the direct regulation of the coordinated targets), whereas the hetero-clustered miRNAs show an indirect regulatory coordination that is realized by a regulation comprising at least three steps (e.g. the regulation on the coordinated targets by a miRNA through a sequential action of two TFs). The direct and indirect regulation target different categories of genes, the former predominantly regulating genes involved in emergent responses, the latter targeting genes that imply long-term effects. The genomic clustering of miRNAs is closely related to the coordinated regulation in the gene regulatory network. The pattern of regulatory coordination is dependent on the composition of the miRNA cluster. The homo-clustered miRNAs mainly coordinate their regulation rapidly, while the hetero-clustered miRNAs exert control with a delay. The diverse pattern of regulatory coordination suggests distinct roles of the homo-clustered and the hetero-clustered miRNAs in biological processes.	categories;cluster analysis;emergence;function (biology);gene expression;gene regulatory network;interaction;micrornas;regulation;transcription factor;transcription (software);transcription, genetic;physical hard work;statistical cluster	Zhongjing Wang;Martin Haubrock;Kunming Cao;Xu Hua;Chenyu Zhang;Edgar Wingender;Jie Li	2011		10.1186/1752-0509-5-199	biology;gene regulatory network;bioinformatics;genetics;systems biology;microrna;transcription factor	Comp.	4.921586812990148	-61.7831354268288	7331
baa3afefbb6b5c6323e6e8f382b89368773735be	asap: analysis of peptide composition	site web;computer program;peptides;secuencia aminoacido;alignement sequence;proteine;sequence aminoacide;aminoacid sequence;interrogation base donnee;interrogacion base datos;bioinformatique;alineacion secuencia;peptido;proteomique;peptide;asap;proteina;sequence alignment;sitio web;proteomics;protein;programa computador;database query;web site;programme ordinateur;bioinformatics	SUMMARY ASAP is a web tool designed to search for specific dipeptides, tripeptides and tetrapeptides in a protein sequence database. The server allows for: (a) identification of frequent and infrequent peptides and the creation of peptide probability tables for a given database of sequences (GenerNet program), (b) determination of the compatibility of an amino-acid sequence to the given peptide probability tables (ClonErrNet program); and (c) comparison of different protein databases based on peptide composition (CompNet program). ASAP server can be useful in protein engineering and/or protein classification studies.	amino acid metabolism, inborn errors;amino acid sequence;asynchronous array of simple processors;data table;databases, protein;dipeptides;emoticon;map9 gene;protein engineering;published database;sequence database;server (computing);staphylococcal protein a	Xavier Serra-Hartmann;Xavier Rebordosa;Jaume Piñol;Enrique Querol;Marc A. Martí-Renom	2000	Bioinformatics	10.1093/bioinformatics/16.12.1153	peptide bond;biology;computer science;bioinformatics;sequence alignment;proteomics;world wide web	Comp.	-4.138721573804242	-56.30862966685613	7332
98eff5e4cdb0a57da6c932a20bfb14695f873d91	robust angle invariant 1d barcode detection	transforms accuracy cameras decoding mobile handsets databases image edge detection;image coding;1d barcode hough transform object detection;bar codes;robust angle invariant 1d barcode detection barcode reading algorithm arte lab 1d medium barcode dataset wwu muenster barcode database two dimensional hough transform space 1d barcode identification supervised machine learning algorithm camera captured images object detection algorithms barcode detection task degraded barcode decoding barcode reading approaches mobile devices product identification barcode reading mobile applications;hough transforms;hough transform;learning artificial intelligence;mobile computing;cameras;object detection bar codes cameras hough transforms image coding learning artificial intelligence mobile computing;1d barcode;object detection	Barcode reading mobile applications that identify products from pictures taken using mobile devices are widely used by customers to perform online price comparisons or to access reviews written by others. Most of the currently available barcode reading approaches focus on decoding degraded barcodes and treat the underlying barcode detection task as a side problem that can be addressed using appropriate object detection methods. However, the majority of modern mobile devices do not meet the minimum working requirements of complex general purpose object detection algorithms and most of the efficient specifically designed barcode detection algorithms require user interaction to work properly. In this paper, we present a novel method for barcode detection in camera captured images based on a supervised machine learning algorithm that identifies one-dimensional barcodes in the two-dimensional Hough Transform space. Our model is angle invariant, requires no user interaction and can be executed on a modern mobile device. It achieves excellent results for two standard one-dimensional barcode datasets: WWU Muenster Barcode Database and ArTe-Lab 1D Medium Barcode Dataset. Moreover, we prove that it is possible to enhance the overall performance of a state-of-the-art barcode reading algorithm by combining it with our detection method.	algorithm;barcode;hough transform;machine learning;mobile app;mobile device;object detection;requirement;supervised learning	Alessandro Zamberletti;Ignazio Gallo;Simone Albertini	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.17	computer vision;computer science;theoretical computer science;computer graphics (images)	AI	26.690123250752247	-59.289751466647104	7334
64fdf8e40f91f2f6fb9ca024505fd9d9a9dc833d	annotating early esophageal cancers based on two saliency levels of gastroscopic images	early esophageal cancer;gastroscopic image;lesion annotation;superpixel segmentation;visual saliency	Early diagnoses of esophageal cancer can greatly improve the survival rate of patients. At present, the lesion annotation of early esophageal cancers (EEC) in gastroscopic images is generally performed by medical personnel in a clinic. To reduce the effect of subjectivity and fatigue in manual annotation, computer-aided annotation is required. However, automated annotation of EEC lesions using images is a challenging task owing to the fine-grained variability in the appearance of EEC lesions. This study modifies the traditional EEC annotation framework and utilizes visual salient information to develop a two saliency levels-based lesion annotation (TSL-BLA) for EEC annotations on gastroscopic images. Unlike existing methods, the proposed framework has a strong ability of constraining false positive outputs. What is more, TSL-BLA is also placed an additional emphasis on the annotation of small EEC lesions. A total of 871 gastroscopic images from 231 patients were used to validate TSL-BLA. 365 of those images contain 434 EEC lesions and 506 images do not contain any lesions. 101 small lesion regions are extracted from the 434 lesions to further validate the performance of TSL-BLA. The experimental results show that the mean detection rate and Dice similarity coefficients of TSL-BLA were 97.24 and 75.15%, respectively. Compared with other state-of-the-art methods, TSL-BLA shows better performance. Moreover, it shows strong superiority when annotating small EEC lesions. It also produces fewer false positive outputs and has a fast running speed. Therefore, The proposed method has good application prospects in aiding clinical EEC diagnoses.	101 mouse;annotation;biologics license application;blatella germanica allergen 4;coefficient;computation (action);esophageal neoplasms;extraction;fastest;fatigue;heart rate variability;pc-mos/386;patients;region of interest;biologic segmentation	Ding-Yun Liu;Nini Rao;Byungsoo Oh;Hongxiu Jiang;Shengqi Yu;Cheng-Si Luo;Qian Li;Stefan Carpenter;Bing Zeng;Tao Gan	2018	Journal of Medical Systems	10.1007/s10916-018-1063-x	medical diagnosis;data mining;lesion;salience (neuroscience);small lesion;pattern recognition;medicine;artificial intelligence;annotation	Vision	33.74172397097048	-76.86024370433137	7352
1fb997f9ad4d8cb46fdf61d0e494cdda9087290f	zernike's feature descriptors for iris recognition with svm	pseudo zernike moments;support vector machine iris recognition zernike moments pseudo zernike moments feature extraction;iris recognition feature extraction iris support vector machines polynomials databases educational institutions;zernike moments;iris recognition;zernike polynomials feature extraction image texture iris recognition support vector machines;feature extraction;support vector machine;bath university repository zernike feature descriptors iris recognition svm natural texture valuable information feature extraction biometric recognition iris pattern pseudozernike polynomials iris image identification support vector machine iris data set	Valuable information of the iris is intrinsically located in its natural texture, therefore preserve and extract the most relevant features for biometric recognition is of paramount importance. The iris pattern is subject to translation, scaling and rotation, consequently the variations produced by these artifacts must be minimized. The main contribution of this work consists on performing a comparison between the descriptive power of the Zernike and pseudo Zernike polynomials for the identification of iris images using a Support Vector Machine (SVM) as a classifier. Experiments with the iris data set obtained from the Bath University repository show that our proposal yields high levels of accuracy.	biometrics;feature vector;image scaling;iris recognition;polynomial;support vector machine	Juan Reyes-Lopez;Sergio Campos;Héctor Allende;Rodrigo Salas	2011	2011 30th International Conference of the Chilean Computer Science Society	10.1109/SCCC.2011.36	computer vision;computer science;machine learning;pattern recognition;iris recognition	Vision	34.84146441838458	-60.907817237372754	7357
bddb6df28db51de7de10b8e07ca9134a90cc9ae6	a look at non-cooperative presentation attacks in fingerprint systems		Scientific literature lacks of countermeasures specifically for fingerprint presentation attacks (PAs) realized with non-cooperative methods; even though, in realistic scenarios, it is unlikely that individuals would agree to duplicate their fingerprints. For example, replicas can be created from finger marks left on a surface without the person’s knowledge. Existing anti-spoofing mechanisms are trained to detect presentation attacks realized with cooperation of the user and are assumed to be able to identify non-cooperative spoofs as well. In this regard, latent prints are perceived to be of low quality and less likely to succeed in gaining unauthorized access. Thus, they are expected to be blocked without the need of a particular presentation attack detection system. Currently, the lowest Presentation Attack Detection (PAD) error rates on spoofs from latent prints are achieved using frameworks involving Convolutional Neural Networks (CNNs) trained on cooperative PAs; however, the computational requirement of these networks does not make them easily portable for mobile applications. Therefore, the focus of this paper is to investigate the degree of success of spoofs made from latent fingerprints to improve the understanding of their vitality features. Furthermore, we experimentally show the performance drop of existing liveness detectors when dealing with non-cooperative attacks and analyze the quality estimates pertaining to such spoofs, which are commonly believed to be of lower quality compared to the molds fabricated with user’s consensus.		Emanuela Marasco;Stefany Cando;Larry Tang;Luca Ghiani;Gian Luca Marcialis	2018	2018 Eighth International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2018.8608133	convolutional neural network;fingerprint;artificial intelligence;pattern recognition;scientific literature;fingerprint recognition;histogram;computer science;liveness	Web+IR	27.47617263427905	-63.59049890325506	7365
41c90fbf2cde33ee3f8cb6d3cc09ccbf29f4f362	a unified approach to noise removal, image enhancement, and shape recovery	noise shaping image enhancement shape level set colored noise color switches mathematics equations tracking;stopping criteria;mathematics;image processing;colored noise;enhancement parameter;salt and pepper grey scale noise;edge detection;color;surface motion;level set;procesamiento imagen;shape recovery;motion estimation;pde based algorithms;image smoothing;min max curvature;traitement image;image smoothing noise removal image enhancement shape recovery level set formulation surface motion pde based algorithms min max curvature mean curvature salt and pepper grey scale noise full image continuous noise black and white images grey scale images texture images color images enhancement parameter stopping criteria image dependent speed function;image texture;reduccion ruido;deteccion contorno;texture images;detection contour;reconstruction image;minimax techniques;image enhancement;shape;color images;reconstruccion imagen;image reconstruction;noise reduction;reduction bruit;full image continuous noise;level set formulation;noise shaping;image dependent speed function;maximum principle;mean curvature;pepper;switches;grey scale images;noise removal;curves and surfaces;tracking;black and white images;noise;minimax techniques image enhancement image texture motion estimation noise maximum principle	We present a unified approach to noise removal, image enhancement, and shape recovery in images. The underlying approach relies on the level set formulation of the curve and surface motion, which leads to a class of PDE-based algorithms. Beginning with an image, the first stage of this approach removes noise and enhances the image by evolving the image under flow controlled by min/max curvature and by the mean curvature. This stage is applicable to both salt-and-pepper grey-scale noise and full-image continuous noise present in black and white images, grey-scale images, texture images, and color images. The noise removal/enhancement schemes applied in this stage contain only one enhancement parameter, which in most cases is automatically chosen. The other key advantage of our approach is that a stopping criteria is automatically picked from the image; continued application of the scheme produces no further change. The second stage of our approach is the shape recovery of a desired object; we again exploit the level set approach to evolve an initial curve/surface toward the desired boundary, driven by an image-dependent speed function that automatically stops at the desired boundary.	algorithm;choose (action);grayscale;image editing;maxima and minima;noise reduction;population parameter;stage level 1;stage level 2	Ravi Malladi;James A. Sethian	1996	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.541425	image texture;computer vision;mathematical optimization;colors of noise;edge detection;noise shaping;image processing;shape;computer science;noise;level set;mean curvature;motion estimation;noise reduction;mathematics;tracking;maximum principle	Vision	52.897468711927544	-68.88702457488358	7389
24784424e6deebc3853d86934d506de5dc049ba3	automatic recognition of arabic sign language finger spelling.	sign language	In this paper, the task of sign language recognition at sentence level is addressed. The idea of Sign Energy Image (SEI) and a method of extracting Fuzzy-Gaussian Local Binary Pattern (FzGLBP) features from SEI to characterize the sign are explored. The suitability of interval valued type symbolic data for efficient representation of signs in the knowledgebase is studied. A Chi-square proximity measure is used to establish matching between reference and test signs. A simple nearest neighbor classification technique is used for recognizing signs. Extensive experiments are conducted to study the efficacy of the proposed system. A data base of signs called UoM-ISL is created for experimental analysis.	chi;database;experiment;knowledge base;software engineering institute;isl	Mohammad A. Al-Rousan;Mohammed Hussain	2001	I. J. Comput. Appl.		natural language processing;manually coded language;sign language;computer science	AI	28.26419036333828	-65.30283970359211	7390
d78ab5d6456b82b800514a20b2c82897d8a3af15	inference of species phylogenies from bi-allelic markers using pseudo-likelihood		Motivation Phylogenetic networks represent reticulate evolutionary histories. Statistical methods for their inference under the multispecies coalescent have recently been developed. A particularly powerful approach uses data that consist of bi-allelic markers (e.g. single nucleotide polymorphism data) and allows for exact likelihood computations of phylogenetic networks while numerically integrating over all possible gene trees per marker. While the approach has good accuracy in terms of estimating the network and its parameters, likelihood computations remain a major computational bottleneck and limit the method's applicability.   Results In this article, we first demonstrate why likelihood computations of networks take orders of magnitude more time when compared to trees. We then propose an approach for inference of phylogenetic networks based on pseudo-likelihood using bi-allelic markers. We demonstrate the scalability and accuracy of phylogenetic network inference via pseudo-likelihood computations on simulated data. Furthermore, we demonstrate aspects of robustness of the method to violations in the underlying assumptions of the employed statistical model. Finally, we demonstrate the application of the method to biological data. The proposed method allows for analyzing larger datasets in terms of the numbers of taxa and reticulation events. While pseudo-likelihood had been proposed before for data consisting of gene trees, the work here uses sequence data directly, offering several advantages as we discuss.   Availability and implementation The methods have been implemented in PhyloNet (http://bioinfocs.rice.edu/phylonet).	anatomy, regional;big data;cloud research;computation;cyberinfrastructure;estimated;ibm notes;image scaling;inference;large;lineage (evolution);mutation;nucleotides;numerical analysis;numerical integration;partial;phylogenetic network;phylogenetics;population parameter;pseudo brand of pseudoephedrine;requirement;scalability;single-chain antibodies;sorting;statistical model;trees (plant);funding grant	Jiafan Zhu;Luay Nakhleh	2018		10.1093/bioinformatics/bty295	computer science;artificial intelligence;robustness (computer science);phylogenetic tree;machine learning;biological data;phylogenetic network;inference;data set;statistical model;coalescent theory	Comp.	2.6172470160282506	-52.31029107821123	7391
b58ab266d8f366a350aa91dccf2af43bfb784736	transthoracic cardiac stimulation thresholds for short pulses	current 10 9 a transthoracic cardiac stimulation thresholds electric shock ventricular fibrillation current threshold prediction single response cardiac capture transthoracic electrodes repetitive ventricular responses transthoracic charge thresholds tachycardia time 1 ms current 1 12 a time 0 1 ms;fibrillation predictive models electrodes computational modeling electric shock pacemakers diseases;patient treatment bioelectric potentials biomedical electrodes	The most common cause of death due to electric shock is ventricular fibrillation (VF). This work reviews applicable results from the literature and provides an estimation model for the risk of VF with short-duration pulses. Methods and Results - For 1 ms pulses, the predicted current and charge thresholds required for successful transthoracic cardiac stimulation were 1.12 A and 1.12 mC, respectively. For pulses of 0.1 ms durations, the transthoracic current and charge thresholds predicted by the model are 10.9 A and 1.09 mC, respectively. Conclusion - In humans, the charge required for single-response cardiac capture using transthoracic electrodes and 0.1 ms pulses is at least 0.5 mC. The transthoracic charge required to trigger repetitive ventricular responses in humans is at least several times higher than that for single responses. Hence, in adult humans, the transthoracic charge threshold required to induce repetitive ventricular responses, tachycardia, or fibrillation, with 0.1 ms pulses is expected to be significantly greater than 1 mC.	cessation of life;fibrillations;shock from electric current;tachycardia;ventricular fibrillation;electrode	Dorin Panescu;Mark W. Kroll;Michael Brave	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944616	medicine;biological engineering;anesthesia;cardiology	Robotics	17.19633740115386	-83.60634961571107	7430
69707551f9f554ce10454337e85a3e3e7fbd6bb0	relationship between weight of our developed white cane and muscle load on the upper limbs during swinging action of the cane		The present study aimed to investigate the influence of the weight of white canes on upper limb load. Concretely, we conducted quantitative evaluations of the load on upper limb muscles during swinging action of the cane. The white canes used were a new type of white cane newly fabricated using aramid fibers, as well as a conventional type of white cane fabricated using carbon fibers. The results indicated that the newly developed cane reduced the load on the muscles by about 50% in comparison with the conventional type of cane. It became clear that it was possible to sustain the same posture even when used continuously over a long period of time.		Kouki Doi;Atsushi Sugama;Takahiro Nishimura;Akihiko Seo;Shuichi Ino;Kiyohiko Nunokawa;Kazuhiko Kosuge;Akito Miyazaki;Masaaki Sugiyama;Yoshihiro Tanaka;Mayumi Sawada;Ken Kaneko;Susumu Ouchi;Katsuhiro Kanamori	2013		10.1007/978-3-642-39473-7_47	human–computer interaction;biomedical engineering;computer science;carbon fibers;cane;quantitative evaluations	Robotics	14.661112578271183	-82.6864125630207	7482
1f5b3fc4eaac00fb84c995d0adbe67c4a397ec98	learning to detect targets using scale-space and genetic search	genetics;scale space		scale space	Jerzy W. Bala;Harry Wechsler	1993			biology;bioinformatics;machine learning;genetics	Robotics	1.8713021623100372	-63.76262854836798	7485
0a9cb697f71cd867edb9bcd0d983b802c7388cc5	single-histogram class models for image segmentation	modelizacion;image recognition;object recognition;reconocimiento imagen;vision ordenador;base donnee;image segmentation;image processing;database;procesamiento imagen;base dato;image classification;reconnaissance objet;traitement image;computer vision;modelisation;vecino mas cercano;histogram;histogramme;machine learning;segmentation image;classification image;reconnaissance image;pattern recognition;plus proche voisin;nearest neighbour;vision ordinateur;k nearest neighbour;reconnaissance forme;reconocimiento patron;bag of visual words;classification accuracy;histograma;modeling;compact model	Histograms of visual words (or textons) have proved effective in tasks such as image classification and object class recognition. A common approach is to represent an object class by a set of histograms, each one corresponding to a training exemplar. Classification is then achieved by k-nearest neighbour search over the exemplars. In this paper we introduce two novelties on this approach: (i) we show that new compact single histogram models estimated optimally from the entire training set achieve an equal or superior classification accuracy. The benefit of the single histograms is that they are much more efficient both in terms of memory and computational resources; and (ii) we show that bag of visual words histograms can provide an accurate pixel-wise segmentation of an image into object class regions. In this manner the compact models of visual object classes give simultaneous segmentation and recognition of image regions. The approach is evaluated on the MSRC database [5] and it is shown that performance equals or is superior to previous publications on this database.	bag-of-words model in computer vision;boosting (machine learning);computation;computational resource;discriminative model;graph labeling;image segmentation;k-nearest neighbors algorithm;kl-one;kullback–leibler divergence;linear search;markov random field;nearest neighbor search;pattern recognition;pixel;scalability;test set;while	Florian Schroff;Antonio Criminisi;Andrew Zisserman	2006		10.1007/11949619_8	computer vision;computer science;machine learning;pattern recognition	Vision	43.68815056303048	-60.31008798791415	7486
331d12e04c6a02c73585935dab662f6e54927338	comparison of linear and morphological shared-weight neural networks	networks;neural networks;handwritten digit recognition;false alarm rate;feature extraction;networked learning;automatic target recognition;pattern recognition;back propagation;neural network	A hybrid neural network that can learn nonlinear morphological feature extraction and classification simultaneously, called the morphological shared-weight network (MSNN), is described. The feature extraction operation is performed by a gray scale hit-miss transform. The network learns morphological structuring elements by a back-propagation type learning rule. It provides a general problem-independent methodology for designing morphological structuring elements for pattern recognition. The network was applied to handwritten digit recognition and automatic target recognition (ATR) of occluded vehicles and compared to the standard shared-weight neural networks (SSNN) that perform linear feature extraction. For binary handwritten digit recognition, it produced performance comparable to that obtained using existing shared-weight networks. However, it trained faster. For ATR, a set of parking lot images containing a certain type of vehicle was used. An MSNN was trained with non- occluded training vehicles and tested with images containing the training vehicles at various degrees of occlusion. An efficient training method to improve background rejection is introduced. Two target-aim-point selection methods are defined. The MSNN performed significantly better than the SSNN at detecting occluded vehicles and reducing false alarm rates. Furthermore, the morphological network trained significantly faster.© (1996) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Yonggwan Won;Paul D. Gader	1996		10.1117/12.235821	speech recognition;engineering;machine learning;pattern recognition	NLP	28.868109746310424	-55.74678683092172	7499
8f83039868e517d539c009d0e07fa34c1db626ad	enforcing stochastic inverse consistency in non-rigid image registration and matching	least squares approximations;image stitching stochastic inverse consistency nonrigid image registration image matching smoothness constraint post processing algorithm least squares fitting stereo matching;convergence;image matching;stochastic processes image registration biomedical imaging gaussian noise spline robustness cost function tensile stress belief propagation least squares methods;stochastic inverse consistency;noise measurement;image stitching;fitting;non rigid image registration;total least square;stochastic processes image matching image registration least squares approximations stereo image processing;stereo matching;stochastic processes;least squares fitting;belief propagation;image registration;stereo image processing;nonrigid image registration;post processing algorithm;smoothness constraint;image registration and matching;noise	This paper presents a new method to enforce inverse consistency in nonrigid image registration and matching. Conventional approaches assume diffeomorphic transformation, implicitly or explicitly. However, the inherent smoothness constraint discourages discontinuity consideration. We propose a post-processing algorithm that integrates the input forward and backward fields, which are output by existing registration/matching algorithms, to produce more robust results. Given such a pair of input fields, our algorithm alternately refines the fields by tensor belief propagation, and enforces inverse consistency in stochastic sense by generalized total least squares fitting. To show the efficacy of our stochastic inverse consistency approach, we first present results on very noisy fields. We then demonstrate improvement on existing stereo matching where occlusion is naturally handled by localizing violations of inverse consistency. Finally, we propose a novel application on image stitching, where stochastic inverse consistency is employed in structure deformation, in order to seamlessly align overlapping images with severe misalignment in structure and intensity.	algorithm;align (company);belief propagation;computer stereo vision;image registration;image stitching;reflections of signals on conducting lines;software propagation;stochastic gradient descent;total least squares;video post-processing	Sai Kit Yeung;Chi-Keung Tang;Pengcheng Shi;Josien P. W. Pluim;Max A. Viergever;Albert C. S. Chung;Helen C. Shen	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587400	stochastic process;computer vision;mathematical optimization;convergence;image stitching;computer science;noise measurement;noise;image registration;machine learning;pattern recognition;mathematics;least squares;local consistency;statistics;belief propagation	Vision	52.44130475050798	-52.758715583423665	7516
3eec9fb906c164ddce43cef426052c12ac619be4	cerebral serotonin transporter binding is inversely related to body mass index	tobacco smoke;linear regression model;pet;alcohol consumption;healthy volunteer;serotonin transporter;body mass index bmi;imaging;obesity;body weight;serotonin;body mass index;food intake;animal model	Overweight and obesity is a health threat of increasing concern and understanding the neurobiology behind obesity is instrumental to the development of effective treatment regimes. Serotonergic neurotransmission is critically involved in eating behaviour; cerebral level of serotonin (5-HT) in animal models is inversely related to food intake and body weight and some effective anti-obesity agents involve blockade of the serotonin transporter (SERT). We investigated in 60 healthy volunteers body mass index (BMI) and regional cerebral SERT binding as measured with [(11)C]DASB PET. In a linear regression model with adjustment for relevant covariates, we found that cortical and subcortical SERT binding was negatively correlated to BMI (-0.003 to -0.012 BP(ND) unit per kg/m(2)). Tobacco smoking and alcohol consumption did not affect cerebral SERT binding. Several effective anti-obesity drugs encompass blockade of the SERT; yet, our study is the first to demonstrate an abnormally decreased cerebral SERT binding in obese individuals. Whether the SERT has a direct role in the regulation of appetite and eating behaviour or whether the finding is due to a compensatory downregulation of SERT secondary to other dysfunction(s) in the serotonergic transmitter system, such as low baseline serotonin levels, remains to be established.		David Erritzoe;Vibe G. Frokjaer;Mette T. Haahr;Jan Kalbitzer;Claus Svarer;Klaus K. Holst;D. L. Hansen;Terry L. Jernigan;Szabolcs Lehel;Gitte Moos Knudsen	2010	NeuroImage	10.1016/j.neuroimage.2010.03.086	psychology;medical imaging;endocrinology;psychiatry;obesity;radiology;medicine;body weight;pet;diabetes mellitus	HCI	20.128212634142127	-81.15015268096573	7517
b64504a7d6865b1b68d4e2774e8e4bab75ec5aa2	bayesian integrated modeling of expression data: a case study on rhog	statistical approach;software;bayesian hierarchical model;point estimation;integrable model;bayes theorem;databases genetic;computational biology bioinformatics;gene expression;microarray analysis;posterior distribution;differential expression;algorithms;differentially expressed gene;dna microarray;combinatorial libraries;computer appl in life sciences;oligonucleotide array sequence analysis;microarrays;bioinformatics	DNA microarrays provide an efficient method for measuring activity of genes in parallel and even covering all the known transcripts of an organism on a single array. This has to be balanced against that analyzing data emerging from microarrays involves several consecutive steps, and each of them is a potential source of errors. Errors tend to accumulate when moving from the lower level towards the higher level analyses because of the sequential nature. Eliminating such errors does not seem feasible without completely changing the technologies, but one should nevertheless try to meet the goal of being able to realistically assess degree of the uncertainties that are involved when drawing the final conclusions from such analyses. We present a Bayesian hierarchical model for finding differentially expressed genes between two experimental conditions, proposing an integrated statistical approach where correcting signal saturation, systematic array effects, dye effects, and finding differentially expressed genes, are all modeled jointly. The integration allows all these components, and also the associated errors, to be considered simultaneously. The inference is based on full posterior distribution of gene expression indices and on quantities derived from them rather than on point estimates. The model was applied and tested on two different datasets. The method presents a way of integrating various steps of microarray analysis into a single joint analysis, and thereby enables extracting information on differential expression in a manner, which properly accounts for various sources of potential error in the process.	dna microarray;dyes;estimated;gene expression;hierarchical database model;quantity;transcript	Rashi Gupta;Dario Greco;Petri Auvinen;Elja Arjas	2009		10.1186/1471-2105-11-295	biology;dna microarray;computer science;bioinformatics;theoretical computer science;data mining;genetics	Comp.	4.506936135709602	-53.60380214935193	7518
e6df5b3a9202d534b94d8ba40e8db1b6ae259fe5	phased lstm: accelerating recurrent network training for long or event-based sequences		Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.	event-driven programming;frequency band;long short-term memory;memory cell (binary);neural networks;phased array;random neural network;recurrent neural network;run time (program lifecycle phase);sampling (signal processing);sensor;sparse matrix	Daniel Neil;Michael Pfeiffer;Shih-Chii Liu	2016			real-time computing;computer science;artificial intelligence;machine learning	ML	19.091210482816372	-63.80803486030916	7519
6561f1e3a4fc797f674fcc5c2dc8a7de8af76991	will they participate? predicting patients’ response to clinical trial invitations in a pediatric emergency department	predictive modeling;machine learning;patient directed precision recruitment;socioeconomic status	OBJECTIVE (1) To develop an automated algorithm to predict a patient's response (ie, if the patient agrees or declines) before he/she is approached for a clinical trial invitation; (2) to assess the algorithm performance and the predictors on real-world patient recruitment data for a diverse set of clinical trials in a pediatric emergency department; and (3) to identify directions for future studies in predicting patients' participation response.   MATERIALS AND METHODS We collected 3345 patients' response to trial invitations on 18 clinical trials at one center that were actively enrolling patients between January 1, 2010 and December 31, 2012. In parallel, we retrospectively extracted demographic, socioeconomic, and clinical predictors from multiple sources to represent the patients' profiles. Leveraging machine learning methodology, the automated algorithms predicted participation response for individual patients and identified influential features associated with their decision-making. The performance was validated on the collection of actual patient response, where precision, recall, F-measure, and area under the ROC curve were assessed.   RESULTS Compared to the random response predictor that simulated the current practice, the machine learning algorithms achieved significantly better performance (Precision/Recall/F-measure/area under the ROC curve: 70.82%/92.02%/80.04%/72.78% on 10-fold cross validation and 71.52%/92.68%/80.74%/75.74% on the test set). By analyzing the significant features output by the algorithms, the study confirmed several literature findings and identified challenges that could be mitigated to optimize recruitment.   CONCLUSION By exploiting predictive variables from multiple sources, we demonstrated that machine learning algorithms have great potential in improving the effectiveness of the recruitment process by automatically predicting patients' participation response to trial invitations.		Yizhao Ni;Andrew F. Beck;Regina Taylor;Jenna Dyas;Imre Solti;Jacqueline Grupp-Phelan;Judith W. Dexheimer	2015		10.1093/jamia/ocv216	predictive analytics;simulation;medicine;computer science;socioeconomic status;machine learning;data mining	ML	6.6288796322560986	-75.63742188955649	7524
9bf081b6dd065c05dd25ebc12035406ad60f5b15	case based time series prediction using biased time warp distance for electrical evoked potential forecasting in visual prostheses	electrical evoked potential;bias configuration;distance metric;temporal similarity measure;time series prediction	Case based time series prediction (CTSP) is a machine learning technique to predict the future behavior of the current time series by referring similar old cases. To reduce the cost of the visual prostheses research, we devote to the investigation of predictive performance of CTSP in electrical evoked potential (EEP) prediction instead of doing numerous biological experiments. The heart of CTSP for EEP prediction is a similarity measure of training case for target electrical stimulus by using distance metric. As EEP experimental case consists of the stationary electrical stimulation values and time-varying EEP elicited values, this paper proposes a new distance metric which takes the advantage of point-to-point distance’s efficient operation in stationary data and time series distance’s high capability in temporal data, called as biased time warp distance (BTWD). In BTWD metric, stimulation set difference (Diff I) and EEP sequence difference (Diff II) are calculated respectively, and a time-dependent bias configuration is added to reflect the different influences of Diff I and Diff II to the numerical computation of BTWD. Similarity-related adaptation coefficient summation is employed to yield the predictive EEP values at given time point in principle of k nearest neighbors. The proposed predictor using BTWD was empirically tested with data collected from the electrophysiological EEP eliciting experiments. We statistically validated our results by comparing them with other predictor using classical point-to-point distances and time series distances. The empirical results indicated that our proposed method produces superior performance in EEP prediction in terms of predictive accuracy and computational complexity. © 2012 Elsevier B.V. All rights reserved.	computation;computational complexity theory;dynamic time warping;experiment;fibre channel point-to-point;functional electrical stimulation;k-nearest neighbors algorithm;kerrison predictor;machine learning;matthews correlation coefficient;numerical analysis;point-to-point protocol;similarity measure;stationary process;time series;uk educational evidence portal;visual prosthesis	Jin Qi;Jie Hu;Ying-hong Peng;Xinyu Chai;Qiushi Ren	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.11.047	econometrics;metric;artificial intelligence;machine learning;time series;mathematics;statistics	ML	20.12234050944922	-74.53774473560814	7532
4033a7fd1d173e716c39ec0d567f05dfe9cc40a7	patch-based dti grading: application to alzheimer's disease classification		Early diagnosis is one of the most important challenges related to Alzheimer's disease (AD). To address this issue, numerous studies proposed biomarkers based on anatomical MRI. Among them, patchbased grading demonstrated state-of-the-art results when applied to T1weighted MRI. In this work, we propose to use a similar framework on di erent di usion parameters extracted from DTI. We also propose to use a fast patch-based search strategy to provide novel biomarkers for the early detection of AD. We intensively compare our new grading-based DTI features with basic MRI/DTI biomarkers and evaluate our method within a cross validation classi cation framework. Finally, we demonstrate that the proposed biomarkers obtain competitive results for the identi cation of the di erent stages of AD.	ambiguous name resolution;central processing unit;complementarity theory;cross-validation (statistics);feature extraction	Kilian Hett;Ta Vinh Thong;Rémi Giraud;Mary Mondino;José V. Manjón;Pierrick Coupé	2016		10.1007/978-3-319-47118-1_10	pathology	ML	31.438450377838507	-77.98089327575218	7536
2a58eef592ec7a2262f3ed0f8657e8ff9f5871fc	subspace reduction for appearance-based navigation of a mobile robot	visual robot navigation;image resolution;navigation mobile robots principal component analysis data mining computational efficiency image databases histograms feature extraction robot vision systems cameras;mobile robot;robot navigation;mobile robots;robot vision;principal component analysis;translational speed subspace reduction appearance based navigation mobile robot visual robot navigation pca subspace image resolution partial occlusions;robot vision image resolution mobile robots principal component analysis;non structural;subspace reduction;partial occlusions;appearance based navigation;translational speed;pca subspace	The appearance-based approach in visual robot navigation supposes several advantages, such as its application to non-structured environments and the relatively simple extraction of control laws that it offers. However, the main drawback is the requirement of extensive memories and the high computational cost. This way, the nature and the quantity of information to store about the environment is very important. This work presents how to reduce the dimension of the database, by means of calculating just the most significant information of each image. We show how it can be done working in the PCA subspace. This method allows lowering the computational cost without necessity of reducing the resolution of the images, what implies that it could be used in very non-structured environments, in the presence of partial occlusions and with considerably high translational speed of the robot.	algorithmic efficiency;computation;image resolution;mobile robot;principal component analysis;robotic mapping	Luis Payá;Óscar Reinoso;Maria Asunción Vicente;Arturo Gil;Jose Manuel Pedrero	2007	14th International Conference on Image Analysis and Processing (ICIAP 2007)	10.1109/ICIAP.2007.121	mobile robot;computer vision;simulation;computer science;machine learning	Robotics	42.913699341422216	-52.871331339465414	7538
6fe47fb17ddedd4cbacabe2d466bfa8de8b281a2	myogenic potential pattern discernment method using genetic programming for hand gesture	hand gesture genetic programming mutation rate crossover rate electromyograph;gp parameters myogenic potential pattern discernment method hand gesture discernment surface electromyogram rock paper scissors genetic programming technique optimum classification algorithm hand gesture classification;mutation rate;genetic programming;hand gesture;medical signal processing electromyography genetic algorithms gesture recognition;electromyograph;muscles electromyography thumb genetic programming classification algorithms electrodes;crossover rate	The authors study on the hand gesture discernment based on the surface electromyogram of forearm. In order to discern finger shapes of the rock-paper-scissors, genetic programming technique is applied to establish the optimum classification algorithm of hand gestures by composing of arithmetic functions. We measur myoelectric potential signals of forearm related to rock-paper-scissors, and applies them to genetic evolution of hand gesture classification. We also evaluated the effects of the target number of nodes, crossover rate, mutation rate of GP parameters. Realtime hand gesture identification experiments are carried out and the typical hand gestures are actually distinguished in accuracy of 99%.	algorithm;electromyography;experiment;genetic programming	Takahiro Hashimoto;Takeshi Tsujimura;Kiyotaka Izumi	2014	2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2014.7044713	genetic programming;mutation rate;speech recognition;computer science;artificial intelligence	Robotics	14.506848570726744	-91.13781437586799	7541
bc8fef276b0f7949db5009d3d0b7a25e5a300702	skin detection for single images using dynamic skin color modeling	modelo dinamico;modelizacion;metodo adaptativo;cluster algorithm;skin detection;clustering algorithm;dynamic model;methode adaptative;adaptive skin color model;algorithme;modelisation;algorithm;accuracy;skin color;precision;modele dynamique;adaptive method;signal classification;poursuite cible;classification signal;classification automatique;target tracking;bayes classifier;automatic classification;modeling;clasificacion automatica;algoritmo	Up-to-date skin detection techniques use adaptive skin color modeling to overcome the varying skin color problem. Most methods for tracking skin regions in videos utilize the correlation between contiguous frames. This paper proposes a new approach for detecting skin in a single image. This approach uses a local skin model to shift a globally trained skin model to adapt the final skin model to the current image. Experimental results show that the proposed method can achieve better accuracy. Two improvements for speeding up the processing are also discussed.	autostereogram	Hung-Ming Sun	2010	Pattern Recognition	10.1016/j.patcog.2009.09.022	computer vision;speech recognition;computer science;artificial intelligence;machine learning;accuracy and precision;statistics	Vision	46.178015435193416	-58.43990773314694	7567
5942eacf87229b14c0234726d2c4c9326076c0ad	image classification of bowel abnormalities and ischemia	computed tomography;medical image processing computerised tomography feature extraction image classification medical disorders;pattern classification intestinal abnormalities computed tomography indicator kriging;indicator kriging;image classification uncertainty mapping model diagnostic detection feature extraction computed tomography image spatial uncertainty gastrointestinal disorders automated identification computerized systems inadequate blood supply intestine injury intestine inflammation intestinal abnormality ischemia bowel abnormality;pattern classification;computed tomography uncertainty medical diagnostic imaging feature extraction attenuation pattern recognition;intestinal abnormalities	Intestinal abnormalities and ischemia are medical conditions in which inflammation and injury of the intestine are caused by inadequate blood supply. Developments of computerized systems for the automated identification of these types of complex gastrointestinal disorders are rarely reported. In this paper, we introduce a mapping model of spatial uncertainty in computed tomography images for feature extraction, which can be effectively applied for diagnostic detection. Experimental results obtained from the analysis of clinical data suggest the usefulness of the proposed uncertainty mapping model.	ct scan;feature extraction;tomography	Tuan D. Pham;Taichiro Tsunoyama;Truong Cong Thang;Takashi Fujita;Tetsuya Sakamoto	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025559	computer vision;computed tomography	Robotics	35.140937082957905	-77.28062363618552	7568
ab63f538da7ceab0f3887bab47da2db51fccfc43	a stylistic human motion editing system based on a subspace motion model	modelizacion;animacion por computador;mouvement corporel;estimation mouvement;estimacion movimiento;motion styles;vector space;editing system;human motion editing;motion estimation;style;modelisation;motion capture;transferencia movimiento;transfert mouvement;motion simulation;human motion;character animation;espace vectoriel;computer animation;movimiento corporal;modeling;espacio vectorial;motion transfer;body movement;motion modelling;animation par ordinateur	In this paper, a stylistic human motion editing system is presented. This system concentrates on extracting and editing motion styles from motion pairs. A new low-dimensional motion model is introduced into this system. In this model, motion style is defined quantitatively as a low-dimensional subspace of motion data and can be obtained easily from a motion pair. This system provides several style editing tools and accessorial modules based on the proposed motion model. Using these tools and modules, animators can not only translate the style of the original motions but also transfer and add styles between two motions.	kinesiology	Gengdai Liu;Zhigeng Pan;Xi Cheng	2010	IJCAT	10.1504/IJCAT.2010.034134	character animation;computer vision;match moving;structure from motion;motion capture;simulation;systems modeling;vector space;computer science;motion estimation;computer animation;motion field;computer graphics (images)	Robotics	49.64927170984569	-55.523574376455024	7577
4020e64ee2afbf1cb2b865d31b6492cac1663642	warnings and caveats in brain controllability	brain controllability;brain networks;complex networks;null models;whole brain modelling	"""A recent article by Gu et al. (Nat. Commun. 6, 2015) proposed to characterize brain networks, quantified using anatomical diffusion imaging, in terms of their """"controllability"""", drawing on concepts and methods of control theory. They reported that brain activity is controllable from a single node, and that the topology of brain networks provides an explanation for the types of control roles that different regions play in the brain. In this work, we first briefly review the framework of control theory applied to complex networks. We then show contrasting results on brain controllability through the analysis of five different datasets and numerical simulations. We find that brain networks are not controllable (in a statistical significant way) by one single region. Additionally, we show that random null models, with no biological resemblance to brain network architecture, produce the same type of relationship observed by Gu et al. between the average/modal controllability and weighted degree. Finally, we find that resting state networks defined with fMRI cannot be attributed specific control roles. In summary, our study highlights some warning and caveats in the brain controllability framework."""	anatomic node;anatomy, regional;complex network;control theory;diffusion anisotropy;electroencephalography;modal logic;network address translation;network architecture;null value;numerical analysis;rest;resting state fmri;simulation	Chengyi Tu;Rodrigo P. Rocha;Maurizio Corbetta;Sandro Zampieri;Marco Zorzi;Samir Suweis	2018	NeuroImage	10.1016/j.neuroimage.2018.04.010	brain activity and meditation;machine learning;cognitive psychology;resting state fmri;controllability;psychology;network architecture;complex network;artificial intelligence	ML	21.005697255704064	-76.15266587157146	7581
9f337b21b53558d2afbab9ae5c979d9c5e7b0f4f	hierarchical spatiotemporal feature extraction using recurrent online clustering	unsupervised feature extraction;online clustering;deep machine learning;spatiotemporal signals;recurrent clustering;pattern recognition	Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN – a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node – that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scala-bility properties of the proposed framework. The mainstream approach for addressing high-dimensional data is to employ a feature extraction process charged with mapping the data to a lower-dimensional space, while retaining as much information as possible in order to accurately process (e.g. classify) the data. As a result, it is argued that the intelligence behind most pattern recognition engines rests on the human-engineered feature extraction process used. At times, such feature extraction can be challenging to construct and highly application-dependent. Moreover, if incomplete or erroneous features are produced, the classification process is inherently limited in its performance. Recent research into the mammalian brain has inspired new ideas for designing systems that represent information. This research claims that the neocortex, which is responsible for many cognitive abilities, does not perform explicit pre-processing of sensory signals. Instead, these signals are propagated through a complex hierarchy of modules (Lee and Mumford, 2003), that learn to represent the information based on the regularities that exist in the signals over time. This finding led to the emergence of the field of deep machine learning (DML) (Arel et al., 2010), which focuses on extracting data efficiently in an unsupervised manner. While the spatial information in real-life data is important, the temporal information is also very important. When humans observe a sequence of patterns, they can gain an understanding of what is occurring that could not be inferred from a single pattern. We often infer a meaning from events observed in short time periods (Wallis and Bulthoff, 1999; …	algorithm;bayesian network;bottom-up parsing;central processing unit;cluster analysis;cognition;computer vision;deep belief network;deep learning;emergence;feature extraction;general-purpose modeling;graphics processing unit;inference engine;mnist database;machine learning;pattern recognition;preprocessor;real life;requirement;scala;scalability;semi-supervised learning;semiconductor industry;streaming media;temporal logic;top-down and bottom-up design;unsupervised learning	Steven R. Young;Andrew S. Davis;Aaron Mishtal;Itamar Arel	2014	Pattern Recognition Letters	10.1016/j.patrec.2013.07.013	computer science;machine learning;pattern recognition;data mining;deep belief network	AI	21.440022927420834	-53.95898789785453	7590
d612a1dd7aa359f1e315a22a825936b4dcb641e2	weakly supervised region proposal network and object detection		The Convolutional Neural Network (CNN) based region proposal generation method (i.e. region proposal network), trained using bounding box annotations, is an essential component in modern fully supervised object detectors. However, Weakly Supervised Object Detection (WSOD) has not benefited from CNN-based proposal generation due to the absence of bounding box annotations, and is relying on standard proposal generation methods such as selective search. In this paper, we propose a weakly supervised region proposal network which is trained using only image-level annotations. The weakly supervised region proposal network consists of two stages. The first stage evaluates the objectness scores of sliding window boxes by exploiting the low-level information in CNN and the second stage refines the proposals from the first stage using a region-based CNN classifier. Our proposed region proposal network is suitable for WSOD, can be plugged into a WSOD network easily, and can share its convolutional computations with the WSOD network. Experiments on the PASCAL VOC and ImageNet detection datasets show that our method achieves the state-of-the-art performance for WSOD with performance gain of about 3% on average.	computation;convolutional neural network;high- and low-level;imagenet;microsoft customer care framework;minimum bounding box;object detection;open research;sensor;supervised learning;zhi-li zhang	Peng Tang;Xinggang Wang;Pinghao Jia;Yongluan Yan;Wenyu Liu;Junzhou Huang;Alan L. Yuille	2018		10.1007/978-3-030-01252-6_22	machine learning;convolutional neural network;sliding window protocol;computer science;artificial intelligence;object detection;computation;minimum bounding box	Vision	27.400209318449424	-52.91238853305269	7611
50e5e8f47da3576a7fc2a55a0a0624da5141d4f1	a study of repeatability and reproducibility for tongue diagnosis instrument in tcm	traditional chinese medicine tcm repeatability reproducibility tongue diagnosis instrument;instruments;phantoms;image capture tongue diagnosis instrument traditional chinese medicine solid phantom painted tongue body color distance phantom images precision to tolerance ratio;biological organs;traditional chinese medicine tcm;tongue instruments image color analysis analysis of variance medical diagnostic imaging educational institutions;image color analysis;analysis of variance;tongue;biomedical optical imaging;tongue diagnosis instrument;biomedical measurement;biomedical equipment;cameras;repeatability;medical diagnostic imaging;reproducibility;phantoms biological organs biomedical equipment biomedical measurement biomedical optical imaging cameras	We design a solid phantom with painted tongue body as a simulation of human tongue and analyze the average color distance of the phantom images to evaluate the repeatability and reproducibility of this instrument. The color difference is applied as a tolerance of Precision-to-Tolerance ratio (P/T) in QS9000. The P/T ratio of total measurement is used to indicate whether the instrument is correctly built or in good condition for continuous use. We may find some confounding factors of these instruments through this examination. The result shows the unsteady condition for image capture by the instrument or operator factors such as light, vibration, operators training and so on could be adjusted or modified. The study of repeatability and reproducibility needs to be performed when a new tongue diagnosis instrument is being introduced, and could help TCM doctors to establish a standard tongue diagnosis environment.	computational human phantom;computer cooling;phantom reference;repeatability;simulation	Chiung-Hung Chiang;Tzung-Yan Lee;Kang-Ping Lin;Su-Tso Yang;Juei-Chao Chen;Hen-Hong Chang	2012	2012 IEEE 14th International Conference on e-Health Networking, Applications and Services (Healthcom)	10.1109/HealthCom.2012.6380054	computer vision;medicine;pathology;biological engineering	Visualization	28.532260874301677	-83.87726910087669	7640
d3ef886570649f3d4abf405f6826ba7744280f26	automatic detection system of micro sleeps of car drivers based on eeg analysis	microprocessor;brain computing;eeg analysis;computer systems;experiments;car simulator	Many car incidents happen because the driver falls asleep. The onset of micro-sleeps can be detected by decreasing activity of alpha brain waves and increasing activity of delta brain waves. To develop an automated system for micro-sleeps detection an analysis tool for EEG recordings has been developed. The system has been tested in several experiments of drivers in a car simulator. The developed prototype, experiments and experimental results will be reported in this paper.	device driver;electroencephalography;experiment;neural oscillation;onset (audio);prototype;simulation	Léon J. M. Rothkrantz	2016		10.1145/2983468.2983487	embedded system;simulation;computer hardware	Robotics	11.369606570811916	-89.54330366622281	7642
d38685e36e41a927ede7168f2dd5eb2206bae930	robust road detection from a single image using road shape prior	graph theory;graph cuts road detection shape prior;image segmentation;roads;visual databases graph theory image segmentation object detection roads;robust road detection sun database training data graph cut segmentation framework prelearned information road shape prior single image;object detection;visual databases	Many road detection algorithms require pre-learned information, which may be unreliable as the road scene is usually unexpectable. Single image based (i.e., without any pre-learned information) road detection techniques can be adopted to overcome this problem, while their robustness needs improving. To achieve robust road detection from a single image, this paper proposes a general road shape prior to enforce the detected region to be road-shaped by encoding the prior into a graph-cut segmentation framework, where the training data is automatically generated from a predicted road region of the current image. By iteratively performing the graph-cut segmentation, an accurate road region will be obtained. Quantitative and qualitative experiments on the challenging SUN Database validate the robustness and efficiency of our method. We believe that the road shape prior can also be used to yield improvements for many other road detection algorithms.	algorithm;autostereogram;cut (graph theory);experiment;graph cuts in computer vision;robustness (computer science)	Zhen He;Tao Wu;Zhipeng Xiao;Hangen He	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738568	computer vision;computer science;graph theory;machine learning;pattern recognition;mathematics;image segmentation	Robotics	44.83338533859063	-52.50642651298303	7647
517a0e87e590c015391ed53ed916801c199b0d8a	a video-based image processing system for the automatic implementation of the eye involuntary reflexes measurements involved in the drug recognition expert (dre)	biology computing;image processing;video based image processing system;video sequences;eye involuntary reflexes measurements;image processing drugs image recognition system testing convergence eyes video sequences law enforcement image sequence analysis feature extraction;feature extraction;law enforcement;blood alcohol concentration;image processing biology computing feature extraction;feature extraction video based image processing system eye involuntary reflexes measurements drug recognition expert horizontal gaze nystagmus test eye convergence test pupil dark room examinations test video sequences;horizontal gaze nystagmus test;drug recognition expert;eye convergence test;pupil dark room examinations test	In order to detect drivers under the influence of substances such as alcohol or drugs, police officers use a standardized set of tests such as the Horizontal Gaze Nystagmus (HGN) test, the eye convergence test and the pupil's reaction to light test. These tests are part of the more complete Drug Recognition Expert (DRE) procedures and are essentially applied to the eyes of a driver. These procedures are performed manually by law enforcement officers. The present work describes a video-based image processing system implementing the HGN test, the convergence test and the pupil's dark room examinations test. This system generates visual stimuli and captures video sequences of the eyes following and reacting to these visual stimuli. The video sequences are processed and analyzed using feature extraction techniques. In the present study, the video-based image processing system is used to detect alcohol related intoxication. This system was tested in an experiment involving 32 subjects dosed to a blood alcohol concentration (BAC) in the interval of 0.04% to 0.22%. In order to demonstrate the effects of alcohol on eye signs comparisons are made between pre-dose and post-dose BAC.	batman: arkham city;blood substitute;device driver;failure rate;feature extraction;image processing;prototype	François Meunier;David Laperrière	2008	2008 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2008.4493592	computer vision;simulation;image processing;feature extraction;computer science;machine learning	Robotics	30.962629592689296	-69.48910302676562	7651
ea04cbfeef937909c4ea1654f05f95ffc7d7419c	contribution to the cerebral forward model by depth electric stimulation and seeg measurements : application in epilepsy. (contribution au modèle direct cérébral par stimulation électrique de profondeur et mesures seeg : application a l'épilepsie)			functional electrical stimulation;sacral nerve stimulation	Janis Hofmanis	2013				ML	24.986690173873622	-85.25266410099174	7652
5ba1816c020c514d52bf6c504b01903379bdc033	lexical-semantic activation in broca's and wernicke's aphasia: evidence from eye movements	female;lexical semantics;aphasia wernicke;middle aged;reference values;male;semantics;aging;brain mapping;lexical processing;language tests;adult;case control studies;eye movements;eye movement;cerebral cortex;normal control;aphasia broca;humans;eye tracking;semantic relations;verbal behavior;reaction time;comprehension;aged	Lexical processing requires both activating stored representations and selecting among active candidates. The current work uses an eye-tracking paradigm to conduct a detailed temporal investigation of lexical processing. Patients with Broca's and Wernicke's aphasia are studied to shed light on the roles of anterior and posterior brain regions in lexical processing as well as the effects of lexical competition on such processing. Experiment 1 investigates whether objects semantically related to an uttered word are preferentially fixated, for example, given the auditory target hammer, do participants fixate a picture of a nail? Results show that, like normal controls, both groups of patients are more likely to fixate on an object semantically related to the target than an unrelated object. Experiment 2 explores whether Broca's and Wernicke's aphasics show competition effects when words share onsets with the uttered word, for instance, given the auditory target hammer, do participants fixate a picture of a hammock? Experiment 3 investigates whether these patients activate words semantically related to onset competitors of the uttered word, for example, given the auditory target hammock, do participants fixate a nail due to partial activation of the onset competitor hammer? Results of Experiments 2 and 3 show pathological patterns of performance for both Broca's and Wernicke's aphasics under conditions of lexical onset competition. However, the patterns of deficit differed, suggesting different functional and computational roles for anterior and posterior areas in lexical processing. Implications of the findings for the functional architecture of the lexical processing system and its potential neural substrates are considered.	broca aphasia;cd244 protein, human;computation;experiment;eye abnormalities;eye tracking;lexicon;movement;onset (audio);patients;physical object;programming paradigm;wernicke aphasia;wernicke encephalopathy	Eiling Yee;Sheila E. Blumstein;Julie C. Sedivy	2008	Journal of Cognitive Neuroscience	10.1162/jocn.2008.20056	psychology;cognitive psychology;developmental psychology;semantics;linguistics;communication;eye movement	NLP	16.73381875339551	-77.89508173414895	7654
7d2301e3337657d25bec8f02461f94571b689238	the smal web server: global multiple network alignment from pairwise alignments		MOTIVATION Alignments of protein-protein interaction networks (PPIN) can be used to predict protein function, study conserved aspects of the interactome, and to establish evolutionary correspondences. Within this problem context, determining multiple network alignments (MNA) is a significant challenge that involves high computational complexity. A limited number of public MNA implementations are available currently and the majority of the pairwise network alignment (PNA) algorithms do not have MNA counterparts. Furthermore, current MNA algorithms do not allow choosing a specific PPIN relative to which an MNA could be constructed. Also, once an MNA is obtained, it cannot easily be modified, such as through addition of a new network, without expensive re-computation of the entire MNA.   RESULTS SMAL (Scaffold-Based Multiple Network Aligner) is a public, open-source, web-based application for determining MNAs from existing PNAs that addresses all the aforementioned challenges. With SMAL, PNAs can be combined rapidly to obtain an MNA. The software also supports visualization and user-data interactions to facilitate exploratory analysis and sensemaking. SMAL is especially useful when multiple alignments relative to a particular PPIN are required; furthermore, SMAL alignments are persistent in that existing correspondences between networks (obtained during PNA or MNA) are not lost as new networks are added. In comparative studies alongside existent MNA techniques, SMAL MNAs were found to be superior per a number of measures, such as the total number of identified homologs and interologs as well as the fraction of all identified correspondences that are functionally similar or homologous to the scaffold. While directed primarily at PPIN-alignment, SMAL is a generic network aligner and may be applied to arbitrary networks.Availability information: The SMAL web server and source code is available at: http://haddock6.sfsu.edu/smal/ CONTACT: rahul@sfsu.eduSupplementary information: Supplementary data are available at Bioinformatics online.		Jakob Dohrmann;Rahul Singh	2016	Bioinformatics	10.1093/bioinformatics/btw402	computer science;protein interaction map;data mining;problem context;computational complexity theory;software;visualization;web server;source code;pairwise comparison	Comp.	0.31036583893296993	-58.58674961445771	7706
4404301a74bd97aeed6a2148a85985609a56698c	curve segmentation by relaxation labeling	angle detection;picture processing;curve segmentation;angle detection curve segmentation pattern recognition picture processing scene analysis;pattern recognition;iteration method;scene analysis	"""This correspondence discusses parallel iterative methods of segmenting the border of a shape into """"angles"""" and """"sides."""" Initially, smoothed """"slope"""" and """"curvature"""" values of the border are measured at every point, and the curvature value determines the point's initial probabilities of being an angle or a side. The values are then iteratively adjusted, and the probabilities are reinforced or weakened, in a manner dependent on the values and probabilities at neighboring points. For example, a point p's probability of being on a side is reinforced if p and its neighbors have similar slopes, and our estimate of p's slope can be improved by (say) averaging with these slopes. Similarly, p's probability of being an angle is reinforced if appropriate slope or curvature differences exist between p and its neighbors, and our estimate of p's curvature can be improved by taking the neighbors' slopes into account. A set of such reinforcement and adjustment rules is formulated, and examples are given of their effects on various types of shapes."""	iterative method;linear programming relaxation;smoothing	Larry S. Davis;Azriel Rosenfeld	1977	IEEE Transactions on Computers	10.1109/TC.1977.1674746	computer vision;mathematical optimization;computer science;mathematics;geometry;iterative method	Vision	48.049933678708	-70.02257927724007	7709
671d98e382d1e9ce556008af6229f294203dd9e0	algorithm for the computation of 3d fourier descriptors	vertices 3d fourier descriptor computation iterative watershed algorithm surface graph 3d object classification 3d object recognition polygonized surface unit sphere inflation algorithm polyhedron spherical harmonic functions homogeneous distribution;object recognition;spherical harmonic;image classification;iterative methods;fourier descriptors;image reconstruction;character recognition iterative algorithms diffusion processes iterative methods noise robustness fourier transforms zinc sampling methods;fourier transforms;multidimensional signal processing;image reconstruction object recognition multidimensional signal processing fourier transforms image classification iterative methods	Describes a new approach for the computation of 3D Fourier descriptors, which are used for characterization, classification, and recognition of 3D objects. The method starts with a polygonized surface which is mapped onto a unit sphere using an inflation algorithm, after which the polyhedron is expanded in spherical harmonic functions. Homogeneous distribution of the vertices is achieved by applying an iterative watershed algorithm to the surface graph.	algorithm;computation;fast fourier transform	Jan Sijbers;Tom Ceulemans;Dirk Van Dyck	2002		10.1109/ICPR.2002.1048420	iterative reconstruction;multidimensional signal processing;fourier transform;computer vision;mathematical optimization;contextual image classification;cognitive neuroscience of visual object recognition;mathematics;geometry;iterative method;spherical harmonics	Vision	49.58031493629078	-63.396223140619384	7744
f405404e52a7b07add9c1fc91ae179b4c4cca9e3	a hawkes' eye view of network information flow	dynamic programming;hawkes processes;network theory graphs complex networks dynamic programming information theory;network node behavior network information flow hawkes eye view complex networks common pathway identification multidimensional hawkes processes hawkes process based model capability dynamic programming;signal processing algorithms technological innovation heuristic algorithms signal processing conferences neurons timing;network information flow;point processes;network information flow point processes hawkes processes dynamic programming	An important problem that arises in the analysis of many complex networks is to identify the common pathways that enable the flow of information (or other quantities) through the network. This is a particularly challenging problem when the only information observed consists of the timing of events in the network. We develop a framework based on multidimensional Hawkes processes that can be used to determine how events are related. This extends the capability of Hawkes process-based models to infer how network events relate. We then show how a simple dynamic program can exploit this data to recognize chains of events and provide a much deeper insight into the behavior of nodes within the network. Simulations are provided to demonstrate the capabilities and limitations of this framework.	complex network;computer simulation;information flow (information theory)	Michael G. Moore;Mark A. Davenport	2016	2016 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2016.7551779	computer science;artificial intelligence;theoretical computer science;machine learning	Metrics	23.100743895548547	-74.97076591920073	7773
2729c8e8f4ca28df55ef0678985997a95cca6ef5	efficient discovery of responses of proteins to compounds using active learning	drug discovery;computational biology bioinformatics;proteins;artificial intelligence;algorithms;humans;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Drug discovery and development has been aided by high throughput screening methods that detect compound effects on a single target. However, when using focused initial screening, undesirable secondary effects are often detected late in the development process after significant investment has been made. An alternative approach would be to screen against undesired effects early in the process, but the number of possible secondary targets makes this prohibitively expensive. This paper describes methods for making this global approach practical by constructing predictive models for many target responses to many compounds and using them to guide experimentation. We demonstrate for the first time that by jointly modeling targets and compounds using descriptive features and using active machine learning methods, accurate models can be built by doing only a small fraction of possible experiments. The methods were evaluated by computational experiments using a dataset of 177 assays and 20,000 compounds constructed from the PubChem database. An average of nearly 60% of all hits in the dataset were found after exploring only 3% of the experimental space which suggests that active learning can be used to enable more complete characterization of compound effects than otherwise affordable. The methods described are also likely to find widespread application outside drug discovery, such as for characterizing the effects of a large number of compounds or inhibitory RNAs on a large number of cell or tissue phenotypes.	active learning (machine learning);computation;description;drug discovery;experiment;machine learning;phenotype;predictive modelling;pubchem;silo (dataset);throughput	Joshua D Kangas;Armaghan W. Naik;Robert F. Murphy	2013		10.1186/1471-2105-15-143	biology;dna microarray;computer science;bioinformatics;data science;drug discovery	ML	8.297030259490182	-54.05696228101246	7781
18d5fc8a3f2c7e9bac55fff40e0ecf3112196813	performance analysis of classification algorithms on medical diagnoses-a survey	high dimensional dataset;c5 0;classification;medical diagnosis	Corresponding Author: Vanaja, S., Research Scholar and Research guide, Research and Development, Bharathiar University, Coimbatore, Tamil Nadu, India Email: vanajasha@yahoo.com Abstract: The aim of this research paper is to study and discuss the various classification algorithms applied on different kinds of medical datasets and compares its performance. The classification algorithms with maximum accuracies on various kinds of medical datasets are taken for performance analysis. The result of the performance analysis shows the most frequently used algorithms on particular medical dataset and best classification algorithm to analyse the specific disease. This study gives the details of different classification algorithms and feature selection methodologies. The study also discusses about the data constraints such as volume and dimensionality problems. This research paper also discusses the new features of C5.0 classification algorithm over C4.5 and performance of classification algorithm on high dimensional datasets. This research paper summarizes various reviews and technical articles which focus on the current research on Medical diagnosis.	c4.5 algorithm;email;feature selection;profiling (computer programming);statistical classification	S. Vanaja;K. Rameshkumar	2015	JCS	10.3844/jcssp.2015.30.52	biological classification;computer science;data science;medical diagnosis;data mining;information retrieval	ML	-0.5304070081498753	-74.2449384515982	7806
eb96362636bd423a8cf4a18abe11765d4d749df4	biologically-inspired dense local descriptor for indirect immunofluorescence image classification	gaussian processes;image classification;sampling methods;bag of words;biologically-inspired dense local descriptor;discriminative representation;indirect immunofluorescence image classification;log-polar sampling;spatially-varying gaussian smoothing;staining patterns;hep-2000 cells classification;dense local descriptors;indirect immunofluorescence images	This work deals with the design of a classification method for cells extracted from Indirect Immunofluorescence images. In particular, we propose to use a dense local descriptor invariant both to scale changes and to rotations in order to classify the six categories of staining patterns of the cells. The descriptor is able to give a compact and discriminative representation and combines a log-polar sampling with spatially-varying gaussian smoothing applied on the gradients images in specific directions. Bag of Words is finally used to perform classification and experimental results show very good performance.	bag-of-words model;gaussian blur;gradient;sampling (signal processing);smoothing	Diego Gragnaniello;Carlo Sansone;Luisa Verdoliva	2014	2014 1st Workshop on Pattern Recognition Techniques for Indirect Immunofluorescence Images	10.1109/I3A.2014.19	computer vision;machine learning;pattern recognition;mathematics	Vision	32.38070018968288	-55.05460231162234	7818
02027b62b424ea78cf085b2de817e3c763d5e173	computational analysis of gefitinib and methylated-hydroxypropylated cyclodextrin inclusion complexes for the treatment of childhood malignancies		Cyclodextrins (CD) have been playing a very important role in the formulation of poorly water-soluble drugs by improving apparent drug solubility and/or dissolution. In the current study we focused on the investigation of the cytotoxic effect of cyclodextrin/Gefitinib in inclusion with three types of cyclodextrin, as well as in its inhibitory activity of Epidermal Growth Factor Receptor (EGFR) tyrosine kinase activity using biological assays. The activity of Gefitinib and Gefitinib/ cyclodextrin were measured in pediatric neuroblastoma (NB) tumor cell lines. We have shown that CD/Gefitinib complexes are effective against neuroblastoma cells in vitro. Our studies provided a detailed picture of inclusion of Gefitinib in cyclodextrins and verify improved efficacy of the CD-encapsulated drug in cytotoxicity assays on pediatric tumor cell lines. An understanding of the structural details of guest inclusion in CDs may be useful in the engineering of modified guest-host preparations with optimized pharmacological properties and shape future therapeutic strategies.	computation;naive bayes classifier	Kyriaki Hatziagapiou;Maria Braoudaki;Kostas Bethanis;Konstantina Yannakopoulou;Athanasios Anastasiou;Georgia D. Koutsouri;Dimitrios D. Koutsouris;George I. Lambrou	2018	2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)	10.1109/BHI.2018.8333402	drug;neuroblastoma;cyclodextrin;epidermal growth factor receptor;tyrosine kinase;in vitro;cytotoxicity;gefitinib;cancer research;chemistry	Visualization	9.390875282529361	-62.04475622502002	7834
ab40dba3b264581e33ac68de81f2870be3a38a17	evaluation of band generation process for classification of cerebrospinal fluid in magnetic resonance images	fluids;irrigation;support vector machines;training;magnetic resonance imaging;band expansion process;lumbar spinal stenosis;unsupervised classification;support vector machine;correlation;conferences;cerebrospinal fluid	Cross section area (CSA) of spinal canal has been an important indicator for lumbar spinal stenosis (LSS), which remains the leading preoperative diagnosis for adults older than 65 years. Until recently, the machine learning algorithms had been investigated in [5–7] for an automatic classification system. The automatic classification system exploited the luminance of cerebrospinal fluid (CSF) as the major features. Unfortunately, the limited sequences of magnetic resonance images, which included only T1 and T2 sequences, produced certain level of false alarm and reduced the classification rate. The band expansion process(BEP) proposed in [8] shed light on this issue by generating additional bands with non-linear functions. The idea of BEP unveils the non-linear relationship among sequences to increase the classification rate. The utilities of BEP had been evaluated in brain MR images [9]. This paper would like to extend the applications of BEP for classification of CSF. The experimental studies further demonstrated the benefits of the BEP.	algorithm;computational fluid dynamics;linear function;machine learning;non-volatile memory;nonlinear system;resonance	Kuan-Ru Lee;Chao-Cheng Wu;Yung-Hsiao Chiang;Jiannher Lin	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844908	support vector machine;computer science;magnetic resonance imaging;machine learning	Robotics	30.577788611819457	-77.38936184549507	7836
2a1c825f25c6b078d721b8e652807f8e40b63c2e	emg recurrence quantifications in dynamic exercise	recurrence quantification analysis;left right;physical therapy;exercise therapy;low back pain;motor unit	This study was designed to evaluate the suitability of nonlinear recurrence quantification analysis (RQA) in assessing electromyograph (EMG) signals during dynamic exercise. RQA has been proven to be effective in analyzing nonstationary signals. The subject group consisted of 19 male patients diagnosed with low back pain. EMG signals were recorded from left and right paraspinal muscles during isoinertial exercise both before and after 12 weeks of regimented physical therapy. Autorecurrence analysis was performed between the left and right EMG signals individually, and cross-recurrence analysis was performed on the left-right EMG pairs. Spectral analysis of the EMG signals was employed as an independent, objective measure of fatigue. Increase in the RQA variable % determinism during the 90-s dynamic tests was found to be a good marker for fatigue. Before physical therapy, this nonlinear marker revealed simultaneous increases in motor unit recruitment within each pool and between left and right pools. After physical therapy, the motor unit recruitment was less within and between pools, indicative of increased fatigue resistance. Finally, fatigue resistance (less increase in % determinism) correlated well with subjective scores of pain relief. Taken together, these latter results indicate that recurrence analysis may be useful in charting the efficacy of a specific exercise therapy program in reducing low back pain by elevating the fatigue threshold.	contract agreement;cross-recurrence quantification;detection error tradeoff;electromyographs;electromyography;fatigue;low back pain;motor unit;muscle;nonlinear system;pain relief;paraspinal muscles;patients;physical therapy exercises;protocols documentation;quantitation;recurrence quantification analysis;recurrence relation;spectrum analyzer;diethyltoluamide	Yiwei Liu;Markku Kankaanpää;Joseph P. Zbilut;Charles L. Webber	2004	Biological Cybernetics	10.1007/s00422-004-0474-6	recurrence quantification analysis;physical medicine and rehabilitation;mathematics	ML	17.820449200350556	-84.53341842267024	7842
9455629ee18266d3054a25b6096f92c8b0252725	color-based skin segmentation: an evaluation of the state of the art	skin face recognition gesture recognition image colour analysis image segmentation;training data color based skin segmentation state of the art evaluation face detection gesture recognition multiple datasets ecu dataset;skin decision support systems image color analysis color image segmentation training lighting	Skin segmentation is widely used, e.g. in face detection and gesture recognition. In the last years, the number of skin segmentation approaches has grown. However, multiple datasets and varying performance measurements make direct comparison difficult. We address these shortcomings and evaluate 5 threshold-based methods, 5 model-based methods, and 2 region-based state-of-the-art skin segmentation methods. We discuss each algorithm and provide the segmentation performance along with the processing time. All methods are evaluated on the ECU dataset which provides a great amount of training data besides other important attributes.	algorithm;engine control unit;face detection;gesture recognition	Frerk Saxen;Ayoub Al-Hamadi	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025906	computer vision;speech recognition;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	32.19826469085382	-57.215022083051245	7873
7ae1d58a47a28149690e271aa6509e9a364daf5b	steady-state-preserving simulation of genetic regulatory systems		A novel family of exponential Runge-Kutta (expRK) methods are designed incorporating the stable steady-state structure of genetic regulatory systems. A natural and convenient approach to constructing new expRK methods on the base of traditional RK methods is provided. In the numerical integration of the one-gene, two-gene, and p53-mdm2 regulatory systems, the new expRK methods are shown to be more accurate than their prototype RK methods. Moreover, for nonstiff genetic regulatory systems, the expRK methods are more efficient than some traditional exponential RK integrators in the scientific literature.	exponential distribution;linux/rk;numerical analysis;numerical integration;prototype;runge–kutta methods;scientific literature;simulation;steady state	Ruqiang Zhang;Julius Osato Ehigie;Xilin Hou;Xiong You;Chunlu Yuan	2017		10.1155/2017/2729683	bioinformatics;algorithm	HPC	16.253064127562915	-59.93584404434643	7879
5324cf51bd5a9fdb74722a54a3e07060ec007158	automated delineation of lung tumors in pet images based on monotonicity and a tumor-customized criterion	fuzzy c mean;adaptive thresholding;image segmentation;computed tomography;positron emission tomography lungs computed tomography lesions pixel polynomials;edge detection;lungs;pet imaging;nonsmall cell lung cancer;tumours;tumor delineation lung tumor segmentation nonsmall cell lung cancer nsclc positron emission tomography pet;polynomials;positron emission tomography;lung;positron emission tomography pet;hot spot;qa75 electronic computers computer science;standardized uptake value;lesions;medical image processing;pixel;lung tumor segmentation;dice s similarity coefficient automated lung tumor delineation pet images monotonicity tumor customized criterion positron emission tomography tumor boundary definition lung tumor separation tumor customized downhill method tcd method monotonic property standardized uptake value tumor suv tumor heterogeneity;nonsmall cell lung cancer nsclc;tumours edge detection image segmentation lung medical image processing positron emission tomography;automation carcinoma non small cell lung humans lung neoplasms positron emission tomography;tumor delineation	Reliable automated or semiautomated lung tumor delineation methods in positron emission tomography should provide accurate tumor boundary definition and separation of the lung tumor from surrounding tissue or “hot spots” that have similar intensities to the lung tumor. We propose a tumor-customized downhill (TCD) method to achieve these objectives. Our approach includes: 1) automatic formulation of a tumor-customized criterion to improve tumor boundary definition, 2) a monotonic property of the standardized uptake value (SUV) of tumors to separate the tumor from adjacent regions of increased metabolism (“hot spot”), and 3) accounts for tumor heterogeneity. Three simulated lesions and 30 PET-CT studies, grouped into “simple” and “complex” groups, were used for evaluation. Our main findings are that TCD, when compared to the threshold based on 40% and 50% maximum SUV, adaptive threshold, Fuzzy c-means, and watershed techniques achieved the highest Dice's similarity coefficient average for simulation data (0.73) and “complex” group (0.71); the least volumetric error in the “simple” (1.76 mL) and the “complex” group (14.59 mL); and TCD solves the problem of leakage into adjacent tissues when many other techniques fail.	biomarkers, tumor;body tissue;bone structure of spine;coefficient;customize;esophageal tissue;exanthema;extravasation;fluorodeoxyglucose f18;functional discourse grammar;genetic heterogeneity;gradient;loss function;lung neoplasms;mediastinum;non-small cell lung carcinoma;patients;pericardial sac structure;pet therapy;polyethylene terephthalate;positron-emission tomography;positrons;simulation;spectral leakage;spinal cord;standardized uptake value;teller assist unit;vertebral column;voxel;watershed (image processing)	Cherry G. Ballangan;Xiuying Wang;Michael J. Fulham;Stefan Eberl;Yong Yin;David Dagan Feng	2011	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2011.2159307	computer vision;edge detection;radiology;medicine;computer science;thresholding;image segmentation;computed tomography;nuclear medicine;standardized uptake value;hot spot;pixel;polynomial;medical physics	Visualization	40.19886067775027	-77.60896577402144	7882
23db32413bb4cb8a34187e5cd16f44be2797118b	analysis of mr images deformed by magnetic implants	magnetic field effects;mr measurement;phase measurement;magnetic fields;mr images;systematic error;prosthetics biomedical measurement biomedical mri conducting materials magnetic field effects magnetic materials magnetic susceptibility measurement errors medical image processing;electrically conducting materials;magnetic implants;image deformation;magnetically conducting materials;prosthetics;materials;magnetic resonance image;magnetic materials;image phase unwrapping;magnetostatics;phase image processing;conducting materials;systematic errors;mr imaging;magnetic susceptibility;magnetic field measurement;medical image processing;human body;magnetic resonance imaging;magnetic analysis;mr measurement mr images magnetic implants electrically conducting materials magnetically conducting materials human body magnetic resonance imaging magnetic field deformation suppression magnetic susceptibility phase image processing systematic errors;mr images image deformation image phase unwrapping magnetically conducting materials;implants;image analysis;magnetic field deformation suppression;image analysis magnetic analysis implants magnetic field measurement magnetic susceptibility magnetic materials magnetostatic waves conducting materials magnetic resonance imaging phase measurement;magnetic domains;biomedical measurement;measurement errors;magnetostatic waves;biomedical mri;phase unwrapping	Electrically and magnetically conducting materials found in the human body cause major deformations in images obtained by magnetic resonance imaging (MRI). To study the deformations and to develop new MR methods with these deformations suppressed, a method for measuring magnetic susceptibility has been designed for samples that do not produce MR signals. In the paper, a method is described for processing the phase image being measured, in order to greatly eliminate systematic errors in the MR measurement of magnetic susceptibility and to calculate it.	resonance	Karel Bartusek;Zdenek Smékal;Eva Gescheidtova	2008	Third International Conference on Systems (icons 2008)	10.1109/ICONS.2008.19	magnetic resonance imaging;systematic error;quantum mechanics;statistics	Robotics	46.36033583925663	-84.23991086688105	7886
28912aa432f65fd01aac92d7bbd84b245cf1fc46	ultrasound elastography using multiple images	liver ablation;elasticity imaging;ultrasound elastography;expectation maximization em;strain imaging	Displacement estimation is an essential step for ultrasound elastography and numerous techniques have been proposed to improve its quality using two frames of ultrasound RF data. This paper introduces a technique for calculating a displacement field from three (or multiple) frames of ultrasound RF data. To calculate a displacement field using three images, we first derive constraints on variations of the displacement field with time using mechanics of materials. These constraints are then used to generate a regularized cost function that incorporates amplitude similarity of three ultrasound images and displacement continuity. We optimize the cost function in an expectation maximization (EM) framework. Iteratively reweighted least squares (IRLS) is used to minimize the effect of outliers. An alternative approach for utilizing multiple images is to only consider two frames at any time and sequentially calculate the strains, which are then accumulated. We formally show that, compared to using two images or accumulating strains, the new algorithm reduces the noise and eliminates ambiguities in displacement estimation. The displacement field is used to generate strain images for quasi-static elastography. Simulation, phantom experiments and in vivo patient trials of imaging liver tumors and monitoring ablation therapy of liver cancer are presented for validation. We show that even with the challenging patient data, where it is likely to have one frame among the three that is not optimal for strain estimation, the introduction of physics-based prior as well as the simultaneous consideration of three images significantly improves the quality of strain images. Average values for strain images of two frames versus ElastMI are: 43 versus 73 for SNR (signal to noise ratio) in simulation data, 11 versus 15 for CNR (contrast to noise ratio) in phantom data, and 5.7 versus 7.3 for CNR in patient data. In addition, the improvement of ElastMI over both utilizing two images and accumulating strains is statistically significant in the patient data, with p-values of respectively 0.006 and 0.012.	contrast resolution;contrast-to-noise ratio;displacement mapping;elastography;expectation–maximization algorithm;experiment;frame (physical object);imaging phantom;iteratively reweighted least squares;liver and intrahepatic biliary tract carcinoma;liver neoplasms;loss function;mechanics;patients;phantom reference;phantoms, imaging;psychologic displacement;radio frequency;scott continuity;signal-to-noise ratio;simulation;video-in video-out	Hassan Rivaz;Emad Boctor;Michael A. Choti;Gregory D. Hager	2014	Medical image analysis	10.1016/j.media.2013.11.002	computer vision	Vision	45.212558245824226	-82.28679742141013	7898
f07e02226ded9a648a9fe8fa56378188385ddc6b	unsupervised texture segmentation using a nonlinear energy optimization method	texture segmentation;eprints newcastle university;dr sasan mahmoodi;professor bayan sharif;energy optimization;smoothing;open access;algorithms	A nonlinear functional is considered for segmentation of images containing structural textures. A structural texture pattern in an image is characterized by a certain amplitude spectrum, and segmentation of different patterns is obtained by detecting different regions with different amplitude spectra. A gradient-descent-based algorithm is proposed by deriving equations minimizing the functional. This algorithm, implementing the solutions minimizing the functional, is based on the level set method. An effective method employed in this algorithm is shown to be robust in a noisy environment. Experimental results demonstrate that the proposed method outperforms segmentation obtained by using the simulated annealing algorithm based on Gaussian Markov random fields. © 2006 SPIE and IS&T. DOI: 10.1117/1.2234370	algorithm;coefficient;effective method;gradient descent;markov chain;markov random field;mathematical optimization;nonlinear system;sensor;simulated annealing;smoothing;spectral density;statistical learning theory	Sasan Mahmoodi;Bayan S. Sharif	2006	J. Electronic Imaging	10.1117/1.2234370	image texture;computer vision;computer science;artificial intelligence;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;algorithm;statistics;smoothing	Vision	50.989598571718936	-70.43979547605862	7928
0f636e999be64bc9d500a2e662da6e443d19acf1	characterization of sustained bold activation in the rat barrel cortex and neurochemical consequences	barrel cortex;serveur institutionnel;sustained;trigeminal;lactate;archive institutionnelle;open access;bold;archive ouverte unige;fmrs;cybertheses;cibm ait;institutional repository	To date, only a couple of functional MR spectroscopy (fMRS) studies were conducted in rats. Due to the low temporal resolution of (1)H MRS techniques, prolonged stimulation paradigms are necessary for investigating the metabolic outcome in the rat brain during functional challenge. However, sustained activation of cortical areas is usually difficult to obtain due to neural adaptation. Anesthesia, habituation, high variability of the basal state metabolite concentrations as well as low concentrations of the metabolites of interest such as lactate (Lac), glucose (Glc) or γ-aminobutyric acid (GABA) and small expected changes of metabolite concentrations need to be addressed. In the present study, the rat barrel cortex was reliably and reproducibly activated through sustained trigeminal nerve (TGN) stimulation. In addition, TGN stimulation induced significant positive changes in lactate (+1.01 μmol/g, p<0.008) and glutamate (+0.92 μmol/g, p<0.02) and significant negative aspartate changes (-0.63 μmol/g, p<0.004) using functional (1)H MRS at 9.4 T in agreement with previous changes observed in human fMRS studies. Finally, for the first time, the dynamics of lactate, glucose, aspartate and glutamate concentrations during sustained somatosensory activation in rats using fMRS were assessed. These results allow demonstrating the feasibility of fMRS measurements during prolonged barrel cortex activation in rats.	arm cortex-m;acclimatization;aspartic acid;basal (phylogenetics);chromatography, gas-liquid;getty thesaurus of geographic names;glucose;glutamic acid;heart rate variability;lactic acid;metabolic process, cellular;minimal recursion semantics;sacral nerve stimulation;thioctic acid;trigeminal nerve structure;video-in video-out;fmri;gamma-aminobutyric acid	Nathalie Just;Lijing Xin;Hanne Frenkel;Rolf Gruetter	2013	NeuroImage	10.1016/j.neuroimage.2013.02.042	psychology;neuroscience;anesthesia;communication	ML	20.37095820734292	-78.95921835511034	7929
c4bea4cd5f8128cca76047bcb26d105864e4a79a	does a cbir system really impact decisions of physicians in a clinical environment?	medical image processing decision making decision support systems health care image classification image retrieval;image classification;realistic condition physician decisions clinical environment content based image retrieval systems medical field healthcare institution decision making cbir techniques medical image radiologist clinical routine user decision image classification certainty degree medical cbir system clinical practice domain;medical diagnostic imaging educational institutions accuracy feature extraction medical services image retrieval;medical image processing;decision support systems;health care;image retrieval	Content-based image retrieval systems are employed in several areas. One of the most prominent area is the medical field, due to the huge volume of digital images daily generated in healthcare institutions employed for decision making. There are several works applying CBIR techniques over medical images. However, the great majority of them do not verify whether the systems are actually considered by the specialists as a pontential aid in a real environment. In order to fill this research void in the literature, this work explores user experiments in a CBIR system involving resident physicians and radiologists. To do so, we developed a CBIR system according to requirements provided by the specialists and employed a methodology to analyze the effectiveness of the system for supporting them in clinical routine. The methodology aims at evaluating the system's impact in the user's decision, inquiring the specialists about the image classification and their degree of certainty in different situations using the system. By analyzing the obtained results we can argue that the proposed methodology joined with our medical CBIR system presented a high acceptance and viability rate regarding the radiologists interests in the clinical practice domain, providing a novel approach to analyze CBIR systems under realistic conditions.	computer vision;content-based image retrieval;digital image;experiment;radiology;requirement	Marcelo Ponciano-Silva;Juliana P. Souza;Pedro Henrique Bugatti;Marcos Vinicius Naves Bedo;Daniel dos Santos Kaster;Rosana T. V. Braga;Angela D. Bellucci;Paulo Mazzoncini de Azevedo Marques;Caetano Traina;Agma J. M. Traina	2013	Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2013.6627762	computer vision;contextual image classification;medicine;image retrieval;computer science;data mining;multimedia;health care	Embedded	33.89932688910942	-77.78672752143206	7946
14961c7d9c3eb41ed11bddef1408cc9b7081cee6	integrative mixture of experts to combine clinical factors and gene markers	apoptosis;networks;mixture of experts;male;bayes theorem;gen;models biological;expression;prostate carcinoma;progression;classification;marqueur;tumor markers biological;cells;models genetic;marcador;prostatic neoplasms;medical oncology;reproducibility of results;gene;models statistical;algorithms;chemotherapy;humans;marker;extension;computational biology;breast cancer;genetic markers;gene expression profiling;oligonucleotide array sequence analysis	MOTIVATION Microarrays are being increasingly used in cancer research to better characterize and classify tumors by selecting marker genes. However, as very few of these genes have been validated as predictive biomarkers so far, it is mostly conventional clinical and pathological factors that are being used as prognostic indicators of clinical course. Combining clinical data with gene expression data may add valuable information, but it is a challenging task due to their categorical versus continuous characteristics. We have further developed the mixture of experts (ME) methodology, a promising approach to tackle complex non-linear problems. Several variants are proposed in integrative ME as well as the inclusion of various gene selection methods to select a hybrid signature.   RESULTS We show on three cancer studies that prediction accuracy can be improved when combining both types of variables. Furthermore, the selected genes were found to be of high relevance and can be considered as potential biomarkers for the prognostic selection of cancer therapy.   AVAILABILITY Integrative ME is implemented in the R package integrativeME (http://cran.r-project.org/).	biological markers;cytology;dna microarray;forecast of outcome;gene expression;large;linear predictive coding;neoplasms;non-small cell lung carcinoma;nonlinear system;patients;relevance;sensitivity and specificity	Kim-Anh Lê Cao;Emmanuelle Meugnier;Geoffrey J. McLachlan	2010		10.1093/bioinformatics/btq107	biology;biological classification;bioinformatics;apoptosis;expression;breast cancer;gene;genetic marker;gene expression profiling;bayes' theorem;genetics	Comp.	7.195111243910984	-53.59301376259222	7949
b9e445199a135b4e29224b5c9991f8c96ee63771	manipulating brain connectivity with δ<ce:sup loc=post>9</ce:sup>-tetrahydrocannabinol: a pharmacological resting state fmri study	brain connectivity;cns effects;resting state fmri;pharmacokinetics;thc;pharmacodynamics	Resting state-functional magnetic resonance imaging (RS-FMRI) is a neuroimaging technique that allows repeated assessments of functional connectivity in resting state. While task-related FMRI is limited to indirectly measured drug effects in areas affected by the task, resting state can show direct CNS effects across all brain networks. Hence, RS-FMRI could be an objective measure for compounds affecting the CNS. Several studies on the effects of cannabinoid receptor type 1 (CB(1))-receptor agonist δ(9)-tetrahydrocannabinol (THC) on task-dependent FMRI have been performed. However, no studies on the effects of cannabinoids on resting state networks using RS-FMRI have been published. Therefore, we investigated the effects of THC on functional brain connectivity using RS-FMRI. Twelve healthy volunteers (9 male, 3 female) inhaled 2, 6 and 6 mg THC or placebo with 90-minute intervals in a randomized, double blind, cross-over trial. Eight RS-FMRI scans of 8 min were obtained per occasion. Subjects rated subjective psychedelic effects on a visual analog scale after each scan, as pharmacodynamic effect measures. Drug-induced effects on functional connectivity were examined using dual regression with FSL software (FMRIB Analysis Group, Oxford). Eight maps of voxel-wise connectivity throughout the entire brain were provided per RS-FMRI series with eight predefined resting-state networks of interest. These maps were used in a mixed effects model group analysis to determine brain regions with a statistically significant drug-by-time interaction. Statistical images were cluster-corrected, and results were Bonferroni-corrected across multiple contrasts. THC administration increased functional connectivity in the sensorimotor network, and was associated with dissociable lateralized connectivity changes in the right and left dorsal visual stream networks. The brain regions showing connectivity changes included the cerebellum and dorsal frontal cortical regions. Clear increases were found for feeling high, external perception, heart rate and cortisol, whereas prolactin decreased. This study shows that THC induces both increases and (to a lesser extent) decreases in functional brain connectivity, mainly in brain regions with high densities of CB(1)-receptors. Some of the involved regions could be functionally related to robust THC-induced CNS-effects that have been found in previous studies (Zuurman et al., 2008), such as postural stability, feeling high and altered time perception.	analog;basal ganglia diseases;cns;cerebellum;double-blind method;dual;euphoric mood;evaluation procedure;fmrib software library (fsl);hallucinogens;hydrocortisone;inspiration function;magnetic resonance imaging;map;maxima and minima;mike lesser;mike zuurman;neuroimaging;pharmacodynamics;pharmacology;randomized algorithm;rating (action);reed–solomon error correction;rest;resting state fmri;scientific publication;voxel;cannabinoid receptor;density;nabiximols	Linda E. Klumpers;David M. Cole;Najmeh Khalili-Mahani;Roelof P. Soeter;Erik T. te Beek;Serge A. R. B. Rombouts;Joop M. A. van Gerven	2012	NeuroImage	10.1016/j.neuroimage.2012.07.051	psychology;neuroscience;developmental psychology;pharmacodynamics;pharmacokinetics;communication;resting state fmri	ML	19.68765988718463	-80.12897907225708	7959
a8a634a89a07daf6d017f0f13aa2d8b932b9aa82	microdomain [ca2] fluctuations alter temporal dynamics in models of ca2-dependent signaling cascades and synaptic vesicle release		Ca2-dependent signaling is often localized in spatially restricted microdomains and may involve only 1 to 100 Ca2 ions. Fluctuations in the microdomain Ca2 concentration (Ca2) can arise from a wide range of elementary processes, including diffusion, Ca2 influx, and association/dissociation with Ca2 binding proteins or buffers. However, it is unclear to what extent these fluctuations alter Ca2-dependent signaling. We construct Markov models of a general Ca2-dependent signaling cascade and Ca2-triggered synaptic vesicle release. We compare the hitting (release) time distribution and statistics for models that account for [Ca2] fluctuations with the corresponding models that neglect these fluctuations. In general, when Ca2 fluctuations are much faster than the characteristic time for the signaling event, the hitting time distributions and statistics for the models with and without Ca2 fluctuation are similar. However, when the timescale of Ca2 fluctuations is on the same order as the signaling cascade or slower, the hitting time mean and variability are typically increased, in particular when the average number of microdomain Ca2 ions is small, a consequence of a long-tailed hitting time distribution. In a model of Ca2-triggered synaptic vesicle release, we demonstrate the conditions for which [Ca2] fluctuations do and do not alter the distribution, mean, and variability of release timing. We find that both the release time mean and variability can be increased, demonstrating that Ca2 fluctuations are an important aspect of microdomain Ca2 signaling and further suggesting that Ca2 fluctuations in the presynaptic terminal may contribute to variability in synaptic vesicle release and thus variability in neuronal spiking.	cascade device component;cell signaling;heart rate variability;ions;iontophoresis;markov chain;markov model;presynaptic terminals;quantum fluctuation;spatial variability;synaptic package manager;synaptic vesicles;tail;vesicle (morphologic abnormality);negative regulation of synaptic vesicle uncoating	Seth H. Weinberg	2016	Neural Computation	10.1162/NECO_a_00811	simulation;communication	ML	17.2390668630962	-71.93196567948836	7970
1bf070d27dbbbe133c1498cf480c595515425784	a fully-implantable wireless system for human brain-machine interfaces using brain surface electrodes: w-herbs	brain surface electrodes;wireless;motor restoration;brain machine interface;wireless systems;human brain;implantable device	The brain-machine interface (BMI) is a new method for man-machine interface, which enables us to control machines and to communicate with others, without input devices but directly using brain signals. Previously, we successfully developed a real time control system for operating a robot arm using brain-machine interfaces based on the brain surface electrodes, with the purpose of restoring motor and communication functions in severely disabled people such as amyotrophic lateral sclerosis patients. A fully-implantable wireless system is indispensable for the clinical application of invasive BMI in order to reduce the risk of infection. This system includes many new technologies such as two 64-channel integrated analog amplifier chips, a Bluetooth wireless data transfer circuit, a wirelessly rechargeable battery, 3 dimensional tissue-fitting high density electrodes, a titanium head casing, and a fluorine polymer body casing. This paper describes key features of the first prototype of the BMI system for clinical application. key words: brain-machine interface, implantable device, wireless, brain surface electrodes, motor restoration	amplifier;bluetooth;brain–computer interface;circuit restoration;control system;input device;lateral thinking;polymer;prototype;rechargeable battery;robotic arm;user interface	Masayuki Hirata;Kojiro Matsushita;Takafumi Suzuki;Takeshi Yoshida;Fumihiro Sato;Shayne Morris;Takufumi Yanagisawa;Tetsu Goto;Mitsuo Kawato;Toshiki Yoshimine	2011	IEICE Transactions	10.1587/transcom.E94.B.2448	brain–computer interface;telecommunications;computer science;wireless	Mobile	9.32901884158718	-90.14470332336089	7973
973bf073893ef41777a2a4f1dc095712a0631e12	hierarchical convolutional neural network for face detection		In this paper, we propose a new approach of hierarchical convolutional neural network (CNN) for face detection. The first layer of our architecture is a binary classifier built on a deep convolutional neural network with spatial pyramid pooling (SPP). Spatial pyramid pooling reduces the computational complexity and remove the fixed-size constraint of the network. We only need to compute the feature maps from the entire image once and generate a fixed-length representation regardless of the image size and scale. To improve the localization effectiveness, in the second layer, we design a bounding box regression network to refine the relative high scored non-face output from the first layer. The proposed approach is evaluated on the AFW dataset, FDDB dataset and Pascal Faces, and it reaches the state-of-the-art performance. Also, we apply our bounding box regression network to refine the other detectors and find that it has effective generalization.	convolutional neural network;face detection	Jing Yang;Jiankang Deng;Qingshan Liu	2015		10.1007/978-3-319-21963-9_34	time delay neural network;deep learning;convolutional neural network;neocognitron	Vision	26.157155446684662	-52.92117793666204	7988
22a40f977a6b4d3d1bffb75e627d06eeb1cb73f6	query adaptive fusion for graph-based visual reranking		Developing effective fusion schemes for multiple feature types has always been a hot issue in content-based image retrieval. In this paper, we propose a novel method for graph-based visual reranking, which addresses two major limitations in existing methods. First, in the phase of graph construction, our method introduces fine-grained measurements for image relations, by assigning the edge weights using normalized similarity. Furthermore, in the phase of graph fusion, rather than summing up all the graphs for different single features indiscriminately, we propose to estimate the reliability of each feature through a statistical model, and selectively fuse the single graphs via query-adaptive fusion weights. Fusion methods with either labeled data and unlabeled data are proposed and the performance are evaluated and compared by experiments. Our method is evaluated on five public datasets, by fusing scale-invariant feature transform (SIFT), CNN, and hue, saturation, hue (HSV), three complementary features. Experimental results demonstrate the effectiveness of the proposed method, which yields superior results than the competing methods.	content-based image retrieval;experiment;scale-invariant feature transform;statistical model	Muyuan Fang;Yu-Jin Zhang	2017	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2017.2726977	labeled data;normalization (statistics);computer vision;fusion;computer science;hsl and hsv;scale-invariant feature transform;machine learning;statistical model;image retrieval;artificial intelligence;pattern recognition;graph	Vision	35.380653349733194	-55.7693500842473	7998
d30500ab9abf1faa624334549c9d1c988cd72aaa	structural adaptation and heterogeneity of normal and tumor microvascular networks	intravital microscopy;microvessels;mice nude;animals;mice;red blood cell;treatment efficacy;branch point;computer model;oxygen;splanchnic circulation;neoplasm transplantation;blood vessel;neovascularization pathologic;hemodynamics;structure and function;adaptation physiological;random variable;adaptive response;neoplasms;computer simulation;models cardiovascular	Relative to normal tissues, tumor microcirculation exhibits high structural and functional heterogeneity leading to hypoxic regions and impairing treatment efficacy. Here, computational simulations of blood vessel structural adaptation are used to explore the hypothesis that abnormal adaptive responses to local hemodynamic and metabolic stimuli contribute to aberrant morphological and hemodynamic characteristics of tumor microcirculation. Topology, vascular diameter, length, and red blood cell velocity of normal mesenteric and tumor vascular networks were recorded by intravital microscopy. Computational models were used to estimate hemodynamics and oxygen distribution and to simulate vascular diameter adaptation in response to hemodynamic, metabolic and conducted stimuli. The assumed sensitivity to hemodynamic and conducted signals, the vascular growth tendency, and the random variability of vascular responses were altered to simulate 'normal' and 'tumor' adaptation modes. The heterogeneous properties of vascular networks were characterized by diameter mismatch at vascular branch points (d(3) (var)) and deficit of oxygen delivery relative to demand (O(2def)). In the tumor, d(3) (var) and O(2def) were higher (0.404 and 0.182) than in normal networks (0.278 and 0.099). Simulated remodeling of the tumor network with 'normal' parameters gave low values (0.288 and 0.099). Conversely, normal networks attained tumor-like characteristics (0.41 and 0.179) upon adaptation with 'tumor' parameters, including low conducted sensitivity, increased growth tendency, and elevated random biological variability. It is concluded that the deviant properties of tumor microcirculation may result largely from defective structural adaptation, including strongly reduced responses to conducted stimuli.	acclimatization;anatomy, regional;biomarkers, tumor;blood cells;blood vessel;body tissue;computation;computer simulation;diameter (qualifier value);erythrocytes;exhibits as topic;genetic heterogeneity;heart rate variability;hemodynamics;hypoxia;interferometric microscopy;mesentery;metabolic process, cellular;microcirculation;neoplasms;oxygen;velocity (software development)	Axel Radlach Pries;Annemiek J. M. Cornelissen;Anoek A. Sloot;Marlene Hinkeldey;Matthew R. Dreher;Michael Höpfner;Mark W. Dewhirst;Timothy W. Secomb	2009	PLoS Computational Biology	10.1371/journal.pcbi.1000394	computer simulation;random variable;biology;branch point;adaptive response;hemodynamics;oxygen;intravital microscopy;genetics;anatomy;statistics	ML	20.681968962446874	-81.12479111490916	8013
8edbaa833ef4c8f929054f57e17ef3bbc57171bb	effects of the antiarrhythmic drug dofetilide on transmural dispersion of repolarization in ventriculum. a computer modeling study	drugs;brain;bioelectric potentials;cardiology;neurophysiology;cellular biophysics;muscle	Dofetilide is a class-Ill drug that inhibits the rapid component of the delayed potassium current (IKr). Experimental studies have shown that the different layers of ventricular muscle present differences in action potential duration (APD) and different responses to class III agents. It has been suggested that it contributes to APD heterogeneity in the ventricles. However, in vivo studies suggest that the strong cellular coupling reduces APD dispersion in intact heart. The aim of this paper is to study the effect of dofetilide on the action potentials (APs) in isolated ventricular cells and on APD dispersion in a strand of ventricular tissue. A mathematical model of dofetilide effects on IKr has been developed and incorporated into the Luo-Rudy dynamic model of ventricular AP. Our results show that dofetilide induces in midmyocardium cells a faster time-course inhibition of IKr than in endocardial or epicardial cells, and periods of instability with beat-to-beat APs variability. This behavior could favor temporal dispersion of repolarization between the different cells. The results also indicate that although dofetilide increases, the transmural gradient of APD in the ventricular wall, early after depolarizations (EADs) did not appear even under strong uncoupling conditions. However, reduced repolarization reserve favors the induction of EADs, even under normal coupling conditions.	ap computer science;action potentials;action potential;adverse reaction to drug;anti-arrhythmia agents;auditory processing disorder;computer simulation;coupling (computer programming);endocardium;gradient;heart ventricle;instability;kinetics internet protocol;mathematical model;mathematics;muscle;neural binding;potassium;spatial variability;strand (programming language);ventricular remodeling;video-in video-out;wall of ventricle;anatomical layer;dofetilide;pamidronate	Javier Saiz;Julio Gomis-Tena;Marta Monserrat;Jose Maria Ferrero;Karen Cardona;Javier Chorro	2011	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2010.2077292	pharmacology;muscle;anesthesia;neurophysiology;cardiology	Visualization	17.699124493389533	-71.97619667230356	8018
6b8ff3238e5f26da71a864c57a216c27fe34ebe9	segmentation of medical images by region growing	cancer detection;software;image segmentation;biomedical imaging;visualization;medical image;lesions;cervical cancer;region of interest;pixel;unified modeling language;diseases;image analysis;image segmentation lesions pixel biomedical imaging software unified modeling language medical diagnostic imaging;region growing;early detection;digital images;medical diagnosis;medical diagnostic imaging	The possibility of combining different technologies and developing better performing systems that offer quality results leads to the creation of systems with enhanced adaptation and analysis. Such analysis allows the interaction with previously evaluated techniques, providing reliable, successful results. Such is the case for the research work detailed in this article, which is focused on a technique that allows reusing previously analyzed and formalized information with the support of the Unified Modeled Language (UML). Information is handled in modules aimed to generate the segmentation of medical images without intervention of a specialist. The purpose is to deliver regions of interest for the early detection of cervical cancer.	medical imaging;region growing;region of interest;unified modeling language	Itzel Abundez Barrera;Citlalih Gutierrez Estrada;Sergio Diaz Zagal;Martin de Jesus Nieto Perez	2008	2008 IEEE International Conference on Information Reuse and Integration	10.1109/IRI.2008.4583071	unified modeling language;computer vision;image analysis;visualization;computer science;medical diagnosis;multimedia;region growing;image segmentation;digital image;pixel;region of interest	Robotics	38.99753440365379	-74.35826333530467	8038
b0ab9520c351f44606a43358295a02cd6d51a62c	whole body imaging with dynamic volume 320-row ct	whole body;detectors;protocols;whole body imaging;history;computed tomography;phantoms;availability;variable helical pitch 320 row ct whole body imaging;biological organs;reconstruction algorithms;biomedical imaging;imaging phantoms;logic gates;whole body imaging computed tomography reconstruction algorithms imaging phantoms biomedical imaging detectors image reconstruction europe availability history;320 row ct;medical image processing biological organs computerised tomography image reconstruction;image reconstruction;medical image processing;computerised tomography;scan time;dynamic volume 320 row ct;helical scan mode whole body imaging dynamic volume 320 row ct computerised tomography z coverage biological organs nonhelical rotation scan time;nonhelical rotation;variable helical pitch;europe;helical scan mode;z coverage	State of the art of CT technology will be presented. It has a z-coverage of 16cm to cover most of the organs in one single, non-helical, rotation. Whole body imaging using different scan methods will be presented, and a comparison in terms of scan time with helical scan mode will be discussed.	whole body imaging	R. Irwan	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490176	iterative reconstruction;medical imaging;communications protocol;availability;detector;radiology;medicine;logic gate;computer science;computed tomography;nuclear medicine;medical physics	Arch	43.907890670742404	-86.26148302994301	8043
e9e4838ac0f0bc1117ca03c73421b571c62fd92c	a learning-based approach for fast and robust vessel tracking in long ultrasound sequences		We propose a learning-based method for robust tracking in long ultrasound sequences for image guidance applications. The framework is based on a scale-adaptive block-matching and temporal realignment driven by the image appearance learned from an initial training phase. The latter is introduced to avoid error accumulation over long sequences. The vessel tracking performance is assessed on long 2D ultrasound sequences of the liver of 9 volunteers under free breathing. We achieve a mean tracking accuracy of 0.96 mm. Without learning, the error increases significantly (2.19 mm, p<0.001).	blood vessel tissue;tree accumulation	Valeria De Luca;Michael Tschannen;Gábor Székely;Christine Tanner	2013	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-40811-3_65	computer vision	Vision	42.41356857744153	-83.0557196788477	8052
f3c722365a5572706df2929de3485ced9508d694	new biometric approach based on geometrical humain brain patterns recognition: some preliminary results	databases;image recognition;magnetic resonance imaging databases accuracy feature extraction biometrics shape humans;brain;brain geometrical descriptor vector;open access series of imaging studies;gdb vector;biometrics access control;similarity biometric brain descriptors ellipse geometric mri;visual databases biomedical mri biometrics access control brain feature extraction image recognition;ellipse;biometrics;mri volumetric image;biometric;brain shape;interindividual variability;magnetic resonance image;geometrical human brain pattern recognition brain shape biometric feature extraction magnetic resonance imaging image mri volumetric image interindividual variability brain geometrical descriptor vector gdb vector similarity measurement open access series of imaging studies oasis database;similarity measurement;oasis database;accuracy;shape;geometric;feature extraction;magnetic resonance imaging;open access;similarity;descriptors;mri;pattern recognition;biometric feature extraction;humans;geometrical human brain pattern recognition;similarity measure;biomedical mri;magnetic resonance imaging image;visual databases	In this paper, we describe a new biometric approach based on geometrical characteristics of brain shape. Specifically, we use these geometrics characteristics as a biometric feature to identify individuals. For this purpose, Magnetic Resonance Imaging (MRI) images are considered. We show that using a single slice from an MRI volumetric image, acquired at a given level, one can extract many significant geometrical descriptors related to inter-individual variability of brain shape that can be also used to identify individuals. Explicitly, the proposed biometric approach combines two main phases. In the first phase, features extraction (FE) are achieved in order to obtain brain geometrical descriptors vector, called in this work GDB vector. A second phase is called similarity measurement (SM). Finally, the proposed algorithm is evaluated on the Open Access Series of Imaging Studies (OASIS) database containing brain MRI Images. Results using 220 classes show that high accuracy of 98.76% to identify individuals are obtained.	algorithm;biometrics;gnu debugger;resonance;spatial variability;volumetric display	Kamel Aloui;Amine Naït-Ali;Mohamed Saber Naceur	2011	3rd European Workshop on Visual Information Processing	10.1109/EuVIP.2011.6045521	computer vision;geography;pattern recognition;data mining	Vision	35.986283489754925	-71.85980069135836	8071
835eafcf1ba7daf55dbdcd393310a7c4fb986f68	elucidating the adaptation and temporal coordination of metabolic pathways using in-silico evolution	flux balance analysis;systems biology;evolutionary algorithms;metabolic oscillations;metabolism	Cellular metabolism, the interconversion of small molecules by chemical reactions, is a tightly coordinated process that requires integration of diverse environmental and intracellular cues. While for many organisms the topology of the network of metabolic reactions is increasingly known, the regulatory principles that shape the network's adaptation to diverse and changing environments remain largely elusive. To investigate the principles of metabolic adaptation and regulation in metabolic pathways, we propose a computational approach based on in-silico evolution. Rather than analyzing existing regulatory schemes, we let a population of minimal, prototypical metabolic cells evolve rate constants and appropriate regulatory schemes that allow for optimal growth in static and fluctuating environments. Applying our approach to a small, but already sufficiently complex, minimal system reveals intricate transitions between metabolic modes. These results have implications for trade-offs in resource allocation. Going from static to varying environments, we show that for fluctuating nutrient availability, active metabolic regulation results in a significantly increased overall rate of metabolism.		Willi Gottstein;Stefan Müller;Hanspeter Herzel;Ralf Steuer	2014	Bio Systems	10.1016/j.biosystems.2013.12.006	biology;computer science;bioinformatics;flux balance analysis;ecology;metabolism;systems biology	Comp.	6.8482502816667505	-63.8969951385608	8081
6139e460edd434543f532478f7b3eea60731a060	identification of cataract and post-cataract surgery optical images using artificial intelligence techniques	cluster algorithm;old age;long period;fuzzy k means;training;cataract surgery;neural network classifier;artificial intelligent;accuracy;sensitivity;histogram;classifier;optical imaging;normal;backpropagation algorithm;health problems;specificity;anova test;neuron;cataract;optic nerve;neural network	Human eyes are most sophisticated organ, with perfect and interrelated subsystems such as retina, pupil, iris, cornea, lens and optic nerve. The eye disorder such as cataract is a major health problem in the old age. Cataract is formed by clouding of lens, which is painless and developed slowly over a long period. Cataract will slowly diminish the vision leading to the blindness. At an average age of 65, it is most common and one third of the people of this age in world have cataract in one or both the eyes. A system for detection of the cataract and to test for the efficacy of the post-cataract surgery using optical images is proposed using artificial intelligence techniques. Images processing and Fuzzy K-means clustering algorithm is applied on the raw optical images to detect the features specific to three classes to be classified. Then the backpropagation algorithm (BPA) was used for the classification. In this work, we have used 140 optical image belonging to the three classes. The ANN classifier showed an average rate of 93.3% in detecting normal, cataract and post cataract optical images. The system proposed exhibited 98% sensitivity and 100% specificity, which indicates that the results are clinically significant. This system can also be used to test the efficacy of the cataract operation by testing the post-cataract surgery optical images.	area striata structure;artificial intelligence;backpropagation;cataract;class;classification;cluster analysis;disorder of eye;early diagnosis;eye neoplasms;image;iris (eye);k-means clustering;ophthalmology specialty;optic nerve (gchq);oracle bpa suite;pupil;retina;rigor - temperature-associated observation;sensitivity and specificity;sensor;statistical classification;test set;algorithm;statistical cluster	U. Rajendra Acharya;Wenwei Yu;Kuanyi Zhu;Jagadish Nayak;Teik-Cheng Lim;Joey Yiptong Chan	2009	Journal of Medical Systems	10.1007/s10916-009-9275-8	computer vision;normal;classifier;sensitivity;computer science;backpropagation;machine learning;optical imaging;histogram;accuracy and precision;artificial neural network;surgery;statistics	AI	33.44977662833875	-76.197884879684	8098
c10de532d3b36a87e0454d231952a64455d52b6f	multi-level integrative analysis of protein - protein interaction networks: connecting completeness, depth and robustness	protein protein interaction network;ppi networks;depth;protein protein interactions;network analysis;stability;sampling;fault tolerance	A fully extended Protein Protein Interaction (PPI) network can consist of upwards of several thousand nodes and edges. To simplify analysis, smaller child samples are often used in substitution of the global network. In this study, the impact of different levels of sampling was evaluated on six PPI networks. Results from the case studies suggest that restricting analysis to the first network level, using metrics such as degree and BC, could lead to misrepresentative results, omitting potentially significant nodes. Fault-tolerance analysis also indicates that key nodes within the second network level, and above, contribute to the stability of the global network.	global network;pixel density;proton pump inhibitors;sampling (signal processing);protein protein interaction	Jaine K. Blayney;Huiru Zheng;Haiying Wang;Francisco Azuaje	2010	International journal of computational biology and drug design	10.1504/IJCBDD.2010.034465	protein–protein interaction;biology;sampling;fault tolerance;stability;network analysis;computer science;bioinformatics;machine learning;data mining;statistics	Comp.	5.511529043090871	-57.84882810099318	8121
1ecb7100e2c6295b4305920b74b9538f08d5374c	specificity of the effect of a nicotinic receptor polymorphism on individual differences in visuospatial attention	genetique;especificidad;sensitivity and specificity;female;alleles;recepteur nicotinique;percepcion espacio;genetica;genotype;perception espace;interindividual comparison;attention spatiale;male;gen;hombre;attention;atencion visual;genetics;polymorphism genetic;comparacion interindividual;comparaison interindividuelle;adult;visual search;polymorphism;nicotinic receptor;cognitive performance;cognition;spatial attention;human;neuropsychological tests;reproducibility of results;reverse transcriptase polymerase chain reaction;gene;analysis of variance;individuality;cognicion;receptor nicotinico;atencion espacial;polymorphisme;receptors nicotinic;humans;polimorfismo;attention visuelle;specificity;specificite;individual difference;dopamine;visual attention;vision;domain specificity;space perception;reaction time;homme;pattern recognition visual	Cortical neurotransmitter availability is known to exert domain-specific effects on cognitive performance. Hence, normal variation in genes with a role in neurotransmission may also have specific effects on cognition. We tested this hypothesis by examining associations between polymorphisms in genes affecting cholinergic and noradrenergic neurotransmission and individual differences in visuospatial attention. Healthy individuals were administered a cued visual search task which varied the size of precues to the location of a target letter embedded in a 15-letter array. Cues encompassed 1, 3, 9, or 15 letters. Search speed increased linearly with precue size, indicative of a spatial attentional scaling mechanism. The strength of attentional scaling increased progressively with the number of C alleles (0, 1, or 2) of the alpha-4 nicotinic receptor gene C1545T polymorphism (n = 104). No association was found for the dopamine beta hydroxylase gene G444A polymorphism (n = 135). These findings point to the specificity of genetic neuromodulation. Whereas variation in a gene linked to cholinergic transmission systematically modulated the ability to scale the focus of visuospatial attention, variation in a gene governing dopamine availability did not. The results show that normal variation in a gene controlling a nicotinic receptor makes a selective contribution to individual differences in visuospatial attention.	alleles;cognition;dopamine;embedded system;embedding;genes, vif;genetic polymorphism;image scaling;interleukin receptor common gamma subunit;mental association;mixed function oxygenases;modulation;neuromodulation (medicine);neurotransmitters;nicotinic receptors;sensitivity and specificity;spatial–temporal reasoning;synaptic transmission;test scaling;physical hard work	Pamela M. Greenwood;John A. Fossella;Raja Parasuraman	2005	Journal of Cognitive Neuroscience	10.1162/089892905774597281	psychology;vision;polymorphism;dopamine;cognition;developmental psychology;attention;analysis of variance;visual search;gene;genotype;communication;social psychology	ML	16.81733739522418	-76.43304742615317	8129
fc5767b9cfdbcbf6bd3f502a1a00132bb6029162	improved robust t-wave alternans detectors		New statistical and spectral detectors, the modified matched pairs t test, the extended spectral method and the modified spectral method, were proposed for T-wave alternans (TWA) detection gaining robustness according to trend and single-frequency interferences. They were compared to classic detectors such as matched pairs t test, unpaired t test, spectral method, generalized likelihood ratio test and estimated TWA amplitude within a simulation framework and applied to real data. The optimal detection threshold was selected by using a full Monte-Carlo simulation where signals, with and without alternans episodes, were corrupted by Gaussian noise with different power and single-frequency interferences with different tones. All the combinations of noise and frequency were selected and repeated 500 times in order to compute probability of detection ( $$P_{\mathrm{d}}$$ P d ) and the false alarm probability ( $$P_{\mathrm{fa}}$$ P fa ), providing ROC curves. The study group consisted of 50 patients with implantable cardioverter-defibrillator (age: $$55.3 \pm 16.4$$ 55.3 ± 16.4 ; LVEF: $$42.8 \pm 15.5$$ 42.8 ± 15.5 ), who were paced (ventricular pacing) at 100 bpm. Two-minute recordings were analyzed. The XYZ orthogonal lead system was used. The best performance was reached by using the modified matched pairs t test (in comparison with the spectral method and other reference methods).	algorithmic efficiency;cedax;computer-aided design;defibrillators;detectors;diethylstilbestrol;earthbound;implantable cardioverter-defibrillator;implants;interference (communication);left ventricular ejection fraction;microsoft windows;monte carlo method;normal statistical distribution;patients;preprocessor;roc curve;sensitivity and specificity;silo (dataset);simulation;single-frequency network;spectral method;statistical test;ventricular fibrillation;xyz file format;alternan;defibrillator/cardioverters;likelihood ratio;t test	Olivier Meste;Dariusz Janusek;S. Karczmarewicz;A. Przybylski;Michal Kania;A. Maciag;Roman Maniewski	2015	Medical & Biological Engineering & Computing	10.1007/s11517-015-1243-5	electronic engineering;speech recognition;mathematics;statistics	ML	22.834341956940833	-90.18044366749068	8136
66e542963ad52527b56bd899dde9d563403758b0	preclinical testing of a new venous valve		Venous valvular incompetency is a debilitating disease affecting millions of patients. Unfortunately, the current physiologic and surgical treatments are prone to the extreme risk of post-operative thrombosis. A new design for venous valves has been proposed using biomimicry. The medical device has the shape of a natural valve with sufficient elasticity to maintain patency and competency in the leg veins. The venous valve was tested for patency, competency, cyclic fatigue, compressibility, and thrombogenicity. Patency is maintained with a low opening pressure of less than 3 mmHg. Competency is maintained with backpressures exceeding 300 mmHg. The valve is fatigue resistant to over 1⁄4 million cycles. The valve can maintain its integrity when compressed in a stent and deployed without tilting or mal-alignment. Little thrombus forms on the valve with perfusion of whole blood under pulsatile flow conditions. The preclinical tests demonstrate efficacy as a new venous valve for treatment of chronic venous insufficiency.	biomimetics;convergence insufficiency;elasticity (data store);medical ultrasound	Laura-Lee Farrell;David N. Ku	2008			engineering;electronic engineering;scrap;mechanical engineering	HCI	13.132034664459225	-83.15936842861299	8147
562405c5dd26784410fe549dec4cc7f151b0728e	generalizations of angular radial transform for 2d and 3d shape retrieval	angular radial transform;binary image;shape descriptor;three dimensional;3d model;indexation;polar coordinate;shape retrieval;3d models;content based retrieval;images;color image	The Angular Radial transform (ART) is a moment-based image description method adopted in MPEG-7 as a 2D region-based shape descriptor. Efficiency and robustness were demonstrated on binary image. This paper proposes the generalization of the ART to describe two dimensional color images and three dimensional models. First, the ART recommended by the MPEG-7 standard is only limited to binary images and is not robust to perspective deformations. We propose two extensions which allow applying ART to color images and to insure robustness to all possible rotations and to perspective deformations. In other words, the descriptor is not adapted to natural color images according to the shape and the color attributes. We also generalize the ART to index 3D models. ART is a 2D complex transform in polar coordinate and can be extend to 3D data using spherical coordinates while keeping the robustness properties. The new 3D shape descriptor called 3D ART, have same properties that the original transform: robustness to rotation, translation, noise and scaling while keeping compact size and good retrieval cost. The size of the descriptor is an essential evaluation parameter on which depends the response time of a content based retrieval system. Results on large 3D databases are presented and discussed.	3d modeling;3d projection;angularjs;binary image;color;color image;computation;database;distortion;image scaling;mpeg-7;radial (radio);requirement;response time (technology);robustness (computer science)	Julien Ricard;David Coeurjolly;Atilla Baskurt	2005	Pattern Recognition Letters	10.1016/j.patrec.2005.03.030	three-dimensional space;computer vision;polar coordinate system;simulation;color image;binary image;computer science;machine learning;mathematics;geometry	Vision	40.80249953419506	-59.84885053845008	8150
7eea132df8f843045225430983d4656125cfcf6e	estimation of driver's steering direction about lane change maneuver at the preceding car avoidance by brain source current estimation method	estimation accuracy driver steering direction estimation lane change maneuver preceding car avoidance brain source current estimation method vehicle technology human machine interaction human electronics japan driver information steering operation pedal operation camera images physiological information driver operational intention brain activities biological information time frequency analysis fft electroencephalogram parallel factor analysis feature factor longitudinal behavior parafac analysis multichannel eeg analysis multidimensional data right frontpolar cortex precuneus posterior cingulate cortex driver mental state visual recognition visual judgment driver intention hierarchical bayesian method sparse logistic regression;vehicles estimation electroencephalography brain modeling wavelet transforms electrodes;cortical current estimation driving behavior;time frequency analysis automobiles bayes methods behavioural sciences driver information systems electroencephalography fast fourier transforms human computer interaction medical signal processing regression analysis road safety road traffic	Vehicle technology regarding the interaction between human and the machine has been called human-electronics in Japan. It is necessary to achieve a better relationship between human and vehicle. A driver's information, which can be obtained from steering operation, pedal operation, camera images and physiological information, particularly is crucial to find a method to determine a driver's operational intention. It is beneficial to find a method to determine a driver's operational intention. Therefore, we have focused on the brain activities in the biological information. The time frequency analysis such as FFT has been used in the traditional decomposition of the electroencephalogram (EEG). However, these conventional methods can only use two-dimensional data. In this paper, we described that the driver's EEG during car following was decomposed by parallel factor analysis (PARAFAC), and we investigated the feature factor of longitudinal behavior for recognize and judgment from that decomposition result. PARAFAC analysis has known as a multi-channel EEG analysis of multi-dimensional data. In previous research [1], we investigated the driver's EEG of during lane change maneuver using the parallel factor (PARAFAC) analysis. Consequently, all subjects have two common factors of the frequency component which exist in the 5-7 Hz and 8-13 Hz region. Those factors were located at the right frontpolar cortex and the precuneus posterior cingulate cortex, and this factor was changed by the driver's mental state during visual recognition and judgment. In this paper, we estimated the driver's intention from a driver's EEG using source current distribution estimation with Hierarchical Bayesian method and the sparse logistic regression. From the estimation results, the estimation accuracy of driver's intention was higher than about 70 % of three subject's in the lateral operation.	bayesian network;electroencephalography;factor analysis;fast fourier transform;frequency analysis;lateral thinking;logistic regression;mental state;sparse matrix	Toshihito Ikenishi;Takayoshi Kamada	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974354	computer vision;simulation;speech recognition	Robotics	15.304913516288082	-79.53443919261441	8160
ba50118c1607bb4c38aac55f15c679cc13de0587	performance comparison of screening tests for poag for t2dm using comp2roc package		Primary open angle glaucoma (POAG) is a leading cause of blindness (8). The commonly accepted risk factors for POAG include older age, high intraocular pressure, positive family history of glaucoma, and race (3). Individuals with type 2 diabetes mellitus (T2DM) are at greater risk of having open-angle glaucoma (POAG) than those without T2DM. Thus, screening programs for diabetic retinopathy may be an excellent oppor- tunity for screening for POAG and implementation of additional tests. In this work we intend to evaluate the performance of GDx measures for detecting POAG through the methodology of ROC curves. One retrospective cross-sectional study was carried out on individuals with T2DM with evaluation of diabetic retinopathy from 2008 to 2010 in Hospital Centre of Vila Nova de Gaia Northern Portugal. Individuals who had a positive screen were referred for scanning the nerve fiber layer of the retina with GDx TM device. In this study, the diagnosis of POAG was based on a computerized ophthalmoscopy and static analysis and based on this, the eyes were classified as healthy (nN = 85) and with definite glaucoma (nA = 37). The comparison of diagnostic indicators was performed by Comp2ROC package (2).		Ana Cristina Braga;Lígia Figueiredo;Dália Meira	2014		10.1007/978-3-319-09150-1_43	optometry;mathematical optimization;family history;computer science;high intraocular pressure;blindness;glaucoma	NLP	10.255895022460116	-77.37822693580284	8165
fb7ff16f840317c1f725f81f4f1cc8aafacb1516	a generic visual perception domain randomisation framework for gazebo		The impressive results of applying deep neural networks in tasks such as object recognition, language translation, and solving digital games are largely attributed to the availability of massive amounts of high quality labelled data. However, despite numerous promising steps in incorporating these techniques in robotic tasks, the cost of gathering data with a real robot has halted the proliferation of deep learning in robotics. In this work, a plugin for the Gazebo simulator is presented, which allows rapid generation of synthetic data. By introducing variations in simulator-specific or irrelevant aspects of a task, one can train a model which exhibits some degree of robustness against those aspects, and ultimately narrow the reality gap between simulated and real-world data. To show a use-case of the developed software, we build a new dataset for detection and localisation of three object classes: box, cylinder and sphere. Our results in the object detection and localisation task demonstrate that with small datasets generated only in simulation, one can achieve comparable performance to that achieved when training on real-world images.	artificial neural network;color vision;cylinder seal;deep learning;display resolution;international symposium on fundamentals of computation theory;item unique identification;object detection;open-source software;outline of object recognition;pascal;plug-in (computing);relevance;robot;robotics;simulation;synthetic data;tensorflow	João Borrego;Rui Figueiredo;Atabak Dehban;Plinio Moreno;Alexandre Bernardino;José Santos-Victor	2018	2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)	10.1109/ICARSC.2018.8374189	computer vision;robustness (computer science);object detection;software;deep learning;artificial neural network;synthetic data;visual perception;artificial intelligence;computer science;robotics	Robotics	23.270230334190668	-53.504755264040256	8169
5cf1fcaace5f7d8b7177dbe19dcfaaec987b2547	temporal sound processing by cochlear nucleus octopus neurons	background noise;bande passante;passband;nucleo coclear;logica temporal;detection signal;banda pasante;cochlear nucleus;low frequency;temporal logic;signal analysis;signal detection;human auditory system;analisis de senal;reverse correlation;frequency spectrum;band pass;deteccion senal;inner ear;analyse correlation;ruido fondo;audition;audicion;noyau cochleaire;bruit fond;amplitude modulated;logique temporelle;hearing;analyse signal;analisis correlacion;steady state;correlation analysis	Spike-triggered reverse-correlation analysis revealed that octopus neurons fire preferentially if coincident input spikes follow a short interval of relative low excitation. The frequency spectrum of the reversecorrelation revealed that octopus neurons perform a band pass analysis of the incoming signal, with the pass band ranging from about 110 to 650 Hz. The low-frequency slope was approximately 6 dB/octave, which indicates that octopus neurons process the first derivative of the input signal. This mechanism suppresses steady-state activity and accentuates onsets, but also enhances amplitude modulation in the frequency region of voiced speech sounds.	cochlear implant;modulation;neuron;spectral density;spike-triggered average;steady state	Werner Hemmert;Marcus Holmberg;Ulrich Ramacher	2005		10.1007/11550822_91	frequency spectrum;speech recognition;temporal logic;computer science;signal processing;band-pass filter;background noise;low frequency;steady state;passband;detection theory	ML	-3.463523880426675	-91.41475114068794	8177
7e37fb38fc30ff15fcbbcb0c3571011a8db65061	modeling prosopagnosia using dynamic artificial neural networks	brain;self organising feature maps brain content addressable storage encoding face recognition medical computing;fusiform face area;medical computing;face recognition;self organising feature maps;feature extraction;lesions face recognition mathematical model equations brain modeling artificial neural networks associative memory;process simulation;content addressable storage;sparse representation;encoding;bidirectional associative memory;face recognition prosopagnosia modeling dynamic artificial neural network brain disorder fusiform face area feature extracting bidirectional associative memory self organizing maps 2d topological map sparse representation localist encoding distributed encoding attractor like behavior face categorization semantic labels;artificial neural network	Prosopagnosia is a brain disorder causing the inability to recognize faces. Previous studies have shown that the lesions producing the disorder can occur in diverse areas of the brain. However, the most common region is the “fusiform face area” (FFA). In order to model the basic properties of prosopagnosia two networks have been used concurrently: the Feature Extracting Bidirectional Associative Memory (FEBAM-SOM) and the Bidirectional Associative Memory (BAM). The FEBAM-SOM creates a 2D topological map from correlated inputs through the categorization of various exemplars (faces and various objects). This model has the advantage of using a sparse representation which encompass both localist and distributed encoding. This process simulates the FFA in the brain by exhibiting attractor-like behavior for the categorization of all faces. Once the faces have been learned, the BAM model associates specific faces (and objects) to their corresponding semantic labels. Simulations were performed to study the recall performance in function of the size of the lesions. Results show that the recall performance of the names associated with faces decrease with the size of lesion without affecting the performance of the objects.	artificial neural network;bi-directional text;bidirectional associative memory;categorization;computer simulation;prosopagnosia;sparse approximation;sparse matrix	Robyn Vandermeulen;Laurence Morissette;Sylvain Chartier	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033482	computer vision;fusiform face area;process simulation;feature extraction;computer science;artificial intelligence;machine learning;sparse approximation;bidirectional associative memory;artificial neural network;encoding	Vision	22.89137300720462	-63.67599470962138	8180
cc72e9b6602cb95328e0b7efb6aeab1a52956413	indexing of fuzzy regions	color image	This paper first exposes an algorithm that leads to fuzzy segmentation of color images. This algorithm performs, as in the watershed method, a progressive flood of the gradient image from pixels of lowest gradients. Membership degrees of pixels to regions depend on topographic distance, which takes into acc ount both the distance to the core and the gradient norms. Geometric and colorimetric features are defined to build a region signature. A distance between fuzzy regions is then proposed, allowing ranking fuzzy regions by similarity. Applications concern region indexing and retrieval.	algorithm;flood fill;gradient;pixel;topography;watershed (image processing)	Sylvie Philipp-Foliguet;Marcelo Bernardes Vieira	2002			fuzzy logic;search engine indexing;artificial intelligence;color image;pattern recognition;mathematics	Vision	42.20404021731964	-67.22224181553284	8183
e274a423deb0b9f8f8d4c2cec853542cf4eea14c	towards solving the hard problem of consciousness: the varieties of brain resonances and the conscious experiences that they support	attention;adaptive resonance;emotion;consciousness;audition;vision	"""The hard problem of consciousness is the problem of explaining how we experience qualia or phenomenal experiences, such as seeing, hearing, and feeling, and knowing what they are. To solve this problem, a theory of consciousness needs to link brain to mind by modeling how emergent properties of several brain mechanisms interacting together embody detailed properties of individual conscious psychological experiences. This article summarizes evidence that Adaptive Resonance Theory, or ART, accomplishes this goal. ART is a cognitive and neural theory of how advanced brains autonomously learn to attend, recognize, and predict objects and events in a changing world. ART has predicted that """"all conscious states are resonant states"""" as part of its specification of mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony. It hereby provides functional and mechanistic explanations of data ranging from individual spikes and their synchronization to the dynamics of conscious perceptual, cognitive, and cognitive-emotional experiences. ART has reached sufficient maturity to begin classifying the brain resonances that support conscious experiences of seeing, hearing, feeling, and knowing. Psychological and neurobiological data in both normal individuals and clinical patients are clarified by this classification. This analysis also explains why not all resonances become conscious, and why not all brain dynamics are resonant. The global organization of the brain into computationally complementary cortical processing streams (complementary computing), and the organization of the cerebral cortex into characteristic layers of cells (laminar computing), figure prominently in these explanations of conscious and unconscious processes. Alternative models of consciousness are also discussed."""	adaptive resonance theory;brain;capability maturity model;cerebral cortex;clarify;classification;cognition disorders;computation (action);conscious;consciousness;emergence;emotions;experience;feelings;interaction;patients;physical object;specification;unconscious personality factor;vision;anatomical layer;explanation	Stephen Grossberg	2017	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2016.11.003	vision;awareness;attention;emotion;artificial intelligence;machine learning;consciousness	ML	15.335434401878219	-72.5989926361634	8202
757d760831ea41b866283a5970cd6907691aa5c7	indel-tolerant read mapping with trinucleotide frequencies using cache-oblivious kd-trees	genomics;nucleotides;high throughput nucleotide sequencing;chromosome mapping;sequence analysis dna;indel mutation;genetic variation;genome human;humans	MOTIVATION Mapping billions of reads from next generation sequencing experiments to reference genomes is a crucial task, which can require hundreds of hours of running time on a single CPU even for the fastest known implementations. Traditional approaches have difficulties dealing with matches of large edit distance, particularly in the presence of frequent or large insertions and deletions (indels). This is a serious obstacle both in determining the spectrum and abundance of genetic variations and in personal genomics.   RESULTS For the first time, we adopt the approximate string matching paradigm of geometric embedding to read mapping, thus rephrasing it to nearest neighbor queries in a q-gram frequency vector space. Using the L(1) distance between frequency vectors has the benefit of providing lower bounds for an edit distance with affine gap costs. Using a cache-oblivious kd-tree, we realize running times, which match the state-of-the-art. Additionally, running time and memory requirements are about constant for read lengths between 100 and 1000 bp. We provide a first proof-of-concept that geometric embedding is a promising paradigm for read mapping and that L(1) distance might serve to detect structural variations. TreQ, our initial implementation of that concept, performs more accurate than many popular read mappers over a wide range of structural variants.   AVAILABILITY AND IMPLEMENTATION TreQ will be released under the GNU Public License (GPL), and precomputed genome indices will be provided for download at http://treq.sf.net.   CONTACT pavelm@cs.rutgers.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	approximate string matching;bioinformatics;biopolymer sequencing;cpu (central processing unit of computer system);cache;cache-oblivious algorithm;central processing unit;clinical act of insertion;download;edit distance;embedding;ephrin type-b receptor 1, human;experiment;fastest;gnu;genome;indel mutation;massively-parallel sequencing;n-gram;one thousand;precomputation;programming paradigm;public-private sector partnerships;requirement;single linkage cluster analysis;string searching algorithm;time complexity;trees (plant);variation (genetics);gram	Md Pavel Mahmud;John Wiedenhoeft;Alexander Schliep	2012		10.1093/bioinformatics/bts380	biology;genomics;nucleotide;bioinformatics;theoretical computer science;genetic variation;genetics	Comp.	-1.1691173493059954	-53.64953055679453	8206
82ec3184a525f421a673fd01152eb8d76590246c	color and texture features for content based image retrieval system	performance measure;cbir;dwt;discrete wavelet transform;gabor transform;standard deviation;color histogram;texture features;bdip and bvlc;gwt;content based image retrieval;gabor wavelets	Content Based Image Retrieval System retrieves images using color, texture and shape properties of the image. Different methods which are implemented in this paper are Discrete wavelet transform (DWT), Gabor wavelet transform (GWT), Color histogram (CH), color autocorrelogram (CA). Integration of color and texture features is done using different methods and their comparison is done using precision and recall as performance measures. .DWT (D) method is implemented using the combination of statistical mean and standard deviation features and perceptual feature directionality. The best results are obtained with GWT (A) + CH method as compared to all other ten methods as phase information from the Gabor transformed coefficients is taken into consideration.	coefficient;color histogram;content-based image retrieval;discrete wavelet transform;gabor wavelet;precision and recall	A. J. Nirmal;V. B. Gaikwad	2011		10.1145/1980022.1980027	color histogram;image texture;computer vision;speech recognition;pattern recognition;mathematics;gabor wavelet	Vision	37.62095245202644	-60.54835981199414	8223
3da810b7e0bd75c8541b143b1e7e9ae103ea7018	af-dcgan: amplitude feature deep convolutional gan for fingerprint construction in indoor localization system		Wi-Fi positioning is currently the mainstream indoor positioning method, and the construction of fingerprint database is crucial to Wi-Fi based localization system. However, the accuracy requirement needs to sample enough data at many reference points, which consumes significant manpower and time. In this paper, we convert the CSI data collected at reference points into amplitude feature maps and then extend the fingerprint database using the proposed Amplitude Feature Deep Convolutional Generative Adversarial Network (AF-DCGAN) model. Using this model, the convergence process in the training phase can be accelerated, and the diversity of the CSI amplitude feature maps can be increased significantly. Based on the extended fingerprint database, the accuracy of indoor localization system can be improved with reduced human effort.	af-heap;anisotropic filtering;bibliographic database;experiment;feature model;fingerprint;indoor positioning system;map;network model;test set	Qiyue Li;Heng Qu;Zhi Liu;Nana Zhou;Wei Sun;Jie Li	2018	CoRR		distributed computing;computer science;fingerprint;amplitude;artificial intelligence;pattern recognition;convergence (routing)	Vision	19.505703738319898	-55.92903712282008	8239
2ca761938bd789b82d1a4ca85e7b8d5661093660	enhancing music information retrieval by incorporating image-based local features	co-occurrence matrix;colour moments;fourier transform;k-means algorithm;local features	This paper presents a novel approach to music genre classification. Having represented music tracks in the form of two dimensional images, we apply the “bag of visual words” method from visual IR in order to classify the songs into 19 genres. By switching to visual domain, we can abstract from musical concepts such as melody, timbre and rhythm. We obtained classification accuracy of 46% (with 5% theoretical baseline for random classification) which is comparable with existing state-of-the-art approaches. Moreover, the novel features characterize different properties of the signal than standard methods. Therefore, the combination of them should further improve the performance of existing techniques. The motivation behind this work was the hypothesis, that 2D images of music tracs (spectrograms) perceived as similar would correspond to the same music genres. Conversely, it is possible to treat real life images as spectrograms and utilize music-based features to represent these images in a vector form. This points to an interesting interchangeability between visual and music information retrieval.	bag-of-words model in computer vision;baseline (configuration management);information retrieval;real life;spectrogram	Leszek Kaliciak;Ben Horsburgh;Dawei Song;Nirmalie Wiratunga;Jeff Z. Pan	2012		10.1007/978-3-642-35341-3_19	speech recognition;computer science;multimedia	ML	37.04919571411812	-58.3937909323269	8248
501e92dadf1cc2b9b6a7d69526eb730c897c6f8c	automatic estimation of osteoporotic fracture cases by using ensemble learning approaches	ensemble learning classification;ibk;bone mineral density;random forest;osteoporosis;t score	Ensemble learning methods are one of the most powerful tools for the pattern classification problems. In this paper, the effects of ensemble learning methods and some physical bone densitometry parameters on osteoporotic fracture detection were investigated. Six feature set models were constructed including different physical parameters and they fed into the ensemble classifiers as input features. As ensemble learning techniques, bagging, gradient boosting and random subspace (RSM) were used. Instance based learning (IBk) and random forest (RF) classifiers applied to six feature set models. The patients were classified into three groups such as osteoporosis, osteopenia and control (healthy), using ensemble classifiers. Total classification accuracy and f-measure were also used to evaluate diagnostic performance of the proposed ensemble classification system. The classification accuracy has reached to 98.85 % by the combination of model 6 (five BMD + five T-score values) using RSM-RF classifier. The findings of this paper suggest that the patients will be able to be warned before a bone fracture occurred, by just examining some physical parameters that can easily be measured without invasive operations.	artificial neural network;bone tissue;bootstrap aggregating;cns disorder;ct scan;classification;computer-aided design;decision support system;densitometry;dual-energy x-ray absorptiometry;effective method;ensemble kalman filter;ensemble learning;f1 score;fracture;gradient boosting;hl7publishingsubsection <operations>;k-nearest neighbors algorithm;metabolic bone disorder;neural network simulation;osteoporosis;osteoporosis, postmenopausal;osteoporotic fractures;patients;quantitative computed tomography;radio frequency;random forest;random subspace method;resonance;response surface methodology;single linkage cluster analysis;support vector machine;x-ray computed tomography	Niyazi Zekiye Kiliç;Erkan Hosgormez	2015	Journal of Medical Systems	10.1007/s10916-015-0413-1	random subspace method;random forest;standard score;computer science;machine learning;pattern recognition;data mining;ensemble learning	ML	7.644863754174687	-77.6475462507623	8262
3e6bae4615a98d3734838a1764d03c9f127bdbda	the richer representation the better registration	minimisation;residual error minimization;levenberg marquadt algorithm;implicit polynomials;point wise correspondence term linear least squares fitting implicit polynomial implicit b spline point to point registration problem point to implicit problem levengberg marquardt algorithm point to model distance minimization;levenberg marquadt algorithm rigid registration surface fitting implicit polynomials implicit b splines registration error estimation residual error minimization;registration error estimation;surface fitting;splines mathematics;ip networks splines mathematics vectors polynomials three dimensional displays estimation approximation methods;image representation;image registration;splines mathematics image registration image representation minimisation;implicit b splines;rigid registration	In this paper, the registration problem is formulated as a point to model distance minimization. Unlike most of the existing works, which are based on minimizing a point-wise correspondence term, this formulation avoids the correspondence search that is time-consuming. In the first stage, the target set is described through an implicit function by employing a linear least squares fitting. This function can be either an implicit polynomial or an implicit B-spline from a coarse to fine representation. In the second stage, we show how the obtained implicit representation is used as an interface to convert point-to-point registration into point-to-implicit problem. Furthermore, we show that this registration distance is smooth and can be minimized through the Levengberg-Marquardt algorithm. All the formulations presented for both stages are compact and easy to implement. In addition, we show that our registration method can be handled using any implicit representation though some are coarse and others provide finer representations; hence, a tradeoff between speed and accuracy can be set by employing the right implicit function. Experimental results and comparisons in 2D and 3D show the robustness and the speed of convergence of the proposed approach.	algorithm;b-spline;convergence (action);fits;global optimization;gradient descent;ichthyosis bullosa of siemens;interface device component;least-squares analysis;linear least squares (mathematics);mathematical optimization;point-to-point protocol;polynomial;population parameter;rate of convergence;seizures;stage level 1;stage level 2;algorithm;registration - actclass	Mohammad Rouhani;Angel Domingo Sappa	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2281427	minimisation;mathematical optimization;discrete mathematics;computer science;image registration;mathematics;geometry;statistics	Vision	47.94324692934746	-77.01184085799068	8267
117885a2652d51a901d7b4c4a9da0cdb16a28e8a	adaptive narrowband level set model of underwater objects detection	level se t;image segmentation;level set;会议论文;detection;underwater objects;sonar detection;sonar image;adaptive narrowband;sonar image segmentation object detection radar imaging smoothing methods;level set sonar image underwater objects detection adaptive narrowband;mathematical model;energy function adaptive narrowband level set model underwater objects detection nonlinear characteristics original sonar image big data block mode k means clustering algorithm zero level set function detection speed local optimization;adaptation models;level set narrowband adaptation models sonar detection mathematical model image segmentation;narrowband	According to the nonlinear characteristics of original sonar image with big data, adaptive narrowband level set model to detect sonar image is proposed in this paper. In order to estimate the approximate position of the underwater objects after smoothing the original image, initial segmentation is processed with the block mode k-means clustering algorithm. On this basis, zero level set function is adaptively initialized using the approximate position of the underwater objects, which can not only reduce human intervention but also improve the detection speed. To further improve the robustness and detection accuracy, new adaptive narrowband level set model are provided to complete local optimization by minimizing each new energy function. Detection experiments demonstrate that the proposed method improves the detection speed and accuracy, and it has certain adaptability.		Xingmei Wang;Yanxia Wu;Zhipeng Liu	2015		10.1109/IIKI.2015.25	computer vision;speech recognition;computer science;level set;mathematical model;image segmentation	Vision	49.78599586057954	-67.8434730270887	8279
97d45707426b58be65fd3d1db8e5eb89114b01c3	prediction of proteins putatively involved in the thiol: disulfide redox metabolism of a bacterium (listeria): the cxxc motif as query sequence	cxxc motif;prediccion;estructura 3 dimensiones;listeria;proteine;phylogeny;redox;bacterie;phylogenese;bioinformatique;pregunta documental;structure 3 dimensions;metabolismo;filogenesis;microbiology;query;proteina;bacteria;bioinformatica;three dimensional structure;protein;thiol disulfide;3d structure;thiol;prediction;l monocytogenes egd e;metabolism;requete;metabolisme;tiol;bioinformatics	Thiol:disulfide redox metabolism (TDRM) is a central metabolic network in all living cells. However, numerous proteins with different biochemical functions and several structural domains are involved, making it not trivial to identify and annotate its constituents in sequenced genomes. We developed an uncomplicated approach to solve the problem using existing web-based tools and public databases with the gram-positive bacterium Listeria monocytogenes EGD-e as a model organism. A pattern search for the Cys-Xaa-Xaa-Cys (CXXC) motif--a hallmark of TDRM proteins--in the genome sequence of the bacterium yielded 156 proteins. After initial refinement by protein and domain analysis, 14 candidate proteins remained. Subsequent detailed analyses, supported by modeling of 3D structures and data integration yielded 6 thioredoxin-like proteins plus thioredoxin reductase, glutaredoxin, one redox-sensitive regulator, one peptide methionine reductase - all typical TDRM constituents - and three putative novel components of the TDRM. For all 14 proteins orthologues were found in other Listeria species. Homology searches and phylogenetic analyses showed that related proteins are present mainly in other Firmicutes. This fast approach required minimal resources. It is immediately applicable to any genome with appropriate modifications and should be practicable also for other conserved, functionally important amino acid motifs.		Shubha Gopal;Vanishree Srinivas;Farhan Zameer;Jürgen Kreft	2009	In silico biology	10.3233/ISB-2009-0409	biology;biochemistry;redox;prediction;bacteria;bioinformatics;genetics;metabolism;statistics;phylogenetics	Comp.	1.3693386302396242	-59.88659582292171	8304
3fa628e7cff0b1dad3f15de98f99b0fdb09df834	people recognition in ambiguously labeled photo collections	training;testing;distance based face description method people recognition consumer photo collections ambiguous labels graphical model;photo collections people recognition ambiguous labels;photo collections;graphical models;face recognition;feature extraction;ambiguous labels;mathematical model;face recognition graphical models training feature extraction testing context mathematical model;people recognition;context	We show how to recognize people based on their faces in Consumer Photo Collections while also incorporating context in the form of ambiguous labels. Such labels can be assigned to single photos (depicting multiple people) as well as to entire sets of photos (e.g. relating to events). To achieve this, we devise a unified framework that has a graphical model along a distance-based face description method at its core. We evaluate our probabilistic approach by performing experiments on two datasets, one of which includes around 5000 face appearances spanning nearly ten years.	experiment;file spanning;graphical model;graphical user interface;unified framework	Markus Brenner;Ebroul Izquierdo	2013	2013 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2013.6607603	facial recognition system;natural language processing;computer vision;feature extraction;computer science;machine learning;mathematical model;multimedia;software testing;graphical model	Vision	20.695063576197462	-60.473240208832635	8307
3b2ddbc39c2660dbe75340dee2d9823e724b2fc3	corrigendum to 'analysis of factors that induce cysteine bonding state' [computers in biology and medicine 39 (2009) 332-339]	cysteine bonding state		computers in biology and medicine	Samad Jahandideh;Somayyeh Hoseini;Mina Jahandideh;Afsaneh Hoseini;AliSalehzadeh Yazdi	2010	Comp. in Bio. and Med.	10.1016/j.compbiomed.2010.03.013	physiology	NLP	2.836807416611817	-64.96149872589723	8320
75e17dcbd4c184bd9869cfc16171424b48aa7551	automated quantification of brain magnetic resonance image hyperintensities using hybrid clustering and knowledge-based methods	magnetic resonance image;knowledge base	Previous computerized methods of hyperintensity identification in brain magnetic resonance images (MRI) either rely heavily on human intervention or on simple thresholding techniques. Such methods can lead to considerable variation in the quantification of brain hyperintensities depending upon image parameters such as contrast. This paper describes an automated, knowledge-guided method of hyperintensity detection in brain MRI that addresses problems associated with human subjectivity and thresholding techniques. This method, which we call knowledge-guided hyperintensity detection (KGHID), uses encoded knowledge of brain anatomy and MRI characteristics of individual tissues to reclassify pixels from an initial unsupervised segmentation. With this encoded knowledge, KGHID discriminates lesions embedded within the white matter, hyperintense lesions of the basal ganglia and the periventricular ring. The method is designed for high sensitivity detection and monitoring of subtle lesions in patients with neurodegenerative diseases. © 1999 John Wiley & Sons, Inc. Int J Imaging Syst Technol, 10, 287–293, 1999	basal (phylogenetics);cluster analysis;embedded system;ganglia;john d. wiley;pixel;resonance;thresholding (image processing)	Karen M. Gosche;Robert P. Velthuizen;F. Reed Murtagh;John A. Arrington;William W. Gross;James A. Mortimer;Laurence P. Clarke	1999	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(1999)10:3%3C287::AID-IMA9%3E3.0.CO;2-Z	knowledge base;radiology;pathology;computer science;magnetic resonance imaging;nuclear medicine	Vision	37.04478040334323	-78.81488509007747	8327
4631d172d174e26890e87058cedd44d5b1866863	sicilia: a smart sensor system for clothing insulation inference using heat exchange	human comfort;design;hvac;human thermal comfort;miscellaneous;thermal sensing	We present SiCILIA, a hardware platform that extracts physical and personal variables of an individual's thermal environment to infer the amount of clothing insulation and thermal sensation without human intervention. The proposed inference algorithms build upon theories of body heat transfer, and are corroborated by empirical data. Experimental results show the algorithm is capable of accurately predicting an occupant's thermal insulation with a confidence interval of approximately ±0.3 and a mean prediction error of 0.2.	algorithm;smart transducer;theory	Ala Shaabana;Rong Zheng;Zhipeng Xu	2015		10.1145/2737095.2742934	embedded system;hvac;simulation	AI	8.093268386505269	-85.60586613689543	8328
80ea9178084e241f0d65ed872368de06054d3161	poster: a study on eeg oscillations for bci during standing	human information processing;input devices and strategies;frequency 4 8 hz;oscillations;visual evoked potentials electroencephalography oscillations statistical analysis user interfaces;steady state visual evoked potentials;brain computer interface;electroencephalographic oscillations;bci;frequency 4 8 hz eeg oscillations bci brain activities electroencephalographic oscillations steady state visual evoked potentials linear discriminant analysis eye gaze directions brain computer interfaces frequency 6 9 hz;h 1 2 user machine systems human information processing;linear discriminate analysis;visual evoked potentials;eye gaze directions;h 5 2 user interfaces;statistical analysis;h 5 2 user interfaces input devices and strategies;h 5 2 user interfaces input devices and strategies brain computer interface electroencephalography steady state visual evoked potentials standing posture h 1 2 user machine systems human information processing;eeg oscillations;frequency 6 9 hz;brain activation;brain computer interfaces;electroencephalography;brain activities;linear discriminant analysis;user interfaces;steady state visual evoked potential;eye gaze;standing posture;electroencephalography steady state virtual environment frequency brain computer interfaces human factors user interfaces displays information science linear discriminant analysis;immersive virtual environment;user machine systems	Brain activities in a standing posture were studied in immersing virtual environments. Electroencephalographic oscillations induced by two flickering virtual buttons were recorded for one subject. The flickering frequencies were set to be 6.9 and 4.8 Hz. Even in standing, clear steady-state visual evoked potentials were observed. Linear discriminant analysis with the single trial brain activities for 2 seconds yielded 74.9% of the average classification performance in sitting and 80.7% in standing in inferring three eye-gaze directions. The results in this study contribute to reduce both mental and physical stress of the users of brain-computer interfaces.	brain–computer interface;electroencephalography;flicker (screen);linear discriminant analysis;neural oscillation;poor posture;steady state;virtual reality	Hideaki Touyama;Michitaka Hirose	2008	2008 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2008.4476605	computer vision;speech recognition;computer science;communication	Visualization	13.508396858404996	-93.7654004662859	8330
60345e6103c1b09052244553f024288a46fc2610	automatic ribs segmentation and counting from mouse x-ray images		The automatic detection and quantification of skeletal structures has a variety of applications for biological research. This paper proposes an automatic solution for rib segmentation and counting based on structural properties of ribs in mouse X-ray images. The solution consists of five stages, including alignment, cropping the region of interest, image enhancement, rib segmentation, filtering of non-rib objects and rib counting. Experimental results on a set of 100 X-ray images showed consistent good performance of the algorithm compared with a manual annotation by domain experts, with an overall accuracy of about 88%.	algorithm;image editing;radiography;region of interest;x-ray (amazon kindle)	Omar Al Okashi;Hongbo Du;Joanne L. Selway;Christopher J Lelliott;Simon Maguire;Dave Melvin;Kenneth Langlands;Hisham Al-Assam	2014			rib cage;computer vision;artificial intelligence;x-ray;segmentation;computer science	Vision	39.18621538715143	-77.0442455717708	8372
47440eab8168af67016aedbec22647cd6b9f8ec7	constructing statistically unbiased cortical surface templates using feature-space covariance	gray matter cortical surface;cortical surface feature space;unbiased template	The choice of surface template plays an important role in cross-sectional subject analyses involving cortical brain surfaces because there is a tendency toward registration bias given variations in inter-individual and inter-group sulcal and gyral patterns. In order to account for the bias and spatial smoothing, we propose a feature-based unbiased average template surface. In contrast to prior approaches, we factor in the sample population covariance and assign weights based on feature information to minimize the influence of covariance in the sampled population. The mean surface is computed by applying the weights obtained from an inverse covariance matrix, which guarantees that multiple representations from similar groups (e.g., involving imaging, demographic, diagnosis information) are down-weighted to yield an unbiased mean in feature space. Results are validated by applying this approach in two different applications. For evaluation, the proposed unbiased weighted surface mean is compared with un-weighted means both qualitatively and quantitatively (mean squared error and absolute relative distance of both the means with baseline). In first application, we validated the stability of the proposed optimal mean on a scan-rescan reproducibility dataset by incrementally adding duplicate subjects. In the second application, we used clinical research data to evaluate the difference between the weighted and unweighted mean when different number of subjects were included in control versus schizophrenia groups. In both cases, the proposed method achieved greater stability that indicated reduced impacts of sampling bias. The weighted mean is built based on covariance information in feature space as opposed to spatial location, thus making this a generic approach to be applicable to any feature of interest.	baseline (configuration management);clinical use template;cross-sectional data;feature vector;generic drugs;mean squared error;sampling (signal processing);sampling - surgical action;schizophrenia;silo (dataset);smoothing (statistical technique);weight	Prasanna Parvathaneni;Ilwoo Lyu;Yuankai Huo;Justin A. Blaber;Allison E. Hainline;Hakmook Kang;Neil D. Woodward;Bennett A. Landman	2018	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2293641	statistics;smoothing;feature vector;covariance matrix;mathematics;population;mean squared error;weighted arithmetic mean;sampling bias;covariance	ML	43.67890437360324	-79.50014761046921	8394
08f6350184a7dc97352664979c7f1a2e1bc00725	multimodal emotion recognition from expressive faces, body gestures and speech	bayesian classifier;multimodal fusion;emotion recognition;facial expression	In this paper we present a multimodal approach for the recognition of eight emotions that integrates information from facial expressions, body movement and gestures and speech. We trained and tested a model with a Bayesian classifier, using a multimodal corpus with eight emotions and ten subjects. First individual classifiers were trained for each modality. Then data were fused at the feature level and the decision level. Fusing multimodal data increased very much the recognition rates in comparison with the unimodal systems: the multimodal approach gave an improvement of more than 10% with respect to the most successful unimodal system. Further, the fusion performed at the feature level showed better results than the one performed at the decision level.	bayesian network;emotion recognition;modality (human–computer interaction);multimodal interaction;naive bayes classifier	George Caridakis;Ginevra Castellano;Loïc Kessous;Amaryllis Raouzaiou;Lori Malatesta;Stylianos Asteriadis;Kostas Karpouzis	2007		10.1007/978-0-387-74161-1_41	naive bayes classifier;speech recognition;computer science;machine learning;pattern recognition;facial expression	NLP	-2.9375177746023886	-85.3270337207311	8399
d15947da705b0ce12801d275b92e867172e78694	a data mining approach for prediction of students' depression using logistic regression and artificial neural network		Due to the rapid changes in human lifestyles, stress is one of the main aspects of our modern life. A new environment and system of education could lead to students' depression. Two data mining techniques were used in this study; Logistic Regression and Artificial Neural Network. The purpose of this study is to compare the performance of different data mining algorithm for prediction of depression among UniKL MIIT students. Stress factors, social factors (interpersonal and intrapersonal) and environment factor attributes to predict the students' depression. The performances of these techniques are compared, based on accuracy. From the findings of our analysis, accuracy of Logistic Regression and Artificial Neural Network are 62.5% and 71.8% respectively. Therefore, it is shown that Artificial Neural Networks technique predicts the depression with the highest accuracy out of these two classification models.	algorithm;artificial neural network;data mining;logistic regression;neural networks;performance	Norhatta Mohd;Yasmin Yahya	2018		10.1145/3164541.3164604	computer science;artificial neural network;logistic regression;data mining;interpersonal communication;intrapersonal communication	ML	6.202031765480963	-78.15624433246138	8410
6e4af8540bcbe83300926696e286d5ead7383228	edge-connectivity indices in qspr/qsar studies, 1. comparison to other topological indices in qspr studies	topological indices	The linear independence of the edge-connectivity index to other first-, second-, and third-generation topological indices is demonstrated by using principal component analysis for octane isomers. Most of the topological indices are loaded in one factor, while the edge connectivity is loaded in another independent factor. The edge-connectivity index does not produce linear correlations ( R e 0.7) with any of the almost 40 topological indices studied. This index produced the best single-variable quantitative structure -property relationship (QSPR) models for five of the seven physicochemical properties of octanes studied. It is concluded that the edge connectivity is an independent index containing important structural information to be used in QSPR/ QSAR (QSAR) quantitative structure -activity relationship) studies.	k-edge-connected graph;principal component analysis;quantitative structure–activity relationship;sgi octane;topological index	Ernesto Estrada;Lissette Rodríguez	1999	Journal of Chemical Information and Computer Sciences	10.1021/ci990030p	stereochemistry;econometrics;chemistry;computer science;computational chemistry;mathematics	Theory	12.933657291742849	-57.872133046952094	8412
a2d3fa2b455960148b6c235b7ce2cb43dca4ddee	extracting significant sample-specific cancer mutations using their protein interactions		"""We present a joint analysis method for mutation and gene expression data employing information about proteins that are highly interconnected at the level of protein to protein (pp) interactions, which we apply to the TCGA Acute Myeloid Leukemia (AML) dataset. Given the low incidence of most mutations in virtually all cancer types, as well as the significant inter-patient heterogeneity of the mutation landscape, determining the true causal mutations in each individual patient remains one of the most important challenges for personalized cancer diagnostics and therapy. More automated methods are needed for determining these """"driver"""" mutations in each individual patient. For this purpose, we are exploiting two types of contextual information: (1) the pp interactions of the mutated genes, as well as (2) their potential correlations with gene expression clusters. The use of pp interactions is based on our surprising finding that most AML mutations tend to affect nontrivial protein to protein interaction cliques."""	causal filter;causality;classification;cluster analysis;copy number;device driver;gene expression;gene regulatory network;genetic heterogeneity;incidence matrix;interaction;large;leukemia, b-cell;leukemia, myelocytic, acute;maximal set;mutation;neoplasms;numerous;patients;personalization;silo (dataset);the cancer genome atlas;leukemia;statistical cluster	Liviu Badea	2014	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		genetics;cancer;bioinformatics;protein–protein interaction;biology	Comp.	5.451343636356301	-57.1340170893243	8453
10695d04b673472535633c59d0f5f04ca856a61c	combining self-organizing neural nets with multivariate statistics for efficient color image retrieval	busqueda informacion;graph theory;vision ordenador;contenu image;image content;test statistique;teoria grafo;analisis estadistico;data compression;graph method;recherche image;multivariate wald wolfowitz test;image databank;information retrieval;echantillonnage;test estadistico;statistical test;metric;color histogram;nonparametric statistic;probabilistic approach;metodo grafo;theorie graphe;similitude;methode graphe;computer vision;color image retrieval;sampling;neural gas;hybrid approach;histogram;neural net;neural gas network;histogramme;statistical analysis;recherche information;enfoque probabilista;approche probabiliste;banco imagen;banque image;analyse statistique;similarity;self organizing neural networks;similarity measures;autoorganizacion;self organization;multivariate statistics;metrico;vision ordinateur;compresion dato;similitud;reseau neuronal;muestreo;graph theoretic methods;imagen color;contenido imagen;histograma;similarity measure;red neuronal;image couleur;autoorganisation;metrique;compression donnee;color image;neural network;image retrieval	An efficient novel strategy for color-based image retrieval is introduced. It is a hybrid approach combining a data compression scheme based on self-organizing neural networks with a nonparametric statistical test for comparing vectorial distributions. First, the color content in each image is summarized by representative RGB-vectors extracted using the Neural-Gas network. The similarity between two images is then assessed as commonality between the corresponding representative color distributions and quantified using the multivariate Wald-Wolfowitz test. Experimental results drawn from the application to a diverse collection of color images show a significantly improved performance (approximately 10-15% higher) relative to both the popular, simplistic approach of color histogram and the sophisticated, computationally demanding technique of Earth Mover's Distance.	artificial neural network;color image;image retrieval;organizing (structure);self-organization	Christos Theoharatos;Nikolaos A. Laskaris;George Economou;Spiros Fotopoulos	2006	Computer Vision and Image Understanding	10.1016/j.cviu.2006.02.008	neural gas;data compression;color histogram;sampling;computer vision;multivariate statistics;statistical hypothesis testing;self-organization;color quantization;similarity;color normalization;color image;metric;computer science;artificial intelligence;similitude;machine learning;histogram;mathematics;artificial neural network;statistics	Vision	44.883600519888674	-62.16424364211393	8478
59429d72a3f8803fdd490688c1cbbff64cc9e1e4	tele-control of an endoscopic surgical robot system between japan and thailand for tele-notes	health informatics;health information management;biomedical engineering	We describe our experience of the development of a endoscopic surgical robot system that can penetrate into the body through the esophagus and perform surgeries in the upper gastric tubes and several organs in the abdominal cavity. In this paper, we describe the results of an experiment using this robot. We describe the configuration of the control system using a gigabit ethernet system named JGN2 for the endoscopic surgical robot. We also describe the results of the first telesurgery experiment using the NOTES (natural orifice transluminal endoscopic surgery) procedure (tele-NOTES), performed at a distance of about 3,750 km.	abdomen;abdominal cavity;anatomical orifice;control system;dental caries;esophagus;gigabit;name;operative surgical procedures;organ;remote surgery;robot;surgical endoscopy;telephone number;television;biomedical tube device	Naoki Suzuki;Asaki Hattori;Satoshi Ieiri;Kozo Konishi;Takashi Maeda;Yuichi Fujino;Yukihiro Ueda;Patpong Navicharern;Kazuo Tanoue;Makoto Hashizume	2009	Studies in health technology and informatics	10.3233/978-1-58603-964-6-374	health informatics;medicine;pathology;biological engineering	Robotics	40.074011930317226	-86.54852336982911	8485
8e4374ed6028f001491a9c92b9628d0989ebd2c9	crystal structure prediction of flexible molecules using parallel genetic algorithms with a standard force field	parallel genetic algorithm;computer program;distributed computing;genetics;crystallization;gaff;force field;force fields;crystal structure prediction;algorithms;genetic algorithm;genetic algorithms;crystal structure;energy minimization;computer simulation	This article describes the application of our distributed computing framework for crystal structure prediction (CSP) the modified genetic algorithms for crystal and cluster prediction (MGAC), to predict the crystal structure of flexible molecules using the general Amber force field (GAFF) and the CHARMM program. The MGAC distributed computing framework includes a series of tightly integrated computer programs for generating the molecule's force field, sampling crystal structures using a distributed parallel genetic algorithm and local energy minimization of the structures followed by the classifying, sorting, and archiving of the most relevant structures. Our results indicate that the method can consistently find the experimentally known crystal structures of flexible molecules, but the number of missing structures and poor ranking observed in some crystals show the need for further improvement of the potential.	archive;charmm;classification;computation (action);computer program;crystal structure prediction;distributed computing;energy minimization;experiment;force field (chemistry);genetic algorithm;microscopy, atomic force;sampling (signal processing);sorting	Seonah Kim;Anita M. Orendt;Marta B. Ferraro;Julio C. Facelli	2009	Journal of computational chemistry	10.1002/jcc.21189	computer simulation;genetic algorithm;chemistry;theoretical computer science;computational chemistry;crystal structure prediction	HPC	12.118907985768942	-54.49554975817429	8494
8582178730e944a3515a61807eaddf889c998609	gdcrnatools: an r/bioconductor package for integrative analysis of lncrna, mirna and mrna data in gdc		Motivation The large-scale multidimensional omics data in the Genomic Data Commons (GDC) provides opportunities to investigate the crosstalk among different RNA species and their regulatory mechanisms in cancers. Easy-to-use bioinformatics pipelines are needed to facilitate such studies.   Results We have developed a user-friendly R/Bioconductor package, named GDCRNATools, for downloading, organizing and analyzing RNA data in GDC with an emphasis on deciphering the lncRNA-mRNA related competing endogenous RNAs regulatory network in cancers. Many widely used bioinformatics tools and databases are utilized in our package. Users can easily pack preferred downstream analysis pipelines or integrate their own pipelines into the workflow. Interactive shiny web apps built in GDCRNATools greatly improve visualization of results from the analysis.   Availability and implementation GDCRNATools is an R/Bioconductor package that is freely available at Bioconductor (http://bioconductor.org/packages/devel/bioc/html/GDCRNATools.html). Detailed instructions, manual and example code are also available in Github (https://github.com/Jialab-UCR/GDCRNATools).		Ruidong Li;Han Qu;Shibo Wang;Julong Wei;Le Zhang;Renyuan Ma;Jianming Lu;Jianguo Zhu;Wei-De Zhong;Zhenyu Jia	2018	Bioinformatics	10.1093/bioinformatics/bty124	competing endogenous rna;data mining;microrna;bioconductor;computer science;web application;crosstalk;bioinformatics;workflow;omics;messenger rna	Comp.	-2.1065482755685396	-58.40654870328704	8496
7d508a40facbbcf6ee05668b4fd093e887c6f818	an adaptable $k$ -nearest neighbors algorithm for mmse image interpolation	baja resolucion;embedding;nonlinear filters;nearest neighbor searches;interpolation;image processing;image resolution;least mean squares methods;modelo markov;learning;mmse image interpolation;interpolacion;minimum mean squared error solution;erreur quadratique moyenne;low resolution;markov random fields;basse resolution;image interpolation;procesamiento imagen;image classification;optimum global;testing;real world images;probabilistic approach;global optimum;classification;traitement image;adaptive k nearest neighbor algorithm;markov random field;weighting filter;algorithme;aprendizaje;filtre ponderation;training data;algorithm;vecino mas cercano;apprentissage;markov model;campo aleatorio;mean square error;heuristic algorithms;enfoque probabilista;approche probabiliste;filtro ponderacion;nearest neighbor;plus proche voisin;nearest neighbour;k nearest neighbor;global optimization;humans;markov processes;estimation error;superresolution;error medio cuadratico;modele markov;minimum mean square error;high dimension;optimo global;filter generation;champ aleatoire;nearest;algoritmo;random field	We propose an image interpolation algorithm that is nonparametric and learning-based, primarily using an adaptive k-nearest neighbor algorithm with global considerations through Markov random fields. The empirical nature of the proposed algorithm ensures image results that are data-driven and, hence, reflect ldquoreal-worldrdquo images well, given enough training data. The proposed algorithm operates on a local window using a dynamic k -nearest neighbor algorithm, where k differs from pixel to pixel: small for test points with highly relevant neighbors and large otherwise. Based on the neighbors that the adaptable k provides and their corresponding relevance measures, a weighted minimum mean squared error solution determines implicitly defined filters specific to low-resolution image content without yielding to the limitations of insufficient training. Additionally, global optimization via single pass Markov approximations, similar to cited nearest neighbor algorithms, provides additional weighting for filter generation. The approach is justified in using a sufficient quantity of training per test point and takes advantage of image properties. For in-depth analysis, we compare to existing methods and draw parallels between intuitive concepts including classification and ideas introduced by other nearest neighbor algorithms by explaining manifolds in low and high dimensions.	approximation;embedding;euclidean distance;exhibits as topic;feature vector;global optimization;image quality;interpolation imputation technique;k-nearest neighbors algorithm;learning disorders;markov chain;markov random field;mathematical optimization;mean squared error;nonlinear dimensionality reduction;overfitting;parallels desktop for mac;pixel;reduction (complexity);relevance;scott continuity;semiconductor industry;single linkage cluster analysis;test point;texture synthesis;tree (data structure);window function;manifold;statistical cluster	Karl S. Ni;Truong Q. Nguyen	2009	IEEE Transactions on Image Processing	10.1109/TIP.2009.2023706	nearest-neighbor chain algorithm;large margin nearest neighbor;computer vision;best bin first;image resolution;image processing;interpolation;computer science;machine learning;pattern recognition;mathematics;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm;statistics;global optimization	Vision	52.49738128171459	-67.85605571252015	8518
2df04ec76261cb34be433bf7e5a0ef461e1937dc	fpga-based computation of the inductance of coils used for the magnetic stimulation of the nervous system	nervous system;error processing;floating point;hardware implementation	In the last years the interest for magnetic stimulation of the human nervous tissue has increased considerably, because this technique has proved its utility and applicability both as a diagnostic and as a treatment instrument. Research in this domain is aimed at removing some of the disadvantages of the technique: the lack of focalization of the stimulated region and the reduced efficiency of the energetic transfer from the stimulating coil to the tissue. Better stimulation coils can solve these problems. Designing coils is so far a trial-and error process, relying on very compute-intensive simulations. In software, such a simulation has a very high running time. This paper proposes and demonstrates an FPGA-based hardware implementation of this simulation, which reduces the computation time by 4 orders of magnitude. Thanks to this powerful tool, some significant improvements in the design of the coils have already been obtained.	computation;field-programmable gate array;simulation;time complexity	Ionut Trestian;Octavian Cret;Laura Cret;Lucia Vacariu;Radu Tudoran;Florent de Dinechin	2008			embedded system;electronic engineering;computer hardware;computer science;floating point;operating system;nervous system	EDA	28.47299608164877	-76.68400878710183	8536
061d303381266e1ee751f5b7551d25324c043bed	parametric image segmentation of humans with structural shape priors	datorseende och robotik autonoma system	The figure-ground segmentation of humans in images captured in natural environments is an outstanding open problem due to the presence of complex backgrounds, articulation, varying body proportions, partial views and viewpoint changes. In this work we propose class-specific segmentation models that leverage parametric max-flow image segmentation and a large dataset of human shapes. Our contributions are as follows: (1) formulation of a submodular energy model that combines class-specific structural constraints and data-driven shape priors, within a parametric max-flow optimization methodology that systematically computes all breakpoints of the model in polynomial time; (2) design of a data-driven class-specific fusion methodology, based on matching against a large training set of exemplar human shapes (100,000 in our experiments), that allows the shape prior to be constructed on-the-fly, for arbitrary viewpoints and partial views. (3) demonstration of state of the art results, in two challenging datasets, H3D and MPII (where figure-ground segmentation annotations have been added by us), where we substantially improve on the first ranked hypothesis estimates of mid-level segmentation methods, by 20%, with hypothesis set sizes that are up to one order of magnitude smaller.	biconnected component;breakpoint;experiment;humans;image segmentation;mathematical optimization;maximum flow problem;polynomial;sensor;submodular set function;test set;time complexity	Alin-Ionut Popa;Cristian Sminchisescu	2016		10.1007/978-3-319-54184-6_5	computer vision;computer science;machine learning;pattern recognition;mathematics;scale-space segmentation	Vision	45.80525287095956	-52.319833932671706	8553
8b8775b59e777b2b303fd7a74a09b2292d10151e	discrete camera calibration from pixel streams	contraste;modelizacion;medida informacion;traitement signal;mesure deplacement;vision ordenador;analisis estadistico;relative orientation;dissimilarity measure;mesure information;statistical signal processing;light field;sphere embedding;orientation;teoria medida;computer vision;sphere;modelisation;captador medida;measurement sensor;capteur mesure;displacement measurement;statistical analysis;information measure;distance geometry;signal processing;non parametric camera calibration;analyse statistique;orientacion;vision ordinateur;etalonnage;medicion desplazamiento;esfera;theorie information;camera calibration;theorie mesure;procesamiento senal;modeling;calibration;measure theory;information theory;teoria informacion	We consider the problem of estimating the relative orientat ion of a number of individual photocells -or pixelsthat hold fixed relative posit ions. The photocells measure the intensity of light traveling on a pencil of lines. We assu me that the light-field thus sampled is changing, e.g. as the result of motion of the senso rs and use the obtained measurements to estimate the orientations of the photocell s. Our approach is based on correlation and information-theor y dissimilarity measures. Experiments with real-world data show that the dissi milarity measures are strongly related to the angular separation between the phot ocells, and the relation can be modeled quantitatively. In particular we show that this m odel allows to estimate the angular separation from the dissimilarity. Although th e resulting estimators are not very accurate, they maintain their performance through t different visual environments, suggesting that the model encodes a very general prop erty of our visual world. Finally, leveraging this method to estimate angles from sig nal pairs, we show how distance geometry techniques allow to recover the complete sensor geometry.	angularjs;camera resectioning;experiment;pixel;signature block	Etienne Grossmann;José António Gaspar;Francesco Orabona	2010	Computer Vision and Image Understanding	10.1016/j.cviu.2009.03.009	computer vision;calibration;camera resectioning;systems modeling;measure;information theory;light field;signal processing;mathematics;geometry;orientation;distance geometry;statistical signal processing;sphere;statistics	Vision	51.06179394024704	-57.11168793594411	8554
8c114db6981c3c4c5043fd5da1936a8d4eeba622	hidden-markov-model-based segmentation confidence applied to container code character extraction	character extraction;coding systems;container code;goods distribution;probability;management system;image segmentation;image processing;segmented character probability hidden markov model based segmentation confidence container code character extraction intelligent container management systems missing character;hidden markov model;optical character recognition;training;segmentation confidence character extraction container code hidden markov models hmms image processing;segmentation communications;containers image segmentation hidden markov models image processing;hidden markov models;probability character recognition containers feature extraction goods distribution hidden markov models image segmentation;feature extraction;hidden markov models hmms;algorithms;markov processes;segmentation confidence;character recognition;containers	Automatic container code recognition (ACCR) has become an indispensable aspect of current intelligent container management systems. In real applications, an ACCR module sometimes faces the problem of missing characters, i.e., not all the 11 container code characters (CCCs) appear in the input image. However, a few of the present methods can process container code images with missing characters. Therefore, a method is proposed to extract the CCCs for both the situation wherein all the 11 CCCs appear in an image and the situation wherein some CCCs are missing. In this method, hidden Markov model (HMM)-based segmentation confidence is proposed to describe the probability of the segmented characters belonging to the container code. Based on the segmentation confidence, the segmented characters are determined whether they belong to the container code or not, and if there are some characters missing, the positions of these characters can be estimated. Various container code images have been used to test the proposed method. The results of the tests show that the method is effective.	algorithm;align (company);automatic number plate recognition;code;definition;hidden markov model;markov chain;trust (emotion);wildcard character;cgroups	Mo Chen;Wei Wu;Xiaomin Yang;Xiaohai He	2011	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2011.2145417	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;probability;management system;image segmentation;markov process;optical character recognition;hidden markov model;statistics	Robotics	34.64522761079181	-66.15258673299466	8562
0d37a29e128946a9b45061a82e9879d5653ce954	on the spectral formulation of granger causality	transfer entropy;partial directed coherence;multivariate processes;sims causality;directed transfer function	Spectral measures of causality are used to explore the role of different rhythms in the causal connectivity between brain regions. We study several spectral measures related to Granger causality, comprising the bivariate and conditional Geweke measures, the directed transfer function, and the partial directed coherence. We derive the formulation of dependence and causality in the spectral domain from the more general formulation in the information-theory framework. We argue that the transfer entropy, the most general measure derived from the concept of Granger causality, lacks a spectral representation in terms of only the processes associated with the recorded signals. For all the spectral measures we show how they are related to mutual information rates when explicitly considering the parametric autoregressive representation of the processes. In this way we express the conditional Geweke spectral measure in terms of a multiple coherence involving innovation variables inherent to the autoregressive representation. We also link partial directed coherence with Sims’ criterion of causality. Given our results, we discuss the causal interpretation of the spectral measures related to Granger causality and stress the necessity to explicitly consider their specific formulation based on modeling the signals as linear Gaussian stationary autoregressive processes.	autoregressive model;bivariate data;causality;information theory;mutual information;normal statistical distribution;spectral method;stationary process;stellar classification;transfer entropy;transfer function	D. Chicharro	2011	Biological Cybernetics	10.1007/s00422-011-0469-z	psychology;granger causality;econometrics;combinatorics;coherence;transfer entropy;mathematics;statistics	ML	22.558665037386334	-76.09891103367646	8591
cb3ab7ca2cda0a784c0a94ad2bdff2bcbc376afe	a shunting inhibitory convolutional neural network for gender classification	biometric applications;selected works;neural nets;local receptive field processing;gender classification;image classification;multilayer perceptron;human recognition;feret benchmark dataset;era2012;bepress;weight sharing;social applications;receptive field;feret benchmark dataset shunting inhibitory convolutional neural network gender classification demographic features human recognition social applications biometric applications convolutional neural networks local receptive field processing weight sharing;convolutional neural networks;demographic features;neural networks shape testing spatial databases table lookup humans application software image databases neurons telecommunication computing;classification accuracy;demography;neural nets demography image classification;shunting inhibitory convolutional neural network;neural network	Demographic features, such as gender, are very important for human recognition and can be used to enhance social and biometric applications. In this paper, we propose to use a class of convolutional neural networks for gender classification. These networks are built upon the concepts of local receptive field processing and weight sharing, which makes them more tolerant to distortions and variations in two dimensional shapes. Tested on two separate data sets, the proposed networks achieve better classification accuracy than the conventional feedforward multilayer perceptron networks. On the Feret benchmark dataset, the proposed convolutional neural networks achieve a classification rate of 97.1%	artificial neural network;benchmark (computing);biometrics;convolutional neural network;distortion;feret (facial recognition technology);feret database;feedforward neural network;multilayer perceptron;preprocessor;statistical classification;toeplitz hash algorithm	Fok Hing Chi Tivive;Abdesselam Bouzerdoum	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.173	contextual image classification;computer science;artificial intelligence;machine learning;pattern recognition;deep learning;convolutional neural network;multilayer perceptron;receptive field;artificial neural network	Vision	25.52608804927004	-61.95682086447348	8599
3f8bf5063deec89c41f2dfdbd4bb9daf68895946	a dcnn and sdm based face alignment algorithm		We present a coarsely locating little points and finely locating many points approach for face alignment. This cascade structure replies to 2 problems existing all the time in face alignment: the initialization and great accuracy difference between inner points and outline points. First, we adopt DCNN to coarsely localize 5 points: two pupils, nose and two mouth corners. Second, based on shape initialization of coarse location, using SDM with extracting simplified SIFT features, we finely localizes 49 inner points and 17 outline points. Experiments on CAS-PEAL-R1 and FERET database show that our approach is accurate and robust. The proposed method achieves 99.23% localization accuracy of eyes on CAS-PEAL-R1.	algorithm	Qingsong Tang;Qinqin Zhang;Xiaomeng Zhang;Zhenlin Cai;Xiangde Zhang	2015		10.1007/978-3-319-25417-3_12	computer vision;feret database;artificial intelligence;initialization;scale-invariant feature transform;computer science	Vision	40.97702188403769	-56.47558333829672	8627
587a32d4e2135059b532782e466d5cca6828bf8e	road hazard detection and sharing with multimodal sensor analysis on smartphones	mobile sensor;multimodal sensor analysis;image processing;pervasive computing multimodal sensor analysis mobile sensor mobile gis applications signal processing;pervasive computing;mobile gis applications;hazards;smart phones;vehicles mobile communication roads hazards accelerometers sensors image processing;feature extraction;signal processing;road safety;smart phones feature extraction hazards image processing intelligent sensors road safety;road hazard detection automatic image extraction speed bumps critical hazard detection road segment video section synchronized sensor readings plugin based multimodal analysis signal processing based smart mobile applications image processing based smart mobile applications built in multimodal sensor analysis single modal sensor analysis smart sensor based in car mobile application information sharing sensor management image processing smart cities smart applications smartphones multimodal sensor analysis road hazard sharing;intelligent sensors	The sensing, computing and communicating capabilities of smart phones bring new possibilities for creating smart applications, including in-car mobile applications for smart cities. However, due to the dynamic nature of vehicles, many requirements such as sensor management, signal and image processing or information sharing needs exist when developing a smart sensor-based in-car mobile application. On the other hand, most in-car applications generally employ single-modal sensor analysis, which also yields limited results. Using the advanced capabilities of smart phones, this study proposes a framework with built-in multimodal sensor analysis capability, and enables easy and rapid development of signal and image processing-based smart mobile applications. Within this framework, an abstraction for fast access to synchronized sensor readings, a plug in based multimodal analysis interface for signal and image processing applications, and a toolset to connect to other users or servers for sharing the results are provided built-in. As part of this study, a sample mobile application is also developed to demonstrate the applicability of the framework. This application is used for detecting defects on the road, such as potholes and speed bumps, and it automatically extracts the video section and the image of the corresponding road segment containing the defect. Upon such critical hazard detection, the application instantly informs nearby users about the incident. A good detection rate of speed bumps is obtained in the performed tests, while the advantage of automatic image extraction based on the multimodal approach is also demonstrated.	algorithm;anomaly detection;canonical account;image processing;institute for operations research and the management sciences;microphone;mobile app;modal logic;multimodal interaction;real-time clock;requirement;sensor;smart city;smart transducer;smartphone;software bug	Fatih Orhan;P. Erhan Eren	2013	2013 Seventh International Conference on Next Generation Mobile Apps, Services and Technologies	10.1109/NGMAST.2013.19	embedded system;simulation;image processing;feature extraction;hazard;computer science;signal processing;computer security;intelligent sensor;visual sensor network	Mobile	4.137926758146319	-86.60571072400984	8629
8590e2c9dbf0df8ae636d66e88994cc85a891df7	bootstrapping gait data from people with cerebral palsy	cerebral palsy;birth disorders surgery hospitals biological tissues testing hip educational institutions time measurement laboratories thigh;medical computing;data analysis;confidence interval;maximum value gait data bootstrap technique cerebral palsy abnormal gait patterns surgical intervention internal rotation soft tissue procedures varus rotation osteotomy diplegia quadriplegia pre operative test post operative test mean difference curve confidence interval bootstrapped curves;medical computing gait analysis surgery bone data analysis;bone;surgery;gait analysis;soft tissue	Abnormal gait patterns are common in people with cerebral palsy. A surgical intervention to diminish internal rotation during gait in the cerebral palsy population has included soft tissue procedures and rotation osteotomies. One common procedure is varus rotation osteotomy, VRO. We report here on the results for 37 people with cerebral palsy who were at least 3 years old, cooperative and able to follow directions. The average age at the pre-operative test stage was 8.7+3.3 years. There were 14 males and 23 females. Thirty-five were diplegics and two were quadraplegics. Each was tested pre-operatively, at six months post-operative and 12 months post-operative. The mean difference curve along with the 95% confidence interval about the difference was calculated using a bootstrap technique. The depiction of the bootstrapped curves is compared with the use of the maximum value for comparisons.	backup rotation scheme;booting;bootstrapping (compilers)	Margaret G. E. Peterson;Mary P. T. Murray-Weir;Leon Root;M. Lenhoff;L. Daly;C. Wagner	2000		10.1109/CBMS.2000.856876	confidence interval;gait analysis;medicine;physical therapy;data analysis;soft tissue;surgery	HCI	14.343020305240211	-84.03664552647759	8635
9d80f2e1ac0ffa25f7d63b90f1627a5bee5daea2	development of a computer vision system for the automatic quality grading of mandarin segments	quality grade;computer vision;image acquisition;machine vision;automatic inspection	This work focuses on the development of a computer vision system for the automatic on-line inspection and classification of Satsuma segments. During the image acquisition the segments are in movement, wet and frequently in contact with other pieces. The segments are transported over six semi-transparent conveyor belts that advance at speed of 1 m/s. During on-line operation, the system acquires images of the segments using two cameras connected to a single computer and process the images in less than 50 ms. Extracting morphological features from the objects, the system identifies automatically pieces of skin and row material and separates entire segments from broken ones, discriminating between those with slight or large breaking degree. Combinations of morphological parameters were employed to decide the quality of each segment, classifying correctly 95% of sound segments.	computer vision;super robot monkey team hyperforce go!	José Blasco;Sergio Cubero;Raúl Arias;Juan Gómez-Sanchís;Florentino Juste;Enrique Moltó	2007		10.1007/978-3-540-72849-8_58	computer vision;machine vision;computer science;computer graphics (images)	Vision	35.06540365722033	-66.92666008911674	8636
0beae2bcaf9d84243dc7ee9a8b3be39d06f4b9b6	towards numerical temporal-frequency system modelling of associations between electrocardiogram and ballistocardiogram	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;electrocardiography signal processing algorithms mathematical model numerical models approximation methods prediction algorithms algorithm design and analysis;research articles;linear modelling numerical temporal frequency system modelling electrocardiogram ballistocardiogram heartbeat bcg computing physiological signals signal processing domain heart rate detection embedded information wavelet based temporal frequency system model bcg to ecg predicting algorithm;abstracts;open access;life sciences;clinical guidelines;full text;wavelet transforms electrocardiography medical signal processing;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Ballistocardiogram (BCG) is a vital sign of ballistic forces generated by each heartbeat. With the advancements in related sensor and computing technologies in recent years, BCG has become far more accessible and thus regained its interest in both research and industry fields. Here we would like to promote the system modelling approach to BCG computing that allows to explore the underlying association between BCG and other physiological signals such as electrocardiogram (ECG). This is in contrast to most of the existing works in the related signal processing domain, which focus on detecting heart rate only. The system modelling approach may eventually improve the clinical significance of the BCG by extracting deeply embedded information. Towards this goal, here we present our preliminary study where we design a Wavelet-based temporal-frequency system model for associating BCG and ECG. To validate the model, we also collect simultaneous BCG and ECG recordings from 4 healthy subjects. We use the system model to build a BCG to ECG predicting algorithm. We demonstrate that this temporal-frequency model and algorithm is far superior, in terms of accuracy, to the naïve method of linear modelling.	ballistocardiography;computation (action);electrocardiography;embedded system;embedding;mental association;numerical analysis;sensor;signal processing;vital signs;wavelet;algorithm	Aravind Srinivasan;Haihong Zhang;Zhiping Lin;Jit Biswas;Zhihao Chen	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318382	text mining;medical research;computer science;artificial intelligence;data science;machine learning;data mining;statistics	Robotics	21.344862351014164	-87.94828616640756	8639
8d3a9c9d5458cdcdf2db1954f7557ce9eb99613b	genepath: a system for automated construction of genetic networks from mutant data	computer and information science;genetics;data analysis;genetic network	MOTIVATION Genetic networks are often used in the analysis of biological phenomena. In classical genetics, they are constructed manually from experimental data on mutants. The field lacks formalism to guide such analysis, and accounting for all the data becomes complicated when large amounts of data are considered.   RESULTS We have developed GenePath, an intelligent assistant that automates the analysis of genetic data. GenePath employs expert-defined patterns to uncover gene relations from the data, and uses these relations as constraints in the search for a plausible genetic network. GenePath formalizes genetic data analysis, facilitates the consideration of all the available data in a consistent manner, and the examination of the large number of possible consequences of planned experiments. It also provides an explanation mechanism that traces every finding to the pertinent data.   AVAILABILITY GenePath can be accessed at http://genepath.org.   SUPPLEMENTARY INFORMATION Supplementary material is available at http://genepath.org/bi-.supp.	biological science disciplines;cellular material:mcnt:pt:calculus:qn:estimated;experiment;gene regulatory network;relevance;semantics (computer science);tracing (software);mutant	Blaz Zupan;Janez Demsar;Ivan Bratko;Peter Juvan;John A. Halter;Adam Kuspa;Gad Shaulsky	2003	Bioinformatics	10.1093/bioinformatics/btf871	biology;computer science;bioinformatics;artificial intelligence;data mining;mathematics;data analysis;genetics;statistics	Comp.	3.0371975051156004	-57.93398827384321	8646
9f7b11f8f34a71bc9dd2e8f8c54922c01e4fd8a2	measurement of the arm movement using arm support system with three-dimensional acceleration sensor	geriatrics;mechanoception;sensors;gait analysis;acceleration measurement;biomedical measurement;biomedical equipment	Development of technologies for people with disabilities is necessary to improve the quality of life of elderly people in the future aging society. A light compact arm balancer with three-dimensional acceleration sensors accomplishes that goal. This device particularly features a motion analysis system. Results of experiments show that the program ascertains the speed from the acceleration sensor input. The system has an actual operating principle that is simple and reliable.		Kazuto Miyawaki;Kyohei Konishi;Satoru Kizawa;Takehiro Iwami;Goro Obinata	2013	MHS2013	10.1109/MHS.2013.6710437	embedded system;simulation;engineering;electrical engineering	HCI	7.979271832808211	-89.02883985065216	8677
d739ae126d95c9c47fba2996857cd5cb15d533c7	variable selection for disease progression models: methods for oncogenetic trees and application to cancer and hiv	disease progression model;oncogenetic tree;variable selection	Disease progression models are important for understanding the critical steps during the development of diseases. The models are imbedded in a statistical framework to deal with random variations due to biology and the sampling process when observing only a finite population. Conditional probabilities are used to describe dependencies between events that characterise the critical steps in the disease process. Many different model classes have been proposed in the literature, from simple path models to complex Bayesian networks. A popular and easy to understand but yet flexible model class are oncogenetic trees. These have been applied to describe the accumulation of genetic aberrations in cancer and HIV data. However, the number of potentially relevant aberrations is often by far larger than the maximal number of events that can be used for reliably estimating the progression models. Still, there are only a few approaches to variable selection, which have not yet been investigated in detail. We fill this gap and propose specifically for oncogenetic trees ten variable selection methods, some of these being completely new. We compare them in an extensive simulation study and on real data from cancer and HIV. It turns out that the preselection of events by clique identification algorithms performs best. Here, events are selected if they belong to the largest or the maximum weight subgraph in which all pairs of vertices are connected. The variable selection method of identifying cliques finds both the important frequent events and those related to disease pathways.		Katrin Hainke;Sebastian Szugat;Roland Fried;Jörg Rahnenführer	2017		10.1186/s12859-017-1762-1	genetics;path (graph theory);sampling (statistics);biology;conditional probability;bioinformatics;bayesian network;cancer;feature selection;machine learning;disease;population;artificial intelligence	ML	5.34014128234265	-55.47715728604184	8684
0457e0787059f3f56f3db13e4168e78849900da8	simulation results of a small animal liquid xenon pet detector	energy resolution;experimental tests;solid scintillation detectors biomedical equipment image resolution monte carlo methods positron emission tomography;energy resolution liquid xenon pet detector small animal imaging scintillation detector module scintillator crystals axial geometry depth of interaction measurement spatial resolution axial direction;axial geometry;image resolution;liquid xenon pet detector;small animal imaging;animals xenon positron emission tomography solid scintillation detectors energy resolution testing spatial resolution crystals geometry analytical models;positron emission tomography;depth of interaction;scintillation detectors;scintillation detector module;liquid xenon;scintillator crystals;axial direction;solid scintillation detectors;depth of interaction measurement;scintillation detector;monte carlo simulation;monte carlo simulations;monte carlo simulations positron emission tomography scintillation detectors;monte carlo methods;biomedical equipment;spatial resolution	Monte Carlo simulations of a novel concept PET detector for small animal imaging are presented. The scintillation medium of the detector is liquid xenon whose characteristics in terms of detection rival with the common scintillator crystals. Moreover, the axial geometry of the detector enables depth of interaction measurement. A detector module has been built and an experimental test bench has been developed. Simulations of the test bench enabled to determine the methods to use for analysing the experimental data. Moreover, they indicate the spatial resolution in the axial direction and the energy resolution which can be expected from the detector. The results show an axial resolution of 2.87plusmn0.12 mm and an energy resolution of 7.59plusmn0.34%.	computer simulation;monte carlo method;polyethylene terephthalate;preclinical imaging;test bench;xenon (processor)	Yannick Grondin;Michel Desvignes;Laurent Desbat;Stéphane Mancini;Marie-Laure Gallin-Martel;Laurent Gallin-Martel;Olivier Rossetto	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4541224	image resolution;mathematics;optics;nuclear medicine;physics;statistics;monte carlo method;medical physics	Embedded	46.91523157267047	-85.15192315663161	8690
95e77b20aad5d8fb3da25a73a7fbbb2b5b5c80aa	high accuracy optical flow method based on a theory for warping: implementation and qualitative/quantitative evaluation	brightness gradient;fixed point iteration;regularization;computer vision;numerical scheme;smoothing constraints;multiscale pyramid;optical flow;warping;quantitative evaluation	We describe the implementation of a 2D optical flow algorithm published in the European Conference on Computer Vision (ECCV 2004) by Brox et al. [1] (best paper award) and a qualitative and quantitative evaluation of it for a number of synthetic and real image sequences. Their optical flow method combines three assumptions: a brightness constancy assumption, a gradient constancy assumption and a spatio-temporal smoothness constraint. A numerical scheme based on fixed point iterations is used. Their method uses a coarse-to-fine warping strategy to measure larger optical flow vectors. We have investigated the algorithm in detail and our evaluation of the method demonstrates that it produces very accurate optical flow fields from only 2 input images.	algorithm;european conference on computer vision;fixed point (mathematics);gradient;image warping;iteration;maximum flow problem;numerical analysis;optical flow;synthetic data	Mohammad Faisal;John Barron	2007		10.1007/978-3-540-74260-9_46	image warping;fixed-point iteration;regularization;computer vision;mathematical optimization;computer science;theoretical computer science;optical flow;mathematics	Vision	52.800505307060504	-71.67843542287375	8702
dd8b778c3f15503a64dc26404e90a22df5f7f28f	a new active contours approach for finger extensor tendon segmentation in ultrasound images using prior knowledge and phase symmetry		This work proposes a new approach for the segmentation of the extensor tendon in ultrasound images of the second metacarpophalangeal joint (MCPJ). The MCPJ is known to be frequently involved in early stages of rheumatic diseases like rheumatoid arthritis. The early detection and follow up of these diseases is important to start and adapt the treatments properly and, in that way, preventing irreversible damage of the joints. This work relies on an active contours framework, preceded by a phase symmetry preprocessing and with prior knowledge energies, to automatically identify the extensor tendon. Active contours methods are widely used in ultrasound images because of their robustness to speckle noise and ability to join unconnected smaller regions into a coherent shape. The tendon is formulated as a line so open ended active contours were used. Phase symmetry highlights the tendon, by setting a proper scale range and angle span. The distance between structures and the tendon slope were also included to enforce the model based on anatomical characteristics. And finally, the concavity measures were used because, given the anatomy of the finger, we know that the tendon line should have less than two concavities. To solve the active contours energy minimization a genetic algorithm approach was used. Several energy metric configurations were compared using the modified Hausdorff distance and results showed that this segmentation is not only possible, but exhibits errors smaller than 0.5 mm with a confidence of 95% with the phase symmetry preprocessing and energies based on the line neighborhood, area ratio, slope, and concavity measurements.	anatomic structures;articular system;coherence (physics);concave function;early diagnosis;energy minimization;energy, physics;exhibits as topic;genetic algorithm;hausdorff dimension;metacarpophalangeal joint structure;preprocessor;rheumatism;rheumatoid arthritis;small;span distance;structure of extensor tendon;tendon structure;biologic segmentation	Nelson Martins;Saad Sultan;Diana Veiga;Manuel Ferreira;Filipa Teixeira;Miguel Tavares Coimbra	2018	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2017.2723819	computer vision;pattern recognition;artificial intelligence;robustness (computer science);metacarpophalangeal joint;computer science;hausdorff distance;tendon;energy minimization;image segmentation;speckle noise;segmentation	Vision	41.293305856935994	-77.11122349138527	8742
9dcb32944e2bc0ecefc2d9e4fe5bafcf105465f7	virtual screening: an overview on methods and applications		Virtual screening, or VS, is emerging as a valuable tool in discovering new candidate inhibitors for many biologically relevant targets including the many chemotherapeutic targets that play key roles in cell signaling pathways. However, despite the great advances made in the field thus far, VS is still in constant development with a relatively low success rate that needs to be improved by parallel experimental validation methods. This chapter reviews the recent advances in VS, focusing on the range and type of computational methods and their successful applications in drug discovery. The chapter also discusses both the advantages and limitations of the various techniques used in VS and outlines a number of future directions in which the field may progress. DOI: 10.4018/978-1-60960-491-2.ch002	cell signaling;computation;virtual screening	Khaled H. Barakat;Jonathan Y. Mane;Jack Adam Tuszynski	2011		10.4018/978-1-60960-491-2.ch002	biomedical engineering;virtual screening;computer science	ML	2.82795607304706	-67.50117911075445	8758
26f48278ce3eab1e1da757e53a2fdb7936e3e042	eeg-based motor imagery classification using enhanced active segment selection and adaptive classifier	brain computer interface;brain computer interface bci;kalman filter;linear discriminate analysis;adaptive classifier;event related brain potential;fractal dimension;right handed;wavelet transform;motor imagery;continuous wavelet transform;electroencephalogram eeg;electroencephalogram;active segment selection;motor imagery mi	In this study, an adaptive electroencephalogram (EEG) analysis system is proposed for a two-session, single-trial classification of motor imagery (MI) data. Applying event-related brain potential (ERP) data acquired from the sensorimotor cortices, the adaptive linear discriminant analysis (LDA) is used for classification of left- and right-hand MI data and for simultaneous and continuous update of its parameters. In addition to the original use of continuous wavelet transform (CWT) and Student's two-sample t-statistics, the 2D anisotropic Gaussian filter is proposed to further refine the selection of active segments. The multiresolution fractal features are then extracted from wavelet data by means of modified fractal dimension. The classification in session 2 is performed by adaptive LDA, which is trial-by-trial updated using the Kalman filter after the trial is classified. Compared with original active segment selection and non-adaptive LDA on six subjects from two data sets, the results indicate that the proposed method is helpful to realize adaptive BCI systems.	adaptive grammar;anterior descending branch of left coronary artery;classification;complex wavelet transform;continuous wavelet;erp;electroencephalography phase synchronization;extraction;fractal dimension;guided imagery;kalman filter;linear discriminant analysis;motor neuron disease;normal statistical distribution;sensorimotor cortex	Wei-Yen Hsu	2011	Computers in biology and medicine	10.1016/j.compbiomed.2011.05.014	kalman filter;brain–computer interface;computer vision;speech recognition;continuous wavelet transform;computer science;machine learning;pattern recognition;mathematics;fractal dimension;motor imagery;wavelet transform	ML	16.411742732002153	-91.5948464242806	8794
caaf70438962f898b6e9dbd5846d9ac13e449f86	developing the integrated discharge planning system for the high-risk premature infants			discharger	Li-Man Lin;Ming-Huei Lu;Wei-Chu Shin;Polun Chang	2012				Robotics	9.491376356716938	-80.06344584153359	8797
7cb6da7966367731212300ad11d3f8df6c03f41e	automatic image registration using virtual circles	virtual circles;image registration;hausdorff distance;control point matching	The main contribution of this work is a novel set of image features called the virtual circles and their use in the registration of images under similarity transformations. A virtual circle is a circle with maximal radius encompassing a background area that does not contain edge points. It has many useful properties such as its radius, and its dominant edge direction for example, which can be utilized for efficient registration. Furthermore, virtual circles are frequent and can be extracted efficiently with the help of the distance transform from many types of images. We have tested the new virtual circles method in the registration of 66 pairs of images, half of which are printed labels and the other half are indoor scenes. Experimental results have shown that this method has a linear complexity in terms of the number of pixels. It is also highly automatic, because it has a small number of parameters, which almost never need to be changed throughout the experiments.	image registration	Haikel Salem Alhichri;Mohamed S. Kamel	2004	Int. J. Image Graphics	10.1142/S0219467804001415	hausdorff distance;computer vision;topology;computer science;image registration;mathematics;geometry	Vision	49.29526524210249	-54.03375013535219	8799
15431f936a4be6bb754b8dd56252ade0fdb5c6fa	human behavior classification using thinning algorithm and support vector machine	human behavior;support vector machine;thinning algorithm		algorithm;support vector machine;thinning	Muhammad Rahmat Widyanto;Sukmawati Nur Endah;Kaoru Hirota	2010	JACIII	10.20965/jaciii.2010.p0028	support vector machine;computer science;machine learning;pattern recognition;relevance vector machine;human behavior;structured support vector machine	ML	29.68354005017283	-58.25642251855819	8823
a71ff54a5329fc39542205565ce72bd063e0b6c3	on the use of graphical models to study icu outcome prediction in septic patients treated with statins	part of book or chapter of book	Sepsis is a common pathology in Intensive Care Units (ICU). At its most acute phase, namely septic shock, mortality rates can be as high as 60%. Early administration of antibiotics is known to be crucial for ICU outcomes. In particular, statins, a class of drug usually associated to the regulation of the biosynthesis of cholesterol, have been shown to present good anti-inflammatory properties. In this brief study, we hypothesized that the pre-admission use of statins should improve ICU outcomes, reducing the mortality rate. We tested this hypothesis in a prospective study with patients admitted with severe sepsis and multi-organ failure at the ICU of Vall d’Hebron University Hospital (Barcelona, Spain). Outcome was predicted using statistic algebraic models and the Bayesian Networks that can naturally be derived from these models.	graphical model;international components for unicode;septic equation	Vicent J. Ribas;Jesús Caballero Lopez;Anna Sáez de Tejada;Juan Carlos Ruiz-Rodríguez;Adolfo Ruiz-Sanmartin;Jordi Rello;Alfredo Vellido	2011		10.1007/978-3-642-35686-5_9	computer science	ML	7.8755127616857825	-73.82677120891593	8884
25d409edb1b68d7c815d5f424ff15017ed487e94	evaluation and real-time monitoring of data quality in electrical impedance tomography	phantoms;electrodes tomography image reconstruction phantoms measurement ventilation real time systems;acute lung injury adult child electric impedance electrodes humans male phantoms imaging respiration artificial respiratory distress syndrome adult tomography;patient monitoring data quality electrical impedance tomography image quality;real time monitoring phantom measurements saline tank experiments electrode connection variability image reconstructeion ventilation conductivity distributions eit electrical impedance tomography data quality;electric impedance imaging;tomography biomedical electrodes electric impedance imaging image reconstruction medical image processing phantoms;image reconstruction;medical image processing;biomedical electrodes;tomography	Electrical impedance tomography (EIT) is a noninvasive method to image conductivity distributions within a body. One promising application of EIT is to monitor ventilation in patients as a real-time bedside tool. Thus, it is essential that an EIT system reliably provide meaningful information, or alert clinicians when this is impossible. Because the reconstructed images are very sensitive to system instabilities (primarily from electrode connection variability and movement), EIT systems should continuously monitor and, if possible, correct for such errors. Motivated by this requirement, we describe a novel approach to quantitatively measure EIT data quality. Our goals are to define the requirements of a data quality metric, develop a metric q which meets these requirements, and an efficient way to calculate it. The developed metric q was validated using data from saline tank experiments and a retrospective clinical study. Additionally, we show that q may be used to compare the performance of EIT systems using phantom measurements. Results suggest that the calculated metric reflects well the quality of reconstructed EIT images for both phantom and clinical data. The proposed measure can thus be used for real-time assessment of EIT data quality and, hence, to indicate the reliability of any derived physiological information.	characteristic impedance;data quality;dielectric spectroscopy;electromagnetically induced transparency;experiment;how true feel alert right now;imaging phantom;nominal impedance;patients;phantom reference;phantoms, imaging;real-time clock;requirement;respiration;salineos;spatial variability;tomography;tomography, emission-computed;electric impedance	Yasin Mamatjan;Bartlomiej Grychtol;Pascal Gaggero;Jorn Justiz;Volker M. Koch;Andy Adler	2013	IEEE Transactions on Medical Imaging	10.1109/TMI.2013.2269867	iterative reconstruction;computer vision;radiology;medicine;tomography;medical physics	Visualization	43.706458384875546	-86.77126624701536	8924
d62490d5a7b4af39bf2a9367966dfec1b9322f82	spatial models for fuzzy clustering	fuzzy c means algorithm;fuzzy c mean;image segmentation;markov random fields;iterative algorithm;magnetic resonance image;markov random field;objective function;fuzzy clustering;fuzzy c means;membership function;cross validation;spatial model;penalty function	A novel approach to fuzzy clustering for image segmentation is described. The fuzzy C-means objective function is generalized to include a spatial penalty on the membership functions. The penalty term leads to an iterative algorithm that is only slightly different from the original fuzzy C-means algorithm and allows the estimation of spatially smooth membership functions. To determine the strength of the penalty function, a criterion based on cross-validation is employed. The new algorithm is applied to simulated and real magnetic resonance images and is shown to be more robust to noise and other artifacts than competing approaches. c © 2001 Elsevier Science (USA)	algorithm;cluster analysis;cross-validation (statistics);fuzzy clustering;fuzzy cognitive map;image processing;image segmentation;iterative method;loss function;membership function (mathematics);optimization problem;penalty method;resonance	Dzung L. Pham	2001	Computer Vision and Image Understanding	10.1006/cviu.2001.0951	mathematical optimization;membership function;defuzzification;fuzzy clustering;flame clustering;fuzzy classification;computer science;fuzzy number;magnetic resonance imaging;machine learning;penalty method;pattern recognition;mathematics;iterative method;image segmentation;fuzzy associative matrix;cross-validation	Vision	53.59728673283416	-72.49770881581651	8945
0d90bbaf0ae131de3a2d792aca429ccbae958a4c	derivation of cancer related biomarkers from dna methylation data from an epidemiological cohort		DNA methylation profiling methods exploit microarray technologies and provide a wealth of high-volume data. This data solicits generic, analytical pipelines for the meaningful systems-level analysis and interpretation. In the current study, an intelligent framework is applied, encompassing epidemiological.DNA methylation data produced from the Illumina’s Infinium Human Methylation 450K Bead Chip platform, in an effort to correlate interesting methylation patterns with cancer predisposition and in particular breast cancer and B-cell lymphoma. Specifically, feature selection and classification are exploited in order to select the most reliable predictive cancer biomarkers, and assess their classification power for discriminating healthy versus cancer related classes. The selected features, which could represent predictive biomarkers for the two cancer types, attained high classification accuracies when imported to a series of classifiers. The results support the expediency of the methodology regarding its application in epidemiological studies.		Ioannis Valavanis;Emmanouil G. Sifakis;Panagiotis Georgiadis;Sotirios Kyrtopoulos;Aristotelis A. Chatziioannou	2013		10.1007/978-3-642-41016-1_27	bioinformatics	Logic	7.239567574489446	-54.775718571415474	8960
dec0f1e1eca2d91462c0eba950e48e24c4a3679e	health record tracking enhancement based on multimedia and machine learning for mobile healthcare: trends and challenges		As mobile and wearable devices become widespread, sensors and media input methods are used to track daily health records. Previously, the step counts, workouts, and sleep efficiencies are measured through accelerometer or GPS sensor in the device. It has been gradually expanded to other health information monitoring using various media input peripherals. For example, camera-based image recognition and voice input can be used as easy tracking methods. And optical sensors can continuously measure the various biometric information, such as heart rate, oxygen saturation, stress level, and even blood pressure. Using these various information, the mobile healthcare services can encourage users to engage in healthy activities or provide personalized recommendation. In this talk, I will introduce the related market trends and some challenges.	biometrics;computer vision;global positioning system;input method;machine learning;peripheral;personalization;sensor;wearable technology	Seongho Cho	2018		10.1145/3264996.3265006	medical record;lifelog;direct voice input;biometrics;accelerometer;global positioning system;multimedia;wearable technology;health care;computer science	HCI	5.485195401205423	-87.3308175938239	8964
cc2093a1ad8d5f35da0fece018fa248a5d1eb8f2	neural indexes of attention extracted from eeg correlate with elderly reaction time in response to an attentional task		In the present paper, we analyze electroencephalogram (EEG) signals recorded by a single frontal channel from 105 elderly subjects while they were responding to an attention-demanded task (Stroop color test). The first objective is to discover how post-cue frequency band oscillations of EEG, as neural index of attention, are correlated with elderly response time (RT), as behavioral index of attention. Furthermore, we aim to detect the most informative period of brain activity (EEG) in which the strongest correlations with reaction time exist. Our results show that 1) there is significant negative correlation between alpha gamma ratio (AGR) and response time (p<0.0001), 2) theta beta ratio (TBR) is positively correlated with subjects' response time (p<0.0001) and 3) these correlations are stronger in a 500ms period right after triggering the cue (question onset in Stroop test). Our study provides an insight into the research on analysis and prediction of subject behavior from EEG. Moreover, it has potential to be used in implementation of feasible and efficient single channel EEG-based brain computer interface (BCI) training systems for elderly.	adaptive gabor representation;brain-computer interfaces;brain–computer interface;electroencephalography phase synchronization;extraction;frequency band;index;information;interface device component;onset (audio);response time (technology)	Fatemeh Fahimi;Parth Singhal;Tih-Shih Lee;Cuntai Guan	2018		10.1145/3265689.3265722	brain activity and meditation;brain–computer interface;stroop effect;audiology;frequency band;response time;correlation;electroencephalography;alpha (ethology);psychology	ML	12.143817234775288	-91.69203029405433	8965
c7a0baa90dbd60911974a4ea85caab15c6bdb8eb	cholinergic modulation of ca1 pyramidal cells via m1 muscarinic receptor activation: a computational study at physiological and supraphysiological levels		The hippocampus receives extensive cholinergic modulation from the basal forebrain, which has been shown to have a prominent role in attention, learning, and synaptic plasticity. Disruptions of this modulation have been linked to a variety of neural disorders including Alzheimer’s Disease. Pyramidal cells of the CA1 region of the hippocampus express several cholinergic receptor types in different locations throughout the cells’ morphology. Developing a computational model of these cells and their modulation provides a unique opportunity to explore how each receptor type alters the overall computational role of the cell. To this end we implemented a kinetic model of the most widely distributed receptor type, the M1 muscarinic receptor and examined its role on excitation of a compartmental model of a CA1 pyramidal cell. We demonstrate that the proposed model replicates the increased pyramidal cell excitability seen in experimental results. We then used the model to replicate the effect of organophosphates, a class of pesticides and chemical weapons, whose effects consist in inhibiting the hydrolysis of acetylcholine; we demonstrated the effect of increasing concentrations of acetylcholine on the pyramidal cell’s excitability. The cell model we implemented and its associated modulation constitute a basis for exploring the effects of cholinergic modulation in a large scale network model of the hippocampus both under physiological and supraphysiological levels.	alzheimer's disease;basal (phylogenetics);basal forebrain;ca1 field;cholinergic receptors;computation (action);computational model;delta modulation;excitation;excited state;inhibition;mathematical morphology;multi-compartment model;muscarinic acetylcholine receptor;network model;neuronal plasticity;pesticides;phosphoric acid esters;prosencephalon;pyramidal cells;self-replicating machine;synaptic package manager;weapons	Adam Mergenthal;Jean-Marie Bouteiller;Theodore W. Berger	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8512574	muscarinic acetylcholine receptor;acetylcholine;synaptic plasticity;electronic engineering;pyramidal cell;cholinergic;cell biology;hippocampus;basal forebrain;computer science;receptor	ML	18.233113128818424	-71.91498498067206	8994
bc08c1a99fe79ed5c43559989f352db6d336b498	analysis of evolving processes in pulmonary nodules using a sequence of three-dimensional thoracic images	rigid registration	This paper presents a method to analyze volume evolutions of pulmonary nodules for discrimination between malignant and benign nodules. Our method consists of four steps; The 3D rigid registration of the two successive 3D thoracic CT images, the 3D affine registration of the two successive region-of-interest (ROI) images, non rigid registration between local volumetric ROIs, and analysis of the local displacement field between successive temporal images. In preliminary study, the method was applied to the successive 3D thoracic images of two pulmonary lesions including a metastasis malignant case and an inflammatory benign to quantify the evolving process in the pulmonary nodules and surrounding structure. The time intervals between successive 3D thoracic images for the benign and malignant cases were 120 and 30 days, respectively. From the display of the displacement fields and the contrasted image by the vector field operator based on the Jacobian, it was observed that the benign case reduced in the volume and the surrounding structure was involved into the nodule in the evolution process. It was also observed that the malignant case expanded in the volume. These experimental results indicate that our method is a promising tool to quantify how the lesions evolve their volume and surrounding structures.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Yoshiki Kawata;Noboru Niki;Hironobu Ohmatsu;Masahiko Kusumoto;Ryutaro Kakinuma;Kiyoshi Mori;Hiroyuki Nishiyama;Kenji Eguchi;Masahiro Kaneko;Noriyuki Moriyama	2001		10.1117/12.431081	computer vision;pathology;geography;surgery	ML	38.9281552622315	-78.34346019144108	9038
2c195cbfd097de346a0f861a3eede8654049f0c4	using brca1 to treat cancer.	software;computer graphics;web service;large scale;internet;rna;nucleic acid conformation;models statistical;algorithms;source code;c programming language;rna secondary structure prediction;user computer interface;computational biology;computer simulation;free energy;sliding window;programming languages	RNA pseudoknots are an important structural feature of RNAs, but often neglected in computer predictions for reasons of efficiency. Here, we present the pknotsRG Web Server for single sequence RNA secondary structure prediction including pseudoknots. pknotsRG employs the newest Turner energy rules for finding the structure of minimal free energy. The algorithm has been improved in several ways recently. First, it has been reimplemented in the C programming language, resulting in a 60-fold increase in speed. Second, all suboptimal foldings up to a user-defined threshold can be enumerated. For large scale analysis, a fast sliding window mode is available. Further improvements of the Web Server are a new output visualization using the PseudoViewer Web Service or RNAmovies for a movie like animation of several suboptimal foldings. The tool is available as source code, binary executable, online tool or as Web Service. The latter alternative allows for an easy integration into bioinformatics pipelines. pknotsRG is available at the Bielefeld Bioinformatics Server (http://bibiserv. techfak.uni-bielefeld.de/pknotsrg).	algorithm;brca1 gene;bio-informatics;bioinformatics;british informatics olympiad;executable;imagery;informatics (discipline);pipeline (computing);protein folding, globular;protein structure prediction;rna;rule (guideline);server (computer);source code;the c programming language;web server;web service;window function;world wide web;free energy	Jens Reeder;Peter Steffen;Robert Giegerich	1996	Environmental Health Perspectives	10.1093/nar/gkm258	computer simulation;web service;sliding window protocol;the internet;rna;bioinformatics;computer graphics;source code	Comp.	-0.841801533237912	-58.86231461864007	9040
0c704594d35f44d3de205f1d9776ab5637afdebe	reconstruction of ancient operons from complete microbial genome sequences	dna;genetic engineering;complete genome;protein sequence;evolution biological;statistical method;genetics;proof of concept;proteins;statistical analysis;genetics dna proteins evolution biological statistical analysis cellular biophysics genetic algorithms genetic engineering;statistical method ancient operons microbial genome sequence dna sequence evolutionary information orthologous genes protein sequence cluster of orthologous group ncbi cog database random cost algorithm global optimization algorithm;genetic algorithms;global optimization;dna sequence;cellular biophysics;genomics bioinformatics assembly evolution biology dna databases protein sequence genetic mutations frequency cost function;genome sequence	Completed genomes not only provide DNA sequence information, but also reveal the relative locations of genes. In this paper, we propose a new method for reconstruction of “ancient operons” by taking advantages of the evolutionary information in both orthologous genes and their locations in a genome. The basic assumption is that the closer two genes were in an ancient genome, the more likely they will stay close in the current genome. An assembly of non-random neighboring pairs of genes in current genomes should be able to reconstruct the gene groups that were together at a certain point of time during evolution. Given the fact that genes that are close neighbors are more likely functionally related, the gene groups generated by this assembly process are named “ancient operons”. The assembly is only meaningful when enough non-random pairs can be found. This was made possible by over 100 microbial genomes available in recent years. For proof of concept, we chose 63 non-redundant complete microbial genomes from RefSeq database[May 2003 release] at NCBI. In order to normalize the effect of protein sequence mutations and other changes due to evolution, we only consider assembly of COGs (Cluster of Orthologous Group) in these genomes. There are total 4901 COGs from NCBI COG database are used. The assembly process is similar to the one that assembles DNA sequences into contigs. In our case, the neighbor COG pairs are used as basic assembly units. A target function is defined based on neighbor frequency of pair-wise link among all 4901 COGs after analysis for all 63 genomes. We used random cost algorithm, a global optiomization algorithm to minimize the target function and assembled COGs into contigs. The significance of these contigs are then assessed by statistical methods. The results suggest that the assembled contigs are statistically and biologically significant. This method and the assembled ancient operons provides a new way for studying microbial genomes, their evolution and for annotating proteins of unknown functions. Proceedings of the Computational Systems Bioinformatics (CSB’03) 0-7695-2000-6/03 $17.00 © 2003 IEEE	algorithm;bioinformatics;computation;homology (biology);randomness;refseq;sequence homology	Yuhong Wang;John P. Rose;Bi-Cheng Wang;Dawei Lin	2003		10.1109/CSB.2003.1227383	genetic engineering;biology;dna sequencing;whole genome sequencing;genetic algorithm;bioinformatics;protein sequencing;proof of concept;genetics;dna;global optimization	Comp.	1.7040759079596002	-53.185706854407684	9048
1c0b9bb4dbe0a92fd9b6637b9ea620cd441f8073	imputation-based assessment of next generation rare exome variant arrays	genotype;algorithms;computational biology;genome wide association study;individualized medicine;sample size;exome;genetic variation	A striking finding from recent large-scale sequencing efforts is that the vast majority of variants in the human genome are rare and found within single populations or lineages. These observations hold important implications for the design of the next round of disease variant discovery efforts-if genetic variants that influence disease risk follow the same trend, then we expect to see population-specific disease associations that require large sample sizes for detection. To address this challenge, and due to the still prohibitive cost of sequencing large cohorts, researchers have developed a new generation of low-cost genotyping arrays that assay rare variation previously identified from large exome sequencing studies. Genotyping approaches rely not only on directly observing variants, but also on phasing and imputation methods that use publicly available reference panels to infer unobserved variants in a study cohort. Rare variant exome arrays are intentionally enriched for variants likely to be disease causing, and here we assay the ability of the first commercially available rare exome variant array (the Illumina Infinium HumanExome BeadChip) to also tag other potentially damaging variants not molecularly assayed. Using full sequence data from chromosome 22 from the phase I 1000 Genomes Project, we evaluate three methods for imputation (BEAGLE, MaCH-Admix, and SHAPEIT2/IMPUTE2) with the rare exome variant array under varied study panel sizes, reference panel sizes, and LD structures via population differences. We find that imputation is more accurate across both the genome and exome for common variant arrays than the next generation array for all allele frequencies, including rare alleles. We also find that imputation is the least accurate in African populations, and accuracy is substantially improved for rare variants when the same population is included in the reference panel. Depending on the goals of GWAS researchers, our results will aid budget decisions by helping determine whether money is best spent sequencing the genomes of smaller sample sizes, genotyping larger sample sizes with rare and/or common variant arrays and imputing SNPs, or some combination of the two.	alleles;biopolymer sequencing;chromosomes, human, pair 22;gene frequency;genome;genome-wide association study;genotype determination;geo-imputation;inference;ki-1+ anaplastic large cell lymphoma;köppen climate classification;mental association;money;next-generation network;one thousand;population;randomness;rare events;reference architecture;snp array;sample size;single nucleotide polymorphism;small;statistical imputation;tracer;whole exome sequencing;beagle	Alicia R. Martin;Gerard Tse;Carlos Daniel Bustamante;Eimear E. Kenny	2014	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		genome-wide association study;sample size determination;biology;molecular biology;personalized medicine;exome sequencing;bioinformatics;genetic variation;genotype;exome;genetics	Comp.	3.06787684913101	-53.296736974797405	9059
6f676c78b41bcac4899ef2d39c1bd086103b4969	body position classification for cardiorespiratory measurement	gyroscopes;magnetic sensors;torso;magnetometers;decision trees;accelerometers	Heart activity, or at least heart rate variability, is associated with body position. Our previous studies have confirmed that impedance pneumography may be used to record respiratory function, but the calibration coefficients for this method depend on position. Data were collected from 24 students (12 male, 12 female), who alternated positions between lying (on front, back, and right side), sitting and standing. Signals from an attached iPhone's internal sensors (accelerometer, gyroscope, magnetometer) were recorded and attitude relative to gravity was calculated. The signals were subsequently segmented and marked. Five algorithms were trained and cross-validated for different sensor combinations. Without differentiation of sitting and standing, 100% accuracy was achieved using all algorithms. The classifier best differentiating these two states was based on random forests, with overall accuracy of 90%. Simple methods based on a proposed hybrid classifier were tested for online measurement without the need for signal segmentation, with 99% accuracy. The prospect of the algorithms' use in long-term studies (particularly cardiorespiratory monitoring) was assessed.	ability to sit question;body dysmorphic disorders;body position;cardiography, impedance;cell hybridization;characteristic impedance;coefficient;cross-correlation;forests;gyroscope;heart rate variability;lewy body disease;ninety nine;output impedance;quantitative impedance;random forest;respiration;segmentation action;smartphone;telling untruths;accelerometers;algorithm;biologic segmentation;magnetometers;sensor (device)	Marcel Mlynczak;Martin Berka;Wiktor Niewiadomski;Gerard Cybulski	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591486	electronic engineering;magnetometer;simulation;gyroscope;torso;computer science;engineering;electrical engineering;decision tree;accelerometer;physics;quantum mechanics	Visualization	13.851248597308308	-85.80965533822229	9079
4b446145066c6f3088f87bf78e79ed7c27ac20fb	data mining and the functional relationship between heart rate variability and emotional processing - comparative analyses, validation and application	heart rate variability;data mining	Aims of the study are to 1-classify emotional responses in healthy and conscious brain injured subjects by Data Mining analysis of subjective reports and Heart Rate Variability (HRV), 2-compare different procedures for reliability, and 3-test applicability in patients with disordered consciousness (vegetative state). We measured HRV of 26 healthy and 16 posttraumatic subjects listening music samples selected by emotions they evoke. Each subject was interviewed and the reported emotions were used for identifing a model assessing the most probable emotion by the HRV parameters. Two macro-categories were defined: positive and negative emotions. The study matched a three-phases strategy. First, we applied several classification approaches to healthy subjects evaluating them through suitable validation techniques. Secondly, the best performing classifiers were used to forecast emotions of posttraumatic patients, without retraining. In the 3rd phase we used the most reliable decision model both for validation (1st phase) and independent test (2nd phase) in order to classify the “emotional” response of 9 subjects in vegetative state. One HRV parameter (normalized Low-Frequency Band Power) proved sufficient to forecast a reliable classification. Accuracy was greater than 70% on training, validation and test. Model represents an objective criterion to investigate possible emotional responses also in unconscious patients.	data mining;data validation;frequency band;heart rate variability;persistent vegetative state	Francesco Riganello;Antonio Candelieri	2010			heart rate variability;computer science;bioinformatics;data science;data mining	ML	10.207826920701036	-91.19717867823617	9095
67d9b263f3869c675ddfdb42d35260a7ce0357a0	a model-based deconvolution approach to solve fiber crossing in diffusion-weighted mr imaging	richardson lucy algorithm;diffusion weighted mr imaging;fiber tracking;multicompartment model;noisy data;dti;deconvolution magnetic resonance imaging signal to noise ratio signal processing character generation signal generators convolution brain modeling in vivo signal resolution;medical image processing biomedical mri deconvolution;magnetic resonance image;impulse response function;algorithms artificial intelligence brain cluster analysis computer simulation diffusion magnetic resonance imaging humans image enhancement image interpretation computer assisted imaging three dimensional models anatomic models neurological nerve fibers pattern recognition automated reproducibility of results sensitivity and specificity;medical image processing;brain structure;deconvolution;hardi;dw mri;signal to noise ratio;fiber crossing;spherical deconvolution;spherical deconvolution dti dw mri fiber crossing hardi multicompartment model richardson lucy algorithm;biomedical mri;fiber connectivity model based deconvolution approach fiber crossing diffusion weighted mr imaging magnetic resonance imaging multicompartment model richardson lucy algorithm impulse response function complex brain structures fiber tracking	A deconvolution approach is presented to solve fiber crossing in diffusion magnetic resonance imaging. In order to provide a direct physical interpretation of the signal generation process, we started from the classical multicompartment model and rewrote this in terms of a convolution process, identifying a significant scalar parameter alpha to characterize the physical system response. Deconvolution is performed by a modified version of the Richardson-Lucy algorithm. Simulations show the ability of this method to correctly separate fiber crossing, even in the presence of noisy data, with lower signal-to-noise ratio, and imprecision in the impulse response function imposed during deconvolution. The in vivo data confirms the efficacy of this method to resolve fiber crossing in real complex brain structures. These results suggest the usefulness of our approach in fiber tracking or connectivity studies	algorithm;angularjs;brain;computer simulation;convolution;diffusion magnetic resonance imaging;frequency response;optical fiber;population parameter;richardson number;richardson–lucy deconvolution;signal-to-noise ratio;tissue fiber;video-in video-out	Flavio Dell'Acqua;Giovanna Rizzo;Paola Scifo;Rafael Alonso Clarke;Giuseppe Scotti;Ferruccio Fazio	2007	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2006.888830	computer vision;speech recognition;impulse response;computer science;deconvolution;magnetic resonance imaging;mathematics;blind deconvolution;nuclear magnetic resonance;signal-to-noise ratio	Visualization	47.980922335751686	-82.38921968793878	9098
fd1038d326fe7278e536fb266bdb87b5a7f8bf98	editorial: neural and computational modeling of movement control	computational modelling;neural circuits;movement and posture;neurorehabilitation;sensorimotor control	There exists a gap from experimental data to the understanding of neural control of movements. This research topic was dedicated to promote computational modeling approach that can facilitate data interpretation (Niu et al.; Ranjbaran and Galiana; Pearson et al.; Sharif Razavian et al.; Malik et al.), elucidate control theories (Ueyama; Ota et al.; Takemura et al.), shed light on systemic mechanisms (Buhrmann and DiPaolo; Pearson et al.; Li et al.), suggest testable hypothesis (Loeb and Tsianos; Jiang et al.), and aid design of rehabilitation or therapeutic strategies (Zitella et al.). The 14 articles reflected these different aspects of computational modeling in bridging this gap between functions of neural circuits and observable behaviors. This research topic demonstrated that computational modeling is playing a more and more prominent role in sensorimotor control studies.	behavior;bridging (networking);computation;computer simulation;control theory;lithium;movement;observable;neural circuits;ochratoxin a	Ning Lan;Vincent C. K. Cheung;Simon C. Gandevia	2016		10.3389/fncom.2016.00090	psychology;neurorehabilitation;biological neural network;neuroscience;simulation;computer science;artificial intelligence;machine learning;communication	NLP	15.809694102133388	-70.70031466927625	9113
0a9ce1450a8cff48fa54a1bd9044f2e52ccfdbb4	a novel radio propagation radiation model for location of the capsule in gi tract	signal strength;local algorithm;antenna radiation patterns;radio propagation gastrointestinal tract biological system modeling antennas and propagation electromagnetic propagation humans azimuth finite difference methods time domain analysis electromagnetic modeling;dipole antennas;biological organs;biological system modeling;finite difference time domain;electromagnetic analysis;prosthetics;human body model antenna orientation radiation pattern localization algorithm received signal strength indicator radio propagation radiation model human gastro intestine tract signal propagation empirical model numerical electromagnetic analysis finite difference time domain analysis dipole antenna azimuth radiation attenuation overlooked antenna orientation signal strength sensitivity compensated model gi tract implantable sources;attenuation;radiation pattern;received signal strength indicator;biomedical engineering;radiowave propagation antenna radiation patterns biological organs biomedical engineering dipole antennas finite difference time domain analysis physiological models prosthetics;finite difference time domain analysis;human body;path loss;empirical model;radiowave propagation;receiving antennas;transmitting antennas;numerical models;physiological models;radio propagation	In this paper, we discuss the influence of the antenna orientation radiation pattern in localization algorithm based on Received Signal Strength Indicator (RSSI). We also improve the empirical model of signal propagation by building the path loss function of the human gastro-intestine (GI) tract. The novel model includes information of both the distance and azimuth angle variables. The numerical electromagnetic analysis with the finite-difference time-domain (FDTD) is applied to model the vivo radio propagation channels by using a dipole antenna suitable for the model related to the human body. The proposed propagation model is compared with empirical model, and the simulation results show that the compensated model is more accurate by calculating the azimuth radiation attenuation. It demonstrates that the often overlooked antenna orientation has the dominant effect on the signal strength sensitivity.	algorithm;dipole antenna;finite-difference time-domain method;galaxy morphological classification;loss function;numerical analysis;poor posture;radiation pattern;simulation;software propagation;tract (literature);video-in video-out	Lujia Wang;Chao Hu;Longqiang Tian;Mao Li;Max Q.-H. Meng	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420456	attenuation;signal strength;finite-difference time-domain method;electronic engineering;human body;telecommunications;computer science;engineering;path loss;two-ray ground-reflection model;radio propagation;radio propagation model;radiation pattern;optics;log-distance path loss model;dipole antenna;empirical modelling	Robotics	49.1824298131534	-87.96331170690627	9118
b8187bc697b3b1a62852b49884fae3677c862b46	ekf monocular slam with relocalization for laparoscopic sequences	cavity resonators;image motion analysis;computer model;real time;kalman filters;three dimensional;medical robotics;camera motion;computational modeling;3d model;medical image;three dimensional displays;medical image processing;endoscopes;image sequence;simultaneous localization and mapping;surgery;robust performance;experimental validation;augmented reality;real medical image sequences ekf monocular slam laparoscopic sequences visual slam 3d model 3d camera motion medical endoscopic sequences augmented reality 3d information medical robots 1 point ransac randomised list relocalization abdominal cavity images tissue deformation surgical tools;slam robots;solid modelling augmented reality cameras endoscopes image motion analysis image sequences kalman filters medical image processing medical robotics slam robots;cameras simultaneous localization and mapping three dimensional displays laparoscopes cavity resonators surgery computational modeling;cameras;solid modelling;laparoscopes;image sequences	In recent years, research on visual SLAM has produced robust algorithms providing, in real time at 30 Hz, both the 3D model of the observed rigid scene and the 3D camera motion using as only input the gathered image sequence. These algorithms have been extensively validated in rigid human-made environments -indoor and outdoor- showing robust performance in dealing with clutter, occlusions or sudden motions. Medical endoscopic sequences naturally pose a monocular SLAM problem: an unknown camera motion in an unknown environment. The corresponding map would be useful in providing 3D information to assist surgeons, to support augmented reality insertions or to be exploited by medical robots. In this paper we propose the combination EKF Monocular SLAM + 1-Point RANSAC + Randomised List Relocalization to process laparoscopic sequences -abdominal cavity images-. The sequences are challenging due to: 1) cluttering produced by tools; 2) sudden motions of the camera; 3) laparoscope frequently goes in and out of abdominal cavity; 4) tissue deformation caused by respiration, heartbeats and/or surgical tools. Real medical image sequences provide experimental validation.	algorithm;augmented reality;clutter;extended kalman filter;medical robot;polygonal modeling;random sample consensus;simultaneous localization and mapping	Oscar G. Grasa;Javier Civera;J. M. M. Montiel	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980059	kalman filter;three-dimensional space;computer vision;augmented reality;simulation;computer science;engineering;computational model;computer graphics (images);simultaneous localization and mapping	Robotics	42.496104765120975	-85.7869100424073	9127
d501090215db09aef1773562bacbcf2d911af56d	edge detection using a new definition of entropy	histograms;clustering entropy image analysis uniformity predicate image histogram gaussian distribution grey level image problem solving evaluation function quadtree optimal path image segmentation chebyshev distance edge detection;evaluation function;maximum entropy methods;image segmentation;uniformity predicate;edge detection;biomedical imaging;computer vision;smoothing methods;optimal path;shape;image edge detection;clustering;image analysis;chebyshev distance;humans;image edge detection entropy image segmentation biomedical imaging humans histograms frequency chebyshev approximation shape smoothing methods;entropy;chebyshev approximation;frequency;quadtree;grey level image;image histogram;human perception;quadtrees;gaussian distribution;control strategy;problem solving	The paper describes a possible model of the human perceptive process. In this paper the relation between the entropy of an image domain and the entropy of its subdomains is explored as a uniformity predicate. Such entropy is obtained from the analysis of the image histogram associating a Gaussian distribution to the maximum frequency of grey levels. With the aim of implementing the model, we have introduced a well known technique of problem solving. The most important roles of our model are played by the evaluation function (EF) and the control strategy. So the EF is related to the ratio between the entropy of one region or zone of the picture and the entropy of the entire picture. The control strategy determines the optimal path in the quadtree so that the nodes of the optimal path have minimal entropy. The paper shows some comparisons between the method and classical edge detection techniques.	edge detection;entropy (information theory)	Sergio Vitulano;Michele Nappi;Domenico Vitulano;C. Masuovito	1996		10.1109/ICPR.1996.546740	normal distribution;entropy power inequality;computer vision;entropy;mathematical optimization;joint entropy;image analysis;edge detection;binary entropy function;transfer entropy;maximum entropy probability distribution;shape;computer science;principle of maximum entropy;chebyshev distance;frequency;quadtree;evaluation function;pattern recognition;histogram;mathematics;image segmentation;joint quantum entropy;differential entropy;cluster analysis;cross entropy;perception;maximum entropy spectral estimation;conditional entropy;image histogram;sample entropy;approximation theory	Vision	43.55666527691364	-67.32916523229406	9132
e2826ee8e458315f85850a0545141d3c4166c73a	early experiences in copd exacerbation detection	biomedical monitoring;telemonitoring chronic obstructive pulmonary disease aged;chronic obstructive pulmonary disease;pneumodynamics cardiology diseases mobile computing mobile handsets patient treatment;accuracy;sensitivity;weight measurement;diseases sensitivity accuracy monitoring biomedical monitoring weight measurement;monitoring;diseases;independent data set copd exacerbation detection chronic obstructive pulmonary disease slow progressive disease airway obstruction respiratory health deteriorations patient treatment hospitalization risk pulse oximeter mobile phone heart rate oxygen saturation elderly patient cohort worrisome events validation based evaluation;telemonitoring;aged	Chronic obstructive pulmonary disease (COPD) is a slowly progressive disease characterized by airway obstruction. Patients suffering from COPD could report exacerbations, which are deteriorations in respiratory health that worsen the course of the disease. The prompt recognition of an exacerbation from daily variations and its early treatment reduces the healing time and the risk of hospitalization. Using a pulse oximeter connected to a mobile phone, we remotely collect the values of heart rate and of the oxygen saturation on a cohort of seven elderly patients affected by severe COPD. We analyze if a score given by the weighted composition of these signals permits to detect the worrisome events prefiguring an exacerbation onset. A cross validation-based evaluation allows us to assess how the results generalize to an independent data set. We found that the tested score does not provide satisfactory results in terms of sensitivity and specificity, suggesting that it is not able to disambiguate between exacerbation onset and other COPD related events.	cross-validation (statistics);disease ontology;mobile phone;onset (audio);robertson–seymour theorem;sensitivity and specificity;sensor;thresholding (image processing)	Mario Merone;Leonardo Onofri;Paolo Soda;Claudio Pedone;Raffaele Antonelli Incalzi;Giulio Iannello	2014	2014 IEEE 27th International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2014.46	intensive care medicine;medicine;sensitivity;physical therapy;accuracy and precision;medical emergency;statistics	Metrics	13.723492462190524	-86.94740922634189	9136
2e899c20aad2c906feb742d5eb6091004c1fe62d	representation analysis and synthesis of lip images using dimensionality reduction	locally linear embedding;image sequence processing;speech synthesis;dimension reduction;motion synthesis;automatic generation;automatic lipreading;inverse problem;image representation;image sequence;multidimensional scaling;image analysis;facial expression;dimensional reduction;local linear embedding;visual speech recognition	Understanding facial expressions in image sequences is an easy task for humans. Some of us are capable of lipreading by interpreting the motion of the mouth. Automatic lipreading by a computer is a challenging task, with so far limited success. The inverse problem of synthesizing real looking lip movements is also highly non-trivial. Today, the technology to automatically generate an image series that imitates natural postures is far from perfect. We introduce a new framework for facial image representation, analysis and synthesis, in which we focus just on the lower half of the face, specifically the mouth. It includes interpretation and classification of facial expressions and visual speech recognition, as well as a synthesis procedure of facial expressions that yields natural looking mouth movements. Our image analysis and synthesis processes are based on a parametrization of the mouth configuration set of images. These images are represented as points on a two-dimensional flat manifold that enables us to efficiently define the pronunciation of each word and thereby analyze or synthesize the motion of the lips. We present some examples of automatic lips motion synthesis and lipreading, and propose a generalization of our solution to the problem of lipreading different subjects.	dimensionality reduction;image analysis;speech recognition	Michal Aharon;Ron Kimmel	2006	International Journal of Computer Vision	10.1007/s11263-006-5166-3	computer vision;image analysis;speech recognition;multidimensional scaling;computer science;inverse problem;machine learning;mathematics;speech synthesis;facial expression;dimensionality reduction	Vision	23.540536878140053	-61.00410673074956	9143
36bc0c230110343421a4bd842a9e9aae0db1e674	homonuclear broad-band-decoupled chemical shift imaging by singular value decomposition with optimization	homonuclear broad band decoupled chemical shift imaging;simulation ordinateur;nuclear magnetic resonance imaging;phase encoding;spectroscopy;deplacement chimique;chemicals;distribucion espacial;optimisation;spectrometrie rmn;decomposition valeur singuliere;chemicals singular value decomposition spatial resolution spectroscopy image resolution nuclear magnetic resonance encoding in vivo time domain analysis magnetic fields;magnetic fields;magnetic field;localized nmr spectroscopy;image resolution;chemical shift segregation;resolution spatiale;optimizacion;resolucion espacial;etude theorique;time domain data;genie biomedical;singular value decomposition;biomedical nmr;incremental t 1 period;chemical shift imaging;nuclear magnetic resonance;desplazamiento quimico;incremental magnetic field gradients;chemical shift;nmr spectroscopy;t 1 encoding parameters;time domain analysis;imageria rmn;repartition spatiale;simulated data;biomedical engineering;desacoplamiento homonuclear;t sub 1 encoding parameters homonuclear broad band decoupled chemical shift imaging simulated data medical diagnostic imaging singular value decomposition optimization localized nmr spectroscopy time domain data spatial information phase encoding incremental magnetic field gradients spatially well resolved images chemical shift segregation incremental t sub 1 period real phantom data;spatial distribution;nmr spectrometry;estudio teorico;tecnica;time domain;ingenieria biomedica;imagerie rmn;optimization;decomposicion valor singular;simulacion computadora;chemical shift biomedical nmr;theoretical study;espectrometria rmn;real phantom data;encoding;computer simulation;decouplage homonucleaire;in vivo;technique;homonuclear decoupling;spatial information;medical diagnostic imaging;spatially well resolved images;spatial resolution	The conventional viewpoint of localized NMR spectroscopy is to acquire spectral information through time-domain data, while leaving spatial information to phase encoding by incremental magnetic field gradients. A second viewpoint, much less frequently used, places the emphasis on the acquisition of spatially well-resolved images by conventional means, leaving the chemical shift segregation through phase encoding in the incremental t(1) period (in a 2-DNMR parlance). The feasibility and practicality of the second viewpoint are demonstrated by implementation of a modified version of the SLIM technique, which was originally designed for the first viewpoint, using simulated and real phantom data with optimization of the t(1)-encoding parameters.	departure - action;gradient;imaging phantom;increment;magnetic fields;mathematical optimization;phantoms, imaging;published comment;singular value decomposition;spectroscopy, nuclear magnetic resonance	Ning Li;Hong N. Yeung	1993	IEEE transactions on medical imaging	10.1109/42.232265	computer simulation;image resolution;magnetic field;spectroscopy;computer science;nuclear magnetic resonance	Visualization	52.97674636878073	-78.92602892867944	9146
52a8a3354a6c1a724da60113418dfe8731238bb2	detection of oriented repetitive alternating patterns in color images (a computational model of monkey grating cells)	areas v1;monkey primary visual cortex;new type;grating cell operator;oriented repetitive alternating patterns;computational model;grating cell;appropriate orientation;specific orientation;monkey grating cells;color images;new operator;grating pattern;color image;computer model	In 1992 neurophysiologists [20] found a new type of cells in areas V1 and V2 of the monkey primary visual cortex, which they called grating cells. These cells respond vigorously to a grating pattern of appropriate orientation and periodicity. Three years later a computational model inspired by these findings was published [9]. The study of this paper is to create a grating cell operator that has similar response profiles as monkey grating cells have. Three different databases containing a total of 338 real world images of textures are applied to the new operator to get better a insight to which natural patterns grating cells respond. Based on these images, our findings are that grating cells respond best to repetitive alternating patterns of a specific orientation. These patterns are in common human made structures, like buildings, fabrics, and tiles.	computation;computational model;database;nicolai petkov;patterns in nature;quasiperiodicity;texture mapping	Tino Lourens;Hiroshi G. Okuno;Hiroaki Kitano	2001		10.1007/3-540-45720-8_12	computer simulation;image texture;computer vision;color image;computer science;computer graphics (images)	ML	22.075411698204856	-67.21779927972916	9160
e68c70fdcda8e33b6ef0de37557d520c7535b7ec	a million-plus neuron model of the hippocampal dentate gyrus: dependency of spatio-temporal network dynamics on topography	spatiotemporal phenomena bioelectric potentials biomembrane transport brain feedback feedforward medical signal processing neurophysiology physiological models;animals;models neurological;spatiotemporal network dynamics granule cell activity rhythmic burst firing intrinsic membrane properties spiking poisson random firing lateral entorhinal cortical neurons medial entorhinal cortical neurons axonal transport studies gabaergic gabaa like ipsp inhibitory inputs gabaergic gabaa like ipsp synaptic excitatory inputs glutamatergic ampa like epsp biophysical properties morphological properties feedback inhibitory input feedforward inhibitory input rat hippocampal dentate gyrus million plus granule cell compartmental model topography;rats;entorhinal cortex;hippocampus;synaptic transmission;nerve net;neurons;dendrites;space time clustering;firing organizations neurons biological system modeling brain modeling correlation surfaces;computer simulation;alpha amino 3 hydroxy 5 methyl 4 isoxazolepropionic acid;gamma aminobutyric acid;dentate gyrus	This paper describes a million-plus granule cell compartmental model of the rat hippocampal dentate gyrus, including excitatory, perforant path input from the entorhinal cortex, and feedforward and feedback inhibitory input from dentate interneurons. The model includes experimentally determined morphological and biophysical properties of granule cells, together with glutamatergic AMPA-like EPSP and GABAergic GABAA-like IPSP synaptic excitatory and inhibitory inputs, respectively. Each granule cell was composed of approximately 200 compartments having passive and active conductances distributed throughout the somatic and dendritic regions. Modeling excitatory input from the entorhinal cortex was guided by axonal transport studies documenting the topographical organization of projections from subregions of the medial and lateral entorhinal cortex, plus other important details of the distribution of glutamatergic inputs to the dentate gyrus. Results showed that when medial and lateral entorhinal cortical neurons maintained Poisson random firing, dentate granule cells expressed, throughout the million-cell network, a robust, non-random pattern of spiking best described as spatiotemporal “clustering”. To identify the network property or properties responsible for generating such firing “clusters”, we progressively eliminated from the model key mechanisms such as feedforward and feedback inhibition, intrinsic membrane properties underlying rhythmic burst firing, and/or topographical organization of entorhinal afferents. Findings conclusively identified topographical organization of inputs as the key element responsible for generating a spatio-temporal distribution of clustered firing. These results uncover a functional organization of perforant path afferents to the dentate gyrus not previously recognized: topography-dependent clusters of granule cell activity as “functional units” that organize the processing of entorhinal signals.	anatomical compartments;axonal transport;biological neuron model;cellular phone;cerebral cortex;cluster analysis;diploid cell;documented;excitatory postsynaptic potentials;experiment;feedback;feedforward neural network;granule (oracle dbms);hippocampus (brain);inhibitory postsynaptic potentials;interneurons;lateral thinking;medial graph;multi-compartment model;perforant pathway;projection defense mechanism;randomness;software documentation;structure of dentate gyrus;structure of entorhinal cortex;synaptic package manager;tissue membrane;topography;emotional dependency;granule cell;statistical cluster	Phillip J. Hendrickson;Gene J. Yu;Dong Song;Theodore W. Berger	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319446	computer simulation;biology;neuroscience;developmental psychology;computer science;hippocampus;hippocampal formation;anatomy;neurotransmission	ML	18.537276276263015	-72.58910532988791	9187
6e559654e8378ceca7c53cbc8ed1474b344af763	illumination compensation algorithm using eigenspaces transformation for facial images	illumination modeling;face modeling;projective space;linear transformation;illumination compensation	This paper presents a new low-dimensional face representation using the proposed eigenspaces transformation. The proposed algorithm is based on face images which is acquired with c illumination conditions. We define face images as a non-illumination class and illumination classes from light source conditions and derive the linear transformation function in a low-dimensional eigenspace between a non-illumination class and illumination classes. The proposed illumination compensation algorithm is composed of two steps. In the optimal projection space which is obtained from the DirectLDA algorithm, we first select the illumination class for a given image and then we generate a nonilluminated image by using eigenspace transformation of the illuminated class. We provide experimental results to demonstrate the performance of the proposed algorithm with varying parameters of proposed algorithm.	algorithm	Junyeong Yang;Hyeran Byun	2007		10.1007/978-3-540-71457-6_18	computer vision;projective space;topology;mathematics;geometry;linear map	Vision	43.24821466791149	-57.44124324900796	9199
9f23295ed0ce98bffdd327ba07cb34765f13352b	efficient texture representation using multi-scale regions	multiple scales;similarity measure	This paper introduces an efficient way of representing textures using connected regions which are formed by coherent multi-scale over-segmentations. We show that the recently introduced covariancebased similarity measure, initially applied on rectangular windows, can be used with our newly devised, irregular structure-coherent patches; increasing the discriminative power and consistency of the texture representation. Furthermore, by treating texture in multiple scales, we allow for an implicit encoding of the spatial and statistical texture properties which are persistent across scale. The meaningfulness and efficiency of the covariance based texture representation is verified utilizing a simple binary segmentation method based on min-cut. Our experiments show that the proposed method, despite the low dimensional representation in use, is able to effectively discriminate textures and that its performance compares favorably with the state of the art.	coherence (physics);computer vision;experiment;maxima and minima;microsoft windows;minimum cut;self-similarity;similarity measure;texture mapping	Horst Wildenauer;Branislav Micusík;Markus Vincze	2007		10.1007/978-3-540-76386-4_5	computer science;pattern recognition	Vision	49.144324048003945	-70.26424625100104	9203
405ac297e37ef59477bd5e2b30cb796e3c4e1a72	a comparative study of different color space models using fcm-based automatic grabcut for image segmentation		GrabCut is one of the powerful color image segmentation techniques. One main disadvantage of GrabCut is the need for initial user interaction to initialize the segmentation process which classifies it as a semi-automatic technique. The paper presents the use of Fuzzy C-means clustering as a replacement of the user interaction for the GrabCut automation. Several researchers concluded that no single color space model can produce the best results of every image segmentation problem. This paper presents a comparative study of different color space models using automatic GrabCut for the problem of color image segmentation. The comparative study includes the test of five color space models; RGB, HSV, XYZ, YUV and CMY. A dataset of different 30 images are used for evaluation. Experimental results show that the YUV color space is the one generating the best segmentation accuracy for the used dataset of images.	color space;fuzzy cognitive map;grabcut;image segmentation	Dina Reda Khattab;Hala Mousher Ebied;Ashraf Saad Hussein;Mohamed F. Tolba	2015		10.1007/978-3-319-21404-7_36	computer vision;grabcut;scale-space segmentation	Vision	41.725527957239514	-68.58511926388273	9240
228ecdc60ae8872cd8502b54f4321f77e85c04c8	mri patterns of atrophy and hypoperfusion associations across brain regions in frontotemporal dementia	brain atrophy;atrophy;software;female;brain;frontotemporal dementia;data interpretation statistical;multimodality mri;brain hypoperfusion;middle aged;psychiatric status rating scales;male;image processing computer assisted;cerebrovascular circulation;principal component analysis;magnetic resonance imaging;nonlinear dynamics;neuropsychological tests;dementia;neurodegenerative diseases;algorithms;anatomy cross sectional;joint ica;humans;behavior;aged	Magnetic Resonance Imaging (MRI) provides various imaging modes to study the brain. We tested the benefits of a joint analysis of multimodality MRI data in combination with a large-scale analysis that involved simultaneously all image voxels using joint independent components analysis (jICA) and compared the outcome to results using conventional voxel-by-voxel unimodality tests. Specifically, we designed a jICA to decompose multimodality MRI data into independent components that explain joint variations between the image modalities as well as variations across brain regions. We tested the jICA design on structural and perfusion-weighted MRI data from 12 patients diagnosed with behavioral variant frontotemporal dementia (bvFTD) and 12 cognitively normal elderly individuals. While unimodality analyses showed widespread brain atrophy and hypoperfusion in the patients, jICA further revealed two significant joint components of variations between atrophy and hypoperfusion across brain regions. The 1st joint component revealed associated brain atrophy and hypoperfusion predominantly in the right brain hemisphere in behavioral variant frontotemporal dementia, and the 2nd joint component revealed greater atrophy relative to hypoperfusion affecting predominantly the left hemisphere in behavioral variant frontotemporal dementia. The patterns are consistent with the clinical symptoms of behavioral variant frontotemporal dementia that relate to asymmetric compromises of the left and right brain hemispheres. The joint components also revealed that that structural alterations can be associated with physiological alterations in spatially separated but potentially connected brain regions. Finally, jICA outperformed voxel-by-voxel unimodal tests significantly in terms of an effect size, separating the behavioral variant frontotemporal dementia patients from the controls. Taken together, the results demonstrate the benefit of multimodality MRI in conjunction with jICA for mapping neurodegeneration, which may lead ultimately to an improved diagnosis of behavioral variant frontotemporal dementia and other forms of neurodegenerative diseases.	atrophic;cerebral atrophy;cerebral hemisphere structure (body structure);frontotemporal dementia;independent component analysis;mental association;multimodal imaging;neurodegenerative disorders;patients;perfusion magnetic resonance imaging;right cerebral hemisphere;voxel;benefit;hypoperfusion;nervous system disorder	Duygu Tosun;Howard J. Rosen;Bruce L. Miller;Michael Weiner;Norbert Schuff	2012	NeuroImage	10.1016/j.neuroimage.2011.10.031	psychology;neuroscience;developmental psychology;radiology;medicine;pathology;magnetic resonance imaging;principal component analysis;behavior	ML	21.06246809564702	-79.33772534405908	9253
3e53ad6de28e25edce3937357f9529da8c611529	self-navigated low-rank mri for mpio-labeled immune cell imaging of the heart	superparamagnetism biomedical mri cardiology cellular biophysics diseases image sequences iron compounds magnetic materials medical image processing nanomagnetics nanomedicine nanoparticles navigation physiological models;fe 3 o 4 pre clinical rodent ischemic reperfusion injury model myocardial inflammation assessment self navigated pulse sequence ungated cardiac imaging free breathing cardiac imaging heart inflammatory response imaging magnetic resonance imaging super paramagnetic iron oxide particles mpio labeled immune cell imaging self navigated low rank mri;navigation magnetic resonance imaging in vivo myocardium heart acceleration	Super-paramagnetic iron oxide (SPIO) particles can magnetically label immune cells in circulation; the accumulation of labeled cells can then be detected by magnetic resonance imaging (MRI). This has enormous potential for imaging inflammatory responses in the heart, but it has been difficult to do in vivo using conventional free-breathing, ungated cardiac imaging. Subspace imaging with temporal navigation and sparse sampling of (k, t)-space has previously been used to accelerate several cardiac imaging applications, conventionally alternating between acquiring navigator data and sparse data every other TR. Here we describe a more efficient self-navigated pulse sequence to acquire both navigator and sparse (k, t)-space data in the space of a single TR, doubling imaging speed to approach 100 frames per second (fps). We show the feasibility of using the resulting method to assess myocardial inflammation in a pre-clinical rodent ischemic reperfusion injury (IRI) model using micron-sized paramagnetic iron oxide (MPIO) particles to label immune cells in situ.	cardiomyopathies;dental plaque;evaluation procedure;frame (physical object);gadolinium;magnetic resonance imaging;micron;myocardium;period-doubling bifurcation;register machine;reperfusion therapy;spio nanoparticle;sampling (signal processing);senile plaques;sparse matrix;tracer;tree accumulation;video-in video-out;ferric oxide;ischemic reperfusion injury	Anthony G. Christodoulou;Yijen L. Wu;T. Kevin Hitchens;Chien Ho;Zhi-Pei Liang	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6943893	radiology;medicine;pathology;nuclear magnetic resonance	Visualization	43.480055124543625	-84.65659792629658	9271
2e0de35d5b580220e03e387b8802005497ecb7c1	four-dimensional lv tissue tracking from tagged mri with a 4d b-spline model	left ventricle;image sequence;short axis	Accurate delineation of the volumetric motion of left ventri-cle (LV) of the heart over time from tagged MRI is an important area of research. We have built a system that takes tagged short-axis (SA) and long-axis (LA) image sequences as input, ts a 4D B-spline model to the LV of the heart by simultaneously tting knot solids to the SA and LA frame sequences via matching 3 sequences of model knot planes to LV tag planes for 4D tracking. The advantage of the 4D model is that 3D material point localization and displacement reconstruction is achieved in a single step. The generated 3D displacement elds are validated with a cardiac motion simulator, and 3D motion elds capturing in-vivo deformations in a parcine model of a LV with postero-lateral myocardial infarction are illustrated.	apache axis;b-spline;displacement mapping;lateral thinking;logical volume management;motion simulator;optic axis of a crystal;simulation;video-in video-out	Jiantao Huang;Amir A. Amini	1999		10.1007/3-540-48714-X_27	computer vision;geometry;nuclear medicine	Vision	41.741281683942454	-82.81007585001673	9274
5389a39df73c5d8ee3abfde1f13f388c070e9b17	supersite: dictionary of metabolite and drug binding sites in proteins	software;molecular recognition;computer graphics;drug targeting;ligands;receptor binding;search algorithm;pharmaceutical preparations;binding site;binding sites;dictionaries chemical;proteins;vitamin b 6;encyclopedias as topic;evolutionary conservation;metabolism;databases protein	The increasing structural information about target-bound compounds provide a rich basis to study the binding mechanisms of metabolites and drugs. SuperSite is a database, which combines the structural information with various tools for the analysis of molecular recognition. The main data is made up of 8000 metabolites including 1300 drugs, bound to about 290,000 different receptor binding sites. The analysis tools include features, like the highlighting of evolutionary conserved receptor residues, the marking of putative binding pockets and the superpositioning of different binding sites of the same ligand. User-defined compounds can be edited or uploaded and will be superimposed with the most similar co-crystallized ligand. The user can examine all results online with the molecule viewer Jmol. An implemented search algorithm allows the screening of uploaded proteins, in order to detect potential drug binding sites, which are similar to known binding pockets. The huge data set of target-bound compounds in combination with the provided analysis tools allow to inspect the characteristics of molecular recognition, especially for drug target interactions. SuperSite is publicly available at: http://bioinformatics.charite.de/supersite.	binding sites;bioinformatics;crystal structure;database;dictionary;drug delivery systems;interaction;item unique identification;jmol;ligands;molecule editor;search algorithm;substance abuse detection	Raphael André Bauer;Stefan Günther;Dominic Jansen;Carolin Heeger;Paul Florian Thaben;Robert Preissner	2009		10.1093/nar/gkn618	pharmacology;biology;bioinformatics;binding site	Comp.	-1.05370033498459	-59.840303131783514	9289
7f5fbca08ae417037c466b0d0358055252779c1a	an improved adaboost face detection algorithm based on optimizing skin color model	improved adaboost algorithm;face detection algorithm;image segmentation;cascade classifier construction;cascade classifier construction face detection algorithm skin color detection improved adaboost algorithm skin region segmentation statistical characteristics;skin color detection;skin;training;cascade classifier;image classification;cascade classifier skin color detection face detection adaboost update weight;skin color;statistical analysis face recognition image classification image colour analysis image segmentation learning artificial intelligence;update weight;face recognition;statistical analysis;image color analysis;image colour analysis;statistical characteristics;classification algorithms;adaboost;skin region segmentation;face;humans;learning artificial intelligence;face detection;classification algorithms skin image color analysis training face face detection humans	This paper proposes a face detection algorithm combined skin color detection and improved AdaBoost algorithm. First, skin regions are segmented from the detected image, and candidate face regions are obtained in terms of the statistical characteristics of human face; Then focusing on the phenomena of overfitting in training process of classical AdaBoost algorithm, this paper proposes a novel method to update weight. At the same time, the process of constructing cascade classifier is added to training process. Finally, the candidate face regions are scanned by cascade classifier for more exact face orientation. A mass of experimental results show that the new approach obtains better results and improves detection performance obviously.	adaboost;algorithm;cascading classifiers;face detection;overfitting	Gang Li;Yinping Xu;Jiaying Wang	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5582393	computer vision;computer science;machine learning;pattern recognition	Vision	34.33257320784036	-62.38521690502472	9314
12a0c577d617100d86d2aff2ae60b575b6c3b247	real-time face detection using hybrid ga based on selective attention	random searching selective attention real time face detection human like visual function image processing hybrid genetic algorithm;intelligent transport system;real time;computer vision;face detection image recognition humans face recognition object detection image processing intelligent systems cameras real time systems genetic algorithms;face recognition;computer vision face recognition genetic algorithms object detection;secure system;image processing methods;genetic algorithm;genetic algorithms;selective attention;face detection;random search;object detection	There have been many attempts to realize human-like visual function by image processing. Methods for recognition and tracking of the human face are expected to be applied in security systems and in the field of ITS (intelligent transport systems). This study was performed to construct a detection system capable of recognizing multiple areas of the human face in real tune. We employed a hybrid GA (genetic algorithm) based on selective attention, which is the human visual function used to reduce processing load, to search for the position of a face in input images. The hybrid GA consisted of random searching as a rough search and an improved GA that performed a more careful search. These methods were combined by grouping to allow simultaneous detection of multiple faces in input images in real time. We confirmed the effectiveness of our proposed detection system by experiments involving detection of multiple targets.	experiment;face detection;genetic algorithm;image processing;real-time transcription;software release life cycle	Hidekazu Suzuki;Mamoru Minami	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389580	facial recognition system;computer vision;face detection;object-class detection;genetic algorithm;computer science;artificial intelligence;viola–jones object detection framework;machine learning;three-dimensional face recognition	Robotics	30.432192411622346	-59.51968202310315	9370
bcaac2b6f229647eee74397b2c7e44b060399775	bottom up modeling of the connectome: linking structure and function in the resting brain and their changes in aging	aging;multiscale entropy;resting-state models;mse;structure–function;complexity;criticality	With the increasing availability of advanced imaging technologies, we are entering a new era of neuroscience. Detailed descriptions of the complex brain network enable us to map out a structural connectome, characterize it with graph theoretical methods, and compare it to the functional networks with increasing detail. To link these two aspects and understand how dynamics and structure interact to form functional brain networks in task and in the resting state, we use theoretical models. The advantage of using theoretical models is that by recreating functional connectivity and time series explicitly from structure and pre-defined dynamics, we can extract critical mechanisms by linking structure and function in ways not directly accessible in the real brain. Recently, resting-state models with varying local dynamics have reproduced empirical functional connectivity patterns, and given support to the view that the brain works at a critical point at the edge of a bifurcation of the system. Here, we present an overview of a modeling approach of the resting brain network and give an application of a neural mass model in the study of complexity changes in aging.		Tristan T. Nakagawa;Viktor K. Jirsa;Andreas Spiegler;Anthony Randal McIntosh;Gustavo Deco	2013	NeuroImage	10.1016/j.neuroimage.2013.04.055	neuroscience;computer science;artificial intelligence;communication	ML	21.37454494006884	-76.09232399589196	9379
0d7c56e7db88e418215063a7920bb7afe4fa8fa2	morphological operations by locally variable structuring elements and their applications to region extraction in ultrasound images	variable structuring element;edge enhancement;variable structure;morphological operation;ultrasound imaging;ultrasound image;morphology;region extraction	In this paper, we define a new adaptive morphological operation, in which the value of a structuring element varies adaptively depending on the local intensity information of the processing image of interest. We prove that the proposed adaptive morphological operation has almost the same mathematical structure and properties as the conventional ones have. There are many useful functions in the method. Among them are the opening and closing, which implement both smoothing of the image and emphasizing of the edges at a time. Conventional opening operation has a smoothing function but not both. Applying our method to relatively unclear images such as ultrasound ones with speckle noise, its usefulness can be found in extracting regions. We also discuss parameter setting, and its enhancement effect together with experimental results is demonstrated. © 2003 Wiley Periodicals, Inc. Syst Comp Jpn, 34(3): 33–43, 2003; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/scj.10196	closing (morphology);digi-comp i;grayscale;john d. wiley;mathematical morphology;mathematical structure;phantom reference;smoothing;structuring element	Masayasu Ito;Masayoshi Tsubai;Akira Nomura	2003	Systems and Computers in Japan	10.1002/scj.10196	computer vision;morphology;computer science;artificial intelligence;edge enhancement;structuring element;top-hat transform	Vision	45.24916379152603	-72.69799744775808	9390
148b3d89c1356ba42f20f01ac458a171f62cfd18	early diagnosis of mild cognitive impairment: a case study in approaches to inductive-logic programming		Recent rapid advances in data collection routines in clinical science have led to a trend of storing patient data in a heterogeneous database. The lack of existing computing tools to enable operability to use machine learning on these heterogeneous data sources is a barrier to the healthcare sciences. Healthcare data is usually complex and highly context-dependent, and it requires modern computational tools to handle the complexity of such data. This study sought to utilize the data collected from virtual reality (VR)-based software and a leap-motion device used for learning in mild cognitive impairment (MCI) cases to enable early detection of MCI by analyzing the classification rules for errors (action slips) based on finger-action transitions when performing instrumental activities of daily living (IADL). Finger motion was recorded as a time-series database. An induction technique known as Inductive-Logic Programming (ILP), which uses logical and clausal language to represent the training data, was then used to discover a concise classification rule using logical programming. We were able to generate rules on how action transitions of the finger in the experiments were related to the pattern of micro-errors that indicate the difference of error regarding the length of the no-motion state of the finger.	computation;context-sensitive language;experiment;heterogeneous database system;inductive logic programming;machine learning;mathematical induction;operability;time series;virtual reality	Niken Prasasti;Takehiko Yamaguchi;Hayato Ohwada	2017	2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)	10.1109/ICCI-CC.2017.8109760	classification rule;operability;data collection;software;activities of daily living;inductive logic programming;cognition;logical conjunction;machine learning;computer science;artificial intelligence	DB	5.2182277212442285	-81.51275062642264	9407
e40313f6a29d75249a710d45836c06a52e23ab84	a 3d high resolution ex vivo white matter atlas of the common squirrel monkey (saimiri sciureus) based on diffusion tensor imaging	animals;brain;white matter atlas;structural imaging;magnetic resonance imaging;brain atlas;neuroanatomy;diffusion tensor imaging;squirrel monkey;tractography	Modern magnetic resonance imaging (MRI) brain atlases are high quality 3-D volumes with specific structures labeled in the volume. Atlases are essential in providing a common space for interpretation of results across studies, for anatomical education, and providing quantitative image-based navigation. Extensive work has been devoted to atlas construction for humans, macaque, and several non-primate species (e.g., rat). One notable gap in the literature is the common squirrel monkey - for which the primary published atlases date from the 1960's. The common squirrel monkey has been used extensively as surrogate for humans in biomedical studies, given its anatomical neuro-system similarities and practical considerations. This work describes the continued development of a multi-modal MRI atlas for the common squirrel monkey, for which a structural imaging space and gray matter parcels have been previously constructed. This study adds white matter tracts to the atlas. The new atlas includes 49 white matter (WM) tracts, defined using diffusion tensor imaging (DTI) in three animals and combines these data to define the anatomical locations of these tracks in a standardized coordinate system compatible with previous development. An anatomist reviewed the resulting tracts and the inter-animal reproducibility (i.e., the Dice index of each WM parcel across animals in common space) was assessed. The Dice indices range from 0.05 to 0.80 due to differences of local registration quality and the variation of WM tract position across individuals. However, the combined WM labels from the 3 animals represent the general locations of WM parcels, adding basic connectivity information to the atlas.	atlases;cervical atlas;diffusion tensor imaging;display resolution;forty nine;gray matter;macaca;magnetic resonance imaging;modal logic;numerous;primates;scientific publication;squirrel;tracer;track (course);tract (literature);video-in video-out;white matter	Yurui Gao;Prasanna Parvathaneni;Kurt Schilling;Feng Wang;Iwona Stepniewska;Zhoubing Xu;Ann S. Choe;Zhaohua Ding;John C. Gore;Li Min Chen;Bennett A. Landman;Adam M. Anderson	2016	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2217325	diffusion mri;neuroanatomy;magnetic resonance imaging;tractography	Visualization	34.197829207231905	-83.7491258760664	9425
0031a27ca6feafb15c34bc6180e0b36b6d7b6187	peace-home: probabilistic estimation of abnormal clinical events using vital sign correlations for reliable home-based monitoring	hidden markov model;ambient assisted living;clinical event;vital signs;remote monitoring	The objective of this study is to develop a probabilistic model for predicting the future clinical episodes of a patient using observed vital sign values prior to the clinical event. Vital signs (e.g. heart rate, blood pressure) are used to monitor a patient’s physiological functions of health and their simultaneous changes indicate a transition of a patient’s health status. If such changes are abnormal then it may lead to serious physiological deterioration. Chronic patients living alone at home die of various diseases due to the lack of an efficient automated system having prior prediction ability. Our developed system can make probabilistic predictions of future clinical events of an unknown patient in real-time using the learned temporal correlations of multiple vital signs from many similar patients. In this paper, Principal Component Analysis (PCA) is used to separate patients with known medical conditions into multiple categories and then Hidden Markov Model (HMM) is adopted for probabilistic classification and prediction of future clinical states. The advantage of using dynamic probabilistic model over static predictor model for solving our problem is analysed by comparing the results obtained from HMM with a neural network based learning model. Both the learning models are trained and evaluated using six vital signs data of 1023 patient records collected from the MIMIC-II database of MIT physiobank archive. The best HMM models are selected using maximum likelihood probabilities and further used in personalized remote monitoring system to forecast the most probable forthcoming clinical states of a continuously monitored patient. The scalable power of cloud computing is utilized for fast learning of various clinical events from large samples. Our results suggest that the developed technique using multiple physiological parameter trends can significantly enhance the traditional home-based monitoring systems in terms of clinical abnormality detections and predictions.		Abdur Forkan;Ibrahim Khalil	2017	Pervasive and Mobile Computing	10.1016/j.pmcj.2016.12.009	data mining;artificial neural network;probabilistic logic;principal component analysis;vital signs;distributed computing;artificial intelligence;hidden markov model;probabilistic classification;computer science;pattern recognition;statistical model;abnormality	Mobile	4.767095660936579	-77.30492214198058	9437
fed8b67d096c45cafec550f8607ce86319b612ba	novel and fast approach for iris location	image recognition;image recognition biometrics access control;biometrics access control;specialized boundary detection mask iris recognition biometric identification system iris location algorithm minimum local block mean;iris recognition biometrics fingerprint recognition humans reliability engineering image databases authentication personnel facial features retina;image database;iris recognition;boundary detection	Iris recognition is regarded as the most reliable and accurate biometric identification system. Speed and accuracy of iris location are required to achieve the success of iris recognition. This paper presents a new iris location algorithm to improve speed and accuracy. The first step is to detect a point inside the pupil using the method of minimum local block mean. Then the specialized boundary detection mask (SBDM) is employed to locate three points along the inner and outer iris boundaries. Finally, Thales' theorem is applied to find the hypotenuse that is a diameter of their circumcircle and then the circle parameters can be calculated. Based on the CASIA iris image database, experiment results show that the proposed method is superior to the existing approaches on the speed and precision of iris location.	algorithm;biometrics;iris recognition;the circle (file system)	Jen-Chun Lee;Ping Sheng Huang;Chien-Ping Chang;Te-Ming Tu	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.230	computer vision;speech recognition;computer science;pattern recognition;iris recognition	Robotics	33.899597035545554	-62.41163570715201	9441
78674238386867a25d983e9a36ffabeb1773c0f8	caveat: a program to facilitate the design of organic molecules	relative orientation;enzyme;biological activity;drug design;design and implementation;functional group;database search	A frequently encountered problem in the design of enzyme inhibitors and other biologically active molecules is the identification of molecular frameworks to serve as templates or linking units that can position functional groups in specific relative orientations. The program CAVEAT was designed to address this problem by searching 3D databases for such molecular fragments. Key innovations introduced in CAVEAT are a focus on relationships between bonds and the provision of automated methods to identify and classify structural frameworks. Performance has been a particular concern in formulating CAVEAT, since it is intended to be used in an interactive manner. The focus in this report is the design and implementation of the principal algorithms and the performance achieved.		Georges Lauri;Paul A. Bartlett	1994	Journal of computer-aided molecular design	10.1007/BF00124349	biochemistry;enzyme;database search engine;chemistry;bioinformatics;biological activity;nanotechnology;drug design	EDA	10.068811801786362	-60.444456015279656	9454
e695fcfbf49e29087f424db9935f364c85d9f7ce	contribution to a new robotic concept of prostate brachytherapy	supervision prostate brachytherapy adaptive target tracking active contour detection;active contour;access point;phantoms;cancer;ultrasound;edge detection;ultrasonic imaging;virtual reality;adaptive grid;service robots;ultrasonic applications;medical robotics;ultrasound imaging;robot control;brachytherapy;medical image processing;industrial robots;adaptive target tracking;diseases;patient treatment;prostate brachytherapy;active contour detection;robot frame new robotic concept prostate brachytherapy mobile targets adaptive tracking ultrasound control industrial robot virtual simulator;target tracking;robot kinematics needles brachytherapy ultrasonic imaging service robots phantoms;supervision;needles;virtual reality brachytherapy cancer diseases edge detection industrial robots medical image processing medical robotics patient treatment ultrasonic applications;robot kinematics	In this paper, we experiment a new robotic concept for the brachytherapy of prostate, using adaptive tracking of mobile targets under ultrasound control of industrial robot. Contours detection' method of the prostate from ultrasound images is presented. The targets to be achieved safely by the robot, are defined from a virtual and adaptive grid. A virtual simulator is used firstly, to transfer the coordinates of the targets from the image to the robot frame, then secondly to supervise by comparing between the theoritical and experimental trajectories, in order to improve the accuracy of the robot control. Experimental results applied on a prostate phantom from one access point, can show the advantage of the proposed concept in the context of reducing trauma and recovery times for real patients.	active contour model;adaptive mesh refinement;algorithm;contour line;imaging phantom;industrial robot;robot control;robot end effector;shepp–logan phantom;simulation;wireless access point	Vincent Coelen;Rochdi Merzouki;X. Liem;Eric Lartigau;Belkacem Ould Bouamama	2010	2010 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2010.5723422	computer vision;simulation;edge detection;computer science;engineering;artificial intelligence;ultrasound;active contour model;virtual reality;robot control;robot kinematics;cancer;medical physics	Robotics	41.445877837621	-85.43220222529351	9467
1a201c774200bd7e97398dfb7eda96e2317b230c	compilation of dna sequences of escherichia coli k12 (ecd and ecdc; update 1995)	embl;escherichia coli;dna bacterial;computer communication networks;genetic map;indexation;world wide web;databases factual;cd rom;base sequence;dna sequence	We have compiled the DNA sequence data for Escherichia coli available from the GenBank and EMBL data libraries and independently from the literature. Unlike the previous updates of our E.coli databases, we provide the most recent version preferentially via the World Wide Web System (use URL: http://susi.bio.unigiessen.de/usr/local/www++ +/html/ecdc.html). Our database includes an assembled set of contiguous sequences. Each of these contigs compiles all available sequence information, including those derived from a variety of elder sequences. The organization of the database allows one to find the exact physical location of each individual gene or regulatory region, even regarding discrepancies in nomenclature. The WWW program allows access into the original EMBL and SWISSPROT datafiles. A FASTA and BLAST search may be performed online. Besides the WWW format a flat file version may be obtained via ftp. The complete compilation, including a full set of genetic map data and the E.coli protein index, can be obtained in machine readable form from the EMBL data library as a part of the CD-ROM issue of the EMBL sequence database, released and updated every three months. After deletion of all detected overlaps a total of 3 333 878 individual bp was determined by the end of September 1995. This corresponds to a total of 71.71% of the entire E.coli chromosome consisting of about 4720 kbp. About 94 kbp (2%) are available additionally, but have not yet been definitely mapped.	blast;cd-rom;compiler;deletion mutation;explanatory combinatorial dictionary;fasta;flat file database;genbank;human-readable medium;libraries;nomenclature;sequence database;tacrolimus binding proteins;uniprot;uniform resource locator;www;world wide web	Manfred Kröger;Ralf Wahl	1996	Nucleic acids research	10.1093/nar/24.1.29	biology;dna sequencing;cd-rom;bioinformatics;escherichia coli;genetics	Comp.	-2.085609235822043	-60.28010264570131	9468
8071a1529a970b48bd08bc4d96304f83d41d71c2	the québec bcg vaccination registry (1956–1992): assessing data quality and linkage with administrative health databases	health services research;health informatics;information systems and communication service;registries;vaccination;management of computing and information systems;quebec;humans;databases factual;electronic health records	BACKGROUND Vaccination registries have undoubtedly proven useful for estimating vaccination coverage as well as examining vaccine safety and effectiveness. However, their use for population health research is often limited. The Bacillus Calmette-Guérin (BCG) Vaccination Registry for the Canadian province of Québec comprises some 4 million vaccination records (1926-1992). This registry represents a unique opportunity to study potential associations between BCG vaccination and various health outcomes. So far, such studies have been hampered by the absence of a computerized version of the registry. We determined the completeness and accuracy of the recently computerized BCG Vaccination Registry, as well as examined its linkability with demographic and administrative medical databases.   METHODS Two systematically selected verification samples, each representing ~0.1% of the registry, were used to ascertain accuracy and completeness of the electronic BCG Vaccination Registry. Agreement between the paper [listings (n = 4,987 records) and vaccination certificates (n = 4,709 records)] and electronic formats was determined along several nominal and BCG-related variables. Linkage feasibility with the Birth Registry (probabilistic approach) and provincial Healthcare Registration File (deterministic approach) was examined using nominal identifiers for a random sample of 3,500 individuals born from 1961 to 1974 and BCG vaccinated between 1970 and 1974.   RESULTS Exact agreement was observed for 99.6% and 81.5% of records upon comparing, respectively, the paper listings and vaccination certificates to their corresponding computerized records. The proportion of successful linkage was 77% with the Birth Registry, 70% with the Healthcare Registration File, 57% with both, and varied by birth year.   CONCLUSIONS Computerization of this Registry yielded excellent results. The registry was complete and accurate, and linkage with administrative databases was highly feasible. This study represents the first step towards assembling large scale population-based epidemiological studies which will enable filling important knowledge gaps on the potential health effects of early life non-specific stimulation of the immune function, as resulting from BCG vaccination.	bcg vaccine;certificate (record artifact);data quality;epidemiology;estimated;identifier;linkage (software);mental association;published database;registries;resident evil 3: nemesis;verification of theories;genetic linkage	Marie-Claude Rousseau;Florence Conus;Jun Li;Marie-Élise Parent;Mariam El-Zein	2014		10.1186/1472-6947-14-2	health informatics;medicine;environmental health;nursing;data mining;vaccination	Metrics	8.202380662661437	-73.0181489726496	9501
4613229aff389bf0330cd63efab3733ec12b73e5	pgltools: a genomic arithmetic tool suite for manipulation of hi-c peak and other chromatin interaction data	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	Genomic interaction studies use next-generation sequencing (NGS) to examine the interactions between two loci on the genome, with subsequent bioinformatics analyses typically including annotation, intersection, and merging of data from multiple experiments. While many file types and analysis tools exist for storing and manipulating single locus NGS data, there is currently no file standard or analysis tool suite for manipulating and storing paired-genomic-loci: the data type resulting from “genomic interaction” studies. As genomic interaction sequencing data are becoming prevalent, a standard file format and tools for working with these data conveniently and efficiently are needed. This article details a file standard and novel software tool suite for working with paired-genomic-loci data. We present the paired-genomic-loci (PGL) file standard for genomic-interactions data, and the accompanying analysis tool suite “pgltools”: a cross platform, pypy compatible python package available both as an easy-to-use UNIX package, and as a python module, for integration into pipelines of paired-genomic-loci analyses. Pgltools is a freely available, open source tool suite for manipulating paired-genomic-loci data. Source code, an in-depth manual, and a tutorial are available publicly at www.github.com/billgreenwald/pgltools , and a python module of the operations can be installed from PyPI via the PyGLtools module.	annotation;bioinformatics;biopolymer sequencing;communications satellite;experiment;hl7 data type;hl7publishingsubsection <operations>;interaction;intersection of set of elements;locus;massively-parallel sequencing;open-source software;paired-associate learning;pipeline (computing);programming tool;pypy;python package index;source code;unix;negative regulation of reactive oxygen species biosynthetic process	William W. Greenwald;He Li;Erin Newman-Smith;Paola Benaglio;Naoki Nariai;Kelly A. Frazer	2017		10.1186/s12859-017-1621-0	data type;genetics;dna microarray;locus (genetics);chromosome conformation capture;genome;bioinformatics;chromatin;computer science;annotation;file format	Comp.	-2.2025663651603673	-57.71529099139607	9506
161692adfd13142a20aa53d10582a70eb4ef89c5	generalized likelihood ratio tests for complex fmri data: a simulation study	analytical models;sensitivity and specificity;models neurological;brain;algorithms artificial intelligence brain mapping computer simulation humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval likelihood functions magnetic resonance imaging models neurological models statistical reproducibility of results sensitivity and specificity;generalized likelihood ratio tests;statistical parametric map;imaging three dimensional;fmri;generalized likelihood ratio test;constant false alarm rate;statistical test;testing;testing magnetic resonance imaging analytical models magnetic resonance time series analysis magnetic analysis data visualization humans blood flow brain;time series;indexing terms;statistical analysis biomedical mri;simulation experiment;image enhancement;statistical parametric maps fmri generalized likelihood ratio test magnitude data;complex data;statistical analysis;complex functional magnetic resonance time series;brain mapping;statistical parametric maps;image interpretation computer assisted;time series analysis;likelihood functions;magnetic resonance;magnetic resonance imaging;magnetic analysis;data visualization;reproducibility of results;detection rate;constant false alarm rate generalized likelihood ratio tests complex functional magnetic resonance time series detection rate;models statistical;artificial intelligence;simulation study;algorithms;value function;humans;blood flow;magnitude data;computer simulation;information storage and retrieval;biomedical mri	Statistical tests developed for the analysis of (intrinsically complex valued) functional magnetic resonance time series, are generally applied to the data's magnitude components. However, during the past five years, new tests were developed that incorporate the complex nature of fMRI data. In particular, a generalized likelihood ratio test (GLRT) was proposed based on a constant phase model. In this work, we evaluate the sensitivity of GLRTs for complex data to small misspecifications of the phase model by means of simulation experiments. It is argued that, in practical situations, GLRTs based on magnitude data are likely to perform better compared to GLRTs based on complex data in terms of detection rate and constant false alarm rate properties.	baseline (configuration management);chamaecyparis lawsoniana;constant false alarm rate;experiment;linear phase;low back pain;population parameter;programming paradigm;resonance;sensor;signal-to-noise ratio;simulation;statistical test;time series;fmri;likelihood ratio	Jan Sijbers;Arnold Jan den Dekker	2005	IEEE Transactions on Medical Imaging	10.1109/TMI.2005.844075	econometrics;computer science;magnetic resonance imaging;machine learning;time series;mathematics;statistics	Comp.	21.188590748376527	-86.88825030667861	9509
cf0cf1d83c0d46b5a7cd72df8520d1764b2ba0b9	generation of interpolative images by determining elasticity of active contour model	active contour model	To generate the movement of an object from its observed images, an active-contour model having locally different elasticities has been proposed. Images are interpolated between two adjacent observed images by using a process in which the contour of each image converges into the next image. However, the locus of convergence of the contour cannot be used for representing the movement of an object, since the convergence times are locally different. To control the convergence time locally, a group of parameters which determines the elasticity of the contour is extracted so that their convergence time becomes constant. By using several images (elastic objects having different degrees of expansion and contraction) and MRI images of an active human heart, the effectiveness of the proposed method has been confirmed. © 1999 Scripta Technica, Syst Comp Jpn, 30(11): 5158, 1999	active contour model;contour line;digi-comp i;elasticity (data store);interpolation;locus	Satoru Morita;Hiroaki Takata	1999	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199910)30:11%3C51::AID-SCJ6%3E3.0.CO;2-9	computer vision;mathematical optimization;computer science;active contour model;mathematics;geometry	Vision	47.297841077385925	-71.33558438525567	9516
2cc455a74306d2be9ba348ca255e4bc85682b120	retrieving indoor objects: 2d-3d alignment using single image and interactive roi-based refinement		Abstract Given a single indoor image, this paper proposes an automatic retrieval system to estimate the best-matching 3D models with consistent style and pose. To support this system, we combine a deep CNN based object detection approach with a deformable part based alignment model. The key idea is to cast a 2D-3D alignment problem as a part-based cross-domain matching. We also provide an interactive refinement interface that allows users to browse models based on similarities and differences between shapes in user-specified regions of interest (ROIs). We demonstrate the ability of our system on numerous examples.		Fuchang Liu;Shuangjian Wang;Dandan Ding;Qingshu Yuan;Zhengwei Yao;Zhigeng Pan;Haisheng Li	2018	Computers & Graphics	10.1016/j.cag.2017.07.029	computer science;artificial intelligence;computer vision;object detection	Vision	33.01062312516699	-52.4542081227794	9527
c0b6c29149b89ac27631e54cf5e7bd8f53236f9c	a wireless sensor system for the training of hammer throwers	motion analysis;displacement measurements wireless sensor system hammer throwers training training targets training routes biomechanical feedback device infrared proximity sensor hip vertical movement load cell wire tension xbees data transmission arduino processor wireless system control;xbee;wire tension;training;wires;wireless sensor networks biomechanics data communication displacement measurement sport telecontrol;verticle hip displacement;wireless sensor network;receivers;biofeedback training;wireless communication;science based training;arduino;hammer throw;thesis;monitoring;zigbee;hammer throw wireless sensor network zigbee xbee arduino motion analysis biofeedback training;kinesiology computer science a wireless sensor system for the training of hammer throwers university of lethbridge canada hua li;biomechanical feedback;security;wireless sensor networks;gongbing shan wang ye;wireless sensor networks training wireless communication receivers security wires monitoring	Hammer throw has a long-standing history in track and field, but unlike other events, hammer throw has not seen a new world record since 1986. One reason for this stagnation is the lack of scientifically based training. In this paper, we propose to establish scientifically described training targets and routes, which in turn require tools that can measure and quantify characteristics of effective hammer-throw. Towards this end, we have developed a real-time biomechanical feedback device -- a wireless sensor system -- to help the training of hammer throwers. The system includes two sensors -- an infrared proximity sensor for tracing the hip vertical movement and a load cell for recording the wire tension during a hammer throw. The system uses XBees for data transmission and an Arduino processor for the wireless system control. It is hypothesized that wire tension and vertical hip displacement measurements would be sufficient to supply key features when analyzing hammer throw.	arduino;displacement mapping;effective dimension;real-time clock;sensor	Ye Wang;Shaotsung Chang;Gongbing Shan;Hua Li	2014	2014 Tenth International Conference on Computational Intelligence and Security	10.1109/CIS.2014.30	embedded system;simulation;wireless sensor network;computer science;artificial intelligence;information security;computer security	Mobile	7.370819260637399	-87.92008991735354	9533
eb01bba7ddc0aa9d9cf331300c427415e3a05007	three dimensional segmentation for cement microtomography images using self-organizing map and neighborhood features		The performance of cement is strongly influenced by its microstructure, among which dynamic microstructure can reveal the formation and development of the cement paste. Therefore, the investigation of dynamic microstructure enables us to understand the cement hydration and try to improve the cement properties. However, the constituents of cement paste are hard to directly segment by human vision due to the fully mixed phases, a lot of noise and low image definition, which influences phase extraction, substance analysis and the study on the change of material composition. This paper studies the three dimensional image segmentation for cement microtomography images using self-organizing map and neighborhood features. The method takes advantage of the neighborhood features and the fault-tolerance to missing, confusing, noisy data of self-organizing map. The experimental results manifest that this method perform well. Furthermore, the evolution of cement three-dimensional microstructure during hydration is analyzed by the segmented images.	competitive learning;cut, copy, and paste;fault tolerance;image segmentation;organizing (structure);self-organization;self-organizing map;signal-to-noise ratio;x-ray microtomography	Liangliang Zhang;Lin Wang;Bo Yang;Zhenxiang Chen;Jin Zhou;Yamin Han;Meihui Li	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8280980	noisy data;computer vision;computed tomography;self-organizing map;microstructure;feature extraction;cement;image segmentation;segmentation;computer science;artificial intelligence	Robotics	40.17010496201868	-71.90440359815457	9540
4bf7f6d63471cd52d57119729d8a7a2c8e818c85	context-aware stacked convolutional neural networks for classification of breast carcinomas in whole-slide histopathology images	breast cancer;context-aware cnn;convolutional neural networks;deep learning;histopathology	Currently, histopathological tissue examination by a pathologist represents the gold standard for breast lesion diagnostics. Automated classification of histopathological whole-slide images (WSIs) is challenging owing to the wide range of appearances of benign lesions and the visual similarity of ductal carcinoma in-situ (DCIS) to invasive lesions at the cellular level. Consequently, analysis of tissue at high resolutions with a large contextual area is necessary. We present context-aware stacked convolutional neural networks (CNN) for classification of breast WSIs into normal/benign, DCIS, and invasive ductal carcinoma (IDC). We first train a CNN using high pixel resolution to capture cellular level information. The feature responses generated by this model are then fed as input to a second CNN, stacked on top of the first. Training of this stacked architecture with large input patches enables learning of fine-grained (cellular) details and global tissue structures. Our system is trained and evaluated on a dataset containing 221 WSIs of hematoxylin and eosin stained breast tissue specimens. The system achieves an AUC of 0.962 for the binary classification of nonmalignant and malignant slides and obtains a three-class accuracy of 81.3% for classification of WSIs into normal/benign, DCIS, and IDC, demonstrating its potential for routine diagnostics.	artificial neural network;binary classification;carcinoma in situ;cardiomyopathy, familial idiopathic;convolutional neural network;data center;eosine yellowish;hematoxylin and eosin stain method;histopathology;invasive ductal breast carcinoma;mammary gland parenchyma;minimally invasive education;neural network simulation;patch (computing);pixel;silo (dataset);slide (glass microscope);specimen;whole-body irradiation	Babak Ehteshami Bejnordi;Guido C. A. Zuidhof;Maschenka Balkenhol;Meyke Hermsen;Peter Bult;Bram van Ginneken;Nico Karssemeijer;Geert J. S. Litjens;Jeroen van der Laak	2017	Journal of medical imaging	10.1117/1.JMI.4.4.044504	h&e stain;convolutional neural network;breast lesion;radiology;breast cancer;deep learning;binary classification;ductal carcinoma;pathology;histopathology;computer science;artificial intelligence	Vision	30.688775662828206	-75.99570364179226	9553
aa611b16b3efe72b1a2ffe08c391951aa18239d0	learning flexible sensori-motor mappings in a complex network	hebbian learning;multilayer;learning;reinforcement learning;complex network;hebbian;synaptic plasticity;visuomotor task;reward modulated	Given the complex structure of the brain, how can synaptic plasticity explain the learning and forgetting of associations when these are continuously changing? We address this question by studying different reinforcement learning rules in a multilayer network in order to reproduce monkey behavior in a visuomotor association task. Our model can only reproduce the learning performance of the monkey if the synaptic modifications depend on the pre- and postsynaptic activity, and if the intrinsic level of stochasticity is low. This favored learning rule is based on reward modulated Hebbian synaptic plasticity and shows the interesting feature that the learning performance does not substantially degrade when adding layers to the network, even for a complex problem.	activation function;algorithm;artificial neural network;biological neural networks;complex network;feedforward neural network;hebbian theory;learning rule;machine learning;mental association;modulation;monkeys;multilayer perceptron;neuronal plasticity;norm (social);reinforcement learning;rule (guideline);simulation;stochastic process;synaptic package manager;anatomical layer	Eleni Vasilaki;Stefano Fusi;Xiao-Jing Wang;Walter Senn	2008	Biological Cybernetics	10.1007/s00422-008-0288-z	psychology;neuroscience;hebbian theory;anti-hebbian learning;computer science;artificial intelligence;machine learning;leabra;competitive learning;communication;reinforcement learning	ML	19.164541970078478	-69.63376329252446	9556
2d287af63d4c47e7f7741ccf5783bbbb469c44ec	scalar connectivity measures from fast-marching tractography reveal heritability of white matter architecture	structural equation models;environmental factors;biological tissues;brain;brain magnetic resonance imaging genetics nervous system algorithms;frequency modulation;white matter;tensile stress;measurement;nervous system;biomedical imaging;statistical significance;diffusion weighted mri;structural equation model;complex white matter tissue architecture;magnetic resonance image;genetics;fast marching;dti dataset;scalar connectivity;central nervous system connectivity;genetic factors;physiological models biological tissues biomedical mri brain environmental factors feature extraction genetics medical image processing neurophysiology;structural equation models scalar connectivity fast marching tractography diffusion weighted mri complex white matter tissue architecture genetic factors environmental factors central nervous system connectivity dti dataset;feature extraction;image reconstruction;medical image processing;magnetic resonance imaging;mathematical model;algorithms;fast marching tractography;humans;neurophysiology;genetics magnetic resonance imaging tensile stress image reconstruction environmental factors laboratories biomedical imaging diffusion tensor imaging equations humans;diffusion tensor imaging;physiological models;central nervous system;biomedical mri;environmental factor;diffusion weighted	Recent advances in diffusion-weighted MRI (DWI) have enabled studies of complex white matter tissue architecture in vivo. To date, the underlying influence of genetic and environmental factors in determining central nervous system connectivity has not been widely studied. In this work, we introduce new scalar connectivity measures based on a computationally-efficient fast-marching algorithm for quantitative tractography. We then calculate connectivity maps for a DTI dataset from 92 healthy adult twins and decompose the genetic and environmental contributions to the variance in these metrics using structural equation models. By combining these techniques, we generate the first maps to directly examine genetic and environmental contributions to brain connectivity in humans. Our approach is capable of extracting statistically significant measures of genetic and environmental contributions to neural connectivity.	algorithm;map;scalar processor;structural equation modeling;video-in video-out	Vishal Patel;Ming-Chang Chiang;Paul M. Thompson;Katie L. McMahon;Greig I. de Zubicaray;Nicholas G. Martin;Margaret J. Wright;Arthur W. Toga	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490187	diffusion mri;structural equation modeling;radiology;medicine;pathology;computer science;magnetic resonance imaging;mathematics;nuclear magnetic resonance	Robotics	24.117409711544628	-80.35521416413938	9563
f1ed181394507e4101e977c4f924ec4d4d57ced8	a new method for extracting primitives of regular textures based on wavelet transform	texture analysis and synthesis;transformation ondelette;image processing;threshold detection;binary image;edge detection;texture image;extraction forme;texture primitive;procesamiento imagen;transformacion hough;traitement image;image texture;deteccion contorno;detection contour;detection seuil;methode matricielle;texture analysis;deteccion umbral;wavelet transform;extraccion forma;matrix method;image binaire;metodo matriz;imagen binaria;hough transformation;hough transform;transformation hough;transformacion ondita;article;pattern extraction;wavelet transformation;co occurrence matrix;regular texture;displacement vector	In this paper, a new method for extracting primitives of a regular texture via wavelet transform is provided. Each primitive is restricted to be a parallelogram. The main work of the proposed method is to extract the two displacement vectors that form the primitive of a regular texture. A contrast-based criterion is used to select appropriate subimages obtained from wavelet transform for detecting displacement vectors. An edge thresholding method is then performed on the subimages selected to locate edges. Based on these edges, Hough transform is applied to extract the displacement vectors. The proposed method is quite efficient and can get more accurate results for displacement vectors when comparing to traditional co-occurrence matrix based methods. Synthesized textures are provided to show the effectiveness of the proposed method.	autocorrelation;co-occurrence matrix;computational complexity theory;data compression;displacement mapping;document-term matrix;edge detection;horseland;hough transform;lh (complexity);sensor;speech synthesis;statistical classification;texture filtering;texture mapping;thresholding (image processing);wavelet transform	Kuen-Long Lee;Ling-Hwei Chen	2002	IJPRAI	10.1142/S0218001402001526	arithmetic;hough transform;computer vision;image processing;computer science;mathematics;computer graphics (images)	Vision	49.95625428210807	-62.78517244580233	9564
a547a566e4a15571f211ef47e9abd1f24b8e58e6	user-adaptive fall detection for patients using wristband	unsupervised learning;robot sensing systems;wrist;hospitals;training data;heart rate;accelerometers	Fall detection systems have been proposed to prevent additional injuries following fall accidents. This paper introduces an easily learnable fall detection system based on the data of an individual patient in a hospital room. The improvement of low performance using a single accelerometer at wrists and the inconvenience of sensor attached to a waist in the conventional approach was concentrated on by integrating heart rate signals to the conventional acceleration approach and changing the sensor location from a waist to wrists. As for the optimal heart rate feature selection, we proposed a four-feature vector combination (root mean square of successive differences, standard deviation of successive differences, normal to normal 50, normal to normal 20) with correlation and mutual information analysis in addition to mean absolute deviation selected as an accelerometer feature. To easily acquire and train the patients' fall data, our system was based on unsupervised learning approaches using Gaussian mixture models for optimal classifiers with the optimal cluster number decided by cluster validation index of square error sum. A 10-fold cross validation was applied for a final performance evaluation where each threshold for separating fall state from non-fall state was automatically decided in several comparison groups, which were created on the basis of fusion timing and used sensors. As a result, despite sensors attached to the wrist, the wearable inconvenience of the conventional is overcome using the feature-level fused approach between heart rates and accelerations with the accuracy up to 98.39 %, which is closest to 99.34 % of the case using a single accelerometer located at the waist.	cluster analysis;cross-validation (statistics);feature selection;feature vector;mean squared error;mixture model;mutual information;performance evaluation;sensor;sensor web;unsupervised learning;wearable computer	Young-Hoon Nho;Jong Gwan Lim;Dae-Eon Kim;Dong-Soo Kwon	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759097	unsupervised learning;embedded system;training set;simulation;computer science;engineering;accelerometer	Robotics	9.166805001199839	-86.2178139873306	9593
ae8554895084d0048b89750010805c144d8451ea	on the computation of optical flow using the 3-d gabor transform	fourier transform;gabor transform;motion estimation;local frequency representations;image sequence;optical flow	The motion of brightness patterns in an image sequence (optical flow) is most intuitively considered as a spatiotemporal phenomenon. It has been shown, however, that motion has a characteristic signature in the spatiotemporal-frequency (Fourier) domain. This fact can be exploited for the computation of optical flow. However, for cases which involve a number of regions in a sequence with different motions, as in scenes with one or more objects moving against a stationary or moving background, the global nature of the Fourier transform makes it unsuitable for this task. The signatures of the different motions cannot be resolved in the Fourier domain, nor associated with their respective regions in the image sequence. Local frequency representations provide a means to address this problem. In this paper, we consider the application of a 3-D version of the widely used Gabor transform to the computation of optical flow.		Todd R. Reed	1998	Multidim. Syst. Sign. Process.	10.1023/A:1008431029442	gabor transform;fourier transform;computer vision;discrete-time fourier transform;mathematical analysis;s transform;time–frequency analysis;harmonic wavelet transform;short-time fourier transform;computer science;fractional fourier transform;discrete fourier transform;motion estimation;optical flow;mathematics;geometry;spectral density estimation;discrete fourier transform	Vision	52.154745531900396	-54.389276078106484	9594
dd8bf802b5bacb17fa146f19f788d6e71e4cc11c	accurate chequerboard corner localisation for camera calibration	levenberg marquardt;chequerboard;image processing;mobile robot;scene reconstruction;efficient implementation;point spread function;least square;corner localisation;ground truth;digital image;camera calibration;pose estimation	In this article we describe a novel approach to obtain the position of a chequerboard corner at sub-pixel accuracy from digital images. Applications of this method include photogrammetric scene reconstruction, pose estimation, self localisation of (mobile) robots, and camera calibration. Chequerboard patterns are especially suitable for calibrating non-pinhole cameras such as fisheye or catadioptric cameras. We model the grey values of an imaged corner by a simulated imaging process. In order to obtain an efficient implementation on standard hardware, several approximations are presented. The grey value model is used to perform a least-squares fit to the input image using a Levenberg–Marquardt optimisation. The model is described by four geometric parameters (position, rotation, and skew angle of the chequerboard corner), the width of the point spread function, and two photometric parameters (gain and offset). We compare our non-linear algorithm with two linear chequerboard corner localisation algorithms and the classical localisation of photogrammetric circular targets. Ground truth is obtained by mechanically moving a target pattern in front of the camera at sub-pixel accuracy. The corner localisation algorithm is then used to measure the displacement. On the average, our algorithm achieves a displacement error (half the difference between the 75% and 25% quantiles) of 0.032 pixels, while it becomes 0.024 pixels for high contrast and 0.043 pixels for low contrast conditions. The classical photogrammetric method based on circular targets achieves 0.045 pixels in the average case, 0.017 pixels under high contrast and 0.132 pixels under low contrast conditions. The actual positional errors of the corner point positions are lower by a factor of 1= ffiffiffi 2 p than the measured displacement errors. 2011 Elsevier B.V. All rights reserved.	approximation;best, worst and average case;computation;corner case;digital image;displacement mapping;fisheye;ground truth;image processing;least squares;levenberg–marquardt algorithm;low-pass filter;mathematical optimization;nonlinear system;parabolic antenna;photogrammetry;pixel;relevance;robot;simulation	Lars Krüger;Christian Wöhler	2011	Pattern Recognition Letters	10.1016/j.patrec.2011.04.002	mobile robot;computer vision;camera resectioning;simulation;pose;levenberg–marquardt algorithm;image processing;ground truth;computer science;point spread function;least squares;digital image	Vision	51.1288525217347	-52.557014341796396	9596
32465a1d0b797059424ec71431289193cc5dc7fc	temporal dynamics of prediction error processing during reward-based decision making	prediction error;performance monitoring;temporal dynamics;reinforcement learning;data collection;reward;single trial;model;eeg;reversal learning;qualitative evaluation	Adaptive decision making depends on the accurate representation of rewards associated with potential choices. These representations can be acquired with reinforcement learning (RL) mechanisms, which use the prediction error (PE, the difference between expected and received rewards) as a learning signal to update reward expectations. While EEG experiments have highlighted the role of feedback-related potentials during performance monitoring, important questions about the temporal sequence of feedback processing and the specific function of feedback-related potentials during reward-based decision making remain. Here, we hypothesized that feedback processing starts with a qualitative evaluation of outcome-valence, which is subsequently complemented by a quantitative representation of PE magnitude. Results of a model-based single-trial analysis of EEG data collected during a reversal learning task showed that around 220ms after feedback outcomes are initially evaluated categorically with respect to their valence (positive vs. negative). Around 300ms, and parallel to the maintained valence-evaluation, the brain also represents quantitative information about PE magnitude, thus providing the complete information needed to update reward expectations and to guide adaptive decision making. Importantly, our single-trial EEG analysis based on PEs from an RL model showed that the feedback-related potentials do not merely reflect error awareness, but rather quantitative information crucial for learning reward contingencies.	choice behavior;decision making;electroencephalography;experiment;reinforcement learning;rewards;cisplatin/etoposide protocol	Marios G. Philiastides;Guido Biele;Niki Katerina Vavatzanidis;Philipp Kazzer;Hauke R. Heekeren	2010	NeuroImage	10.1016/j.neuroimage.2010.05.052	psychology;reverse learning;reward-based selection;artificial intelligence;machine learning;mean squared prediction error;social psychology;reinforcement learning;statistics;data collection	ML	13.657366028924947	-74.61599837389447	9599
d5f4bab814172dff6aa26486e3db1ba07223aa09	visual role in the dynamic postural balance due to aging-assessment by using an augmented reality perturbation system		The sensorimotor modulation is degenerated due to aging, and it may cause the elderly falling in a daily activity. The clinical balance assessments, somatosensory test, muscle strength and the joint motion measurement are usually applied to evaluate the falling potential in the aged population. Nevertheless, the ceiling effect is found among the sub-healthy elderly who has a highly functional mobility but a potential risk of falling. Therefore, virtual-reality based scene combined with the continuously perturbed platform was used in this study to investigate the visual effect in cortical modulation in responding to the degenerated sensorimotor biofeedback. A total of 30 healthy participants (fourteen young adults, and sixteen elderly) without any known defects were recruited in this study. The results concluded that vision plays a significant factor in restoring the postural stability in the complicated sensorimotor task. The young adults performed a dynamic and rhythmic postural response strategy with lower cortical excitation to maintain stability during the disturbance. The sensorimotor integration training programs are especially significant to the aging population in order to improve the neuromuscular coordination and to foreclose the potential of fall risk.	augmented reality;modulation;virtual reality;visual effects	Chun-Ju Chang;Saiwei Yang;Jen-Suh Chern;Tsui-Fen Yang	2017	2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2017.142	electroencephalography;ceiling effect;elderly falling;computer science;population;physical medicine and rehabilitation;augmented reality;biofeedback;postural balance;rhythm	Visualization	13.917326741039256	-81.87247913666204	9617
1e93063adf5d10d8273804d51f3bd2a72717a701	a cartoon image classification system using mpeg-7 descriptors	mpeg 7 visual descriptor;cartoon;image classification;neural network	Today cartoon images take more portion of digital multimedia than ever as we notice this phenomenon in the entertainment business. With the explosive proliferation of cartoon image contents on the Internet, we seem to need a classification system to categorize these cartoon images. This paper presents a new approach of cartoon image classification based on cartoonists. The proposed cartoon image classification system employs effective MPEG-7 descriptors as image feature values and learns features of particular cartoon images, and classifies the images as multiple classes according to each cartoonist. In the performance simulation we evaluate the effectiveness of the proposed system on a large set of cartoon images and the system successfully classifies images into multiple classes with the rate of over 90%.	categorization;computer vision;feature (computer vision);internet;mpeg-7;performance prediction;simulation	Junghyun Kim;Sung Wook Baik;Kangseok Kim;Changduk Jung;Wonil Kim	2011		10.1007/978-3-642-23887-1_46	computer vision;contextual image classification;computer science;machine learning;multimedia;artificial neural network;computer graphics (images)	Vision	29.020127886068895	-58.6252249186402	9625
1b68719b4b0e3ad27034a36720bc26fa592f486f	choose of wart treatment method using naive bayes and k-nearest neighbors classifiers		In this study, the success of cyrotheraphy and immunotherapy methods on common warts and plantar warts were predicted among 180 patients using machine learning methods. As a classifier, Naive Bayes and k-nearest neighbors with different neighborhood values of k were experimented. Data sets that are online available via Internet were used in the study. As a result, whether the treatment method by considering given features will give positive result could be estimated with the accuracy of 80% by using k-nearest neighbors classifier with the neighborhood value of k=7.	internet;k-nearest neighbors algorithm;machine learning;naive bayes classifier	Ruhiye Uzun;Yalcin Isler;Mualla Toksan	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404398	plantar warts;computer science;pattern recognition;naive bayes classifier;artificial intelligence;k-nearest neighbors algorithm;data set;wart treatment;classifier (linguistics);common warts	ML	7.040435136899019	-77.33736727107409	9627
e5cb85bd0fdc818aabb5a8a9c95bde53280b8819	learning a compositional hierarchy of disparity descriptors for 3d orientation estimation in an active fixation setting		Interaction with everyday objects requires by the active visual system a fast and invariant reconstruction of their local shape layout, through a series of fast binocular fixation movements that change the gaze direction on the 3-dimensional surface of the object. Active binocular viewing results in complex disparity fields that, although informative about the orientation in depth (e.g., the slant and tilt), highly depend on the relative position of the eyes. Assuming to learn the statistical relationships between the differential properties of the disparity vector fields and the gaze directions, we expect to obtain more convenient, gaze-invariant visual descriptors. In this work, local approximations of disparity vector field differentials are combined in a hierarchical neural network that is trained to represent the slant and tilt from the disparity vector fields. Each gaze-related cell’s activation in the intermediate representation is recurrently merged with the other cells’ activations to gain the desired gaze-invariant selectivity. Although the representation has been tested on a limited set of combinations of slant and tilt, the resulting high classification rate validates the generalization capability of the approach.	binocular disparity	Katerina Kalou;Agostino Gibaldi;Andrea Canessa;Silvio P. Sabatini	2017		10.1007/978-3-319-68612-7_22	gaze;vector field;differential (mechanical device);artificial neural network;machine learning;pattern recognition;artificial intelligence;hierarchy;binocular disparity;computer vision;computer science;invariant (mathematics);active vision	Vision	21.356119915793837	-66.50434777590858	9630
7ff37472e6cb4c02b25b832f3f38b37c5e702dcd	greensim: a network simulator for comprehensively validating and evaluating new machine learning techniques for network structural inference	neural nets digital simulation genetic engineering inference mechanisms learning artificial intelligence;genetic engineering;analytical models;2nd order nonlinear regulatory functions;networks;2nd order nonlinear regulatory functions greensim network simulator machine learning network structural inference biological networks structural inference techniques genome wide structural inference;neural nets;greensim;simulation;biological system modeling;genetic regulatory network;structural inference;inference mechanisms;automatic generation;network simulator;genome size;degree distribution;network structural inference;machine learning;time series analysis;structural inference techniques;limit cycles;genome wide structural inference;biological system modeling noise data models limit cycles analytical models time series analysis;genetic regulatory networks;structural inference networks simulation genetic regulatory networks;biological networks;network structure;learning artificial intelligence;biological network;digital simulation;noise;data models	Networks are very important in many fields of machine learning research. Within networks research, inferring the structure of unknown networks is often a key problem; e.g. of genetic regulatory networks. However, there are very few well-known biological networks, and good simulation is essential for validating and evaluating novel structural inference techniques. Further, the importance of large, genome-wide structural inference is increasingly recognised, but there does not appear to be a good simulator available for large networks. This paper presents GreenSim, a simulator that helps address this gap. GreenSim automatically generates large, genome-size networks with more biologically realistic structural characteristics and 2nd-order non-linear regulatory functions. The simulator itself and the novel method used for generating a network structure with appropriate in- and out-degree distributions may also generalise easily to other types of network. GreenSim is available online at: http://syntilect.com/cgf/pubs:software	biological network;directed graph;gene regulatory network;machine learning;nonlinear system;simulation	Christopher Fogelberg;Vasile Palade	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.105	biological network;computer science;bioinformatics;machine learning;data mining;artificial neural network	HPC	6.7000549533865525	-58.52595422708517	9647
7c0651dba46b915a9fbb700e4e5c9d72cb0a6672	immersive visualization with automated collision detection for radiotherapy treatment planning		Intensity modulated radiotherapy (IMRT) is a technique for treating cancer tumours using external delivery of radiation. To create a treatment plan the directions of the external radiation beams (typically 5 to 9) need to be specified. Normally the beams are all coplanar due to the added complexity of planning and patient set-up for non-coplanar beams. RTStar provides a virtual environment of a radiotherapy (RT) treatment room that provides a range of views and visualizations that aid a treatment planner to choose non-coplanar beam directions efficiently. RTStar also automatically warns the planner when a collision would occur during patient set-up. A study was conducted on 8 prostate IMRT cancer patients using RTStar to create RT plans using non-coplanar beams. The study demonstrated that these IMRT prostate plans with non-coplanar beams had a dosimetric advantage over their coplanar conterparts.		James W. Ward;R. Phillips;Thomas J Williams;C. Shang;L. Page;C. Prest;Andy W. Beavis	2007	Studies in health technology and informatics		simulation;visualization;collision detection;radiation therapy;treatment room;radiation treatment planning;medicine	HCI	39.22231892232266	-85.78081619667009	9653
cb3709a0b1018f8230387efd36e27c01cb16dd52	modeling internal radiation therapy	modeling technique;treatment effect;radiation therapy;distance transform;article in monograph or in proceedings;fast exact euclidean distance	A new technique is described to model (internal) radiation therapy. It is founded on morphological processing, in particular distance transforms. Its formal basis is presented as well as its implementation via the Fast Exact Euclidean Distance (FEED) transform. Its use for all variations of internal radiation therapy is described. In a benchmark trial, FEED proved to be truly exact as well as faster than a comparable technique. These features can be of crucial importance in radiation therapy as the balance between maximization of treatment effect and doses that cause unwanted damage to healthy tissue is fragile. This balance can be secured using the modeling technique presented here.	benchmark (computing);distance transform;euclidean distance;expectation–maximization algorithm	Egon L. van den Broek;Theo E. Schouten	2011			radiation therapy;simulation;medicine;mathematics;distance transform;average treatment effect;algorithm	DB	39.91222018086064	-78.80757670478907	9655
4686cf4ffea94582e70de42a773aa56b3314965a	mutual information, fisher information, and efficient coding	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Fisher information is generally believed to represent a lower bound on mutual information (Brunel & Nadal, 1998), a result that is frequently used in the assessment of neural coding efficiency. However, we demonstrate that the relation between these two quantities is more nuanced than previously thought. For example, we find that in the small noise regime, Fisher information actually provides an upper bound on mutual information. Generally our results show that it is more appropriate to consider Fisher information as an approximation rather than a bound on mutual information. We analytically derive the correspondence between the two quantities and the conditions under which the approximation is good. Our results have implications for neural coding theories and the link between neural population coding and psychophysically measurable behavior. Specifically, they allow us to formulate the efficient coding problem of maximizing mutual information between a stimulus variable and the response of a neural population in terms of Fisher information. We derive a signature of efficient coding expressed as the correspondence between the population Fisher information and the distribution of the stimulus variable. The signature is more general than previously proposed solutions that rely on specific assumptions about the neural tuning characteristics. We demonstrate that it can explain measured tuning characteristics of cortical neural populations that do not agree with previous models of efficient coding.	algorithmic efficiency;approximation;fisher information;mutual information;neural coding;neural ensemble;neural oscillation;population;quantity	Xue-Xin Wei;Alan A. Stocker	2016	Neural Computation	10.1162/NECO_a_00804	variation of information;text mining;medical research;computer science;multivariate mutual information;data science;machine learning;data mining;mathematics;interaction information;information retrieval;statistics;pointwise mutual information	ML	22.445820875839594	-72.23227577478043	9658
fc112f916c8e65f60b5aef809b24c8b1b703ffa3	importance of rna secondary structure information for yeast donor and acceptor splice site predictions by neural networks	secondary structure prediction;negative control;levenberg marquardt;saccharomyces cerevisiae;rna secondary structure;nucleotides;positive affect;splice site prediction;backpropagation;rna structure;neural net;secondary structure;gene prediction;cross validation;correlation coefficient;neural network	Previously, Patterson et al. showed that mRNA structure information aids splice site prediction in human genes [Patterson, D.J., Yasuhara, K., Ruzzo, W.L., 2002. Pre-mRNA secondary structure prediction aids splice site prediction. Pac. Symp. Biocomput. 7, 223-234]. Here, we have attempted to predict splice sites in selected genes of Saccharomyces cerevisiae using the information obtained from the secondary structures of corresponding mRNAs. From Ares database, 154 genes were selected and their structures were predicted by Mfold. We selected a 20-nucleotide window around each site, each containing 4 nucleotides in the exon region. Based on whether the nucleotide is in a stem or not, the conventional four-letter nucleotide alphabet was translated into an eight-letter alphabet. Two different three-layer-based perceptron neural networks were devised to predict the 5' and 3' splice sites. In case of 5' site determination, a network with 3 neurons at the hidden layer was chosen, while in case of 3' site 20 neurons acted more efficiently. Both neural nets were trained applying Levenberg-Marquardt backpropagation method, using half of the available genes as training inputs and the other half for testing and cross-validations. Sequences with GUs and AGs non-sites were used as negative controls. The correlation coefficients in the predictions of 5' and 3' splice sites using eight-letter alphabet were 98.0% and 69.6%, respectively, while these values were 89.3% and 57.1% when four-letter alphabet is applied. Our results suggest that considering the secondary structure of mRNA molecules positively affects both donor and acceptor site predictions by increasing the capacity of neural networks in learning the patterns.		Sayed-Amir Marashi;Hani Goodarzi;Mehdi Sadeghi;Changiz Eslahchi;Hamid Pezeshk	2006	Computational biology and chemistry	10.1016/j.compbiolchem.2005.10.009	biology;nucleic acid structure;nucleotide;levenberg–marquardt algorithm;computer science;bioinformatics;backpropagation;machine learning;nucleic acid secondary structure;genetics;gene prediction;artificial neural network;cross-validation;scientific control;protein secondary structure;affect	ML	9.84533931913668	-56.303610164133545	9679
f60c7d3e3570e1cd5e08f2ae15c11ceb664d1437	graph-partitioned spatial priors for functional magnetic resonance images	sensitivity and specificity;resonance images;models neurological;high resolution;high resolution functional magnetic;imaging three dimensional;spatial coherence;cuts;segmentation;euclidean distance;functional response;three dimensional;model comparison;algorithm;image enhancement;comparative modeling;graph partitioning;dimensionality reduction;brain mr images;brain mapping;image interpretation computer assisted;expectation maximization;fmri analysis;general linear models;block diagonalization;functional magnetic resonance images;magnetic resonance imaging;reproducibility of results;normalization;gaussian kernel;phantoms imaging;high resolution functional magnetic resonance images;evoked potentials visual;general linear model;algorithms;humans;weighted graph;subtraction technique;graph laplacian;mixture models;diffusion based spatial priors;computer simulation;information storage and retrieval;visual cortex;inference;spatial model;covariance matrix	Spatial models of functional magnetic resonance imaging (fMRI) data allow one to estimate the spatial smoothness of general linear model (GLM) parameters and eschew pre-process smoothing of data entailed by conventional mass-univariate analyses. Recently diffusion-based spatial priors [Harrison, L.M., Penny, W., Daunizeau, J., and Friston, K.J. (2008). Diffusion-based spatial priors for functional magnetic resonance images. NeuroImage.] were proposed, which provide a way to formulate an adaptive spatial basis, where the diffusion kernel of a weighted graph-Laplacian (WGL) is used as the prior covariance matrix over GLM parameters. An advantage of these is that they can be used to relax the assumption of isotropy and stationarity implicit in smoothing data with a fixed Gaussian kernel. The limitation of diffusion-based models is purely computational, due to the large number of voxels in a brain volume. One solution is to partition a brain volume into slices, using a spatial model for each slice. This reduces computational burden by approximating the full WGL with a block diagonal form, where each block can be analysed separately. While fMRI data are collected in slices, the functional structures exhibiting spatial coherence and continuity are generally three-dimensional, calling for a more informed partition. We address this using the graph-Laplacian to divide a brain volume into sub-graphs, whose shape can be arbitrary. Their shape depends crucially on edge weights of the graph, which can be based on the Euclidean distance between voxels (isotropic) or on GLM parameters (anisotropic) encoding functional responses. The result is an approximation the full WGL that retains its 3D form and also has potential for parallelism. We applied the method to high-resolution (1 mm(3)) fMRI data and compared models where a volume was divided into either slices or graph-partitions. Models were optimized using Expectation-Maximization and the approximate log-evidence computed to compare these different ways to partition a spatial prior. The high-resolution fMRI data presented here had greatest evidence for the graph partitioned anisotropic model, which was best able to preserve fine functional detail.	approximation algorithm;coherence (physics);computation;diffusion weighted imaging;euclidean distance;exhibits as topic;expectation–maximization algorithm;general linear model;generalized linear model;graph - visual representation;image resolution;kernel;laplacian matrix;magnetic resonance imaging;normal statistical distribution;parallel computing;preprocessor;scott continuity;smoothing (statistical technique);stationary process;voxel;wgl (api);fmri	Lee M. Harrison;William D. Penny;Guillaume Flandin;Christian C. Ruff;Nikolaus Weiskopf;Karl J. Friston	2008	NeuroImage	10.1016/j.neuroimage.2008.08.012	computer simulation;functional response;three-dimensional space;computer vision;covariance matrix;mathematical optimization;homology modeling;image resolution;laplacian matrix;expectation–maximization algorithm;graph partition;magnetic resonance imaging;machine learning;normalization;generalized linear model;mixture model;euclidean distance;mathematics;brain mapping;segmentation;gaussian function;statistics;dimensionality reduction;general linear model	ML	48.79023497315494	-77.93704914427916	9688
245e8c0c0a60ae4ec834e573728a44ca6ef845dc	a gpu accelerated moving mesh correspondence algorithm with applications to rv segmentation	moving mesh correspondence algorithm serial image registration image concatenation approach parallel compute unified device architecture gpu computing graphics processing unit mri image sequence magnetic resonance imaging cardiac right ventricle delineation delineate organ parallel nonrigid registration algorithm right ventricle segmentation;image processing computer assisted;graphics processing units image registration image segmentation parallel processing algorithm design and analysis optimization magnetic resonance imaging;heart ventricles;magnetic resonance imaging gpu computing image registration cardiac functional analysis segmentation moving mesh;magnetic resonance imaging;algorithms;humans;medical image processing biomedical mri cardiology graphics processing units image registration	This study proposes a parallel nonrigid registration algorithm to obtain point correspondence between a sequence of images. Several recent studies have shown that computation of point correspondence is an excellent way to delineate organs from a sequence of images, for example, delineation of cardiac right ventricle (RV) from a series of magnetic resonance (MR) images. However, nonrigid registration algorithms involve optimization of similarity functions, and are therefore, computationally expensive. We propose Graphics Processing Unit (GPU) computing to accelerate the algorithm. The proposed approach consists of two parallelization components: 1) parallel Compute Unified Device Architecture (CUDA) version of the non-rigid registration algorithm; and 2) application of an image concatenation approach to further parallelize the algorithm. The proposed approach was evaluated over a data set of 16 subjects and took an average of 4.36 seconds to segment a sequence of 19 MR images, a significant performance improvement over serial image registration approach.	algorithm;analysis of algorithms;autostereogram;cuda;computation (action);concatenation;graphics processing unit;heart ventricle;image registration;mathematical optimization;muscle rigidity;organ;parallel computing;residual volume;resonance;right ventricular structure;biologic segmentation;registration - actclass	Kumaradevan Punithakumar;Michelle Noga;Pierre Boulanger	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319322	computer vision;radiology;medicine;image processing;computer science;theoretical computer science;magnetic resonance imaging;computer graphics (images)	Vision	46.29698119179183	-80.96189261477339	9692
0a9bf8645f2f727dbd75fb2a29596204411711e2	evidence of a pathway of reduction in bacteria: reduced quantities of restriction sites impact trna activity in a trial set	palindromic avoidance;trna;pathway of reduction	Occurring naturally along the genomes of many viruses and other pathogens, short palindromic restriction sites (<14bps) are often exploited by bacterial restriction enzymes as autoimmune defenses to end pathogen threats. These motifs may also appear in the host's genome where they are methylated so as not to attract restriction enzymes to the host's genetic material. Since these motifs in the host's genome may pose a significant danger, it is likely that their numbers have been reduced due to possible failures of methylation during evolutionary time.  These palindromes are composed of bases likely containing information relating to codons used for protein translation. If palindromes are reduced in the genome, then its sequence composition making up the codons may also be found in reduced quantities. Furthermore, during translation codons are associated with tRNAs for protein fabrication which may also occur in reduced numbers.  We suggest that a pathway of reduction that can be followed from the onset of these missing palindromes to the reduction (or absence) of specific tRNAs correlated to the codons from the palindromes. To create evidence for this pathway, we studied the bacterial genomes of Bacillus subtilis, Escherichia coli, Haemophilus influenzae, Methanococcus jannaschii, Mycoplasma genitalium, Synechocystis sp. and Marchantia polymorpha. Across these organisms, we applied statistical data from reduced palindromic populations (biological and non-relevant words) to regression models and performed an analysis of genomic tRNA presence from their compositions. We illustrate a pathway of reduction that extends from palindromes to tRNAs which may follow from evolutionary pressures concerning restriction site handling.	gene regulatory network;machine translation;mycoplasma genitalium;onset (audio);population;reduction (complexity)	Oliver Bonham-Carter;Lotfollah Najjar;Dhundy Bastola	2013		10.1145/2506583.2512365	biology;transfer rna;bioinformatics;genetics	Comp.	5.309508587780856	-62.909865395541004	9699
193dfff6c6d9ef5306c9f9d72ea746ae011ff1c2	adaptive spatial filters with predefined region of interest for eeg based brain-computer-interfaces	brain computer interface;common spatial pattern;electric field;left handed;motor cortex;region of interest;spatial filtering	The performance of EEG-based Brain-Computer-Interfaces (BCIs) critically depends on the extraction of features from the EEG carrying information relevant for the classification of different mental states. For BCIs employing imaginary movements of different limbs, the method of Common Spatial Patterns (CSP) has been shown to achieve excellent classification results. The CSP-algorithm however suffers from a lack of robustness, requiring training data without artifacts for good performance. To overcome this lack of robustness, we propose an adaptive spatial filter that replaces the training data in the CSP approach by a-priori information. More specifically, we design an adaptive spatial filter that maximizes the ratio of the variance of the electric field originating in a predefined region of interest (ROI) and the overall variance of the measured EEG. Since it is known that the component of the EEG used for discriminating imaginary movements originates in the motor cortex, we design two adaptive spatial filters with the ROIs centered in the hand areas of the left and right motor cortex. We then use these to classify EEG data recorded during imaginary movements of the right and left hand of three subjects, and show that the adaptive spatial filters outperform the CSP-algorithm, enabling classification rates of up to 94.7 % without artifact rejection.	algorithm;electroencephalography;imaginary time;mental state;region of interest;rejection sampling	Moritz Grosse-Wentrup;Klaus Gramann;Martin Buss	2006			brain–computer interface;computer vision;common spatial pattern;artificial intelligence;electric field;machine learning;spatial filter;region of interest	ML	14.998501846738707	-92.85557469283397	9702
0a20f1cb76d569bdf66f7b766360bceaa779bd58	remote sensing image representation based on hierarchical histogram propagation	remote sensing image;geophysical image processing;object oriented methods;image segmentation;global low level extraction approaches remote sensing image hierarchical histogram propagation remote sensing technologies image segmentation algorithms target object delineating geographic object based applications multiscale segmentation multiscale classification;image classification;feature extraction histograms visualization remote sensing image segmentation indexes dictionaries;geophysics computing;geobia histogram propagation image representation multiscale analysis;feature extraction;remote sensing;remote sensing feature extraction geophysical image processing geophysics computing image classification image segmentation object oriented methods	Many methods have been recently proposed to deal with the large amount of data provided by high-resolution remote sensing technologies. Several of these methods rely on the use of image segmentation algorithms for delineating target objects. However, a common issue in geographic object-based applications is the definition of the appropriate data representation scale, a problem that can be addressed by exploiting multiscale segmentation. The use of multiple scales, however, raises new challenges related to the definition of effective and efficient mechanisms for extracting features. In this paper, we address the problem of extracting histogram-based features from a hierarchy of regions for multiscale classification. The strategy, called H-Propagation, exploits the existing relationships among regions in a hierarchy to iteratively propagate features along multiple scales. The proposed method speeds up the feature extraction process and yields good results when compared with global low-level extraction approaches.	algorithm;data (computing);exploit (computer security);feature extraction;geographic information system;high- and low-level;image histogram;image resolution;image segmentation;object-based language;segmentation fault;software propagation	Jefersson Alex dos Santos;Otávio Augusto Bizetto Penatti;Ricardo da Silva Torres;Philippe Henri Gosselin;Sylvie Philipp-Foliguet;Alexandre X. Falcão	2013	2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS	10.1109/IGARSS.2013.6723452	image texture;computer vision;contextual image classification;feature detection;feature extraction;computer science;machine learning;segmentation-based object categorization;pattern recognition;data mining;region growing;image segmentation;image fusion;scale-space segmentation;remote sensing;image histogram	Robotics	47.303431352645795	-67.46154406622136	9713
edb402dd19c59637ba3744c5b21fa46ad2051e0a	a systematic framework to derive n-glycan biosynthesis process and the automated construction of glycosylation networks	computational biology bioinformatics;algorithms;computer appl in life sciences;article;microarrays;bioinformatics	Abnormalities in glycan biosynthesis have been conclusively related to various diseases, whereas the complexity of the glycosylation process has impeded the quantitative analysis of biochemical experimental data for the identification of glycoforms contributing to disease. To overcome this limitation, the automatic construction of glycosylation reaction networks in silico is a critical step. In this paper, a framework K2014 is developed to automatically construct N-glycosylation networks in MATLAB with the involvement of the 27 most-known enzyme reaction rules of 22 enzymes, as an extension of previous model KB2005. A toolbox named Glycosylation Network Analysis Toolbox (GNAT) is applied to define network properties systematically, including linkages, stereochemical specificity and reaction conditions of enzymes. Our network shows a strong ability to predict a wider range of glycans produced by the enzymes encountered in the Golgi Apparatus in human cell expression systems. Our results demonstrate a better understanding of the underlying glycosylation process and the potential of systems glycobiology tools for analyzing conventional biochemical or mass spectrometry-based experimental data quantitatively in a more realistic and practical way.	anabolism;apollonian network;contribution;gnat;glycomics;leukemoid reaction;matlab;mass spectrometry;name;polysaccharides;rule (guideline);sensitivity and specificity;glycosylation;polysaccharide biosynthetic process	Wenpin Hou;Yushan Qiu;Nobuyuki Hashimoto;Wai-Ki Ching;Kiyoko F. Aoki-Kinoshita	2016		10.1186/s12859-016-1094-6	biology;biochemistry;dna microarray;computer science;bioinformatics	ML	6.639806640889568	-59.411238834480564	9714
88e408742b6a0686a632dbb8b519d3f8e339acec	study of the linguistic variables of heart rate variability using fuzzy entropy	frequency bands;heart rate variability entropy pragmatics frequency domain analysis fuzzy sets biomedical measurements hafnium;pragmatics;variables sd1;biomedical measurements;ecg;linguistic variable;fuzzy set;nn50;heart rate variability;variables sd2;very low frequency;fuzzy number;one minute electrocardiogram;low frequency;frequency domain analysis;time frequency;sdnn;pnn50;fuzzy entropies heart rate variability fuzzy sets;frequency 0 15 hz to 0 4 hz linguistic variables heart rate variability fuzziness values three age groups fuzzy entropy function one minute electrocardiogram ecg short term hrv measurement time domain analysis sdnn hr stdhr rmssd nn50 pnn50 triangle tinn frequency domain analysis spectral analysis frequency bands coronary disease variables sd1 variables sd2 nonlinear domain linguistic terms trapezoidal fuzzy numbers triangular fuzzy numbers lf hf frequency 0 hz to 0 04 hz frequency 0 04 hz to 0 15 hz;nonlinear domain;lf;tinn;satisfiability;fuzzy set theory;fuzzy sets;stdhr;time domain analysis;time domain analysis diseases electrocardiography entropy frequency domain analysis fuzzy set theory linguistics medical signal processing spectral analysis;frequency 0 04 hz to 0 15 hz;electrocardiography;total power;triangular fuzzy numbers;triangle;hr;hf;linguistic terms;fuzziness values;coronary disease;frequency 0 15 hz to 0 4 hz;diseases;fuzzy entropy function;time domain;short term hrv measurement;linguistic variables;frequency 0 hz to 0 04 hz;entropy;three age groups;spectral analysis;fuzzy entropies;frequency domain;rmssd;high frequency;medical signal processing;age groups;trapezoidal fuzzy numbers;electrocardiogram;hafnium;linguistics	The aim of this investigation was to determine the fuzziness values of the variables that represent heart rate variability (HRV) for three age groups, using the fuzzy entropy function. We used one hundred and four waves taken from one-minute electrocardiogram (ECG) of males subjects 20-70 years old, this being a short-term HRV measure. In the time domain, the values of the variables SDNN, HR, STDHR, RMSSD, NN50, PNN50, Triangle, TINN, were calculated. In the frequency domain, spectral analysis of continuous power was conducted, quantifying the following powers: very low frequency (VLF,0-0.04 Hz), low frequency (LF, 0.04 to 0.15 Hz), high frequency (HF, 0.15-0.4 Hz), and the ratio of LF to HF (LF / HF). The existing literature on the subject states, that the three frequency bands into which the total power is divided are similar regardless the age, sex and the presence or absence of coronary disease. In the nonlinear domain the values of the variables SD1 and SD2, which describe short and long-term variability were calculated. Measures of the selected patients were taken in the evening after 10 minutes rest. Then the linguistic variables and linguistic terms were defined and such variables were fuzzyfied using triangular and trapezoidal fuzzy numbers. All the variables values in the proposed conditions, satisfy the property that HRV decreases with age. Then the fuzziness of the linguistic terms was analyzed using the continuous fuzzy entropy thus concluding that the variables in the time, frequency and nonlinear domains: SDNN, RMSSD, NN50, PNN50, LF, HF, SD1, SD2 corresponding to the first and second age group have the greatest disorder.	frequency band;fuzzy concept;fuzzy number;heart rate variability;nonlinear system;spectrum analyzer	F. Nuno Almirantearena;F. Clara;P. Burillo Lopez	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6233719	computer science;artificial intelligence;calculus;mathematics;fuzzy set;algorithm;pragmatics	SE	17.479007222983316	-86.28487485308241	9724
690ef94605f63fd0094beff056521b117430ee6c	designing a smart card face verification system		This thesis describes a face verification system that is smart-card-based. The objectives were to identify the key parameters that affect the design of such a system, to investigate die general optimisation problem and test its robustness when each key parameter is optimised. Some of these parameters have been coarsely investigated in the literature in the context of the general face recognition problem. However, the previous work only partially fulfilled the requirements of a smart-card-based system, in which the severe engineering constraints and limitations imposed by smart cards have to be taken into account in the overall design process. To address these problems on the proposed fully localised architecture of the smart card face verification system (SCFVS), the work starts with the selection of the client specific linear discriminant analysis (CS-LDA) algorithm, suitable to be ported to the target platform on which the biometric process can run. Then the main functional parts of the system are presented: face image geometric alignment, photometric normalisation, feature extraction, and on-card verification. Each part consists of a series of basic steps, where the role of each step is fixed. However, the algorithm is systematically varied in some steps to investigate the effect on system performance, and system complexity in terms of speed and memory management. Two major problems have been considered. The first problem are the restrictions that both face verification and smart card technology impose and the second is the extreme complexity of the system, in terms of the number of processing stages and system design parameters. In the simplified search procedure adopted, a number of parameters has been selected out of the complete parameter set involved in a generic SCFVS. This set was recommended by previous main-frame based studies, and deemed to provide acceptable performance. System optimisation in the context of smart card implementation has been conducted starting from those parameters involved in the pre-processing stage of the system, and then those involved in the remaining stages. A joint optimisation framework of the key parameters can also be adopted, assuming that then- effect is independent. Experimental results obtained on a number of publicly available face databases (used to evaluate the system performance) show the significant benefits of this design both in terms of performance and system speed. The different results achieved on different databases indicate that optimum parameters of the system are, to a certain extent, training database dependent.	smart card	Thirimachos Bourlai	2006			robustness (computer science);real-time computing;memory management;smart card;data mining;systems design;architecture;feature extraction;linear discriminant analysis;design process;engineering	AI	33.17321304975064	-57.73341555298929	9729
a9f87cf87c4e8e1dd54154eeb1fe8d48f38a9839	estimation and filtering of potential protein-protein docking positions	software;prediccion;investigation method;evaluation performance;proteine;performance evaluation;methode etude;logiciel;interaction moleculaire;computerized processing;tratamiento informatico;molecular interaction;evaluacion prestacion;software systems;algorithme;algorithm;interaccion molecular;metodo estudio;protein protein docking;logicial;proteina;protein;traitement informatique;prediction;free energy;algoritmo	MOTIVATION Software systems predicting automatically whether and how two proteins may interact are highly desirable, both for understanding biological processes and for the rational design of new proteins. As a part of a future complete solution to this problem, a bundle of programs is presented designed (i) to estimate initial docking positions for a given pair of docking candidates, (ii) to adjust them, and (iii) to filter them, thus preparing more detailed computations of free energies.   RESULTS The system is evaluated on a test set of 51 co-crystallized complexes aiming at redocking the subunits. It works completely automatically and the evaluation is performed using one single set of parameters for all complexes in the test set. The number of solutions is fixed to 50 positions with a median CPU time of 26 min. For 30 complexes, these contain a near-correct solution with root mean square deviation ( RMSD ) </=5.0 A, which is ranked first in five cases. For all complexes, the best solution is scored on rank 16 as the worst case, and has a median RMSD of 4.3 A. Alternatively to this initial estimation of docking positions, a global sampling of rotations was tested. Whereas this yields top-ranked solutions with RMSD </=3.0 A for all 51 complexes, the median CPU time increases to 11 h. This shows that this blind sampling is not feasible for most applications.   AVAILABILITY The system and its components are available on request from the authors.   CONTACT friedric@techfak.uni-bielefeld or posch@techfak.uni-bielefeld.de	best, worst and average case;boat dock;cpu (central processing unit of computer system);central processing unit;computation;crystal structure;docking (molecular);energy, physics;macromolecular docking;maxima and minima;mean squared error;plant roots;preparation;sampling (signal processing);sampling - surgical action;score;software system;solutions;test set	Friedrich Ackermann;Grit Herrmann;Stefan Posch;Gerhard Sagerer	1998	Bioinformatics	10.1093/bioinformatics/14.2.196	simulation;prediction;artificial intelligence;algorithm;statistics;software system	Comp.	-3.5225921916618606	-55.370009726574864	9773
3ed61941878133355ed256ee794fe4f3502136f8	on automatic threshold selection for polygonal approximations of digital curves	analisis imagen;image numerique;time complexity;percolacion;scale space;percolation;theory;estructura datos;percolation theory;imagen numerica;teoria;image analysis;set disjoint datastructure;structure donnee;polygonal approximation;digital image;analyse image;data structure;scale space analysis;theorie	"""Polygonal approximation is a very common representation of digital curves. A polygonal approximation depends on a parameter e, which is the error value. In this paper we present a method for an automatic selection of the error value, e. Let F ~) be a polygonal approximation of the original curve F, with an error value e. We define a set of function, {Ns(e) }s~s, such that for a given value of s, Ns(e) is the number of (~) edges that contain at least s vertices in F . The time complexity for computing the set of functions {N~(e) }s~ is almost linear in n, the number of vertices in F. In this paper we analyse the N~(e) graph, and show that Ior adequate values ofs a wide plateau is expected to appear at the top of the graph, This plateau corresponds to a stable state in the multi-scale representation of {F~}~E We show that the functions {N~(e)}~ are a statistical representation of some kind of scale-space image. Copyright © 1996 Pattern Recognition Society. Published by Elsevier Science Ltd. Image analysis Digital curves Automatic threshold selection Polygonal approximations Scale-space analysis Percolation theory Set Disjoint datastructure 1. I N T R O D U C T I O N Polygonal approximat ion is a common representation of a digital curve. It preserves local properties of the curve, and it is simple for computat ion and for maintenance. Polygonal approximat ion depends on a parameter e, which is an error value. Since polygonal approximat ion is usually an early stage in image analysis tasks, it is desired to automatical ly select the appropriate value of ~. The """"appropriate value"""" is application dependent, but for many applications an ideal selection of e is the maximal value that yields a polygonal approximat ion that is visually hardly different from the original curve. In order to explain the not ion of """"visually similar"""", let us observe a sequence of polygonal approximations of a given digital curve, for increasing values ore. The specific algorithm for polygonal approximat ion is the IPE of Pikaz and Dinstein (1) (see Appendix A), though the concept is independent of the chosen algorithm. Figure 1 presents such a sequence of polygonal approximations. Let us denote by F the original digital curve, that is, the input curve. Also we denote by F (~) the polygonal approximat ion with an error value of e. Observing polygonal approximat ion for increasing values of e leads to the conclusion that there is a range of values of ~ for which the polygonal approximations are visually hardly differ from the original curve, F. The following phenomena are observed: (1) The number of vertices of F (~) exponentially decreases as a function ofe. This was also observed by * Author to whom all correspondence should be addressed. Bengtsson and Eklundh. (2) That is, from some value of e, a plateau appears in the graph. Bengtsson and Eklundh also noted that if feature points (such as inflection points or corners) are counted instead of the number of vertices, the plateau is much more conspicuous. This stems from the fact that feature points hold more information than the vertices of the curve. The observed plateau is analogous to the stable scale of Witkin.I 3) (2) Fo r some values of s, the graph that depicts the number of edges of F ("""") with length not less than s (as a function of e), has a conspicuous plateau at the top. This plateau corresponds to a stable state. The graph itself is usually unimodal, up to small deviations. In the view of the second observation, let us denote by N~(e) the number of edges with length not less than s, in the polygonal curve F (""""). We will show in this paper that for a given value of s, the graph of the function Ns(e ) is usually expected to be convex. The higher the value ofs is, the less smooth is the N~(e) graph, and the deviations from convexity are more substantial. For adequate values of s, the graph of Ns(e ) has a wide plateau. This plateau corresponds to a stable state. Values ofe that correspond to the plateau are adequate values for the parameter e of the polygonal approximation. The plateau is expected to be at the top of the graph, and it is explained using ideas that are also found in percolation theory. The presented method does not depend on the particular algori thm for the polygonal approximation. Fo r the demonstrations, we choose the IPE algori thm of Pikaz and DinsteinJ ~) The IPE algorithm iteratively eliminates vertices from the current approximation"""	algorithm;approximation;convex function;data structure;fo (complexity);image analysis;maximal set;pattern recognition;percolation theory;scale space;time complexity	Arie Pikaz;Amir Averbuch	1996	Pattern Recognition	10.1016/0031-3203(96)00037-4	time complexity;computer vision;combinatorics;discrete mathematics;scale space;image analysis;data structure;computer science;polygonal chain;percolation;mathematics;geometry;theory;digital image;percolation theory	Theory	47.47294548315861	-63.30567515890941	9787
f8e922d7460817bc20c98caf38b2ddff8bc5b898	visualization of neural dti vector fields using line integral convolution	diffusion tensor images;data visualization;grey matter;line integral convolution;vector field;correspondence principle;diffusion tensor	  Diffusion Tensor Imaging (DTI) provides voxel-wise information related to the local diffusion anisotropy. Recent research  efforts have centered around the use of this information to infer the direction of local fiber bundles. Calculation of the  diffusion tensor and corresponding principle diffusion direction voxel-wise throughout the imaged volume permits the use of  tracking algorithms to reconstruct the fiber-bundle pathways. These algorithms are typically based on line propagation and  provide results that visually resemble anatomical fiber dissections. Despite the success of these methods, they suffer several  limitations, particularly within regions of decreased anisotropy, such as areas of grey matter and fiber crossing or branching.  In this paper we present an alternative method of DTI data visualization, line integral convolution (LIC), which has several  advantages over existing techniques, particularly, the ability to deal with noise and singularities within the vector field  and areas of low anisotropy.    	line integral convolution	Sean C. L. Deoni;Brian K. Rutt;Terry M. Peters	2003		10.1007/978-3-540-39903-2_26	diffusion mri;mathematical analysis;vector field;correspondence principle;topology;tensor field;line integral convolution;cartesian tensor;mathematics;geometry;data visualization	NLP	48.423973821293714	-78.34064889149771	9789
a879cd12da6b52092bfd4a603235018a02ee192d	locomotion activity recognition using stacked denoising autoencoders		Locomotion activity recognition (LAR) is important for a number of applications, such as indoor localization, fitness tracking, and aged care. Existing methods usually use handcrafted features, which requires expert knowledge and is laborious, and the achieved result might still be suboptimal. To relieve the burden of designing and selecting features, we propose a deep learning method for LAR by using data from multiple sensors available on most smart devices. Experimental results show that the proposed method, which learns useful features automatically, outperforms conventional classifiers that require the hand-engineering of features. We also show that the combination of sensor data from four sensors (accelerometer, gyroscope, magnetometer, and barometer) achieves a higher accuracy than other combinations or individual sensors.	activity recognition;activity tracker;deep learning;feature engineering;gyroscope;noise reduction;sensor;smart device	Fuqiang Gu;Kourosh Khoshelham;Shahrokh Valaee;Jianga Shang;Rui Zhang	2018	IEEE Internet of Things Journal	10.1109/JIOT.2018.2823084	computer vision;distributed computing;noise reduction;accelerometer;activity recognition;computer science;feature extraction;deep learning;intelligent sensor;artificial intelligence;gyroscope	Mobile	4.197874457942398	-84.61692933275955	9816
31dc626621bd362306ade52733645a05a30b53ce	the lux score: a metric for lipidome homology	lipids;animals;isomers;yeast;isomerism;models biological;fatty acids;plos computational biology;models molecular;drosophila;lipid analysis;drosophila melanogaster;lipid structure;algorithms;sequence alignment;computational biology;organ specificity	A lipidome is the set of lipids in a given organism, cell or cell compartment and this set reflects the organism's synthetic pathways and interactions with its environment. Recently, lipidomes of biological model organisms and cell lines were published and the number of functional studies of lipids is increasing. In this study we propose a homology metric that can quantify systematic differences in the composition of a lipidome. Algorithms were developed to 1. consistently convert lipids structure into SMILES, 2. determine structural similarity between molecular species and 3. describe a lipidome in a chemical space model. We tested lipid structure conversion and structure similarity metrics, in detail, using sets of isomeric ceramide molecules and chemically related phosphatidylinositols. Template-based SMILES showed the best properties for representing lipid-specific structural diversity. We also show that sequence analysis algorithms are best suited to calculate distances between such template-based SMILES and we adjudged the Levenshtein distance as best choice for quantifying structural changes. When all lipid molecules of the LIPIDMAPS structure database were mapped in chemical space, they automatically formed clusters corresponding to conventional chemical families. Accordingly, we mapped a pair of lipidomes into the same chemical space and determined the degree of overlap by calculating the Hausdorff distance. We named this metric the 'Lipidome jUXtaposition (LUX) score'. First, we tested this approach for estimating the lipidome similarity on four yeast strains with known genetic alteration in fatty acid synthesis. We show that the LUX score reflects the genetic relationship and growth temperature better than conventional methods although the score is based solely on lipid structures. Next, we applied this metric to high-throughput data of larval tissue lipidomes of Drosophila. This showed that the LUX score is sufficient to cluster tissues and determine the impact of nutritional changes in an unbiased manner, despite the limited information on the underlying structural diversity of each lipidome. This study is the first effort to define a lipidome homology metric based on structures that will enrich functional association of lipids in a similar manner to measures used in genetics. Finally, we discuss the significance of the LUX score to perform comparative lipidome studies across species borders.	anatomical compartments;body tissue;ceramides;chemical space;clinical use template;compartment of cell;cultured cell line;estimated;fatty acids;fatty acid biosynthetic process;hausdorff dimension;high-throughput computing;homologous gene;homology (biology);interaction;levenshtein distance;multi-compartment model;mutation abnormality;name;phosphatidylinositols;scientific publication;sequence analysis;simplified molecular input line entry specification;simplified molecular-input line-entry system;structural similarity;synthetic intelligence;throughput;algorithm;lux unit;mapped	Chakravarthy Marella;Andrew E. Torda;Dominik Schwudke	2015		10.1371/journal.pcbi.1004511	computational biology;biology;biochemistry;bioinformatics;sequence alignment;genetics	Comp.	4.200864815064048	-57.68121043522597	9834
b8c10648b01f181e70b1396eda1894fe272b7d49	a passive dsp approach to fetal movement detection for monitoring fetal health	fetal movement;nonlinear behavior passive dsp approach fetal movement detection fetal health monitoring fetal functional development solid state accelerometer abdomen root mean square time series time frequency analysis quadratic tf distribution nonstationary behavior;time series;detection;quadratic tfd;time series accelerometers health care medical signal processing patient monitoring;time frequency analysis accelerometer detection fetal movement quadratic tfd;accelerometer;accelerometers ultrasonic imaging time frequency analysis monitoring acceleration detectors;patient monitoring;accelerometers;time frequency analysis;medical signal processing;health care	Fetal movement can help clinicians understand fetal functional development. Active methods for fetal monitoring such as ultrasound are expensive and there are objections to their long term usage. This paper presents a passive approach for fetal monitoring which uses solid state accelerometers placed on the mother's abdomen for the collection of fetal movements. The proposed fetal movement detection is based on the root-mean-square (RMS) of time series. The detection performance is evaluated against real-time ultrasound imaging. A good detection rate of 80% and a positive predictive value of 77% were achieved based on the analysis of 4 subjects. Time-frequency (TF) analysis of fetal movement signals, using a number of quadratic TF distributions, has shown that some fetal movements are spectrally characterized by nonstationary and nonlinear behavior and that fetal activity is generally below 20 Hz. More data are needed for further TF analysis and future detections will depend on the outcome of this analysis.	medical ultrasound;nonlinear system;real-time clock;sensor;solid-state drive;time series	Mohamed Salah Khlif;Boualem Boashash;Siamak Layeghy;Taoufik Ben Jabeur;Paul B. Colditz;Christine East	2012	2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)	10.1109/ISSPA.2012.6310647	speech recognition;computer science;electrical engineering;accelerometer;statistics	Robotics	13.777708816070136	-87.50847372435186	9889
ef414f4997ea2b955283546c7268d4d418f8cda0	mc64: a web platform to test bioinformatics algorithms in a many-core architecture	third party tools;computational power;web browsers;analytical methodology;resource integration;algorithms;many core architecture;user interfaces;info eu repo semantics bookpart;bioinformatics	New analytical methodologies, like the so-called “next-generation sequencing” (NGS), allow the sequencing of full genomes with high speed and reduced price. Yet, such technologies generate huge amounts of data that demand large raw computational power. Many-core technologies can be exploited to overcome the involved bioinformatics bottleneck. Indeed, such hardware is currently in active development. We have developed parallel bioinformatics algorithms for many-core microprocessors containing 64 cores each. Thus, the MC64 web platform allows executing high-performance alignments (NeedlemanWunsch, Smith-Waterman and ClustalW) of long sequences. The MC64 platform can be accessed via web browsers, allowing easy resource integration into thirdparty tools. Furthermore, the results obtained from the MC64 include timeperformance statistics that can be compared with other platforms.	bioinformatics;clustalw/clustalx;communications satellite;computation;manycore processor;microprocessor;smith–waterman algorithm;webplatform	Francisco José Esteban;David Díaz;Pilar Hernández;Juan Antonio Caballero;Gabriel Dorado;Sergio Gálvez	2011		10.1007/978-3-642-19914-1_2	computer science;data mining;database;world wide web	HPC	-2.097552925814239	-54.75270945496941	9890
fed05ae36cbfa31d27d208ceb8d3a9a3e2e522a4	principal component analysis based colour scheme optimisation in eye fundus images - contrast enhancement for detection and evaluation of drusen in age related macular degeneration patients' follow up		Efficiency of the patient status monitoring in Age Related Macular Degeneration cases, based on evaluation of morphological properties eye fundus images, can be significantly increased by specific contrast enhancement in the images. Objects of interest drusen (focal deposits of extracellular debris located between the basal lamina of the retinal pigment epithelium and the inner collagenous layer of Bruch membrane) usually are represented by various intensity but the same unique color in the image. Construction of the optimal color scheme to increase the contrast of drusen can be realized by means of Principal Component Analysis, which transforms original RGB color representation into principal components space. The study demonstrates that proposed method can increase contrast-to-noise ratio of the drusen areas 10-fold or more.	basal (phylogenetics);contrast-to-noise ratio;eb-eye;focal (programming language);mathematical optimization;pigment;preprocessor;principal component analysis	Algimantas Krisciukaitis;Robertas Petrolis;Daiva Stanislovaitiene;Dalia Zaliuniene	2014		10.5220/0004910500660069	chemistry;macular degeneration;computer vision;artificial intelligence;principal component analysis;drusen;fundus (eye)	Vision	36.11609498486903	-75.9462947670022	9895
2362ef08d36b3f59eca2e56f47dff3f5bb56f5d5	segmentation-level fusion for iris recognition		This paper investigates the potential of fusion at normalisation/segmentation level prior to feature extraction. While there are several biometric fusion methods at data/feature level, score level and rank/decision level combining raw biometric signals, scores, or ranks/decisions, this type of fusion is still in its infancy. However, the increasing demand to allow for more relaxed and less invasive recording conditions, especially for on-the-move iris recognition, suggests to further investigate fusion at this very low level. This paper focuses on the approach of multi-segmentation fusion for iris biometric systems investigating the benefit of combining the segmentation result of multiple normalisation algorithms, using four methods from two different public iris toolkits (USIT, OSIRIS) on the public CASIA and IITD iris datasets. Evaluations based on recognition accuracy and ground truth segmentation data indicate high sensitivity with regards to the type of errors made by segmentation algorithms.	algorithm;biometric device;biometrics;feature extraction;global serializability;ground truth;iris recognition;list of toolkits	Peter Wild;Heinz Hofbauer;James M. Ferryman;Andreas Uhl	2015	2015 International Conference of the Biometrics Special Interest Group (BIOSIG)		computer vision;speech recognition;pattern recognition;iris recognition;scale-space segmentation	Vision	29.8646569094088	-62.117841981705816	9903
3be0c3086479491c56bb247a449e8ae2208e7df3	a novel approach based on computerized image analysis for traditional chinese medical diagnosis of the tongue	analisis imagen;informatica biomedical;biomedical data processing;analisis estadistico;analisis textura;implementation;medecine traditionnelle;analisis cuantitativo;medicina tradicional;informatique biomedicale;diagnostico;hombre;ejecucion;lengua;texture analysis;statistical analysis;traditional chinese medical diagnosis;analyse quantitative;chinese medicine;analyse statistique;human;clinical practice;tongue;quantitative analysis;multivariate statistics;image analysis;computerized image analysis;folk medicine;langue;medicina china;diagnosis;analyse image;analyse texture;medical diagnosis;medecine chinoise;homme;diagnostic	This research is aimed at building a computerized tongue examination system (CTES) based on computerized image analysis for the purpose of quantizing the tongue properties in traditional Chinese medical diagnosis. The chromatic algorithm is developed to identify the colors of the tongue and the thickness of its coating. The textural algorithm is used to detect the grimy coating. CTES is shown to be significantly consistent within itself with P > 0.05 using the Hotelling multivariate statistical test. The overall rate of correctness for CTES to identify the colors of tongue, verifying the thickness of its coating and detecting of any grimy coating exceeds 86%. Therefore, the CTES is helpful to provide the physicians a systematic and objective diagnostic standard for the tongue diagnosis in the clinical practice and research.		Chuang-Chien Chiu	2000	Computer methods and programs in biomedicine	10.1016/S0169-2607(99)00031-0	image analysis;medicine;computer science;artificial intelligence;medical diagnosis;statistics	AI	37.43309159400296	-74.73913196294903	9909
d855f8b7e39c22c1f4b3d760a66cb8e258e4cc8e	computing the differential characteristics of isointensity surfaces	three dimensional imaging;medical imagery;formation image tridimensionnelle;image processing;3d imaging;implementation;extraction forme;procesamiento imagen;courbure;parametrization;calculo automatico;traitement image;parametrizacion;computing;sintesis imagen;calcul automatique;image synthesis;ejecucion;image interpretation;isosurface;interpretacion imagen;medical image;extraccion forma;robustesse;imagerie medicale;superficie;curvatura;synthese image;robustness;curvature;surface;imageneria medical;interpretation image;formacion imagen tridimensional;parametric surface;pattern extraction;parametrisation;coordinate system;robustez;implicit function theorem	In this paper, we present a new method to compute the differential characteristics of isointensity surfaces from three-dimensional images. We show applications where those differentials properties are used to extract characteristic lines from 3D images, called crest lines. The crest lines extracted from different images of the same object are then registered to demonstrate the precision and robustness of our computation. Those experiments also show the direct correspondence between geometrical and anatomical features, for medical images. To compute the differential characteristics of surfaces, such as the principal curvatures, and directions, a traditional approach is to fit a parametric surface model to the 3D image and then to compute the differential characteristics of the surface in the local coordinate system. On the contrary, our method is based on the implicit representation of the surface, and the differential values of the isointensity surfaces are directly computed from the voxel image, without extracting any surface first. In our method, the principal curvatures and directions equations have been derived from the implicit functions theorem, leading to entirely new formulas, which make use of only the differentials of the 3D image, and which allow us to get rid of the problem of parametrizing the surfaces.		Jean-Philippe Thirion;Alexis Gourdon	1995	Computer Vision and Image Understanding	10.1006/cviu.1995.1015	parametrization;computer vision;image processing;computer science;mathematics;geometry;algorithm	Vision	50.39283417087626	-60.77776021693542	9911
14cf5fea35a2f7f5f3d17f9f3da36ee935f2889b	snap: workbench management tool for evolutionary population genetic analysis	population;gestion;bioinformatique;genetica poblacion;poblacion;management tool;genetique population;bioinformatica;management;bioinformatics;population genetics	UNLABELLED The reconstruction of population processes from DNA sequence variation requires the coordinated implementation of several coalescent-based methods, each bound by specific assumptions and limitations. In practice, the application of these coalescent-based methods for parameter estimation is difficult because they make strict assumptions that must be verified a priori and their parameter-rich nature makes the estimation of all model parameters very complex and computationally intensive. A further complication is their distribution as console applications that require the user to navigate through console menus or specify complex command-line arguments. To facilitate the implementation of these coalescent-based tools we developed SNAP Workbench, a Java program that manages and coordinates a series of programs. The workbench enhances population parameter estimation by ensuring that the assumptions and program limitations of each method are met and by providing a step-by-step methodology for examining population processes that integrates both summary-statistic methods and coalescent-based population genetic models.   AVAILABILITY SNAP Workbench is freely available at http://snap.cifr.ncsu.edu. The workbench and tools can be downloaded for Mac, Windows and Unix operating systems. Each package includes installation instructions, program documentation and a sample dataset.   SUPPLEMENTARY INFORMATION A description of system requirements and installation instructions can be found at http://snap.cifr.ncsu.edu.	command-line interface;console application;documentation generator;estimation theory;genetic algorithm;java programming language;microsoft windows;operating system;pierre robin syndrome;population parameter;requirement;silo (dataset);system requirements;unix;workbench	Eric W. Price;Ignazio Carbone	2005	Bioinformatics	10.1093/bioinformatics/bti003	biology;simulation;computer science;bioinformatics;data mining;population genetics;genetics;population	Comp.	-2.8178726844630493	-56.552447325532995	9912
3324103b4be85e2e3570e046a1e9521774adc955	the first lines of divergence in the bacteria domain were the hyperthermophilic organisms, the thermotogales and the aquificales, and not the mesophilic planctomycetales	hyperthermophilic ancestor of bacteria;high temperature;deep divergences;origin of life;protein concatenamers;luca;phylogenetic analysis	In order to establish whether the first lines of divergence in the Bacteria domain were the mesophilic or the hyperthermophilic organisms, we have performed a phylogenetic analysis on a concatenamer obtained from the fusion of 20 different proteins. The phylogenetic analysis carried out using five different methods has shown that, contrary to what is reported in the literature [Brochier, C., Philippe, H., 2002. A non-hyperthermophilic ancestor for Bacteria. Nature 417, 244], it was probably the hyperthermophilic organisms, the Thermotogales and the Aquificales, which were the first lines of divergence in the Bacteria domain, and not the mesophilic Planctomycetales. This strengthens the hypothesis that the last universal common ancestor might have been a hyperthermophilic 'organism' and that, more generally, life might have originated at high temperature.	fever;last universal common ancestor;order planctomycetales;phylogenetics;planctomycetales - phylum	Sacha Barion;Marco Franchi;Enzo Gallori;Massimo Di Giulio	2007	Bio Systems	10.1016/j.biosystems.2006.02.011	biology;last universal ancestor;abiogenesis;microbiology;ecology	Comp.	3.248154452965115	-62.613432759464686	9921
fad5f0b8a9afbf742c4a9d188f49cabba849e7ec	influence of single centers in a multicenter trial on robot-assisted therapy?	robots training protocols games clinical trials spinal cord injury;protocols;training;spinal cord injury;armin multicenter trial robot assisted therapy stroke disability neurorehabilitation;games;robots;patient treatment handicapped aids medical robotics patient rehabilitation;clinical trials	Stroke is one of the leading causes of disability, and advances in neurorehabilitation bring new therapy forms that aim to enhance recovery after stroke. In a multicenter clinical trial, we tested whether robot-assisted therapy of the arm with the therapy robot ARMin is superior to conventional therapy with regard to improvement in arm motor function. In this article, we describe differences in motor function gains among the four participating centers. Three centers showed a tendency favoring robotics whereas one center showed the converse. Results might be accidental considering the small and different number of patients within the centers. However, it indicates that not only study procedures but soft factors that are generally not taken into consideration when planning a multicenter study, such as therapeutic attitude or center differences, might influence the study outcome.	error-tolerant design;robot;robotics	Verena Klamroth-Marganska;Robert Riener	2014	5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics	10.1109/BIOROB.2014.6913831	medicine;physical medicine and rehabilitation;physical therapy;surgery	Robotics	12.841702177866466	-82.85605240076919	9928
67e5b069be9a13b07e26506bf8aabf77a11fdbe0	semantic web repositories for genomics data using the exframe platform	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;data mining and knowledge discovery;journal article;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	BACKGROUND With the advent of inexpensive assay technologies, there has been an unprecedented growth in genomics data as well as the number of databases in which it is stored. In these databases, sample annotation using ontologies and controlled vocabularies is becoming more common. However, the annotation is rarely available as Linked Data, in a machine-readable format, or for standardized queries using SPARQL. This makes large-scale reuse, or integration with other knowledge bases very difficult.   METHODS To address this challenge, we have developed the second generation of our eXframe platform, a reusable framework for creating online repositories of genomics experiments. This second generation model now publishes Semantic Web data. To accomplish this, we created an experiment model that covers provenance, citations, external links, assays, biomaterials used in the experiment, and the data collected during the process. The elements of our model are mapped to classes and properties from various established biomedical ontologies. Resource Description Framework (RDF) data is automatically produced using these mappings and indexed in an RDF store with a built-in Sparql Protocol and RDF Query Language (SPARQL) endpoint.   CONCLUSIONS Using the open-source eXframe software, institutions and laboratories can create Semantic Web repositories of their experiments, integrate it with heterogeneous resources and make it interoperable with the vast Semantic Web of biomedical knowledge.	annotation;canonical account;class;communication endpoint;controlled vocabulary;database;databases;experiment;genetic heterogeneity;genomics;human-readable medium;index;interoperability;knowledge bases;knowledge base;laboratory;linked data;ontology (information science);open-source software;query language;repository;resource description framework;reuse (action);root-finding algorithm;sparql;semantic web;triplestore;citation	Emily Merrill;Stephane Corlosquet;Paolo Ciccarese;Timothy W Clark;Sudeshna Das	2014		10.1186/2041-1480-5-S1-S3	named graph;medical research;computer science;sparql;bioinformatics;data science;social semantic web;linked data;data mining;semantic web stack;rdf query language;information retrieval;semantic analytics;algorithm;rdf schema	Web+IR	-3.9911562094778543	-63.13917284443628	9948
670218f2d236431f87d410eed5d8fef230979dfd	ant colony optimization for image regularization based on a nonstationary markov modeling	metodo regularizacion;remote sensing image;modelizacion;teledetection;evaluation performance;optimisation;image segmentation;ant colony optimization;fixed form neighborhood;performance evaluation;image processing;modelo markov;optimizacion;image segment;graph color affectation problem;evaluacion prestacion;ant colony;optimisation graph colouring image classification image segmentation markov processes;regularization method;procesamiento imagen;markov random field mrf ant colony classification image model;methode regularisation;image classification;graph coloring;probabilistic approach;algorithms animals ants artificial intelligence behavior animal biomimetics image enhancement image interpretation computer assisted markov chains models statistical pattern recognition automated reproducibility of results sensitivity and specificity social behavior video recording;classification;traitement image;medida sin contacto;markov random field;image model;algorithme;modelisation;algorithm;transposition;image classification regularization;markov model;campo aleatorio;enfoque probabilista;approche probabiliste;ant colony optimization image segmentation pixel markov random fields classification algorithms simulated annealing image edge detection image classification remote sensing;remote sensing;nonstationary markov modeling;markov random field mrf;segmentation image;classification image;teledeteccion;markov random field regularization techniques;optimization;non contact measurement;remote sensing images ant colony optimization image classification regularization nonstationary markov modeling graph color affectation problem image segment fixed form neighborhood markov random field regularization techniques;markov processes;modele markov;remote sensing images;modeling;mesure sans contact;champ aleatoire;graph colouring;transposicion;algoritmo;random field	Ant colony optimization (ACO) has been proposed as a promising tool for regularization in image classification. The algorithm is applied here in a different way than the classical transposition of the graph color affectation problem. The ants collect information through the image, from one pixel to the others. The choice of the path is a function of the pixel label, favoring paths within the same image segment. We show that this corresponds to an automatic adaptation of the neighborhood to the segment form, and that it outperforms the fixed-form neighborhood used in classical Markov random field regularization techniques. The performance of this new approach is illustrated on a simulated image and on actual remote sensing images	acclimatization;algorithm;ant colony optimization algorithms;ants;approximation;belief propagation;classification;colony collapse disorder;computation;computer vision;default;extracellular matrix;feasible region;graph - visual representation;graph coloring;image segmentation;iterated conditional modes;iterative method;markov chain;markov random field;mathematical optimization;normal statistical distribution;numerical analysis;optimization problem;pixel;pixel connectivity;population parameter;self-organization;simulated annealing;software propagation;time complexity;message	Sylvie Le Hégarat-Mascle;Abdelaziz Kallel;Xavier Descombes	2007	IEEE Transactions on Image Processing	10.1109/TIP.2007.891150	computer vision;contextual image classification;random field;ant colony optimization algorithms;systems modeling;transposition;image processing;biological classification;computer science;ant colony;machine learning;pattern recognition;graph coloring;mathematics;image segmentation;markov process;markov model;statistics	Vision	51.34383117597194	-68.97040512935921	9955
7b0008894c1227193f865c4dd67c8cd876cc2f06	modeling the growth of fingerprints improves matching for adolescents	image matching;isotropic rescaling fingerprint growth modeling juvenile fingerprint juvenile criminal record stature growth growth chart federal criminal police office germany automated fingerprint identification system;shape analysis;indexing terms;fingerprint recognition;indexation;error rate;longitudinal data;fingerprint identification;shape analysis automated fingerprint identification systems afis biometrics finger growth fingerprint recognition matching;size measurement shape bones correlation indexes fingers;image matching fingerprint identification	We study the effect of growth on the fingerprints of adolescents, based on which we suggest a simple method to adjust for growth when trying to recover a juvenile's fingerprint in a database years later. Based on longitudinal data sets in juveniles' criminal records, we show that growth essentially leads to an isotropic rescaling, so that we can use the strong correlation between growth in stature and limbs to model the growth of fingerprints proportional to stature growth as documented in growth charts. The proposed rescaling leads to a 72% reduction of the distances between corresponding minutiae for the data set analyzed. These findings were corroborated by several verification tests. In an identification test on a database containing 3.25 million right index fingers at the Federal Criminal Police Office of Germany, the number of identification errors was reduced from 10 errors in 48 identification attempts to 1 in 48 by rescaling. The presented method is of striking simplicity and can easily be integrated into existing automated fingerprint identification systems.	automated fingerprint identification;chart;database;human height;minutiae;software verification	Carsten Gottschlich;Thomas Hotz;Robert Lorenz;Stefanie Bernhardt;Michael Hantschel;Axel Munk	2011	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2011.2143406	fingerprint;computer vision;speech recognition;index term;word error rate;computer science;data mining;shape analysis;computer security;fingerprint recognition	Visualization	32.044793860675135	-63.18427651259044	9960
8477a6db24300f50c2d4b10da18813d2ba0fbbf3	piclust: a density based pirna clustering algorithm	rna seq;pirna;clustering;genome;piwi	Piwi-interacting RNAs (piRNAs) are recently discovered, endogenous small non-coding RNAs. piRNAs protect the genome from invasive transposable elements (TE) and sustain integrity of the genome in germ cell lineages. Small RNA-sequencing data can be used to detect piRNA activations in a cell under a specific condition. However, identification of cell specific piRNA activations requires sophisticated computational methods. As of now, there is only one computational method, proTRAC, to locate activated piRNAs from the sequencing data. proTRAC detects piRNA clusters based on a probabilistic analysis with assumption of a uniform distribution. Unfortunately, we were not able to locate activated piRNAs from our proprietary sequencing data in chicken germ cells using proTRAC. With a careful investigation on data sets, we found that a uniform or any statistical distribution for detecting piRNA clusters may not be assumed. Furthermore, small RNA-seq data contains many different types of RNAs which was not carefully taken into account in previous studies. To improve piRNA cluster identification, we developed piClust that uses a density based clustering approach without assumption of any parametric distribution. In previous studies, it is known that piRNAs exhibit a strong tendency of forming piRNA clusters in syntenic regions of the genome. Thus, the density based clustering approach is effective and robust to the existence of non-piRNAs or noise in the data. In experiments with piRNA data from human, mouse, rat and chicken, piClust was able to detect piRNA clusters from total small RNA-seq data from germ cell lines, while proTRAC was not successful. piClust outperformed proTRAC in terms of sensitivity and running time (up to 200 folds). piClust is currently available as a web service at http://epigenomics.snu.ac.kr/piclustweb.	assumed;biopolymer sequencing;chicken;cluster analysis;computation;cultured cell line;dna transposable elements;experiment;genome;germ cells;interaction;piwil1 gene;piwi-interacting rna;probabilistic analysis of algorithms;sensor;sequence number;statistical distributions;synteny;tellurium;test engineer;time complexity;web service;algorithm;statistical cluster	Inuk Jung;Jong Chan Park;Sun Kim	2014	Computational biology and chemistry	10.1016/j.compbiolchem.2014.01.008	biology;bioinformatics;piwi-interacting rna;genetics	Comp.	3.296940802145842	-58.63663717240902	9966
19d78d8c072b60294792c523742a8609accf3890	a primer on neural network models for natural language processing		Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.	artificial neural network;computation;computer vision;convolutional neural network;gradient;machine learning;natural language processing;primer;recurrent neural network;recursion;speech processing	Yoav Goldberg	2016	J. Artif. Intell. Res.	10.1613/jair.4992	natural language processing;nervous system network models;types of artificial neural networks;computer science;artificial intelligence;recurrent neural network;machine learning;time delay neural network;deep learning;convolutional neural network	ML	23.809735226602747	-54.985453888904054	10016
8352e746429ad21a35cece2f7993e7557f00d338	marhs: mobility assessment system with remote healthcare functionality for movement disorders	classification algorithm;force sensors;movement disorders;clinical trial;chronic obstructive pulmonary disease;data analysis;research and development;healthcare system;movement disorder;feature selection;remote health;force sensor	Due to the global trend of aging societies with increasing demand for low cost and high quality healthcare services, there has been extensive research and development directed toward wireless and remote healthcare technology that considers age-associated ailments. In this paper, we introduce Mobility Assessment and Remote Healthcare System (MARHS) that utilizes a force sensor in order to provide quantitative assessment of the mobility level of patients with movement disorder ailment, which is one common age-associated ailment. The proposed system also enables the remote healthcare services that allow patients to receive diagnoses from clinical experts without his/her presence. MARHS also contains a data analysis unit in order to provide information that summarizes the characteristics of symptoms of a group of patients (e.g., patients with a certain type of ailment) using a combination of feature ranking, feature selection, and classification algorithms. The results of the analyses on the data from a clinical trial show that the examination results of the proposed system can accurately recognize various groups of patients, such as, patients with (i) chronic obstructive pulmonary disease, (ii) hypertension, and (iii) cerebral vascular accident with an average accuracy of 90.05%, 82.60%, and 93.54%, respectively.	algorithm;display resolution;feature selection	Sunghoon Ivan Lee;Jonathan Woodbridge;Ani Nahapetian;Majid Sarrafzadeh	2012		10.1145/2110363.2110402	medicine;pathology;physical therapy;biological engineering	HCI	10.60371684621304	-86.04757786026259	10028
5b89662a5a1da9db8afbf63fb04d7cd3fc0fe589	using single cell sequencing data to model the evolutionary history of a tumor	evolution molecular;tumor cells cultured;high throughput nucleotide sequencing;single cell analysis;computational biology bioinformatics;genome;algorithms;humans;neoplasms;combinatorial libraries;computational biology;computer appl in life sciences;mutation;microarrays;bioinformatics	The introduction of next-generation sequencing (NGS) technology has made it possible to detect genomic alterations within tumor cells on a large scale. However, most applications of NGS show the genetic content of mixtures of cells. Recently developed single cell sequencing technology can identify variation within a single cell. Characterization of multiple samples from a tumor using single cell sequencing can potentially provide information on the evolutionary history of that tumor. This may facilitate understanding how key mutations accumulate and evolve in lineages to form a heterogeneous tumor. We provide a computational method to infer an evolutionary mutation tree based on single cell sequencing data. Our approach differs from traditional phylogenetic tree approaches in that our mutation tree directly describes temporal order relationships among mutation sites. Our method also accommodates sequencing errors. Furthermore, we provide a method for estimating the proportion of time from the earliest mutation event of the sample to the most recent common ancestor of the sample of cells. Finally, we discuss current limitations on modeling with single cell sequencing data and possible improvements under those limitations. Inferring the temporal ordering of mutational sites using current single cell sequencing data is a challenge. Our proposed method may help elucidate relationships among key mutations and their role in tumor progression.	biopolymer sequencing;color gradient;communications satellite;estimated;genetic heterogeneity;inference;massively-parallel sequencing;most recent common ancestor;mutation;neoplasms;phylogenetic tree;phylogenetics;tumor progression;mixture	Kyung In Kim;Richard Simon	2013		10.1186/1471-2105-15-27	mutation;biology;dna microarray;bioinformatics;single cell sequencing;genetics;genome	Comp.	3.446566865964157	-58.75058261421493	10038
b8841509aae902d114f222cf7f28a1ee23005a0c	adaptive plastic-landmine visualizing radar system: effects of aperture synthesis and feature-vector dimension reduction	complex variable method;analisis contenido;traitement signal;metodo variable compleja;evaluation performance;texture;tecnologia electronica telecomunicaciones;deteccion blanco;methode variable complexe;performance evaluation;analisis textura;landmine detection;visualizacion;dimension reduction;estudio comparativo;evaluacion prestacion;synthese ouverture;interferometrie;pattern synthesis;aperture synthesis;sistema complejo;complex valued self organizing map;carte autoorganisatrice;detection cible;etude comparative;feature vector;synthese forme;detection objet;visualization;content analysis;texture analysis;plastic landmine;systeme complexe;visualisation;self organising feature maps;complex system;methode domaine frequence;frequency domain method;feature extraction;signal processing;frequence spatiale;textura;comparative study;autoorganizacion;self organization;interferometry;extraction caracteristique;metodo dominio frecuencia;analyse contenu;tecnologias;frecuencia espacial;interferometria;grupo a;sintesis forma;procesamiento senal;target detection;analyse texture;detection mine terrestre;autoorganisation;spatial frequency;interferometric radar;object detection;radar	We propose an adaptive plastic-landmine visualizing radar system employing a complex-valued self-organizing map (CSOM) dealing with a feature vector that focuses on variance of spatial- and frequency-domain inner products (V-CSOM) in combination with aperture synthesis. The dimension of the new feature vector is greatly reduced in comparison with that of our previous texture feature-vector CSOM (T-CSOM). In experiments, we first examine the effect of aperture synthesis on the complex-amplitude texture in space.and frequency domains. We also compare the calculation cost and the visualization performance of V- and T-CSOMs. Then we discuss merits and drawbacks of the two types of CSOMs with/without the aperture synthesis in the adaptive plastic-landmine visualization task. The V-CSOM with aperture synthesis is found promising to realize a useful plastic-landmine detection system.	dimensionality reduction;feature vector	Takahiro Hara;Akira Hirose	2005	IEICE Transactions	10.1093/ietele/e88-c.12.2282	visualization;content analysis;telecommunications;computer science;signal processing;physics	EDA	45.64925259114107	-61.68933792373187	10055
30af0c2509a7d9fe7c212dc90c3e53a238692c50	color and texture applied to a signature-based bag of visual words method for image retrieval		This article addresses the problem of representation, indexing and retrieval of images through the signature-based bag of visual words (S-BoVW) paradigm, which maps features extracted from image blocks into a set of words without the need of clustering processes. Here, we propose the first ever method based on the S-BoVW paradigm that considers information of texture to generate textual signatures of image blocks. We also propose a strategy that represents image blocks with words which are generated based on both color as well as texture information. The textual representation generated by this strategy allows the application of traditional text retrieval and ranking techniques to compute the similarity between images. We have performed experiments with distinct similarity functions and weighting schemes, comparing the proposed strategy to the well-known cluster-based bag of visual words (C-BoVW) and S-BoVW methods proposed previously. Our results show that the proposed strategy for representing images is a competitive alternative for image retrieval, and overcomes the baselines in many scenarios.	bag-of-words model in computer vision;baseline (configuration management);belief propagation;cluster analysis;document retrieval;experiment;image retrieval;latent semantic analysis;local binary patterns;map;programming paradigm;synchronous data link control;type signature;whole earth 'lectronic link	Joyce Miranda dos Santos;Edleno Silva de Moura;Altigran Soares da Silva;Ricardo da Silva Torres	2016	Multimedia Tools and Applications	10.1007/s11042-016-3955-4	computer vision;computer science;automatic image annotation;visual word;artificial intelligence;cluster analysis;pattern recognition;bag-of-words model in computer vision;search engine indexing;image texture;image retrieval;weighting	Vision	35.34455189534225	-55.38909213955726	10060
7512f3182d58ea6eab5aeb465b081ef0bc6a4ed8	mmcode: enhancing color channels for screen-camera communication with semi-supervised clustering		With the pervasive availability of LCD displays and phone cameras, screen-camera communication has attracted grate attentions due to the characteristics of convenience, security, infrastructure-free, and contactless. The existing screen-camera communication systems using dynamic barcodes suffer from poor ability of color recognition. In this paper, we propose a machine learning based multi-color dynamic barcode system called MMCode to overcome such limit. We formulate the color recognition problem as a machine learning task, and propose a semi-supervised clustering algorithm to achieve finer-grained color recognition. The proposed mechanism inserts reference colors in the barcode design, and adopts a semi-supervised Gaussian Mixed Model (GMM) algorithm for frame decoding. We implement MMCode as an Android APP and test its performance under real screen-camera communication scenarios. Extensive experiments show that the proposed MMCode achieves significant enhancement on the capacity of dynamic barcodes compared to the state-of-the-arts.	algorithm;android;barcode system;channel (digital image);cluster analysis;color;contactless smart card;experiment;machine learning;mixed model;semi-supervised learning;semiconductor industry	Xu Chen;Wenzhong Li;Tong Zhan;Sanglu Lu	2018	2018 27th International Conference on Computer Communication and Networks (ICCCN)	10.1109/ICCCN.2018.8487328	barcode;communications system;computer vision;decoding methods;android (operating system);cluster analysis;computer science;artificial intelligence;distributed computing;channel (digital image)	Mobile	30.600542167323738	-52.90096277486	10063
a812d476e83f76efab70d38a3c75e32ab179cd28	a multi-task network to detect junctions in retinal vasculature		Junctions in the retinal vasculature are key points to be able to extract its topology, but they vary in appearance, depending on vessel density, width and branching/crossing angles. The complexity of junction patterns is usually accompanied by a scarcity of labels, which discourages the usage of very deep networks for their detection. We propose a multitask network, generating labels for vessel interior, centerline, edges and junction patterns, to provide additional information to facilitate junction detection. After the initial detection of potential junctions in junctionselective probability maps, candidate locations are re-examined in centerline probability maps to verify if they connect at least 3 branches. The experiments on the DRIVE and IOSTAR showed that our method outperformed a recent study in which a popular deep network was trained as a classifier to find junctions. Moreover, the proposed approach is applicable to unseen datasets with the same degree of success, after training it only once.		Fatmatülzehra Uslu;Anil A. Bharath	2018		10.1007/978-3-030-00934-2_11	pattern recognition;computer vision;computer science;artificial intelligence;branching (version control);boltzmann machine;retinal	ML	30.583291855459215	-72.94194357278937	10127
5ce4dc8b48219bde7aff38151f8063e0d161e313	blind blur estimation using low rank approximation of cepstrum	baja resolucion;analisis imagen;image recognition;reconocimiento imagen;restauration image;image processing;image resolution;cepstre;low resolution;basse resolution;procesamiento imagen;image restoration;blind;qualite image;image bruitee;traitement image;imagen sonora;restauracion imagen;resolucion imagen;cepstrum;noisy image;image quality;analyse spectrale;reconnaissance image;analisis espectral;image analysis;calidad imagen;rapport signal bruit;relacion senal ruido;spectral analysis;signal to noise ratio;low rank approximation;analyse image;resolution image;ciego;aveugle	The quality of image restoration from degraded images is highly dependent upon a reliable estimate of blur. This paper proposes a blind blur estimation technique based on the low rank approximation of cepstrum. The key idea that this paper presents is that the blur functions usually have low ranks when compared with ranks of real images and can be estimated from cepstrum of degraded images. We extend this idea and propose a general framework for estimation of any type of blur. We show that the proposed technique can correctly estimate commonly used blur types both in noiseless and noisy cases. Experimental results for a wide variety of conditions i.e., when images have low resolution, large blur support, and low signal-to-noise ratio, have been presented to validate our proposed method.	cepstrum;circuit restoration;gaussian blur;image resolution;image restoration;low-rank approximation;signal-to-noise ratio	Adeel A. Bhutta;Hassan Foroosh	2006		10.1007/11867586_9	computer vision;image analysis;speech recognition;image resolution;image processing;computer science	Vision	53.532605223178166	-66.1520535196286	10130
71c890a0caa90f0e9dd3e49acdd6c07a2ce86a95	clustering reveals limits of parameter identifiability in multi-parameter models of biochemical dynamics	simulation and modeling;systems biology;physiological cellular and medical topics;computational biology bioinformatics;algorithms;bioinformatics	Compared to engineering or physics problems, dynamical models in quantitative biology typically depend on a relatively large number of parameters. Progress in developing mathematics to manipulate such multi-parameter models and so enable their efficient interplay with experiments has been slow. Existing solutions are significantly limited by model size. In order to simplify analysis of multi-parameter models a method for clustering of model parameters is proposed. It is based on a derived statistically meaningful measure of similarity between groups of parameters. The measure quantifies to what extend changes in values of some parameters can be compensated by changes in values of other parameters. The proposed methodology provides a natural mathematical language to precisely communicate and visualise effects resulting from compensatory changes in values of parameters. As a results, a relevant insight into identifiability analysis and experimental planning can be obtained. Analysis of NF- κB and MAPK pathway models shows that highly compensative parameters constitute clusters consistent with the network topology. The method applied to examine an exceptionally rich set of published experiments on the NF- κB dynamics reveals that the experiments jointly ensure identifiability of only 60 % of model parameters. The method indicates which further experiments should be performed in order to increase the number of identifiable parameters. We currently lack methods that simplify broadly understood analysis of multi-parameter models. The introduced tools depict mutually compensative effects between parameters to provide insight regarding role of individual parameters, identifiability and experimental design. The method can also find applications in related methodological areas of model simplification and parameters estimation.	anatomy, regional;cluster analysis;design of experiments;dynamical system;estimation theory;experiment;gene regulatory network;level of detail;mathematics;network topology;numerous;population parameter;scientific publication;solutions;synthetic biology;statistical cluster	Karol Nienaltowski;Michal Wlodarczyk;Tomasz Lipniacki;Michal Komorowski	2015		10.1186/s12918-015-0205-8	computational biology;biology;computer science;bioinformatics;systems biology	ML	6.839514238308746	-58.78869685775866	10136
feb299f3892e7dabb01296e748ecd5581112adbb	accurate estimation of the signal baseline in dna chromatograms	dna;capillary electrophoresis;clusters separation improvement;digital signal processing;histograms;optimisation;expectation maximization algorithms;statistical learning framework;faster iterative histogram;gel electrophoresis;dna fragments;bayesian methods;accurate signal baseline estimation;optimisation dna chromatography statistical analysis biological techniques;data mining;iterative methods;classification errors reduction;statistical learning;statistical analysis;dna chromatogram;expectation maximization algorithm;classification error;signal resolution;statistical learning framework classification errors reduction expectation maximization algorithm accurate base calling dna chromatogram accurate signal baseline estimation dna fragments gel electrophoresis clusters separation improvement capillary electrophoresis faster iterative histogram;dna computing;biological techniques;signal to noise ratio;chromatography;accurate base calling;statistical learning bayesian methods signal resolution signal to noise ratio data mining digital signal processing dna computing expectation maximization algorithms iterative methods histograms	Estimating accurately the varying baseline level in different parts of a DNA chromatogram is a challenging and important problem for accurate base-calling. We are formulating the problem in a statistical learning framework and propose an Expectation-Maximization algorithm for its solution. In addition we also present a faster, iterative histogram based method for estimating the background of the signal in small size windows. The two methods can be combined with regression techniques to correct the baseline in all regions of the chromatogram and are shown to work well even in areas of low SNR. By improving the separation of clusters, baseline correction actions reduce the classification errors when using the BEM base-caller developed in our group.	baseline (configuration management);dna computing	Lucio Andrade;Elias S. Manolakos	2002		10.1109/NNSP.2002.1030015	computer science;machine learning;pattern recognition;statistics	ML	50.68387369727939	-82.89149509402533	10146
b20b61b97725ba6b3ff9aa390bc52d30e3bec690	developing a program for tracking heart failure	biomedical research;bioinformatics	Introduction. Heart failure is a chronic condition representing a major and increasing cause of death and disability currently afflicting over four million Americans. A number of recent studies have shown that careful attention to the early signs of decompensation and appropriate care can decrease hospitalizations and improve quality of life. Approaches thus far have been labor intensive, using specially trained personnel to monitor and advise the patient at home. If the process of monitoring the patient’s status was automated, more patients should benefit from improved control.		William J. Long;Hamish S. F. Fraser;Shapur Naimi;Kathleen Perry;Linda Cotter;Marvin A. Konstam	2000			intensive care medicine;alternative medicine;heart failure;medicine	HCI	6.885674894870886	-75.80033137421148	10167
523c43565cbadee875ecf160114ded4841313763	statistical classification based on svm for raman spectra discrimination of nasopharyngeal carcinoma cell	patient diagnosis;support vector machines cancer cellular biophysics medical signal processing patient diagnosis raman spectroscopy signal classification;support vector machines;cancer;raman spectroscopy;signal classification;linear discriminant analysis nasopharyngeal carcinoma raman spectroscopy support vector machine;cellular biophysics;medical signal processing;nasopharyngeal normal cell line svm based statistical classification raman spectra discrimination raman spectroscopy molecular change detection tissue pathology noninvasive optical diagnosis method normal cells tumor cells support vector machine lda classification model linear discriminant analysis nasopharyngeal carcinoma cell lines	Raman spectroscopy(RS) has shown its advantages in detecting molecular changes associated with tissue pathology, which makes it possible to diagnose with optical methods non-invasively and real-time. It is very important to validate an existing classification model using different algorithms used in the discrimination of normal and tumor cells. In this work, three algorithms of SVM (Support Vector Machine) are used to validate LDA classification model of nasopharyngeal carcinoma (NPC) cell lines and nasopharyngeal normal cell line. All of these three SVM algorithms use the same data set as the same LDA model and achieve great sensitivity and specificity. Experimental results show that LDA classification model could be supported by different SVM algorithms and this demonstrates our classification model is reliable and may be helpful to the realization of RS to be one of diagnostic techniques of NPC.	algorithm;np-completeness;raman scattering;real-time clock;sensitivity and specificity;sensor;statistical classification;support vector machine	Guannan Chen;Hengyang Hu;Rong Chen;Daner Xu	2012	2012 5th International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2012.6513016	support vector machine;raman spectroscopy;speech recognition;pathology;computer science;machine learning;cancer	EDA	27.542153123438265	-73.98279815834367	10169
3146ab3747ea929ac2c7cbf3874559397ba7aea8	a multivariate circular distribution with applications to the protein structure prediction problem	marginally specified distribution;test of independence;asymmetric generalized von mises distribution;62j99;multivariate circular distribution;bioinformatics	The protein structure prediction problem is considered to be the holy grail of bioinformatics, and circular variables in protein structure problem are ubiquitous. For example, conformational angles appear in g turns, a helices, b sheets. It is well known that dihedral angles (f and y) together with w (torsion angle of the peptide bond) and c (torsion angle of the side chain) are considered to be important for protein structure prediction since they define the entire conformation of a protein. For the study of k conformational angles, we need a k-variate angular distribution. In this paper, we propose a multivariate circular distribution and inferential methods, which could be useful for jointly modeling those circular variables. Our proposed family of k-variate circular distributions and testing methods were applied to trivariate circular data set arising from g turns consisting of GlycinePhenylalanine-Threonine sequence. We have shown that there is a three-way dependent structure among f, y and c, and that the side chain angles are relevant to the relationship between dihedral angles for the given sequence. The proposed model was compared with two existing multivariate circular models using a bivariate and a trivariate circular datasets.	angularjs;bioinformatics;bivariate data;consistency model;inferential theory of learning;protein structure prediction;torsion (gastropod)	Sungsu Kim;Ashis SenGupta;Barry C. Arnold	2016	J. Multivariate Analysis	10.1016/j.jmva.2015.09.024	combinatorics;circular distribution;mathematics;geometry;graphical models for protein structure;statistics	Comp.	14.712028617532246	-58.89202718053709	10170
c884ba0f6b164156f9c0d07f46aceb9ba20ec4d5	surface plasmon resonance based biosensors for exploring the influence of alkaloids on aggregation of amyloid-β peptide	biosensing techniques;surface plasmon resonance;amyloid beta peptides;gold;alkaloids;humans;alzheimer disease;thioaliphatic acid monolayer;amyloid β peptide	The main objective of the presented study was the development of a simple analytical tool for exploring the influence of naturally occurring compounds on the aggregation of amyloid-β peptide (Aβ(40)) in order to find potential anti-neurodegenerative drugs. The gold discs used for surface plasmon resonance (SPR) measurements were modified with thioaliphatic acid. The surface functionalized with carboxylic groups was used for covalent attaching of Aβ(40) probe by creation of amide bonds in the presence of EDC/NHS. The modified SPR gold discs were used for exploring the Aβ(40) aggregation process in the presence of selected alkaloids: arecoline hydrobromide, pseudopelletierine hydrochloride, trigonelline hydrochloride and α-lobeline hydrochloride. The obtained results were discussed with other parameters which govern the phenomenon studied such as lipophilicity/hydrophilicy and Aβ(40)-alkaloid association constants.	11-mercaptoundecanoic acid;academy;alkaloids;arecoline hydrobromide;biosensors;cns disorder;covalent interaction;error detection and correction;real-time clock;surface plasmon resonance;system of measurement;thioctic acid;vitamin b6;ethylene dichloride;lipophilicity;pyridoxine	Bartlomiej Emil Krazinski;Jerzy Radecki;Hanna Radecka	2011		10.3390/s110404030	gold;biochemistry;stereochemistry;surface plasmon resonance;chemistry;organic chemistry;physical chemistry;physics	Metrics	11.327213013981124	-64.67070711324762	10184
808a6f0768604d7343f9fb8d03c77aa78a7da445	the sorcerer's apprentice a serious game aiding rehabilitation in the context of subacromial impingement syndrome	motor rehabilitation efficacy improvement serious game aiding rehabilitation home environment use case low cost io device microsoft kinect motion tracking design process motivational factors game play patient health status improvement one sided shoulder impingement syndrome supplementary exercises supervised physiotherapy shoulder area strength improvement shoulder area mobility improvement the sorcerer s apprentice serious game;patient rehabilitation;motor rehabilitation;shoulder impingement syndrome serious game motor rehabilitation motion tracking;serious games computing human factors patient rehabilitation;motion tracking;human factors;serious games computing;shoulder impingement syndrome;indexes tracking muscles surgery;serious game	"""Serious games can help to improve efficacy of motor rehabilitation especially in a home environment. We introduce """"The Sorcerer's Apprentice"""", a serious game improving strength and mobility of the shoulder area targeting support of supervised physiotherapy. It proposes a customizable environment for supplementary exercises in the context of rehabilitation for a onesided Shoulder-Impingement-Syndrome. We introduce the medical background of the shoulder impingement syndrome, how the game aims to improve the health status of the patients through several options of exercises and how these exercises are embedded into the flow of game play. We will further explain how motivational factors are implemented and which additional factors were relevant in the design process. As the game makes use of motion tracking for input, we utilized Microsoft Kinect as a low-cost IO device suitable for a home-environment use case."""	carpal tunnel syndrome;embedded system;kinect;sorcerer's apprentice syndrome	Peter Fikar;Christian Schönauer;Hannes Kaufmann	2013	2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops	10.4108/icst.pervasivehealth.2013.252224	match moving;simulation;physical medicine and rehabilitation;computer science;engineering;human factors and ergonomics;physical therapy;mechanical engineering	HCI	11.46919225798305	-83.66836132850592	10188
9a20f08e5f4e25f564e29b1fc3e56d33dcbbc6ce	a simplified dynamic model for the p53-mdm2 feedback loop	feedback;proteins;oscillators dna proteins degradation feedback loop maintenance engineering delay effects;proteins delays feedback;transcription time delay p53 mdm2 feedback loop nonlinear term simplified dynamic model dephosphorylation p53 protein single pulse transient responses environmental stress stimulation mdm2 dynamic behaviors p53 sustained oscillations p53 dynamics;delays	In this paper, a simplified dynamic model for the p53-Mdm2 feedback loop without the time lag effect of transcription is proposed. In the dynamic model, a nonlinear term is introduced to represent the dephosphorylation of the p53 protein. The results show that while the p53 and Mdm2 exhibit single pulse transient responses for the lower environmental stress stimulation, the sustained oscillations are observed for larger environmental stress stimulation. It is concluded that the simplified dynamic model is also able to reproduce the p53 and Mdm2 dynamic behaviors as shown in the experimental results and as predicted by previous complicated models in the literature. And the transcription time delay is dispensable for the p53 sustained oscillations. The results are helpful for understanding p53 dynamics.	broadcast delay;feedback;mathematical model;negative feedback;nonlinear system;transcription (software)	Xiao-Min Shi;Kai-Rong Qin;Zeng-Rong Liu;Yufan Zheng	2013	2013 10th IEEE International Conference on Control and Automation (ICCA)	10.1109/ICCA.2013.6565180	control engineering;simulation;computer science;engineering;control theory;feedback	Robotics	7.973244609061932	-66.35170619180082	10199
34ded18ce80acede8f7e3780a088b56859917146	joint embeddings of shapes and images via cnn image purification	embedding;3d shapes;deep learning	"""Both 3D models and 2D images contain a wealth of information about everyday objects in our environment. However, it is difficult to semantically link together these two media forms, even when they feature identical or very similar objects. We propose a joint embedding space populated by both 3D shapes and 2D images of objects, where the distances between embedded entities reflect similarity between the underlying objects. This joint embedding space facilitates comparison between entities of either form, and allows for cross-modality retrieval. We construct the embedding space using 3D shape similarity measure, as 3D shapes are more pure and complete than their appearance in images, leading to more robust distance metrics. We then employ a Convolutional Neural Network (CNN) to """"purify"""" images by muting distracting factors. The CNN is trained to map an image to a point in the embedding space, so that it is close to a point attributed to a 3D model of a similar object to the one depicted in the image. This purifying capability of the CNN is accomplished with the help of a large amount of training data consisting of images synthesized from 3D shapes. Our joint embedding allows cross-view image retrieval, image-based shape retrieval, as well as shape-based image retrieval. We evaluate our method on these retrieval tasks and show that it consistently out-performs state-of-the-art methods, and demonstrate the usability of a joint embedding in a number of additional applications."""	3d modeling;convolutional neural network;embedded system;entity;image retrieval;modality (human–computer interaction);population;purification of quantum state;purify;similarity measure;usability	Yangyan Li;Hao Su;Charles Ruizhongtai Qi;Noa Fish;Daniel Cohen-Or;Leonidas J. Guibas	2015	ACM Trans. Graph.	10.1145/2816795.2818071	computer vision;computer science;theoretical computer science;machine learning;embedding;mathematics;deep learning	Graphics	32.877158462952025	-52.49667141510772	10204
a83b06dc1d381e6a8bb2bad2c2ea51a5bbee7fa1	the xxmotif web server for exhaustive, weight matrix-based motif discovery in nucleotide sequences	dna;software;position specific scoring matrices;color;sequence analysis dna;binding sites;internet;rna;nucleotide motifs;graphical displays;functional genomics;sequence analysis rna;base sequence	The discovery of regulatory motifs enriched in sets of DNA or RNA sequences is fundamental to the analysis of a great variety of functional genomics experiments. These motifs usually represent binding sites of proteins or non-coding RNAs, which are best described by position weight matrices (PWMs). We have recently developed XXmotif, a de novo motif discovery method that is able to directly optimize the statistical significance of PWMs. XXmotif can also score conservation and positional clustering of motifs. The XXmotif server provides (i) a list of significantly overrepresented motif PWMs with web logos and E-values; (ii) a graph with color-coded boxes indicating the positions of selected motifs in the input sequences; (iii) a histogram of the overall positional distribution for selected motifs and (iv) a page for each motif with all significant motif occurrences, their P-values for enrichment, conservation and localization, their sequence contexts and coordinates. Free access: http://xxmotif.genzentrum.lmu.de.	access network;binding sites;box;cluster analysis;de novo transcriptome assembly;deutsche mark;experiment;functional genomics;gene ontology term enrichment;german research centre for artificial intelligence;graph - visual representation;nucleotides;p-value;page (document);position-specific scoring matrices;rna;sequence motif;server (computer);server (computing);web server;statistical cluster	Sebastian Luehr;Holger Hartmann;Johannes Söding	2012		10.1093/nar/gks602	consensus sequence;functional genomics;biology;the internet;rna;bioinformatics;binding site;position weight matrix;multiple em for motif elicitation;genetics;dna;sequence motif	Comp.	-1.0491425976265953	-58.7387701941282	10209
5f56c70c5dee38f3b4056be7296c95ca2ad4cf93	development of an implantable flexible probe for simultaneous near-infrared spectroscopy and electrocorticography	flexible electronics;neurosurgery implantable flexible probe near infrared spectroscopy electrocorticography multimodal measurement capability cortical activity measurement chronic subdural implantation clinical diagnosis neuroscience optical fiber based nirs probe light scattering effect ecog electrodes simultaneous nirs ecog light emitting diodes photodiodes polyimide based flexible substrate spatial resolution submicromolar hemoglobin concentration changes acute implantation source detector distance presurgical assessment intrasurgical assessment;electrodes probes cooling light emitting diodes optical reflection temperature measurement substrates;electrocorticography ecog;subdural implantation electrocorticography ecog flexible electronics multimodality probe near infrared spectroscopy nirs;light emitting diodes;near infrared spectroscopy nirs;prosthetics;fibre optic sensors;subdural implantation;photodiodes;proteins;spectrochemical analysis biochemistry chemical variables measurement electroencephalography fibre optic sensors infrared spectroscopy light emitting diodes photodiodes prosthetics proteins;infrared spectroscopy;electroencephalography;multimodality probe;biochemistry;spectrochemical analysis;chemical variables measurement	A combination of near-infrared spectroscopy (NIRS) and electrocorticography (ECoG) provides beneficial information on cortical activity from different aspects. Integration of such multimodal measurement capability into a single apparatus and the direct measurement of cortical activity during chronic subdural implantation may be a powerful means for clinical diagnosis and neuroscience. However, an optical fiber-based NIRS probe cannot be miniaturized for implantation into the brain, and the light-scattering effect of ECoG electrodes in NIRS measurements is unknown. We describe here the development of a flexible probe, small enough for chronic subdural implantation, for simultaneous NIRS and ECoG. Two light-emitting diodes of different wavelengths and two photodiodes were mounted on a polyimide-based flexible substrate, and ECoG electrodes were formed with a design minimizing artifacts in NIRS recording. The fabricated probe measured ECoGs at sufficient spatial resolution and submicromolar changes in hemoglobin concentrations in in vivo experiments with acute implantation into a rat. Comparison of measured changes in hemoglobin concentrations for different source-detector distances reveals the reliability of the measured values and the practicality of the simulation model. The proposed intracranial multimodality probe may provide beneficial evidence for pre- and intrasurgical assessment of neurosurgery and reveal the interaction of electrophysiology and hemodynamics at high spatial resolution without artifacts due to scalp blood flow.	detectors;diagnosis, clinical;diode;distance;electrocorticography;experiment;hemodynamics;implants;kidney failure, chronic;large;metabolic process, cellular;monkeys;morphologic artifacts;multimodal imaging;multimodal interaction;neuroscience discipline;optical fiber;polyethylene terephthalate;science of neurosurgery;simulation;spectroscopy, near-infrared;subdural space;video-in video-out;electrode;wavelength	Toshitaka Yamakawa;Takao Inoue;Yeting He;Masami Fujii;Michiyasu Suzuki;Masatsugu Niwayama	2014	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2013.2279888	infrared spectroscopy;photodiode;electroencephalography;flexible electronics;biological engineering;physics;surgery;light-emitting diode	Visualization	25.91474116498326	-83.67253849934363	10216
480c4342b48e9fbe719149c573e4fe2e8ca4e290	learning to detect salient region of image under weak supervision	minimization;image segmentation;humans robustness object segmentation image segmentation entropy minimization correlation;saliency detection;object segmentation;graph cut;graph cuts saliency detection interactive image segmentation;robustness;interactive image segmentation;humans;entropy;correlation;graph cuts	Salient region of an image usually contains the crucial information for image analysis and understanding. Most conventional approaches learn the saliency by utilizing the low-level features, which ignore the participation of human. In this paper, we propose an effective and robust approach to detect the salient region of an image by combining the bottom-up and top-down cues. The proposed method not only consider the low-level attention features, but also take human into the loop for better understanding of human attention. Furthermore, we build an asymmetrical graph model to integrate these bottom-up and top-down cues into an energy function of saliency. A compact but exact saliency region can be obtained by minimizing posterior energy function. The compact constraint and global minimization manner of the asymmetrical graph cuts guarantee the good performance of saliency extraction. Extensive experiments demonstrate the proposed method is promising.	bottom-up proteomics;cut (graph theory);experiment;global optimization;high- and low-level;image analysis;mathematical optimization;top-down and bottom-up design	Jian Cheng;Yu Fu;Hanqing Lu	2011	2011 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2011.6011926	computer vision;cut;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;programming language;scale-space segmentation	Vision	38.41645613227555	-54.291478398031884	10218
bbd71ae94ad9c54318996a1bf1d9890445ba493d	sidirect: highly effective, target-specific sirna design software for mammalian rna interference	software;rna interference;animals;software systems;small interfering rna;internet;functional genomics;gene silencing;rna small interfering;algorithms;molecular sequence data;mammals	siDirect (http://design.RNAi.jp/) is a web-based online software system for computing highly effective small interfering RNA (siRNA) sequences with maximum target-specificity for mammalian RNA interference (RNAi). Highly effective siRNA sequences are selected using novel guidelines that were established through an extensive study of the relationship between siRNA sequences and RNAi activity. Our efficient software avoids off-target gene silencing to enumerate potential cross-hybridization candidates that the widely used BLAST search may overlook. The website accepts an arbitrary sequence as input and quickly returns siRNA candidates, providing a wide scope of applications in mammalian RNAi, including systematic functional genomics and therapeutic gene silencing.	blast;cloud computing;enumerated type;functional genomics;gene silencing;interference (communication);mammals;nucleic acid hybridization;rna interference;rna, small interfering;sensitivity and specificity;software design;software system;web site;web application	Yuki Naito;Tomoyuki Yamada;Kumiko Ui-Tei;Shinichi Morishita;Kaoru Saigo	2004	Nucleic acids research	10.1093/nar/gkh442	functional genomics;biology;molecular biology;the internet;gene silencing;bioinformatics;rna interference;trans-acting sirna;rna silencing;small interfering rna;genetics;software system	SE	0.6201877690151585	-57.69396000201746	10223
ba667fbc238d90aa469e85f684710e8a4c77a106	training frankenstein's creature to stack: hypertree architecture search		We propose HyperTrees for the low cost automatic design of multiple-input neural network models. Much like how Dr. Frankenstein’s creature was assembled from pieces before he came to life in the eponymous book, HyperTrees combine parts of other architectures to optimize for a new problem domain. We compare HyperTrees to rENAS, our extension of Efficient Neural Architecture Search (ENAS) [1]. To evaluate these architectures we introduce the CoSTAR Block Stacking Dataset for the benchmarking of neural network models. We utilize 5.1 cm colored blocks and introduce complexity with a stacking task, a bin providing wall obstacles, dramatic lighting variation, and object ambiguity in the depth space. We demonstrate HyperTrees and rENAS on this dataset by predicting full 3D poses semantically for the purpose of grasping and placing specific objects. Inputs to the network include RGB images, the current gripper pose, and the action to take. Predictions with our best model are accurate to within 30 degrees 90% of the time, 4cm 72% of the time, and have an average test error of 3.3cm and 12.6 degrees. The dataset contains more than 10,000 stacking attempts and 1 million frames of real data. Code and dataset instructions are available at github.com/jhu-lcsr/costar_plan.	artificial neural network;computer vision;deep learning;graphics processing unit;local interconnect network;mnist database;problem domain;robot end effector;robotics;software deployment;stacking;vii;workstation	Andrew Hundt;Varun Jain;Chris Paxton;Gregory D. Hager	2018	CoRR		simulation;hyperbolic tree;bin;engineering;ambiguity;machine learning;benchmarking;artificial neural network;architecture;rgb color model;problem domain;artificial intelligence	ML	27.450073309606328	-53.653148365713506	10224
b2eca8e1cb791aa953d78bef8b5677e2fcefd931	an optimized system to solve text-based captcha		CAPTCHA(Completely Automated Public Turing test to Tell Computers and Humans Apart) can be used to protect data from auto bots. Countless kinds of CAPTCHAs are thus designed, while we most frequently utilize text-based scheme because of most convenience and user-friendly way cite{bursztein2011text}. Currently, various types of CAPTCHAs need corresponding segmentation to identify single character due to the numerous different segmentation ways. Our goal is to defeat the CAPTCHA, thus firstly the CAPTCHAs need to be split into character by character. There isnu0027t a regular segmentation algorithm to obtain the divided characters in all kinds of examples, which means that we have to treat the segmentation individually. In this paper, we build a whole system to defeat the CAPTCHAs as well as achieve state-of-the-art performance. In detail, we present our self-adaptive algorithm to segment different kinds of characters optimally, and then utilize both the existing methods and our own constructed convolutional neural network as an extra classifier. Results are provided showing how our system work well towards defeating these CAPTCHAs.	adaptive algorithm;adaptive system;angular defect;artificial neural network;captcha;convolutional neural network;display resolution;humans;optical character recognition;statistical classification;text-based (computing);turing test;twisted;usability	Ye Wang;Mi Lu	2018	CoRR		turing test;convolutional neural network;pattern recognition;artificial intelligence;captcha;classifier (linguistics);segmentation;computer science;single character	NLP	31.424762839613134	-67.25900251253759	10235
12c5d2abd2bd07d8bfe59f359da3ecb8525edd8c	characterizing the empirical distribution of prokaryotic genome n-mers in the presence of nullomers	dna;probability;genome analysis;statistics	Characterizing the empirical distribution of the frequency of n-mers is a vital step in understanding the entire genome. This will allow for researchers to examine how complex the genome really is, and move beyond simple, traditional modeling frameworks that are often biased in the presence of abundant and/or extremely rare words. We hypothesize that models based on the negative binomial distribution and its zero-inflated counterpart will characterize the n-mer distributions of genomes better than the Poisson. Our study examined the empirical distribution of the frequency of n-mers (6 ≤ n ≤ 11) in 2,199 genomes. We considered four distributions: Poisson, negative binomial, zero-inflated Poisson, and zero-inflated negative binomial (ZINB). The number of genomes that have nullomers in 6-, 7-, and 8-mers was 150, 602 and 2,012, respectively, whereas all of the genomes for the 9-, 10-, and 11-mers had nullomers. In each n-mer considered, the negative binomial model performed the best for at least 93% of the 2,199 genomes; however, a small percentage (i.e., <7%) of the genomes did prefer the ZINB. The negative binomial and zero-inflation distributions extend the traditional Poisson setting and are more flexible in handling overdispersion that can be caused by an increase in nullomers. In an effort to characterize the distribution of the frequency of n-mers, researchers should also consider other discrete distributions that are more flexible and adjust for possible overdispersion.	arabic numeral 0;discrete distribution;genome;handling (psychology);k-mer;mer;negative base	Loni Philip Tabb;Wei Zhao;Jingyu Huang;Gail L. Rosen	2014	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2014.0108	biology;combinatorics;bioinformatics;probability;mathematics;dna;statistics	Comp.	4.65724092818218	-53.312958991558304	10241
0f513bfa9c3db7713fdbafc52215473fd48573aa	feature fusion: parallel strategy vs. serial strategy	complex feature space;image database;linear discriminate analysis;feature space;face recognition;principal component analysis pca;feature extraction;principal component analysis;feature fusion;classification accuracy;character recognition;k l expansion;linear discriminant analysis lda	A new strategy of parallel feature fusion is introduced in this paper. A complex vector is first used to represent the parallel combined features. Then, the traditional linear projection analysis methods, including principal component analysis, K–L expansion and linear discriminant analysis, are generalized for feature extraction in the complex feature space. Finally, the developed parallel feature fusion methods are tested on CENPARMI handwritten numeral database, NUST603 handwritten Chinese character database and ORL face image database. The experimental results indicate that the classification accuracy is increased significantly under parallel feature fusion and also demonstrate that the developed parallel fusion is more effective than the classical serial feature fusion.		Jian Yang;Jingyu Yang;David Zhang;Jianfeng Lu	2003	Pattern Recognition	10.1016/S0031-3203(02)00262-5	facial recognition system;speech recognition;feature vector;feature;feature extraction;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;k-nearest neighbors algorithm;feature;dimensionality reduction;principal component analysis	Vision	34.10000786209023	-58.377251541617944	10259
94ca3797351671121b85c1c9f6e3643ea48cf88d	alteration of autonomic blood pressure control during hemodialysis in peripheral vascular disease patients	biomedical monitoring;patient treatment blood blood pressure measurement cardiology diseases kidney neurophysiology;pulse pressure;heart rate variability;fluids;sympathetic activity autonomic blood pressure control hemodialysis peripheral vascular disease pathophysiology vascular calcification end stage renal disease heart rate variability autonomic nervous system pulse pressure autonomic peripheral control;end stage renal disease;cardiology;blood pressure control;blood pressure;autonomic nervous system;hemodialysis;adaptation physiological aged autonomic nervous system baroreflex blood pressure feedback physiological humans kidney failure chronic male middle aged peripheral vascular diseases renal dialysis;blood;immune system;high definition video;diseases;patient treatment;vascular calcification;high definition video blood pressure biomedical monitoring diseases fluids heart rate variability immune system;neurophysiology;blood pressure measurement;high definition;peripheral vascular disease;kidney	Blood pressure (BP) response to volume depletion induced by hemodialysis (HD) treatment may be important to understand the pathophysiology of the increased mortality in HD patients with vascular calcification. In the present study a comparison between end stage renal disease (ESRD) patients affected by peripheral vascular disease (PVD) and ESRD patients without PVD was performed. Continuous blood pressure was recorded at the beginning and at the end of HD. BP and heart rate variability (HRV) were analyzed to quantify the autonomic nervous system regulation of heart beat and peripheral resistance. PVD patients showed an increase of pulse pressure (PP) during HD, an altered autonomic peripheral control, a lower sympathetic activity, with respect to ESRD patients without PVD.	autonomic computing;autonomic nervous system;depletion region;heart rate variability;hemodialysis;kidney diseases;kidney failure, chronic;nervous system structure;patients;peripheral vascular diseases;physical vapor deposition;pulse pressure;vascular calcification	Jasmine Ion Titapiccolo;Sergio Cerutti;Francesco Garzotto;Dinna Cruz;Ulrich Moissl;Ciro Tetta;Maria G. Signorini;Claudio Ronco;Manuela Ferrario	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091406	neuroscience;intensive care medicine;medicine;blood pressure;neurophysiology;diabetes mellitus;cardiology	Visualization	17.840802157253687	-83.47857479589258	10277
2b0275a5fc999d2115f9b383738689a1512aa92a	imf-based chaotic characterization of ap and ml visually-driven postural responses		The objective was to analyze visually driven postural responses and characterize any non-linear behaviour. We recorded physiological responses for two adults, 260 trials each. The subjects maintained quite stance while fixating for four seconds within an immersive room, EON Icube, where the reference to the visual stimuli, i.e., the virtual platform, randomly oscillated in Gaussian orientation 90° and 270° for antero-posterior (AP), and, 0° and 180° for medio-lateral (ML) at three different frequencies (0.125, 0.25, and 0.5 Hz). We accomplished stationary derivatives of posture time series by taking the intrinsic mode functions (IMFs). The phase space plot of IMF shows evidence of the existence of non-linear attractors in both ML and AP. Correlation integral slope with increasing embedding dimension is similar to random white noise for ML, and similar to non-linear chaotic series for AP. Next, recurrence plots indicate the existence of more non-linearity for AP than that for ML. The patterns of the dots after 200th time stamp (near onset) appears to be aperodic in AP. At higher temporal windows, AP entropy tends more toward chaotic series, than that of ML. There are stronger non-linear components in AP than that in ML regardless of the speed conditions. © (2013) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Hanif Azhar;Guillaume Giraudet;Jocelyn Faubert	2013		10.1117/12.2003047	real-time computing;simulation;telecommunications	ML	22.758391962226334	-92.12590721502453	10283
5f2dbe0a70cdbbcd33934fe0d0eaec5e9ab962df	two new scale-adapted texture descriptors for image segmentation	optimal test;test kolmogorov smirnov;complexite;metodo adaptativo;adaptacion;analyse amas;entropia;effet echelle;image segmentation;scale effect;image processing;kolmogorov smirnov test;texture image;speech processing;complejidad;gradiente;tratamiento palabra;procesamiento imagen;traitement parole;methode adaptative;texture segmentation;complexity;gradient;probabilistic approach;classification;effet dimensionnel;traitement image;image texture;test optimal;optimal scaling;cluster analysis;polaridad;enfoque probabilista;approche probabiliste;size effect;clustering method;adaptation;efecto escala;entropie;adaptive method;segmentation image;pattern recognition;analisis cluster;entropy;reconnaissance forme;efecto dimensional;polarity;reconocimiento patron;polarite;clasificacion;scale dependence	In texture segmentation it is key to develop descriptors which provide acceptable results without a significant increment of their temporal complexity. In this contribution, we propose two probabilistic texture descriptors: polarity and texture contrast. These descriptors are related to the entropy of both the local distributions of gradient orientation and magnitude. As such descriptors are scale-dependent, we propose a simple method for selecting the optimal scale. Using the features at their optimal scale, we test the performance of these measures with an adaptive version of the ACM clustering method, in which adaptation relies on the Kolmogorov-Smirnov test. Our results with only these two descriptors are very promising.	image segmentation	Miguel Angel Lozano;Francisco Escolano	2003		10.1007/978-3-540-24586-5_16	computer vision;entropy;image processing;computer science;artificial intelligence;speech processing;mathematics;scale-space segmentation	Vision	45.075432037500434	-62.317395609169246	10284
48c92013bd1fd410210399f5521bc6adf1998e72	arousal detection for biometric data in built environments using machine learning		This paper describes an approach using wearables to demonstrate the viability of measuring physiometric arousal indicators such as heart rate in assessing how urban built environments can induce physiometric arousal indicators in a subject. In addition, a machine learning methodology is developed to classify sensor inputs based on annotated arousal output as a target. The results are then used as a foundation for designing and implementing an affective intelligent systems framework for arousal state detection via supervised learning and classification.	biometrics;machine learning;supervised learning;wearable computer	Heath Yates;Brent Chamberlain;Greg Norman;William H. Hsu	2017			machine learning;biometrics;computer vision;arousal;computer science;artificial intelligence	AI	5.4257378051504315	-85.63580179232216	10285
8299ac51fc32e4078e259b16f78511f28f88d36d	rapid estimation of camera motion from compressed video with application to video annotation	tratamiento datos;extraction information;probable shots camera motion estimation processing time reduction video annotation digital video video searching digital video libraries management content based retrieval content based browsing structured video mpeg compressed video storage efficiency basketball annotation system high level content analysis wide angle views close up views fast breaks;analisis contenido;content based retrieval code standards telecommunication standards video coding data compression motion estimation search problems;estimation mouvement;navegacion informacion;algorithm performance;image processing;data compression;information extraction;forme onde;information retrieval;navigation information;digital library;mpeg video;estimacion movimiento;information browsing;procesamiento imagen;data processing;motion estimation;traitement donnee;stockage donnee;prior knowledge;segmentation;code standards;indexing terms;traitement image;experimental result;processing time;development tool;video coding;camera motion;data storage;content analysis;biblioteca electronica;motion estimation cameras video compression transform coding information analysis content management software libraries content based retrieval motion analysis data mining;forma onda;senal video;signal video;recherche information;resultado algoritmo;telecommunication standards;performance algorithme;resultado experimental;video annotation;video signal;temps traitement;almacenamiento datos;electronic library;search problems;digital video;waveform;compresion dato;recuperacion informacion;analyse contenu;classification automatique;resultat experimental;automatic classification;clasificacion automatica;digital video library;content based retrieval;tiempo proceso;video search;segmentacion;bibliotheque electronique;compression donnee;compressed video;extraction informacion	As digital video becomes more pervasive, e cient ways of searching and annotating video according to content will be increasingly important. Such tasks arise, for example, in the management of digital video libraries for content-based retrieval and browsing. In this paper, we develop tools based on camera motion for analyzing and annotating a class of structured video using the low-level information available directly from MPEG compressed video. In particular, we show that in certain structured settings it is possible to obtain reliable estimates of camera motion by directly processing data easily obtained from the MPEG format. Working directly with the compressed video greatly reduces the processing time and enhances storage e ciency. As an illustration of this idea, we have developed a simple basketball annotation system which combines the low-level information extracted from an MPEG stream with the prior knowledge of basketball structure to provide high level content analysis, annotation and browsing for events such as wide-angle and close-up views, fast breaks, probable shots at the basket, etc. The methods used in this example should also be useful in the analysis of high-level content of structured video in other domains.	data compression;digital video;high- and low-level;high-level programming language;library (computing);moving picture experts group;pervasive informatics	Yap-Peng Tan;Drew D. Saur;Sanjeev R. Kulkarni;Peter J. Ramadge	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.825867	video compression picture types;data compression;computer vision;digital library;waveform;index term;data processing;content analysis;image processing;computer science;video tracking;motion estimation;computer data storage;multimedia;video processing;smacker video;motion compensation;segmentation;information extraction;multiview video coding;computer graphics (images)	Vision	47.60033685600665	-54.97393453954202	10289
978dc9c2fadc5b4e35797a837a7031f556b7f395	facial expression recognition based on local double binary mapped pattern		Local feature descriptors play an important role in facial expression recognition. Local Binary Pattern (LBP) only considers the signal information of the difference between the gray value of the center pixel and the neighbor pixel. It does not take the magnitude information into consideration and has poor robustness. Local Mapped Pattern (LMP) is not ideal for discriminating differences between different textures and experimental result is not excellent. This paper proposes a novel feature descriptor based on gray-level difference mapping, called Local Double Binary Mapped Pattern (LDBMP).This new approach is an improvement over the previous LBP and LMP, not only retains the advantages of LBP and LMP but also preserves the information of magnitude and captures nuances that occur in the image. In our experiments, the new descriptor performs favorably.		Chunjian Yang;Min Hu;Yaqin Zheng;Xiaohua Wang;Yong Gao;Hao Wu	2018		10.1007/978-3-030-00767-6_45	robustness (computer science);local binary patterns;computer vision;pixel;artificial intelligence;computer science;magnitude (mathematics);pattern recognition;facial expression;binary number	Vision	36.277546472058624	-58.684530850082794	10294
5aa082b0fcfba0c96228d1044111fa7fb23e659a	prenatal development of ocular dominance and orientation maps in a self-organizing model of v1	spontaneous activity;development;orientation map;ocular dominance;self organization;visual cortex;ocular dominance maps;orientation maps	How orientation and ocular-dominance maps develop before visual experience begins is controversial. Possible influences include molecular signals and spontaneous activity, but their contributions remain unclear. This paper presents LISSOM simulations suggesting that previsual spontaneous activity alone is sufficient for realistic OR and OD maps to develop. Individual maps develop robustly with various previsual patterns, and are aided by background noise. However, joint OR/OD maps depend crucially on how correlated the patterns are between eyes, even over brief initial periods. Therefore, future biological experiments should account for multiple activity sources, and should measure map interactions rather than maps of single features.	experiment;google analytics;interaction;map;neural oscillation;organizing (structure);self-organization;simulation;spontaneous order	Stefanie Jegelka;James A. Bednar;Risto Miikkulainen	2006	Neurocomputing	10.1016/j.neucom.2005.12.094	computer vision;self-organization;computer science;ocular dominance	ML	18.55274611550855	-73.87075330897521	10310
bf41e295594a31f000dd71a5c52fef257c6516cd	on discontinuity-adaptive smoothness priors in computer vision	minimisation;ill posed problems;modele comportement;behavior model;minimization;discontinuity;vision ordenador;discontinuite;image motion analysis;modelo markov;application software;equation euler;ill posed problems discontinuity adaptive smoothness priors computer vision markov random fields low level vision euler equation energy minimization necessary condition adaptive interaction functions;modelo comportamiento;coaccion;smoothing methods minimisation computer vision markov processes;contrainte;markov random fields;integrated optics;surface texture;indexing terms;energy functions;surface reconstruction;markov random field;energy function;regularization;computer vision;probabilistic model;smoothing methods;markov model;constraint;campo aleatorio;discontinuities;necessary condition;difference equations;adaptive interaction functions;image reconstruction;ecuacion euler;computer vision markov random fields difference equations differential equations application software surface reconstruction surface texture image reconstruction integrated optics image motion analysis;modele probabiliste;vision ordinateur;adaptive smoothing;differential equations;discontinuidad;energy minimization;markov processes;modele markov;low level vision;champ aleatoire;modelo probabilista;discontinuity adaptive smoothness priors;random field;euler equation	|A variety of analytic and probabilistic models in connection to Markov random elds (MRFs) have been proposed in the last decade for solving low level vision problems involving discontinuities. This paper presents a systematic study on these models and deenes a general discontinuity adaptive (DA) MRF model. By analyzing the Euler equation associated with the energy minimization, it shows that the fundamental diierence between diierent models lies in the behavior of interaction between neighboring points, which is determined by the a priori smoothness constraint encoded into the energy function An important necessary condition is derived for the interaction to be adaptive to discontinu-ities to avoid oversmoothing. This forms the basis on which a class of adaptive interaction functions (AIFs) is deened. The DA model is deened in terms of the Euler equation constrained by this class of AIFs. Its solution is C 1 continuous and allows arbitrarily large but bounded slopes in dealing with discontinuities. Because of the continuous nature, it is stable to changes in parameters and data, a good property for regularizing ill-posed problems. Experimental results are shown.	computer vision;energy minimization;euler;markov chain;markov random field;mathematical optimization;reflections of signals on conducting lines;well-posed problem	Stan Z. Li	1995	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.387504	iterative reconstruction;behavioral modeling;surface finish;statistical model;regularization;minimisation;mathematical optimization;mathematical analysis;application software;random field;recurrence relation;index term;surface reconstruction;computer science;discontinuity;mathematics;geometry;markov process;markov model;constraint;euler equations;energy minimization;differential equation;statistics;classification of discontinuities	Vision	52.22581496416735	-56.43553428746069	10321
e29d104c8f139ce90fb8d78c9a814e15eab64c56	magnetic resonance field strength effects on diffusion measures and brain connectivity networks	graph theory;neural pathways;brain;high field mri;dti;high angular resolution diffusion imaging hardi;brain network analysis;brain mapping;image interpretation computer assisted;nerve net;fractional anisotropy;humans;signal to noise ratio;tractography;diffusion magnetic resonance imaging	The quest to map brain connectivity is being pursued worldwide using diffusion imaging, among other techniques. Even so, we know little about how brain connectivity measures depend on the magnetic field strength of the scanner. To investigate this, we scanned 10 healthy subjects at 7 and 3 tesla-using 128-gradient high-angular resolution diffusion imaging. For each subject and scan, whole-brain tractography was used to estimate connectivity between 113 cortical and subcortical regions. We examined how scanner field strength affects (i) the signal-to-noise ratio (SNR) of the non-diffusion-sensitized reference images (b(0)); (ii) diffusion tensor imaging (DTI)-derived fractional anisotropy (FA), mean, radial, and axial diffusivity (MD/RD/AD), in atlas-defined regions; (iii) whole-brain tractography; (iv) the 113 × 113 brain connectivity maps; and (v) five commonly used network topology measures. We also assessed effects of the multi-channel reconstruction methods (sum-of-squares, SOS, at 7T; adaptive recombine, AC, at 3T). At 7T with SOS, the b0 images had 18.3% higher SNR than with 3T-AC. FA was similar for most regions of interest (ROIs) derived from an online DTI atlas (ICBM81), but higher at 7T in the cerebral peduncle and internal capsule. MD, AD, and RD were lower at 7T for most ROIs. The apparent fiber density between some subcortical regions was greater at 7T-SOS than 3T-AC, with a consistent connection pattern overall. Suggesting the need for caution, the recovered brain network was apparently more efficient at 7T, which cannot be biologically true as the same subjects were assessed. Care is needed when comparing network measures across studies, and when interpreting apparently discrepant findings.		Liang Zhan;Bryon A. Mueller;Neda Jahanshad;Yan Jin;Christophe Lenglet;Essa Yacoub;Guillermo Sapiro;Kamil Ugurbil;Noam Y Harel;Arthur W. Toga;Kelvin O. Lim;Paul M. Thompson	2013	Brain connectivity	10.1089/brain.2012.0114	psychology;neuroscience;tractography;nuclear magnetic resonance;nuclear medicine	ML	21.451425493952716	-79.65462132827746	10343
9e32d358ad8e12c73270a73ed7b395ee97e094e4	regularized background adaptation: a novel learning rate control scheme for gaussian mixture modeling	teletrafic;modelizacion;processus gauss;evaluation performance;learning rate;surveillance background subtraction gaussian mixture modeling learning rate control;video surveillance;performance evaluation;image resolution;learning;gaussian processes;flow rate regulation;surveillance;computer model;evaluacion prestacion;gestion trafic;localization;heuristic method;learning rate control scheme;video surveillance gaussian processes image resolution learning artificial intelligence;traffic control;false alarm rate;metodo heuristico;maintenance engineering;arriere plan;localizacion;traffic management;gaussian mixture modeling;regularized background adaptation;surveillance scenario;aprendizaje;modelisation;sensitivity;gmm approach;apprentissage;teletrafico;background;gaussian mixture model;computational modeling;adaptation model;localisation;foreground;monitoring;high level feedback;regulation debit;gmm approach regularized background adaptation learning rate control scheme gaussian mixture modeling surveillance scenario image pixel high level feedback heuristic rooting false foreground alarm over quick lighting change;pixel adaptation model computational modeling maintenance engineering sensitivity feedback control robustness;robustesse;pixel;background subtraction;teletraffic;false foreground alarm;gestion trafico;heuristic rooting;avant plan;robustness;learning rate control;teoria mezcla;methode heuristique;monitorage;gaussian process;regulation trafic;learning artificial intelligence;proceso gauss;monitoreo;taux fausse alarme;mixture theory;modeling;porcentaje falsa alarma;theorie melange;article;image pixel;feedback control;regulacion trafico;regulacion caudal;over quick lighting change;robustez	To model a scene for background subtraction, Gaussian mixture modeling (GMM) is a popular choice for its capability of adaptation to background variations. However, GMM often suffers from a tradeoff between robustness to background changes and sensitivity to foreground abnormalities and is inefficient in managing the tradeoff for various surveillance scenarios. By reviewing the formulations of GMM, we identify that such a tradeoff can be easily controlled by adaptive adjustments of the GMM's learning rates for image pixels at different locations and of distinct properties. A new rate control scheme based on high-level feedback is then developed to provide better regularization of background adaptation for GMM and to help resolving the tradeoff. Additionally, to handle lighting variations that change too fast to be caught by GMM, a heuristic rooting in frame difference is proposed to assist the proposed rate control scheme for reducing false foreground alarms. Experiments show the proposed learning rate control scheme, together with the heuristic for adaptation of over-quick lighting change, gives better performance than conventional GMM approaches.	acclimatization;background subtraction;clarify;congenital abnormality;discriminant;google map maker;gradient;heuristic;high- and low-level;mental suffering;normal statistical distribution;observable;pixel;plant roots;racepinephrine;review [publication type];scalability;weight	Horng-Horng Lin;Jen-Hui Chuang;Tyng-Luh Liu	2011	IEEE Transactions on Image Processing	10.1109/TIP.2010.2075938	maintenance engineering;computer vision;simulation;speech recognition;computer science;machine learning;gaussian process;statistics	Vision	47.04493297652072	-56.5405920413131	10345
3d862343fa3750171bc0951ef1acd1bc206fce65	intrinsic insula network engagement underlying children's reading and arithmetic skills	arithmetic;central executive network;cognitive control;connectivity;individual difference;insula;reading;resting state;salience network	The neural substrates of children's reading and arithmetic skills have long been of great interest to cognitive neuroscientists. However, most previous studies have focused on the contrast between these skills as specific domains. Here, we investigate the potentially shared processes across these domains by focusing on how the neural circuits associated with cognitive control influence reading and arithmetic proficiency in 8-to-10-year-old children. Using a task-free resting state approach, we correlated the intrinsic functional connectivity of the right anterior insula (rAI) network with performance on assessments of Chinese character recognition, reading comprehension, subtraction, and multiplication performance. A common rAI network strengthened for reading and arithmetic skill, including the right middle temporal gyrus (MTG) and superior temporal gyrus (STG) in the lateral temporal cortex, as well as the inferior frontal gyrus (IFG). In addition, performance measures evidenced rAI network specializations. Single character recognition was uniquely associated with connectivity to the right superior parietal lobule (SPL). Reading comprehension only, rather than character recognition, was associated with connectivity to the right IFG, MTG and angular gyrus (AG). Furthermore, subtraction was associated with connectivity to premotor cortex whereas multiplication was associated with the supramarginal gyrus. Only reading comprehension and multiplication were associated with hyper connectivity within local rAI network. These results indicate that during a critical period for children's acquisition of reading and arithmetic, these skills are supported by both intra-network synchronization and inter-network connectivity of rAI circuits. Domain-general intrinsic insular connectivity at rest contained also, functional components that segregated into different sets of skill-related networks. The embedded components of cognitive control may be essential to understanding the interplay of multiple functional circuits necessary to more fully characterize cognitive skill acquisition.	angularjs;cognitive tutor;contain (action);embedded system;embedding;escuela superior latinoamericana de informática;evaluation procedure;frontal lobe gyrus;hyperactive behavior;ifng gene;inferior frontal gyrus;insula of reil;lateral thinking;multiplication;optical character recognition;parahippocampal gyrus;rest;resting state fmri;star trek generations;structure of angular gyrus;structure of anterior hypothalamic nucleus;structure of middle temporal gyrus;structure of superior parietal lobule;superior temporal gyrus;temporal lobe	Klarissa Ting-Ting Chang;Pei-Hong Lee;Arron W. S. Metcalfe	2018	NeuroImage	10.1016/j.neuroimage.2017.11.027	angular gyrus;inferior frontal gyrus;premotor cortex;superior temporal gyrus;cognitive psychology;psychology;developmental psychology;resting state fmri;cognition;supramarginal gyrus;arithmetic;reading comprehension	AI	19.028411864187945	-77.63888356568035	10349
c7b95e6182a4e02250d28e55d2fa009f483f7de0	a regularized discriminative framework for eeg analysis with application to brain–computer interface	spatio temporal factorization;discriminative modeling of brain imaging signals;brain computer interface;signal analysis;trace norm;group lasso;convex optimization;regularization;empirical risk minimization;discrimination learning;feature extraction;dual spectral norm;brain imaging;discriminative learning;feature selection;p300 speller;electroencephalography;point of view;discriminative model;spectral norm	We propose a framework for signal analysis of electroencephalography (EEG) that unifies tasks such as feature extraction, feature selection, feature combination, and classification, which are often independently tackled conventionally, under a regularized empirical risk minimization problem. The features are automatically learned, selected and combined through a convex optimization problem. Moreover we propose regularizers that induce novel types of sparsity providing a new technique for visualizing EEG of subjects during tasks from a discriminative point of view. The proposed framework is applied to two typical BCI problems, namely the P300 speller system and the prediction of self-paced finger tapping. In both datasets the proposed approach shows competitive performance against conventional methods, while at the same time the results are easier accessible to neurophysiological interpretation. Note that our novel approach is not only applicable to Brain imaging beyond EEG but also to general discriminative modeling of experimental paradigms beyond BCI.	brain–computer interface;convex optimization;electroencephalography phase synchronization;empirical risk minimization;feature extraction;feature selection;interface device component;mathematical optimization;optimization problem;signal processing;sparse matrix	Ryota Tomioka;Klaus-Robert Müller	2010	NeuroImage	10.1016/j.neuroimage.2009.07.045	brain–computer interface;regularization;convex optimization;speech recognition;empirical risk minimization;electroencephalography;feature extraction;matrix norm;machine learning;pattern recognition;mathematics;feature selection;discriminative model;discrimination learning	ML	15.299164367275807	-93.33702017638804	10352
e84f9300d3f47806d81a470102fd2c0fd722bcda	the conformal monogenic signal	degree of freedom;riesz transform;rotation invariance;scale space;local features	The conformal monogenic signal is a novel rotational invariant approach for analyzing i(ntrinsic)1D and i2D local features of twodimensional signals (e.g. images) without the use of any heuristics. It contains the monogenic signal as a special case for i1D signals and combines scale-space, phase, orientation, energy and isophote curvature in one unified algebraic framework. The conformal monogenic signal will be theoretically illustrated and motivated in detail by the relation of the Radon and the Riesz transform. One of the main ideas is to lift up two-dimensional signals to a higher dimensional conformal space where the signal can be analyzed with more degrees of freedom. The most interesting result is that isophote curvature can be calculated in a purely algebraic framework without the need of any derivatives.	contour line;heuristic (computer science);image analysis;scale space;time complexity	Lennart Wietzke;Gerald Sommer	2008		10.1007/978-3-540-69321-5_53	mathematical analysis;topology;mathematics;geometry	Vision	50.487502258539344	-62.7024525771623	10354
d6e51c993c4fb50a938844d8499fa57705ae8b45	an innovative measurement system based on mems for telerehabilitation	biomedical monitoring;magnetic sensors;objective assessment telerehabilitation weareable mems sensors clinical tests;joints;telemedicine biomems body sensor networks patient rehabilitation;monitoring;apulian ict living lab telerehabilitation innovative measurement system development lower limb rehabilitation wearable mem sensor 3d limb orientation central remote unit pro domo sud project apulia region;goniometers;magnetic sensors biomedical monitoring monitoring joints goniometers	This paper proposes the development of an innovative measurement system for telerehabilitation with the aim to provide objective evaluation of functional capacity of patients subject to lower limb rehabilitation. In particular the system has based on the set of wearable MEMs sensors which detect the 3D orientation of the limbs and communicate with a Central Remote Unit for storing and elaborating all measurement data. The proposed study has been performed under the research project PRO-DOMO SUD funded by Apulia Region within the Project “Apulian ICT Living Labs”.	microelectromechanical systems;sensor;system of measurement;wearable computer	Gregorio Andria;Anna Maria Lucia Lanzolla;Giulia Russo;M. Parabita;Raffaele Antonelli Incalzi;G. Cavallo;Monica Benvenuto	2015	2015 IEEE International Symposium on Medical Measurements and Applications (MeMeA) Proceedings	10.1109/MeMeA.2015.7145243	simulation;engineering;physical therapy;biological engineering	Embedded	8.224618320082644	-88.72012314842469	10359
e92505673854afb9767f0a7b60ade3d241c5f432	simultaneous imaging of multiple neurotransmitters and neuroactive substances in the brain by desorption electrospray ionization mass spectrometry	dopamine;imaging;mass spectrometry;neurotransmitter;parkinson's disease;serotonin	With neurological processes involving multiple neurotransmitters and neuromodulators, it is important to have the ability to directly map and quantify multiple signaling molecules simultaneously in a single analysis. By utilizing a molecular-specific approach, namely desorption electrospray ionization mass spectrometry imaging (DESI-MSI), we demonstrated that the technique can be used to image multiple neurotransmitters and their metabolites (dopamine, dihydroxyphenylacetic acid, 3-methoxytyramine, serotonin, glutamate, glutamine, aspartate, γ-aminobutyric acid, adenosine) as well as neuroactive drugs (amphetamine, sibutramine, fluvoxamine) and drug metabolites in situ directly in brain tissue sections. The use of both positive and negative ionization modes increased the number of identified molecular targets. Chemical derivatization by charge-tagging the primary amines of molecules significantly increased the sensitivity, enabling the detection of low abundant neurotransmitters and other neuroactive substances previously undetectable by MSI. The sensitivity of the imaging approach of neurochemicals has a great potential in many diverse applications in fields such as neuroscience, pharmacology, drug discovery, neurochemistry, and medicine.	-desis;3,4-dihydroxyphenylacetic acid;3-methoxytyramine;adenosine;amine oxidase (copper-containing);amines;amphetamine;aspartic acid;cell signaling;dopamine;drug discovery;experiment;fluvoxamine;glutamic acid;glutamine;ions;msi protocol;mass spectrometry;metabolite;microsatellite instability;n-methylsuccinimide;neostriatum;neuromodulators;neuroscience discipline;neurotransmitters;pharmacology;quantitation;rainbow 100;sagittal plane;science of neurochemistry;semiconductor industry;serotonin;spectrometry, mass, electrospray ionization;spectrometry, mass, matrix-assisted laser desorption-ionization;substantia nigra structure;tandem mass spectrometry;thioctic acid;undetectable;ventral striatum;pars reticulata of substantia nigra;sibutramine;substance	Mohammadreza Shariatgorji;Nicole Strittmatter;Anna Nilsson;Patrik Källback;Alexandra Alvarsson;Xiaoqun Zhang;Theodosia Vallianatou;Per Svenningsson;Richard J. A. Goodwin;Per E. Andrén	2016	NeuroImage	10.1016/j.neuroimage.2016.05.004	drug discovery;dopamine;psychology;cognitive psychology;mass spectrometry imaging;mass spectrometry;neurotransmitter;desorption electrospray ionization;neurochemistry;chromatography;amphetamine	Vision	11.539772877155434	-65.03689967709491	10388
b4bba5374535071db5891bf7b034d6e90ae0656f	interactive natural image segmentation via spline regression	analisis imagen;fonction green;sistema interactivo;spline;greens function;interpolation;green s function methods;image segmentation;image processing;funcion green;complexite calcul;algorithme k moyenne;esplin;procesamiento imagen;algorithms animals cluster analysis humans image processing computer assisted nonlinear dynamics photography regression analysis reproducibility of results;ecuacion lineal;arriere plan;traitement image;surges;splines mathematics;systeme conversationnel;sobolev space;aproximacion esplin;complejidad computacion;background;espacio sobolev;foreground;splines mathematics green s function methods image segmentation interpolation regression analysis;interactive system;image segmentation spline clustering algorithms laboratories automation equations interpolation computational complexity image processing surges;computational complexity;spline approximation;approximation spline;segmentation image;signal classification;spline regression interactive natural image segmentation k means clustering;classification signal;clustering algorithms;avant plan;algoritmo k media;k means clustering algorithm;k means algorithm;image analysis;regression analysis;classification automatique;espace sobolev;linear equation;automatic classification;spline regression;interpolation interactive natural image segmentation spline regression greens function linear equation k means clustering algorithm;clasificacion automatica;analyse image;interactive natural image segmentation;k means clustering;green function;equation lineaire;automation	This paper presents an interactive algorithm for segmentation of natural images. The task is formulated as a problem of spline regression, in which the spline is derived in Sobolev space and has a form of a combination of linear and Green's functions. Besides its nonlinear representation capability, one advantage of this spline in usage is that, once it has been constructed, no parameters need to be tuned to data. We define this spline on the user specified foreground and background pixels, and solve its parameters (the combination coefficients of functions) from a group of linear equations. To speed up spline construction, K-means clustering algorithm is employed to cluster the user specified pixels. By taking the cluster centers as representatives, this spline can be easily constructed. The foreground object is finally cut out from its background via spline interpolation. The computational complexity of the proposed algorithm is linear in the number of the pixels to be segmented. Experiments on diverse natural images, with comparison to existing algorithms, illustrate the validity of our method.	algorithm;cluster analysis;coefficient;computational complexity theory;experiment;image segmentation;interpolation imputation technique;k-means clustering;linear equation;matrix regularization;nonlinear system;pixel;smoothing spline;spline (mathematics);spline interpolation;transduction (machine learning);biologic segmentation;statistical cluster	Shiming Xiang;Feiping Nie;Chunxia Zhang;Changshui Zhang	2009	IEEE Transactions on Image Processing	10.1109/TIP.2009.2018570	spline interpolation;spline;computer vision;mathematical optimization;image analysis;smoothing spline;image processing;computer science;machine learning;cubic hermite spline;mathematics;geometry;thin plate spline;polyharmonic spline;statistics;k-means clustering	Vision	51.74846563813089	-68.36172031534208	10394
bc9d0c71e450a0aa98f81fa42508b8f6aa9504ee	scalable patients tracking framework for mass casualty incidents	pedestrian safety;time measurement;poison control;injury prevention;active rfid tags;algorithms cellular phone emergency medical tags emergency medical technicians mass casualty incidents massachusetts radio frequency identification device triage;mobility anchor point;global position system;safety literature;rfid tag;traffic safety;injury control;wireless sensor network;home safety;accuracy;injury research;safety abstracts;human factors;global positioning system accuracy time measurement mobile communication active rfid tags wireless sensor networks;global positioning system;occupational safety;safety;mobile communication;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;falls;ergonomics;wireless sensor networks;suicide prevention	We introduce a system that tracks patients in a Mass Casualty Incident (MCI) using active RFID triage tags and mobile anchor points (DM-tracks) carried by the paramedics. The system does not involve any fixed deployment of the localization devices while maintaining a low cost triage tag. The localization accuracy is comparable to GPS systems without incurring the cost of providing a GPS based device to every patient in the disaster scene.	deploy;gray platelet syndrome;mass casualty incidents;patients;radio frequency identification device;radio-frequency identification;reduced cost;tracer;track (course);triage;decimeter	Xunyi Yu;Aura Ganz	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090224	simulation;wireless sensor network;computer science;engineering;human factors and ergonomics;forensic engineering;computer security	Robotics	6.494074855837703	-88.26733933074107	10396
64d599c21359f03d6224f749d2b16b47cef889bc	automatic detection of retinal vascular bifurcations and crossovers based on isotropy and anisotropy	databases;cardiovascular disorders;anisotropy;matrices;image registration;gaussian filters	The analysis of retinal blood vessels is very important in the detection of some diseases in early stages, such as hypertension, diabetes, arteriosclerosis, cardiovascular disease, and stroke. The bifurcations and crossovers are important feature points, which play important roles in the analysis of the retinal vessel tree. These feature points have been demonstrated to be important features in many visual tasks such as image registration, mosaicing, and segmentation. In this paper, a new method is proposed to detect vascular bifurcations and crossovers in fundus images. The Gaussian filter is applied to the blue channel of the original color retinal images to suppress the central reflex and reduce the candidate points. The eigenvalues and eigenvectors of Hessian matrix are then obtained in multiple scales to provide the structural and directional information. By computing the anisotropy and isotropy of neighboring image segments for each pixel in a retinal image, we define a multi-scale vessel filter which combines the responses of tubular structures and the responses of bifurcations and crossovers. Finally, the proposed method has been tested with publicly available database STARE and DRIVE. The experimental results show that bifurcations, crossovers and tubular structures can be detected simultaneously. And the proposed method performs well in detecting the bifurcations and crossovers which are in thin vessels or low contrast vessels. © (2013) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	audio crossover	Guodong Li;Dehui Xiang;Fei Yang;Xiaonan Wan;Xin Yang;Jie Tian	2013		10.1117/12.2006710	computer vision;simulation;image registration;optics;anisotropy;physics;matrix	Vision	40.205334259326904	-75.1151323494411	10398
70870bfcfdf251c7b494fbb093a3d981ef00c058	clinical programming software to manage patient's data with a cochlear implantat	speech perception;clinical programming;speech processor;sequence diagram;cochlear implant system	Auditory prostheses (AP) using Cochlear Implant System a.k.a bionic ears are widely used electronic devices that electrically stimulate the auditory nerve using an electrode array, surgically placed in the inner ear for patients suffering from severe to profound senosorineural deafness. The AP mainly contains an external Body Worn Speech Processor (BWSP) and an internal Implantable Receiver Stimulator (IRS). The BWSP receives an external sound or speech and generates encoded speech data bits for transmission to the IRS via a Radio Frequency transcutaneous link to excite the electrode array. After surgical placement of the electrode array in the inner ear, the BWSP should be fine-tuned to achieve the 80 to 100% speech reception abilities of the patient by an audiologist using Clinical Programming Software (CPS). The tuning process involves several tasks such as identifying the active electrode contacts, determining the detection and pain threshold of each active electrode, and loading these values into BWSP by reprogramming the BWSP. The main objective of this paper is to describe a simple personal-computer based, user-friendly CPS, which fine tunes the BWSP to achieve the best possible speech reception abilities of each patient and to perform post-operative fitting procedures by an audiologist. The CPS was developed to perform the post-operative fine tuning procedures such as (i) measurement of electrode tissue impedance, (ii) fitting to determine the hearing threshold and comfort levels for each active electrode, and (iii) reprogramming the speech processor using the identified threshold and comfort values. Finally, experimental results are presented.	auditory processing disorder;characteristic impedance;cochlear implant;excite;personal computer;programming tool;radio frequency;usability	V. Bhujanga Rao;Seetha Ramaiah Panchumarthy;K. Raja Kumar	2013	ACM SIGSOFT Software Engineering Notes	10.1145/2464526.2464535	sequence diagram;speech recognition;speech perception;engineering	HCI	9.823865417315286	-90.00240884961705	10402
97e7baf1a67c7d002f7ade068cf9bdea12285a18	automatic left ventricle detection in echocardiographic images for deformable contour initialization	automatic;histograms;left;speckle;image segmentation;contour;bepress selected works;ultrasonic imaging;speckle noise;computational time automatic left ventricle detection 2d echocardiographic image deformable contour initialization watershed segmentation speckle noise;left ventricular;deformable models;detection;left ventricle;image enhancement;image segmentation deformable models ultrasonic imaging histograms noise speckle muscles;ventricle;medical image processing echocardiography image enhancement image segmentation;papillary muscle;medical image processing;algorithms automatic data processing cardiology contrast media echocardiography heart ventricles humans image interpretation computer assisted image processing computer assisted models statistical signal processing computer assisted time factors;watershed segmentation;echocardiography;boundary detection;automatic left ventricle detection echocardiographic images for deformable contour initialization;for;deformable model;initialization;images;deformable;noise;muscles;echocardiographic	The accurate left ventricular boundary detection in echocardiographic images allow cardiologists to study and assess cardiomyopathy in patients. Due to the tedious and time consuming manner of manually tracing the borders, deformable models are generally used for left ventricle segmentations. However, most deformable models require a good initialization, which is usually outlined manually by the user. In this paper, we propose an automated left ventricle detection method for two-dimensional echocardiographic images that could serve as an initialization for deformable models. The proposed approach consists of pre-processing and post-processing stages, coupled with the watershed segmentation. The pre-processing stage enhances the overall contrast and reduces speckle noise, whereas the post-processing enhances the segmented region and avoids the papillary muscles. The performance of the proposed method is evaluated on real data. Experimental results show that it is suitable for automatic contour initialization since no prior assumptions nor human interventions are required. Besides, the computational time taken is also lower compared to an existing method.	cardiomyopathies;computation;heart ventricle;left ventricular structure;patients;preprocessor;structure of papillary muscle;time complexity;video post-processing;watershed (image processing);algorithm;biologic segmentation	Cher Hau Seng;Ramazan Demirli;Moeness G. Amin;Jason L. Seachrist;Abdesselam Bouzerdoum	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091823	speckle pattern;speckle noise;computer vision;initialization;radiology;watershed;computer science;noise;histogram;image segmentation;automatic transmission;statistics;computer graphics (images)	Vision	39.26243002347944	-78.69709949926249	10412
6fb009713218d62e8a32db0a87a82221551e6d3a	associating genes and protein complexes with disease via network propagation	genes;protein complex;human health;disease;prior information;male;databases genetic;diabetes mellitus;protein protein interactions;genetic causes of cancer;genetic networks;proteins;prostatic neoplasms;random walk;and type 2;reproducibility of results;protein complexes;multiprotein complexes;algorithms;genetic loci;humans;cross validation;protein interaction mapping;alzheimer disease;protein interaction networks;prostate cancer	A fundamental challenge in human health is the identification of disease-causing genes. Recently, several studies have tackled this challenge via a network-based approach, motivated by the observation that genes causing the same or similar diseases tend to lie close to one another in a network of protein-protein or functional interactions. However, most of these approaches use only local network information in the inference process and are restricted to inferring single gene associations. Here, we provide a global, network-based method for prioritizing disease genes and inferring protein complex associations, which we call PRINCE. The method is based on formulating constraints on the prioritization function that relate to its smoothness over the network and usage of prior information. We exploit this function to predict not only genes but also protein complex associations with a disease of interest. We test our method on gene-disease association data, evaluating both the prioritization achieved and the protein complexes inferred. We show that our method outperforms extant approaches in both tasks. Using data on 1,369 diseases from the OMIM knowledgebase, our method is able (in a cross validation setting) to rank the true causal gene first for 34% of the diseases, and infer 139 disease-related complexes that are highly coherent in terms of the function, expression and conservation of their member proteins. Importantly, we apply our method to study three multi-factorial diseases for which some causal genes have been found already: prostate cancer, alzheimer and type 2 diabetes mellitus. PRINCE's predictions for these diseases highly match the known literature, suggesting several novel causal genes and protein complexes for further investigation.	causal filter;coherence (physics);cross reactions;diabetes mellitus;diabetes mellitus, non-insulin-dependent;factor analysis;inference;interaction;knowledge bases;knowledge base;mental association;nsa product types;numerous;online mendelian inheritance in man;prostatic neoplasms;software propagation	Oron Vanunu;Oded Magger;Eytan Ruppin;Tomer Shlomi;Roded Sharan	2010		10.1371/journal.pcbi.1000641	biology;bioinformatics;multiprotein complex;genetics	Comp.	5.419369972679513	-57.134911634231656	10432
e3bcae7c42887fbd733a996a6c6daac12a81b9fa	survival associated pathway identification with group lp penalized global auc maximization	genes;gene expression profile;health research;uk clinical guidelines;biological patents;female;breast neoplasms diagnosis;neoplasms diagnosis;breast neoplasms therapy;division of biostatistics;neoplasm genetics;europe pubmed central;citation search;physiological cellular and medical topics;neoplasms genetics;gene expression data;thomas jefferson university;genetics;computational biology bioinformatics;survival time;area under curve;uk phd theses thesis;signal transduction genetics;gene expression regulation;survival analysis;life sciences;breast neoplasms genetics;algorithms;mantle cell genetics;humans;cross validation;functional group;department of pharmacology and experimental therapeutics;neoplastic;computational biology;uk research reports;medical journals;prognosis;europe pmc;biomedical research;lymphoma;biological process;bioinformatics	It has been demonstrated that genes in a cell do not act independently. They interact with one another to complete certain biological processes or to implement certain molecular functions. How to incorporate biological pathways or functional groups into the model and identify survival associated gene pathways is still a challenging problem. In this paper, we propose a novel iterative gradient based method for survival analysis with group L p penalized global AUC summary maximization. Unlike LASSO, L p (p < 1) (with its special implementation entitled adaptive LASSO) is asymptotic unbiased and has oracle properties [1]. We first extend L p for individual gene identification to group L p penalty for pathway selection, and then develop a novel iterative gradient algorithm for penalized global AUC summary maximization (IGGAUCS). This method incorporates the genetic pathways into global AUC summary maximization and identifies survival associated pathways instead of individual genes. The tuning parameters are determined using 10-fold cross validation with training data only. The prediction performance is evaluated using test data. We apply the proposed method to survival outcome analysis with gene expression profile and identify multiple pathways simultaneously. Experimental results with simulation and gene expression data demonstrate that the proposed procedures can be used for identifying important biological pathways that are related to survival phenotype and for building a parsimonious model for predicting the survival times.	area under curve;expectation–maximization algorithm;gene expression profiling;gene regulatory network;gradient;iterative method;lasso;microarray analysis;occam's razor;simulation;test data;triangulation	Zhenqiu Liu;Laurence S. Magder;Terry Hyslop;Li Mao	2010		10.1186/1748-7188-5-30	biology;regulation of gene expression;integral;bioinformatics;gene;survival analysis;biological process;genetics;cross-validation	ML	7.567786377959135	-53.3745649730636	10442
8d57f1a39de3af9c0cd29533ec62224194817077	shape and data driven texture segmentation using local binary patterns	filtering;image segmentation;measurement;qa mathematics;training;shape;signal processing;europe;tk electrical engineering electronics nuclear engineering	We prop ose a shape and data driven texture segmentation method using loca l binary patterns (LBP) and active contours. In particular, we pass textured images through a new LBP-based filter, which produces non-textured images. In this “filtered ” doma in each textured region of the original imag e exhibits a characteristic intensity distribution. In this domain we pose the segmentation problem as an optimization problem in a Bayesian framework. The cost functional contains a data-driven term, as well as a term that b ring s in information about the shapes of the objects to be segmented. We solve the optimization problem u sing level set-based active contours. Our experimental results on synthetic and real textures demonstrate the effectiveness of our approach in segmenting challenging textures as well as its robustness to missing data and occlusions.	bayesian network;code;enea ose;experiment;gradient descent;local binary patterns;mathematical optimization;missing data;mutual information;optimization problem;robustness (computer science);spice;synthetic intelligence	Erkin Tekeli;Müjdat Çetin;Aytül Erçil	2007	2007 15th European Signal Processing Conference		computer vision;computer science;pattern recognition;scale-space segmentation;engineering drawing	Vision	50.09536996201275	-71.28347042746671	10471
f04e4c2cc7b9131cf280ebd8c2ec23403be2db9b	design of continuous emg classification approaches towards the control of a robotic exoskeleton in reaching movements		Myoelectric control of rehabilitation devices engages active recruitment of muscles for motor task accomplishment, which has been proven to be essential in motor rehabilitation. Unfortunately, most electromyographic (EMG) activity-based controls are limited to one single degree-of-freedom (DoF), not permitting multi-joint functional tasks. On the other hand, discrete EMG-triggered approaches fail to provide continuous feedback about muscle recruitment during movement. For such purposes, myoelectric interfaces for continuous recognition of functional movements are necessary. Here we recorded EMG activity using 5 bipolar electrodes placed on the upper-arm in 8 healthy participants while they performed reaching movements in 8 different directions. A pseudo on-line system was developed to continuously predict movement intention and attempted arm direction. We evaluated two hierarchical classification approaches. Movement intention detection triggered different movement direction classifiers (4 or 8 classes) that were trained and tested over a 5-fold cross validation. We also investigated the effect of 3 different window lengths to extract EMG features on classification. We obtained classification accuracies above 70% for both hierarchical approaches. These results highlight the viability of classifying online 8 upper-arm different directions using surface EMG activity of 5 muscles and represent a first step towards an online EMG-based control for rehabilitation devices.	attempt;auditory recruitment;class;classification;cross-sectional studies;electromyography;exoskeleton device;movement;muscle;numerous;online and offline;pseudo brand of pseudoephedrine;reaching;robot;window function;electrode;on-line system	Nerea Irastorza-Landa;Andrea Sarasola-Sanz;Eduardo Lopez-Larraz;Carlos Bibian;Farid Shiman;Niels Birbaumer;Ander Ramos-Murguialday	2017	2017 International Conference on Rehabilitation Robotics (ICORR)	10.1109/ICORR.2017.8009234	computer vision;feature extraction;powered exoskeleton;motor unit recruitment;functional movement;electromyography;speech recognition;artificial intelligence;engineering	Robotics	13.779449650455103	-91.8090901483198	10475
8c6df7219cacec2327cd7957369037005d4e2b25	ordering bac clones from fingerprint data obtained through four enzymes digestion and fluorescent labeling	enzyme	We implement a solution for the automatic ordering of BAC clones from fingerprint data obtained through full digestion of the clones with four enzymes that leave different 3’ recessed ends and fluorescent labeling. The algorithm is inspired to a previous existing approach for building restriction maps.	algorithm;batman: arkham city;fingerprint;functional programming;map;signal-to-noise ratio	Giulio Marcon;Nicola Cannata;Mercè Llabrés;Marta Simeoni;Giorgio Valle	2004			enzyme;computer science;bioinformatics	Mobile	2.890374429724422	-63.62218676876151	10491
717ffde99c0d6b58675d44b4c66acedce0ca86e8	age estimation based on face images and pre-trained convolutional neural networks		Age estimation based on face images plays an important role in a wide range of scenarios, including security and defense applications, border control, human-machine interaction in ambient intelligence applications, and recognition based on soft biometric information. Recent methods based on deep learning have shown promising performance in this field. Most of these methods use deep networks specifically designed and trained to cope with this problem. There are also some studies that focus on applying deep networks pre-trained for face recognition, which perform a fine-tuning to achieve accurate results. Differently, in this paper, we propose a preliminary study on increasing the performance of pre-trained deep networks by applying postprocessing strategies. The main advantage with respect to fine-tuning strategies consists of the simplicity and low computational cost of the post-processing step. To the best of our knowledge, this paper is the first study on age estimation that proposes the use of post-processing strategies for features extracted using pre-trained deep networks. Our method exploits a set of pre-trained Convolutional Neural Networks (CNNs) to extract features from the input face image. The method then performs a feature level fusion, reduces the dimensionality of the feature space, and estimates the age of the individual by using a Feed-Forward Neural Network (FFNN). We evaluated the performance of our method on a public dataset (Adience Benchmark of Unfiltered Faces for Gender and Age Classification) and on a dataset of nonideal samples affected by controlled rotations, which we collected in our laboratory. Our age estimation method obtained better or comparable results with respect to state-of-the-art techniques and achieved satisfactory performance in non-ideal conditions. Results also showed that CNNs trained on general datasets can obtain satisfactory accuracy for different types of validation images, also without applying fine-tuning methods.		Abhinav Anand;Ruggero Donida Labati;Angelo Genovese;Enrique Muñoz Ballester;Vincenzo Piuri;Fabio Scotti	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285381	convolutional neural network;pattern recognition;artificial intelligence;computer science	Vision	27.229014458130564	-57.172725321905006	10495
97d3708dfcae89cbcbd260029601f2c1de4d7017	semantic localisation via globally unique instance segmentation		This video illustrates results of globally unique instance segmentation and localisation for CamVid-360 dataset. It uses building and tree instances and label image alignment similarity function to establish image matches. The top left image corresponds to the query image. The top right image corresponds to predicted labels. The bottom left image corresponds to the best matching database image and the bottom right image corresponds to the label image of the best matching database image.	similarity measure	Ignas Budvytis;Patrick Sauer;Roberto Cipolla	2018			computer science;computer vision;artificial intelligence;pattern recognition;segmentation	Vision	42.59600344152934	-54.97934096874418	10496
115700452eb8c4d8d6383df6a6dd07251ccd5cc4	effective connectivity anomalies in human amblyopia	amblyopia;effective connectivity;nonlinear system identification;fmri;time series;functional connectivity;effective functional connectivity;single cell;lateral geniculate nucleus;human;visual cortex	We investigate the effective connectivity in the lateral geniculate nucleus and visual cortex of humans with amblyopia. Six amblyopes participated in this study. Standard retinotopic mapping stimuli were used to define the boundaries of early visual cortical areas. We obtained fMRI time series from thalamic, striate and extrastriate cortical regions for the connectivity study. Thalamo-striate and striate-extrastriate networks were constructed based on known anatomical connections and the effective connectivities of these networks were assessed by means of a nonlinear system identification method. The effective connectivity of all networks studied was reduced when driven by the amblyopic eye, suggesting contrary to the current single-cell model of localized signal reduction, that a significant part of the amblyopic deficit is due to anomalous interactions between cells in disparate brain regions. The effective connectivity loss was unrelated to the fMRI loss but correlated with the degree of amblyopia (ipsilateral LGN to V1 connection), suggesting that it may be a more relevant measure. Feedforward and feedback connectivities were similarly affected. A hemispheric dependence was found for the thalamo-striate feedforward input that was not present for the feedback connection, suggesting that the reduced function of the LGN recently found in amblyopic humans may not be solely determined by the feedback influence from the cortex. Both ventral and dorsal connectivities were reduced.	amblyopia;area striata structure;cell nucleus;cerebral cortex;feed forward (control);feedforward neural network;geniculate body structure;interaction;lateral thinking;nonlinear system identification;sensorineural hearing loss (disorder);thalamic structure;time series;fmri;lateral geniculate complex	Xingfeng Li;Kathy T. Mullen;Benjamin Thompson;Robert F. Hess	2011	NeuroImage	10.1016/j.neuroimage.2010.07.053	psychology;computer vision;neuroscience;time series;nonlinear system identification;communication;statistics	ML	18.29320733368186	-75.21480276516114	10503
56513be4bf3b431bdc209da43e9f662298df58ca	heuristic approach to handwritten numeral recognition	image recognition;reconocimiento imagen;manuscript character;heuristic method;reconnaissance caractere;reconnaissance image;pattern recognition;numeral;methode heuristique;reconnaissance forme;chiffre;reconocimiento patron;cifra;caractere manuscrit;character recognition	Abstract   An efficient and reliable heuristic method to recognize handwritten numerals is discussed. The numerals are written in a completely random manner such as can be generated and still recognized by humans. The method is designed to read the worst written numerals, where structural differences among numerals are investigated and the extracted heuristic features are: the order of interesting points, numbers of endpoints, forkpoints, crosspoints, breakpoints and subpieces, the slope-change of each subpiece, the orientation and the length of each subpiece, the properties of start- and endpoints of each subpiece. A learning table is built to classify numerals of large variability with very high accuracy.	heuristic	Jun S. Huang;Keren Chuang	1986	Pattern Recognition	10.1016/0031-3203(86)90027-0	arithmetic;computer vision;speech recognition;computer science;artificial intelligence;pattern recognition;algorithm;numeral system	Vision	34.3064813225666	-67.3204625274701	10514
87de2ee9b4b55330042ae05cb8aeb78fd9842a9b	dynamic neural network models of the premotoneuronal circuitry controlling wrist movements in primates	model generation;motor system;input output;gradient descent;dynamic neural network;recurrent neural network;motor unit	Dynamic recurrent neural networks were derived to simulate neuronal populations generating bidirectional wrist movements in the monkey. The models incorporate anatomical connections of cortical and rubral neurons, muscle afferents, segmental interneurons and motoneurons; they also incorporate the response profiles of four populations of neurons observed in behaving monkeys. The networks were derived by gradient descent algorithms to generate the eight characteristic patterns of motor unit activations observed during alternating flexion-extension wrist movements. The resulting model generated the appropriate input-output transforms and developed connection strengths resembling those in physiological pathways. We found that this network could be further trained to simulate additional tasks, such as experimentally observed reflex responses to limb perturbations that stretched or shortened the active muscles, and scaling of response amplitudes in proportion to inputs. In the final comprehensive network, motor units are driven by the combined activity of cortical, rubral, spinal and afferent units during step tracking and perturbations. The model displayed many emergent properties corresponding to physiological characteristics. The resulting neural network provides a working model of premotoneuronal circuitry and elucidates the neural mechanisms controlling motoneuron activity. It also predicts several features to be experimentally tested, for example the consequences of eliminating inhibitory connections in cortex and red nucleus. It also reveals that co-contraction can be achieved by simultaneous activation of the flexor and extensor circuits without invoking features specific to co-contraction.	artificial neural network;biological neural networks;cell nucleus;electronic circuit;emergence;experiment;gradient descent;image scaling;interneurons;monkeys;motor neurons;motor unit;movement;muscle;neural network simulation;perturbation theory;population;recurrent neural network;red nucleus structure;reflex action;test scaling;algorithm	M. A. Maier;Larry E. Shupe;Eberhard E. Fetz	2005	Journal of Computational Neuroscience	10.1007/s10827-005-0899-5	gradient descent;input/output;neuroscience;computer science;artificial intelligence;recurrent neural network;machine learning;motor system	ML	18.357328560266275	-70.66610305995	10515
b469d5e1682de479bed1c8571f7af8c30a2a687c	development of an in silico method for the identification of subcomplexes involved in the biogenesis of multiprotein complexes in saccharomyces cerevisiae	complex assembly;graph clustering;ppi network;protein complex;protein-protein interactions;subcomplex	Large sets of protein-protein interaction data coming either from biological experiments or predictive methods are available and can be combined to construct networks from which information about various cell processes can be extracted. We have developed an in silico approach based on these information to model the biogenesis of multiprotein complexes in the yeast Saccharomyces cerevisiae. Firstly, we have built three protein interaction networks by collecting the protein-protein interactions, which involved the subunits of three complexes, from different databases. The protein-protein interactions come from different kinds of biological experiments or are predicted. We have chosen the elongator and the mediator head complexes that are soluble and exhibit an architecture with subcomplexes that could be functional modules, and the mitochondrial bc 1 complex, which is an integral membrane complex and for which a late assembly subcomplex has been described. Secondly, by applying a clustering strategy to these networks, we were able to identify subcomplexes involved in the biogenesis of the complexes as well as the proteins interacting with each subcomplex. Thirdly, in order to validate our in silico results for the cytochrome bc1 complex we have analysed the physical interactions existing between three subunits by performing immunoprecipitation experiments in several genetic context. For the two soluble complexes (the elongator and mediator head), our model shows a strong clustering of subunits that belong to a known subcomplex or module. For the membrane bc 1 complex, our approach has suggested new interactions between subunits in the early steps of the assembly pathway that were experimentally confirmed. Scripts can be downloaded from the site: http://bim.igmors.u-psud.fr/isips .	cluster analysis;database;experiment;extraction;fundamental interaction;gene regulatory network;interaction network;mediator brand of benfluorex hydrochloride;monte carlo method;multiprotein complexes;saccharomyces cerevisiae;tissue membrane;uqcrc1 gene;protein protein interaction;statistical cluster	Annie Glatigny;Philippe Gambette;Alexa Bourand-Plantefol;Geneviève Dujardin;Marie-Hélène Mucchielli-Giorgi	2017		10.1186/s12918-017-0442-0	cell biology;immunoprecipitation;protein–protein interaction;bioinformatics;in silico;systems biology;biology;coenzyme q – cytochrome c reductase;saccharomyces cerevisiae;biogenesis;mediator	Comp.	8.584636424903948	-58.32551778069097	10516
c6673c5b6382e31e545dfb04d7b4dd02750dd6e0	identification of circular codes in bacterial genomes and their use in a factorization method for retrieving the reading frames of genes	statistical approach;circular code;nucleotides;statistical method;nucleotide sequence;factorization method;frame;bacterial genome;gene selection	We developed a statistical method that allows each trinucleotide to be associated with a unique frame among the three possible ones in a (protein coding) gene. An extensive gene study in 175 complete bacterial genomes based on this statistical approach resulted in identification of 72 new circular codes. Finding a circular code enables an immediate retrieval of the reading frame locally anywhere in a gene. No knowledge of location of the start codon is required and a short window of only a few nucleotides is sufficient for automatic retrieval. We have therefore developed a factorization method (that explores previously found circular codes) for retrieving the reading frames of bacterial genes. Its principle is new and easy to understand. Neither complex treatment nor specific information on the nucleotide sequences is necessary. Moreover, the method can be used for short regions in nucleotide sequences (less than 25 nucleotides in protein coding genes). Selected additional properties of circular codes and their possible biological consequences are also discussed.	code;codon, initiator;frame (physical object);genes, bacterial;genome;genome, bacterial;nucleotides;reading frames (nucleotide sequence)	Gabriel Frey;Christian J. Michel	2006	Computational biology and chemistry	10.1016/j.compbiolchem.2005.11.001	gene-centered view of evolution;frame;biology;nucleotide;nucleic acid sequence;bioinformatics;genetics;bacterial genome size	Comp.	1.6949226288718882	-59.403575652295	10518
ec5fa557b9e8ba7390a7e8f9385021ef55b4614e	towards a first-reflection ultrasonic sensor array for compensatory movement identification in stroke sufferers	acoustics;torso;wearable sensors;contactless sensing;statistics;first reflection echolocation;sociology;sensor arrays;stroke	The use of compensatory motion strategies amongst stroke sufferers has been well documented in the literature. While these modified movement patterns allow individuals to address functional deficits, research suggests that employing such techniques may inhibit motor skill recovery. Although detection of these movements using either wearable sensors or gaming technologies within home-based rehabilitation regiments has been demonstrated, both the physical limitations and technological preferences of the target population limit the efficacy of such solutions. The objective of this extended abstract is to demonstrate progress towards employing a contactless first-reflection ultrasonic echolocation sensor to detect compensatory movements associated with excessive trunk flexion in response to reduced upper extremity functionality. Results from preliminary experiments in which compensatory motions are induced using a motion-restricting elbow brace are described herein. Preliminary results are promising, with average classification accuracy exceeding 78%.	contactless smart card;experiment;sensor;wearable computer	Henry Griffith;Rajiv Ranganathan;Subir Biswas	2016	2016 IEEE 35th International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2016.7820611	simulation;stroke;torso;statistics	Visualization	11.342248199280117	-92.63538044995846	10545
02bc2d60f85b0c5fdf6edbc83daf6cd1715b81e5	interpretation of the binding affinities of ptp1b inhibitors with the mm-gb/sa method and the x-score scoring function	score function;binding affinity	We have studied the binding affinities of a set of 45 small-molecule inhibitors to protein tyrosine phosphatase 1B (PTP1B) through computational approaches. All of these compounds share a common oxalylamino benzoic acid (OBA) moiety. The complex structure of each compound was modeled by using the GOLD program plus the ASP scoring function. Each complex structure was then subjected to a molecular dynamics (MD) simulation of 2 ns long by using the AMBER program. Based on the configurational ensembles retrieved from MD trajectories, both MM-GB/SA and MM-PB/SA were employed to compute the binding free energies of all 45 PTP1B inhibitors. The correlation coefficient between the MM-GB/SA results and experimental binding data was 0.87 and the standard deviation was 0.60 kcal/mol. The performance of MM-PB/SA was slightly inferior to that of MM-GB/SA. Several aspects of the MM-GB(PB)/SA method were explored in our study to obtain optimized results. The X-Score scoring function was found to produce equally good results as MM-GB/SA on both the complex structures prepared by molecular docking and the configurational ensembles obtained through lengthy MD simulations. The structure-activity relationship of this set of compounds is also discussed based on the computed results. The computational approaches validated in our study are hopefully applicable to the study of other classes of PTP1B inhibitors.		Xing-long Zhang;Xun Li;Renxiao Wang	2009	Journal of chemical information and modeling	10.1021/ci8004429	chemistry;bioinformatics;artificial intelligence;computational chemistry;ligand;score;statistics	Comp.	11.54396782572414	-59.41819276804068	10546
804e1dcda8995ab64251b88f51379c6addc80603	triplex-inspector: an analysis tool for triplex-mediated targeting of genomic loci	dna;software;genomics;gene targeting;peptide nucleic acids;genetic loci;humans;helices	SUMMARY At the heart of many modern biotechnological and therapeutic applications lies the need to target specific genomic loci with pinpoint accuracy. Although landmark experiments demonstrate technological maturity in manufacturing and delivering genetic material, the genomic sequence analysis to find suitable targets lags behind. We provide a computational aid for the sophisticated design of sequence-specific ligands and selection of appropriate targets, taking gene location and genomic architecture into account.   AVAILABILITY Source code and binaries are downloadable from www.bioinformatics.org.au/triplexator/inspector.   CONTACT t.bailey@uq.edu.au   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioinformatics;capability maturity model;experiment;ligands;sequence analysis;source code;cellular targeting	Fabian A. Buske;Denis C. Bauer;John S. Mattick;Timothy L. Bailey	2013	Bioinformatics	10.1093/bioinformatics/btt315	biology;genomics;biotechnology;bioinformatics;gene targeting;helix;locus;genetics;dna	Comp.	0.981439890198457	-59.304994527118524	10550
29b02a4cd4ef855b35bc64b93bdf1b306650f3e1	unsupervised and adaptive category classification for a vision-based mobile robot	vision system;unsupervised learning;image recognition;self organizing maps;mobile robot;image recognition robots;mobile robots;time series;unsupervised category classification;adaptive category classification;art 2;robot vision;incremental learning;spatial relation;sift;self organising feature maps;scale invariant feature transform;robots;transforms;pattern classification;self organized map;som;adaptive resonance theory 2;art neural nets;unsupervised learning unsupervised category classification adaptive category classification vision based mobile robot time series image incremental learning adaptive resonance theory 2 art 2 counter propagation network self organizing maps som scale invariant feature transform sift;counter propagation network;vision based mobile robot;unsupervised learning art neural nets mobile robots pattern classification robot vision self organising feature maps transforms;time series image	This paper presents an unsupervised category classification method for time-series images that combines incremental learning of Adaptive Resonance Theory-2 (ART-2) and self-mapping characteristic of Counter Propagation Networks (CPNs). Our method comprises the following procedures: 1) generating visual words using Self-Organizing Maps (SOM) from 128-dimensional descriptors in each feature point of a Scale-Invariant Feature Transform (SIFT), 2) forming labels using unsupervised learning of ART-2, and 3) creating and classifying categories on a category map of CPNs for visualizing spatial relations between categories. We use a vision system on a mobile robot for taking time-series images. Experimental results show that our method can classify objects into categories according to their change of appearance during the movement of a robot.	mobile robot;resonance;scale-invariant feature transform;software propagation;time series;unsupervised learning	Masahiro Tsukada;Hirokazu Madokoro;Kazuhito Sato	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596323	unsupervised learning;mobile robot;computer vision;computer science;machine learning;pattern recognition;scale-invariant feature transform	Robotics	24.451517654002803	-63.40386078595734	10560
2d54c1a345e6fb365854dc3f2e5d15f9856369a4	spatial co-occurrence of local intensity order for face recognition	local intensity order face recognition spatial co occurrence;illumination variance spatial cooccurrence of local intensity order feature face recognition colio face databases ar lfw fast extraction speed;face recognition;face databases training face recognition lighting feature extraction correlation;feature extraction;feature extraction face recognition	In this paper, we propose a simple but effective spatial co-occurrence of local intensity order (CoLIO) feature for face recognition. Local intensity order (LIO) is robust to illumination variance. Spatial co-occurrence of LIO not only preserves great invariance to illumination, but also greatly enhances the discriminative power of the descriptor as Co- LIO well captures the correlation between locally adjacent regions. The proposed feature has been successfully applied to two widely used face databases including AR [1] and LFW [2]. Superior performance on these two databases fully demonstrates the effectiveness of the proposed feature. Meanwhile, the extremely fast extraction speed makes the proposed feature practically useful.	ar (unix);database;facial recognition system	Xianbiao Qi;Yi Lu;Shifeng Chen;Chun-Guang Li;Jun Guo	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618447	facial recognition system;computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	36.56685733332979	-57.46727964350066	10562
ca9c040b622779097a45ddcaa6d01498082bf1b4	a new prediction protein structure method based on genetic algorithm and coarse-grained protein model	carbon;conformational change;conformational change protein structure prediction method genetic algorithm coarse grained protein model de novo prediction amino acid sequences bioinformatics langevin molecular dynamics;amino acid sequences;molecular configurations;optimal method;amino acid sequence;prediction algorithms;proteins bioinformatics genetic algorithms molecular biophysics molecular configurations molecular dynamics method;data mining;molecular dynamics method;protein structure;coarse grained protein model;proteins;protein structure prediction;structure prediction;molecular biophysics;langevin molecular dynamics;molecular dynamic;genetic algorithm;predictive models;genetic algorithms;genetic algorithms predictive models assembly protein engineering bioinformatics genetic mutations computer aided instruction optimization evolution biology educational technology;coarse grained;high performance;de novo prediction;gallium;protein structure prediction method;bioinformatics	De novo prediction of protein structures, structures prediction from amino acid sequences which are not similar to those of hitherto resolved structures, has been one of the major challenges in bioinformatics. A new prediction protein structure method based on Genetic Algorithm and Coarse-grained protein model is presented. Genetic Algorithm is a high performance optimization method and can effectively assemble a separate sequence, and Coarse-grained protein model have consistently assembled fragments are dynamically searched by Langevin molecular dynamics of conformational change. This new method combines the advantages of Genetic Algorithm and Coarse-grained protein model, which will improve the accuracy of protein structure prediction.	bioinformatics;de novo protein structure prediction;genetic algorithm;mathematical optimization;molecular dynamics	Cai-Yun Wang;Hao-Dong Zhu;Le-Cai Cai	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5305230	biology;genetic algorithm;computer science;bioinformatics;machine learning;protein structure prediction;genetics;protein design;molecular biophysics	Comp.	12.075246992489527	-54.492131557480725	10571
03e9ed078bdc9e29eba318421e14839fa1b6d69f	deep learning-based enhanced presentation attack detection for iris recognition by combining features from local and global regions based on nir camera sensor	nir camera sensor;deep learning;iris recognition;presentation attack detection;support vector machines	Iris recognition systems have been used in high-security-level applications because of their high recognition rate and the distinctiveness of iris patterns. However, as reported by recent studies, an iris recognition system can be fooled by the use of artificial iris patterns and lead to a reduction in its security level. The accuracies of previous presentation attack detection research are limited because they used only features extracted from global iris region image. To overcome this problem, we propose a new presentation attack detection method for iris recognition by combining features extracted from both local and global iris regions, using convolutional neural networks and support vector machines based on a near-infrared (NIR) light camera sensor. The detection results using each kind of image features are fused, based on two fusion methods of feature level and score level to enhance the detection ability of each kind of image features. Through extensive experiments using two popular public datasets (LivDet-Iris-2017 Warsaw and Notre Dame Contact Lens Detection 2015) and their fusion, we validate the efficiency of our proposed method by providing smaller detection errors than those produced by previous studies.	artificial neural network;convolutional neural network;deep learning;experiment;extraction;image sensor;iris (eye);iris recognition;neural network simulation;small;spectroscopy, near-infrared;support vector machine;weight;anatomical layer;ipad	Tien Dat Nguyen;Tuyen Danh Pham;Young-Woo Lee;Kang Ryoung Park	2018		10.3390/s18082601	electronic engineering;engineering;computer vision;iris recognition;deep learning;image sensor;artificial intelligence	AI	27.016089324338257	-56.62850301101968	10580
55bbb3bf9878fafbd754325fca5a7d5bd8f3df0e	sequence-based gaussian network model for protein dynamics	animals;quorum sensing;rhipicephalus;models molecular;proteins;protein conformation;cattle;lactoglobulins;arthropod proteins;algorithms;linear models;computer simulation;bacterial proteins;sequence analysis protein	MOTIVATION Gaussian network model (GNM) is widely adopted to analyze and understand protein dynamics, function and conformational changes. The existing GNM-based approaches require atomic coordinates of the corresponding protein and cannot be used when only the sequence is known.   RESULTS We report, first of its kind, GNM model that allows modeling using the sequence. Our linear regression-based, parameter-free, sequence-derived GNM (L-pfSeqGNM) uses contact maps predicted from the sequence and models local, in the sequence, contact neighborhoods with the linear regression. Empirical benchmarking shows relatively high correlations between the native and the predicted with L-pfSeqGNM B-factors and between the cross-correlations of residue fluctuations derived from the structure- and the sequence-based GNM models. Our results demonstrate that L-pfSeqGNM is an attractive platform to explore protein dynamics. In contrast to the highly used GNMs that require protein structures that number in thousands, our model can be used to study motions for the millions of the readily available sequences, which finds applications in modeling conformational changes, protein-protein interactions and protein functions.	cross-correlation;entity name part qualifier - adopted;linear iga bullous dermatosis;map;motion;network model;normal statistical distribution;population parameter;protein dynamics;protein protein interaction	Hua Zhang;Lukasz A. Kurgan	2014		10.1093/bioinformatics/btt716	computer simulation;biology;protein structure;quorum sensing;bioinformatics;linear model;genetics	Comp.	7.784175354994429	-58.62270329711536	10586
b5a2b1ffd5279ebe0f46ccf9e736c9f77328c7fb	a probabilistic method for identifying rare variants underlying complex traits	software;genotype;animal genetics and genomics;internet;gene frequency;genetic predisposition to disease;genome human;life sciences general;humans;user computer interface;microbial genetics and genomics;proteomics;markov chains;microarrays;plant genetics genomics	Identifying the genetic variants that contribute to disease susceptibilities is important both for developing methodologies and for studying complex diseases in molecular biology. It has been demonstrated that the spectrum of minor allelic frequencies (MAFs) of risk genetic variants ranges from common to rare. Although association studies are shifting to incorporate rare variants (RVs) affecting complex traits, existing approaches do not show a high degree of success, and more efforts should be considered. In this article, we focus on detecting associations between multiple rare variants and traits. Similar to RareCover, a widely used approach, we assume that variants located close to each other tend to have similar impacts on traits. Therefore, we introduce elevated regions and background regions, where the elevated regions are considered to have a higher chance of harboring causal variants. We propose a hidden Markov random field (HMRF) model to select a set of rare variants that potentially underlie the phenotype, and then, a statistical test is applied. Thus, the association analysis can be achieved without pre-selection by experts. In our model, each variant has two hidden states that represent the causal/non-causal status and the region status. In addition, two Bayesian processes are used to compare and estimate the genotype, phenotype and model parameters. We compare our approach to the three current methods using different types of datasets, and though these are simulation experiments, our approach has higher statistical power than the other methods. The software package, RareProb and the simulation datasets are available at: http://www.engr.uconn.edu/~jiw09003 .	causal filter;disease susceptibility;eaf2 gene;experiment;hidden markov random field;markov chain;mental association;sensor;simulation;trait	Jiayin Wang;Zhongmeng Zhao;Zhi Cao;Aiyuan Yang;Jin Zhang	2013		10.1186/1471-2164-14-S1-S11	biology;markov chain;the internet;dna microarray;bioinformatics;allele frequency;genotype;proteomics;genetics	Comp.	3.8467138413303905	-53.652970913009824	10593
876975128c5f5f77fe4fb184c027edd49bc7b8c9	fetal ecg extraction using wavelet transform	low pass filter;wavelet transforms electrocardiography feature extraction filtering theory medical signal processing obstetrics smoothing methods;wavelet transforms;electrocardiography;smoothing methods;wavelet transform;feature extraction;savitzky golay smoothing filter fetal ecg extraction wavelet transform daubechie wavelet transform two level wavelet transform heart beat waves;medical signal processing;filtering theory;obstetrics;electrocardiography wavelet transforms discrete wavelet transforms low pass filters abdomen fetus smoothing methods frequency signal processing shape	This paper presents a new algorithm to extract the fetal ECG from an ECG recorded on the mother's abdomen. The algorithm consists of two steps: first, ECG of fetus is extracted from the original signal using a two-level wavelet transform. We use Daubechie wavelet transform, which is similar in shape to heart beat wave. The resulted signal is then low-pass filtered using the Savitzky-Golay smoothing filter to attenuate the effect of noise. The filtered signal is considered as the fetal ECG. The algorithm was applied on both synthetic and real ECG signals. The results in this paper show that the proposed algorithm has promising performance.	algorithm;low-pass filter;smoothing;synthetic intelligence;wavelet transform	Hamid Hassanpour;Amin Parsaei	2006	2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06)	10.1109/CIMCA.2006.98	wavelet;computer vision;constant q transform;speech recognition;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Robotics	51.82671837413779	-66.7384866806882	10603
2c394fc4d31e1b92e0415aa5759a63ea2b93b136	motility enhancement through surface modification is sufficient for cyanobacterial community organization during phototaxis	biophysics;synechocystis;surface properties	"""The emergent behaviors of communities of genotypically identical cells cannot be easily predicted from the behaviors of individual cells. In many cases, it is thought that direct cell-cell communication plays a critical role in the transition from individual to community behaviors. In the unicellular photosynthetic cyanobacterium Synechocystis sp. PCC 6803, individual cells exhibit light-directed motility (""""phototaxis"""") over surfaces, resulting in the emergence of dynamic spatial organization of multicellular communities. To probe this striking community behavior, we carried out time-lapse video microscopy coupled with quantitative analysis of single-cell dynamics under varying light conditions. These analyses suggest that cells secrete an extracellular substance that modifies the physical properties of the substrate, leading to enhanced motility and the ability for groups of cells to passively guide one another. We developed a biophysical model that demonstrates that this form of indirect, surface-based communication is sufficient to create distinct motile groups whose shape, velocity, and dynamics qualitatively match our experimental observations, even in the absence of direct cellular interactions or changes in single-cell behavior. Our computational analysis of the predicted community behavior, across a matrix of cellular concentrations and light biases, demonstrates that spatial patterning follows robust scaling laws and provides a useful resource for the generation of testable hypotheses regarding phototactic behavior. In addition, we predict that degradation of the surface modification may account for the secondary patterns occasionally observed after the initial formation of a community structure. Taken together, our modeling and experiments provide a framework to show that the emergent spatial organization of phototactic communities requires modification of the substrate, and this form of surface-based communication could provide insight into the behavior of a wide array of biological communities."""	cell communication;cell secretion;community;elegant degradation;emergence;experiment;image scaling;interaction;microscopy, video;phototaxis;physical phenomenon or property;portable c compiler;rem sleep behavior disorder;spatial organization;synechocystis sp. pcc 6714;velocity (software development)	Tristan Ursell;Rosanna Man Wah Chau;Susanne Wisen;Devaki Bhaya;Kerwyn Casey Huang	2013		10.1371/journal.pcbi.1003205	biology;ecology	HCI	8.292324484969441	-65.2097455642946	10645
49f494d423319072d7f90b2dca14b5326590b979	skeleton model for the neurodynamics of visual action representations		The visual recognition of body motion in the primate brain requires the temporal integration of information over complex patterns, potentially exploiting recurrent neural networks consisting of shapeand optic-flow-selective neurons. The paper presents a mathematically simple neurodynamical model that approximates the mean-field dynamics of such networks. It is based on a two-dimensional neural field with appropriate lateral interaction kernel and an adaptation process for the individual neurons. The model accounts for a number of, so far not modeled, observations in the recognition of body motion, including perceptual multi-stability and the weakness of repetition suppression, as observed in single-cell recordings for the repeated presentation of action stimuli. In addition, the model predicts novel effects in the perceptual organization of action stimuli.	artificial neural network;lateral thinking;neural oscillation;optical flow;recurrent neural network;zero suppression	Martin A. Giese	2014		10.1007/978-3-319-11179-7_89	computer vision;artificial intelligence;machine learning	ML	20.32440578367826	-68.74477618671386	10698
c8e60e5f4af71dd6121d372dc2fa22dde82e5417	insights into the molecular mechanisms of protein-ligand interactions by molecular docking and molecular dynamics simulation: a case of oligopeptide binding protein		Protein-ligand interactions are a necessary prerequisite for signal transduction, immunoreaction, and gene regulation. Protein-ligand interaction studies are important for understanding the mechanisms of biological regulation, and they provide a theoretical basis for the design and discovery of new drug targets. In this study, we analyzed the molecular interactions of protein-ligand which was docked by AutoDock 4.2 software. In AutoDock 4.2 software, we used a new search algorithm, hybrid algorithm of random drift particle swarm optimization and local search (LRDPSO), and the classical Lamarckian genetic algorithm (LGA) as energy optimization algorithms. The best conformations of each docking algorithm were subjected to molecular dynamic (MD) simulations to further analyze the molecular mechanisms of protein-ligand interactions. Here, we analyze the binding energy between protein receptors and ligands, the interactions of salt bridges and hydrogen bonds in the docking region, and the structural changes during complex unfolding. Our comparison of these complexes highlights differences in the protein-ligand interactions between the two docking methods. It also shows that salt bridge and hydrogen bond interactions play a crucial role in protein-ligand stability. The present work focuses on extracting the deterministic characteristics of docking interactions from their dynamic properties, which is important for understanding biological functions and determining which amino acid residues are crucial to docking interactions.		Yi Fu;Ji Zhao;Zhiguo Chen	2018		10.1155/2018/3502514	protein ligand;molecular dynamics;salt bridge;docking (molecular);computational biology;docking (dog);machine learning;artificial intelligence;oligopeptide binding;biological regulation;autodock;computer science	Comp.	8.532373771730809	-62.59562486169294	10707
e7f30c828441b571018eb86ceaab22b5b221978c	automatic quantification of mammary glands on non-contrast x-ray ct by using a novel segmentation approach	image segmentation;computed tomography;tissues;mammary gland;breast;machine learning;breast cancer	This paper describes a brand new automatic segmentation method for quantifying volume and density of mammary gland regions on non-contrast CT images. The proposed method uses two processing steps: (1) breast region localization, and (2) breast region decomposition to accomplish a robust mammary gland segmentation task on CT images. The first step detects two minimum bounding boxes of left and right breast regions, respectively, based on a machine-learning approach that adapts to a large variance of the breast appearances on different age levels. The second step divides the whole breast region in each side into mammary gland, fat tissue, and other regions by using spectral clustering technique that focuses on intra-region similarities of each patient and aims to overcome the image variance caused by different scan-parameters. The whole approach is designed as a simple structure with very minimum number of parameters to gain a superior robustness and computational efficiency for real clinical setting. We applied this approach to a dataset of 300 CT scans, which are sampled with the equal number from 30 to 50 years-old-women. Comparing to human annotations, the proposed approach can measure volume and quantify distributions of the CT numbers of mammary gland regions successfully. The experimental results demonstrated that the proposed approach achieves results consistent with manual annotations. Through our proposed framework, an efficient and effective low cost clinical screening scheme may be easily implemented to predict breast cancer risk, especially on those already acquired scans.	ct scan;cluster analysis;computation;machine learning;spectral clustering	Xiangrong Zhou;Takuya Kano;Yunliang Cai;Shuo Li;Xinxin Zhou;Takeshi Hara;Ryujiro Yokoyama;Hiroshi Fujita	2016		10.1117/12.2217256	breast cancer;image segmentation;computed tomography	Vision	39.53718501796343	-78.34951960260126	10709
10f0eee433f7e3e91341335e0010e151b8d95259	fundamental principles of cortical computation: unsupervised learning with prediction, compression and feedback		There has been great progress in understanding of anatomical and functional microcircuitry of the primate cortex. However, the fundamental principles of cortical computation the principles that allow the visual cortex to bind retinal spikes into representations of objects, scenes and scenarios have so far remained elusive. In an attempt to come closer to understanding the fundamental principles of cortical computation, here we present a functional, phenomenological model of the primate visual cortex. The core part of the model describes four hierarchical cortical areas with feedforward, lateral, and recurrent connections. The three main principles implemented in the model are information compression, unsupervised learning by prediction, and use of lateral and top-down context. We show that the model reproduces key aspects of the primate ventral stream of visual processing including Simple and Complex cells in V1, increasingly complicated feature encoding, and increased separability of object representations in higher cortical areas. The model learns representations of the visual environment that allow for accurate classification and state-of-the-art visual tracking performance on novel objects.	computation;feedforward neural network;lateral thinking;linear separability;phenomenological model;top-down and bottom-up design;unsupervised learning;video tracking	Micah Richert;Dimitry Fisher;Filip Piekniewski;Eugene M. Izhikevich;Todd Hylton	2016	CoRR		computer vision;computer science;artificial intelligence;machine learning	ML	21.074591078510185	-67.08710683297281	10715
531c5565a485371d8f299dd08d77de0872c36a0f	robust background subtraction method based on 3d model projections with likelihood	free viewpoint video;video signal processing;video signal processing image reconstruction solid modelling;energy function;3d model projections;visualization;multi view images;3d model;voxel likelihood;three dimensional displays;image reconstruction;pixel;background subtraction;mathematical model;visualization pixel three dimensional displays mathematical model equations image reconstruction cameras;foreground silhouettes;robust refining process;foreground silhouettes robust background subtraction method 3d model projections multi view images cameras voxel likelihood robust refining process image reconstruction;robust background subtraction method;cameras;solid modelling	We propose a robust background subtraction method for multi-view images, which is essential for realizing free viewpoint video where an accurate 3D model is required. Most of the conventional methods determine background using only visual information from a single camera image, and the precise silhouette cannot be obtained. Our method employs an approach of integrating multi-view images taken by multiple cameras, in which the background region is determined using a 3D model generated by multi-view images. We apply the likelihood of background to each pixel of camera images, and derive an integrated likelihood for each voxel in a 3D model. Then, the background region is determined based on the minimization of energy functions of the voxel likelihood. Furthermore, the proposed method also applies a robust refining process, where a foreground region obtained by a projection of a 3D model is improved according to geometric information as well as visual information. A 3D model is finally reconstructed using the improved foreground silhouettes. Experimental results show the effectiveness of the proposed method compared with conventional works.	3d modeling;background subtraction;pixel;polygonal modeling;refinement (computing);visual hull;voxel;voxel space	Hiroshi Sankoh;Akio Ishikawa;Sei Naito;Shigeyuki Sakazawa	2010	2010 IEEE International Workshop on Multimedia Signal Processing	10.1109/MMSP.2010.5662014	iterative reconstruction;computer vision;visualization;background subtraction;computer science;pattern recognition;mathematical model;mathematics;pixel;statistics;computer graphics (images)	Vision	53.60672985482881	-53.58843875584484	10734
d552a1b3a59da68fb39e8a82c2114bee344a8c97	a morphology-based spatial consistency algorithm to improve egm delineation in ventricular electroanatomical mapping	silicon;manuals;heart;cardiology;nickel;catheters;clustering algorithms	Activation mapping using electroanatomical mapping (EAM) systems helps to guide catheter ablation treatment of common arrhythmias. In focal tachycardias, the earliest activation area becomes the ablation target. Recently, we proposed a single-point wavelet-based algorithm to automatically identify electrogram (EGM) activation onsets for activation mapping. In this work, we propose an EGM morphology-based spatially-consistent algorithm for improving activation mapping in areas with a high-density of mapping points. The algorithm aligns those EGMs spatially close and morphologically similar and checks if the detected bipolar EGM activation onset is determined within a tolerance of ± 5 ms. If not, a weighted average bipolar EGM activation signal is computed and delineated. Then, the new activation onset is used to compute the local activation time (LAT). Automatically detected onsets are compared with manual annotations obtained during ablation procedure by an expert technician in a total of 15 electroanatomical maps (1763 mapping points). The presented algorithm modifies 31% of the studied mapping points and in those cases reduces the difference with manual annotations from 5.1 ± 13 ms to 4.3 ± 11.6 ms.	activation function;algorithm;embedded atom model;focal (programming language);map;mathematical morphology;onset (audio);wavelet	Alejandro Alcaine;David Soto-Iglesias;David Andreu;Juan Acosta;Antonio Berruezo;Pablo Laguna;Oscar Camara;Juan Pablo Martínez	2014	Computing in Cardiology 2014		geography;biological engineering;nuclear medicine;cartography	Visualization	38.70134281549096	-79.19021956328996	10745
995fb1d65260db75a8b10fc6c6e33e4994552299	complex background leaf-based plant identification method based on interactive segmentation and kernel descriptor	kernel descriptor;plant identification;complex background	This paper presents a plant identification method from the images of the simple leaf with complex background. In order to extract leaf from the image, we firstly develop an interactive image segmentation for mobile device with tactile screen. This allows to separate the leaf region from the complex background image in few manipulations. Then, we extract the kernel descriptor from the leaf region to build leaf representation. Since the leaf images may be taken at different scale and rotation levels, we propose two improvements in kernel descriptor extraction that makes the kernel descriptor to be robust to scale and rotation. Experiments carried out on a subset of ImageClef 2013 show an important increase in performance compared to the original kernel descriptor and automatic image segmentation.	computation;computer vision;experiment;image segmentation;kernel (operating system);laurent polynomial;map;mobile device;pattern recognition;switzerland;utility functions on indivisible goods	Thi-Lan Le;Nam-Duong Duong;Van-Toi Nguyen;Hai Vu;Van-Nam Hoang;Thi Thanh-Nhan Nguyen	2015		10.1145/2764873.2764877	computer vision;machine learning;pattern recognition;mathematics	Vision	34.26777430993493	-61.66183225452705	10761
225a05ee2b18a2ff34522ea470d4b65b2f7f5a62	entropy and complexity analysis of intracranially recorded eeg	shannon entropy;complexity analysis;eeg;symbolic dynamics;epilepsy	We present an entropy and complexity analysis of intracranially recorded EEG from patients suffering from a left frontal lobe epilepsy. Our approach is based on symbolic dynamics and Shannon entropy. In particular, we will discuss the possibility to monitor long-term dynamical changes in brain electrical activity. This might offer an alternative approach for the analysis and more fundamental understanding of human epilepsies.	acoustic lobing;analysis of algorithms;electroencephalography;entropy (information theory);shannon (unit)	Ralf Steuer;Werner Ebeling;T. Bengner;C. Dehnicke;H. Hättig;H.-J. Meencke	2004	I. J. Bifurcation and Chaos	10.1142/S021812740400948X	symbolic dynamics;artificial intelligence;theoretical computer science;machine learning;mathematics;statistics;entropy	ML	19.68865936449442	-84.84489194428292	10768
bc026dfeb34159b0cd74e23f54144e315b8a2e29	healthsense: classification of health-related sensor data through user-assisted machine learning	healthcare;sensor data analysis;indexing	Remote patient monitoring generates much more data than healthcare professionals are able to manually interpret. Automated detection of events of interest is therefore critical so that these points in the data can be marked for later review. However, for some important chronic health conditions, such as pain and depression, automated detection is only partially achievable. To assist with this problem we developed HealthSense, a framework for real-time tagging of health-related sensor data. HealthSense transmits sensor data from the patient to a server for analysis via machine learning techniques. The system uses patient input to assist with classification of interesting events (e.g., pain or itching). Due to variations between patients, sensors, and condition types, we presume that our initial classification is imperfect and accommodate this by incorporating user feedback into the machine learning process. This is done by occasionally asking the patient whether they are experiencing the condition being monitored. Their response is used to confirm or reject the classification made by the server and continually improve the accuracy of the classifier's decisions on what data is of interest to the health-care provider.	machine learning;race condition;real-time transcription;sensor;server (computing)	Erich P. Stuntebeck;John S. Davis;Gregory D. Abowd;Marion Blount	2008		10.1145/1411759.1411761	computer science;data science;machine learning;data mining	ML	4.669050555085852	-80.02090542075423	10769
c91ba51068c6218c567edb45a7ed4c0dbdd0e0d6	a new method for character segmentation from multi-oriented video words	image segmentation;video signal processing;video document analysis;institute for integrated and intelligent systems;optical character recognition;faculty of science environment engineering and technology;piece wise linear segmentation line plsl video document analysis video character segmentation multi oriented document processing video character recognition;piece wise linear segmentation line plsl;video signal processing image segmentation optical character recognition text detection;multioriented video character segmentation bottom distance profiles average stroke width information background cluster top distance profile candidate segmentation points text cluster identification touching character segmentation nontouching characters isolated character segmentation video text lines two stage method multioriented video words;image segmentation noise measurement optical character recognition software character recognition educational institutions electronic mail image resolution;080109;pattern recognition and data mining;video character recognition;multi oriented document processing;text detection;video character segmentation	This paper presents a two-stage method for multi-oriented video character segmentation. Words segmented from video text lines are considered for character segmentation in the present work. Words can contain isolated or non-touching characters, as well as touching characters. Therefore, the character segmentation problem can be viewed as a two stage problem. In the first stage, text cluster is identified and isolated (non-touching) characters are segmented. The orientation of each word is computed and the segmentation paths are found in the direction perpendicular to the orientation. Candidate segmentation points computed using the top distance profile are used to find the segmentation path between the characters considering the background cluster. In the second stage, the segmentation results are verified and a check is performed to ascertain whether the word component contains touching characters or not. The average width of the components is used to find the touching character components. For segmentation of the touching characters, segmentation points are then found using average stroke width information, along with the top and bottom distance profiles. The proposed method was tested on a large dataset and was evaluated in terms of precision, recall and f-measure. A comparative study with existing methods reveals the superiority of the proposed method.	cluster analysis;f1 score;piecewise linear continuation;precision and recall;text segmentation	Nabin Sharma;Palaiahnakote Shivakumara;Umapada Pal;Michael Blumenstein;Chew Lim Tan	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.90	computer vision;speech recognition;computer science;segmentation-based object categorization;pattern recognition;image segmentation;optical character recognition;scale-space segmentation	Vision	36.29128520755261	-65.98484162506445	10776
6c6c0e782e046797e38113a798ecd02a9bb459b7	microneedle-based high-density surface emg interface with high selectivity for finger movement recognition	medical image processing electromyography human computer interaction image motion analysis;electrodes needles feature extraction skin metals muscles indexes;fingertip force estimation high density surface emg interface finger movement recognition surface electromyography finger motion recognition metal microneedle mnhd semg interface semg signal extraction static finger flexion test	The human hand shows complex motor skills and is widely used in activities in daily living. Thus, finger movement recognition has many potential applications in rehabilitation, tele-operation, and prosthetic hands. Several surface electromyography (sEMG) interfaces have been developed to recognize finger motion without impeding finger movement. However, conventional interfaces are impractical because they require a large skin contact area, which inconveniences users and restricts their clothing. In this paper, we propose a metal microneedle-based high-density (MNHD) sEMG interface for finger movement recognition, which is smaller than a traditional wet electrode. The highly selective extraction of the sEMG signal was demonstrated in static finger flexion tests and fingertip force estimation.	electromyography;korea internet & security agency;selectivity (electronic);television	Minjae Kim;Dong Sung Kim;Wan Kyun Chung	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487229	computer vision;speech recognition;engineering	Robotics	26.559023839808482	-70.49076392780712	10803
1acfe213bde3f69a8eff515d92ae88220cec4bad	head-shoulder human contour estimation in still images	pose estimation estimation theory face recognition;face shape image edge detection image segmentation histograms estimation computational modeling;human head shoulder estimation;human segmentation human head shoulder estimation omega shaped region;human segmentation;omega shaped region;partial occlusion head shoulder human contour estimation model still image estimation graph generation omega like shape head shoulder shape model face size detection scale invariant	In this paper we propose a head-shoulder contour estimation model for human figures in still images, captured in a frontal pose. The contour estimation is guided by a learned head-shoulder shape model, initialized automatically by a face detector. A graph is generated around the detected face with an omega-like shape, and the estimated head-shoulder contour is a path in the graph with maximal cost. A dataset with labeled data is used to create the head-shoulder shape model and to quantitatively analyze the results. The proposed model is scaled according to the detected face size to be scale invariant. Experimental results indicate that the proposed technique works well in non trivial images, effectively estimating the contour of the head-shoulder even under partial occlusions.	chaitin's constant;maximal set	Julio Cezar Silveira Jacques;Cláudio Rosito Jung;Soraia Raupp Musse	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025055	computer vision;pattern recognition	Vision	44.80353097903293	-53.25036310672831	10810
a460633f20d8d0b3a0418ff58f530d5f6e1ec3bb	current source density analysis as a tool to constrain the parameter space in hippocampal cai neuron models	current source density;compartmental model;parameter space;computer simulation	We propose the use of Current Source Density (CSD) computer simulations as a useful technique to constrain the parameter space in compartmental models of hippocampal CA1 neurons. These simulations allow a direct comparison with physiological data from current source density analysis and straightforward testing of hypothesis.		Pablo Varona;José Manuel Ibarz;Juan Alberto Sirüenza;Oscar Herreras	1997		10.1007/BFb0032466	computer simulation;simulation;computer science;artificial intelligence;machine learning;parameter space	SE	20.64105929946193	-73.33257709314964	10814
2374c9bd8d520d065af09de766ccd0ff8302542c	enzyminer: automatic identification of protein level mutations and their impact on target enzymes from pubmed abstracts	genomics;t technology general;information extraction;amino acid;amino acid sequence;amylases;enzyme;q science general;periodicals as topic;lipase;computational biology bioinformatics;impact analysis;enzymes;proteins;machine learning;reproducibility of results;algorithms;pattern recognition automated;document classification;combinatorial libraries;computational biology;computer appl in life sciences;information storage and retrieval;pubmed;mutation;microarrays;bioinformatics;protein level	A better understanding of the mechanisms of an enzyme's functionality and stability, as well as knowledge and impact of mutations is crucial for researchers working with enzymes. Though, several of the enzymes' databases are currently available, scientific literature still remains at large for up-to-date source of learning the effects of a mutation on an enzyme. However, going through vast amounts of scientific documents to extract the information on desired mutation has always been a time consuming process. In this paper, therefore, we describe an unique method, termed as EnzyMiner, which automatically identifies the PubMed abstracts that contain information on the impact of a protein level mutation on the stability and/or the activity of a given enzyme. We present an automated system which identifies the abstracts that contain an amino-acid-level mutation and then classifies them according to the mutation's effect on the enzyme. In the case of mutation identification, MuGeX, an automated mutation-gene extraction system has an accuracy of 93.1% with a 91.5 F-measure. For impact analysis, document classification is performed to identify the abstracts that contain a change in enzyme's stability or activity resulting from the mutation. The system was trained on lipases and tested on amylases with an accuracy of 85%. EnzyMiner identifies the abstracts that contain a protein mutation for a given enzyme and checks whether the abstract is related to a disease with the help of information extraction and machine learning techniques. For disease related abstracts, the mutation list and direct links to the abstracts are retrieved from the system and displayed on the Web. For those abstracts that are related to non-diseases, in addition to having the mutation list, the abstracts are also categorized into two groups. These two groups determine whether the mutation has an effect on the enzyme's stability or functionality followed by displaying these on the web.	abstract summary;amino acid metabolism, inborn errors;automatic identification and data capture;categorization;database;document classification;f1 score;information extraction;lipase;machine learning;mutation testing;pubmed;scientific literature;staphylococcal protein a;world wide web	Süveyda Yeniterzi;Osman Ugur Sezerman	2009	BMC Bioinformatics	10.1186/1471-2105-10-S8-S2	biology;enzyme;genomics;computer science;bioinformatics;data science;data mining;genetics;information extraction	Comp.	-1.9969810786345066	-63.60890839148293	10815
e1ae6fa196836af75d9bcce57be10f48d446f5ca	extracting fuzzy rules for detecting ventricular arrhythmias based on newfm	ventricular arrhythmia;normal sinus rhythm;heart disease;fuzzy neural network;fuzzy membership function;haar wavelet;vt;fuzzy rules;ventricular tachyarrhythmias;nsr;phase space reconstruction;ventricular tachycardia;ventricular fibrillation;wavelet transform;vf;fuzzy neural networks;arrhythmia;neural network	In the heart disease, the important problem of ECG arrhythmia is to discriminate ventricular arrhythmias from normal cardiac rhythm. This paper presents novel method based on the neural network with weighted fuzzy membership functions (NEWFM) for the discrimination of ventricular tachycardia (VT) and ventricular fibrillation (VF) from normal sinus rhythm (NSR). This paper uses two pre-processes, the Haar wavelet function and extraction feature method are carried out in order. By using these methods, six features can be generated, which are the input data of NEWFM. NEWFM classifies NSR and VT/VF beats by the trained bounded sum of weighted fuzzy membership functions (BSWFMs) using six input features from the Creighton University Ventricular Tachyarrhythmia Data Base (CUDB). The results are better than Amann's phase space reconstruction (PSR) algorithm, accuracy and specificity rates of 90.4% and 93.3%, respectively.		Dong-Kun Shin;Sang-Hong Lee;Joon S. Lim	2009		10.1007/978-3-642-01307-2_73	computer science;artificial intelligence;machine learning;mathematics;artificial neural network;wavelet transform	NLP	17.131953283431603	-90.4120007441072	10820
623378777d2862072118ec89200ea4d883f9d84a	a unified approach to coding and interpreting face images	3d pose;image recognition;face reconstruction;image coding;facial appearance;image databases;multi resolution search algorithm;expression recognition;peer reviewed conference;pose recovery;grey level appearance;image coding face recognition image recognition lighting statistical analysis search problems image reconstruction;face recognition;shape;statistical analysis;image coding face recognition image recognition shape facial features pattern recognition image databases robustness face detection image reconstruction;image reconstruction;person identification;pattern recognition;expression recognition face image interpretation image coding variability 3d pose facial expression lighting facial appearance grey level appearance statistical analysis multi resolution search algorithm face reconstruction person identification pose recovery gender recognition;gender recognition;facial features;robustness;search problems;lighting;facial expression;variability;multi resolution;face detection;face image interpretation	Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression and lighting. We describe a compact parametrised model of facial appearance which takes into account all these sources of variability. The model represents both shape and grey-level appearance and is created by performing a statistical analysis over a training set of face images. A robust multi-resolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located and a set of shape and grey-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, pose recovery, gender recognition and expression recognition. The system performs well on all the tasks listed above. >		Andreas Lanitis;Christopher J. Taylor;Timothy F. Cootes	1995		10.1109/ICCV.1995.466919	iterative reconstruction;facial recognition system;computer vision;face detection;active appearance model;speech recognition;shape;computer science;pattern recognition;lighting;facial expression;face hallucination;robustness	Vision	42.01769250888852	-53.621381434422965	10857
dd42a4fd8eb4883b4e09ef185d79d67429212e4c	fusion of block and keypoints based approaches for effective copy-move image forgery detection	zernike moments;image forensics;adaptive fusion;sift;copy move image forgery detection;electrical engineering electronics nuclear engineering	Keypoint-based and block-based methods are two main categories of techniques for detecting copy-move forged images, one of the most common digital image forgery schemes. In general, block-based methods suffer from high computational cost due to the large number of image blocks used and fail to handle geometric transformations. On the contrary, keypoint-based approaches can overcome these two drawbacks yet are found difficult to deal with smooth regions. As a result, fusion of these two approaches is proposed for effective copy-move forgery detection. First, our scheme adaptively determines an appropriate initial size of regions to segment the image into non-overlapped regions. Feature points are extracted as keypoints using the scale invariant feature transform (SIFT) from the image. The ratio between the number of keypoints and the total number of pixels in that region is used to classify the region into smooth or nonsmooth (keypoints) regions. Accordingly, block based approach using Zernike moments and keypoint based approach using SIFT along with filtering and post-processing are respectively applied to these two kinds of regions for effective forgery detection. Experimental results show that the proposed fusion scheme outperforms the keypoint-based method in reliability of detection and the block-based method in efficiency.		Jiangbin Zheng;Yanan Liu;Jinchang Ren;Tingge Zhu;Yijun Yan;Heng Yang	2016	Multidim. Syst. Sign. Process.	10.1007/s11045-016-0416-1	computer vision;computer science;pattern recognition;scale-invariant feature transform;data mining;mathematics	Vision	36.75304907847644	-61.2469811051798	10891
b5767c7014c0ed0ed13876266e45d49991e28a58	segmentation and features extraction techniques, with applications to biomedical images	image reconstruction feature extraction biomedical nmr brain image segmentation medical image processing;brain;white matter;image segmentation;application software;hippocampus;biomedical nmr;unsupervised identification;surrounding white matter;biomedical imaging;2d images series;grey matter contiguous areas;volumetric mri head study;difficult to segment regions;three dimensional displays;feature extraction;image reconstruction;medical image processing;magnetic resonance imaging;gray matter;false contours;grey matter contiguous areas biomedical image segmentation biomedical image feature extraction magnetic resonance imaging medical diagnostic imaging unsupervised identification false contours missing contours difficult to segment regions abdomen organ volumetric measurements 3d reconstruction 2d images series volumetric mri head study surrounding white matter;abdomen;volume measurement;organ volumetric measurements;head;biomedical image feature extraction;missing contours;biomedical image segmentation;feature extraction three dimensional displays hippocampus magnetic resonance imaging head image segmentation image reconstruction application software abdomen volume measurement;3d reconstruction;medical diagnostic imaging	In order to obtain a 3-D reconstruction of the hippocampus from a volumetric MRI head study, it is necessary to separate it not only from the surrounding white matter, but also from contiguous areas of gray matter. At present it is necessary for a physician to manually segment the hippocampus on each slice of the volume in order to obtain such a reconstruction. The authors propose a novel technique by which a computer may make an unsupervised identification of a given structure through a series of images, even if that structure includes so-called false contours or missing contours. Applications include 3-D reconstruction of difficult-to-segment regions of the brain and abdomen, and volumetric measurements of organs from series of 2-D images. >		Edward A. Ashton;Michel J. Berg;Kevin J. Parker;Jeffrey Weisberg;Chang Wen Chen;Leena Ketonen	1994		10.1109/ICIP.1994.413793	3d reconstruction;iterative reconstruction;computer vision;application software;feature extraction;computer science;magnetic resonance imaging;hippocampus;image segmentation;head	EDA	42.39993417958315	-80.79894319036664	10892
d8bcd7e5764c0b749e0aeee5569db1a9dc3d54a1	combining sequential geometry and texture features for distinguishing genuine and deceptive emotions		In this paper, we explore a new type of automatic emotion recognition task - distinguishing genuine and deceptive emotions from video clips. For this task, it is not enough only using static images clipped from the video data, as there's only subtle differences between two types of emotions, which makes it even harder for automatic analysis. To utilize the temporal information, we introduce temporal attention gated model for this emotion recognition task. Compared to texture features which describe the whole face area, the facial landmark sequences may also indicate the temporal changes of the face, thus we utilize them by encoding feature sequence unsupervisedly.	emotion recognition;video clip	Liandong Li;Tadas Baltrusaitis;Bo Sun;Louis-Philippe Morency	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.372	iterative reconstruction;computer vision;clips;visualization;artificial intelligence;emotion recognition;pattern recognition;encoding (memory);computer science	Vision	34.21433463521322	-52.83137844511537	10897
0eb4328faed00d024b2851db7e491c0d94285bd6	fully automated segmentation of abnormal heart in new born babies		We show an intuitive method to segment the heart chambers and epicardial surfaces, including the colossal vessel dividers, in pediatric cardiovascular in Machine Resonance Imaging (MRI) of inherent coronary illness. Exact entire heart division is important to make tolerant specific 3D heart models for surgical arranging within the sight of complex heart abandons. Anatomical changeability because of inborn deformities blocks completely programmed chart book based division. Our intelligent division technique abuses master segmentations of a little arrangement of short-hub cut locales to consequently delineate the rest of the volume utilizing patch-based division. We too research the capability of dynamic figuring out how to naturally request client contribution to zones where division blunder is probably going to be high. Approval is performed on four subjects with twofold outlet right ventricle, a severe inherent heart imperfection. We demonstrate that procedures asking the client to physically fragment districts of enthusiasm inside short-hub cuts yield higher exactness with less client contribution than those questioning whole short-axis cuts. The proposed system validates the technique of automatic segmentation of heart using the combination of Dice matrices and active appearance model techniques which help the doctors to recognize heart disease.		Attifa Bilal;Aslam Muhammad;Martinez-Enriquez Ana Maria	2017		10.1007/978-3-030-02840-4_25	artificial intelligence;entire heart;heart disease;heart.chambers;active appearance model;computer science;pattern recognition;chart;segmentation	Vision	37.88059604479336	-82.11599760932454	10928
a1d576b96b9c853570bf040ce8caa8234c812d6f	motion artifact correction of multi-measured functional near-infrared spectroscopy signals based on signal reconstruction using an artificial neural network †	artificial neural network;functional near-infrared spectroscopy;motion artifact;signal entropy;wavelet transform	In this paper, a new motion artifact correction method is proposed based on multi-channel functional near-infrared spectroscopy (fNIRS) signals. Recently, wavelet transform and hemodynamic response function-based algorithms were proposed as methods of denoising and detrending fNIRS signals. However, these techniques cannot achieve impressive performance in the experimental environment with lots of movement such as gait and rehabilitation tasks because hemodynamic responses have features similar to those of motion artifacts. Moreover, it is difficult to correct motion artifacts in multi-measured fNIRS systems, which have multiple channels and different noise features in each channel. Thus, a new motion artifact correction method for multi-measured fNIRS is proposed in this study, which includes a decision algorithm to determine the most contaminated fNIRS channel based on entropy and a reconstruction algorithm to correct motion artifacts by using a wavelet-decomposed back-propagation neural network. The experimental data was achieved from six subjects and the results were analyzed in comparing conventional algorithms such as HRF smoothing, wavelet denoising, and wavelet MDL. The performance of the proposed method was proven experimentally using the graphical results of the corrected fNIRS signal, CNR that is a performance evaluation index, and the brain activation map.	algorithm;artifact (error);artificial neural network;autonomous robot;backpropagation;brain-computer interfaces;brain–computer interface;cerebral cortex;classification;computation (action);cross-correlation;deep learning;experiment;frequency response;futures studies;hemodynamics;interface device component;mdl (programming language);manuscripts;mathematical optimization;morphologic artifacts;noise reduction;numerous;performance evaluation;signal reconstruction;smoothing (statistical technique);software propagation;spectroscopy, near-infrared;video post-processing;wavelet transform	Gihyoun Lee;Sang Hyeon Jin;Jinung An	2018		10.3390/s18092957		ML	20.093712595152248	-90.20227726330273	10933
6d75dc36d6d3f1ee18c8ce000e5de294b0813899	cig-db: the database for human or mouse immunoglobulin and t cell receptor genes available for cancer studies	genetic engineering;epitopes;embl;animals;search engine;mice;immunoglobulin;text mining;amino acid sequence;databases genetic;immunoglobulins;computational method;t cell receptor;computational biology bioinformatics;models molecular;canonical discriminant analysis;cancer therapy;clinical study;immune system;algorithms;genes neoplasm;sequence analysis;humans;genes t cell receptor;combinatorial libraries;base sequence;computer appl in life sciences;protein data bank;leave one out cross validation;immune response;microarrays;bioinformatics	Immunoglobulin (IG or antibody) and the T-cell receptor (TR) are pivotal proteins in the immune system of higher organisms. In cancer immunotherapy, the immune responses mediated by tumor-epitope-binding IG or TR play important roles in anticancer effects. Although there are public databases specific for immunological genes, their contents have not been associated with clinical studies. Therefore, we developed an integrated database of IG/TR data reported in cancer studies (the Cancer-related Immunological Gene Database [CIG-DB]). This database is designed as a platform to explore public human and murine IG/TR genes sequenced in cancer studies. A total of 38,308 annotation entries for IG/TR proteins were collected from GenBank/DDBJ/EMBL and the Protein Data Bank, and 2,740 non-redundant corresponding MEDLINE references were appended. Next, we filtered the MEDLINE texts by MeSH terms, titles, and abstracts containing keywords related to cancer. After we performed a manual check, we classified the protein entries into two groups: 611 on cancer therapy (Group I) and 1,470 on hematological tumors (Group II). Thus, a total of 2,081 cancer-related IG and TR entries were tabularized. To effectively classify future entries, we developed a computational method based on text mining and canonical discriminant analysis by parsing MeSH/title/abstract words. We performed a leave-one-out cross validation for the method, which showed high accuracy rates: 94.6% for IG references and 94.7% for TR references. We also collected 920 epitope sequences bound with IG/TR. The CIG-DB is equipped with search engines for amino acid sequences and MEDLINE references, sequence analysis tools, and a 3D viewer. This database is accessible without charge or registration at http://www.scchr-cigdb.jp/ , and the search results are freely downloadable. The CIG-DB serves as a bridge between immunological gene data and cancer studies, presenting annotation on IG, TR, and their epitopes. This database contains IG and TR data classified into two cancer-related groups and is able to automatically classify accumulating entries into these groups. The entries in Group I are particularly crucial for cancer immunotherapy, providing supportive information for genetic engineering of novel antibody medicines, tumor-specific TR, and peptide vaccines.	abstract summary;amino acid sequence;amino acids;annotation;bibliographic reference;cell (microprocessor);classification;cross-sectional studies;cross-validation (statistics);dna data bank ofjapan;database;genbank;genetic engineering;hematologic neoplasms;immune system;immunoglobulins;inscriptiones graecae;leukemia, b-cell;linear discriminant analysis;medline;parsing;protein data bank;sequence analysis;text mining;transistor;vaccines, peptide;web search engine;cancer immunotherapy;contents - htmllinktype;db/db mouse;registration - actclass	Yoji Nakamura;Tomoyoshi Komiyama;Motoki Furue;Takashi Gojobori;Yasuto Akiyama	2010		10.1186/1471-2105-11-398	biology;text mining;immune system;bioinformatics;antibody;immunology;genetics	Comp.	-0.7541307886337543	-60.505035478007244	10938
850f215f64176b97e5dd549e6515994006a8bfad	fusion of mdct-based endoluminal renderings and endoscopic video	virtual endoscopy;lung cancer;bronchoscopy;3d visualizations;cancer;3d visualization;real time;endoscopy;texture mapping;virtual reality;data fusion;three dimensional;visualization;ct video fusion;multimodality display;video;3d structure;chest;ct video registration	Early lung cancer can cause structural and color changes to the airway mucosa. A three-dimensional (3D) multidetector CT (MDCT) chest scan provides 3D structural data for airway walls, but no detailed mucosal information. Conversely, bronchoscopy gives color mucosal information, due to airway-wall inflammation and early cancer formation. Unfortunately, each bronchoscopic video image provides only a limited local view of the airway mucosal surface and no 3D structural/location information. The physician has to mentally correlate the video images with each other and the airway surface data to analyze the airway mucosal structure and color. A fusion of the topographical information from the 3D MDCT data and the color information from the bronchoscopic video enables 3D visualization, navigation, localization, and combined color-topographic analysis of the airways. This paper presents a fast method for topographic airway-mucosal surface fusion of bronchoscopic video with 3D MDCT endoluminal views. Tests were performed on phantom sequences, real bronchoscopy patient video, and associated 3D MDCT scans. Results show that we can effectively accomplish mapping over a continuous sequence of airway images spanning several generations of airways in a few seconds. Real-time navigation and visualization of the combined data was performed. The average surface-point mapping error for a phantom case was estimated to be only on the order of 2 mm for 20 mm diameter airway.© (2009) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	modified discrete cosine transform	Lav Rai;William E. Higgins	2009		10.1117/12.808228	computer vision;visualization;virtual reality	Vision	40.061255846971115	-85.17578062663544	10955
beb2dede03f93132d2ce50809d8459e31bebb93b	spatial template extraction for image retrieval by region matching	image segmentation feature extraction image matching image retrieval image representation;image segmentation;image processing;streaming algorithm;recherche image;image matching;representation image;procesamiento imagen;semantic meanings extraction spatial template extraction image retrieval region matching template relation extraction and estimation algorithm tree algorithm image indexing picture libraries visual concepts image contents understanding image representation template extraction and analysis algorithm tea algorithm spatial template relation extraction and measurement algorithm stream algorithm region based methods spatial layout query images;indexing terms;traitement image;algorithme;relation extraction;algorithm;spatial relation;indexing;image representation;pattern matching;feature extraction;indexation;image retrieval image databases indexing libraries algorithm design and analysis streaming media optimization methods web sites world wide web image generation;indizacion;concordance forme;region matching;extraction caracteristique;template;spatial template;algoritmo;image retrieval	This paper presents a template and its relation extraction and estimation (TREE) algorithm for indexing images from picture libraries with more semantics-sensitive meanings. This algorithm can learn the commonality of visual concepts from multiple images to give a middle-level understanding about image contents. In this approach, each image is represented by a set of templates and their spatial relations as keys to capture the essence of this image. Each template is characterized by a set of dominant regions, which reflect different appearances of an object at different conditions and can be obtained by the template extraction and analysis (TEA) algorithm through region matching. The spatial template relation extraction and measurement (STREAM) algorithm is then proposed for obtaining the spatial relations between these templates. Due to the nature of a template, which can represent object's appearances at different conditions, the proposed approach owns better capabilities and flexibilities to capture image contents than traditional region-based methods. In addition, through maintaining the spatial layout of images, the semantic meanings of the query images can be extracted and lead to significant improvements in the accuracy of image retrieval. Since no time-consuming optimization process is involved, the proposed method learns the visual concepts extremely fast. Experimental results are provided to prove the superiority of the proposed method.	clinical use template;image retrieval;indexes;libraries;matching;mathematical optimization;question (inquiry);relationship extraction;tea;algorithm;contents - htmllinktype	Jun-Wei Hsieh;W. Eric L. Grimson	2003	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2003.816013	spatial relation;template;computer vision;search engine indexing;template matching;index term;image processing;feature extraction;image retrieval;template method pattern;computer science;pattern matching;pattern recognition;streaming algorithm;image segmentation;information retrieval	Vision	41.045813455994164	-60.22931138511297	10971
343b07fc60e2bf9c2a2c8da7d46cee1c3bb85cf6	automatic lip identification applied under soft facial emotion conditions	lip identification;pattern recognition automatic lip identification soft facial emotion conditions neutral expression information extraction public rafd database;face recognition emotion recognition;facial emotion;biometrics;emotion recognition;pattern recognition lip identification facial emotion biometrics;face recognition;pattern recognition	This work shows an identification approach based on lip characteristics. For this proposal, the subject has to be without movement, with frontal face, without moving the lips by a neutral expression, angry or indifferent, therefore, not changing significantly the shape of the lips. After a preprocessing stage, global information from lips are extracted, reaching up to 9979%, using 5 training samples for the public RaFD database.	feature extraction;hidden markov model;markov chain;preprocessor;support vector machine	Antonio M. Rojas;Carlos Manuel Travieso-González;Jesús B. Alonso;Miguel Angel Ferrer-Ballester	2012	2012 IEEE International Carnahan Conference on Security Technology (ICCST)	10.1109/CCST.2012.6393562	computer vision;speech recognition;three-dimensional face recognition;communication	Vision	31.592137766768204	-59.79874027206044	11002
09c9538c0065440952014e18b3c561d9f527771e	génolevures complete genomes provide data and tools for comparative genomics of hemiascomycetous yeasts	evolution molecular;software;genomics;complete genome;protein family;genome fungal;comparative genomics;yeasts;databases genetic;col;internet;gene family;user computer interface;metabolic pathway;systems integration;genome sequence;in silico;fungal proteins	The Génolevures online database (http://cbi.labri.fr/Genolevures/) provides tools and data relative to 4 complete and 10 partial genome sequences determined and manually annotated by the Génolevures Consortium, to facilitate comparative genomic studies of hemiascomycetous yeasts. With their relatively small and compact genomes, yeasts offer a unique opportunity for exploring eukaryotic genome evolution. The new version of the Génolevures database provides truly complete (subtelomere to subtelomere) chromosome sequences, 25 000 protein-coding and tRNA genes, and in silico analyses for each gene element. A new feature of the database is a novel collection of conserved multi-species protein families and their mapping to metabolic pathways, coupled with an advanced search feature. Data are presented with a focus on relations between genes and genomes: conservation of genes and gene families, speciation, chromosomal reorganization and synteny. The Génolevures site includes an area for specific studies by members of its international community.	consortium;gene family;genome;genomics;numerous;protein family;synteny;web search engine;yeasts	David James Sherman;Pascal Durrens;Florian Iragne;Emmanuelle Beyne;Macha Nikolski;Jean-Luc Souciet	2006	Nucleic Acids Research	10.1093/nar/gkj160	biology;metabolic pathway;genomics;whole genome sequencing;the internet;bioinformatics;gene family;protein family;comparative genomics;genetics;system integration	Comp.	-1.2793535901065523	-59.972720333212564	11005
0407ef3ecb5fffddaefa30a96f5a0550bb06de47	relationship between survivability of ant colony and rate of state transition to rest	ant colony survivability colony size ant macrobehaviors ode model red harvester ants resting state searching state state transition rate;maintenance engineering mathematical model adaptation models aging recruitment differential equations legged locomotion;ecology differential equations;ecology;differential equations	Red harvester ants allocate themselves to some tasks. In this paper, we propose the ODE model which represents their macro behaviors. In this model, colony size fluctuates due to workers' birth and death. Using the simulator based on this model, we investigate the relationship between survivability of colony and rate of state transition from searching state to resting state.	ant colony;resting state fmri;simulation;state transition table	Yuichi Ogawa;Shigeto Dobata;Fumitoshi Matsuno	2013	Proceedings of the 2013 IEEE/SICE International Symposium on System Integration	10.1109/SII.2013.6776670	biology;artificial intelligence;communication;ecology	Embedded	13.039837513504976	-69.59426518350588	11009
cba08051806b94648638ac041579476c09a10717	a continuous skeletonization method based on distance transform		A skeleton extracted by distance map is located at geometrical center, but it is discrete, on the other hand, we can get a continuous skeleton with morphological algorithm, but the skeleton is not located at the geometrical center of the object image. To get a continuous skeleton that is located at geometrical center of the object image, a continuous skeletonization method based on distance transform is proposed in this paper. At first, the distance function is calculated with respect to the object boundary, which is defined as a new indicator for the skeletonization. Then, a thinning algorithm with five deletion templates is given, which can be applied to get a continuous and centered skeleton indicated by distance map. The performance of the proposed algorithm is compared with existing algorithms, experimental results confirm the superiority of our proposed approach.	distance transform	Ting-Qin Yan;Chang-Xiong Zhou	2012		10.1007/978-3-642-31837-5_37	metric (mathematics);artificial intelligence;pattern recognition;skeleton (computer programming);computer science;distance transform;skeletonization	Vision	44.280309516929286	-64.6546664138012	11021
b08fde87343fa7a37d14b69a0e1fe7f7e97ad085	cad/cam prefabricated individual skull implants: new aspects in robot resection and guided bone regeneration	cranioplasty;robot;cad/cam;biomaterial;composite;cad cam;calcium carbonate;calcium phosphate;titanium;composite material	According to the desire of single-step skull bone resection and reconstruction the acquired CT data were not only used for the design of the individual implant but also for a robot-guided bone resection in an animal cadaver model. The results of three robot skull bone resections during 4 years showed a perspicuously improved precision combined with a dramatically reduced set-up time. As the evaluation of 166 patients with titanium skull implants showed thermal and psychic stress factors, a biodegradable composite material from polyesters, calcium phosphate and calcium carbonate for guided bone regeneration was developed. Skull implants made of this new bone substitution material showed promising results in animal experiments over a follow-up period from 2 to 11 months so far.	computer-aided design;robot	Stephan Weihe;C. Schiller;Christian Rasche;Stefan Haßfeld;Michael Wehmöller;Hayo Knoop;Lesya Kobylinska;Harald Eufinger	2004			biomaterial;cadaver;cranioplasty;dentistry;resection;implant;medicine;skull;skull bone	Robotics	39.262592399824605	-84.68350131238633	11036
04c6d1961304a1831016ff22d50d6683da359990	role and experience determine decision support interface requirements in a neonatal intensive care environment	decision support;decision support tool;decision aid;genie biomedical;user interface;clinical roles;concept sorts;cluster analysis;biomedical engineering;clinical terminology;ingenieria biomedica;neonatal intensive care;hierarchical cluster analysis;card sorts	"""The aim of this paper is to describe a novel approach to the analysis of data obtained from card-sorting experiments. These experiments were performed as a part of the initial phase of a project, called NEONATE. One of the aims of the project is to develop decision support tools for the neonatal intensive care environment. Physical card-sorts were performed using clinical """"action"""" and patient """"descriptor"""" words. Thirty-two staff (eight junior nurses, eight senior nurses, eight junior doctors, and eight senior doctors) participated in the actions card-sorts and the same number of staff participated in separate descriptors card-sorting experiments. To check for consistency, the card-sorts were replicated for nurses during the action card-sorts. The card-sort data were analysed using hierarchical cluster analysis to produce tree-diagrams or dendrograms. Differences were shown in the way various classes of staff with different levels of experience mentally map clinical concepts. Clinical actions were grouped more loosely by nurses and by those with less experience, with a polarisation between senior doctors and junior nurses. Descriptors were classed more definitively and similarly by nurses and senior doctors but in a less structured way and quite differently by junior doctors. This paper presents a summary of the differences in the card-sort data for the various staff categories. It is shown that concepts are used differently by various staff groups in a neonatal unit and that this may diminish the effectiveness of computerised decision aids unless it is explored during their development."""	acquired immunodeficiency syndrome;categories;class;cluster analysis;decision support systems, clinical;decision support system;dendrogram;diagram;experiment;hierarchical clustering;infant, newborn;neonatal intensive care;parse tree;patients;requirement;sorting;tree (data structure)	Gary Ewing;Yvonne Freer;Robert Logie;Jim Hunter;Neil McIntosh;Sue Rudkin;Lindsey Ferguson	2003	Journal of biomedical informatics	10.1016/j.jbi.2003.09.011	medicine;decision support system;computer science;knowledge management;nursing;machine learning;data mining;hierarchical clustering;cluster analysis;user interface	Robotics	1.8331008064703929	-79.4943813996701	11049
f804ac1bfefd9d633ad5cd559d393fc45c440320	a quantitative comparison of the myocardial fibre orientation in the rabbit as determined by histology and by diffusion tensor-mri	cardiac structure;diffusion tensor magnetic resonance imaging;magnetic resonance image;finite element model;diffusion tensor mri;diffusion tensor	Early models of rabbit cardiac fibre structure where from fitting histological fibre orientation onto finite element models. More recently models have been produced using DT-MRI. In a quantitative comparison of these models, in a selected equatorial slice the fibre helix angle has a transmural change of -111.8±30.8o (mean linear fit ± S.D.) in the histological data [H-1]), -92.4±54.5o in DT-MRI dataset [DTI-1] and -86.3±30.7o in DT-MRI dataset [DTI-2]. Variation is large due to outlier data near the RV posterior insertion, and is less when selected anatomical transmural locations are quantified; the lateral LV has a monotonic transmural change of H-1:-87.5±3.7o (mean linear fit ± S.E of slope); DTI-1: -84.8±2.2o; DTI-2:-77.7±2.5o. There is greater variation in the transmural change of the transverse angle than the helix angle (DTI-1, 4.6±83.0o (mean linear fit ± S.D.), pooled data). Limitations in the datasets from both methodologies are discussed in the light of this analysis.		Stephen H. Gilbert;Olivier Bernus;Arun V. Holden;Alan P. Benson	2009		10.1007/978-3-642-01932-6_6	analytical chemistry;mathematics;geometry;nuclear magnetic resonance	Vision	45.15758219642558	-85.44442608283306	11062
dc6eaf7c9636c4a2070c780cea075c8674bdccfd	dynamic analysis of resting state fmri data and its applications	thalamic connectivity resting state fmri data dynamic analysis resting state fmri time series resting state brain quasistatic states hidden markov model thalamus dynamic parcellations thalamic function;fmri;hidden markov model;functional connectivity;resting state fmri;brain dynamics fmri functional connectivity hidden markov model resting state fmri;brain dynamics;time series biomedical mri hidden markov models;hidden markov models time series analysis switches brain modeling clustering algorithms gaussian distribution	While most resting state connectivity studies assume that resting-state fMRI time series are stationary, there is growing evidence indicating that they are in fact dynamically evolving. This paper describes two pieces of our work related to the resting state dynamics. We assume the resting-state brain to be in quasi-static states with spontaneous switching between them. First, we apply a hidden Markov model to the resting state fMRI data and derive model parameters reflecting the states. With this approach, we identified 9 reproducible states, which resemble resting state networks described in the literature. The second piece of work is the dynamic parcellations of thalamus, leading the state specific parcellations and their merged results, both of which revealed new insights about the thalamic function and connectivity.	hidden markov model;markov chain;resting state fmri;spontaneous order;stationary process;time series	Shiyang Chen;Bing Ji;Zhihao Li;Jason Langley;Xiaoping Hu	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472888	computer science;artificial intelligence;machine learning;resting state fmri;hidden markov model	ML	21.862365641021785	-75.2403157051541	11071
22543716e70fa2ab27981dc9bef44e5dffb29374	classification of mri data using deep learning and gaussian process-based model selection		The classification of MRI images according to the anatomical field of view is a necessary task to solve when faced with the increasing quantity of medical images. In parallel, advances in deep learning makes it a suitable tool for computer vision problems. Using a common architecture (such as AlexNet) provides quite good results, but not sufficient for clinical use. Improving the model is not an easy task, due to the large number of hyper-parameters governing both the architecture and the training of the network, and to the limited understanding of their relevance. Since an exhaustive search is not tractable, we propose to optimize the network first by random search, and then by an adaptive search based on Gaussian Processes and Probability of Improvement. Applying this method on a large and varied MRI dataset, we show a substantial improvement between the baseline network and the final one (up to 20% for the most difficult classes).	baseline (configuration management);brute-force search;cobham's thesis;computer vision;deep learning;gaussian process;medical imaging;model selection;random search;relevance	Hadrien Bertrand;Matthieu Perrot;Roberto Ardon;Isabelle Bloch	2017	2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)	10.1109/ISBI.2017.7950626	computer science;machine learning;pattern recognition;data mining;statistics	Vision	29.91869503929415	-74.13647469695769	11083
f0750d37a8754bc15c01de6f306f7a56c052803d	a vision system based on tof 3d imaging technology applied to robotic citrus harvesting	vision system;tof imaging technology;location;recognition;citrus	This study was conducted to develop a fast machine vision system based on TOF (time of flight) three-dimensional (3D) imaging technology for automatic citrus recognition and location. Supported algorithms were specifically developed and programmed for image acquisition and processing. An adaptive filter coupled between partial differential equations filter and shock filter was presented to remove noise and sharpen edges efficiently. Image analysis algorithms integrated both range and amplitude information to generate regions of interest and parameters that are characteristic or very likely to belong to spherical objects. The three-dimensional position of the fruit and radius are obtained after the recognition stages. The total classification results showed that approximately 81.8% were detected and false detection occurred only once. On the whole, the process of imaging, recognition and location consumes less than 50 ms/fruit.	imaging technology	Li Sun;Jian-Rong Cai;Jie-Wen Zhao	2015	Intelligent Automation & Soft Computing	10.1080/10798587.2015.1015767	computer vision;simulation;machine vision;computer science;location	Robotics	46.1639250236519	-55.03068934721005	11085
82ba029aa6a638bb4efdae6041503fafc2679785	development of olfactory display using pulse ejection for medical applications	healthcare;pulse ejection;olfactory hospitals current measurement conductors alzheimer s disease;healthcare olfactory display pulse ejection;t t olfactometer olfactory display pulse ejection medical applications olfaction alzheimer s dementia scent presentation scent lingering nozzles olfactory disturbance olfactory loss;olfactory display;nozzles chemioception medical disorders	Periodic assessment of olfaction is important because a decline in olfaction can be caused by disorders such as Alzheimer's dementia at an early stage. However, current olfactory tests are rarely conducted due to the time and effort necessary for the scent presentation and the problem of scent lingering in the air. Therefore, we developed an olfactory display using pulse ejection for medical use. We also developed several types of nozzles and compared them. We evaluated the performance of the olfactory display on patients with olfactory disturbance. Then, we measured the degree of olfactory loss of the patients by using T&T olfactometer, the most widely known olfactory test in Japan, and found that our olfactory display could accurately assess the olfactory skills of the patients. In the future, we aim to solve the problems of current olfactory tests by using the olfactory display and familiarize olfactory tests.	expect;experiment;personal computer;the quality of life	Eri Matsuura;Shutaro Homma;Shohei Horiguchi;Sayaka Matsumoto;Ken-ichi Okada	2015	2015 17th International Conference on E-health Networking, Application & Services (HealthCom)	10.1109/HealthCom.2015.7454560	psychology;neuroscience;pathology;communication	Visualization	9.441595239947755	-92.1195892169074	11090
0aaef4336c0e82f96e051630bd8e020bd6b9d6a0	an online biometric authentication system based on eigenfingers and finger-geometry	palmprint recognition computational geometry feature extraction karhunen loeve transforms;authentication;geometry;high security environments online biometric authentication system personal authentication score matching level finger geometry feature extraction eigenfingers feature extraction karhunen loève transform k l transform liveness detection module hand dorsal surface ir image medium security environments;biomedical imaging;thumb;vectors;feature extraction;thumb feature extraction authentication geometry vectors biomedical imaging	An approach to personal authentication using the fusion of the finger-geometry and the novel biometric features called eigenfingers at the score-matching level is presented in this paper. The online biometric system integrates finger-geometry features extracted from the four fingers and eigenfingers features extracted by means of the Karhunen-Loève (K-L) transform applied to the four finger strip-like regions. Additionally, the system has a liveness detection module, which uses an IR image of the dorsal surface of a hand. Authentication experiments were conducted on a database consisting of 1270 hand-images (127 persons). The verification results, EER = 0.04% and minimum TER = 0.04%, suggest that the system can be used in medium/high-security environments.	authentication;biometrics;database;enhanced entity–relationship model;experiment;liveness;principal component analysis	Slobodan Ribaric;Ivan Fratric	2005	2005 13th European Signal Processing Conference		computer vision;speech recognition;engineering;pattern recognition	Robotics	32.93132184017785	-61.90396770463326	11104
34d3b9192010a8b5a6320398e0a881caac779bbc	local kernel for brains classification in schizophrenia	mental health;prefrontal cortex;learning by example;scale invariant feature transform;region of interest;normal control;support vector machine	In this paper a novel framework for brain classification is proposed in the context of mental health research. A learning by example method is introduced by combining local measurements with non linear Support Vector Machine. Instead of considering a voxel-by-voxel comparison between patients and controls, we focus on landmark points which are characterized by local region descriptors, namely Scale Invariance Feature Transform (SIFT). Then, matching is obtained by introducing the local kernel for which the samples are represented by unordered set of features. Moreover, a new weighting approach is proposed to take into account the discriminative relevance of the detected groups of features. Experiments have been performed including a set of 54 patients with schizophrenia and 54 normal controls on which region of interest (ROI) have been manually traced by experts. Preliminary results on Dorsolateral PreFrontal Cortex (DLPFC) region are promising since up to 75% of successful classification rate has been obtained with this technique and the performance has improved up to 85% when the subjects have been stratified by sex.	kernel (operating system);region of interest;relevance;support vector machine;unordered associative containers (c++);voxel	Umberto Castellani;E. Rossato;Vittorio Murino;Marcella Bellani;Gianluca Rambaldelli;Michele Tansella;Paolo Brambilla	2009		10.1007/978-3-642-10291-2_12	computer vision;machine learning;pattern recognition;mathematics	ML	31.696933199771593	-78.36536987997555	11108
eb0886ec80827869af8e30f27c73b5c7b3c2c7e2	an eeg based pervasive depression detection for females	pervasive computing;eeg;depression	Recently, depression detection is mainly completed by some rating scales. This procedure requires attendance of physicians and the results may be more subjective. To meet emergent needs of objective and pervasive depression detection, we propose an EEG based approach for females. In the experiment, EEG of 13 depressed females and 12 age matched controls were collected in a resting state with eyes closed. Linear and nonlinear features extracted from artifact-free EEG epochs were subjected to statistical analysis to examine the significance of differences. Results showed that differences were significant for some EEG features between two groups (p<0.05) and the classification rates reached up to 92.9% and 94.2% with KNN and BPNN respectively. Our methods suggest that the discrimination of depressed females from controls is possible. We expect that our EEG based approach could be a pervasive assistant diagnosis tool for psychiatrists and health care specialists.	electroencephalography;pervasive informatics	Xiaowei Zhang;Bin Hu;Lin Zhou;Philip Moore;Jing Chen	2012		10.1007/978-3-642-37015-1_74	human–computer interaction;electroencephalography;computer science;ubiquitous computing	AI	12.909119280543313	-90.04788368957503	11118
d9b8f0e16efcda5d6fcfbc2c31a0b41fdfa83210	a hybrid approach for recognizing adls and care activities using inertial sensors and rfid	model based approach;hybrid approach;feasibility study;inertial sensor	In this paper we present a feasibility study regarding the recognition of high level daily living and care activities. We examine a hybrid discriminative and model based generative approach based on RFID and inertial sensor data. We show that the presented sensor configuration is able to deliver sensor readings and object sightings at a sufficient rate without forcing user compliance. We further evaluated the advantage of a model based approach over a static classifier, compared the individual contribution of each sensor type and could reach accuracy rates of 97% and 85%.	radio-frequency identification;sensor	Albert Hein;Thomas Kirste	2009		10.1007/978-3-642-02710-9_21	embedded system;simulation;engineering;communication	HCI	7.046010107419064	-85.25889319086264	11127
4219668fbe5753ca4c0c66e6c993ddf4683c6b08	structure of proximal and distant regulatory elements in the human genome	tissue specificity;cis regulatory module;activator protein;negative selection;hidden markov model;gene regulation;transcription factor binding site;regulatory element;transcription factor;human genome;experimental validation;transcription start site	Clustering of multiple transcription factor binding sites (TFBSs) for the same transcription factor (TF) is a common feature of cis-regulatory modules in invertebrate animals, but the occurrence of such homotypic clusters of TFBSs (HCTs) in the human genome has remained largely unknown. To explore whether HCTs are also common in human and other vertebrates, we used known binding motifs for vertebrate TFs and a hidden Markov model-based approach to detect HCTs in the human, mouse, chicken, and fugu genomes, and examined their association with cis-regulatory modules. We found that evolutionarily conserved HCTs occupy nearly 2% of the human genome, with experimental evidence for individual TFs supporting their binding to predicted HCTs. More than half of promoters of human genes contain HCTs, with a distribution around the transcription start site in agreement with the experimental data from the ENCODE project. In addition, almost half of 487 experimentally validated developmental enhancers contain them as well - a number more than 25-fold larger than expected by chance. We also found evidence of negative selection acting on TFBSs within HCTs, as the conservation of TFBSs is stronger than the conservation of sequences separating them. The important role of HCTs as components of developmental enhancers is additionally supported by a strong correlation between HCTs and the binding of the enhancer-associated co-activator protein p300. Experimental validation of HCT-containing elements in both zebrafish and mouse suggest that HCTs could be used to predict both the presence of enhancers and their tissue specificity, and are thus a feature that can be effectively used in deciphering the gene regulatory code. In conclusion, our results indicate that HCTs are a pervasive feature of human cis-regulatory modules and suggest that they play an important role in gene regulation in the human and other vertebrate genomes.		Ivan Ovcharenko	2010		10.1007/978-3-642-13078-6_14	biology;human genome;regulation of gene expression;activator;bioinformatics;cis-regulatory module;transcription;genetics;negative selection;hidden markov model;dna binding site;transcription factor	Crypto	4.484700546605841	-60.812088969066195	11134
d7e5644d41aaf075e5d42970c87eadccc4442b31	the ifit: an integrated physical fitness testing system to evaluate the degree of physical fitness of the elderly	geriatrics;physical fitness;instruments;senior citizens;sensors;training;biomechanics;wireless sensor network physical fitness telecommunications;wireless sensor network;senior citizens educational institutions correlation sensors wireless sensor networks instruments training;correlation;wireless sensor networks biomechanics biomedical communication biomedical measurement geriatrics health care;aged aged 80 and over female humans male middle aged monitoring physiologic muscle strength physical fitness postural balance range of motion articular remote sensing technology reproducibility of results software systems integration user computer interface wireless technology;biomedical measurement;wireless sensor networks;balance ifit integrated physical fitness testing system physical fitness degree evaluation older adult physical fitness wireless sensor network flexibility grip strength;telecommunications;biomedical communication;health care	This paper presents an integrated physical fitness testing system (iFit) that evaluates the physical fitness of older adults. The intent of the test is to help them manage and promote their health and mitigate the effects of aging. National protocols of physical fitness were implemented to support the assessment. The proposed system encompasses four modules of physical fitness assessment for both users and medical professionals. The test information will be recorded and managed through a wireless sensor network that will enable a better understanding of users' fitness states. Furthermore, the iFit has been validated by a test session attended by elderly participants. The results show that there is a significant correlation between iFit use in the test of flexibility, grip strength, and balance, compared to conventional methods.	cdisc sdtm respiratory test name terminology;influenza;interferon-induced protein with tetratricopeptide repeats family;pf (firewall);personalization;protocols documentation	Kevin C. Tseng;Alice May-Kuen Wong;Chien-Lung Hsu;Tsai-Hsuan Tsai;Chang-Mu Han;Ming-Ren Lee	2013	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2012.2211357	simulation;wireless sensor network;medicine;computer science;engineering;biomechanics;physical therapy;biological engineering;geriatrics	HCI	8.495687359896461	-87.55394901947221	11135
8287ebf33d935acec4cf9fc856dfaa5bd8e27dd0	wide-angle intraocular imaging and localization	untethered robotic device;intraocular device;human eye;vitreoretinal surgery;wide-angle intraocular imaging;human surgeon;model eye;human eye optic;localization information;accurate localization	Vitreoretinal surgeries require accuracy and dexterity that is often beyond the capabilities of human surgeons. Untethered robotic devices that can achieve the desired precision have been proposed, and localization information is required for their control. Since the interior of the human eye is externally observable, vision can be used for their localization. In this paper we examine the effects of the human eye optics on imaging and localizing intraocular devices. We propose a method for wide-angle intraocular imaging and localization. We demonstrate accurate localization with experiments in a model eye.	algorithm;cns disorder;experiment;internationalization and localization;magnetic resonance imaging;microbotics;observable;ophthalmoscopy;optics;robot;video-in video-out;visual servoing	Christos Bergeles;Kamran Shamaei;Jake J. Abbott;Bradley J. Nelson	2009	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-04268-3_67	computer vision	Robotics	41.46643524531297	-85.98166548687277	11160
3fd63d00d51d4f64a9f43a5476e8eb6cde70727c	a finite element model for epidermal wound healing	growth factor;epidermal cells;finite element;finite element model;wound healing;finite elements;neo vascularization;wound contraction	A finite element model for epidermal wound healing is proposed. The model takes into account the sequential steps of angiogenesis (neo-vascularization) and wound contraction (the actual healing of a wound). An innovation in the present study is the combination of both partially overlapping processes, yielding novel insights into the process of wound healing. The models consist of nonlinearly coupled diffusion-reaction equations, in which transport of oxygen, growth factors and epidermal cells and mitosis are taken into account.	finite element method	Fred J. Vermolen;John A. Adam	2007		10.1007/978-3-540-72584-8_10	finite element method	ML	29.440432651689243	-85.7420578434566	11164
dad7de08d0ea9a977051ee2b5d68c5b9a894397a	analysis and improvement of image-based insertion point estimation for robot-assisted minimally invasive surgery	estimation theory;point estimation;instruments;minimal invasive surgery;laparoscopy;physiological motions;surgical instruments;surgery endoscopes estimation theory medical robotics robot vision;data mining;medical robotics;statistical properties;robot vision;endoscopic images;image based insertion point estimation;three dimensional displays;endoscopes;surgery;flexible endoscopy;minimally invasive surgery image analysis surgical instruments cameras endoscopes robot vision systems motion estimation robotics and automation laparoscopes in vitro;estimation error;robot assisted minimally invasive surgery;cameras;physiological motions robot assisted minimally invasive surgery image based insertion point estimation surgical instruments endoscopic images laparoscopy flexible endoscopy	Estimating insertion points of surgical instruments for minimally invasive surgery is a necessary step to be able to control surgical instruments using endoscopic images. In this paper, we propose an analysis of possible methods which use image information only. Mathematical properties are detailed together with statistical properties obtained by simulations. Then a specific method is chosen to estimate the insertion point for bi-modal surgery (laparoscopy and flexible endoscopy). In vitro experiments show the accuracy of the approach and how it is possible to track the motion of the insertion point in the case of physiological motions.	camera resectioning;caret;experiment;modal logic;robot;simulation;video-in video-out	Florent Nageotte;Laurent Ott;Philippe Zanne;Michel de Mathelin	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152654	computer vision;point estimation;biological engineering;estimation theory;statistics	Robotics	42.72723033196525	-85.63428101748296	11182
32be5fb0e37365d6779d0f232da28f00cbf39a37	scop family fingerprints: an information theoretic approach to structural classification of protein domains	software tool;domain family characterization;molecular configurations;databases proteins amino acids convergence protein engineering biological system modeling;functional properties;protein domains;proteins bioinformatics genetics molecular biophysics molecular configurations;scop classification;spectrum;genetics;domain family characterization protein domains scop classification blosum spectrum;proteins;molecular biophysics;experimental validation;blosum spectrum;software tool scop family fingerprints information theoretic approach structural classification protein domains domain classification four level hierarchy structural similarity phylogenetic relation;structural classification of proteins;information theoretic;structural similarity;bioinformatics	Protein domain classification is a useful instrument to deduce functional properties of proteins. Several databases have been introduced that collect domains having a known structure, and SCOP is probably the most used one. It classifies domains in a four level hierarchy and it groups sequences according to both structural similarity and phylogenetic relation. Many automatic tools to classify domains according to available databases have been proposed so far. In this paper we introduce the notion of “fingerprint” as an easy and readable digest of the similarities between a sequence and an entire set of sequences, and this concept offers us a rationale for building an automatic SCOP classifier which assigns a query sequence to the most likely family. Fingerprint-based analysis has been implemented in a software tool and we report some experimental validations for it.	cryptographic hash function;database;design rationale;fingerprint;human-readable medium;information theory;phylogenetics;programming tool;scop;structural similarity	Alberto Casagrande;Francesco Fabris	2011	2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW)	10.1109/BIBMW.2011.6112408	biology;spectrum;bioinformatics;structural similarity;machine learning;data mining;protein domain;genetics;molecular biophysics	DB	1.6842125221931792	-58.34089071716868	11184
3a8a4b190628b856a4a4eec8613d00f7a97a0a0e	analyzing locomotion synthesis with feature-based motion graphs	databases;detectors;human like motion planning computer animation locomotion motion capture;motion control;image segmentation;foot;search problems collision avoidance computer animation inverse problems motion control reachability analysis;locomotion;joints;motion capture;motion segmentation;feature extraction;animation locomotion synthesis analyzing feature based motion graph realistic locomotion synthesis search query foot skating removal graph construction time search performance fast channel search method motion graph search free channel obstacle clearance collision checking motion deformation model inverse kinematics continuous deformation range transition cost threshold reachability motion graph technique;search problems;collision avoidance;human like motion planning;computer animation;motion segmentation feature extraction image segmentation detectors joints databases foot;reachability analysis;inverse problems	We propose feature-based motion graphs for realistic locomotion synthesis among obstacles. Among several advantages, feature-based motion graphs achieve improved results in search queries, eliminate the need of postprocessing for foot skating removal, and reduce the computational requirements in comparison to traditional motion graphs. Our contributions are threefold. First, we show that choosing transitions based on relevant features significantly reduces graph construction time and leads to improved search performances. Second, we employ a fast channel search method that confines the motion graph search to a free channel with guaranteed clearance among obstacles, achieving faster and improved results that avoid expensive collision checking. Lastly, we present a motion deformation model based on Inverse Kinematics applied over the transitions of a solution branch. Each transition is assigned a continuous deformation range that does not exceed the original transition cost threshold specified by the user for the graph construction. The obtained deformation improves the reachability of the feature-based motion graph and in turn also reduces the time spent during search. The results obtained by the proposed methods are evaluated and quantified, and they demonstrate significant improvements in comparison to traditional motion graph techniques.	checking (action);choose (action);collision detection;detectors;experiment;graph (discrete mathematics);graph - visual representation;graph traversal;inverse kinematics;maxima and minima;performance;personnameuse - assigned;reachability;requirement;sensor;skating;solutions;web search query;benefit	Mentar Mahmudi;Marcelo Kallmann	2013	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.149	motion control;computer vision;mathematical optimization;detector;motion capture;simulation;feature extraction;computer science;inverse problem;computer animation;image segmentation;foot	Visualization	42.455109500247964	-74.55306665169746	11190
9175ff429007717dd63efcd1e065f2542f27627b	application of bayesian decomposition for analysing microarray data	microarray data;oscillations;expression profile;growth and development;large data sets;chip;gene expression;microarray data analysis;drug treatment;temporal pattern;cell cycle;data analysis techniques;high throughput	MOTIVATION Microarray and gene chip technology provide high throughput tools for measuring gene expression levels in a variety of circumstances, including cellular response to drug treatment, cellular growth and development, tumorigenesis, among many other processes. In order to interpret the large data sets generated in experiments, data analysis techniques that consider biological knowledge during analysis will be extremely useful. We present here results showing the application of such a tool to expression data from yeast cell cycle experiments.   RESULTS Originally developed for spectroscopic analysis, Bayesian Decomposition (BD) includes two features which make it useful for microarray data analysis: the ability to assign genes to multiple coexpression groups and the ability to encode biological knowledge into the system. Here we demonstrate the ability of the algorithm to provide insight into the yeast cell cycle, including identification of five temporal patterns tied to cell cycle phases as well as the identification of a pattern tied to an approximately 40 min cell cycle oscillator. The genes are simultaneously assigned to the patterns, including partial assignment to multiple patterns when this is required to explain the expression profile.   AVAILABILITY The application is available free to academic users under a material transfer agreement. Go to http://bioinformatics.fccc.edu/ for more details.		Thomas D. Moloshok;R. R. Klevecz;Jeffrey D. Grant;Frank J. Manion;IV W.F.Speier;Michael F. Ochs	2002	Bioinformatics	10.1093/bioinformatics/18.4.566	biology;microarray analysis techniques;gene chip analysis;bioinformatics;data science;data mining;genetics	Comp.	4.696273730771026	-55.60807133374035	11196
980fd7f0c84df2667fa54c0df0b3f948db8aa5c0	historical development of heat stroke prevention device in the military	brain;protective clothing;history;biothermics;military equipment;protective clothing biomedical engineering biomedical equipment biothermics brain cooling history military equipment;biomedical engineering;heating sun stress humidity heat sinks temperature;vest collar system historical development heat stroke prevention device development heat stress heat stroke risk reduction soldier survival military operation heat stroke onset heat stroke counter measure development heat stroke cause garment design core body temperature lowering brain temperature lowering cooling device;biomedical equipment;cooling	Understanding of core body temperature, heat stress and heat stroke developed progressively over the centuries. Soldiers involved in military operations have a higher risk to develop heat stroke, and to not survive following the onset. This paper follows the evolving understanding of heat stroke and development of counter measures. At certain times in history, incomplete understanding of the causes of heat stroke led to the development of devices that did not lower the risk. As understanding improved, development of improved methods and devices became possible. In the present day, several designs for garments that can lower core body temperature or brain temperature have been developed, and would reduce the risk of heat stroke. These cooling devices include a vest and collar system. Further refinements of these designs to make them more practical would allow wider deployment.	body temperature;clothing;computer cooling;cool - action;deploy;hl7publishingsubsection <operations>;heat stress disorders;heat stroke;onset (audio)	Jordan D. Countryman;Douglas E. Dow	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610054	engineering;forensic engineering;surgery;mechanical engineering	Visualization	8.709813856118224	-81.91419945325889	11205
4b7cea054c654a56aef496c34bc1a0697f34f9d2	extracting transients from cerebral oxygenation signals of preterm infants: a new singular-spectrum analysis method		Many infants born prematurely develop brain injury within the first few days after birth. Near infrared spectroscopy (NIRS) is a safe technology that can continuously monitor the varying levels of oxygenation in the brain. Analysis of this signal has the potential to detect the onset of brain injury. We develop a method that extracts transient waveforms from the oxygenation signal. This method uses the cosine transform and singular-spectrum analysis to decompose the signal. We test different procedures to select a threshold for estimating the transient component. As part of the development of the method, we build a model of the cerebral oxygenation signals combining clusters of transient waveforms and nonstationary coloured noise. After development, we test on cerebral oxygenation recordings from 10 extremely preterm infants. We find that using the decomposition method to remove the transient components improves detection performance of brain injury, from an area-under the receiver operator characteristic of 0.91 to 1.00. These findings highlight the importance of specific signal processing methods for the cerebral oxygenation signal and the potential for NIRS as a neuromonitoring technology in neonatal intensive care.	brain injuries;cell respiration;estimated;infant, extremely premature;neonatal intensive care;onset (audio);preterm infant;receiver operating characteristic;signal processing;singular;spectroscopy, near-infrared;spectrum analysis	John M. O'Toole;Eugene M. Dempsey;Geraldine B. Boylan	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513523	electronic engineering;computer vision;discrete cosine transform;signal processing;receiver operating characteristic;waveform;singular spectrum analysis;near-infrared spectroscopy;artificial intelligence;computer science	SE	18.57822919491515	-89.60270609348068	11214
90339fc9651341a1253880181c143ee9b1539f9b	integrative modeling of transcriptional regulation in response to antirheumatic therapy	transcription genetic;expression profile;integrable model;immunoglobulin g;receptors tumor necrosis factor;gene regulatory networks;transcription factor binding site;dna binding;gene expression data;scale free network;antirheumatic agents;computational biology bioinformatics;transcription regulation;mechanism of action;network model;rheumatic diseases;gene expression regulation;system biology;algorithms;humans;biological data;parallel architecture;dna microarray;combinatorial libraries;molecular mechanics;computational biology;gene regulatory network;computer appl in life sciences;gene expression profiling;oligonucleotide array sequence analysis;immune response;rheumatoid arthritis;microarrays;bioinformatics	The investigation of gene regulatory networks is an important issue in molecular systems biology and significant progress has been made by combining different types of biological data. The purpose of this study was to characterize the transcriptional program induced by etanercept therapy in patients with rheumatoid arthritis (RA). Etanercept is known to reduce disease symptoms and progression in RA, but the underlying molecular mechanisms have not been fully elucidated. Using a DNA microarray dataset providing genome-wide expression profiles of 19 RA patients within the first week of therapy we identified significant transcriptional changes in 83 genes. Most of these genes are known to control the human body's immune response. A novel algorithm called TILAR was then applied to construct a linear network model of the genes' regulatory interactions. The inference method derives a model from the data based on the Least Angle Regression while incorporating DNA-binding site information. As a result we obtained a scale-free network that exhibits a self-regulating and highly parallel architecture, and reflects the pleiotropic immunological role of the therapeutic target TNF-alpha. Moreover, we could show that our integrative modeling strategy performs much better than algorithms using gene expression data alone. We present TILAR, a method to deduce gene regulatory interactions from gene expression data by integrating information on transcription factor binding sites. The inferred network uncovers gene regulatory effects in response to etanercept and thus provides useful hypotheses about the drug's mechanisms of action.	algorithm;antirheumatic agents;binding sites;color gradient;dna microarray format;dna binding site;etanercept;exhibits as topic;gene expression;gene regulatory network;inference;interaction;least-angle regression;mathematical model;molecular systems biology;network model;parallel computing;patients;rheumatoid arthritis;silo (dataset);transcription factor;therapeutic targets database;traffic collision avoidance system;transcription (software);transcription, genetic;transcriptional regulation;type i interferon receptor	Michael Hecker;Robert Hermann Goertsches;Robby Engelmann;Hans-Jürgen Thiesen;Reinhard Guthke	2009	BMC Bioinformatics	10.1186/1471-2105-10-262	biology;gene regulatory network;molecular biology;dna microarray;bioinformatics;genetics;systems biology	Comp.	6.433161439576918	-56.95500131400526	11216
e5e0d91aeaa50165b17958dba06c450dc74f1571	the mid-developmental transition and the evolution of animal body plans	biological patents;biomedical journals;text mining;europe pubmed central;citation search;gene regulatory networks;citation networks;evolutionary developmental biology;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search;embryogenesis	Animals are grouped into ~35 ‘phyla’ based upon the notion of distinct body plans. Morphological and molecular analyses have revealed that a stage in the middle of development—known as the phylotypic period—is conserved among species within some phyla. Although these analyses provide evidence for their existence, phyla have also been criticized as lacking an objective definition, and consequently based on arbitrary groupings of animals. Here we compare the developmental transcriptomes of ten species, each annotated to a different phylum, with a wide range of life histories and embryonic forms. We find that in all ten species, development comprises the coupling of early and late phases of conserved gene expression. These phases are linked by a divergent ‘mid-developmental transition’ that uses species-specific suites of signalling pathways and transcription factors. This mid-developmental transition overlaps with the phylotypic period that has been defined previously for three of the ten phyla, suggesting that transcriptional circuits and signalling mechanisms active during this transition are crucial for defining the phyletic body plan and that the mid-developmental transition may be used to define phylotypic periods in other phyla. Placing these observations alongside the reported conservation of mid-development within phyla, we propose that a phylum may be defined as a collection of species whose gene expression at the mid-developmental transition is both highly conserved among them, yet divergent relative to other species.	conserved sequence;developmental robotics;gene expression profiling;phylogenetics;phylum (taxon);transcription factor;transcription (software);transcription, genetic;transcriptome;biological signaling	Michal Levin;Leon Anavy;Alison G. Cole;Eitan Winter;Natalia Mostov;Sally Khair;Naftalie Senderovich;Ekaterina Kovalev;David H. Silver;Martin Feder;Selene L. Fernandez-Valverde;Nagayasu Nakanishi;David Simmons;Oleg Simakov;Tomas Larsson;Shang-Yun Liu;Ayelet Jerafi-Vider;Karina Yaniv;Joseph F. Ryan	2016		10.1038/nature16994	biology;zoology;gene regulatory network;embryogenesis;text mining;bioinformatics;genetics;evolutionary developmental biology	PL	4.421784226189621	-62.127291574702014	11217
6041a5f9addc9b8affba16c018b6982a3e3dcb54	a dual kalman filter for parameter-state estimation in real-time dna microarrays	dna;real time;dual kalman filter dna microarrays real time;kalman filters;dual kalman filter;prediction algorithms;kalman filter;dna microarrays;state estimation;probes kalman filters noise dna estimation real time systems prediction algorithms;probes;parameter estimation biochemistry biosensors dna kalman filters lab on a chip molecular biophysics;estimation;algorithms models genetic oligonucleotide array sequence analysis;molecular biophysics;affinity based biosensors parameter state estimation real time dna microarrays binding reaction dual kalman filter;lab on a chip;parameter estimation;dna microarray;kinetics;biochemistry;biosensors;noise;real time systems	Affinity-based biosensors rely on chemical attraction between analytes (targets) and their molecular complements (probes) to detect presence and quantify amounts of the analytes of interest. Real-time DNA microarrays acquire multiple temporal samples of the target-probe binding process. In this paper, estimation of the amount of targets based on early kinetics of the binding reaction is studied. A dual Kalman filter for the parameter-state estimation is proposed. Computational studies demonstrate efficacy of the proposed method.	algorithm;biosensors;complement system proteins;computation;dna microarray;dual;kalman filter;kinetics internet protocol;name binding;population parameter;quantitation;real-time clock;simulation;analyte	Manohar Shamaiah;Haris Vikalo	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091876	kalman filter;biology;dna microarray;computer science;bioinformatics;mathematics;statistics;molecular biophysics	EDA	7.325744989223161	-59.3341129603571	11223
252a28eb24ffed020a85f4becf3748edb7f84b99	human face recognition from a single front view	active contour;learning;edge detection;extraction forme;deformable contour;intelligence artificielle;classification;deteccion contorno;algorithme;aprendizaje;algorithm;detection contour;apprentissage;face recognition;extraccion forma;feature extraction;identification;pattern recognition;greedy algorithm;artificial intelligence;identificacion;inteligencia artificial;reconnaissance forme;reconocimiento patron;pattern extraction;clasificacion;algoritmo	This paper presents a face recognition system which can identify the unknown identity effectively using the front-view facial features. In front-view facial feature extractions, we can capture the contours of eyes and mouth by the deformable template model because of their analytically describable shapes. However, the shapes of eyebrows, nostrils and face are difficult to model using a deformable template. We extract them by using the active contour model (snake). After the contours of all facial features have been captured, we calculate effective feature values from these extracted contours and construct databases for unknown identities classification. In the database generation phase, 12 models are photographed, and feature vectors are calculated for each portrait. In the identification phase if any one of these 12 persons has his picture taken again, the system can recognize his identity.		Ching-Wen Chen;Chung-Lin Huang	1992	IJPRAI	10.1142/S021800149200031X	facial recognition system;identification;computer vision;greedy algorithm;speech recognition;edge detection;feature extraction;biological classification;computer science;artificial intelligence;active contour model	Vision	45.19504077545194	-59.0251991438998	11224
18811d0e8556017e7b371a19f25921c8850894c1	hemodynamics during rotary blood pump support with speed synchronization in heart failure condition: a modelling study	heart mathematical model pathology computational modeling hemodynamics yttrium;haemodynamics blood cardiovascular system;systolic aortic pressure hemodynamics rotary blood pump support speed synchronization heart failure condition continuous mode copulse mode pump speed computer simulation pathological heart condition maximum systolic elestance cardiovascular system model matlab simulink pressure volume loop end systolic pressure volume relationship	The aim of this work is to study the hemodynamic changes in the cardiovascular system under different modes of Rotary Blood Pump (RBP) support. Continuous mode (constant pump speed) and co-pulse mode (increased pump speed in systole) are studied. Computer simulation studies have been conducted to evaluate the performances of these two modes under normal and pathological conditions. The pathological heart condition is simulated by reducing the maximum systolic elestance (Emax) in the cardiovascular system model. The model is implemented by using MATLAB Simulink. The pressure-volume loop of different heart conditions (normal heart: 100% of normal contractility, pathological heart: 30% of normal contractility) and the different modes of RBP support (8krpm and 11krpm in continuous mode, between 8krpm and 11krpm in co-pulse mode) are simulated. The results of this study show the slope of end systolic pressure volume relationship (ESPVR) changes in pathological condition. The reduction of area inside pressure volume loops depend on the increasing level of pump speed. The results indicated systolic aortic pressures in co-pulse mode are higher than in the continuous mode. In normal condition, the value of systolic aortic pressure in co-pulse mode is 113 mmHg and the values of systolic aortic pressures in continuous modes are 109 mmHg (8k) and 95 mmHg (11k). In pathological condition, the value of systolic aortic pressure in co pulse mode is 100 mmHg and the values of systolic aortic pressures in continuous modes are 90 mmHg (8k) and 95 mmHg (11k). The hemodynamics results of this study are comparable in vivo data, clinical data and other simulation studies. Therefore, this simulation enables hemodynamic studies in patients with end-stage heart failure, and patients under different modes of rotary blood pump support	computer simulation;heart failure;hemodynamics;matlab;patients;performance;rotary system;rotary woofer;simulink;systolic pressure;video-in video-out;torr	Zwe Lin Htet;Thin Pa Pa Aye;Thamvarit Singhavilai;Phornphop Naiyanetr	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319099	control engineering;engineering;diabetes mellitus;cardiology	DB	15.622355157542517	-84.0640930005309	11229
23e2b7e8fb8f045037116c239ea9750ed7fdf3d4	bioinformatics techniques for drug discovery		Recent knowledge collected on drug molecules and their intermolecular interactions can be used to predict the mechanisms underlying the human physiological processes. In this scenario, computer-aided drug design (CADD) is commonly employed to facilitate the progression of potential inhibitor identification. Amongst the various computational approaches, pharmacophore modelling is classified as a decent technique to identify the lead inhibitors or drug molecules that fit chemically different structural classes. Besides, biological networks and designed biochemical mathematical models have been employed to explore the pharmacokinetics and pharmacodynamics in biological systems. Moreover, molecular dynamics (MD) simulation, a broadly used computational approach based on Newton’s equation of motion for a given system of atoms, delivers the information about protein–ligand interactions. Additionally, synthetic biology approach has been broadly employed as a precise and vigorous technique to accelerate the genome sequence data and reduction inDNAsynthesis cost. Synthetic biology has been also reported to investigate the biological circuit and behaviour or the role of human physiological system. Prominently, the competences to design potential drugs are highly dependent on the fundamental understanding of drug molecules and their biochemical interactions. In this context, the gap between number of identified hit molecules and authentic or genuine drug molecules can be bridged by utilizing the recent bioinformatics approaches.	bioinformatics;biological system;color gradient;computer-aided design;interaction;mathematical model;molecular dynamics;newton;pharmacophore;simulation;synthetic biology	Aman Chandra Kaushik;Ajay Kumar;Shiv Bharadwaj;Ravi Chaudhary;Shakti Sahi	2018		10.1007/978-3-319-75732-2	drug discovery;bioinformatics;biology	Comp.	8.74323313658308	-59.67508091221037	11234
369c66171757b3178feaa60b75cf16599e19bb96	sleepsight: a wearables-based relapse prevention system for schizophrenia	human information processing;input devices and strategies;mental disorder;mobile sensing;web based services;sensor networks;user centred design;remote monitoring;probabilistic time series modelling;wearable computing	SleepSight is a novel approach to detecting early signs of relapse in psychosis, thereby allowing targeted intervention for relapse prevention. The system uses a wireless-enabled wearable device and smartphone to collect longitudinal accelerometry, heart-rate, ambient light and smartphone usage patterns from patients living in their homes. These data are encrypted and sent via the mobile data network to a secure server for real-time analysis, using the Purple Robot mobile application.  This study tested feasibility and acceptability of the SleepSight system in 15 participants with a diagnosis of schizophrenia. Patient recruitment and data collection was completed in January 2016, at the South London and Maudsley NHS Foundation Trust (SLaM), the largest mental health service provider in the EU. The project is a unique multidisciplinary collaboration between the NIHR Biomedical Research Centre for Mental Health, the Institute of Psychiatry, Psychology and Neuroscience, and the National Health Service.	encryption;https;mobile app;real-time web;sensor;server (computing);smartphone;wearable computer;wearable technology	Maximilian Kerz;Amos Folarin;Nicholas Meyer;Mark Begale;James H MacCabe;Richard J. B. Dobson	2016		10.1145/2968219.2971419	embedded system;simulation;wireless sensor network;wearable computer;computer science;computer security;rmon	HCI	5.885866506847263	-89.83273985177463	11237
73edba8fae44805db46b8a6c9c1405d9e3a04b30	deep networks with shape priors for nucleus detection		Detection of cell nuclei in microscopic images is a challenging research topic, because of limitations in cellular image quality and diversity of nuclear morphology, i.e. varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train for example convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many of these methods are supplemented by spatial or morphological processing. We develop a new approach that we call Shape Priors with Convolutional Neural Networks (SP-CNN) to perform significantly enhanced nuclei detection. A set of canonical shapes is prepared with the help of a domain expert. Subsequently, we present a new network structure that can incorporate ‘expected behavior’ of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate a new regularization term that is targeted at penalizing false positives while simultaneously encouraging detection inside cell nucleus boundary. Experimental results on a challenging dataset reveal that SP-CNN is competitive with or outperforms several state-of-the-art methods.		Mohammad Tofighi;Tiantong Guo;Jairam K. P. Vanamala;Vishal Monga	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451797	pattern recognition;deep learning;nucleus;machine learning;image quality;convolutional neural network;artificial intelligence;computer science;training set;feature extraction;prior probability	Vision	30.604562921039385	-72.99128387059493	11278
099de6f28a26b7bf165ccba0e459e5f78ad64bf7	client-side integration of life science literature resources	ressource;science vie;integration;life science;integracion;life sciences;ciencia vida;recurso;resource	MOTIVATION The online resources in the life sciences are characterized by a great fragmentation and one of the pressing issues of bioinformatics is making the integration of these resources a smoother and more flexible process than it is currently. Here we present i-cite, a browser extension, which implements a client-side model of integration which improves the navigation within the rapidly increasing life science literature and links terms from it to corresponding non-textual data.   AVAILABILITY http://i-cite.org.	bioinformatics;biological science disciplines;browser extension;client-side;dna integration;fragmentation (computing);text corpus	Richard Easty;Nikolay Nikolov	2009	Bioinformatics	10.1093/bioinformatics/btp550	operations research;resource	Comp.	-4.412582620694473	-62.043887642776596	11298
3ff9da470cb0e563fe6fd910918ddad19830ebdd	parapred: antibody paratope prediction using convolutional and recurrent neural networks		Motivation Antibodies play essential roles in the immune system of vertebrates and are powerful tools in research and diagnostics. While hypervariable regions of antibodies, which are responsible for binding, can be readily identified from their amino acid sequence, it remains challenging to accurately pinpoint which amino acids will be in contact with the antigen (the paratope).   Results In this work, we present a sequence-based probabilistic machine learning algorithm for paratope prediction, named Parapred. Parapred uses a deep-learning architecture to leverage features from both local residue neighbourhoods and across the entire sequence. The method significantly improves on the current state-of-the-art methodology, and only requires a stretch of amino acid sequence corresponding to a hypervariable region as an input, without any information about the antigen. We further show that our predictions can be used to improve both speed and accuracy of a rigid docking algorithm.   Availability and implementation The Parapred method is freely available as a webserver at http://www-mvsoftware.ch.cam.ac.uk/and for download at https://github.com/eliberis/parapred.   Supplementary information Supplementary information is available at Bioinformatics online.		Edgar Liberis;Petar Velickovic;Pietro Sormanni;Michele Vendruscolo;Pietro Liò	2018	Bioinformatics	10.1093/bioinformatics/bty305	antibody;paratope;computer science;bioinformatics;recurrent neural network	Comp.	8.526706685266474	-56.505192837071995	11360
3db39d72ff517d98284de528dd12250adacff069	estimating the accuracy level among individual detections in clustered microcalcifications	detectors;clustered microcalcifications mcs computer aided diagnosis cad false positives fps in detection mammography spatial point process spp;cancer;sensitivity;lesions detectors cancer estimation mammography sensitivity solid modeling;estimation;lesions;solid modeling;mammography	Computerized detection of clustered microcalcifications (MCs) in mammograms often suffers from the occurrence of false positives (FPs), which can vary greatly from case to case. We investigate how to apply statistical estimation to determine the number of FPs that are present in a detected MC lesion. First, we describe the number of true positives (TPs) by a Poisson–binomial probability distribution, wherein a logistic regression model is trained to determine the probability for an individual detected MC to be a TP based on its detector output. Afterward, we model the spatial occurrence of FPs in a lesion area by a spatial point process (SPP), of which the distribution parameters are estimated from the detections in the lesion and its surrounding region. Furthermore, to improve the estimation accuracy, we incorporate the Poisson–binomial distribution of the number of TPs into the SPP model using maximum a posteriori estimation. In the experiments, we demonstrated the proposed approach on the detection results from a set of 188 full-field digital mammography (FFDM) images (95 cases) by three existing MC detectors. The results showed that there was a strong consistency between the estimated and the actual number of TPs (or FPs) for these detectors. When the fraction of FPs in detection was varied from 20% to 50%, both the mean and median values of the estimation error were within 11% of the total number of detected MCs in a lesion. In particular, when the number of FPs increased to as high as 11.38 in a cluster on average, the error was 2.51 in the estimated number of FPs. In addition, lesions estimated to be more accurate in detection were shown to have better classification accuracy (for being malignant or benign) than those estimated to be less accurate.	detectors;digital mammography;estimated;experiment;logistic regression;mean squared error;mental suffering;microcalcification;point process;self-propelled particles;statistical estimation;strong consistency;total number;true hermaphroditism (disorder)	Mar&#x00ED;a V. Sainz de Cea;Robert M. Nishikawa;Yongyi Yang	2017	IEEE Transactions on Medical Imaging	10.1109/TMI.2017.2654799	computer vision;estimation;detector;sensitivity;mathematics;solid modeling;statistics;cancer	ML	35.01889928824997	-78.1732237364535	11371
677023b4c326618fa2305b82ec11726ee81839ff	fuzzy logic and scale space approach to microcalcification detection	cancer detection;maximum entropy methods;maximum entropy principle;cancer;image databases;lungs;filters;microcalcification detection;fuzzy set theory;breast carcinoma;carcinomas;fuzzy logic;image enhancement;fuzzy logic breast cancer cancer detection fuzzy set theory clustering algorithms entropy image databases computer science filters lungs;scale space;scale space techniques;medical image processing;operating characteristic;clustering algorithms;computer aided detection;entropy;computer science;mammography;laplacian of a gaussian filter;object detection cancer maximum entropy methods fuzzy set theory fuzzy logic mammography medical image processing image enhancement;maximum entropy principle fuzzy logic scale space techniques microcalcification detection breast cancer masses carcinomas mammograms computer aided detection fuzzy set theory fuzzy entropy principle image enhancement laplacian of a gaussian filter free response operating characteristic curve;breast cancer;mammograms;masses;free response operating characteristic curve;object detection;fuzzy entropy principle	Breast cancer is one of the leading causes of women mortality in the world. Since the causes are unknown, breast cancer cannot be prevented. It is difficult for radiologists to provide both accurate and uniform evaluation over the enormous number of mammograms generated in widespread screening. Microcalcifications and masses are the earliest signs of breast carcinomas and their detection is one of the key issues for breast cancer control. Computer-aided detection of microcalcifications and masses is an important and challenging task in breast cancer control. This paper presents a novel approach for detecting microcalcification clusters (MCCs). First, mammograms are normalized. Then, fuzzy set theory and fuzzy entropy principle are employed to fuzzify the mammograms. Then, the fuzzified images are enhanced. Finally, scalespace and Laplacian-of-a-Gaussian filter techniques are used to determine the sizes and locations of microcalicifications. A free-response operating characteristic (FROC) curve is used to evaluate the performance. The major advantage of the proposed system is its ability to detect microcalcifications even in very dense breast mammograms. A data set of 40 mammograms (Nijmegen database) containing 105 clusters of microcalcifications is studied. Experimental result show that the microcalcifications can be accurately and efficiently detected using the proposed approach.	computer cluster;fuzzy logic;fuzzy set;radiology;scale space;sensor;set theory	Heng-Da Cheng;Jingli Wang	2003		10.1109/ICASSP.2003.1202365	fuzzy logic;computer vision;entropy;scale space;computer science;principle of maximum entropy;breast cancer;machine learning;pattern recognition;mathematics;fuzzy set;cluster analysis;cancer	AI	36.7283017800541	-74.27688637678851	11375
37984cb1389be990a1f7b356f928b1a6b4b6dc37	graphics hardware accelerated reconstruction of spect with a slat collimated strip detector	system modeling;iterative reconstruction;technology and engineering;graphics hardware;field of view;single photon emission computed tomography;programmable graphics hardware;reconstruction algorithm;monte carlo;3d reconstruction	3D iterative reconstruction of Single Photon Emission Computed Tomography (SPECT) acquisitions is a widely used but computationaly intensive process. Reconstructing data from a rotating slat collimated system increases the computation time even further. Since the sensitivity profile of such a camera is not constant over the field of view, it is important to include a system model that is as accurate as possible. We derived a Monte Carlo model of the physics of the measurement process. The advances in programmable graphics hardware make it possible to efficiently model the 3D reconstruction algorithm on consumer grade graphics cards in order to reduce the computation time. This paper describes an implementation of the reconstruction algorithm which is as accurate as the software algorithm while still achieving a 10-fold speed increase over the optimized software version.	3d reconstruction;algorithm;ct scan;computation;graphics hardware;iterative method;iterative reconstruction;monte carlo method;second level address translation;software versioning;time complexity;tomography;video card	Jan De Beenhouwer;Roel Van Holen;Stefaan Vandenberghe;Steven Staelens;Yves D'Asseler;Ignace Lemahieu	2006			3d reconstruction;iterative reconstruction;computer vision;systems modeling;field of view;computer science;graphics hardware;statistics;monte carlo method;computer graphics (images)	Graphics	45.05297651509044	-82.82949571615902	11400
03d0f25410e2a3efe314f631823382398da2cb66	localizing optic disc in retinal image automatically with entropy based algorithm		Examining retinal image continuously plays an important role in determining human eye health; with any variation present in this image, it may be resulting from some disease. Therefore, there is a need for computer-aided scanning for retinal image to perform this task automatically and accurately. The fundamental step in this task is identification of the retina elements; optical disk localization is the most important one in this identification. Different optical disc localization algorithms have been suggested, such as an algorithm that would be proposed in this paper. The assumption is based on the fact that optical disc area has rich information, so its entropy value is more significant in this area. The suggested algorithm has recursive steps for testing the entropy of different patches in image; sliding window technique is used to get these patches in a specific way. The results of practical work were obtained using different common data set, which achieved good accuracy in trivial computation time. Finally, this paper consists of four sections: a section for introduction containing the related works, a section for methodology and material, a section for practical work with results, and a section for conclusion.		Lamia AbedNoor Muhammed	2018		10.1155/2018/2815163	artificial intelligence;retina;computer vision;sliding window protocol;optic disc;optical disc;recursion;human eye;computation;algorithm;retinal;computer science	Vision	33.2029049766179	-76.93883588546092	11403
68ccca25654fc468e74b359e2a8fa7a84ec8fd64	mathematical simulation and analysis of cellular metabolism and regulation	software;modele mathematique;logiciel;computerized processing;tratamiento informatico;simulation;biologie cellulaire;simulacion;modelo matematico;regulation control;biologia celular;algorithme;algorithm;metabolismo;mathematical model;regulation;logicial;regulacion;traitement informatique;metabolism;metabolisme;cell biology;algoritmo	MOTIVATION A better understanding of the biological phenomena observed in cells requires the creation and analysis of mathematical models of cellular metabolism and physiology. The formulation and study of such models must also be simplified as far as possible to cope with the increasing complexity demanded and exponential accumulation of the metabolic reconstructions computed from sequenced genomes.   RESULTS A mathematical simulation workbench, DBsolve, has been developed to simplify the derivation and analysis of mathematical models. It combines: (i) derivation of large-scale mathematical models from metabolic reconstructions and other data sources; (ii) solving and parameter continuation of non-linear algebraic equations (NAEs), including metabolic control analysis; (iii) solving the non-linear stiff systems of ordinary differential equations (ODEs); (iv) bifurcation analysis of ODEs; (v) parameter fitting to experimental data or functional criteria based on constrained optimization. The workbench has been successfully used for dynamic metabolic modeling of some typical biochemical networks (Dolgacheva et al., Biochemistry (Moscow), 6, 1063-1068, 1996; Goldstein and Goryanin, Mol. Biol. (Moscow), 30, 976-983, 1996), including microbial glycolytic pathways, signal transduction pathways and receptor-ligand interactions.   AVAILABILITY DBsolve 5. 00 is freely available from http://websites.ntl.com/ approximately igor.goryanin.   CONTACT gzz78923@ggr.co.uk	algebraic equation;bifurcation theory;constrained optimization;continuation;derivation procedure;differential diagnosis;emoticon;interaction;ligands;mathematical model;mathematical optimization;mathematics;metabolic process, cellular;metabolic control analysis;navier–stokes equations;nonlinear system;population parameter;signal transduction pathways;simulation;time complexity;transduction (machine learning);tree accumulation;workbench;global genome nucleotide-excision repair;non-t, non-b childhood acute lymphoblastic leukemia;physiological aspects	Igor Goryanin;Charlie Hodgman;Evgeni Selkov	1999	Bioinformatics	10.1093/bioinformatics/15.9.749	biology;regulation;computer science;artificial intelligence;mathematical model;operations research;metabolism;algorithm	Comp.	-4.099881373422843	-54.37567272544387	11408
3e39aef0698d0e3e2f7f6590e87274d9327d0e7f	region-based iterative reconstruction of structurally changing objects in ct	biological patents;optimisation;biomedical journals;text mining;europe pubmed central;citation search;citation networks;iterative methods;region estimation computed tomography ct tomography iterative reconstruction;research articles;optimisation computerised tomography image reconstruction iterative methods medical image processing;abstracts;image reconstruction;medical image processing;image reconstruction phantoms computed tomography indexes vectors iterative methods optimization;open access;life sciences;computerised tomography;clinical guidelines;region based iterative reconstruction experimental μct data algebraic reconstruction method iterative optimization high quality image reconstruction time varying objects noninvasive imaging x ray computed tomography ct structurally changing object reconstruction;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	X-ray computed tomography (CT) is a powerful tool for noninvasive imaging of time-varying objects. In the past, methods have been proposed to reconstruct images from continuously changing objects. For discretely or structurally changing objects, however, such methods fail to reconstruct high quality images, mainly because assumptions about continuity are no longer valid. In this paper, we propose a method to reconstruct structurally changing objects. Starting from the observation that there exist regions within the scanned object that remain unchanged over time, we introduce an iterative optimization routine that can automatically determine these regions and incorporate this knowledge in an algebraic reconstruction method. The proposed algorithm was validated on simulation data and experimental μCT data, illustrating its capability to reconstruct structurally changing objects more accurately in comparison to current techniques.	algorithm;diagnostic radiologic examination;display resolution;existential quantification;iterative method;iterative reconstruction;linear algebra;mathematical optimization;physical object;scanning;scott continuity;simulation;x-ray computed tomography;x-ray microtomography	Geert Van Eyndhoven;Kees Joost Batenburg;Jan Sijbers	2014	IEEE Transactions on Image Processing	10.1109/TIP.2013.2297024	iterative reconstruction;computer vision;text mining;computer science;data mining;mathematics;iterative method	Vision	49.66016068483773	-80.80952854405155	11427
9a7389b5c76fb82d5f3ae00d4cc21baa6eb349c4	automatic summarisation and annotation of microarray data	microarray data;genomics;tmev;summarisation;software platform;normalisation;annotation;web service;standardisation;microarray data analysis;genomic dna;microarray;dna microarray;biological process	The study of biological processes within cells is based on the measurement of the activity of different molecules, in particular genes and proteins whose activities are strictly related. The activity of genes is measured through a systematic investigation carried out by microarrays. Such technology enables the investigation of all the genes of an organism in a single experiment, encoding meaningful biological information. Nevertheless, the preprocessing of raw microarray data needs automatic tools that standardise such phase in order to: (a) avoiding errors in analysis phases, and (b) making comparable the results of different laboratories. The preprocessing problem is as much relevant as considering results obtained from analysis platforms of different vendors. Nevertheless, there is currently a lack of tools that allow to manage and preprocess multivendor dataset. This paper presents a software platform (called GSAT, General-purpose Summarisation and Annotation Tool) able to manage and preprocess microarray data. The GSAT allows the summarisation, normalisation and annotation of multivendor microarray data, using web services technology. First experiments and results on Affymetrix data samples are also discussed. GSAT is available online at http://bioingegneria.unicz.it/m-csas a standalone application or as a plugin of the TMEV microarray data analysis platform.	automatic summarization;microarray	Pietro Hiram Guzzi;Maria Teresa Di Martino;Giuseppe Tradigo;Pierangelo Veltri;Pierfrancesco Tassone;Pierosandro Tagliaferri;Mario Cannataro	2011	Soft Comput.	10.1007/s00500-010-0600-4	microarray analysis techniques;genomics;gene chip analysis;computer science;bioinformatics;data mining;microarray databases;information retrieval	NLP	-1.0677291745216368	-58.053927704820325	11429
11a2b9cc5b441e7f5f4e040dace455756401de3b	connectivity asymmetry can explain visual hemispheric asymmetries in local/global, face, and spatial frequency processing		Left-right asymmetries have been noted in tasks requiring the classification of many visual stimuli, including Navon figures, spatial frequency gratings, and faces. The Double Filtering by Frequency (DFF) model (Ivry & Robertson, 1998), which postulates asymmetric frequency filtering on task-relevant frequency bands, has been implemented to account for asymmetric processing of each stimulus type above, but does not provide a fully mechanistic explanation, nor does it have direct neural correlates. The Differential Encoding (DE) model (Hsiao, Shahbazi, & Cottrell, 2008), which postulates that a known asymmetry in patch connectivity drives visual processing asymmetries, has previously been used to account for only one stimulus type. Here, we refine the DE model to match the published patch asymmetry more precisely and show that the DE model generalizes to three of the four datasets mentioned above. Examination of the failure to match all datasets suggest a possible reinterpretation of the original dataset itself.	computation;computational model;digital forensics framework (dff);frequency band;significant figures	Benjamin Cipollini;Janet Hui-wen Hsiao;Garrison W. Cottrell	2012			cognitive psychology;visual processing;social psychology;experimental psychology;visual field;stimulus (physiology);homunculus;visual perception;spatial frequency;frequency;mathematics	ML	16.4247458200613	-75.04707814701055	11452
21104bc2d5de2495bff2d95afad2ad37eac19ebe	regulation effects by programmed molecules for transcription-based diagnostic automata towards therapeutic use	experimental analysis;conformational change;malachite green;transcription regulation;restriction enzyme;therapeutic use;target recognition;dna computing;nucleic acid;read only memory	We have presented experimental analysis on the controlla- bility of our transcription-based diagnostic biomolecular automata by programmed molecules. Focusing on the noninvasive transcriptome di- agnosis by salivary mRNAs, we already proposed the novel concept of diagnostic device using DNA computation. This system consists of the main computational element which has a stem shaped promoter region and a pseudo-loop shaped read-only memory region for tran- scription regulation through the conformation change caused by the recognition of disease-related biomarkers. We utilize the transcription of malachite green aptamer sequence triggered by the target recognition for observation of detection. This algorithm makes it possible to release RNA-aptamer drugs multiply, different from the digestion-based systems by the restriction enzyme which was proposed previously, for the in-vivo use, however, the controllability of aptamer release is not enough at the previous stage. In this paper, we verified the regulation effect on aptamer transcription by programmed molecules in basic conditions towards the developm! ent of therapeutic automata. These results would bring us one step closer to the realization of new intelligent diagnostic and therapeu- tic automata based on molecular circuits.	automaton;medical transcription	Miki Hirabayashi;Hirotada Ohashi;Tai Kubo	2007		10.1007/978-4-431-88981-6_7	biology;molecular biology;bioinformatics;nanotechnology	Logic	11.314878289339592	-65.10296830916046	11463
54030d1f4e5a04ff7501d823d760995584e1b9eb	clustering and synchronous firing of coupled rulkov maps with stdp for modeling epilepsy	joints conferences neural networks;pattern clustering medical disorders neural nets neurophysiology;clustering rulkov maps synchronous firing stdp epilepsy neuropsychiatric disorder long term potentiation long term depression hippocampus synapses ltp ltd spike timing dependent plasticity synapses spiking activity bidirectionally coupled neurons bidirectional connection unidirectional connection unidirectional models in phase anti phase synchronization bidirectional models	Epilepsy of the neuropsychiatric disorder is provoked from an imbalance in the long-term potentiation (LTP) versus long-term depression (LTD) of the synapses in the hippocampus. The LTP and LTD are replicated by using the Spike Timing Dependent Plasticity (STDP). Additionally, the spiking activity of the synapses in the hippocampus can be approximated by using the Rulkov maps. In our previous study, we considered some easy simulation models which are constructed by using Rulkov maps with STDP. Moreover, these simulation models consist of unidirectionally coupled neurons. In this paper, we consider some easy simulation models with bidirectionally coupled neurons. We explore the effect of unidirectional and bidirectional connection on spiking activity, as basic simulation for constructing the approximate simulation model of epilepsy. From these results, the unidirectional models show high accuracy in-phase/anti-phase synchronization, and it shows divergent relatively early. The bidirectional models show the stable waveform (i.e., non-divergent) for a long term compared to unidirectional models.	approximation algorithm;dependent ml;linux test project (ltp);map;simulation;synchronization (computer science);the spike (1997)	Naohiro Shibuya;Charles Unsworth;Yoko Uwate;Yoshifumi Nishio	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889962	artificial intelligence;machine learning	ML	18.103492282402662	-71.59513735543325	11480
023c6e0be85c6474400d4bae2199fd8acff0c7b9	gfkl data mining competition 2005: predicting liquidity crises of companies	data mining	Data preprocessing and a careful selection of the training and classification method are key steps for building a predictive model with high performance. Here, we present the approach used for training and classification in our solutions submitted to the 2005 GfKl Data Mining Competition. The initial step of data preprocessing is described in the first part of this work. The task to be solved for the competition was the prediction (binary classification) of a possible liquidity crisis of a company. The prediction was to be based on a set of 26 variables describing attributes of the companies with unknown semantics. The size of the training data set was 20,000 samples, with a class distribution of 10% positive cases, i.e. where an liquidity crisis occurred, and 90% negative cases. The test data consisted of 10,000 unlabeled samples. The models entering the competition were ranked according to the number of true positive cases within the 2,000 samples regarded most likely to enter a liquidity crisis. From our experience with data mining tasks, we know that it is crucial to avoid overfitting. Furthermore we observed that — given a suitable feature transformation — the actual choice of the classifier has less impact and depends on the task. First, we separated 20% of the training data into a hold-out set to be used for validation of the internal results. Then, a variety of classifiers were examined on the remaining 80% of the training data using five-fold cross-validation. We employed a variety of standard off-the-shelf classifiers as available e.g. in Netlab and Weka (neural networks, nearest-neighbor techniques, decision trees, support vector machines) as well as some in-house classifiers for maximum entropy training and naive Bayes estimation. For each of the classifiers we assessed suitable parameter choices and followed those approaches that gave the best results on the cross-validation data. Finally, we chose a small set of well-performing classifiers, evaluated these on the hold-out set and submitted the predictions of the classifiers with the best performance, now trained on the complete training data. In addition, we also included a combination of three of the classifiers, because often classifier combination can reduce the likeliness of overfitting. The final result showed that the submitted classifier combination (naive Bayes & maximum entropy, alternating decision tree, and logistic regression) and the logistic model tree were both able to correctly detect 894 true positive companies. Only the winning approach that detected 896 companies was better than these results, and the difference is statistically not significant.	alternating decision tree;artificial neural network;binary classification;cross-validation (statistics);cuban missile crisis: the aftermath;data mining;data pre-processing;logistic model tree;logistic regression;naive bayes classifier;overfitting;predictive modelling;preprocessor;support vector machine;test data;test set;weka	Jens Strackeljan;Roland Jonscher;Sigurd Prieur;David Vogel;Thomas Deselaers;Daniel Keysers;Arne Mauser;Ilja Bezrukov;Andre Hegerath	2005		10.1007/3-540-31314-1_92	actuarial science;data mining;commerce	ML	0.9950391089626994	-72.27869656458596	11486
b693c95ded123a1f974e81727622ac62032272c4	semantics-biased rapid retrieval for video databases	database indexing;video databases;video database retrieval;pattern clustering;cluster;information retrieval indexing content based retrieval indexes nearest neighbor searches multimedia databases bayesian methods computers spatial databases visual databases;video retrieval content based retrieval database indexing gaussian processes pattern clustering query formulation video databases;high dimensionality;gaussian processes;query formulation;semantics;video retrieval;phase retrieval;index;sequential search;feature vector;nearest neighbor search semantics biased retrieval video database retrieval high dimensional index content based video retrieval gaussian mixture model semantics supervised clustering;gaussian mixture model;indexation;semantics biased retrieval;visual features;cluster semantics video retrieval index;approximate nearest neighbor;nearest neighbor search;content based video retrieval;high dimensional index;video database;content based retrieval;semantics supervised clustering	High-dimensional index is one of the most challenging tasks for content-based video retrieval. Typically, in video database, there exist two kinds of clues for query: visual features and semantic classes. In this paper, we modeled the relationship between semantic classes and visual feature distributions of data set with the Gaussian mixture model, and proposed a semantics supervised cluster based index (briefly as SSCI) approach to integrate the advantages of both semantic classes and visual features. The entire data set is divided hierarchically by a modified clustering technique into many clusters until the objects within a cluster are not only close in the visual feature space but also within the same semantic class, and then an index entry including semantic clue and visual feature clue is built for each cluster. Especially, the visual feature vectors in a cluster are organized adjacently in disk. So the SSCI-based nearest-neighbor search can be divided into two phases: the first phase computes the distances between the query example and each cluster index and returns the clusters with the smallest distance, here namely candidate clusters; then the second phase retrieves the original feature vectors within the candidate clusters to gain the approximate nearest neighbors. Our experiments showed that for approximate searching the SSCI-based approach was faster than VA+-based approach; moreover, the quality of the result set was better than that of the sequential search in terms of semantics	approximation algorithm;cluster analysis;database;existential quantification;experiment;feature vector;linear search;mixture model;nearest neighbor search;result set;social sciences citation index	Zhiping Shi;Qingyong Li;Zhiwei Shi;Zhongzhi Shi	2006	2006 5th IEEE International Conference on Cognitive Informatics	10.1109/COGINF.2006.365559	computer science;pattern recognition;data mining;information retrieval	DB	38.01762127043377	-57.6440914855282	11498
696fe8f88496edcc38504e02bc236bac11a360cd	surgical duration estimation via data mining and predictive modeling: a case study	classification;hybrid method;prediction;regression;surgery times	Operating rooms (ORs) are one of the most expensive and profitable resources within a hospital system. OR managers strive to utilize these resources in the best possible manner. Traditionally, surgery durations are estimated using a moving average adjusted by the scheduler (adjusted system prediction or ASP). Other methods based on distributions, regression and data mining have also been proposed. To overcome difficulties with numerous procedure types and lack of sufficient sample size, and avoid distributional assumptions, the main objective is to develop a hybrid method of duration prediction and demonstrate using a case study.	aspirate substance;data mining;operating room;predictive modelling;scheduling (computing)	Narges Hosseini;Mustafa Y. Sir;Christopher Jankowski;Kalyan S. Pasupathy	2015	AMIA ... Annual Symposium proceedings. AMIA Symposium		econometrics;engineering;operations management;data mining	Embedded	6.046227318176004	-74.50200784746653	11502
3c94c8d722c85653f2adca7918160b87f458eb75	sleep activity recognition using binary motion sensors		Early detection of frailty signs is important for the senior population that prefers to keep living in their homes instead of moving to a nursing home. Sleep quality is a good predictor for frailty monitoring. Thus we are interested in tracking sleep parameters like sleep wake patterns to predict and detect potential sleep disturbances of the monitored senior residents. We use an unsupervised inference method based on actigraphy data generated by ambient motion sensors scattered around the senior's apartment. This enables our monitoring solution to be flexible and robust to the different types of housings it can equip while still attaining accuracy of 0.94 for sleep period estimates.		Yassine El-Khadiri;Gabriel Corona;C. Rose;François Charpillet	2018	2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2018.00049	artificial intelligence;machine learning;actigraphy;activity recognition;binary number;computer science;inference;population	Robotics	4.251609615394964	-84.72393792300674	11505
36a6c0b5c8dfe0fa2e81fd5c0beadeec95c68052	seqanalref: a sequence analysis bibliographic reference databank	serveur institutionnel;archive institutionnelle;open access;sequence analysis;archive ouverte unige;cybertheses;institutional repository			Amos Bairoch	1991	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/7.2.268	biology;bioinformatics;sequence analysis;data mining;database;world wide web	Theory	-2.4559198318751254	-61.07286045317294	11523
ac5d98bc47f5c3e0c79cfeb1692138b4da82978e	measuring bone density connectivity using dual energy x-ray absorptiometry images			dual-energy x-ray absorptiometry	Li Chen;V. B. Surya Prasath	2017	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2017.17.4.235	dual-energy x-ray absorptiometry;analytical chemistry;bone density;mathematics	Robotics	46.24579941017997	-86.35993857756975	11553
ca062a302efcedb15b680e2a79381002350ea838	quantitative evaluation of color image segmentation algorithms		The present paper addresses the nowadays field of image segmentation by offering an evaluation of several existing approaches. The paper offers a comparison of the experimental results from the error measurement point of view. We introduce a new method of salient object recognition that takes into consideration color and geometric features in order to offer a conclusive result. Our new segmentation method introduced in the paper has revealed very good results in terms of comparison with the already known object detection methods. We use a set of error measures to analyze the consistency of different segmentations provided by several well known algorithms. The experimental results offer a complete basis for parallel analysis with respect to the precision of our algorithm, rather than the individual efficiency.	algorithm;color image;evaluation function;futures studies;google compute engine;image processing;image segmentation;mean shift;object detection;outline of object recognition;visual objects;xfig	Andreea Iancu;Bogdan Popescu;Marius Brezovan;Eugen Ganea	2011	IJCSA		color image;algorithm;computer science;object detection;segmentation-based object categorization;image texture;scale-space segmentation;image segmentation;region growing;segmentation	Vision	41.740609809027944	-68.30496906388758	11554
05616a2ef60e9f2bfe01d36138c2df9604a78d25	multi-dimensional interval algebra with symmetry for describing block layouts	relative position;intervalo;interval algebra;document analysis;image processing;mesa;block;multidimensional system;structure formation;procesamiento imagen;intervalle;traitement image;algebre;multi dimensional;analyse documentaire;interval;spatial relation;algebra;table;higher dimensions;sistema n dimensiones;analisis documental;systeme n dimensions;bloque;bloc	Describing the relative positions of Rectangular boxes on a page is a fundamental task in document layout processing. Typically, this is achieved by comparing quantitative values of the endpoints of the rectangle. Such a representation expresses a property that is basic for the as a conjunction of relations for the point. In this work, we adopt a qualitative interval projectionmodel to describe the relative positions of such blocks using interval algebra, which defines the spatial relation of two points only in terms of precedence, coincidence and post-occurrence. Such relations have not been found very meaningful in document or other media layout con texts since they cannot capture symmetry.rnrnIn this work, we propose an extension of interval algebra by defining secondary operators (e.g. centered) which are expressed in terms of basic interval algebra operators. By extending the ordering of intervals to higher dimensions, Multidimensional Interval Algebra can capture the notion of tangency and alignment between blocks while retaining the relative size information. We present several examples from the document domain to show that this information is sufficient to identify the layout of block structured formats. While this representation does not provide any immediate benefit to document analysis perse - the fact that it provides a compact yet complete vocabulary enables its use in abstraction tasks such as learning the grammar of a document sets by studying a series of examples.	allen's interval algebra	Ankur Lahoti;Rohit Singh;Amitabha Mukerjee	1999		10.1007/3-540-40953-X_12	spatial relation;interval;computer vision;combinatorics;discrete mathematics;multidimensional systems;image processing;structure formation;computer science;artificial intelligence;operating system;table;machine learning;database;mathematics;distributed computing;algorithm;block;statistics	Logic	48.172167069059604	-61.0629221928249	11559
e2c9d6668500800f38dded30ad6ecea78e8c0692	processing tracking in jmrui software for magnetic resonance spectra quantitation reproducibility assurance	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	Proton magnetic resonance spectroscopy is a non-invasive measurement technique which provides information about concentrations of up to 20 metabolites participating in intracellular biochemical processes. In order to obtain any metabolic information from measured spectra a processing should be done in specialized software, like jMRUI. The processing is interactive and complex and often requires many trials before obtaining a correct result. This paper proposes a jMRUI enhancement for efficient and unambiguous history tracking and file identification. A database storing all processing steps, parameters and files used in processing was developed for jMRUI. The solution was developed in Java, authors used a SQL database for robust storage of parameters and SHA-256 hash code for unambiguous file identification. The developed system was integrated directly in jMRUI and it will be publically available. A graphical user interface was implemented in order to make the user experience more comfortable. The database operation is invisible from the point of view of the common user, all tracking operations are performed in the background. The implemented jMRUI database is a tool that can significantly help the user to track the processing history performed on data in jMRUI. The created tool is oriented to be user-friendly, robust and easy to use. The database GUI allows the user to browse the whole processing history of a selected file and learn e.g. what processing lead to the results, where the original data are stored, to obtain the list of all processing actions performed on spectra.	biochemical processes;browsing;graphical user interface;hl7publishingsubsection <operations>;hash function;java programming language;magnetic resonance spectroscopy;marijuana abuse;metabolic process, cellular;point of view (computer hardware company);protons;quantitation;sha-2;sql;usability;user interface device component;user experience	Michal Jablonski;Jana Starcukova;Zenon Starcuk	2017		10.1186/s12859-017-1459-5	software;signal processing;bioinformatics;magnetic resonance imaging;computer science	DB	-3.7486319626997693	-58.87204213484976	11583
2be1c0af160e76f181d136e9881322edde636198	intrinsic image decomposition using color invariant edge	intrinsic image decomposition;image classification intrinsic image decomposition color invariant edge reflectance image shading image computer vision;color invariant edge;reflectivity;edge detection;color;image decomposition reflectivity lighting pixel computer vision image restoration computer graphics chaos laboratories computer science education;shading image;image classification;image restoration;reflectivity computer vision edge detection image classification image colour analysis image restoration;computer vision;smoothing methods;image edge detection;image color analysis;image colour analysis;pixel;lighting;image decomposition;reflectance image	The intrinsic image composed of reflectance and shading images plays important roles in various computer vision applications. This paper focuses on solving the problem of intrinsic image decomposition. Based on the assumption that the image derivatives can be classified into either reflectance-related or shading-related, the reflectance and shading image can be restored from the classified derivatives. We improve the classification result using only color information by introducing the color invariant edge. Considering some color invariant properties in the image, the color invariant edge can provide more useful information in guiding the classification and producing more robust decomposition result as it is shown in the experiment.	color;computer vision;image derivatives;shading	Boxin Shi;Yangxi Li;Chao Xu	2009	2009 Fifth International Conference on Image and Graphics	10.1109/ICIG.2009.21	color histogram;image restoration;computer vision;contextual image classification;edge detection;image gradient;computer science;lighting;mathematics;reflectivity;color balance;pixel;computer graphics (images)	Vision	53.17094417259995	-62.97284227757763	11589
1ff36a5b1158d4a3fe922904ab5d5cbcc99e883f	shape classification based on skeleton-branch distances		In recent decades, the need for efficient and effective image search from large databases has increased. In this paper, we present a novel shape matching framework based on structures that are likely to exist in similar shapes. After representing shapes as medial axis graphs, where vertices show skeletons and edges connect nearby skeletons, we determine the branches connecting or representing shape’s different parts. Using the shortest path distance from each vertex (skeleton) to each of the branches, we effectively retrieve similar shapes to the given query through a transportation-based distance function. A set of shape retrieval experiments including the comparison with two previous approaches demonstrate the proposed algorithm’s effectiveness and perturbation experiments present its robustness.	algorithm;apache axis;database;experiment;image retrieval;medial graph;shortest path problem;statistical classification;vertex (geometry)	Salih Arda Boluk;M. Fatih Demirci	2015		10.5220/0005300503530359	computer vision;artificial intelligence;computer science;skeleton (computer programming);pattern recognition	Vision	40.35090600804983	-57.938197809914456	11595
855dc16964e37b6c6ad4c2deb9e1f059590ebb29	detecting shape similarities in 3d pottery repositories	databases;3d digitisation;compact shape descriptors;search engine;shape similarity detection;3d computer graphics;axial symmetry feature;archaeologists;search engines;image matching;3d virtual reality environment shape similarity detection 3d pottery repository shape matching archaeologists 3d computer graphics 3d digitisation content based retrieval compact shape descriptors axial symmetry feature 3d pottery ground truth repository experimental 3d pottery search engine;3d pottery ground truth repository;virtual reality;search engine pottery content based retrieval shape matching pottery mpeg 7 3d vessel modelling 3d repositories;archaeology;experimental 3d pottery search engine;computational modeling;3d pottery repository;shape;three dimensional displays;shape matching;solid modeling;3d vessel modelling;pottery;virtual reality archaeology content based retrieval image matching search engines;3d virtual reality environment;content based retrieval;mpeg 7;3d repositories;three dimensional displays shape search engines solid modeling computational modeling cultural differences databases;cultural differences	Shape matching is one of the main research procedures performed by archaeologists. Real time 3D computer graphics, 3D digitisation and content based retrieval are technologies that can facilitate the automation of the shape matching process. As pottery plays a significant role in understanding ancient societies, we focus on the development of compact shape descriptors that can be used for content based retrieval of complete or nearly complete 3D pottery replicas. In this work, we present shape descriptors that exploit the axial symmetry feature and attempt to enhance the archaeological study of pottery by using 3D graphics technologies. We evaluated the performance of the descriptors using a 3D pottery ground truth repository as a test bed. We created an experimental 3D pottery search engine and attempted to integrate a content based retrieval mechanism in a 3D virtual reality environment.	3d computer graphics;ground truth;sensor;shape context;testbed;virtual reality;web search engine	Anestis Koutsoudis;George Pavlidis;Christodoulos Chamzas	2010	2010 IEEE Fourth International Conference on Semantic Computing	10.1109/ICSC.2010.50	computer vision;computer science;virtual reality;search engine;3d computer graphics	Visualization	40.577080049288554	-59.31537814376985	11604
6aae0fc8a7e0dabe5659bedd01003683010a74dc	fast volume reconstruction from motion corrupted stacks of 2d slices	ultrasonic imaging biomedical mri biomedical ultrasonics graphics processing units image reconstruction liver medical image processing obstetrics optical transfer function;image based reconstruction methods fast volume reconstruction motion corrupted stacks 2d slices fast individual image slice acquisition motion artefacts slice volume reconstruction methods svr methods high quality 3d image data fast multigpu accelerated framework 2d 3d registration superresolution automatic outlier rejection intensity bias correction artificial motion corrupted phantom data tracked freehand ultrasound liver fetal magnetic resonance imaging single cpu system state of the art multicore cpu methods high reconstruction accuracy point spread function input data point speed up factor;journal article;image reconstruction three dimensional displays magnetic resonance imaging approximation methods spatial resolution;three dimensional displays;gpu acceleration motion correction magnetic resonance imaging freehand compound ultrasound fetal imaging;image reconstruction;magnetic resonance imaging;approximation methods;spatial resolution	Capturing an enclosing volume of moving subjects and organs using fast individual image slice acquisition has shown promise in dealing with motion artefacts. Motion between slice acquisitions results in spatial inconsistencies that can be resolved by slice-to-volume reconstruction (SVR) methods to provide high quality 3D image data. Existing algorithms are, however, typically very slow, specialised to specific applications and rely on approximations, which impedes their potential clinical use. In this paper, we present a fast multi-GPU accelerated framework for slice-to-volume reconstruction. It is based on optimised 2D/3D registration, super-resolution with automatic outlier rejection and an additional (optional) intensity bias correction. We introduce a novel and fully automatic procedure for selecting the image stack with least motion to serve as an initial registration target. We evaluate the proposed method using artificial motion corrupted phantom data as well as clinical data, including tracked freehand ultrasound of the liver and fetal Magnetic Resonance Imaging. We achieve speed-up factors greater than 30 compared to a single CPU system and greater than 10 compared to currently available state-of-the-art multi-core CPU methods. We ensure high reconstruction accuracy by exact computation of the point-spread function for every input data point, which has not previously been possible due to computational limitations. Our framework and its implementation is scalable for available computational infrastructures and tests show a speed-up factor of 1.70 for each additional GPU. This paves the way for the online application of image based reconstruction methods during clinical examinations. The source code for the proposed approach is publicly available.	adobe freehand;algorithm;c++;cpu (central processing unit of computer system);cuda;central processing unit;clinical use template;computation;data point;deny (action);deploy;display resolution;fastest;feedback;graphics processing unit;image resolution;imaging phantom;input/output atomic (input);interpolation;low-rank approximation;memory footprint;morphologic artifacts;multi-core processor;numerous;organ;phantoms, imaging;rejection sampling;resonance;simd;sampling - surgical action;scalability;scanner device component;software deployment;source code;super-resolution imaging;tv tuner card;total peripheral resistance;tuner device component;web application;web framework;registration - actclass	Bernhard Kainz;Markus Steinberger;Wolfgang Wein;Maria Murgasova;Christina Malamateniou;Kevin Keraudren;Thomas Torsney-Weir;Mary A. Rutherford;Paul Aljabar;Joseph V. Hajnal;Daniel Rueckert	2015	IEEE Transactions on Medical Imaging	10.1109/TMI.2015.2415453	iterative reconstruction;computer vision;simulation;radiology;image resolution;medicine;computer science;magnetic resonance imaging;computer graphics (images)	Vision	45.118017726099154	-82.70230009810503	11622
88a495d9c832d7d539dd122fb0be59875d30886e	ym500v3: a database for small rna sequencing in human cancer research		We previously presented the YM500 database, which contains >8000 small RNA sequencing (smRNA-seq) data sets and integrated analysis results for various cancer miRNome studies. In the updated YM500v3 database (http://ngs.ym.edu.tw/ym500/) presented herein, we not only focus on miRNAs but also on other functional small non-coding RNAs (sncRNAs), such as PIWI-interacting RNAs (piRNAs), tRNA-derived fragments (tRFs), small nuclear RNAs (snRNAs) and small nucleolar RNAs (snoRNAs). There is growing knowledge of the role of sncRNAs in gene regulation and tumorigenesis. We have also incorporated >10 000 cancer-related RNA-seq and >3000 more smRNA-seq data sets into the YM500v3 database. Furthermore, there are two main new sections, 'Survival' and 'Cancer', in this updated version. The 'Survival' section provides the survival analysis results in all cancer types or in a user-defined group of samples for a specific sncRNA. The 'Cancer' section provides the results of differential expression analyses, miRNA-gene interactions and cancer miRNA-related pathways. In the 'Expression' section, sncRNA expression profiles across cancer and sample types are newly provided. Cancer-related sncRNAs hold potential for both biotech applications and basic research.	base sequence;carcinogenesis;database;gene expression programming;interaction;micrornas;neoplasms;non-small cell lung carcinoma;piwi-interacting rna;rna, small untranslated;sequence number;tyrosine-trna ligase	I-Fang Chung;Shing-Jyh Chang;Chen-Yang Chen;Shu-Hsuan Liu;Chia-Yang Li;Chia-Hao Chan;Chuan-Chi Shih;Wei-Chung Cheng	2017		10.1093/nar/gkw1084	biology;molecular biology;bioinformatics;genetics	Comp.	0.5355314140161623	-59.55740980253265	11646
78a9f76573785a51118b41f8ee562fd135187ee4	the value of 99mtc-methylene diphosphonate single photon emission computed tomography/computed tomography in diagnosis of fibrous dysplasia	99mtc-mdp;computed tomography;fibrous dysplasia;single photon emission computed tomography	BACKGROUND Fibrous dysplasia (FD) is a rare benign bone disorder in which the normal bone is replaced by immature fibro-osseous tissue. However, some case reports have reported that FD showed significantly increased 99mTc-methylene diphosphonate (99mTc-MDP) uptake on whole-body bone scintigraphy (WBS), which may mimic bone metastasis or skeletal involvement of the patients with known cancer. Thus, the purpose of present study is to observe the reliable characteristics and usefulness of single photon emission computed tomography/computed tomography (SPECT/CT) for the diagnosis of FD.   METHODS This was a retrospective review of 21 patients with FD (14 males and 7 females, mean age 51.2 ± 12.5 years) who were referred to have WBS to determine whether there was any osseous metastasis. WBS and SPECT/CT images were independently interpreted by two experienced nuclear medicine physician together with a diagnostic radiologist. In cases of discrepancy, consensus was obtained by a joint reading. The final diagnosis was based on biopsy proof and radiologic follow-up over at least 1 year.   RESULTS The lesions of FD were most frequently found in craniofacial region (15/21). Eighteen of the 21 (85.7%) cases showed moderate and high metabolism on WBS (compared to sternum). On CT imaging, GGO and expansion were the most common finding, were noted in 90.5% and 85.7% of the patients. Lytic lesions were present in 61.9% of the patients, and sclerosis was present in 38.1% of the patients. Cortical disruption was not seen in any patient.   CONCLUSIONS FD has certain characteristic appearance on SPECT/CT. It should be enrolled in the differential diagnoses when lesions show elevated 99mTc-MDP uptake on WBS. For SPECT/CT, the CT features of GGO and expansion in the areas of abnormal radiotracer uptake are helpful for the diagnosis of FD.	abnormal red blood cell;bone diseases;bone tissue;bone scintigraphy;ct scan;denial-of-service attack;differential diagnosis;diphosphonates;discrepancy function;fibrous dysplasia;follow-up report;medical imaging;neoplasms;patients;photons;primary melanocytic lesion of meninges;radiology;radionuclide imaging;region-based memory management;sternum;technetium tc 99m medronate;tomography, emission-computed;tomography, emission-computed, single-photon;whole-body irradiation;work breakdown structure;x-ray computed tomography;bone metastases;carbene;craniofacial;radiotracer	Linqi Zhang;Qiao He;Wei Li;Rusen Zhang	2017		10.1186/s12880-017-0218-4	pathology;cancer;computed tomography;radiology;bone metastasis;bone scintigraphy;fibrous dysplasia;99mtc-methylene diphosphonate;single-photon emission computed tomography;medicine	HCI	35.3071768970238	-83.65813466728406	11654
431437b4b94599fa7d39783203411d1c10f334be	uncertain fuzzy reasoning: a case study in modelling expert decision making	expert medical decision making;fuzzy reasoning;membership function variability uncertain fuzzy reasoning expert medical decision making type 1 fuzzy expert system infants monte carlo simulations;membership function variability;fuzzy reasoning decision making fuzzy systems hybrid intelligent systems pediatrics uncertainty context modeling biochemical analysis blood white noise;medical decision making;medical expert systems;fuzzy expert system;paediatrics;umbilical acid base assessment interval type 2 fuzzy expert systems medical decision making nonstationary fuzzy reasoning nonstationary type 1 fuzzy expert systems;umbilical acid base assessment;infants;interval type 2 fuzzy expert systems;nonstationary type 1 fuzzy expert systems;uncertain fuzzy reasoning;nonstationary fuzzy reasoning;monte carlo simulations;fuzzy systems;monte carlo methods;paediatrics decision making fuzzy reasoning fuzzy systems medical expert systems monte carlo methods;type 1 fuzzy expert system	"""This paper presents a case study in which the introduction of vagueness or uncertainty into the membership functions of a fuzzy system was investigated in order to model the variation exhibited by experts in a medical decision-making context. A conventional (type-1) fuzzy expert system had previously been developed to assess the health of infants immediately after birth by analysis of the biochemical status of blood taken from infants' umbilical cords. Variation in decision making was introduced into the fuzzy expert system by means of membership functions which altered in small, predetermined manners over time. Three types of variation in membership functions were investigated: i) variation in the centre points, ii) variation in the widths, and iii) the addition of """"white noise"""". Different levels (amounts) of uniformly distributed random variation were investigated for each of these types. Monte Carlo simulations were carried out to propagate the variation through the inferencing process in order to determine distributions of the conclusions reached. Interval valued type-2 fuzzy systems were also implemented to investigate the boundaries of variability in decisions. The results obtained were compared to the experts' decisions in order to determine which type and size of membership function variability best matched the experts' variability. The novel reasoning technique introduced in this study is termed nonstationary fuzzy reasoning"""	expert system;fuzzy concept;fuzzy control system;fuzzy logic;heart rate variability;membership function (mathematics);monte carlo method;simulation;vagueness;white noise	Jonathan M. Garibaldi;Turhan Ozen	2007	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2006.889755	membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy control system;statistics;monte carlo method	SE	5.9213876731382085	-80.18047006303588	11670
0d45b4065308b6a267fcb10583c7e68284c4fa4d	an empirical investigation of the scalability of a multiple viewpoint cbir system	busqueda informacion;multiple representation;estensibilidad;analisis contenido;sistema multiple;raisonnement base sur cas;razonamiento fundado sobre caso;espace image;image processing;information retrieval;interrogation base donnee;procesamiento imagen;image multiple;interrogacion base datos;multiple system;imagen multiple;feature space;traitement image;multiple image;content analysis;image space;recherche information;espacio imagen;extensibilite;scalability;analyse contenu;case based reasoning;content based image retrieval;content based retrieval;database query;recherche par contenu;systeme multiple	Our work in content-based image retrieval (CBIR) relies on content-analysis of multiple representations of an image which we term multiple viewpoints or channels. The conceptual idea is to place each image in multiple feature spaces and then perform retrieval by querying each of these spaces and merging the several responses. We have shown that a simple realization of this strategy can be used to boost the retrieval effectiveness of conventional CBIR. In this work we evaluate our framework in a larger, more demanding test environment and find that while absolute retrieval effectiveness is reduced, substantial relative improvement can be consistently attained.	content-based image retrieval;deployment environment;scalability;viewpoint	James C. French;Xiangyu Jin;Worthy N. Martin	2004		10.1007/978-3-540-27814-6_32	case-based reasoning;computer vision;scalability;feature vector;content analysis;image processing;computer science;artificial intelligence;mathematics;information retrieval;algorithm	Web+IR	42.66645787478683	-60.82050117388071	11671
02b43f8f029b66fddf87d74fd7cd31f53bbce0c6	learning hierarchical spatio-temporal pattern for human activity prediction	variable order markov model;hebbian learning;3d trajectory segmentation;3d action feature representation;self organizing map;probabilistic suffix tree;skeleton joints;rgb d dataset	Human activity prediction has become increasingly valuable in many applications. This paper, initially from the perspective of cognition science, presents a novel approach to learning a hierarchical spatiotemporal pattern of human activities to predict ongoing activities from videos that contain only the onsets of the activities. Spatio-temporal pattern can be learned by a Hierarchical Self-Organizing Map (HSOM), which consists of two self-organizing maps (i.e., action map and actionlet map) connected via associative links trained by Hebbian learning. Ongoing activities can be predicted by Variable order Markov Model (VMM), which provides the means for capturing both large and small order Markov dependencies based on the training actionlet sequences. Experiments of the proposed method on four challenging 3D action datasets captured by commodity depth cameras show promising results. 2015 Elsevier Inc. All rights reserved.	activity recognition;cognition;hebbian theory;interaction;markov chain;organizing (structure);program structure tree;self-organization;self-organizing map;spatiotemporal pattern;variable-order markov model;virtual machine manager	Wenwen Ding;Kai Liu;Fei Cheng;Jin Zhang	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2015.12.006	computer vision;self-organizing map;hebbian theory;computer science;artificial intelligence;machine learning;pattern recognition;variable-order markov model	AI	20.885391628259082	-63.58055886111937	11695
8db6510261a6edb3688c3caff5b882ed1a354f0b	lipidgo: database for lipid-related go terms and applications		MOTIVATION Lipid, an essential class of biomolecules, is receiving increasing attention in the research community, especially with the development of analytical technique like mass spectrometry. Gene Ontology (GO) is the de facto standard function annotation scheme for gene products. Identification of both explicit and implicit lipid-related GO terms will help lipid research in many ways, e.g. assigning lipid function in protein function prediction.   RESULTS We have constructed a Web site 'LipidGO' that facilitates browsing and searching lipid-related GO terms. An expandable hierarchical GO tree is constructed that allows users to find lipid-related GO terms easily. To support large-scale analysis, a user is able to upload a list of gene products or a list of GO terms to find out which of them is lipid related. Finally, we demonstrate the usefulness of 'LipidGO' by two applications: (i) identifying lipid-related gene products in model organisms and (ii) discovering potential novel lipid-related molecular functions   AVAILABILITY AND IMPLEMENTATION LipidGO is available at http://compbio.ddns.comp.nus.edu.sg/%7elipidgo/index.php.	annotation;gene ontology;protein function prediction;type iii site-specific deoxyribonuclease;upload	Mengyuan Fan;Hong Sang Low;Hufeng Zhou;Markus R. Wenk;Limsoon Wong	2014	Bioinformatics	10.1093/bioinformatics/btt689	computer science;bioinformatics;data mining;world wide web	Comp.	-1.3411553278755288	-59.33354427722986	11710
7c710bb22857cc34d75998214e0d3b4b8d8cd029	human olfactory lateralization requires trigeminal activation	olfaction;attention;localization;myelinated;lateralization;fmri	"""Rats are able to lateralize odors. This ability involves specialized neurons in the orbitofrontal cortex which are able to process the left, right and bilateral presentation of stimuli. However, it is not clear whether this function is preserved in humans. Humans are in general not able to differentiate whether a selective olfactory stimulant has been applied to the left or right nostril; however exceptions have been reported. Following a screening of 152 individuals with an olfactory lateralization test, we identified 19 who could lateralize odors above chance level. 15 of these """"lateralizers"""" underwent olfactory fMRI scanning in a block design and were compared to 15 controls matched for age and sex distribution. As a result, both groups showed comparable activation of olfactory eloquent brain areas. However, subjects with lateralization ability had a significantly enhanced activation of cerebral trigeminal processing areas (somatosensory cortex, intraparietal sulcus). In contrast to controls, lateralizers furthermore exhibited no suppression in the area of the trigeminal principal sensory nucleus. An exploratory study with an olfactory change detection paradigm furthermore showed that lateralizers oriented faster towards changes in the olfactory environment. Taken together, our study suggests that the trigeminal system is activated to a higher degree by the odorous stimuli in the group of """"lateralizers"""". We conclude that humans are not able to lateralize odors based on the olfactory input alone, but vary in the degree to which the trigeminal system is recruited."""	anterior nares;bilateral filter;cell nucleus;cerebral cortex;eloquent brain areas;groove;humans;odors;programming paradigm;sex factors;smell perception;structure of central sulcus;zero suppression;fmri;recurrent childhood brain stem glioma	Ilona Croy;Max Schulz;Anna Blumrich;Cornelia Hummel;Johannes C. Gerber;Thomas Hummel	2014	NeuroImage	10.1016/j.neuroimage.2014.05.004	psychology;neuroscience;olfactory memory;communication;anatomy	ML	17.832327632149912	-77.75261357539624	11716
575bd22c35f28dfe027d2974ee7490a8bc2207bf	a computational account of the role of cochlear nucleus and inferior colliculus in stabilizing auditory nerve firing for auditory category learning		It is well known that auditory nerve (AN) fibers overcome bandwidth limitations through the volley principle, a form of multiplexing. What is less well known is that the volley principle introduces a degree of unpredictability into AN neural firing patterns that may be affecting even simple stimulus categorization learning. We use a physiologically grounded, unsupervised spiking neural network model of the auditory brain with spike time dependent plasticity learning to demonstrate that plastic auditory cortex is unable to learn even simple auditory object categories when exposed to the raw AN firing input without subcortical preprocessing. We then demonstrate the importance of nonplastic subcortical preprocessing within the cochlear nucleus and the inferior colliculus for stabilizing and denoising AN responses. Such preprocessing enables the plastic auditory cortex to learn efficient robust representations of the auditory object categories. The biological realism of our model makes it suitable for generating neurophysiologically testable hypotheses.	abducens nerve diseases;action potential;artificial neural network;auditory area;auditory processing disorder;categories;categorization;cell nucleus;cerebral cortex;cochlear nerve;cochlear implant;cochlear nucleus structure;cochlear structure;computation;concept learning;hearing problem;inferior colliculus;multiplexing;network model;noise reduction;preprocessor;spiking neural network;tissue fiber;volley theory	Irina Higgins;Simon M. Stringer;Jan W. H. Schnupp	2018	Neural Computation	10.1162/neco_a_01085	mathematics;inferior colliculus;artificial intelligence;machine learning;auditory cortex;categorization;stimulus (physiology);cochlear nucleus;spiking neural network;volley theory;concept learning	ML	18.237542654586722	-72.84903448066734	11718
d2f06bfe5e799a797bad3e7bf162d6f0a6c31640	a computer-aided decision support system for shoulder pain pathology	shoulder pain;decision support;musculoskeletal system;musculoskeletal disorder;support system;decision support system;primary health care;general population;machine learning;covariance estimation;clinical assessment	  A musculoskeletal disorder is a condition of the musculoskeletal system which consists in that part of it is injured continuously  over time. Shoulder disorders are one of the most common musculoskeletal cases attended in primary health care services. Shoulder  disorders cause pain and limit the ability to perform many routine activities and affect about 15-25 % of the general population.  Several clinical tests have been described to aid diagnosis of shoulder disorders. However, the current literature acknowledges  a lack of concordance in clinical assessment, even among musculoskeletal specialists. In this work a Computer-Aided Decision  Support (CADS) system for Shoulder Pain Pathology has been developed. The paper presents the medical method and the development  of the database and the (CADS) system based on several classical classification paradigms improve by covariance estimation  methods. Finally the system was evaluated by a medical specialist.    	decision support system	Karmele López de Ipiña;Carmen Hernández;Manuel Graña;E. Martínez;C. Vaquero	2010		10.1007/978-3-642-12433-4_83	medicine;physical medicine and rehabilitation;physical therapy;surgery	Robotics	4.508641582538969	-79.41533280740126	11731
5a197214de38c6c776666dc979a67b6349014d5c	visual similarity based document layout analysis	similar test;texture features;dynamic clustering;texture analysis;document layout analysis	In this paper, a visual similarity based document layout analysis (DLA) scheme is proposed, which by using clustering strategy can adaptively deal with documents in different languages, with different layout structures and skew angles. Aiming at a robust and adaptive DLA approach, the authors first manage to find a set of representative filters and statistics to characterize typical texture patterns in document images, which is through a visual similarity testing process. Texture features are then extracted from these filters and passed into a dynamic clustering procedure, which is called visual similarity clustering. Finally, text contents are located from the clustered results. Benefit from this scheme, the algorithm demonstrates strong robustness and adaptability in a wide variety of documents, which previous traditional DLA approaches do not possess.	algorithm;cluster analysis;document layout analysis;drive letter assignment	Di Wen;Xiaoqing Ding	2006	Journal of Computer Science and Technology	10.1007/s11390-006-0459-0	computer science;machine learning;document layout analysis;pattern recognition;data mining;database;information retrieval	Web+IR	37.1135137528999	-63.4183685418221	11732
c9593cc1fab8b650127351cbee2896ed8b81826c	robotic rehabilitation tasks and measurements of psychophysiological responses	audio systems;healthy control;peripheral skin temperature;control group;psychophysiological responses;stroop test;cardiology;skin;temperature control;patient rehabilitation;biothermics;rehabilitation robotics psychology skin temperature control robot vision systems machine vision audio systems heart rate testing frequency;virtual reality;skin temperature;testing;physical activity;psychology;force;medical robotics;skin conductance response;skin conductance;healthy subjects;heart rate;rehabilitation robotics;machine vision;rehabilitation robots;robots;diseases;vision systems;temperature measurement;neurophysiology;bioelectric phenomena;haptic interfaces;respiratory rate;respiratory rate psychophysiological responses rehabilitation robots vision systems audio systems robotic tasks heart rate skin conductance peripheral skin temperature stroop test;frequency;pneumodynamics;robotic tasks;robot vision systems;biomedical measurement;physiological response;virtual reality bioelectric phenomena biomedical measurement biothermics cardiology diseases medical robotics neurophysiology patient rehabilitation pneumodynamics psychology skin	Rehabilitation robots, together with vision and audio systems form the multimodal environment for exercising the person in a number of ways, unavoidably influencing the physiological state of the subject. This paper examines viability of measuring psycho physiological responses to different robotic tasks. The heart rate, skin conductance, respiration and peripheral skin temperature were observed to verify if physical activity obstructs useful recordings and to verify responses in stroke population. 30 healthy subjects were checked with a control task, a purely mental task and task with physical load. 23 subacute stroke persons did a control task, pick and place task (+ inverted version) and Stroop test, same as 22 healthy control subjects. Psycho physiological measurements yielded results even in the presence of physical load and can thus potentially be useful for rehabilitation robotics. Similar responses as in healthy control group were found in the stroke group. Skin conductance response frequency, respiratory rate, skin conductance and skin temperature (all changes from baseline) were confirmed as parameters signaling changes in arousal and valence of both, stroke and control groups.	baseline (configuration management);conductance (graph);heart rate variability;multimodal interaction;peripheral;rehabilitation robotics;robot;smt placement equipment	Marko Munih;Domen Novak;Jaka Ziherl;Andrej Olenšek;Janez Podobnik;Tadej Bajd;Matjaz Mihelj	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509409	simulation;machine vision;computer science;artificial intelligence;skin conductance;virtual reality;neurophysiology;quantum mechanics	Robotics	14.71384047434553	-82.52290380325843	11739
99854b4f42f59c98dd15523c47aef2727386d929	fully automatic and real-time catheter segmentation in x-ray fluoroscopy		Augmenting X-ray imaging with 3D roadmap to improve guidance is a common strategy. Such approaches benefit from automated analysis of the X-ray images, such as the automatic detection and tracking of instruments. In this paper, we propose a real-time method to segment the catheter and guidewire in 2D X-ray fluoroscopic sequences. The method is based on deep convolutional neural networks. The network takes as input the current image and the three previous ones, and segments the catheter and guidewire in the current image. Subsequently, a centerline model of the catheter is constructed from the segmented image. A small set of annotated data combined with data augmentation is used to train the network. We trained the method on images from 182 X-ray sequences from 23 different interventions. On a testing set with images of 55 X-ray sequences from 5 other interventions, a median centerline distance error of 0.2 mm and a median tip distance error of 0.9 mm was obtained. The segmentation of the instruments in 2D X-ray sequences is performed in a real-time fully-automatic manner.	artificial neural network;convolutional neural network;radiography;real-time clock;real-time computing;real-time transcription;x-ray (amazon kindle)	Pierre Ambrosini;Daniel Ruijters;Wiro J. Niessen;Adriaan Moelker;Theo van Walsum	2017		10.1007/978-3-319-66185-8_65	pattern recognition;artificial intelligence;computer science;computer vision;convolutional neural network;catheter;deep learning;segmentation;fluoroscopy	Vision	36.704935554742455	-80.28147915609476	11744
29325b0053ee7a290a60efc9d838729403008a62	an advanced method in fetal phonocardiography	home care;noise cancellation acoustic analysis;acoustic analysis;ultrasound;phonocardiography;signal processing;noise cancellation;health status;fetal heart rate	The long-term variability of the fetal heart rate (FHR) provides valuable information on the fetal health status. The routine clinical FHR measurements are usually carried out by the means of ultrasound cardiography. Although the frequent FHR monitoring is recommendable, the high quality ultrasound devices are so expensive that they are not available for home care use. The passive and fully non-invasive acoustic recording called phonocardiography, provides an alternative low-cost measurement method. Unfortunately, the acoustic signal recorded on the maternal abdominal surface is heavily loaded by noise, thus the determination of the FHR raises serious signal processing issues. The development of an accurate and robust fetal phonocardiograph has been since long researched. This paper presents a novel two-channel phonocardiographic device and an advanced signal processing method for determination of the FHR. The developed system provided 83% accuracy compared to the simultaneously recorded reference ultrasound measurements.	acoustic cryptanalysis;display resolution;fetal heart;heart rate variability;home care of patient;phonocardiography;signal processing;cardiography	Péter Várady;Ludwig Wildt;Zoltán Benyó;Achim Hein	2003	Computer methods and programs in biomedicine	10.1016/S0169-2607(02)00111-6	radiology;medicine;computer science;signal processing;active noise control;ultrasound	Visualization	13.435975305058612	-87.42808506292809	11746
0f020d154a1ffccd7faf231820c5b81666249f28	automated left ventricular cardiac mri segmentation, quantification and visualization using a radial-ray approach	cardiac vitality indicator;left ventricle;segmentation;three dimensional;gradient analysis;high resolution;stereo vision;higher order	A new segmentation algorithm is proposed that automatically segments the endocardium, epicardium and papillary muscles of the left ventricle (LV) from short-axis cardiac MRI data sets. The algorithm is based on image intensity gradient analysis using higher order derivatives and local parameterisation to obtain reliable segmentation results. User interaction is minimised to just the definition of centre points for the ventricle base, apex and papillary muscles. From the approximate centre of the ventricle on each slice, rays are cast and then scanned to find the extent of the ventricle wall. The segmentation algorithm parameterises three dynamic intersection tests to detect endocardium and epicardium intersections for each emitted ray. A high-resolution mesh describing both surfaces of the myocardium is then constructed from these intersections. Automated segmentation and reconstruction of the papillary muscles allows their exclusion from LV volume calculations and their inclusion in myocardial mass calculations. A software package was developed to evaluate the algorithm, providing DICOM compatibility and user interaction for segmentation corrections. Three-dimensional visualization of the segmentation results is also catered for, as both still and animated meshes, in mono or stereo vision. Currently, the algorithm detects endocardium intersections with great accuracy and the algorithm is robust even on problematic data sets. It also provides accurate reconstruction of the papillary muscles.	radial (radio)	C. Prest;R. Phillips;N. Nikitin;C. M. Langton;John G. F. Cleland	2004			biology;three-dimensional space;computer vision;gradient analysis;higher-order logic;image resolution;computer science;stereopsis;segmentation;genetics;anatomy	Vision	41.376160005428225	-80.93363606921274	11752
f288f7da24e5aa8efd6fbe07dce688ca8eb96a5b	integral scale histogram local binary patterns for classification of narrow-band gastroenterology images	graph theory;image resolution;vectors biological organs decision support systems feature extraction graph theory image classification image resolution medical image processing;biological organs;image classification;vectors;narrow band gastroenterology image classification integral scale histogram local binary pattern descriptor effectiveness state of the art method descriptor accuracy multiresolution analysis low dimensional feature vector image resolution aggregated histogram propagation integrated scale histogram local binary pattern image descriptor computer assisted decision system design image processing narrow band imaging imaging technology;feature extraction;medical image processing;decision support systems;histograms image resolution gastroenterology cancer feature extraction imaging visualization	The introduction of various novel imaging technologies such as narrow-band imaging have posed novel image processing challenges to the design of computer assisted decision systems. In this paper, we propose an image descriptor refered to as integrated scale histogram local binary patterns. We propagate an aggregated histogram of local binary patterns of an image at various resolutions. This results in low dimensional feature vectors for the images while incorporating their multiresolution analysis. The descriptor was used to classify gastroenterology images into four distinct groups. Results produced by the proposed descriptor exhibit around 92% accuracy for classification of gastroenteroloy images outperforming other state-of-the-art methods, endorsing the effectiveness of the proposed descriptor.	feature vector;histogram;image processing;imaging technology;local binary patterns;multiresolution analysis;visual descriptor	Farhan Riaz;Mário Dinis-Ribeiro;Pedro Pimentel-Nunes;Miguel Tavares Coimbra	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610350	computer vision;contextual image classification;feature detection;local binary patterns;image resolution;binary image;image processing;feature extraction;computer science;histogram matching;graph theory;machine learning;digital image processing;pattern recognition;adaptive histogram equalization;automatic image annotation;feature;image histogram	Vision	35.34002861273929	-72.21129691832245	11755
13bbd542067b63ffc5d1bf42cd20da9c6c24efea	saliency-enhanced image aesthetics class prediction	image features;image segmentation;support vector machines;saliency;discriminative global image features saliency enhanced image aesthetics class prediction image feature extraction 5 fold cross validation classification visual saliency model;image enhancement feature extraction image classification;image classification;indexing terms;classification;aesthetics;accuracy;visualization;image enhancement;image edge detection;feature extraction;classification algorithms;feature extraction data mining image retrieval content based retrieval image color analysis information analysis classification algorithms thumb photography support vector machines;cross validation;classification accuracy;classification aesthetics saliency	We present a saliency-enhanced method for the classification of professional photos and snapshots. First, we extract the salient regions from an image by utilizing a visual saliency model. We assume that the salient regions contain the photo subject. Then, in addition to a set of discriminative global image features, we extract a set of salient features that characterize the subject and depict the subject-background relationship. Our high-level perceptual approach produces a promising 5-fold cross-validation (5-CV) classification accuracy of 78.8%, significantly higher than existing approaches that concentrate mainly on global features.	cross-validation (statistics);high- and low-level;snapshot (computer storage)	Lai-Kuan Wong;Kok-Lim Low	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413825	statistical classification;support vector machine;computer vision;contextual image classification;visualization;index term;feature extraction;biological classification;computer science;machine learning;salience;pattern recognition;accuracy and precision;image segmentation;feature;cross-validation	Robotics	39.60383755945836	-62.15530658939788	11758
1f0abcb3868abb039cf117355d4a8ab9de6a0b6f	forecasting potential diabetes complications	forecast diabetes complications;sparse factor graph;feature sparseness	Diabetes complications often afflict diabetes patients seriously: over 68% of diabetes-related mortality is caused by diabetes complications. In this paper, we study the problem of automatically diagnosing diabetes complications from patients’ lab test results. The objective problem has two main challenges: 1) feature sparseness: a patient only takes 1.26% lab tests on average, and 65.5% types of lab tests are taken by less than 10 patients; 2) knowledge skewness: it lacks comprehensive detailed domain knowledge of association between diabetes complications and lab tests. To address these challenges, we propose a novel probabilistic model called Sparse Factor Graph Model (SparseFGM). SparseFGM projects sparse features onto a lower-dimensional latent space, which alleviates the problem of sparseness. SparseFGM is also able to capture the associations between complications and lab tests, which help handle the knowledge skewness. We evaluate the proposed model on a large collections of real medical records. SparseFGM outperforms (+20% by F1) baselines significantly and gives detailed associations between diabetes complications and lab tests.	baseline (configuration management);factor graph;neural coding;sparse matrix;statistical model	Yang Yang;Walter Luyten;Lu Liu;Marie-Francine Moens;Jie Tang;Juan-Zi Li	2014			computer science;machine learning;data mining;statistics	AI	4.39165840228769	-74.61661570634685	11761
4f1c5ac05aed2003641995721b66c10caea5bde2	longitudinal finger rotation in finger-vein recognition		Finger-vein scanners or vein-based biometrics in general are becoming more and more popular. Commercial off-the-shelf finger-vein scanners usually capture only one finger from the palmar side using transillumination. Most scanners have a contact area and a finger-shaped support where the finger has to be placed onto in order to prevent misplacements of the finger including shifts, planar rotation and tilts. However, this is not able to prevent rotation of the finger along its longitudinal axis (also called non-planar finger rotation). This kind of finger rotation poses a severe problem in finger-vein recognition as the resulting vein image may represent entirely different patterns due to the perspective projection. We evaluated the robustness of several finger-vein recognition schemes against longitudinal finger rotation. Therefore, we established a finger-vein data set exhibiting longitudinal finger rotation in steps of 1° covering a range of ±90°. Our experimental results confirm that the performance of most of the simple recognition schemes rapidly decreases for more than 10° of rotation, while more advanced schemes are able to handle up to 30°.		Bernhard Prommegger;Christof Kauba;Andreas Uhl	2018	2018 International Conference of the Biometrics Special Interest Group (BIOSIG)	10.23919/BIOSIG.2018.8553036	pattern recognition;artificial intelligence;computer science;computer vision;robustness (computer science);finger vein recognition;feature extraction;contact area;planar	Robotics	34.6598543083132	-56.751652686348535	11767
b94755b4da62b867b7be1d828e30afa4ee18062b	image analysis for assessing molecular activity changes in time-dependent geometries	sensitivity and specificity;optical image processing;animals;genomics;time dependent;mice;fluorophore distribution monitoring image analysis molecular activity changes time dependent geometries in vivo fluorescence molecular imaging tomography genomics proteomics robust registration aligning temporal data surface anatomical features;fluorescence;algorithms animals fluorescent dyes image enhancement image interpretation computer assisted metabolic clearance rate mice microscopy fluorescence reproducibility of results sensitivity and specificity subtraction technique tomography optical;fluorophore distribution monitoring;robust registration;image sequence analysis;optical tomography;temporal data;geometry;molecular imaging;optical image processing image registration image sequence analysis;indexing terms;three dimensional;genetics;image enhancement;metabolic clearance rate;optical imaging;tomography optical;proteins;monitoring;image interpretation computer assisted;molecular activity changes;medical image processing;image registration;fluorescent dyes;molecular biophysics;microscopy fluorescence;reproducibility of results;medical image processing fluorescence genetics proteins molecular biophysics biomedical optical imaging optical tomography image registration;algorithms;animal imaging;image analysis;subtraction technique;biomedical optical imaging;in vivo fluorescence molecular imaging;proteomics;surface anatomical features;tomography;in vivo;time dependent geometries;image analysis geometry animals monitoring in vivo fluorescence molecular imaging tomography genomics bioinformatics;bioinformatics;aligning temporal data	In vivo fluorescence molecular imaging and tomography has facilitated monitoring of genomics and proteomics over time and on the same animal. A highly important issue, however, has been the robust registration of animals imaged at different time points to obtain accurate description of activity and location. This paper presents a method for aligning temporal data of small animals based on surface anatomical features and improving the accuracy of monitoring fluorophore distribution. The method can account for differences in the positioning and compression of small animals and can be extended to three-dimensional as well as to other imaging modalities.	compression;fluorescence;image analysis;molecular imaging;proteomics;tomography;video-in video-out	Kostas Marias;Jorge Ripoll;Heiko Meyer;Vasilis Ntziachristos;Stelios C. Orphanoudakis	2005	IEEE Transactions on Medical Imaging	10.1109/TMI.2005.848612	three-dimensional space;computer vision;genomics;image analysis;index term;radiology;fluorescence;computer science;image registration;optical imaging;temporal database;tomography;in vivo;molecular imaging;proteomics;medical physics;molecular biophysics	Visualization	45.18428599259925	-80.83830279730799	11784
5a5056be64e156d7e481af6f4a7301d89f3880f9	potential energy landscape and robustness of a gene regulatory network: toggle switch	ultrasensitivity;field theory;escherichia coli;time scale;proteome;signal transduction;basin of attraction;models biological;gene network;logistic models;expression;phase space;stability;protein synthesis;stochasticity;dynamics;gene expression regulation;cellular network;potential energy;gene regulatory network;computer simulation;construction;energy metabolism;protein degradation;noise;steady state	Finding a multidimensional potential landscape is the key for addressing important global issues, such as the robustness of cellular networks. We have uncovered the underlying potential energy landscape of a simple gene regulatory network: a toggle switch. This was realized by explicitly constructing the steady state probability of the gene switch in the protein concentration space in the presence of the intrinsic statistical fluctuations due to the small number of proteins in the cell. We explored the global phase space for the system. We found that the protein synthesis rate and the unbinding rate of proteins to the gene were small relative to the protein degradation rate; the gene switch is monostable with only one stable basin of attraction. When both the protein synthesis rate and the unbinding rate of proteins to the gene are large compared with the protein degradation rate, two global basins of attraction emerge for a toggle switch. These basins correspond to the biologically stable functional states. The potential energy barrier between the two basins determines the time scale of conversion from one to the other. We found as the protein synthesis rate and protein unbinding rate to the gene relative to the protein degradation rate became larger, the potential energy barrier became larger. This also corresponded to systems with less noise or the fluctuations on the protein numbers. It leads to the robustness of the biological basins of the gene switches. The technique used here is general and can be applied to explore the potential energy landscape of the gene networks.	elegant degradation;gene regulatory network;large;monostable;network switch;potential energy;protein biosynthesis;steady state;switch device component	Keun-Young Kim;Zhongjing Wang	2007	PLoS Computational Biology	10.1371/journal.pcbi.0030060	computer simulation;biology;gene regulatory network;bioinformatics;ecology;genetics	Comp.	7.284850492018499	-64.50382824206636	11785
1902a0474ab7eb3bf42c9eda9e82a568137b56fe	a new method to measure the semantic similarity from query phenotypic abnormalities to diseases based on the human phenotype ontology	diagnosis;disease;human phenotype ontology (hpo);semantic similarity	Although rapid developed sequencing technologies make it possible for genotype data to be used in clinical diagnosis, it is still challenging for clinicians to understand the results of sequencing and make correct judgement based on them. Before this, diagnosis based on clinical features held a leading position. With the establishment of the Human Phenotype Ontology (HPO) and the enrichment of phenotype-disease annotations, there throws much more attention to the improvement of phenotype-based diagnosis. In this study, we presented a novel method called RelativeBestPair to measure similarity from the query terms to hereditary diseases based on HPO and then rank the candidate diseases. To evaluate the performance, we simulated a set of patients based on 44 complex diseases. Besides, by adding noise or imprecision or both, cases closer to real clinical conditions were generated. Thus, four simulated datasets were used to make comparison among RelativeBestPair and seven existing semantic similarity measures. RelativeBestPair ranked the underlying disease as top 1 on 93.73% of the simulated dataset without noise and imprecision, 93.64% of the simulated dataset with noise and without imprecision, 39.82% of the simulated dataset without noise and with imprecision, and 33.64% of the simulated dataset with both noise and imprecision. Compared with the seven existing semantic similarity measures, RelativeBestPair showed similar performance in two datasets without imprecision. While RelativeBestPair appeared to be equal to Resnik and better than other six methods in the simulated dataset without noise and with imprecision, it significantly outperformed all other seven methods in the simulated dataset with both noise and imprecision. It can be indicated that RelativeBestPair might be of great help in clinical setting.	biopolymer sequencing;congenital abnormality;diagnosis, clinical;gene ontology term enrichment;hpo formalism;hereditary diseases;human phenotype ontology;judgment;patients;question (inquiry);semantic similarity;silo (dataset)	Xiaofeng Gong;Jianping Jiang;Zhongqu Duan;Hui Lu	2018		10.1186/s12859-018-2064-y	semantic similarity;phenotype;judgement;genotype;bioinformatics;human phenotype ontology;biology	NLP	6.93693439322053	-54.65890610230442	11793
deeb04f39fcbef24d3c4aa32d4c7642699d8de51	efficient methods for implementation of multi-level nonrigid mass-preserving image registration on gpus and multi-threaded cpus	multi level;lungs;gpu;mass preserving image registration	BACKGROUND AND OBJECTIVE Faster and more accurate methods for registration of images are important for research involved in conducting population-based studies that utilize medical imaging, as well as improvements for use in clinical applications. We present a novel computation- and memory-efficient multi-level method on graphics processing units (GPU) for performing registration of two computed tomography (CT) volumetric lung images.   METHODS We developed a computation- and memory-efficient Diffeomorphic Multi-level B-Spline Transform Composite (DMTC) method to implement nonrigid mass-preserving registration of two CT lung images on GPU. The framework consists of a hierarchy of B-Spline control grids of increasing resolution. A similarity criterion known as the sum of squared tissue volume difference (SSTVD) was adopted to preserve lung tissue mass. The use of SSTVD consists of the calculation of the tissue volume, the Jacobian, and their derivatives, which makes its implementation on GPU challenging due to memory constraints. The use of the DMTC method enabled reduced computation and memory storage of variables with minimal communication between GPU and Central Processing Unit (CPU) due to ability to pre-compute values. The method was assessed on six healthy human subjects.   RESULTS Resultant GPU-generated displacement fields were compared against the previously validated CPU counterpart fields, showing good agreement with an average normalized root mean square error (nRMS) of 0.044±0.015. Runtime and performance speedup are compared between single-threaded CPU, multi-threaded CPU, and GPU algorithms. Best performance speedup occurs at the highest resolution in the GPU implementation for the SSTVD cost and cost gradient computations, with a speedup of 112 times that of the single-threaded CPU version and 11 times over the twelve-threaded version when considering average time per iteration using a Nvidia Tesla K20X GPU.   CONCLUSIONS The proposed GPU-based DMTC method outperforms its multi-threaded CPU version in terms of runtime. Total registration time reduced runtime to 2.9min on the GPU version, compared to 12.8min on twelve-threaded CPU version and 112.5min on a single-threaded CPU. Furthermore, the GPU implementation discussed in this work can be adapted for use of other cost functions that require calculation of the first derivatives.		Nathan D. Ellingwood;Youbing Yin;Matthew Smith;Ching-Long Lin	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2015.12.018	parallel computing;computer science;theoretical computer science;computer graphics (images)	HPC	48.39904165381478	-81.1830239464231	11804
ab1219571a3e4f297e7a800581df6fbb8315619a	low-power wearable respiratory sound sensing	health research;uk clinical guidelines;biological patents;wheeze detection;europe pubmed central;citation search;short term fourier transform;uk phd theses thesis;life sciences;respiratory sounds;wearable sensor;decision trees;uk research reports;medical journals;europe pmc;biomedical research;dsp;bioinformatics;low power implementation	Building upon the findings from the field of automated recognition of respiratory sound patterns, we propose a wearable wireless sensor implementing on-board respiratory sound acquisition and classification, to enable continuous monitoring of symptoms, such as asthmatic wheezing. Low-power consumption of such a sensor is required in order to achieve long autonomy. Considering that the power consumption of its radio is kept minimal if transmitting only upon (rare) occurrences of wheezing, we focus on optimizing the power consumption of the digital signal processor (DSP). Based on a comprehensive review of asthmatic wheeze detection algorithms, we analyze the computational complexity of common features drawn from short-time Fourier transform (STFT) and decision tree classification. Four algorithms were implemented on a low-power TMS320C5505 DSP. Their classification accuracies were evaluated on a dataset of prerecorded respiratory sounds in two operating scenarios of different detection fidelities. The execution times of all algorithms were measured. The best classification accuracy of over 92%, while occupying only 2.6% of the DSP's processing time, is obtained for the algorithm featuring the time-frequency tracking of shapes of crests originating from wheezing, with spectral features modeled using energy.	algorithm;asthma;computational complexity theory;dsp gene;decision tree;digital signal processor;low-power broadcasting;on-board data handling;respiratory sounds;short-time fourier transform;signal processing;silo (dataset);status asthmaticus;transmitter;wearable computer;wheezing	Dinko Oletic;Bruno Arsenali;Vedran Bilas	2014		10.3390/s140406535	embedded system;simulation;speech recognition;short-time fourier transform;telecommunications;computer science;bioinformatics;engineering;electrical engineering;digital signal processing;decision tree;physics;quantum mechanics	Mobile	11.735796941544159	-88.59690106056398	11806
5b993b924432b1f1b9bdcfcb1432fe70a5aa9501	3d shape analysis for point clouds using variational geometric	three dimensional displays surface reconstruction shape solid modeling surface treatment topology covariance matrices;active contours;point clouds;3d shape analysis;3d shape modeling 3d shape analysis point clouds variational geometric active contours variational scheme surface reconstruction;variational techniques computational geometry solid modelling;variational scheme 3d shape analysis point clouds active contours;variational scheme	I propose novel 3D shape analysis and modeling algorithm for active contours using variational scheme. The proposed algorithm utilizes an unknown surface of 3D shape by normal to 3D shape analysis and modeling. It would be used to get high quality reconstruction's results from many dimensions' data. I give it by utilizing continuous anisotropism weights would finish comparable ameliorate to surface reconstruction.	algorithm;display resolution;point cloud;shape analysis (digital geometry);variational principle	Longcun Jin	2015	2015 11th International Conference on Natural Computation (ICNC)	10.1109/ICNC.2015.7378112	active shape model;mathematical optimization;topology;shape analysis;mathematics;geometry	Vision	47.64287235262786	-71.9444511719933	11807
fb04e214da3820c1ea8decfcf81735a1a859d7f5	scansite 2.0: proteome-wide prediction of cell signaling interactions using short sequence motifs	substrate specificity;software;position specific scoring matrix;proteome;protein sequence;phosphorylation;signal transduction;binding sites;internet;proteins;molecular weight;protein structure tertiary;two dimensional gel electrophoresis;world wide web;algorithms;sequence motif;amino acid motifs;cell signaling;sequence analysis protein;databases protein;in silico	Scansite identifies short protein sequence motifs that are recognized by modular signaling domains, phosphorylated by protein Ser/Thr- or Tyr-kinases or mediate specific interactions with protein or phospholipid ligands. Each sequence motif is represented as a position-specific scoring matrix (PSSM) based on results from oriented peptide library and phage display experiments. Predicted domain-motif interactions from Scansite can be sequentially combined, allowing segments of biological pathways to be constructed in silico. The current release of Scansite, version 2.0, includes 62 motifs characterizing the binding and/or substrate specificities of many families of Ser/Thr- or Tyr-kinases, SH2, SH3, PDZ, 14-3-3 and PTB domains, together with signature motifs for PtdIns(3,4,5)P(3)-specific PH domains. Scansite 2.0 contains significant improvements to its original interface, including a number of new generalized user features and significantly enhanced performance. Searches of all SWISS-PROT, TrEMBL, Genpept and Ensembl protein database entries are now possible with run times reduced by approximately 60% when compared with Scansite version 1.0. Scansite 2.0 allows restricted searching of species-specific proteins, as well as isoelectric point and molecular weight sorting to facilitate comparison of predictions with results from two-dimensional gel electrophoresis experiments. Support for user-defined motifs has been increased, allowing easier input of user-defined matrices and permitting user-defined motifs to be combined with pre-compiled Scansite motifs for dual motif searching. In addition, a new series of Sequence Match programs for non-quantitative user-defined motifs has been implemented. Scansite is available via the World Wide Web at http://scansite.mit.edu.	14-3-3 proteins;amino acid sequence;cell signaling;compiler;databases, protein;dual;electrophoresis, gel, two-dimensional;ensembl;experiment;gel electrophoresis (lab technique);interaction;interface device component;isoelectric point;ligands;molecular weight;peptide library;phage display;phosphotyrosine binding domain;position weight matrix;proteome;swiss-model;score;sequence motif;sorting;substrate specificity;switzerland;tyrosine;uniprot;world wide web	John C. Obenauer;Lewis C. Cantley;Michael B. Yaffe	2003	Nucleic acids research	10.1093/nar/gkg584	phosphorylation;biology;two-dimensional gel electrophoresis;molecular biology;the internet;cell signaling;bioinformatics;binding site;protein sequencing;proteome;molecular mass;genetics;signal transduction;sequence motif	Comp.	0.1536406432473001	-59.626885821104544	11830
89ed5d61242011990b72282aa8ea7ab932da056c	deep learning to classify difference image for image change detection	unsupervised learning;high level abstractions;pre classification;image change detection;neural networks;neural nets;multitemporal image analysis;joints conferences neural networks;image classification;joints;unsupervised feature learning;unsupervised learning image classification neural nets object detection;difference image analysis;deep neural network learning algorithm;supervised fine tuning;object detection;conferences;high level abstractions difference image classification image change detection multitemporal image analysis difference image analysis deep neural network learning algorithm unsupervised feature learning supervised fine tuning pre classification;difference image classification	Image change detection is a process to analyze multi-temproal images of the same scene for identifying the changes that have occurred. In this paper, we propose a novel difference image analysis approach based on deep neural networks for image change detection problems. The deep neural network learning algorithm for classification includes unsupervised feature learning and supervised fine-tuning. Some samples with the labels of high accuracy obtained by a pre-classification are used for fine-tuning. Since a deep neural network can learn complicated functions that can represent high-level abstractions, it can obtain satisfactory results. Theoretical analysis and experiment results on real datasets show that the proposed method outperforms some other methods.	algorithm;artificial neural network;deep learning;feature learning;high- and low-level;image analysis;supervised learning	Jiaojiao Zhao;Maoguo Gong;Jia Liu;Licheng Jiao	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889510	computer vision;contextual image classification;computer science;machine learning;pattern recognition;artificial neural network	AI	28.029833678651833	-53.20919468957013	11844
e44a568486ddee2eda9fb557a26f76334e919ce0	cardioids-based faster authentication and diagnosis of remote cardiovascular patients	cardioid;patient authentication;remote monitoring;wireless monitoring;cardiovascular disease detection;mission critical alerting	In recent times, dealing with deaths associated with cardiovascular diseases (CVD) has been one of the most challenging issues. The usage of mobile phones and portable Electrocardiogram (ECG) acquisition devices can mitigate the risks associated with CVD by providing faster patient diagnosis and patient care. The existing technologies entail delay in patient authentication and diagnosis. However, for the cardiologists minimizing the delay between a possible CVD symptom and patient care is crucial, as this has a proven impact in the longevity of the patient. Therefore, every seconds counts in terms of patient authentication and diagnosis. In this paper, we introduce the concept of Cardioid based patient authentication and diagnosis. According to our experimentations, the authentication time can be reduced from 30.64 s (manual authentication in novice mobile user) to 0.4398 s (automated authentication). Our ECG based patient authentication mechanism is up to 4878 times faster than conventional biometrics like, face recognition. The diagnosis time could be improved from several minutes to less than 0.5 s (cardioid display on a single screen). Therefore, with our presented mission critical alerting mechanism on wireless devices, minute’s worth of tasks can be reduced to second’s, without compromising the accuracy of authentication and quality of diagnosis. Copyright © 2011 John Wiley & Sons, Ltd.	authentication;biometrics;facial recognition system;john d. wiley;mission critical;mobile phone	Fahim Sufi;Ibrahim Khalil;Ibrahim Habib	2011	Security and Communication Networks	10.1002/sec.262	telecommunications;internet privacy;cardioid;computer security;rmon	HCI	4.84549352806595	-88.61753509053267	11873
5aedfca32d7ce112663df6d904fcb79e54315c7a	pairwise sequence alignment using a prosite pattern-derived similarity score	dynamic programming;dynamic programming algorithm;pairwise sequence alignment;dynamic program;databank scanning;prosite;sequence alignment;letter by letter	Existing methods for alignments are based on edition costs computed additionally position by position, according to a fixed substitution matrix: a substitution always has the same weight regardless of the position. Nevertheless the biologist favours a similarity according to his knowledge of the structure or the function of the sequences considered. In the particular case of proteins, we present a method consisting in integrating other information, such as patterns of the PROSITE databank, in the classical dynamic programming algorithm. The method consists in making an alignment by dynamic programming taking a decision not only letter by letter as in the Smith & Waterman algorithm but also by giving a reward when aligning patterns.	clinical act of insertion;databases;dynamic programming;prosite;rewards;sequence alignment;sequence motif;smith–waterman algorithm;substitution matrix	Jean-Paul Comet;Jacques Henry	2002	Computers & chemistry	10.1016/S0097-8485(02)00005-0	computer science;bioinformatics;machine learning;dynamic programming;data mining	ML	0.21730325471326886	-60.81294148472818	11874
4532710d3d44e4685e2dc44f4a45223f4dc03ff3	eul1db: the european database of l1hs retrotransposon insertions in humans	databases nucleic acid;disease;polymorphism genetic;long interspersed nucleotide elements;humans;phenotype	Retrotransposons account for almost half of our genome. They are mobile genetics elements-also known as jumping genes--but only the L1HS subfamily of Long Interspersed Nuclear Elements (LINEs) has retained the ability to jump autonomously in modern humans. Their mobilization in germline--but also some somatic tissues--contributes to human genetic diversity and to diseases, such as cancer. Here, we present euL1db, the European database of L1HS retrotransposon insertions in humans (available at http://euL1db.unice.fr). euL1db provides a curated and comprehensive summary of L1HS insertion polymorphisms identified in healthy or pathological human samples and published in peer-reviewed journals. A key feature of euL1db is its sample--wise organization. Hence L1HS insertion polymorphisms are connected to samples, individuals, families and clinical conditions. The current version of euL1db centralizes results obtained in 32 studies. It contains >900 samples, >140,000 sample-wise insertions and almost 9000 distinct merged insertions. euL1db will help understanding the link between L1 retrotransposon insertion polymorphisms and phenotype or disease.	body tissue;clinical act of insertion;diploid cell;genetic polymorphism;hereditary diseases;insertion mutation;insertion sort;journal;merge;neoplasms;retrotransposons;scientific publication;subfamily	Ashfaq A. Mir;Claude Philippe;Gaël Cristofari	2015		10.1093/nar/gku1043	biology;bioinformatics;phenotype;genetics	ML	1.4594525349483298	-62.13070951943157	11876
bdd6703d44d24895accd1c6caa47da01a4f0d81c	random forests based monitoring of human larynx using questionnaire data	data proximity;random forests;variable importance;variable selection;engineering and technology;classifier;teknik och teknologier;human larynx;datavetenskap datalogi;computer science	This paper is concerned with soft computing techniques-based noninvasive monitoring of human larynx using subject’s questionnaire data. By applying random forests (RF), questionnaire data are categorized into a healthy class and several classes of disorders including: cancerous, noncancerous, diffuse, nodular, paralysis, and an overall pathological class. The most important questionnaire statements are determined using RF variable importance evaluations. To explore data represented by variables used by RF, the t-distributed stochastic neighbor embedding (t-SNE) and the multidimensional scaling (MDS) are applied to the RF data proximity matrix. When testing the developed tools on a set of data collected from 109 subjects, the 100% classification accuracy was obtained on unseen data in binary classification into the healthy and pathological classes. The accuracy of 80.7% was achieved when classifying the data into the healthy, cancerous, noncancerous classes. The t-SNE and MDS mapping techniques applied allow obtaining two-dimensional maps of data and facilitate data exploration aimed at identifying subjects belonging to a ‘‘risk group’’. It is expected that the developed tools will be of great help in preventive health care in laryngology. 2011 Elsevier Ltd. All rights reserved.	binary classification;categorization;data mining;data visualization;image scaling;importance sampling;map;multidimensional scaling;radio frequency;random forest;soft computing;t-distributed stochastic neighbor embedding;the 100	Marija Bacauskiene;Antanas Verikas;Adas Gelzinis;Aurelija Vegiene	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.11.070	random forest;speech recognition;classifier;computer science;artificial intelligence;machine learning;data mining;feature selection;statistics	ML	3.859264212877445	-76.54906797215426	11881
4949ccafcc71ae1b2b28fef172ba4b8adfc504f0	a real-time rodent tracking system for both light and dark cycle behavior analysis	biological effects of optical radiation;drugs;video signal processing biological effects of optical radiation drugs genetics medical image processing;locomotor activity;tracking system;video signal processing;real time;genetics;white light;near infrared;open field;open field locomotor activity real time rodent tracking system light cycle behavior analysis dark cycle behavior analysis position tracking genetic mutations drug action environmental stimuli continuous overhead video monitoring near infrared illumination;medical image processing;real time systems rodents lighting biomedical monitoring genetic mutations drugs condition monitoring standards development infrared surveillance robustness;behavior analysis;real time systems	"""Position tracking of rodents is useful and necessary to help elucidate the behavioral and physiological effects of genetic mutations, drug action, and environmental stimuli. In this paper we describe a real-time system developed to allow continuous overhead video monitoring of rodent behavior in a home cage environment, either in a daylight condition (light-cycle) using standard visible illumination or a night-time condition (dark-cycle), using overhead near infrared illumination (NIR). Due to the lack of research on the effects of NIR on rodent behavior, we also examined open-field locomotor activity under 880 nm and 940 nm wavelengths of NIR, as well as visible white light and a """"dark"""" condition consisting of a very dim level of NIR. The experimental result validated the described system and robustly tracked the target rodent in the light cycle, and for high contrast conditions in the dark cycle."""	daylight;overhead (computing);real-time clock;real-time computing;real-time transcription;tracking system	Jane Brooks Zurn;Drew Hohmann;Steven I. Dworkin;Yuichi Motai	2005	2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1	10.1109/ACVMOT.2005.9	near-infrared spectroscopy;computer vision;simulation;tracking system;computer science	Robotics	12.7460038847951	-65.88843698103253	11887
9618c80f7767e1c2b1998b80d98f4571dd29c34d	segmentation of the heart and great vessels in ct images using a model-based adaptation framework	pulmonary vein;model based segmentation;computed tomography;generalized hough transform;adaptive mesh;automatic segmentation;coronary sinus;atrial fibrillation;mesh adaptation;left atrium;model building;linear transformation;ground truth;shape variability;heart segmentation;ct angiography;cardiac resynchronization therapy	Recently, model-based methods for the automatic segmentation of the heart chambers have been proposed. An important application of these methods is the characterization of the heart function. Heart models are, however, increasingly used for interventional guidance making it necessary to also extract the attached great vessels. It is, for instance, important to extract the left atrium and the proximal part of the pulmonary veins to support guidance of ablation procedures for atrial fibrillation treatment. For cardiac resynchronization therapy, a heart model including the coronary sinus is needed. We present a heart model comprising the four heart chambers and the attached great vessels. By assigning individual linear transformations to the heart chambers and to short tubular segments building the great vessels, variable sizes of the heart chambers and bending of the vessels can be described in a consistent way. A configurable algorithmic framework that we call adaptation engine matches the heart model automatically to cardiac CT angiography images in a multi-stage process. First, the heart is detected using a Generalized Hough Transformation. Subsequently, the heart chambers are adapted. This stage uses parametric as well as deformable mesh adaptation techniques. In the final stage, segments of the large vascular structures are successively activated and adapted. To optimize the computational performance, the adaptation engine can vary the mesh resolution and freeze already adapted mesh parts. The data used for validation were independent from the data used for model-building. Ground truth segmentations were generated for 37 CT data sets reconstructed at several cardiac phases from 17 patients. Segmentation errors were assessed for anatomical sub-structures resulting in a mean surface-to-surface error ranging 0.50-0.82mm for the heart chambers and 0.60-1.32mm for the parts of the great vessels visible in the images.	acclimatization;algorithm;atrial fibrillation;blood vessel;ct scan;cardiac resynchronization therapy;cardiac chamber structure;computation;computed tomography of the heart;coronary sinus structure;decompression sickness;ground truth;heart atrium;heart diseases;hough transform;left atrial structure;numerous;pet/ct scan;part dosing unit;patients;pulmonary veins;sinus - general anatomical term;angiogram;biologic segmentation;cell transformation	Olivier Ecabert;Jochen Peters;Matthew J. Walker;Thomas Ivanc;Cristian Lorenz;Jens von Berg;Jonathan Lessick;Mani Vembar;Jürgen Weese	2011	Medical image analysis	10.1016/j.media.2011.06.004	model building;radiology;ground truth;mathematics;linear map;computed tomography;anatomy;cardiology	Vision	40.40545937186919	-80.1790790179856	11900
894ce38db417541591b06791e53749fa01453ceb	computational reconstruction of primordial prototypes of elementary functional loops in modern proteins	elementary chemical;conserved functional motif;building block;primordial prototype;elementary functional loop;ancient prototype;modern protein;binding amino acid;biochemical function;distinct functional signature;complex catalytic machine;computational reconstruction;ancient elementary function	MOTIVATION Enzymes are complex catalytic machines, which perform sequences of elementary chemical transformations resulting in biochemical function. The building blocks of enzymes, elementary functional loops (EFLs), possess distinct functional signatures and provide catalytic and binding amino acids to the enzyme's active sites. The goal of this work is to obtain primordial prototypes of EFLs that existed before the formation of enzymatic domains and served as their building blocks.   RESULTS We developed a computational strategy for reconstructing ancient prototypes of EFLs based on the comparison of sequence segments on the proteomic scale, which goes beyond detection of conserved functional motifs in homologous proteins. We illustrate the procedure by a CxxC-containing prototype with a very basic and ancient elementary function of metal/metal-containing cofactor binding and redox activity. Acquiring the prototypes of EFLs is necessary for revealing how the original set of protein folds with enzymatic functions emerged in predomain evolution.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.   CONTACT igor.berezovsky@uni.no.	amino acids;antivirus software;automated theorem proving;bioinformatics;computation;double strand break repair;elementary function;functional gastrointestinal disorders;functional derivative;functional genomics;greater than;homology (biology);manuscripts;position weight matrix;proteomics;prototype;s-adenosylmethionine;strand (programming language);thioredoxin reductase (nadph);type signature;cell transformation;chemical cofactor;cofactor binding;nuclease	Alexander Goncearenco;Igor N. Berezovsky	2011	Bioinformatics	10.1093/bioinformatics/btr396	biology;bioinformatics;algorithm	Comp.	1.9937478844060175	-61.815790866102475	11901
ac52b571e5a8624e641646d7c96d7b1e3e67c3de	identifying neuronal oscillations using rhythmicity	article letter to editor	Neuronal oscillations are a characteristic feature of neuronal activity and are typically investigated through measures of power and coherence. However, neither of these measures directly reflects the distinctive feature of oscillations: their rhythmicity. Rhythmicity is the extent to which future phases can be predicted from the present one. Here, we present lagged coherence, a frequency-indexed measure that quantifies the rhythmicity of neuronal activity. We use this method to identify the sensorimotor alpha and beta rhythms in ongoing magnetoencephalographic (MEG) data, and to study their attentional modulation. Using lagged coherence, the sensorimotor rhythms become visible in ongoing activity as local rhythmicity peaks that are separated from the strong posterior activity in the same frequency bands. In contrast, using conventional power analyses, the sensorimotor rhythms cannot be identified in ongoing data, nor can they be separated from the posterior activity. We go on to show that the attentional modulation of these rhythms is also evident in lagged coherence and moreover, that in contrast to power, it can be visualised even without an experimental contrast. These findings suggest that the rhythmicity of neuronal activity is better suited to identify neuronal oscillations than the power in the same frequency band.	bands;beta rhythm;cache coherence;delta modulation;frequency band;index;magnetoencephalography;neural oscillation	Anne M. M. Fransen;Freek van Ede;Eric Maris	2015	NeuroImage	10.1016/j.neuroimage.2015.06.003	psychology;neuroscience;developmental psychology;communication	ML	19.046441618965957	-76.88896740680218	11902
44d30755434fdaf7ea7f7d774ac300c112042d8b	using a one-dimensional control signal for two different output commands in an implanted bci			brain–computer interface	Sacha Leinders;Elmar Pels;Mariska J. Vansteensel;Mariana P. Branco;Zachary Freudenburg;Detan Liu;R. H. Joon-Yeon Kim;Erik J. Aarnoutse;Nick F. Ramsey	2017		10.3217/978-3-85125-533-1-50		ML	12.237998422134389	-94.04128812315794	11911
574b5ea227732048dc5ea3c06501cd5b0e7635b1	user guided biomedical image segmentation and usable interfaces			image segmentation	Mohammadreza Hosseini	2016				Vision	-0.686738023176205	-76.76335625309191	11921
1dc4cd1f955c4e2df146be9af2538aadf6a6d29c	fatigue estimation with a multivariable myoelectric mapping function	fatigue;neural nets;biomechanics;dynamic contraction;muscle fatigue;multivariable mapping;muscle contraction muscle fatigue estimation multivariable myoelectric mapping function artificial neural network time domain features;neural network dynamic contraction multivariable mapping muscle fatigue myoelectric parameter;time domain;action potentials adult aged algorithms computer simulation diagnosis computer assisted electromyography female humans models biological models statistical multivariate analysis muscle contraction muscle fatigue muscle skeletal nerve net pattern recognition automated;electromyography;myoelectric parameter;signal to noise ratio;medical signal processing;artificial neural network;fatigue life estimation muscles frequency niobium biomedical measurements testing signal mapping biomedical engineering artificial neural networks;neural network;neural nets fatigue biomechanics electromyography medical signal processing	A novel approach to muscle fatigue assessment is proposed. A function is used to map multiple myoelectric parameters representing segments of myoelectric data to a fatigue estimate for that segment. An artificial neural network is used to tune the mapping function and time-domain features are used as inputs. Two fatigue tests were conducted on five participants in each of static, cyclic and random conditions. The function was tuned with one data set and tested on the other. Performance was evaluated based on a signal to noise metric which compared variability due to fatigue factors with variability due to nonfatiguing factors. Signal to noise ratios for the mapping function ranged from 7.89 under random conditions to 9.69 under static conditions compared to 3.34-6.74 for mean frequency and 2.12-2.63 for instantaneous mean frequency indicating that the mapping function tracks the myoelectric manifestations of fatigue better than either mean frequency or instantaneous mean frequency under all three contraction conditions.	artificial neural network;chronic fatigue syndrome;color gradient;heart rate variability;memory-level parallelism	Dawn MacIsaac;Philip A. Parker;Kevin B. Englehart;Daniel R. Rogers	2006	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2006.870220	control engineering;electronic engineering;speech recognition;time domain;computer science;engineering;machine learning;signal-to-noise ratio;artificial neural network	Visualization	16.859730058858975	-86.1513200678667	11925
ba4c66bfd56722df295c4bacd530317d67959e33	sleep stage classification based on respiratory signal	sleep bioelectric potentials biomechanics eye feature extraction lung medical disorders medical signal detection medical signal processing neurophysiology plethysmography signal classification;home sleep monitoring system sleep stage classification rapid eye movement sleep nonrem sleep detection feature extraction respiratory inductive plethysmography signal bagging classifier heuristic based knowledge sleep related breathing disorders leave one subject out cross validation procedure cohen kappa automated sleep structure detection;sleep apnea feature extraction accuracy heart rate variability monitoring bagging	One of the research tasks, which should be solved to develop a sleep monitor, is sleep stages classification. This paper presents an algorithm for wakefulness, rapid eye movement sleep (REM) and non-REM sleep detection based on a set of 33 features, extracted from respiratory inductive plethysmography signal, and bagging classifier. Furthermore, a few heuristics based on knowledge about normal sleep structure are suggested. We used the data from 29 subjects without sleep-related breathing disorders who underwent a PSG study at a sleep laboratory. Subjects were directed to the PSG study due to suspected sleep disorders. A leave-one-subject-out cross-validation procedure was used for testing the classification performance. The accuracy of 77.85 ± 6.63 and Cohen's kappa of 0.59 ± 0.11 were achieved for the classifier. Using heuristics we increased the accuracy to 80.38 ± 8.32 and the kappa to 0.65 ± 0.13. We conclude that heuristics may improve the automated sleep structure detection based on the analysis of indirect information such as respiration signal and are useful for the development of home sleep monitoring system.	algorithm;cross reactions;cross-validation (statistics);extraction;eye movements;heuristics;plethysmography;polysomnography;programmable sound generator;sleep apnea syndromes;sleep disorders;sleep, rem;statistical classification;wakefulness;nervous system disorder	Alexander Tataraidze;Lesya Anishchenko;Lyudmila Korostovtseva;Bert Jan Kooij;Mikhail Bochkarev;Yurii Sviryaev	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318373	speech recognition;medicine;pathology;communication	SE	15.201148606565487	-88.6036089283292	11940
2587434b3887d8785c7e4043e3855b8689ce5d6b	exploring generalized shape analysis by topological representations		One of the most common properties of various data in pattern recognition is the shape, and the shape matters. However, the shape can appear with uncertain appearances, e.g., the shapes of a person in different poses. We realize that the most fundamental feature of any shape is the number of connected components, the number of holes and its higher dimensional counterparts. These are what we call topological invariants. This is the place where topology comes into play for pattern recognition. Persistent homology, one of the most powerful tools in algebraic topology, is proposed to compute these topological invariants at different resolutions. The proposed method, by firstly transferring the given data into a topological graph representation, i.e., the simplicial complex, can assemble discrete points into a global structure. Then by integrating with multiple filtrations and metric learning, both the global structure and different local parts can be taken into account at the same time. We test the proposed method in 2D shape classification, 2.5D gait identification and 3D facial expression recognition. Experimental results demonstrate the effectiveness of this generalized shape analysis method and show its potentials in different applications. Moreover, we provide a new insight for the generalized shape analysis. © 2016 Elsevier B.V. All rights reserved.	2.5d;connected component (graph theory);graph (abstract data type);homology (biology);linear algebra;pattern recognition;persistent homology;shape analysis (digital geometry);shape context;simplicial complex;topological graph	Zhen Zhou;Yongzhen Huang;Liang Wang;Tieniu Tan	2017	Pattern Recognition Letters	10.1016/j.patrec.2016.04.002	active shape model;combinatorics;topology;machine learning;shape analysis;mathematics;geometry;topological skeleton	Vision	49.74629937710507	-62.10846647286923	11946
ee4e8c34294f2ce26f58949c272e8a0e109778d4	identification of immune signatures predictive of clinical protection from malaria		Antibodies are thought to play an essential role in naturally acquired immunity to malaria. Prospective cohort studies have frequently shown how continuous exposure to the malaria parasite Plasmodium falciparum cause an accumulation of specific responses against various antigens that correlate with a decreased risk of clinical malaria episodes. However, small effect sizes and the often polymorphic nature of immunogenic parasite proteins make the robust identification of the true targets of protective immunity ambiguous. Furthermore, the degree of individual-level protection conferred by elevated responses to these antigens has not yet been explored. Here we applied a machine learning approach to identify immune signatures predictive of individual-level protection against clinical disease. We find that commonly assumed immune correlates are poor predictors of clinical protection in children. On the other hand, antibody profiles predictive of an individual's malaria protective status can be found in data comprising responses to a large set of diverse parasite proteins. We show that this pattern emerges only after years of continuous exposure to the malaria parasite, whereas susceptibility to clinical episodes in young hosts (< 10 years) cannot be ascertained by measured antibody responses alone.	adaptive immunity;antivirus software;assumed;emergence;machine learning;malaria vaccines;prospective search;tree accumulation	John Joseph Valletta;Mario Recker	2017		10.1371/journal.pcbi.1005812	malaria;antibody;biology;bioinformatics;acquired immune system;disease;antigen;immunity;immunology;immune system;plasmodium falciparum	ML	5.675720312753757	-62.35074991015141	11967
c27d7c5ef0466f3dcbcc9579a028ace950fd228f	ultrasonic system for shape recognition based on frequency analysis and computational intelligence	computational intelligence;frequency analysis		computation;computational intelligence;frequency analysis	Andrés D. Restrepo;Humberto Loaiza;Eduardo Caicedo Bravo	2008	Inteligencia Artificial, Revista Iberoamericana de Inteligencia Artificial			AI	28.443911218289387	-68.02474362055645	11984
06561775e57fcffe8ba42447d38da4d3727e2093	tracking pylorus in ultrasonic image sequences with edge-based optical flow	smoothness pylorus ultrasonic image sequences edge based optical flow duodenogastric reflux prediction method segmentation method active contour pyloric tracking spatiotemporal information hausdorff distance average distance mean edge distance edge curvature minimum distance metrics;image motion analysis;active contour;image segmentation;edge detection;average distance;temporal information;computer vision;optical imaging;prediction theory;image edge detection;minimum distance;medical image processing;indexation;image sequence;spatiotemporal phenomena;computer simulation humans image processing computer assisted pylorus video recording;hausdorff distance;the snake;pyloric tracking;the snake edge tracking optical flow pyloric tracking;optical flow;target tracking;edge tracking;image motion analysis computer vision optical imaging target tracking adaptive optics image edge detection;biomedical ultrasonics;spatiotemporal phenomena biomedical ultrasonics edge detection image segmentation image sequences medical image processing prediction theory;adaptive optics;image sequences	Tracking pylorus in ultrasonic image sequences is an important step in the analysis of duodenogastric reflux (DGR). We propose a joint prediction and segmentation method (JPS) which combines optical flow with active contour to track pylorus. The goal of the proposed method is to improve the pyloric tracking accuracy by taking account of not only the connection information among edge points but also the spatio-temporal information among consecutive frames. The proposed method is compared with other four tracking methods by using both synthetic and real ultrasonic image sequences. Several numerical indexes: Hausdorff distance (HD), average distance (AD), mean edge distance (MED), and edge curvature (EC) have been calculated to evaluate the performance of each method. JPS achieves the minimum distance metrics (HD, AD, and MED) and a smaller EC. The experimental results indicate that JPS gives a better tracking performance than others by the best agreement with the gold curves while keeping the smoothness of the result.	active contour model;duodenogastric reflux;edge enhancement;experiment;frame (physical object);hausdorff dimension;index;jump point search;near edge x-ray absorption fine structure spectroscopy;numerical analysis;optical flow;pylorus;small;synthetic intelligence;tree accumulation;ultrasonics (sound)	Chaojie Chen;Yuanyuan Wang;Jinhua Yu;Zhuyu Zhou;Li Shen;Yaqing Chen	2012	IEEE Transactions on Medical Imaging	10.1109/TMI.2012.2183884	hausdorff distance;computer vision;mathematical optimization;edge detection;computer science;pattern recognition;optical imaging;active contour model;optical flow;mathematics;image segmentation;adaptive optics	Vision	46.703591566425516	-74.17414351459124	11996
d754095a2eb35e0333415fadf11d652ac0b45558	individual differences in language processing: electrophysiological approaches		Abstract#R##N##R##N#Language processing is a complex task that requires both specialized cognitive processes (e.g. speech decoding) and more general cognitive processes (e.g. working memory). Research on how individual differences in these processes influence language processing and comprehension has primarily relied on behavioral methods, such as reaction time measures, self-paced reading, and eye-tracking. However, a growing number of studies have used electrophysiological (EEG) techniques to study individual differences in language processing. EEG and event-related potential (ERP) methods provide a unique link between neural activity and cognitive processing and can be used to draw specific inferences about the neural basis of language processing and its variability. The primary goal of this paper is to showcase EEG/ERP studies that have made significant contributions to the study of individual differences in how the brain processes language, over and above what would be possible using behavioral methods alone. A secondary goal of this paper is to highlight several methodological issues specific to research on individual differences in language processing and identify ways in which EEG/ERP studies can take advantage of what has been learned from previous research to minimize these issues.		Megan A. Boudewyn	2015	Language and Linguistics Compass	10.1111/lnc3.12167	cognitive psychology;developmental psychology;computer science;communication	NLP	14.360763663362537	-78.2615612482345	12025
cbe568999a2659dd5bef9189fc43a0f808b1c493	capacity limits in oscillatory networks: implications for sensory coding	object recognition;accuracy synchronization oscillators visualization tuning histograms object recognition;multitasking oscillatory network sensory coding capacity limit sensory encoding stage synchronous activity object recognition network oscillatory elements phase synchronization tuning function quantitative modeling sensory representation working memory;brain models;psychology;synchronisation brain models object recognition psychology;synchronisation	Psychological studies have investigated limits in the capacity to simultaneously store multiple objects in working memory, which turns out to be approximately four. In this paper we examine the existence and origin of such a capacity limit in the sensory encoding stage, where synchronous activity can be considered to group related features of a common object. We develop a model of an object recognition network using oscillatory elements that can achieve phase synchronization. Using simulations based on this network, we show that distinct phases of oscillation can be used to label combinations of objects presented simultaneously. This allows the network to separate mixtures of objects, and identify the input elements that belong to each object. We demonstrate that there is a limit of four objects that can be separated from a mixture. Further studies are required to generalize this result by varying the size of the network and the number of objects used for training. We also show that by narrowing a tuning function that governs the dynamics of the system, we can achieve higher separation accuracy. However, this comes at the cost of utilizing a higher number of iterations over which the system settles and learns its synaptic weights. We lay down a framework for the quantitative modeling of factors affecting capacity limits, which has the potential to advance our understanding of sensory representation, attention, working memory and multitasking.	cognition;computable function;computer multitasking;iteration;mathematical model;outline of object recognition;sensory neuroscience;simulation;synaptic package manager;synaptic weight	A. Ravishankar Rao;Guillermo A. Cecchi	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706847	synchronization;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning	ML	20.20108420922537	-68.33301664387763	12030
800c322eda8996145870cbac4c60598d67848866	colour normalization of fundus images based on geometric transformations applied to their chromatic histogram		The high variability in fundus image databases is an important limiting drawback for detecting some retinal pathologies automatically. Age, human retinal pigmentation or lighting conditions affects in the colour of the acquired images. In this paper a colour-normalization method is presented as an initial pre-processing step in order to reduce the heterogeneity of retinal databases. The proposed method is based on geometric transformations applied to the chromaticity diagram of a target image taking into account a reference image. With the aim of quantifying the effect of the proposed colour normalization, a bright lesion detection from pathological images is carried out. A home-made system based on texture analysis and Support Vector Machine classification is used for this purpose. An improvement around a three percent in the detection accuracy demonstrates the importance of a retinal image colour pre-processing before any specific analysis.	database;diagram;preprocessor;sensor;spatial variability;support vector machine	Adrián Colomer;Valery Naranjo;Jesús Angulo	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296860	chromatic scale;support vector machine;computer vision;chromaticity;retinal;normalization (statistics);artificial intelligence;histogram;computer science;pattern recognition;transformation geometry;fundus (eye)	Vision	36.08224209796438	-75.4395093641324	12034
75d2478bae89db33615cf8deb4a61b00fc137b86	quantitative evaluation of normalization techniques of matching scores in multimodal biometric systems	piecewise linear;roc curve;quantitative evaluation	This paper attempts to make an quantitative evaluation of available normalization techniques of matching scores in multimodal biometric systems. Two new normalization techniques Four Segments Piecewise Linear (FSPL) and Linear Tanh Linear (LTL) have been proposed in this paper. FSPL normalization techniques divides the region of genuine and impostor scores into four segments and maps each segment using piecewise linear function while LTL normalization techniques maps the non-overlap region of genuine and impostor score distributions to a constant function and overlap region using tanh estimator. The effectiveness of each technique is shown using EER and ROC curves on IITK database of having more than 600 people on following characteristics: face, fingerprint, and offline-signature. The proposed normalization techniques perform better and particularly, LTL normalization is efficient and robust.	biometrics;constant function;database normalization;enhanced entity–relationship model;fingerprint;linear function;linear logic;map;multimodal interaction;online and offline;piecewise linear continuation;receiver operating characteristic;robustness (computer science)	Yogendra Narain Singh;Phalguni Gupta	2007		10.1007/978-3-540-74549-5_61	econometrics;piecewise linear function;computer science;machine learning;pattern recognition;mathematics;geometry;receiver operating characteristic;statistics	Web+IR	33.543652491037385	-60.96668525192726	12047
7e2745594f67f537f1ce96d3edf338befda57df4	field data extraction for form document processing using a gravitation-based algorithm	field data grouping;gravitation based algorithm;field data;document processing;form document processing;connected component;article;locality property	"""This paper presents a novel approach to grouping Chinese handwritten """"eld data """"lled in form documents using a gravitation-based algorithm. An algorithm is developed to extract handwritten """"eld data which may be written out of form """"elds. First, form lines are extracted and removed from input form images. Connected-components are then detected from remaining data, and the gravitation for each connected-component is computed by using the black pixel counts as their mass. Next, we move connected-components according to their gravitation. As generally known, """"lled-in data have the locality property, i.e., data of the same """"eld are normally written in a local area consecutively. Therefore, the relationship of these connected-components can be determined by this property. Repeatedly moving these connectedcomponents according to their neighbor components allows us to determine which connected-components should be extracted for a particular """"eld. Experimental results demonstrate the e!ectiveness of the proposed method in grouping """"eld data. 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	algorithm;connected component (graph theory);document processing;locality of reference;pattern recognition;pixel;xfig	Jiun-Lin Chen;Hsi-Jian Lee	2001	Pattern Recognition	10.1016/S0031-3203(00)00115-1	connected component;document processing;theoretical computer science;data mining;mathematics;algorithm	AI	38.20819755577741	-65.77433697517701	12050
29c9d3b9f53a0a4ec7ef8f991ac1ac0362990e34	a mean eigenwindow method for partially occluded/destroyed objects recognition	object recognition;indexing terms;computer vision	This paper describes a method for recognizing partially occluded and/or destroyed objects using an eigenspace method referred to as a ‘mean eigenwindow’ method that stores multiple partially occluded/destroyed objects in an eigenspace. We have proposed to store similar poses, that may include disturbed shapes, of an object in a particular window referred to as the ‘eigen window’ and, finally, mean of appearances of each window is taken into consideration in order to obtain a generalized eigen window called the ‘mean eigenwindow’. This mean eigenwindow is further used for recognizing an unfamiliar pose, including partially occluded or destroyed shapes, and the object type itself. We have applied the proposed approach to various image situations and the method has successfully performed recognition of an object with up to 20% of occlusion and/or destruction.	eigen (c++ library);object type (object-oriented programming)	Mohammad Masudur Rahman;Seiji Ishikawa	2003			computer vision;index term;computer science;cognitive neuroscience of visual object recognition;pattern recognition;mathematics	Vision	43.39136843108272	-53.9539363231811	12065
df891988f738b2b7f8f3eeb315ed3813a2c72f82	computational tools and techniques for early diagnosis and screening of geriatric diseases			immune system diseases	Hyuntae Park;Fimiharu Togo;Masashi Miyashita	2018		10.1155/2018/7830584	machine learning;biomedical engineering;artificial intelligence;computer science	HCI	9.682520022304228	-80.20018703133222	12069
744ce2cf3ae52989de8955e1a6c702bbe5543f37	drowsiness detection using fnirs in different time windows for a passive bci	photonics;brain;signal classification brain computer interfaces medical signal processing neurophysiology patient monitoring road safety;automobiles;hemodynamics brain monitoring linear discriminant analysis electroencephalography photonics automobiles;hemodynamics;monitoring;brain computer interface drowsiness activity detection time windows passive bci dorsolateral prefrontal cortex functional near infrared spectroscopy simulated driving task brain activity fnirs monitoring signal mean signal slope linear discriminant analysis classification accuracy;electroencephalography;linear discriminant analysis	In this research, we have investigated the detection of drowsiness activity in dorsolateral-prefrontal cortex in three different time windows (0~3 sec, 0~4 sec and 0~5 sec) using functional near-infrared spectroscopy (fNIRS). Five drowsy subjects participated in a simulated driving task while their brain activity is monitored using fNIRS. The recorded brain activity is segmented into three windows for the acquisition of signal mean, signal slope and number of peaks as features. The data in each window is classified using linear discriminant analysis to find best window size. The results show that the best accuracy is obtained using 0~5 sec window after classification. Although the classification accuracy in 0~4 sec window is lower than in 0~5 sec window, both accuracies are suitable for brain-computer interface applications (i.e. accuracy>70%). The accuracy in 0~3 sec window is less than 70% for two subjects. For driver drowsiness detection, high accuracy with quick detection time is required, therefore we propose drowsiness detection in 0~4 sec window using fNIRS monitoring.	brain–computer interface;detection theory;electroencephalography;linear discriminant analysis;microsoft windows	M. Jawad Khan;Xiaolong Liu;M. Raheel Bhutta;Keum Shik Hong	2016	2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)	10.1109/BIOROB.2016.7523628	computer vision;speech recognition;computer science;communication	Robotics	13.998363496818639	-92.0532830681086	12077
549ba23bb00359d256c6856978f1126f43a88712	optic nerve head detection via group correlations in multi-orientation transforms		Optic nerve head detection is a fundamental step in automated retinal image analysis algorithms. In this paper, we propose a new optic nerve head detection algorithm that is based on the efficient analysis of multi-orientation patterns. To this end, we make use of invertible orientation scores, which are functions on the Euclidean motion group. We apply the classical and fast approach of template matching via cross-correlation, however, we do this in the domain of an orientation score rather than the usual image domain. As such, this approach makes it possible to efficiently detect multi-orientation patterns. The method is extensively tested on public and private databases and we show that the method is generically applicable to images originating from traditional fundus cameras as well as from scanning laser ophthalmoscopes.	optic nerve (gchq)	Erik Johannes Bekkers;Remco Duits;Bart M. ter Haar Romeny	2014		10.1007/978-3-319-11755-3_33	retina;pattern recognition;computer vision;artificial intelligence;computer science;invertible matrix;optic nerve;euclidean geometry;template matching;ophthalmoscopes;fundus (eye)	Vision	34.83995045513832	-56.90197005656801	12093
f9386117eb10bf083e15a910dcf46ab553595510	pop-out: a new cognitive model of visual attention that uses light level analysis to better mimic the free-viewing task of static images		Human gaze is not directed to the same part of an image when lighting conditions change. Current saliency models do not consider light level analysis during their bottom-up processes. In this paper, we introduce a new saliency model which better mimics physiological and psychological processes of our visual attention in case of free-viewing task (bottom-up process). This model analyzes lighting conditions with the aim of giving different weights to color wavelengths. The resulting saliency measure performs better than a lot of popular cognitive approaches.	cognitive model	Makiese Mibulumukini	2015	Adv. Artificial Intellegence	10.1155/2015/471483	computer vision;simulation	HCI	23.226307205677042	-65.85675131789118	12098
ebec200b2e4664feeb7e9c5f7b627689ff70c151	pln24nt: a web resource for plant 24-nt sirna producing loci		Abstract In plants, 24 nucleotide small interfering RNAs (24-nt siRNAs) account for a large percentage of the total siRNA pool, and they play an important role in guiding plant-specific RNA-directed DNA methylation (RdDM), which transcriptionally silences transposon elements, transgenes, repetitive sequences and some endogenous genes. Several loci in plant genomes produce clusters of 24-nt RNAs, and these loci are receiving increasing attention from the research community. However, at present there is no bioinformatics resource dedicated to 24-nt siRNA loci and their derived 24-nt siRNAs. Thus, in this study, Pln24NT, a freely available web resource, was created to centralize 24-nt siRNA loci and 24-nt siRNA information, including fundamental locus information, expression profiles and annotation of transposon elements, from next-generation sequencing (NGS) data for 10 popular plant species. An intuitive web interface was also developed for convenient searching and browsing, and analytical tools were included to help users flexibly analyze their own siRNA NGS data. Pln24NT will help the plant research community to discover and characterize 24-nt siRNAs, and may prove useful for studying the roles of siRNA in RNA-directed DNA methylation in plants.   Availability and Implementation http://bioinformatics.caf.ac.cn/Pln24NT .   Contact suxh@caf.ac.cn.   Supplementary information Supplementary data are available at Bioinformatics online.	annotation;bioinformatics;biopolymer sequencing;centralisation;communications satellite;genome;genome, plant;geographic information systems;interface device component;jumping genes;locus;massively-parallel sequencing;nucleotides;rna;rna, small interfering;rna-directed dna methylation;repetitive region;transgenes;user interface;web resource;negative regulation of reactive oxygen species biosynthetic process	Qi Liu;Changjun Ding;Yanguang Chu;Weixi Zhang;Ganggang Guo;Jiafei Chen;Xiaohua Su	2017	Bioinformatics	10.1093/bioinformatics/btx096	web resource;computer science;locus (genetics);bioinformatics	Comp.	-0.5165799320472182	-59.647743350171254	12106
6a62759e12757e0c12282cfde05685ea33052457	electrophoresis for genotyping: temporal thermal gradient gel electrophoresis for profiling of oligonucleotide dissociation	verification;genes;hydrogen ion concentration;low density lipoprotein receptor;base pairing;polyacrylamide gel;gel electrophoresis;genotype;mobility;electrophoresis polyacrylamide gel;electrophoresis;thermal gradient;autoradiography;binding sites;magnesium chloride;antisense;polymerase chain reaction;heterozygote;oligonucleotides;hot temperature;ldl;receptors ldl;humans;molecular sequence data;hyperlipoproteinemia type ii;oligonucleotides antisense;gel;oligonucleotide probes;mismatch;base sequence;receptors;mutation;exons	Traditional use of an oligonucleotide probe to determine genotype depends on perfect base pairing to a single-stranded target which is stable to a higher temperature than when imperfect binding occurs due to a mismatch in the target sequence. Bound oligonucleotide is detected at a predetermined single temperature 'snapshot' of the melting profile, allowing the distinction of perfect from imperfect base pairing. In heterozygotes, the presence of the alternative sequence must be verified with a second oligonucleotide complementary to the variant. Here we describe a system of real-time variable temperature electrophoresis during which the oligonucleotide dissociates from its target. In 20% polyacrylamide the target strand has minimal mobility and released oligonucleotide migrates extremely quickly so that the 'freed' rather than the 'bound' is displayed. The full profile of oligonucleotide dissociation during gel electrophoresis is represented along the gel track, and a single oligonucleotide is sufficient to confirm heterozygosity, since the profile displays two separate peaks. Resolution is great, with use of short track lengths enabling analysis of dense arrays of samples. Each gel track can contain a different target or oligonucleotide and the temperature gradient can accommodate oligonucleotides of different melting temperatures. This provides a convenient system to examine the interaction of many different oligonucleotides and target sequences simultaneously and requires no prior knowledge of the mutant sequence(s) nor of oligonucleotide melting temperatures. The application of the technique is described for screening of a hotspot for mutations in the LDL receptor gene in patients with familial hypercholesterolaemia.	base pairing;gel electrophoresis (lab technique);genetic hotspot;genotype determination;gradient;heterozygote;hypercholesterolemia;hypercholesterolemia, familial;java hotspot virtual machine;mutation;nucleic acid strand;oligonucleotides;patients;real-time clock;snapshot (computer storage);strand (programming language);polyacrylamide	I. N. Day;S. D. O'Dell;I. D. Cash;Steve E Humphries;G. P. Weavind	1995	Nucleic acids research	10.1093/nar/23.13.2404	mutation;heterozygote advantage;ldl receptor;biology;molecular biology;verification;electrophoresis;base pair;exon;receptor;bioinformatics;binding site;polymerase chain reaction;gel electrophoresis;gene;genotype;magnesium;mobile computing;genetics;polyacrylamide gel electrophoresis;sense;temperature gradient;oligonucleotide	Visualization	4.683498632649479	-64.4087432355186	12115
8169210f2869b106851bf283b4f82386579eca3d	a benchmark database of visible and thermal paired face images across multiple variations		Although visible face recognition systems have grown as a major area of research, they are still facing serious challenges when operating in uncontrolled environments. In attempt to overcome these limitations, thermal imagery has been investigated as a promising direction to extend face recognition technology. However, the reduced number of databases acquired in thermal spectrum limits its exploration. In this paper, we introduce a database of face images acquired simultaneously in visible and thermal spectra under various variations: illumination, expression, pose and occlusion. Then, we present a comparative study of face recognition performances on both modalities against each variation and the impact of bimodal fusion. We prove that thermal spectrum rivals with the visible spectrum not only in the presence of illumination changes, but also in case of expression and poses changes.		Christoph Busch;Antitza Dantcheva;Christian Rathgeb;Khawla Mallat;Jean-Luc Dugelay	2018	2018 International Conference of the Biometrics Special Interest Group (BIOSIG)	10.23919/BIOSIG.2018.8553431	artificial intelligence;computer vision;computer science;pattern recognition;database;statistical classification;visible spectrum;thermal;thermal radiation;multiple variations;facial recognition system	Vision	44.022274945776445	-56.98938719601032	12125
d8f9108e26af682f92b21b6012639ead16ec158f	a bof model based cbcd system using hierarchical indexing and feature similarity constraints	inverted index;hierarchical indexing;interest points;feature similarity constraints;video copy detection;local features;indexation;bag of visual words	Recently, local interest points (also known as key points) are shown to be useful for content based video copy detection. The state-of-art local feature based methods usually build on the bag-of-visual-words model and utilize the inverted index to accelerate search process. In this paper, we offer a detailed description of a novel CBCD system. Compared with the existing local feature based approaches, there are two major differences. First, besides the descriptors, the dominant orientations of local features are also quantized to build the hierarchical inverted index. Second, feature similarity constraints are used to refine the matching of visual words. Experiments performed on a reference video dataset of 50 hours show that our system can deal with 9 types of common video transformations, and due to the hierarchical indexing and feature similarity constraints, the computational costs are reduced as well.	bag-of-words model in computer vision;computation;experiment;inverted index;quantization (signal processing);video copy detection	Nan Nan;Guizhong Liu;Chen Wang	2010		10.1145/1937728.1937771	computer science;pattern recognition;data mining;information retrieval	Vision	36.81517886521961	-55.460816221947766	12134
06e932dbd2fe957e128090a889c271ff776b5fbe	algorithmic and architectural optimizations for computationally efficient particle filtering	gaussian noise;systeme dynamique non lineaire;filtre particule;nonlinear filters;filtering;mcmc algorithm;evaluation performance;optimisation;filtrage;convex program;particle filtering algorithm;chaine markov;cadena markov;metodo monte carlo;auxillary variable;performance evaluation;monte carlo markov chain;optimizacion;video signal processing;convex programming;filtering algorithms navigation particle filters state estimation hardware algorithm design and analysis clustering algorithms particle tracking nonlinear filters nonlinear systems;critere conception;implementation;metropolis hastings;nonlinear dynamical systems;real time;evaluacion prestacion;filtrado;simulacion numerica;methode monte carlo;design criterion;temps minimal;video sequences;paralelisacion;programmation convexe;ruido no gaussiano;filtro particulas;indexing terms;non gaussian noise;state estimation;visual tracking pipelined architectural optimization particle filtering algorithm video sequences nonlinear dynamical system filtering nongaussian noise process independent metropolis hastings sampler convex program;processing time;monte carlo markov chain mcmc;navigation;nonlinear systems;senal video;signal video;filtering algorithms;optical tracking;particle filter;video signal processing convex programming image sequences optical tracking particle filtering numerical methods pipeline processing sampling methods;monte carlo method;parallelisation;simulation numerique;image sequence;poursuite cible;algoritmo mcmc;independent metropolis hastings sampler;parallelization;minimum time;clustering algorithms;video signal;visual tracking auxillary variable design methodologies monte carlo markov chain mcmc particle filter resampling;pipelined architectural optimization;temps traitement;nongaussian noise process;nonlinear dynamical system filtering;secuencia imagen;optimization;methode reechantillonnage;applications of visualization;design methodologies;particle tracking;algorithme mcmc;resampling method	In this paper, we analyze the computational challenges in implementing particle filtering, especially to video sequences. Particle filtering is a technique used for filtering nonlinear dynamical systems driven by non-Gaussian noise processes. It has found widespread applications in detection, navigation, and tracking problems. Although, in general, particle filtering methods yield improved results, it is difficult to achieve real time performance. In this paper, we analyze the computational drawbacks of traditional particle filtering algorithms, and present a method for implementing the particle filter using the Independent Metropolis Hastings sampler, that is highly amenable to pipelined implementations and parallelization. We analyze the implementations of the proposed algorithm, and, in particular, concentrate on implementations that have minimum processing times. It is shown that the design parameters for the fastest implementation can be chosen by solving a set of convex programs. The proposed computational methodology was verified using a cluster of PCs for the application of visual tracking. We demonstrate a linear speedup of the algorithm using the methodology proposed in the paper.	area striata structure;computational technique;convex optimization;dynamical system;fastest;metropolis;metropolis–hastings algorithm;nonlinear system;normal statistical distribution;parallel computing;particle filter;programming paradigm;sampling (signal processing);specification;speedup;video tracking	Aswin C. Sankaranarayanan;Ankur Srivastava;Rama Chellappa	2008	IEEE Transactions on Image Processing	10.1109/TIP.2008.920760	computer simulation;mathematical optimization;convex optimization;simulation;particle filter;nonlinear system;computer science;theoretical computer science;mathematics;statistics	Visualization	52.38302388465137	-59.812420087967496	12183
e607b998b250e1970e760bce102efb5f4e00dab6	a cluster-based approach for efficient content-based image retrieval using a similarity-preserving space transformation method	proposed space transformation method;original low-level image space;metric space;high-level vector space;efficient content-based image retrieval;cbir approach;original space;color-based retrieval;similarity-preserving space transformation method;feature space;space transformation;cluster-based approach	The techniques of clustering and space transformation have been successfully used in the past to solve a number of pattern recognition problems. In this article, the authors propose a new approach to content-based image retrieval (CBIR) that uses (a) a newly proposed similarity-preserving space transformation method to transform the original low-level image space into a high-level vector space that enables efficient query processing, and (b) a clustering scheme that further improves the efficiency of our retrieval system. This combination is unique and the resulting system provides synergistic advantages of using both clustering and space transformation. The proposed space transformation method is shown to preserve the order of the distances in the transformed feature space. This strategy makes this approach to retrieval generic as it can be applied to object types, other than images, and feature spaces more general than metric spaces. The CBIR approach uses the inexpensive “estimated” distance in the transformed space, as opposed to the computationally inefficient “real” distance in the original space, to retrieve the desired results for a given query image. The authors also provide a theoretical analysis of the complexity of their CBIR approach when used for color-based retrieval, which shows that it is computationally more efficient than other comparable approaches. An extensive set of experiments to test the efficiency and effectiveness of the proposed approach has been performed. The results show that the approach offers superior response time (improvement of 1–2 orders of magnitude compared to retrieval approaches that either use pruning techniques like indexing, clustering, etc., or space transformation, but not both) with sufficiently high retrieval accuracy. © 2006 Wiley Periodicals, Inc.	content-based image retrieval;householder transformation	Biren Shah;Vijay V. Raghavan;Praveen Dhatric;Xiaoquan Zhao	2006	JASIST	10.1002/asi.20357	method;similarity;vector space;image retrieval;computer science;artificial intelligence;similitude;comparative research;efficiency;algorithm;cluster	Vision	42.162285485004936	-60.61699101862283	12191
ba37eb54cc47996994331ea17d63540a7d862e7c	machine learning approach to dissimilarity computation: iris matching	training;iris recognition;vegetation;computer vision;image edge detection;pattern recognition	This paper presents a novel approach for iris dissimilarity computation based on Machine Learning paradigms and Computer Vision transformations. Based on the training dataset given by the MICHE II Challenge organizers, a set of classifiers has been constructed and tested, aiming at classifying a single image.	algorithm;autostereogram;computation;computer vision;experiment;machine learning;mobile device;statistical classification	Naiara Aginako;José María Martínez-Otzeta;Igor Rodriguez Rodriguez;Elena Lazkano;Basilio Sierra	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899628	computer vision;computer science;machine learning;pattern recognition;iris recognition;vegetation	Vision	31.455071659391148	-56.546315260198575	12193
881efd4e017759df8406ccfc65d9ccf35f046f21	lead-dbs: a toolbox for deep brain stimulation electrode localizations and visualizations		To determine placement of electrodes after deep brain stimulation (DBS) surgery, a novel toolbox that facilitates both reconstruction of the lead electrode trajectory and the contact placement is introduced. Using the toolbox, electrode placement can be reconstructed and visualized based on the electrode-induced artifacts on post-operative magnetic resonance (MR) or computed tomography (CT) images. Correct electrode placement is essential for efficacious treatment with DBS. Post-operative knowledge about the placement of DBS electrode contacts and trajectories is a promising tool for clinical evaluation of DBS effects and adverse effects. It may help clinicians in identifying the best stimulation contacts based on anatomical target areas and may even shorten test stimulation protocols in the future. Fifty patients that underwent DBS surgery were analyzed in this study. After normalizing the post-operative MR/CT volumes into standard Montreal Neurological Institute (MNI)-stereotactic space, electrode leads (n=104) were detected by a novel algorithm that iteratively thresholds each axial slice and isolates the centroids of the electrode artifacts within the MR/CT-images (MR only n=32, CT only n=10, MR and CT n=8). Two patients received four, the others received two quadripolar DBS leads bilaterally, summing up to a total of 120 lead localizations. In a second reconstruction step, electrode contacts along the lead trajectories were reconstructed by using templates of electrode tips that had been manually created beforehand. Reconstructions that were made by the algorithm were finally compared to manual surveys of contact localizations. The algorithm was able to robustly accomplish lead reconstructions in an automated manner in 98% of electrodes and contact reconstructions in 69% of electrodes. Using additional subsequent manual refinement of the reconstructed contact positions, 118 of 120 electrode lead and contact reconstructions could be localized using the toolbox. Taken together, the toolbox presented here allows for a precise and fast reconstruction of DBS contacts by proposing a semi-automated procedure. Reconstruction results can be directly exported to two- and three-dimensional views that show the relationship between DBS contacts and anatomical target regions. The toolbox is made available to the public in form of an open-source MATLAB repository.	algorithm;ct scan;cervical atlas;clinical use template;deep brain stimulation;matlab;morphologic artifacts;music visualization;open-source software;patients;protocols documentation;refinement (computing);resonance;semiconductor industry;simulation;sixty nine;x-ray computed tomography;electrode;nervous system disorder	Andreas Horn;Andrea A. Kühn	2015	NeuroImage	10.1016/j.neuroimage.2014.12.002	surgery	Visualization	39.30250217513231	-83.44455606208292	12198
ddc03ed1892970a81a727b22f7f03ee9a3b7e12c	a sas macro for the analysis of multivariate longitudinal binary outcomes	multivariate longitudinal binary outcomes;marginal models;statistical method;gee;marginal model;not significant;oral health;log odds;logistic regression model;health care	Multiple binary outcomes occur quite frequently in oral health research, as well as other areas of health care research. When there is interest in comparing whether covariates influence one outcome more than another, statistical methods that adjust for the correlation that may exist between outcomes are warranted. Available software is limited to the extent that some pre-processing of the data is required. The main objective of this paper is to describe a SAS macro that can be used to estimate separate covariate effects on multiple, correlated binary outcomes. We demonstrate the utility of the macro by applying it to fit a trivariate logistic regression model using GEE where the three correlated longitudinal outcomes of interest include whether a subject had a problem-oriented visit, a dental cleaning, or a routine check-up, or some combination thereof. All three outcomes were measured at four 6-monthly intervals (0-24 months). Estimates from the trivariate logistic regression model are compared to results obtained by fitting three separate binary longitudinal models using GEE for each oral health outcome. The odds of having a problem-oriented visit were greater for males compared to females as estimated from the multivariate model (P = 0.0407), but the odds were not significant in the univariate model (P = 0.0641). The multivariate model also aided in confirming expected results that consistent regular attenders (compared to consistent problem-oriented attenders) had greater odds of having received dental cleaning and check-ups relative to having problem-oriented visits (chi2 = 33.47, P < 0.01), and that those with broken teeth or broken filling (compared to those without) are at greater odds of having a problem-oriented visit relative to having dental cleaning or checkups (chi2 = 34.12, P < 0.01 and chi2 = 17.11, P < 0.01).		Brent J. Shelton;Gregg H. Gilbert;Bin Liu;Monica Fisher	2004	Computer methods and programs in biomedicine	10.1016/j.cmpb.2004.05.005	econometrics;medicine;marginal model;computer science;machine learning;data mining;statistics	ML	10.340098432414061	-74.48868872410749	12204
ff85fa5346fe3f8ccc57fb519edebb90b97f41be	figure-ground separation by a dynamical system	equation non lineaire;background noise;ecuacion no lineal;equation differentielle;image edge detection feature extraction humans computer vision image segmentation machine vision differential equations couplings steady state background noise;image processing;edge detection;nonlinear dynamical systems;procesamiento imagen;differential equation;dynamic system;traitement image;experimental result;ecuacion diferencial;dynamical system;deteccion contorno;systeme dynamique;interference suppression;detection contour;image enhancement;nonlinear dynamical systems feature extraction image enhancement interference suppression;feature extraction;nonlinear differential equation;resultado experimental;image analysis figure ground separation dynamical system nonlinear scheme nonlinear differential equation pixel site steady state solution salient image structures synthetic images real world images image enhancement background noise suppression;ruido fondo;sistema dinamico;resultat experimental;non linear equation;bruit fond;steady state	This correspondence describes a novel nonlinear scheme for figure-ground separation where a nonlinear differential equation is defined at each pixel site and coupled with those at neighboring sites. The steady state solution enhances salient image structures and suppresses background noise. Experimental results on synthetic and real-world images demonstrate the efficacy of this scheme.	dynamical system;nonlinear system;pixel;steady state;synthetic intelligence	Jun Zhang;Jianbo Gao;Jiangchuan Liu	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.736701	computer vision;image processing;computer science;dynamical system;control theory;mathematics	Vision	52.22696314931327	-59.89465290230413	12206
2dad334578ceaa43904a7026c2958d2764bb7706	a constraint satisfaction neural network for medical diagnosis	backpropagation neural network;case base reasoning;cancer;neural nets;receiver operator characteristic;meaningful clinical information constraint satisfaction neural network medical diagnosis breast biopsy backpropagation neural network case based reasoning algorithm receiver operating characteristics analysis;constraint satisfaction;neural networks medical diagnosis medical diagnostic imaging databases breast biopsy backpropagation algorithms performance analysis algorithm design and analysis data mining clinical diagnosis;mammography;cancer medical diagnostic computing mammography neural nets;medical diagnostic computing;medical diagnosis;neural network	The objective of this study was to explore how a constraint satisfaction neural network (CSNN) can be used for medical diagnostic tasks. The study is based on a database of 500 patients who underwent breast biopsy at Duke University Medical Center due to suspicious mammographic findings. A CSNN was developed and evaluated to predict the biopsy result from the patient's mammographic findings. The diagnostic performance of the CSNN network was compared to a traditional backpropagation neural network and a case-based-reasoning algorithm by means of receiver operating characteristics analysis. The study demonstrates (i) how CSNNs can be applied to medical diagnostic tasks and, (ii) how they can be utilized to extract meaningful clinical information regarding underlying relationships among medical findings and associated diagnoses.	artificial neural network;constraint satisfaction	Georgia D. Tourassi;Carey E. Floyd;Joseph Y. Lo	1999		10.1109/IJCNN.1999.836258	constraint satisfaction;computer science;artificial intelligence;machine learning;medical diagnosis;receiver operating characteristic;artificial neural network;cancer	ML	33.68483759998603	-78.16975634544528	12235
a9474a0947a0fee13e43ca95b53ecbf245270c52	heteromed: heterogeneous information network for medical diagnosis		With the recent availability of Electronic Health Records (EHR) and great opportunities they offer for advancing medical informatics, there has been growing interest in mining EHR for improving quality of care. Disease diagnosis due to its sensitive nature, huge costs of error, and complexity has become an increasingly important focus of research in past years. Existing studies model EHR by capturing co-occurrence of clinical events to learn their latent embeddings. However, relations among clinical events carry various semantics and contribute differently to disease diagnosis which gives precedence to a more advanced modeling of heterogeneous data types and relations in EHR data than existing solutions. To address these issues, we represent how high-dimensional EHR data and its rich relationships can be suitably translated into HeteroMed, a heterogeneous information network for robust medical diagnosis. Our modeling approach allows for straightforward handling of missing values and heterogeneity of data. HeteroMed exploits metapaths to capture higher level and semantically important relations contributing to disease diagnosis. Furthermore, it employs a joint embedding framework to tailor clinical event representations to the disease diagnosis goal. To the best of our knowledge, this is the first study to use Heterogeneous Information Network for modeling clinical data and disease diagnosis. Experimental results of our study show superior performance of HeteroMed compared to prior methods in prediction of exact diagnosis codes and general disease cohorts. Moreover, HeteroMed outperforms baseline models in capturing similarities of clinical events which are examined qualitatively through case studies.		Anahita Hosseini;Ting Chen;Wenjun Wu;Yizhou Sun;Majid Sarrafzadeh	2018		10.1145/3269206.3271805	data mining;diagnosis code;medical diagnosis;computer science;data type;semantics;missing data;health informatics;exploit	ML	2.6498003564016583	-73.13505138458562	12240
1e6566b345f2b142462c6f9603bb8879f8cf9ad0	inverse perturbation for optimal intervention in gene regulatory networks	gene regulatory networks;melanoma;stochastic processes;gene expression regulation;models statistical;humans;gene regulatory network;markov chains	MOTIVATION Analysis and intervention in the dynamics of gene regulatory networks is at the heart of emerging efforts in the development of modern treatment of numerous ailments including cancer. The ultimate goal is to develop methods to intervene in the function of living organisms in order to drive cells away from a malignant state into a benign form. A serious limitation of much of the previous work in cancer network analysis is the use of external control, which requires intervention at each time step, for an indefinite time interval. This is in sharp contrast to the proposed approach, which relies on the solution of an inverse perturbation problem to introduce a one-time intervention in the structure of regulatory networks. This isolated intervention transforms the steady-state distribution of the dynamic system to the desired steady-state distribution.   RESULTS We formulate the optimal intervention problem in gene regulatory networks as a minimal perturbation of the network in order to force it to converge to a desired steady-state distribution of gene regulation. We cast optimal intervention in gene regulation as a convex optimization problem, thus providing a globally optimal solution which can be efficiently computed using standard toolboxes for convex optimization. The criteria adopted for optimality is chosen to minimize potential adverse effects as a consequence of the intervention strategy. We consider a perturbation that minimizes (i) the overall energy of change between the original and controlled networks and (ii) the time needed to reach the desired steady-state distribution of gene regulation. Furthermore, we show that there is an inherent trade-off between minimizing the energy of the perturbation and the convergence rate to the desired distribution. We apply the proposed control to the human melanoma gene regulatory network.   AVAILABILITY The MATLAB code for optimal intervention in gene regulatory networks can be found online: http://syen.ualr.edu/nxbouaynaya/Bioinformatics2010.html.	converge;convex optimization;dynamical system;entity name part qualifier - adopted;gene expression regulation;gene regulatory network;illness (finding);matlab;mathematical optimization;maxima and minima;neoplasms;optimization problem;organism;rate of convergence;steady state	Nidhal Bouaynaya;Roman Shterenberg;Dan Schonfeld	2011	Bioinformatics	10.1093/bioinformatics/btq605	stochastic process;biology;gene regulatory network;mathematical optimization;bioinformatics;mathematics;genetics	Comp.	13.467134934257997	-64.20489630992179	12247
b56c36c3f5de348cf396578308fbd1eb435bcfff	image decomposition using a robust regression approach		This paper considers how to separate text and/or graphics from smooth background in screen content and mixed content images and proposes an algorithm to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. This algorithm separates the background and foreground pixels by trying to fit pixel values in the block into a smooth function using a robust regression method. The inlier pixels that can be well represented with the smooth model will be considered as background, while remaining outlier pixels will be considered foreground. We have also created a dataset of screen content images extracted from HEVC standard test sequences for screen content coding with their ground truth segmentation result which can be used for this task. The proposed algorithm has been tested on the dataset mentioned above and is shown to have superior performance over other methods, such as the hierarchical k-means clustering algorithm, shape primitive extraction and coding, and the least absolute deviation fitting scheme for foreground segmentation.	algorithm;basis function;cluster analysis;discrete cosine transform;graphics;ground truth;hierarchical clustering;high efficiency video coding;k-means clustering;least absolute deviations;pixel;random sample consensus;reflections of signals on conducting lines;sharp mz;smoothing	Shervin Minaee;Yao Wang	2016	CoRR		computer vision;machine learning;pattern recognition;mathematics;statistics	Vision	46.152248700907265	-66.63935246359581	12295
b4ea118be8323d1c912e70768eb2ec565953947e	statistical issues in the analysis of illumina data	sensitivity and specificity;data interpretation statistical;database management systems;databases genetic;artifacts;computational biology bioinformatics;gene expression;quality assessment;differential expression;reproducibility of results;algorithms;data quality;combinatorial libraries;computer appl in life sciences;information storage and retrieval;oligonucleotide array sequence analysis;microarrays;bioinformatics	Illumina bead-based arrays are becoming increasingly popular due to their high degree of replication and reported high data quality. However, little attention has been paid to the pre-processing of Illumina data. In this paper, we present our experience of analysing the raw data from an Illumina spike-in experiment and offer guidelines for those wishing to analyse expression data or develop new methodologies for this technology. We find that the local background estimated by Illumina is consistently low, and subtracting this background is beneficial for detecting differential expression (DE). Illumina's summary method performs well at removing outliers, producing estimates which are less biased and are less variable than other robust summary methods. However, quality assessment on summarised data may miss spatial artefacts present in the raw data. Also, we find that the background normalisation method used in Illumina's proprietary software (BeadStudio) can cause problems with a standard DE analysis. We demonstrate that variances calculated from the raw data can be used as inverse weights in the DE analysis to improve power. Finally, variability in both expression levels and DE statistics can be attributed to differences in probe composition. These differences are not accounted for by current analysis methods and require further investigation. Analysing Illumina expression data using BeadStudio is reasonable because of the conservative estimates of summary values produced by the software. Improvements can however be made by not using background normalisation. Access to the raw data allows for a more detailed quality assessment and flexible analyses. In the case of a gene expression study, data can be analysed on an appropriate scale using established tools. Similar improvements can be expected for other Illumina assays.	data quality;estimated;gene expression profiling;health facilities, proprietary;heart rate variability;morphologic artifacts;preprocessor;sensor;spike count:nrat:pt:cerebral cortex:qn:eeg;weight	Mark J. Dunning;Nuno L. Barbosa-Morais;Andy G. Lynch;Simon Tavaré;Matthew E. Ritchie	2007	BMC Bioinformatics	10.1186/1471-2105-9-85	biology;gene expression;dna microarray;data quality;computer science;bioinformatics;data science;data mining;genetics	Comp.	4.596559173487941	-53.080995801091795	12297
40781d1239361b5be66e62fdfc2253489bd6f20a	clinical microwave tomographic imaging of the calcaneus: a first-in-human case study of two subjects	female;microwave measurements;osteoporosis bone bone density dielectric properties fracture risk microwave imaging;2d microwave tomographic image clinical microwave tomographic imaging 3d microwave tomographic image calcaneus bones microwave properties x ray density measures lower leg injury soft prior regularization technique bulk dielectric properties bone region permittivity conductivity computed tomography derived density measures living human;computed tomography;imaging three dimensional;calcaneus;male;microwave imaging;microwaves;permittivity bioelectric phenomena bone computerised tomography electrical conductivity image reconstruction injuries medical image processing microwave imaging;bones;image reconstruction;medical image processing;adult;injuries;bone;computerised tomography;osteoporosis;bone density;fracture risk;humans;bioelectric phenomena;dielectric properties;electric conductivity;electrical conductivity;microwave theory and techniques;microwave imaging microwave theory and techniques bones microwave measurements computed tomography;tomography;tomography x ray computed;permittivity	We have acquired 2-D and 3-D microwave tomographic images of the calcaneus bones of two patients to assess correlation of the microwave properties with X-ray density measures. The two volunteers were selected because each had one leg immobilized for at least six weeks during recovery from a lower leg injury. A soft-prior regularization technique was incorporated with the microwave imaging to quantitatively assess the bulk dielectric properties within the bone region. Good correlation was observed between both permittivity and conductivity and the computed tomography-derived density measures. These results represent the first clinical examples of microwave images of the calcaneus and some of the first 3-D tomographic images of any anatomical site in the living human.	body regions;bone tissue;ct scan;diagnostic radiologic examination;injury of lower leg;leg injuries;microwave;patients;skeletal bone;tomography, emission-computed;tomography	Paul M. Meaney;Douglas Goodwin;Amir H. Golnabi;Tian Zhou;Matthew J. Pallone;Shireen D. Geimer;Gregory Burke;Keith D. Paulsen	2012	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2012.2209202	radiology;tomography;nuclear medicine;electrical resistivity and conductivity;physics;quantum mechanics;medical physics	Vision	43.66279149025836	-84.63008460133614	12298
14971e111fc4a5dffeddfb6c4714552d5576353d	increasing consensus accuracy in dna fragment assemblies by incorporating fluorescent trace representations	dna fragmentation;error rate;majority voting;data classification	We present a new method for determining the consensus sequence in DNA fragment assemblies. The new method, Trace-Evidence, directly incorporates aligned ABI trace information into consensus calculations via our previously described representation, Trace-Data Classifications. The new method extracts and sums evidence indicated by the representation to determine consensus calls. Using the Trace-Evidence method results in automatically produced consensus sequences that are more accurate and less ambiguous than those produced with standard majority-voting methods. Additionally, these improvements are achieved with less coverage than required by the standard methods-using Trace-Evidence and a coverage of only three, error rates are as low as those with a coverage of over ten sequences.	alignment;application binary interface;classification;consensus sequence	Carolyn F. Allex;Schuyler F. Baldwin;Jude W. Shavlik;Frederick R. Blattner	1997	Proceedings. International Conference on Intelligent Systems for Molecular Biology		majority rule;dna fragmentation;word error rate;computer science;bioinformatics;data mining;genetics;algorithm	Comp.	0.3069221738433699	-55.087194327553235	12306
8fa70d24b407eed2efd2064304fc4620115845d7	assessment of the effectiveness of acupuncture on facial paralysis based on semg decomposition		Although acupuncture has been extensively and routinely applied in clinic around the world, its use is conventionally based on empirical knowledge rather than scientific evidence. In this paper, we investigate the surface electromyographic (sEMG) activity on the face of patients with facial paralysis to validate the effectiveness of acupuncture. sEMG decomposition method is employed to quantify of the differences between the healthy and diseased sides, which includes 2 parts: to decompose sEMG into motor units action potential trains (MUAPTs) with Gaussian Mixture Model (GMM); to assess the recovery of patients with facial paralysis according to the differences between the MUAPTs of healthy and diseased sides. Finally, an auto-regression model is used to predict the recovery trends. Results indicate that the proposed method can assess the effectiveness of acupuncture on facial paralysis and achieve a high accuracy of 90.94% to predict the recovery trends. c Springer International Publishing Switzerland 2015.	facial recognition system	Anbin Xiong;Xingang Zhao;Jianda Han;Guangjun Liu	2014		10.1007/978-3-319-16841-8_64	facial paralysis;computer vision;mixture model;computer science;acupuncture;pattern recognition;artificial intelligence	EDA	27.774099206232115	-80.0056435648574	12332
11f91dd8e4c3d11c8cb7dc92d8b9d49eb72f7a77	noise tolerance in a neocognitron-like network	lateral competition;noise tolerance;neocognitron;selectivity;winner take all;hierarchical model	The Neocognitron and its related hierarchical models have been shown to be competitive in recognizing handwritten digits and objects. However, the tolerance of these models to several types of noise can be low. We will start by briefly overviewing some previous results regarding the tolerance of these models. Afterwards, we report the higher noise tolerance of the winner-take-all response in a hierarchical model over related models. We provide an analysis and interpretation of this tolerance under Bayesian decision theory. Finally, we report on how to further improve recognition for extremely noisy patterns.	bayesian network;decision theory;digit structure;hierarchical database model;neocognitron;physical object	Ângelo Cardoso;Andreas Wichert	2014	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2013.09.007	winner-take-all;selectivity;speech recognition;computer science;artificial intelligence;machine learning;neocognitron;hierarchical database model	ML	23.955354061380696	-66.38832894903666	12335
96db558d251fcc5243e88baf80894f4dfe79fd5e	bregman divergences for growing hierarchical self-organizing networks	bregman divergences;classification;visualization;self organization	Growing hierarchical self-organizing models are characterized by the flexibility of their structure, which can easily accommodate for complex input datasets. However, most proposals use the Euclidean distance as the only error measure. Here we propose a way to introduce Bregman divergences in these models, which is based on stochastic approximation principles, so that more general distortion measures can be employed. A procedure is derived to compare the performance of networks using different divergences. Moreover, a probabilistic interpretation of the model is provided, which enables its use as a Bayesian classifier. Experimental results are presented for classification and data visualization applications, which show the advantages of these divergences with respect to the classical Euclidean distance.	anatomy, regional;bayesian network;biologic preservation;bregman divergence;cluster analysis;coherence (physics);data visualization;distortion;euclidean distance;experiment;hearing loss, high-frequency;imagery;multispectral image;naive bayes classifier;organizing (structure);self-organization;self-organizing map;statistical classification;stochastic approximation;time complexity;while;exponential;statistical cluster	Ezequiel López-Rubio;Esteban J. Palomo;Enrique Domínguez	2014	International journal of neural systems	10.1142/S0129065714500166	self-organization;visualization;biological classification;computer science;machine learning;pattern recognition;mathematics;statistics;bregman divergence	ML	50.40198050349262	-73.84911022035817	12350
ff5bd8c22b6bcf09bfcbb955f75101b433b77bdd	fast semantic image segmentation with high order context and guided filtering		This paper describes a fast and accurate semantic image segmentation approach that encodes not only the discriminative features from deep neural networks, but also the high-order context compatibility among adjacent objects as well as low level image features. We formulate the underlying problem as the conditional random field that embeds local feature extraction, clique potential construction, and guided filtering within the same framework, and provide an efficient coarse-tofine solver. At the coarse level, we combine local feature representation and context interaction using a deep convolutional network, and directly learn the interaction from high order cliques with a message passing routine, avoiding time-consuming explicit graph inference for joint probability distribution. At the fine level, we introduce a guided filtering interpretation for the mean field algorithm, and achieve accurate object boundaries with 100 faster than classic learning methods. The two parts are connected and jointly trained in an end-to-end fashion. Experimental results on Pascal VOC 2012 dataset have shown that the proposed algorithm outperforms the state-of-the-art, and that it achieves the rank 1 performance at the time of submission, both of which prove the effectiveness of this unified framework for semantic image segmentation.	algorithm;artificial neural network;bilateral filter;computer vision;conditional random field;deep learning;encode;end-to-end principle;feature extraction;image segmentation;message passing;solver;unary operation;unified framework	Falong Shen;Gang Zeng	2016	CoRR		computer vision;computer science;theoretical computer science;machine learning;pattern recognition	Vision	26.511747248487612	-52.110059891893634	12356
bd7c9cf0c678eda749cb63762587c4068c419571	automatic generation of morphological opening-closing sequences for texture segmentation	automatic control;image segmentation;image processing;lattices;image databases;texture segmentation;segmentation;gray scale;automatic generation;image texture;image segmentation image texture;image segmentation gray scale pixel image databases costs lattices automatic control image processing spatial databases computational efficiency;opening closing operators;pixel;spatial databases;computational efficiency;segmentation texture segmentation opening closing operators gray scale	Texture segmentation is an important task in image processing. The objective is to assign the same value to those pixels in an image which belong to the same texture. This is usually calledpixel-classification. There exist two fundamental approaches to solve this task. The first and most general approach we call lassification orientedby which we mean that the actual labeling is done by some general classifier, i.e. maximum-likelihood. As these classifiers require some feature vector as input, there has to be such a vector for each pixel. By this we not only get some segmentation but we can also identify every texture with respect to the database used to train the classifier. The problem with this approach is that usually the computational costs will be high for eachimage to be classified, as they are determined primarily by the costs of calculating the feature vector for each pixel. These costs may be reduced by using pruning algorithms, but if a single feature relies on the calculation of several image transformations like in [2] or in [4], the performance gain may be small. In this paper we consider the case that we know which texture combination will appear in an image. A different approach may be used for this problem which we call transformation oriented. By this we mean to find a transformation adapted to a particular texture combination allowing a segmentation of the image by applying some threshold algorithm (i.e. as in [3]) on the transformed image. Our approach therefore consists of three parts: (1) Building a database of texture descriptions, (2) automatic configuration of an appropriate segmentation algorithm for a given texture combination (this is not a particular image, nor any image at all), and (3) segmentation of a particular image. The intention is that the expensive part is Step 1 and has to be done only once for each texture. Step 2 can be performed quickly and has to be done only once for each texture combination in question. Step 3 finally is very cheap and considered to be used with several images containing the same texture combination. This paper is organized as follows: In the next section we give an outline of our approach. This consists of an idealized texture model with an appropriate segmentation strategy. To realize step 1 described above we need a texture description, which is exact for the idealized model and approximative for real textures. It is presented in section 2.1. For step 2 we give two alternative configuration algorithms shown in section 4. step 3 is finally described in section 5. Finally we discuss the results in section 6.	algorithm;closing (morphology);computation;feature vector;image processing;opening (morphology);pixel;pruning (morphology)	Jens Racky;Madhukar Pandit	1999		10.1109/ICIP.1999.817104	image texture;computer vision;image processing;computer science;automatic control;pattern recognition;lattice;mathematics;image segmentation;segmentation;pixel;grayscale;computer graphics (images)	Vision	40.85473538271511	-60.513779184823775	12368
80cbe8592fd1ff27386e4c8e8cc47fe655865047	scale-space texture description on sift-like textons	texture;image feature;wavelet tight frame;journal;multi fractal analysis	Visual texture is a powerful cue for the semantic description of scene structures that exhibit a high degree of similarity in their image intensity patterns. This paper describes a statistical approach to visual texture description that combines a highly discriminative local feature descriptor with a powerful global statistical descriptor. Based upon a SIFT-like feature descriptor densely estimated at multiple window sizes, a statistical descriptor, called the multifractal spectrum (MFS), extracts the power-law behavior of the local feature distributions over scale. Through this combination strong robustness to environmental changes including both geometric and photometric transformations is achieved. Furthermore, to increase the robustness to changes in scale, a multi-scale representation of the multi-fractal spectra under a wavelet tight frame system is derived. The proposed statistical approach is applicable to both static and dynamic textures. Experiments showed that the proposed approach outperforms existing static texture classification methods and is comparable to the top dynamic texture classification techniques.	art & architecture thesaurus;cascade algorithm;cluster analysis;coefficient;decimation (signal processing);feature detection (web development);fractal;frame (linear algebra);frame language;full-spectrum light;global illumination;h2 database engine;image gradient;image scaling;low-pass filter;moose file system;multifractal system;outline of object recognition;scale space;scale-invariant feature transform;sparse approximation;sparse matrix;universal quantification;visual descriptor;wavelet transform	Yong Xu;Si-Bin Huang;Hui Ji;Cornelia Fermüller	2012	Computer Vision and Image Understanding	10.1016/j.cviu.2012.05.003	image texture;computer vision;local binary patterns;machine learning;pattern recognition;mathematics;texture;texture filtering	Vision	37.83271353603546	-58.761778280852766	12385
bacaaa8e59baf853bdd0b3d5f8e3c352c9be8e98	on rgb-d face recognition using kinect	image matching;image classification;image sensors;entropy face face recognition databases visualization feature extraction three dimensional displays;face recognition performance rgb d face recognition algorithm 2d images feature extraction feature matching 3d faces specialized acquisition method 3d images low cost sensors rgb d faces entropy saliency feature rgb d descriptor random decision forest classifier rgb d face database rgb d information kinect;face recognition;feature extraction;decision trees;visual databases decision trees face recognition feature extraction image classification image matching image sensors;visual databases	Face recognition algorithms generally use 2D images for feature extraction and matching. In order to achieve better performance, 3D faces captured via specialized acquisition methods have been used to develop improved algorithms. While such 3D images remain difficult to obtain due to several issues such as cost and accessibility, RGB-D images captured by low cost sensors (e.g. Kinect) are comparatively easier to acquire. This research introduces a novel face recognition algorithm for RGB-D images. The proposed algorithm computes a descriptor based on the entropy of RGB-D faces along with the saliency feature obtained from a 2D face. The probe RGB-D descriptor is used as input to a random decision forest classifier to establish the identity. This research also presents a novel RGB-D face database pertaining to 106 individuals. The experimental results indicate that the RGB-D information obtained by Kinect can be used to achieve improved face recognition performance compared to existing 2D and 3D approaches.	accessibility;algorithm;database;experiment;facial recognition system;feature extraction;integrated information theory;jean;kinect;lucas sequence;matching (graph theory);self-information;sensor;three-dimensional face recognition	Gaurav Goswami;Samarth Bharadwaj;Mayank Vatsa;Richa Singh	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712717	facial recognition system;computer vision;contextual image classification;face detection;feature extraction;computer science;machine learning;decision tree;pattern recognition;image sensor;three-dimensional face recognition	Vision	33.903985112012066	-52.95608995688359	12386
85cbaf591d114d4933ab575fa347d334ffc192fb	a multi-label learning method for efficient affective detection	heart rate variability;biological system modeling;physiology;electrocardiography;computational modeling;feature extraction;affective computing	Biosignals-based affective computing plays an important role in human-computing interaction. In recent years, a lot of works have been successfully implemented for emotion recognition with biological signals. However most of them are computationally expensive due to the complexity of models. In this paper, we present a multi-label learning (MLL) method to map biological signals to an affective model in real time. Multi-label learning combines multiple classifiers in a same training process by evaluating correlations between labels. To evaluate the proposed model, 25 male subjects were recruited to anticipate an experiment for model evaluation. Dynamic images were specifically chosen to elicit emotion described by different arousal and valence levels in the experiment. In terms of physiological signals, electrocardiogram (ECG) and skin conductivity (SC) signals were collected for classification. By applying the MLL method to analyze the relationships between physiological signal features and affective labels, we observed that the proposed method performed better than traditional method, which showing the potential of MLL in affective detection.	affective computing;analysis of algorithms;emotion recognition;mechwarrior: living legends;multi-label classification	Yutong Wang;Tong Wang;Ping Gong;Ying Wu;Chenfei Ye;Jie Li;Ting Ma	2017	2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)	10.1109/BHI.2017.7897205	psychology;simulation;artificial intelligence;communication	Vision	9.466458966309625	-93.70236731226802	12398
f004be0d9feed3b704838b237387fc13d21cdd3f	neural mechanisms of the testosterone–aggression relation: the role of orbitofrontal cortex	testosterone;pedestrian safety;poison control;injury prevention;neural mechanisms;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;aggressive behavior;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;aggression;ultimatum game;poisoning prevention;falls;ergonomics;orbitofrontal cortex;suicide prevention;impulse control	Testosterone plays a role in aggressive behavior, but the mechanisms remain unclear. The present study tested the hypothesis that testosterone influences aggression through the OFC, a region implicated in self-regulation and impulse control. In a decision-making paradigm in which people chose between aggression and monetary reward (the ultimatum game), testosterone was associated with increased aggression following social provocation (rejecting unfair offers). The effect of testosterone on aggression was explained by reduced activity in the medial OFC. The findings suggest that testosterone increases the propensity toward aggression because of reduced activation of the neural circuitry of impulse control and self-regulation.	artificial neural network;choose (action);decision making;electronic circuit;homeostasis;medial graph;money;orofacial cleft 1;personality disorders;programming paradigm;self-control as a personality trait;testosterone	Pranjal H. Mehta;Jennifer Beer	2010	Journal of Cognitive Neuroscience	10.1162/jocn.2009.21389	psychology;ultimatum game;developmental psychology;suicide prevention;human factors and ergonomics;injury prevention;testosterone;social psychology	HCI	16.521921097615397	-79.06286314739965	12411
d07e1a18e0b76ca92ef4ea668c24f341eb388beb	neuroimage-based clinical prediction using machine learning tools		Classification of structural brain magnetic resonance (MR) images is a crucial task for many neurological phenotypes that machine learning tools are increasingly developed and applied to solve this problem in recent years. In this study binary classification of T1-weighted structural brain MR images are performed using state-of-the-art machine learning algorithms when there is no information about the clinical context or specifics of neuroimaging. Image derived features and clinical labels that are provided by the International Conference on Medical Image Computing and ComputerAssisted Intervention 2014 machine learning challenge are used. These morphological summary features are obtained from four different datasets (each N>70) with clinically relevant phenotypes and automatically extracted from the MR imaging scans using FreeSurfer, a freely distributed brain MR image processing software package. Widely used machine learning tools, namely; back-propagation neural network, self-organizing maps, support vector machines and knearest neighbors are used as classifiers. Clinical prediction accuracy is obtained via cross-validation on the training data (N 5 150) and predictions are made on the test data (N 5 100). Classification accuracy, the fraction of cases where prediction is accurate and area under the ROC curve are used as the performance metrics. Accuracy and area under curve metrics are used for tuning the training hyperparameters and the evaluation of the performance of the classifiers. Performed experiments revealed that support vector machines show a better success compared to the other methods on clinical predictions using summary morphological features in the absence of any information about the phenotype. Prediction accuracy would increase greatly if contextual information is integrated into the system. VC 2017 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 27, 89–97, 2017; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/ima.22213	algorithm;artificial neural network;backpropagation;binary classification;cross-validation (statistics);experiment;freesurfer;image processing;john d. wiley;machine learning;medical image computing;organizing (structure);receiver operating characteristic;resonance;self-organization;self-organizing map;software propagation;support vector machine;synapomorphy;test data;vector processor	Ayse Demirhan	2017	Int. J. Imaging Systems and Technology	10.1002/ima.22213	computer science;data science;machine learning;data mining	ML	30.315036469428037	-76.59506879478337	12416
2be45bae2b667f1448e138f88014b25e0e495ab5	optimizing pcr primers targeting the bacterial 16s ribosomal rna gene	16s rrna sequencing;multi objective optimization;primer design	Targeted amplicon sequencing of the 16S ribosomal RNA gene is one of the key tools for studying microbial diversity. The accuracy of this approach strongly depends on the choice of primer pairs and, in particular, on the balance between efficiency, specificity and sensitivity in the amplification of the different bacterial 16S sequences contained in a sample. There is thus the need for computational methods to design optimal bacterial 16S primers able to take into account the knowledge provided by the new sequencing technologies. We propose here a computational method for optimizing the choice of primer sets, based on multi-objective optimization, which simultaneously: 1) maximizes efficiency and specificity of target amplification; 2) maximizes the number of different bacterial 16S sequences matched by at least one primer; 3) minimizes the differences in the number of primers matching each bacterial 16S sequence. Our algorithm can be applied to any desired amplicon length without affecting computational performance. The source code of the developed algorithm is released as the mopo16S software tool (Multi-Objective Primer Optimization for 16S experiments) under the GNU General Public License and is available at http://sysbiobig.dei.unipd.it/?q=Software#mopo16S. Results show that our strategy is able to find better primer pairs than the ones available in the literature according to all three optimization criteria. We also experimentally validated three of the primer pairs identified by our method on multiple bacterial species, belonging to different genera and phyla. Results confirm the predicted efficiency and the ability to maximize the number of different bacterial 16S sequences matched by primers.	algorithm;attribute–value pair;bacterial 16s rna;biopolymer sequencing;computation;contain (action);experiment;gnu;genera;matching;mathematical optimization;multi-objective optimization;numerous;optimizing compiler;primer;programming tool;ribosomal rna;rough set;sensitivity and specificity;source code;whole genome sequencing;cellular targeting	Francesco Sambo;Francesca Finotello;Enrico Lavezzo;Giacomo Baruzzo;Giulia Masi;Elektra Peta;Marco Falda;Stefano Toppo;Luisa Barzon;Barbara Di Camillo	2018		10.1186/s12859-018-2360-6	dna microarray;genetics;gene;amplicon;structural biology;biology;bacteria;primer (molecular biology);polymerase chain reaction;16s ribosomal rna	Comp.	0.9784568464086618	-56.254676110560816	12424
e8747236efad53c3de02f3aeff880b361eb25a1a	a power signal based dynamic approach to detecting anomalous behavior in wireless devices		The health and security of wireless devices are fast gaining importance, and these are vital for effective implementation of sensor networks and Internet of Things (IoT). Any device, wired or wireless, needs a power source, and the power consumed is a consequence of its usage and functionality. In this context, this paper proposes a methodology to detect anomalous behavior of wireless devices by monitoring their power consumption patterns. The proposed methodology utilizes Independent Component Analysis (ICA) to extract information from the current power consumption of the device and generates features of the state of the device by calculating the degree of similarity of the extracted information with the known normal behavior of the device. Then, Recursive Feature Elimination (RFE) is used to select features from the generated feature vector. Finally, Classification algorithms are used to classify and detect the anomalous behavior. We have validated the methodology by emulating anomalous behavior on smartphones through a custom designed app that runs in the background while the main app is being used. Validation results indicate that the proposed methodology can be used to identify even a sparsely active malware existence with very high accuracy. The proposed model has an accuracy of 88% for a malware active for 1% of the total time and accuracy of almost 100% for malware active for 12% of the time.	algorithm;emulator;feature selection;feature vector;independent computing architecture;independent component analysis;internet of things;machine learning;malware;recursion (computer science);sensor;smartphone;statistical classification;time–frequency analysis	Robin Joe Prabhahar Soundar Raja James;C. A. Black;Sagar Naik;Marzia Zaman;Nishith Goel	2018		10.1145/3265863.3265867	real-time computing;wireless sensor network;computer science;wireless;recursion;feature vector;malware;independent component analysis;feature extraction;statistical classification;distributed computing	Mobile	5.083712449806223	-86.23891250331371	12426
406c46e04fc94c3b9636be385108c63ef47cfa80	modeling robust qsar		Quantitative Structure Activity Relationship (QSAR) is a term describing a variety of approaches that are of substantial interest for chemistry. This method can be defined as indirect molecular design by the iterative sampling of the chemical compounds space to optimize a certain property and thus indirectly design the molecular structure having this property. However, modeling the interactions of chemical molecules in biological systems provides highly noisy data, which make predictions a roulette risk. In this paper we briefly review the origins for this noise, particularly in multidimensional QSAR. This was classified as the data, superimposition, molecular similarity, conformational, and molecular recognition noise. We also indicated possible robust answers that can improve modeling and predictive ability of QSAR, especially the self-organizing mapping of molecular objects, in particular, the molecular surfaces, a method that was brought into chemistry by Gasteiger and Zupan.	biological system;chemical similarity;chemicals;classification;interaction;iterative method;jure zupan;national origin;organizing (structure);sampling (signal processing);self-organization;signal-to-noise ratio	Jaroslaw Polanski;Andrzej Bak;Rafal Gieleciak;Tomasz Magdziarz	2006	Journal of chemical information and modeling	10.1021/ci050314b	chemistry;toxicology;bioinformatics;machine learning	Comp.	12.91978462351967	-55.13340581326822	12432
892217128022dd5e5773fc44975794c8839132dd	2d diffuse optical imaging using clustered sparsity	diffuse optical tomography;phantoms biodiffusion biological tissues biomedical optical imaging bio optics brain haemodynamics image reconstruction medical image processing;image reconstruction optical imaging phantoms optical sensors clustering algorithms tomography integrated optics;clustered sparsity;clustered sparsity diffuse optical tomography sparsity regularization overlapping group sparsity;overlapping group sparsity;sparsity reg ularization;phantom 2d diffuse optical imaging clustered sparsity hemodynamic changes biological tissues near infrared light brain functions light propagation image reconstruction sparsity regularization linearized doi problem in vivo absorption changes	Diffuse optical imaging (DOI) is a non-invasive technique which measures hemodynamic changes in the tissue with near infrared light, and has been increasingly used to study brain functions. Due to the nature of light propagation in the tissue, the reconstruction problem is severely ill-posed. Sparsity-regularization has achieved promising results in recent works for linearized DOI problem. In this paper, we exploit more prior information to improve DOI besides sparsity. Based on the functional specialization of the brain, the in vivo absorption changes caused by specific brain function can be clustered in certain region(s) and not randomly distributed. Thus, a new algorithm is proposed to utilize this prior in reconstruction. Results of numerical simulations and phantom experiments have demonstrated the superiority of the proposed method over the state-of-the-art.	algorithm;computational human phantom;computer simulation;diffuse optical imaging;experiment;hemodynamics;matrix regularization;numerical analysis;partial template specialization;randomness;reconstruction conjecture;software propagation;sparse matrix;video-in video-out;well-posed problem	Chen Chen;Fenghua Tian;Jixing Yao;Hanli Liu;Junzhou Huang	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6867946	diffuse optical imaging;computer vision;radiology;medicine;optics;physics;medical physics	Vision	51.41667171240824	-80.47472575070628	12437
11ed741238ec8f0384cfd770bb19af9beea9eb4f	image averaging for improved iris recognition	iris recognition	We take advantage of the temporal continuity in an iris video to improve matching performance using signal-level fusion. From multiple frames of an iris video, we create a single average image. Our signal-level fusion method performs better than methods based on single still images, and better than previously published multi-gallery scorefusion methods. We compare our signal fusion method with another new method: a multi-gallery, multi-probe score fusion method. Between these two new methods, the multi-gallery, multi-probe score fusion has slightly better recognition performance, while the signal fusion has significant advantages in memory and computation requirements. 1	biometrics;computation;experiment;iris recognition;requirement;scott continuity;signal-to-noise ratio;technical support	Karen Hollingsworth;Kevin W. Bowyer;Patrick J. Flynn	2009		10.1007/978-3-642-01793-3_112	computer vision;speech recognition;computer science;pattern recognition;iris recognition	Vision	33.39717219618332	-61.107405571020344	12448
4bea5fda4b7143560f3fbf48d204a27077589c07	a learning-based approach to explosives detection using multi-energy x-ray computed tomography	national security;multi energy x ray tomography;support vector machines computerised tomography explosives feature extraction image classification learning artificial intelligence;computed tomography;x ray imaging;support vector machines;x ray computed tomography;materials explosives attenuation feature extraction x ray imaging computed tomography support vector machines;image classification;dual energy x ray systems learning explosives detection multienergy x ray computed tomography mect energy curves feature extraction non explosive compounds support vector machine svm classifier;attenuation;materials;classification;dimensionality reduction;feature extraction;explosives;computerised tomography;national security multi energy x ray tomography classification support vector machines dimensionality reduction;support vector machine;learning artificial intelligence;dimensional reduction;x ray tomography;x rays	In this paper we consider the task of classifying materials into explosives and non-explosives according to features obtainable from Multi-Energy X-ray Computed Tomography (MECT) measurements. The discriminative ability of MECT derives from its sensitivity to the attenuation versus energy curves of materials. Thus we focus on the fundamental information available in these curves and features extracted from them. We study the dimensionality and span of these curves for a set of explosive and non-explosive compounds and show that their space is larger than two-dimensional, as is typically assumed. In addition, we build support vector machine classifiers with different feature sets and find superior classification performance when using more than two features and when using features different than the standard photoelectric and Compton coefficients. These results suggest the potential for improved detection performance relative to conventional dual-energy X-ray systems.	ct scan;coefficient;photoelectric effect;support vector machine;tomography;x-ray (amazon kindle)	Limor Eger;Synho Do;Prakash Ishwar;W. Clem Karl;Homer H. Pien	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946904	support vector machine;computer vision;computer science;machine learning;pattern recognition;computed tomography	Robotics	34.526869891486385	-72.02817080866267	12468
baca668f934d7cda152fab3e7ce6f37f579efe64	biomedical system based on the discrete hidden markov model using the rocchio-genetic approach for the classification of internal carotid artery doppler signals	discrete hidden markov model;maximum likelihood;genie biomedical;optimization technique;hidden markov model;genetics;dopplerometrie;rocchio algorithm;biomedical engineering;biomedical system;carotid artery;internal carotid artery;doppler ultrasound study;genetic algorithm;ingenieria biomedica;cross validation;classification accuracy;dopplerometria;doppler signal	When the maximum likelihood approach (ML) is used during the calculation of the Discrete Hidden Markov Model (DHMM) parameters, DHMM parameters of the each class are only calculated using the training samples (positive training samples) of the same class. The training samples (negative training samples) not belonging to that class are not used in the calculation of DHMM model parameters. With the aim of supplying that deficiency, by involving the training samples of all classes in calculating processes, a Rocchio algorithm based approach is suggested. During the calculation period, in order to determine the most appropriate values of parameters for adjusting the relative effect of the positive and negative training samples, a Genetic algorithm is used as an optimization technique. The purposed method is used to classify the internal carotid artery Doppler signals recorded from 136 patients as well as of 55 healthy people. Our proposed method reached 97.38% classification accuracy with fivefold cross-validation (CV) technique. The classification results showed that the proposed method was effective for the classification of internal carotid artery Doppler signals.	angular defect;carotid arteries;class;crisscross heart;cross-validation (statistics);genetic algorithm;hidden markov model;internal carotid artery structure;markov chain;mathematical optimization;patients;rocchio algorithm;triangulation	Harun Uguz;Gür Emre Güraksin;Uçman Ergün;Ridvan Saraçoglu	2011	Computer methods and programs in biomedicine	10.1016/j.cmpb.2010.07.001	speech recognition;genetic algorithm;medicine;computer science;machine learning;maximum likelihood;hidden markov model;cross-validation;statistics	AI	15.840528610898094	-89.69941111442502	12469
1bb5dcd88a33c67e64d480df4cb1432a708dc7eb	log-normal and log-gabor descriptors for expressive events detection and facial features segmentation	log gabor filter;multiscale spatial filtering;classification;log normal filter;transient feature;facial expression	"""The current paper investigates the merits of the Log-Normal and Log-Gabor filters for the dynamic analysis and segmentation of facial behavior during facial expression sequences. First, a spatial filtering method based on the Log-Normal filters is introduced for the holistic processing of the face towards the automatic segmentation of consecutive """"emotional segments"""" in video sequences. Secondly, a filtering-based method based on the Log-Gabor filters is applied as a feature-based processing for the automatic and accurate segmentation of the transient facial features (such as nasal root wrinkles and nasolabial furrows) and a precise estimation of their orientation in a single pass. We compared heuristic and machine learning based methods to evaluate the efficiency of the used descriptors for each task. When tested for automatic detection of """"emotional segments"""" in 137 video sequences from the MMI, the Hammal-Caplier facial expression databases, and 20 recorded video sequences of consecutive appearance of multiple facial expressions, the proposed Log-Normal based descriptors achieved an accuracy of 89% with a mean frame error of 8 frames using a heuristic based processing. Higher performances were obtained using the SVM based method leading to an accuracy of 94% with a mean frame error detection of 3.1 frames. Tested on more than 3280 images from 5 benchmark databases (i.e. the Cohn-Kanade database, the CAFE database, the STOIC database, the MMI database, and the Hammal-Caplier database) the proposed Log-Gabor based descriptors for transient facial features detection achieved a mean performance of 82% using a heuristic based processing and a mean performance of 96% using the SVM based classification. The proposed method for the estimation of the corresponding orientation leads to an error of 2.7?."""		Zakia Hammal	2014	Inf. Sci.	10.1016/j.ins.2014.07.002	computer vision;speech recognition;biological classification;computer science;pattern recognition;facial expression	Vision	35.84016057844425	-56.68413490410308	12496
b674f9e2e94bb2be175318b08fdcf8693fb4739b	the number and choice of muscles impact the results of muscle synergy analyses	health research;uk clinical guidelines;biological patents;musculoskeletal model;europe pubmed central;simulation;citation search;uk phd theses thesis;nonnegative matrix factorization;life sciences;muscle synergy;electromyography;keywords muscle synergy;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	One theory for how humans control movement is that muscles are activated in weighted groups or synergies. Studies have shown that electromyography (EMG) from a variety of tasks can be described by a low-dimensional space thought to reflect synergies. These studies use algorithms, such as nonnegative matrix factorization, to identify synergies from EMG. Due to experimental constraints, EMG can rarely be taken from all muscles involved in a task. However, it is unclear if the choice of muscles included in the analysis impacts estimated synergies. The aim of our study was to evaluate the impact of the number and choice of muscles on synergy analyses. We used a musculoskeletal model to calculate muscle activations required to perform an isometric upper-extremity task. Synergies calculated from the activations from the musculoskeletal model were similar to a prior experimental study. To evaluate the impact of the number of muscles included in the analysis, we randomly selected subsets of between 5 and 29 muscles and compared the similarity of the synergies calculated from each subset to a master set of synergies calculated from all muscles. We determined that the structure of synergies is dependent upon the number and choice of muscles included in the analysis. When five muscles were included in the analysis, the similarity of the synergies to the master set was only 0.57 ± 0.54; however, the similarity improved to over 0.8 with more than ten muscles. We identified two methods, selecting dominant muscles from the master set or selecting muscles with the largest maximum isometric force, which significantly improved similarity to the master set and can help guide future experimental design. Analyses that included a small subset of muscles also over-estimated the variance accounted for (VAF) by the synergies compared to an analysis with all muscles. Thus, researchers should use caution using VAF to evaluate synergies when EMG is measured from a small subset of muscles.	algorithm;design of experiments;electromyography;experiment;isometric projection;largest;limb structure;muscle;non-negative matrix factorization;randomness;sample variance;subgroup;synergy	Katherine Muterspaugh Steele;Matthew C. Tresch;Eric J. Perreault	2013		10.3389/fncom.2013.00105	simulation;computer science;bioinformatics;operations research;non-negative matrix factorization	SE	11.674358111017513	-70.33087977077076	12508
cb6d41fbdfa89a95381e1e3e6b237192247e594e	statistical morphological filters for binary image processing	mathematical morphology;probability;image processing;lattices;extensivity properties statistical morphological filters binary image processing statistical morphological operators digitized version mean field approximation noise shape information binary statistical dilation binary statistical erosion;binary image;extensivity properties;filters;bayesian methods;statistical morphological filters;morphological operation;set theory;binary statistical dilation;filters image processing morphology noise level probability educational institutions noise shaping shape lattices bayesian methods;binary image processing;mathematical morphology image processing statistical analysis set theory digital filters noise;statistical morphological operators;morphology;binary statistical erosion;shape information;noise level;shape;statistical analysis;digital filters;noise shaping;mean field approximation;digitized version;noise	A new class of statistical morphological operators for binary image processing is introduced. These operators are based on a digitized version of the mean field approximation. The main advantage of the new operators is provided by the capability of taking into account both noise and shape information. Binary statistical dilation (BSD) and binary statistical erosion (BSE) are considered as a case study. Extensivity properties of BSD and BSE are also discussed. >	binary image;gabor filter;image processing	Carlo S. Regazzoni;Anastasios N. Venetsanopoulos;Gian Luca Foresti;Gianni Vernazza	1994		10.1109/ICASSP.1994.389550	computer vision;mathematical morphology;digital filter;noise shaping;morphology;binary image;image processing;bayesian probability;shape;noise;mean field theory;pattern recognition;lattice;probability;mathematics;statistics;set theory	Vision	51.29551260660857	-66.27016838134955	12513
035de10b2a81081e48e008c1ea040b245b9f02af	icdar 2013 competition on gender prediction from handwriting	handwriting recognition;gender issues	The prediction of gender from handwriting is a very interesting research field. However, no standard benchmark is available for researchers in this field. The aim of this competition is to gather researchers and compare recent advances in gender prediction from handwriting. This competition has been hosted on Kaggle, it has attracted 194 teams from both academia and industry. This paper gives details on this competition, including the dataset used, the evaluation procedure and description of participating methods and their performances.	benchmark (computing);international conference on document analysis and recognition;performance	Abdelaali Hassaïne;Somaya Al-Máadeed;Jihad Mohamad Jaam;Ali Jaoua	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.286	simulation;speech recognition;computer science;machine learning;multimedia;handwriting recognition	Vision	19.498306776005617	-59.14943833249023	12514
1e190811e8f40a5fbc625ef0401f12893904873d	early diagnosis of heart disease using classification and regression trees	patient diagnosis;echocardiograms early heart disease diagnosis regression trees classification trees heart defects medical treatment heart sound segmentation heart disease classification heart disease detection heart sound diagnostic system patient auscultation;trees mathematics;phonocardiography;pattern classification;diseases;regression analysis;heart pediatrics pathology feature extraction diseases phonocardiography sensitivity;medical signal processing;trees mathematics diseases medical signal processing patient diagnosis pattern classification phonocardiography regression analysis	Early diagnosis of heart defects are very important for medical treatment. In this paper, we propose an automatic method to segment heart sounds, which applies classification and regression trees. The diagnostic system, designed and implemented for detecting and classifying heart diseases, has been validated with a representative dataset of 116 heart sound signals, taken from healthy and unhealthy medical cases. The ultimate goal of this research is to implement a heart sounds diagnostic system, to be used to help physicians in the auscultation of patients, with the goal of reducing the number of unnecessary echocardiograms and of preventing the release of newborns that are in fact affected by a heart disease. In this study, 99.14% accuracy, 100% sensitivity, and 98.28% specificity were obtained on the dataset used for experiments.	code;decision tree;decision tree learning;experiment;sensitivity and specificity;sensor;software system;statistical classification	Amir Mohammad Amiri;Giuliano Armano	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6707080	computer science;machine learning;regression analysis;statistics	Vision	15.895809424638404	-88.73722960858697	12523
8a5427390f2427be76daa47facc26b9c6d8e8ed1	pixel-based machine learning in computer-aided diagnosis of lung and colon cancer		Computer-aided diagnosis (CAD) for detection of lesions in medical images has been an active area of research. Machine learning plays an essential role in CAD, because representing lesions and organs requires a complex model that has a number of parameters to determine; thus, medical pattern recognition essentially requires “learning from examples” to determine the parameters of the model. Machine learning has been used to classify lesions into certain classes (e.g., abnormal or normal, lesions or non-lesions, and malignant or benign) in CAD. Recently, as available computational power increased dramatically, pixel/voxel-based machine learning (PML) has emerged in medical image processing/analysis, which uses pixel/voxel values in local regions (or patches) in images instead of features calculated from segmented regions as input information; thus, feature calculation or segmentation is not required. Because PML can avoid errors caused by inaccurate feature calculation and segmentation, the performance of PML can potentially be higher than that of common classifiers. In this chapter, MTANNs (a class of PML) in CAD schemes for detection of lung nodules in CT and for detection of polyps in CTC are presented.	colon classification;machine learning;pixel	Kenji Suzuki	2014		10.1007/978-3-642-40017-9_5	oncology	AI	32.88844951363977	-75.81878072550629	12528
06c4ad425d76d56d4ea2ab310f7e3e690ac0d7c1	accurate boundary localization using dynamic programming on snakes	image sampling;statistical data fusion snake curvature guided importance sampling hmm viterbi algorithm;dynamic programming;viterbi search;snakes;snakes boundary localization dynamic programming contour extraction computer vision medical imaging medical tracking image measurement deterministic iterative statistical data fusion approach hidden markov model viterbi search mean square error;hidden markov model;edge detection;image fusion;mean square error methods dynamic programming edge detection feature extraction hidden markov models image fusion image sampling iterative methods;medical tracking;curvature guided importance sampling;hmm;dynamic programming hidden markov models deformable models computer vision biomedical imaging image converters energy measurement iterative methods data mining viterbi algorithm;snake;dynamic program;data fusion;force;boundary localization;computer vision;statistical data fusion;iterative methods;deterministic iterative statistical data fusion approach;computational modeling;hidden markov models;shape;medical image;viterbi algorithm;mean square error;feature extraction;medical imaging;contour extraction;mean square error methods;image measurement;shape priors;importance sampling;deformable model;monte carlo methods;noise	The extraction of contours using deformable models, such as snakes, is a problem of great interest in computer vision, particular in areas of medical imaging and tracking. Snakes have been widely studied, and many methods are available. In most cases, the snake converges towards the optimal contour by minimizing a sum of internal (prior) and external (image measurement) energy terms. This approach is elegant, but frequently mis-converges in the presence of noise or complex contours. To address these limitations, a novel discrete snake is proposed which treats the two energy terms separately. Essentially, the proposed method is a deterministic iterative statistical data fusion approach, in which the visual boundaries of the object are extracted, ignoring any prior, employing a hidden Markov model (HMM) and Viterbi search, and then applying importance sampling to the boundary points, on which the shape prior is asserted. The proposed implementation is straightforward and achieves dramatic speed and accuracy improvement. Compared to four other published methods and across six different images (two original, four published), the proposed method is demonstrated to be, on average, 7 times faster with a 45 percent reduction in the mean square error. Only the proposed method was able to successfully segment the desired object in each test image.	computer vision;contour line;dynamic programming;estimation theory;experiment;geomatics;hidden markov model;image noise;importance sampling;iteration;iterative method;markov chain;mean squared error;medical imaging;robustness (computer science);sampling (signal processing);standard test image;synthetic intelligence;viterbi algorithm	Akshaya Kumar Mishra;Paul W. Fieguth;David A. Clausi	2008	2008 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2008.13	computer vision;edge detection;feature extraction;shape;viterbi algorithm;importance sampling;computer science;noise;machine learning;dynamic programming;pattern recognition;active contour model;mean squared error;sensor fusion;iterative method;image fusion;computational model;force;hidden markov model;statistics;monte carlo method	Vision	48.64900465678519	-74.7681688060767	12529
fbf1d93704ee40b2119116801b7ad3773a0b565c	multi-scale characteristics of resampled fetal heart rate pattern provide novel fetal developmental indices	magnetocardiography;time scale;complexity theory;functional integration;time series analysis complexity theory entropy fetal heart rate heart beat educational institutions;time series;autonomic nervous system;time series magnetocardiography medical signal processing obstetrics;algorithms cardiotocography diagnosis computer assisted fetal heart heart rate humans reproducibility of results sample size sensitivity and specificity;heart rate;time series analysis;prenatal diagnosis multiscale characteristics fetal developmental indices functional integrity fetal maturation autonomic nervous system prenatal development multiscale complexity fetal magnetocardiographic recordings time irreversibility resampled fetal heart rate time series developmental disorders;developmental disorder;entropy;fetal heart rate;prenatal diagnosis;medical signal processing;heart beat;obstetrics;scale dependence	The increasing functional integrity of the organism during fetal maturation is connected with increasing complex internal coordination mediated by the autonomic nervous system. We hypothesize that time scales of complex and dynamic inter-dependencies over more than one heart beat interval reflect the increasing complex adjustments within the fetal organism during its prenatal development. We investigated multi-scale complexity and time irreversibility from equidistantly resampled heart rate time series of 73 fetal magnetocardiographic recordings over the third trimester. We found scale dependent changes in complexity and time irreversibility. The functions obtained from equidistantly resampled heart rate time series showed qualitatively similar curves compared to those obtained from heart beat intervals series previously reported. Time scales of fetal heart rate characteristics may provide novel information for the identification of developmental disorders in prenatal diagnosis.	autonomic computing;autonomic nervous system;biologic development;cns disorder;developmental disabilities;fetal heart rate pattern;numerous;time series	Dirk Hoyer;Samuel Nowack;Uwe Schneider	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090336	endocrinology;medicine;pathology;time series;biological engineering;physics;quantum mechanics;statistics	Visualization	18.81674352066733	-84.85214472358086	12534
637394d762a3a9139b9f4fbe73a9c93f69510992	robust scene extraction using multi-stream hmms for baseball broadcast	evaluation performance;tecnologia electronica telecomunicaciones;performance evaluation;classification non supervisee;hidden markov model;evaluacion prestacion;robust statistics;modele markov variable cachee;environmental conditions;hmm;radiodifusion;segmentation;video broadcasting;probabilistic approach;scene extraction;diffusion video;hidden markov models;enfoque probabilista;approche probabiliste;robustesse;clasificacion no supervisada;adaptive method;signal classification;classification signal;unsupervised classification;robustness;game adaptation;broadcasting;tecnologias;difusion de senales de video;grupo a;radiodiffusion;segmentacion;cbvir;robustez	In this paper, we propose a robust statistical framework for extracting scenes from a baseball broadcast video. We apply multistream hidden Markov models (HMMs) to control the weights among different features. To achieve a large robustness against new scenes, we used a common simple structure for all the HMMs. In addition, scene segmentation and unsupervised adaptation were applied to achieve greater robustness against differences in environmental conditions among games. The F-measure of scene-extracting experiments for eight types of scene from 4.5 hours of digest data was 77.4% and was increased to 78.7% by applying scene segmentation. Furthermore, the unsupervised adaptation method improved precision by 2.7 points to 81.4%. These results confirm the effectiveness of our framework. key words: CBVIR, HMM, scene extraction, game adaptation	content-based image retrieval;cryptographic hash function;experiment;hidden markov model;markov chain;unsupervised learning	Nguyen Huu Bach;Koichi Shinoda;Sadaoki Furui	2006	IEICE Transactions	10.1093/ietisy/e89-d.9.2553	robust statistics;simulation;speech recognition;computer science;machine learning;segmentation;broadcasting;hidden markov model;robustness	Vision	32.620073402817155	-68.45646343834314	12543
e01c5b8ab9577063b14e1e470f3dc82825e0f01d	fast-sl: an efficient algorithm to identify synthetic lethal sets in metabolic networks		MOTIVATION Synthetic lethal sets are sets of reactions/genes where only the simultaneous removal of all reactions/genes in the set abolishes growth of an organism. Previous approaches to identify synthetic lethal genes in genome-scale metabolic networks have built on the framework of flux balance analysis (FBA), extending it either to exhaustively analyze all possible combinations of genes or formulate the problem as a bi-level mixed integer linear programming (MILP) problem. We here propose an algorithm, Fast-SL, which surmounts the computational complexity of previous approaches by iteratively reducing the search space for synthetic lethals, resulting in a substantial reduction in running time, even for higher order synthetic lethals.   RESULTS We performed synthetic reaction and gene lethality analysis, using Fast-SL, for genome-scale metabolic networks of Escherichia coli, Salmonella enterica Typhimurium and Mycobacterium tuberculosis. Fast-SL also rigorously identifies synthetic lethal gene deletions, uncovering synthetic lethal triplets that were not reported previously. We confirm that the triple lethal gene sets obtained for the three organisms have a precise match with the results obtained through exhaustive enumeration of lethals performed on a computer cluster. We also parallelized our algorithm, enabling the identification of synthetic lethal gene quadruplets for all three organisms in under 6 h. Overall, Fast-SL enables an efficient enumeration of higher order synthetic lethals in metabolic networks, which may help uncover previously unknown genetic interactions and combinatorial drug targets.   AVAILABILITY AND IMPLEMENTATION The MATLAB implementation of the algorithm, compatible with COBRA toolbox v2.0, is available at https://github.com/RamanLab/FastSL CONTACT: kraman@iitm.ac.in   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	algorithm;autocatalytic set;bioinformatics;black and burst;computational complexity theory;computer cluster;dhrystone;drug delivery systems;fast fourier transform;flux balance analysis;gene deletion;genus mycobacterium;integer (number);integer programming;interaction;linear programming;matlab;mycobacterium tuberculosis;parallel computing;sl (complexity);salmonella enterica;sleep polysomnography domain;synthetic intelligence;time complexity	Aditya Pratapa;Shankar Balachandran;Karthik Raman	2015	Bioinformatics	10.1093/bioinformatics/btv352	mathematical optimization;bioinformatics;machine learning;mathematics	Comp.	0.23441005744638369	-52.081704014925165	12550
927bb65a9c37c93040da2c46cc5a44868fcd41dc	optimal parameter estimation for mrf stereo matching	modelizacion;corresponding author;proceso markov;availability;disponibilidad;optimal estimation;conjugate point;disparity;search strategy;probabilistic approach;disparidad;three dimensional;markov random field;taboo search;identificacion sistema;modelisation;stereo matching;realite terrain;system identification;enfoque probabilista;approche probabiliste;processus markov;reactive tabu search;strategie recherche;markov process;estimacion parametro;realidad terreno;tabu search;ground truth;parameter estimation;estimation optimale;estimation parametre;local minima;winner take all;modeling;disponibilite;disparite;identification systeme;3d reconstruction;busqueda tabu;recherche tabou;estimacion optima;estrategia investigacion	This paper presents an optimisation technique to select automatically a set of control parameters for a Markov Random Field applied to stereo matching. The method is based on the Reactive Tabu Search strategy, and requires to define a suitable fitness function that measures the performance of the MRF stereo algorithm with a given parameters set. This approach have been made possible by the recent availability of ground-truth disparity maps. Experiments with synthetic and real images illustrate the approach.	algorithm;binocular disparity;computer stereo vision;estimation theory;fitness function;ground truth;map;markov chain;markov random field;mathematical optimization;performance tuning;synthetic intelligence;tabu search	Riccardo Gherardi;Umberto Castellani;Andrea Fusiello;Vittorio Murino	2005		10.1007/11553595_100	3d reconstruction;winner-take-all;optimal estimation;three-dimensional space;availability;mathematical optimization;systems modeling;system identification;ground truth;tabu search;computer science;artificial intelligence;machine learning;maxima and minima;markov process;estimation theory;statistics	Vision	50.046391281471216	-58.678294038395876	12551
42a3b1e333b1cb0c64fcdf5291caf3394dff21ee	cognitive control: social evolution and emotional regulation	componential;emotional regulation;social complexity;attentional biasing	This commentary argues that theories of cognitive control risk being incomplete unless they incorporate social/emotional factors. Social factors very likely played a critical role in the evolution of human cognitive control abilities, and emotional states are the primary regulatory mechanisms of cognitive control.	partial;published comment;theory	Matt J. Rossano	2011	Topics in cognitive science	10.1111/j.1756-8765.2011.01131.x	psychology;cognitive psychology;social cognition;social complexity;developmental psychology;emotional expression;emotional exhaustion;social psychology	AI	14.004839189571895	-75.6677462757304	12557
f6c85bec044767d1e4573b73c070ac500467d5da	accessibility of microrna binding sites in metastable rna secondary structures in the presence of snps	research outputs;research publications	MOTIVATION We study microRNA (miRNA) bindings to metastable RNA secondary structures close to minimum free energy conformations in the context of single nucleotide polymorphisms (SNPs) and messenger RNA (mRNA) concentration levels, i.e. whether features of miRNA bindings to metastable conformations could provide additional information supporting the differences in expression levels of the two sequences defined by a SNP. In our study, the instances [mRNA/3'UTR; SNP; miRNA] were selected based on strong expression level analyses, SNP locations within binding regions and the computationally feasible identification of metastable conformations.   RESULTS We identified 14 basic cases [mRNA; SNP; miRNA] of 3' UTR-lengths ranging from 124 up to 1078 nt reported in recent literature, and we analyzed the number, structure and miRNA binding to metastable conformations within an energy offset above mfe conformations. For each of the 14 instances, the miRNA binding characteristics are determined by the corresponding STarMir output. Among the different parameters we introduced and analyzed, we found that three of them, related to the average depth and average opening energy of metastable conformations, may provide supporting information for a stronger separation between miRNA bindings to the two alleles defined by a given SNP.   AVAILABILITY AND IMPLEMENTATION At http://kks.inf.kcl.ac.uk/MSbind.html the MSbind tool is available for calculating features of metastable conformations determined by putative miRNA binding sites.		Luke Day;Ouala Abdelhadi Ep Souki;Andreas Alexander Albrecht;Kathleen Steinhöfel	2014	Bioinformatics	10.1093/bioinformatics/btt695	biology;molecular biology;bioinformatics;genetics	Comp.	3.5730843793554867	-60.421498588602766	12559
36de07d2263ed9a7f6fcf359a05d35afbe5143d2	capture of lexical but not visual resources by task-irrelevant emotional words: a combined erp and steady-state visual evoked potential study	human eeg;p2 and n400 component;emotional word processing;visual attention;steady state visual evoked potential	Numerous studies have found that emotionally arousing faces or scenes capture visual processing resources. Here we investigated whether emotional distractor words capture attention in an analogous way. Participants detected brief intervals of coherent motion in an array of otherwise randomly moving squares superimposed on words of positive, neutral or negative valence. Processing of the foreground task was assessed by behavioural responses and steady-state visual evoked potentials (SSVEPs) elicited by the squares flickering at 15 Hz. Although words were task-irrelevant, P2 and N400 deflections to negative words were enhanced, indicating that emotionally negative word content modulated lexico-semantic processing and that emotional significance was detected. In contrast, the time course of behavioural data and SSVEP amplitudes revealed no interference with the task regardless of the emotional connotation of distractor words. This dissociation of emotion effects on early perceptual versus lexical stages of processing suggests that written emotional words do not inevitably lead to attentional modulation in early visual areas. Prior studies have shown a distraction effect of emotional pictures on a similar task. Thus, our results indicate the specificity of emotion effects on sensory processing and semantic encoding dependent on the information channel that emotional significance is derived from.	acoustic evoked brain stem potentials;coherence (physics);distraction - pain management method;erp;face;feedback;hertz (hz);interference (communication);lexico;modulation;photopsia;probability amplitude;randomness;relevance;sensitivity and specificity;sensory process;steady state;wakefulness	Sophie M. Trauer;Søren K. Andersen;Sonja A. Kotz;Matthias M. Müller	2012	NeuroImage	10.1016/j.neuroimage.2011.12.016	psychology;cognitive psychology;communication;social psychology	HCI	15.004835634330666	-77.20005416559107	12576
41347ad6a744dc0cbe98a3005d08bd8caadf08a1	gpu acceleration of nonlinear diffusion tensor estimation using cuda and mpi	levenberg marquardt algorithm;nonlinear diffusion tensor estimation;diffusion tensor imaging dti;high performance clusters;load balance;graphics processing unit gpu	Diffusion MRI is a non-invasive magnetic resonance technique and has been increasingly used in imaging neuroscience. It is currently the only method capable of depicting the complex structure of white matter of the brain in vivo. One of the most popular techniques is diffusion tensor imaging (DTI) which is commonly used clinically to produce in vivo images of biological tissues with local microstructural characteristics such as water diffusion. Diffusion tensor maps are usually computed on a voxel-by-voxel basis by fitting the signal intensities of diffusion weighted images as a function of their corresponding data acquisition parameters (b-matrices). This processing is computation-intensive and time-consuming which can constraint the clinical practice of DTI. This study presents the application of using high performance GPU clusters in addition to CPUs for diffusion tensor estimation by accelerating the multivariate non-linear regression. The results are tested using both simulated and clinical diffusion datasets and show significant performance gain in tensor fitting. The proposed GPU implementation framework can significantly reduce the processing time of DTI data especially for the datasets with high spatial and temporal resolution, and hence further promote the clinical use of DTI. It also can be used to accelerate statistical analysis of DTI where Monte Carlo simulations are employed, be readily applied to quantitative assessment of DTI using bootstrap analysis, robust diffusion tensor estimation and should be applicable to other MR imaging techniques that use univariate or multivariate regression to fit MRI data	cuda;central processing unit;computation;data acquisition;gpu cluster;general linear model;graphics processing unit;image processing;levenberg–marquardt algorithm;medical imaging;message passing interface;monte carlo method;nonlinear system;resonance;robustness (computer science);scalability;simulation;single-core;speedup;video-in video-out;voxel	Lin-Ching Chang;Esam El-Araby;Vinh Q. Dang;Lam H. Dao	2014	Neurocomputing	10.1016/j.neucom.2013.12.035	mathematical optimization;levenberg–marquardt algorithm;computer science;load balancing;machine learning;computer graphics (images)	Vision	48.47948599927217	-80.55044951163202	12593
15661861e0363e1aa918879607008267aaeec70c	an object-distortion based image quality similarity	databases;distortion measurement;object detection feature extraction image representation;surf similarity image quality assessment iqa object distortion speed up robust features surf;image quality databases distortion measurement robustness predictive models feature extraction;feature extraction;image quality;robustness;predictive models;image quality similarity prediction accuracy iqa database distorted image reference image multilevel surf descriptors speed up robust features object detecting feature iqa metric representative metrics human subjective evaluation image quality assessment object distortion	Image quality assessment (IQA) aims to devise perceptual models to predict the image quality consistently with human subjective evaluation. The representative metrics focus on measuring the image quality with low-level features. In this letter, we assumed that the distortion in specific regions containing semantically significant objects would be enhanced by HVS significantly. According to this hypothesis, a novel IQA metric based on a commonly used object-detecting feature, Speed Up Robust Features (SURF), was proposed. First, it determined the interest points which represented significant objects through the SURF features both on the reference image and distorted image. Then it computed the multilevel SURF descriptors differences between the reference image and the distorted one. Finally, all the difference results were combined with a suitable pooling strategy. Comparing with other nine state-of-the-art IQA models on three biggest IQA databases, SURF-SIM demonstrated its highly competitive prediction accuracy especially on complicated applications and excellent robustness across different distortion types.	benchmark (computing);database;distortion;high- and low-level;human visual system model;image quality;interest point detection;object-based language;sensor;speeded up robust features	Fulai Wang;Xian Sun;Zhi Guo;Yu Huang;Kun Fu	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2413891	image quality;computer vision;feature detection;feature extraction;computer science;machine learning;pattern recognition;data mining;predictive modelling;robustness	Vision	36.024390549132505	-56.2657013406234	12607
45468822d6e950f0a96f75802e6eb22f20983ce5	three-dimensional object recognition using support vector machine neural network based on moment invariant features	neural network;three dimensional;support vector machine;object recognition		artificial neural network;image moment;outline of object recognition;support vector machine	Mohammed S. Abdel-Wahaab;Sayed F. Bahgat;Ashraf S. Hussein;Doaa M. Hegazy	2003			structured support vector machine;computer science;3d single-object recognition;artificial neural network;machine learning;support vector machine;cognitive neuroscience of visual object recognition;relevance vector machine;invariant (mathematics);artificial intelligence;pattern recognition	ML	30.35237618702473	-57.89580832459145	12609
73e2787456c1267f90db44272e69a653b33e1a3b	shape contexts and gabor features for face description and authentication	shape authentication data mining robustness lighting face recognition eyes nose mouth image recognition;image texture face recognition image matching feature extraction;image matching;correspondence problem;image texture;face recognition;shape matching;feature extraction;shape matched jets shape contexts gabor features face description face authentication shape matching algorithm feature computation face images extraction	In this paper, we use a robust shape matching algorithm for establishing correspondences between points from face images, and Gabor vectors (jets) for feature computation. After extracting characteristic lines from faces, which is accomplished by using the ridges and valleys operator, shape is sampled through a set of points, where jets are calculated. So, each face is depicted by /spl Rfr//sup 2/ points, and their respective jets. Once two sets of points from face images have been extracted, the shape matching algorithm is used to solve the correspondence problem. A measure between the set of matched points gives an idea of the similarity of the faces. Also, comparison between shape-matched jets is possible, given that they are supposed to be located over the same face parts, leading to a system that is able to take shape and texture into account for face authentication.	algorithm;authentication;computation;correspondence problem;shape context	Daniel González-Jiménez;José Luis Alba-Castro	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530217	facial recognition system;image texture;computer vision;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;mathematics;correspondence problem	Vision	39.87204949561394	-57.29935957177407	12610
72417b812ac7eeb0f24fec1a88fd64f3e9ecc560	a biometric identification system based on eigenpalm and eigenfinger features	hand;doigt;base donnee;karhunen loeve transformation;image segmentation;image processing;threshold detection;eigenpalms;image matching;biometrie;analisis forma;biometrics;database;biometria;procesamiento imagen;base dato;image classification;intelligence artificielle;biometrics fingers feature extraction humans error analysis fingerprint recognition geometry shape system testing image databases;indexing terms;classification;traitement image;k l transform;identificacion sistema;detection seuil;image recognition eigenpalm feature eigenfinger feature multimodal biometric identification system human hand;deteccion umbral;system identification;feature extraction;eigenfingers;algorithms artificial intelligence biometry computer security dermatoglyphics fingers hand humans image enhancement image interpretation computer assisted information storage and retrieval pattern recognition automated photography reproducibility of results sensitivity and specificity;image segmentation fingerprint identification feature extraction image matching image classification;equal error rate;eigenfingers index terms biometrics multimodal systems hand based identification k l transform eigenpalms;mano;error rate;hand based identification;multimodal system;finger;artificial intelligence;multimodal systems;pattern analysis;inteligencia artificial;transformation karhunen loeve;main;dedo;transformacion karhunen loeve;clasificacion;identification systeme;analyse forme;fingerprint identification;index terms biometrics	This paper presents a multimodal biometric identification system based on the features of the human hand. We describe a new biometric approach to personal identification using eigenfinger and eigenpalm features, with fusion applied at the matching-score level. The identification process can be divided into the following phases: capturing the image; preprocessing; extracting and normalizing the palm and strip-like finger subimages; extracting the eigenpalm and eigenfinger features based on the K-L transform; matching and fusion; and, finally, a decision based on the (k, l)-NN classifier and thresholding. The system was tested on a database of 237 people (1,820 hand images). The experimental results showed the effectiveness of the system in terms of the recognition rate (100 percent), the equal error rate (EER = 0.58 percent), and the total error rate (TER = 0.72 percent).	biometrics;bit error rate;clients;clinical use template;database;enhanced entity–relationship model;hand geometry;large;linear discriminant analysis;matching;multimodal interaction;multiple discriminant analysis;palmar surface;percent (qualifier value);preprocessor;projections and predictions;prototype;thresholding (image processing)	Slobodan Ribaric;Ivan Fratric	2005	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2005.209	fingerprint;computer vision;contextual image classification;speech recognition;index term;system identification;image processing;feature extraction;biological classification;word error rate;computer science;machine learning;pattern recognition;image segmentation;biometrics	Vision	33.171025074468126	-62.14095419133947	12619
6e0ba3f74a62750368d220011594a437378c216b	a facial expression recognition system using neural networks	mathematical morphology;facial expression recognition;multilayer perceptrons;multilayer perceptron network automatic facial expression recognition system neural network classifiers rough contour estimation routine mathematical morphology point contour detection method precise contour extraction eyebrows eyes mouth action units muscle movement characteristic point movements radial basis function network;neural network classifier;multilayer perceptron;radial basis function networks;face recognition;face recognition neural networks face detection humans morphology eyebrows eyes mouth shape muscles;radial basis function network;action unit;mathematical morphology face recognition radial basis function networks multilayer perceptrons;facial expression;neural network	This paper proposes an automatic facial expression recognition system using neural network classifiers. First, we use the rough contour estimation routine, mathematical morphology, and point contour detection method to extract the precise contours of the eyebrows, eyes, and mouth of a face image. Then we define 30 facial characteristic points to describe the position and shape of these three organs. Facial expressions can be described by combining different action units that are used for describing the basic muscle movement of a human face. We choose six main action units, being composed of facial characteristic points movements, as the input vectors for two different neural network-based expression classifiers including radial basis function network and multilayer perceptron network. Using these two networks, we have obtained the same recognition rate as high as 92.1%. Simulation results by the computer demonstrate that computers are capable of extracting high-level or abstract information like human.		Jyh-Yeong Chang;Jia-Lin Chen	1999		10.1109/IJCNN.1999.836232	facial recognition system;computer vision;mathematical morphology;computer science;machine learning;pattern recognition;time delay neural network;multilayer perceptron;facial expression;radial basis function network;artificial neural network	ML	33.015101992830054	-68.88204805775482	12631
8492ee66c8c90c792bde275c408388dd59062c7d	a perl package and an alignment tool for phylogenetic networks	evolution molecular;software;web based applications;comparative analysis;phylogeny;proteome;signal transduction;sequence analysis dna;phylogenetic network;computational biology bioinformatics;phylogenetic tree;java applet;lateral gene transfer;algorithms;molecular sequence data;sequence alignment;combinatorial libraries;base sequence;computer appl in life sciences;gene expression profiling;programming languages;open source;microarrays;bioinformatics	Phylogenetic networks are a generalization of phylogenetic trees that allow for the representation of evolutionary events acting at the population level, like recombination between genes, hybridization between lineages, and lateral gene transfer. While most phylogenetics tools implement a wide range of algorithms on phylogenetic trees, there exist only a few applications to work with phylogenetic networks, none of which are open-source libraries, and they do not allow for the comparative analysis of phylogenetic networks by computing distances between them or aligning them. In order to improve this situation, we have developed a Perl package that relies on the BioPerl bundle and implements many algorithms on phylogenetic networks. We have also developed a Java applet that makes use of the aforementioned Perl package and allows the user to make simple experiments with phylogenetic networks without having to develop a program or Perl script by him or herself. The Perl package is available as part of the BioPerl bundle, and can also be downloaded. A web-based application is also available (see availability and requirements). The Perl package includes full documentation of all its features.	algorithm;alignment;bioperl;computation (action);crossbreeding;distance;documentation;existential quantification;experiment;gene transfer;generalization (psychology);java programming language;java applet;lateral computing;lateral thinking;libraries;open-source software;perl;phylogenetic network;phylogenetic tree;phylogenetics;qualitative comparative analysis;requirement;trees (plant);web application	Gabriel Cardona;Francesc Rosselló;Gabriel Valiente	2007	BMC Bioinformatics	10.1186/1471-2105-9-175	computational biology;phylogenetic comparative methods;qualitative comparative analysis;biology;web application;phylogenetic tree;dna microarray;computer science;bioinformatics;computational phylogenetics;sequence alignment;proteome;horizontal gene transfer;gene expression profiling;phylogenetic network;t-rex;signal transduction;java applet;phylogenetics	Comp.	-0.6787839702424535	-58.53525171911257	12649
c61cc2689954b3a09cb757253487653fb7dd2c3a	three-dimensional model of electroretinogram field potentials in the rat eye		Objective: The information derived from the electroretinogram (ERG), especially with regard to local areas of retinal dysfunction or therapeutic rescue, can be enhanced by an increased understanding of the relationship between local retinal current sources and local ERG potentials measured at the cornea. A critical step in this direction is the development of a robust bioelectric field model of the ERG. Methods: A finite-element model was created to simulate ERG potentials at the cornea resulting from physiologically relevant transretinal currents. A magnetic resonance image of a rat eye was segmented to define all major ocular structures, tissues were assigned conductivity values from the literature. The model was optimized to multi-electrode ERG (meERG) data recorded in healthy rat eyes, and validated with meERG data from eyes with experimental lesions in peripheral retina. Results: Following optimization, the simulated distribution of corneal potentials was in good agreement with measured values; residual error was comparable to the average difference of individual eyes from the measured mean. The model predicted the corneal potential distribution for eight eyes with experimental lesions with similar accuracy, and a measure of pre- to post-lesion changes in corneal potential distribution was well correlated with the location of the lesion. Conclusion: An eye model with high anatomical accuracy was successfully validated against a robust dataset. Significance: This model can now be used for optimization of ERG electrode design, and to support functional mapping of the retina from meERG data via solving the inverse bioelectric source problem.	8 eyes;body tissue;current source;electroretinography;ewings sarcoma-primitive neuroectodermal tumor (pnet);eye;mathematical optimization;numerous;peripheral;personnameuse - assigned;resonance;retina;retinal diseases;silo (dataset);simulation	Ashley N. Selner;Zahra Derafshi;Brian E. Kunzer;John R. Hetling	2018	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2018.2816591	computer vision;retina;cornea;blind spot;biomedical engineering;retinal;artificial intelligence;erg;computer science;dimensional modeling;magnetic resonance imaging;peripheral retina	Visualization	27.995705284821256	-84.43879988158135	12654
15d0ef85756fd731945e9956a9f5def2cb1f9513	diffusion-based spatial priors for functional magnetic resonance images	maps;models neurological;brain;eigenmodes;spatial process;formal model;matrix variate normal density;bayesian inference;auditory system;fusiform face area;bayes theorem;lateral geniculate;fmri data;eigenvalues;bayesian model comparison;statistical model;activation;maximum likelihood estimate;non stationary spatial process;image interpretation computer assisted;expectation maximization;time series analysis;fisher scoring;auditory cortex;weighted graph laplacian;functional magnetic resonance images;magnetic resonance imaging;gaussian kernel;restricted maximum likelihood;algorithms;responses;covariance components;humans;diffusion kernel;human superior colliculus;gaussian process;weighted graph;graph laplacian;single subject fmri;visual cortex;stationary process;spatial priors	We recently outlined a Bayesian scheme for analyzing fMRI data using diffusion-based spatial priors [Harrison, L.M., Penny, W., Ashburner, J., Trujillo-Barreto, N., Friston, K.J., 2007. Diffusion-based spatial priors for imaging. NeuroImage 38, 677-695]. The current paper continues this theme, applying it to a single-subject functional magnetic resonance imaging (fMRI) study of the auditory system. We show that spatial priors on functional activations, based on diffusion, can be formulated in terms of the eigenmodes of a graph Laplacian. This allows one to discard eigenmodes with small eigenvalues, to provide a computationally efficient scheme. Furthermore, this formulation shows that diffusion-based priors are a generalization of conventional Laplacian priors [Penny, W.D., Trujillo-Barreto, N.J., Friston, K.J., 2005. Bayesian fMRI time series analysis with spatial priors. NeuroImage 24, 350-362]. Finally, we show how diffusion-based priors are a special case of Gaussian process models that can be inverted using classical covariance component estimation techniques like restricted maximum likelihood [Patterson, H.D., Thompson, R., 1974. Maximum likelihood estimation of components of variance. Paper presented at: 8th International Biometrics Conference (Constanta, Romania)]. The convention in SPM is to smooth data with a fixed isotropic Gaussian kernel before inverting a mass-univariate statistical model. This entails the strong assumption that data are generated smoothly throughout the brain. However, there is no way to determine if this assumption is supported by the data, because data are smoothed before statistical modeling. In contrast, if a spatial prior is used, smoothness is estimated given non-smoothed data. Explicit spatial priors enable formal model comparison of different prior assumptions, e.g., that data are generated from a stationary (i.e., fixed throughout the brain) or non-stationary spatial process. Indeed, for the auditory data we provide strong evidence for a non-stationary process, which concurs with a qualitative comparison of predicted activations at the boundary of functionally selective regions.	algorithmic efficiency;atomic resolution microscopy;biometrics;cns disorder;gaussian process;generalization (psychology);graph - visual representation;kernel;laplacian matrix;magnetic resonance imaging;mathematical model;model selection;normal statistical distribution;normal mode;sample variance;smoothing (statistical technique);stationary process;statistical model;super paper mario;time series;fmri	Lee M. Harrison;William D. Penny;Jean Daunizeau;Karl J. Friston	2008		10.1016/j.neuroimage.2008.02.005	statistical model;stationary process;fusiform face area;scoring algorithm;laplacian matrix;expectation–maximization algorithm;eigenvalues and eigenvectors;magnetic resonance imaging;machine learning;time series;pattern recognition;gaussian process;mathematics;restricted maximum likelihood;maximum likelihood;bayes' theorem;bayesian inference;gaussian function;statistics	ML	24.447637218727444	-75.44089630299446	12666
aabeee0dc44014772e8436421a1fc3716d0f6d74	diversity and coverage of structural sublibraries selected using the sage and sca algorithms		It is often impractical to synthesize and test all compounds in a large exhaustive chemical library. Herein, we discuss rational approaches to selecting representative subsets of virtual libraries that help direct experimental synthetic efforts for diverse library design. We compare the performance of two stochastic sampling algorithms, Simulating Annealing Guided Evaluation (SAGE; Zheng, W.; Cho, S. J.; Waller, C. L.; Tropsha, A. J. Chem. Inf. Comput. Sci. 1999, 39, 738-746.) and Stochastic Cluster Analysis (SCA; Reynolds, C. H.; Druker, R.; Pfahler, L. B. Lead Discovery Using Stochastic Cluster Analysis (SCA): A New Method for Clustering Structurally Similar Compounds J. Chem. Inf. Comput. Sci. 1998, 38, 305-312.) for their ability to select both diverse and representative subsets of the entire chemical library space. The SAGE and SCA algorithms were compared using u- and s-optimal metrics as an independent assessment of diversity and coverage. This comparison showed that both algorithms were capable of generating sublibraries in descriptor space that are diverse and give reasonable coverage (i.e. are representative) of the original full library. Tests were carried out using simulated two-dimensional data sets and a 27 000 compound proprietary structural library as represented by computed Molconn-Z descriptors. One of the key observations from this work is that the algorithmically simple SCA method is capable of selecting subsets that are comparable to the more computationally intensive SAGE method.	algorithm;chemical library;chemical procedure;cluster analysis;dhrystone;sampling (signal processing);simulated annealing;structure of superior cerebellar artery	Charles H. Reynolds;Alexander Tropsha;Lori B. Pfahler;Ross Druker;Subhas Chakravorty;G. Ethiraj;Weifan Zheng	2001	Journal of chemical information and computer sciences	10.1021/ci010041u	computer science;bioinformatics;data mining;algorithm	AI	12.014556106907458	-59.424371884980864	12678
d950f7b0ec450a4f574cc36d595137b07f0f51ab	enhanced dynamic programming approach for subunit modelling to handle segmentation and recognition ambiguities in sign language		Abstract Sign language serves as a primary means of communication among the deaf impaired community. The major challenges faced by the Sign Language Recognition (SLR) system are recognizing signs from large vocabularies in continuous video sequences. In this research paper, a novel subunit sign modelling framework is proposed for vision-based SLR which aims in solving the major issues in SLR systems. The problem of hand segmentation ambiguities and segregating epentheses movements between two adjacent signs in continuous video sequences are addressed. A novel subunit sign modelling framework is presented and illustrated to embark upon these problems while considering large-vocabularies. This framework is developed using a novel methodology of Enhanced Dynamic Programming (EDP) approach in subunit sign modelling. This EDP framework works with a combination of dynamic time warping and spatiotemporal clustering techniques. Since, sign language consists of both spatial and temporal feature vectors, dynamic time warping is used as a distance measure to compute the distance between two adjacent signs in sign trajectories. This distance is used as a temporal feature vector during the clustering of spatial feature vectors using Minimum Entropy Clustering (MEC). This process is done recursively to cluster all the epentheses movements dynamically without using any explicit or implicit modelling. Experimental results have confirmed that the computation cost of the proposed system is less because the epenthesis movements are eliminated before classification and the gesture base space utilized by the sign gestures is very low because the proposed system does not require any modelling to handle epenthesis movements. The results obtained from the proposed subunit sign modelling framework is compared with other existing models in order to prove that the proposed system is best among the existing systems.	dynamic programming	R. Elakkiya;K. Selvamani	2018	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2017.07.001	distributed computing;sign language;dynamic time warping;cluster analysis;computation;recursion;gesture;dynamic programming;computer science;computer vision;feature vector;pattern recognition;artificial intelligence	Vision	30.099649359483045	-60.70164788330746	12679
053ee0d88d0e14b73696d456bbf97d3c35016d00	compensating inhomogeneities of neuromorphic vlsi devices via short-term synaptic plasticity	parallel computing;biological patents;biomedical journals;text mining;europe pubmed central;citation search;neuromorphic hardware;citation networks;leaky integrate and fire neuron;short term synaptic plasticity;research articles;self regulation;abstracts;open access;life sciences;clinical guidelines;robustness;pcsim;full text;spiking neural networks;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Recent developments in neuromorphic hardware engineering make mixed-signal VLSI neural network models promising candidates for neuroscientific research tools and massively parallel computing devices, especially for tasks which exhaust the computing power of software simulations. Still, like all analog hardware systems, neuromorphic models suffer from a constricted configurability and production-related fluctuations of device characteristics. Since also future systems, involving ever-smaller structures, will inevitably exhibit such inhomogeneities on the unit level, self-regulation properties become a crucial requirement for their successful operation. By applying a cortically inspired self-adjusting network architecture, we show that the activity of generic spiking neural networks emulated on a neuromorphic hardware system can be kept within a biologically realistic firing regime and gain a remarkable robustness against transistor-level variations. As a first approach of this kind in engineering practice, the short-term synaptic depression and facilitation mechanisms implemented within an analog VLSI model of I&F neurons are functionally utilized for the purpose of network level stabilization. We present experimental data acquired both from the hardware model and from comparative software simulations which prove the applicability of the employed paradigm to neuromorphic VLSI devices.	analog;artificial neural network;compensating transaction;computation (action);depressive disorder;emulator;generic drugs;inspiration function;mixed-signal integrated circuit;network architecture;neural network simulation;neuromorphic engineering;parallel computing;programming paradigm;self-control as a personality trait;small;spiking neural network;synaptic package manager;transistor;very-large-scale integration;facilitation;negative regulation of synaptic vesicle recycling	Johannes Bill;Klaus Schuch;Daniel Brüderle;Johannes Schemmel;Wolfgang Maass;Karlheinz Meier	2010		10.3389/fncom.2010.00129	text mining;neuroscience;computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning;neuromorphic engineering;robustness	ML	16.995977364774017	-68.70481916294577	12689
85e0c810a79cb28778b44411973304f50420dd1b	a graphical model approach to automated classification of protein subcellular location patterns in multi-cell images	bayes theorem;models biological;image processing computer assisted;system performance;computational biology bioinformatics;cells;proteins;single cell;computational complexity;graphical representation;microscopy fluorescence;reproducibility of results;background knowledge;graphical model;protein subcellular location;algorithms;pattern recognition automated;humans;cellular structures;combinatorial libraries;high throughput;classification accuracy;computational biology;computer appl in life sciences;hela cells;microarrays;bioinformatics;automation	Knowledge of the subcellular location of a protein is critical to understanding how that protein works in a cell. This location is frequently determined by the interpretation of fluorescence microscope images. In recent years, automated systems have been developed for consistent and objective interpretation of such images so that the protein pattern in a single cell can be assigned to a known location category. While these systems perform with nearly perfect accuracy for single cell images of all major subcellular structures, their ability to distinguish subpatterns of an organelle (such as two Golgi proteins) is not perfect. Our goal in the work described here was to improve the ability of an automated system to decide which of two similar patterns is present in a field of cells by considering more than one cell at a time. Since cells displaying the same location pattern are often clustered together, considering multiple cells may be expected to improve discrimination between similar patterns. We describe how to take advantage of information on experimental conditions to construct a graphical representation for multiple cells in a field. Assuming that a field is composed of a small number of classes, the classification accuracy can be improved by allowing the computed probability of each pattern for each cell to be influenced by the probabilities of its neighboring cells in the model. We describe a novel way to allow this influence to occur, in which we adjust the prior probabilities of each class to reflect the patterns that are present. When this graphical model approach is used on synthetic multi-cell images in which the true class of each cell is known, we observe that the ability to distinguish similar classes is improved without suffering any degradation in ability to distinguish dissimilar classes. The computational complexity of the method is sufficiently low that improved assignments of classes can be obtained for fields of twelve cells in under 0.04 second on a 1600 megahertz processor. We demonstrate that graphical models can be used to improve the accuracy of classification of subcellular patterns in multi-cell fluorescence microscope images. We also describe a novel algorithm for inferring classes from a graphical model. The performance and speed suggest that the method will be particularly valuable for analysis of images from high-throughput microscopy. We also anticipate that it will be useful for analyzing the mixtures of cell types typically present in images of tissues. Lastly, we anticipate that the method can be generalized to other problems.	body tissue;class;computational complexity theory;elegant degradation;fluorescence;graphical model;greater than;high-throughput computing;microscope device component;numerous;personnameuse - assigned;probability;staphylococcal protein a;synthetic intelligence;throughput;algorithm;mixture	Shann-Ching Chen;Robert F. Murphy	2005	BMC Bioinformatics	10.1186/1471-2105-7-90	high-throughput screening;dna microarray;computer science;bioinformatics;theoretical computer science;automation;machine learning;computer performance;graphical model;bayes' theorem;computational complexity theory;algorithm	ML	7.014773836395255	-56.641124236637935	12693
b8c484519a8970bf4cb07c3c609c41a05365f258	classifying symmetrical differences and temporal change in mammography using deep neural networks		We investigate the addition of symmetry and temporal context information to a deep convolutional neural network (CNN) with the purpose of detecting malignant soft tissue lesions in mammography. We employ a simple linear mapping that takes the location of a mass candidate and maps it to either the contra-lateral or prior mammogram and regions of interest (ROI) are extracted around each location. Two different architectures are subsequently explored: (1) a fusion model employing two datastreams were both ROIs are fed to the network during training and testing and (2) a stage-wise approach where a single ROI CNN is trained on the primary image and subsequently used as feature extractor for both primary and contra-lateral or prior ROIs. A ’shallow’ gradient boosted tree (GBT) classifier is then trained on the concatenation of these features and used to classify the joint representation. The baseline obtained an AUC of 0.87 with confidence interval [0.853, 0.893]. For the analysis of symmetrical differences, the first architecture where both primary and contra-lateral patches are presented during training obtained an AUC of 0.895 with confidence interval [0.877, 0.913] and the second architecture where a new classifier is retrained on the concatenation an AUC of 0.88 with confidence interval [0.859, 0.9]. We found a significant difference between the first architecture and the baseline at high specificity p = 0.02. When using the same architectures to analyze temporal change we obtained an AUC of 0.884 with confidence interval [0.865, 0.902] for the first architecture and an AUC of 0.879 with confidence interval [0.858, 0.898] in the second setting. Although improvements for temporal analysis were consistent, they were not found to be significant. The results show our proposed method is promising and we suspect performance can greatly be improved when more temporal data becomes available.	artificial neural network;baseline (configuration management);computer-aided design;concatenation;convolutional neural network;deep learning;government and binding theory;gradient boosting;http 404;lateral thinking;map;marginal model;neural networks;nico habermann;randomness extractor;region of interest;sensitivity and specificity;sensor	Thijs Kooi;Nico Karssemeijer	2017	CoRR		convolutional neural network;artificial neural network;temporal database;concatenation;data mining;linear map;classifier (linguistics);mammography;computer science;pattern recognition;artificial intelligence;abnormality	ML	30.96300665143466	-76.04465055239403	12708
04b9ddb6acc1b3c70df8e7e24d97af67d3c8f85d	a machine learning-driven approach to computational physiological modeling of skin cancer		Melanoma is the most lethal form of skin cancer in the world. To improve the accuracy of diagnosis, quantitative imaging approaches have been investigated. While most quantitative methods focus on the surface of skin lesions via hand-crafted imaging features, in this work, we take a machine-learning approach where abstract quantitative imaging features are learned to model physiological traits. In doing so, we investigate skin cancer detection via computational modeling of two major physiological features of melanoma namely eumelanin and hemoglobin concentrations from dermal images. This was done via employing a non-linear random forest regression model to leverage the plethora of quantitative features from dermal images to build the model. The proposed method was validated by separability test applied to clinical images. The results showed that the proposed method outperforms state-of-the-art techniques on predicting the concentrations of the skin cancer physiological features in dermal images (i.e., eumelanin and hemoglobin).	computation;machine learning	Daniel S. Cho;Farzad Khalvati;David A. Clausi;Alexander Wong	2017		10.1007/978-3-319-59876-5_10	pattern recognition;computer science;random forest;skin cancer;machine learning;artificial intelligence	Vision	31.049704147734545	-77.06838736009767	12764
8f00b8fb80a5c0f4c8ef33ca9297f2008ed301a3	analysis of similarity/dissimilarity of dna sequences based on a class of 2d graphical representation	dna;sensitivity analysis;graphical representation;similarity;dna sequence	On the basis of a class of 2D graphical representations of DNA sequences, sensitivity analysis has been performed, showing the high-capability of the proposed representations to take into account small modifications of the DNA sequences. And sensitivity analysis also indicates that the absolute differences of the leading eigenvalues of the L/L matrices associated with DNA increase with the increase of the number of the base mutations. Besides, we conclude that the similarity analysis method based on the correlation angles can better eliminate the effects of the lengths of DNA sequences if compared with the method using the Euclidean distances. As application, the examination of similarities/dissimilarities among the coding sequences of the first exon of beta-globin gene of different species has been performed by our method, and the reasonable results verify the validity of our method.	bio-informatics;bioinformatics;biological database;cartesian closed category;computation;databases;dynamic programming;euclidean distance;globin;graphical user interface;interaction technique;mutation;phylogenetics;sequence tagged sites;sequence alignment;beta-globins	Yuhua Yao;Qi Dai;Xu-Ying Nan;Ping-An He;Zuo-Ming Nie;Song-Ping Zhou;Yao-Zhou Zhang	2008	Journal of computational chemistry	10.1002/jcc.20922	dna sequencing;discrete mathematics;similarity;sensitivity analysis;dna	Comp.	1.2649119971646214	-57.43150936979237	12774
b0a2b1fddc54ad8cdb3681ea6ae405f16209a104	deep learning with attribute profiles for hyperspectral image classification	neural networks;gray scale;machine learning;feature extraction;hyperspectral imaging	Effective spatial-spectral pixel description is of crucial significance for the classification of hyperspectral remote sensing images. Attribute profiles are considered as one of the most prominent approaches in this regard, since they can capture efficiently arbitrary geometric and spectral properties. Lately though, the advent of deep learning in its various forms has also led to remarkable classification performances by operating directly on hyperspectral input. In this letter, we explore the collaboration potential of these two powerful feature extraction approaches. Specifically, we propose a new strategy for hyperspectral image classification, where attribute filtered images are stacked and provided as input to convolutional neural networks. Our experiments with two real hyperspectral remote sensing data sets show that the proposed strategy leads to a performance improvement, as opposed to using each of the involved approaches individually.	artificial neural network;computer vision;convolution;convolutional neural network;deep learning;experiment;feature extraction;network architecture;patch (computing);performance;pixel	Erchan Aptoula;Murat Can Ozdemir;Berrin A. Yanikoglu	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2619354	computer vision;feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition;artificial neural network;grayscale	ML	25.052399675731102	-53.63299826491946	12783
549842c29a0ed95d7ad7223bbca6518dba660290	spak/spar two-component system characterized by a structure-driven                     domain-fusion method and in vitro phosphorylation studies	modelos moleculares;recombinant fusion proteins;escherichia coli;histidine kinase;structural model;mapeamento de interacao de proteinas;response regulator;protein function;reprodutibilidade dos testes;protein complex;dna binding proteins;proteinas de ligacao a dna;proteinas de bacterias;proteinas serina treonina quinases;proteinas recombinantes de fusao;phosphorylation;signal transduction;empirical method;modelos quimicos;transcription factors;models chemical;bacillus subtilis;dominios e motivos de interacao entre proteinas;protein interaction domains and motifs;models molecular;fosforilacao;biologia computacional;protein conformation;multi domain;structural homology protein;reproducibility of results;homology modeling;two component system;protein interaction mapping;computational biology;protein serine threonine kinases;conformacao proteica;wild type;homologia estrutural de proteina;fatores de transcricao;bacterial proteins;physical interaction	Here we introduce a quantitative structure-driven computational domain-fusion method, which we used to predict the structures of proteins believed to be involved in regulation of the subtilin pathway in Bacillus subtilis, and used to predict a protein-protein complex formed by interaction between the proteins. Homology modeling of SpaK and SpaR yielded preliminary structural models based on a best template for SpaK comprising a dimer of a histidine kinase, and for SpaR a response regulator protein. Our LGA code was used to identify multi-domain proteins with structure homology to both modeled structures, yielding a set of domain-fusion templates then used to model a hypothetical SpaK/SpaR complex. The models were used to identify putative functional residues and residues at the protein-protein interface, and bioinformatics was used to compare functionally and structurally relevant residues in corresponding positions among proteins with structural homology to the templates. Models of the complex were evaluated in light of known properties of the functional residues within two-component systems involving His-Asp phosphorelays. Based on this analysis, a phosphotransferase complexed with a beryllofluoride was selected as the optimal template for modeling a SpaK/SpaR complex conformation. In vitro phosphorylation studies performed using wild type and site-directed SpaK mutant proteins validated the predictions derived from application of the structure-driven domain-fusion method: SpaK was phosphorylated in the presence of (32)P-ATP and the phosphate moiety was subsequently transferred to SpaR, supporting the hypothesis that SpaK and SpaR function as sensor and response regulator, respectively, in a two-component signal transduction system, and furthermore suggesting that the structure-driven domain-fusion approach correctly predicted a physical interaction between SpaK and SpaR. Our domain-fusion algorithm leverages quantitative structure information and provides a tool for generation of hypotheses regarding protein function, which can then be tested using empirical methods.	adenosine triphosphate;binding (molecular function);bioinformatics;clinical use template;domino tiling;gls2 gene;gene regulatory network;histidine kinase;homologous gene;homology (biology);homology modeling;human–computer interaction;kanamycin kinase;numerous;phosphotransferases;stk39 gene;signal transduction;staphylococcal protein a;transduction (machine learning);wild type;algorithm;beryllium fluoride;inorganic phosphate;protein complex location	Anu Chakicherla;Carol L. Ecale Zhou;Martha Ligon Dang;Virginia Rodriguez;J. Norman Hansen;Adam Zemla	2009		10.1371/journal.pcbi.1000401	biology;biochemistry;two-component regulatory system;bioinformatics;genetics	Comp.	9.582940118568951	-60.52919489125731	12801
2f0e328a27a910bcdddb339623b652746f3e04b2	inter- and intramolecular cf···c˭o interactions on aliphatic and cyclohexane carbonyl derivatives	cf c o interactions;stereoelectronic effects;journal article;qd chemistry;cf co interactions;organofluorine compounds	Weak inter- and intra- molecular C(δ+)F(δ-)···C(δ+)=O(δ-) interactions were theoretically evaluated in 4 different sets of compounds at different theoretical levels. Intermolecular CH3F···C=O interactions were stabilizing by about 1 kcal mol(-1) for various carbonyl containing functional groups. Intramolecular CF···C=O interactions were also detected in aliphatic and fluorinated cyclohexane carbonyl derivatives. However, the stabilization provided by intramolecular CF···C=O interactions was not enough to govern the conformational preferences of compounds 2-4.	aldehydes;amides;carbonyl cyanide p-trifluoromethoxyphenylhydrazone;cardio-facio-cutaneous syndrome;chlorides;circa;clomiphene;cyclohexane;esters;fluorine;hippocampus (brain);hybrid functional;hydrogen;interaction;ketone measurement;ligands;maxima and minima;penalty method;population;potential energy surface;acyl-coa oxidase;alkyl;kilocalorie	Rodrigo A. Cormanich;Roberto Rittner;David O'Hagan;Michael Bühl	2016	Journal of computational chemistry	10.1002/jcc.23918	stereochemistry;chemistry;organic chemistry	Comp.	9.018853091427188	-63.10328591130724	12812
aa429faee190468a7f0b3014b971cb5e69323b2c	an evaluation framework for lossy compression of genome sequencing quality values		This paper provides the specification and an initial validation of an evaluation framework for the comparison of lossy compressors of genome sequencing quality values. The goal is to define reference data, test sets, tools and metrics that shall be used to evaluate the impact of lossy compression of quality values on human genome variant calling. The functionality of the framework is validated referring to two state-of-the-art genomic compressors. This work has been spurred by the current activity within the ISO/IEC SC29/WG11 technical committee (a.k.a. MPEG), which is investigating the possibility of starting a standardization activity for genomic information representation.	downstream (software development);lossless compression;lossy compression;moving picture experts group;personalization;specification;test set;whole genome sequencing;occupational medicine field	Claudio Alberti;Noah M. Daniels;Mikel Hernaez;Jan Voges;Rachel L. Goldfeder;Ana A. Hernandez-Lopez;Marco Mattavelli;Bonnie Berger	2016	2016 Data Compression Conference (DCC)	10.1109/DCC.2016.39	lossy compression;genomics;distortion;computer science;bioinformatics;theoretical computer science;sequential analysis;data mining;mathematics;metadata;statistics	Visualization	-3.8765007312000574	-64.52718340866743	12816
0ade94e9f6d00ddd246c559b6a0d315fdd809d30	diagnosis of alzheimer's disease based on virtual environments	hci;virtual reality diseases human computer interaction medical diagnostic computing;virtual environments;virtual environments alzheimer s disease dementia early detection screening tests hci;screening tests;alzheimer s disease;dementia;virtual worlds alzheimer s disease diagnosis virtual environments alzheimer s screening tests patient examination e health alzheimer s screening test immersive technologies human computer interaction systems hci systems virtual room screening mechanisms memory loss evaluation;early detection;dementia virtual environments glass sensors	Alzheimer's screening tests are currently used by doctors as part of a thorough patient examination. These tests are performed periodically in specific time intervals trying to estimate the patient's condition and stage as early as possible. This work proposes a novel e-health Alzheimer's screening test based on virtual environments using new immersive technologies combined with advanced Human Computer Interaction (HCI) systems. These new tests are focused on the immersion of the patient in a virtual room, in order to mislead and deceive the patient's mind. As a result, two new tests are introduced demonstrating the wide range of screening mechanisms that could be designed using virtual environments. The proposed tests are focused on the evaluation of memory loss related to common objects, recent conversations and events, the diagnosis of problems in expressing and understanding language and the ability to differentiate between virtual worlds and reality. The proposed screening tests were evaluated and tested using both patients and healthy adults in a comparative study with state-of-the-art Alzheimer's screening tests.	accessibility;head-mounted display;human computer;human–computer interaction;immersion (virtual reality);immersive technology;stereoscopy;virtual reality;virtual world	Juan Manuel Fernández Montenegro;Vasileios Argyriou	2015	2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)	10.1109/IISA.2015.7388023	simulation;medicine;human–computer interaction;multimedia	Visualization	8.694104579448675	-91.66754731839225	12826
d658d973c22dda2f014acc3d384c48d62bda6693	an optimized fuzzy clustering algorithm for brain magnetic resonance image segmentation		Magnetic Resonance Imaging (MRI) technique is an important adjunct to the clinical diagnosis of brain disease with its advantages of non-invasive and noninvasive. The precise segmentation of brain MR images is a significant issue for biomedical research and clinical applications. However, there are various defects in brain MR images, such as gray irregularities, noise, and low contrast, reducing the accuracy of the brain MR images segmentation. In this paper, we propose an optimization solution for a fuzzy clustering algorithm and apply it in brain MR image segmentation. A fuzzy C-means clustering algorithm is proposed based an anisotropic weight model. By introducing the new neighborhood weight calculation method, each point has the weight of anisotropy, effectively overcome the influence of noise on the image segmentation. In addition, two kinds of brain MR image segmentation models are proposed to address the low contrast defects in brain MR images. The local Gaussian probability model is included in the objective function of fuzzy clustering, and a clustering segmentation algorithm of fuzzy local Gaussian probability model is proposed. A clustering segmentation algorithm of adaptive scale fuzzy local Gaussian probability model is proposed. The neighborhood scale corresponding to each pixel in the image is automatically estimated, which improves the robustness of the model and achieves the purpose of precise segmentation.	algorithm;cluster analysis;experiment;fuzzy clustering;image noise;image segmentation;loss function;mathematical optimization;optimization problem;pixel;resonance;statistical model	Shaobo Zhang	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393075	robustness (computer science);fuzzy logic;image segmentation;cluster analysis;fuzzy clustering;pixel;computer science;linear programming;algorithm;medical imaging	Vision	43.56629139435976	-73.14645575891271	12837
246150fb81abc093b25374f3480d5ed9b1f7c252	quantifying the information transmitted in a single stimulus	neural code;satisfiability;place cell;mutual information;stimulus specific information;quantitative method;information theory	Information theory - in particular mutual information- has been widely used to investigate neural processing in various brain areas. Shannon mutual information quantifies how much information is, on average, contained in a set of neural activities about a set of stimuli. To extend a similar approach to single stimulus encoding, we need to introduce a quantity specific for a single stimulus. This quantity has been defined in literature by four different measures, but none of them satisfies the same intuitive properties (non-negativity, additivity), that characterize mutual information. We present here a detailed analysis of the different meanings and properties of these four definitions. We show that all these measures satisfy, at least, a weaker additivity condition, i.e. limited to the response set. This allows us to use them for analysing correlated coding, as we illustrate in a toy-example from hippocampal place cells.	contain (action);information theory;mathematics;mutual information;negativity (quantum mechanics);neural coding;numerical analysis;place cells;quantity;shannon (unit);disease transmission	Michele Bezzi	2007	Bio Systems	10.1016/j.biosystems.2006.04.009	variation of information;quantitative research;information theory;multivariate mutual information;theoretical computer science;machine learning;mathematics;mutual information;neural coding;interaction information;statistics;satisfiability;pointwise mutual information	ML	22.45546895170011	-72.32626619329608	12847
446fc708174304bf51513b4cf637bf5ec45421b7	recognition of human activities with wearable sensors	signal image and speech processing;quantum information technology spintronics	A novel approach for recognizing human activities with wearable sensors is investigated in this article. The key techniques of this approach include the generalized discriminant analysis (GDA) and the relevance vector machines (RVM). The feature vectors extracted from the measured signal are processed by GDA, with its dimension remarkably reduced from 350 to 12 while fully maintaining the most discriminative information. The reduced feature vectors are then classified by the RVM technique according to an extended multiclass model, which shows good convergence characteristic. Experimental results on the Wearable Action Recognition Dataset demonstrate that our approach achieves an encouraging recognition rate of 99.2%, true positive rate of 99.18% and false positive rate of 0.07%. Although in most cases, the support vector machines model has more than 70 support vectors, the number of relevance vectors related to different activities is always not more than 4, which implies a great simplicity in the classifier structure. Our approach is expected to have potential in real-time applications or solving problems with large-scale datasets, due to its perfect recognition performance, strong ability in feature reduction, and simple classifier structure.	feature vector;gnome-db;linear discriminant analysis;real-time locating system;relevance;sensitivity and specificity;sensor;support vector machine;wearable computer;wearable technology	Weihua He;Yongcai Guo;Chao Gao;Xinke Li	2012	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2012-108	computer science;artificial intelligence;machine learning;pattern recognition;mathematics	Vision	28.086739137679313	-60.07574678683511	12878
d1b87da4112e34eef54ebfc1e9581489e740b0b5	neural mechanisms of proactive interference-resolution	task performance;female;models neurological;short term memory;fixation ocular;cues;fmri;adolescent;long term memory;male;image processing computer assisted;prefrontal cortex;anterior prefrontal cortex;adult;magnetic resonance imaging;cognition;gyrus cinguli;event related functional magnetic resonance imaging;mental processes;humans;photic stimulation;ventrolateral prefrontal cortex;functional laterality;proactive interference;reaction time;memory	The ability to mitigate interference from information that was previously relevant, but is no longer relevant, is central to successful cognition. Several studies have implicated left ventrolateral prefrontal cortex (VLPFC) as a region tied to this ability, but it is unclear whether this result generalizes across different tasks. In addition, it has been suggested that left anterior prefrontal cortex (APFC) also plays a role in proactive interference-resolution although support for this claim has been limited. The present study used event-related functional magnetic resonance imaging (fMRI) to investigate the role of these regions in resolving proactive-interference across two different tasks performed on the same subjects. Results indicate that both left VLPFC and left APFC are involved in the resolution of proactive interference across tasks. However, different functional networks related to each region suggest dissociable roles for the two regions. Additionally, regions of the posterior cingulate gyrus demonstrated unique involvement in facilitation when short- and long-term memory converged. This pattern of results serves to further specify models of proactive interference-resolution.	catastrophic interference;cognition;converge;gyrus cinguli;interference (communication);locus;magnetic resonance imaging;memory, long-term;mental recall;proactive inhibition;ventrolateral prefrontal cortex;fmri;facilitation	Derek Evan Nee;John Jonides;Marc G. Berman	2007	NeuroImage	10.1016/j.neuroimage.2007.07.066	psychology;cognitive psychology;interference theory;mental chronometry;neuroscience;long-term memory;cognition;developmental psychology;magnetic resonance imaging;short-term memory;memory	ML	17.26199199016746	-77.74103606953294	12879
56c0271fc3f5a9a56478edeb3415d719be06e29e	synthesis and properties of oligonucleotides containing aminodeoxythymidine units	capacidad de fijacion;capacite fixation;complex;aminonucleosido;analog;oligodeoxyribonucleotides;desoxirribonucleotido;ductus arteriosus;thymine nucleotide;desoxyribonucleotide;dideoxynucleosides;analogo;oligomers;analogue;oligonucleotides;oligonucleotide;timina nucleotido;oligonucleotido;chromatography high pressure liquid;molecular sequence data;temperature;sintesis quimica;base sequence;synthese chimique;dopamine;aminonucleoside;chemical synthesis;deoxyribonucleotide;binding capacity	Procedures are described for synthesis via solid support methodology of oligonucleotide analogues derived in part from 3'-amino-3'-deoxythymidine or 5'-amino-5'-deoxythymidine. Oligothymidylate decamers terminated with a 3'-amino group or containing a 3'-NHP(O)(O-)O-5' internucleoside link are found to form unusually stable complexes with poly(dA), poly(A), and oligo(dA). For related derivatives of 5'-amino-5'-deoxythymidine enhancement is less or absent, and in the case of multiple substitution destabilization of the heteroduplex may be observed. That the effect of the 3'-amino group is general for oligonucleotide derivatives is indicated by enhanced Tm values for heteroduplex complexes of the mixed-base oligomer, d(TATTCAGTCAT(NH2)), and the methyl phosphonate derivatives, TmTmTmTmTmTmTmTmTmT(NH2) and d(TmAmTmTmCmAmGmTmCmAmT(NH2)).	3'-amino-3'-deoxythymidine;5'-amino-5'-deoxythymidine;methylmethacrylate;oligonucleotides;phosphonates	Sergei M. Gryaznov;Robert L. Letsinger	1992	Nucleic acids research	10.1093/nar/20.13.3403	biology;biochemistry;oligonucleotide	HCI	5.510072950273928	-63.680832897697854	12881
a67746385797138f15fd593a673400002f403d3a	control of neuronal output by inhibition at the axon initial segment	basket cell;compartmental model;action potential;visual cortex;pyramidal neuron	We examine the effect of inhibition on the axon initial segment (AIS) by the chandelier (axoaxonic) cells, using a simplified compartmental model of actual pyramidal neurons from cat visual cortex. We show that within generally accepted ranges, inhibition at the AIS cannot completely prevent action potential discharge: only small amounts of excitatory synaptic current can be inhibited. Moderate amounts of excitatory current always result in action potential discharge, despite AIS inhibition. Inhibition of the somadendrite by basket cells enhances the effect of AIS inhibition and vice versa. Thus the axoaxonic cells may act synergistically with basket cells: the AIS inhibition increases the threshold for action potential discharge, the basket cells then control the suprathreshold discharge.	action potential;discharger;multi-compartment model;synaptic package manager;synergy	Rodney J. Douglas;Kevan A. C. Martin	1990	Neural Computation	10.1162/neco.1990.2.3.283	neuroscience;communication;action potential	ML	17.933167497924885	-71.72299740728963	12892
e9f268e5e07005a72cb21fd8520f6d9ecb32e087	feasibility study on effects of freely moving micro-beads insole for walking		Walking for health is currently popular, it should be discussed the effects of insole types. We adopted insoles composed of freely moving micro-beads, expecting the enlargement of sensitivity for muscle fatigue and balance. The target muscles and the locations of surface electrode were determined with the muscle synergies and the spatiotemporal distribution pattern measured with a matrix surface electrode. From 15 healthy mail participants the time courses of %iEMG were acquired for total 100 strokes walk on the treadmill with the speed of 3 km/h. The results showed that induced balance related effects were observed with freely moving micro-beads less filled insole compared with a barefoot: %iEMG decreased at thigh, gluteus, and erector spinae muscles, and significantly at lower leg muscles. With a lot of filling quantity, %iEMG increased at gluteus, erector spinae, and lower leg muscles, while there were not significantly increased at thigh muscles. These might be link to the compensation for the balance stability as stochastic resonance effects.	brainfuck;coefficient;detailed balance;gmax;stochastic resonance;synergy	T. Kiryu;K. Yamaguchi;T. Murayama	2017	2017 IEEE 6th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2017.8229197	treadmill;biomedical engineering;muscle fatigue;thigh;surface electrode;thigh muscles;barefoot;erector spinae muscles;medicine	Visualization	14.609121790043313	-82.94796424471103	12898
2124ea748cc091612b206fc686565b2d03c947d5	dielectrophoretic characteristics of microbeads labeled with dna of various lengths		Polymerase chain reaction (PCR) is one of the most sensitive and specific detection methods of bacterial and viral infections. The authors proposed a new electrical technique for rapid detection of DNA amplified by PCR using dielectrophoresis (DEP) of microbeads. The method is based on dramatic alteration of DEP characteristics of microbeads caused by DNA labelling. DNA labeled microbeads are trapped on a microelectrode under the action of positive DEP, whereas pristine microbeads are not. DEP-trapped microbeads can be measured impedimetrically to realize rapid and quantitative detection of the amplified DNA. In this study, it was aimed to reveal how DNA length affects DEP characteristic of DNA-labeled microbeads. Dielectrophoretic crossover from the negative to the positive was measured for microbeads labeled with DNA length in 204 bp, 391 bp and 796 bp. After theoretical fitting of DEP crossover data, it was revealed that the surface conductance increased when the length of labeled DNA increased.	conductance (graph);executable space protection	Zhenhao Ding;Hiromichi Kasahara;Michihiko Nakano;Junya Suehiro	2015			dna;electronic engineering;engineering;molecular biology	Comp.	5.272876850815873	-65.21745054074135	12904
d7ea195a14757dd2ffdf258fe6d6e6462d7066a8	prediction of polypharmacological profiles of drugs by the integration of chemical, side effect, and therapeutic space		Prediction of polypharmacological profiles of drugs enables us to investigate drug side effects and further find their new indications, i.e. drug repositioning, which could reduce the costs while increase the productivity of drug discovery. Here we describe a new computational framework to predict polypharmacological profiles of drugs by the integration of chemical, side effect, and therapeutic space. On the basis of our previous developed drug side effects database, named MetaADEDB, a drug side effect similarity inference (DSESI) method was developed for drug-target interaction (DTI) prediction on a known DTI network connecting 621 approved drugs and 893 target proteins. The area under the receiver operating characteristic curve was 0.882 ± 0.011 averaged from 100 simulated tests of 10-fold cross-validation for the DSESI method, which is comparative with drug structural similarity inference and drug therapeutic similarity inference methods. Seven new predicted candidate target proteins for seven approved drugs were confirmed by published experiments, with the successful hit rate more than 15.9%. Moreover, network visualization of drug-target interactions and off-target side effect associations provide new mechanism-of-action of three approved antipsychotic drugs in a case study. The results indicated that the proposed methods could be helpful for prediction of polypharmacological profiles of drugs.		Feixiong Cheng;Weihua Li;Zengrui Wu;Xichuan Wang;Jie Li;Guixia Liu;Yun Tang	2013	Journal of chemical information and modeling	10.1021/ci400010x	pharmacology;bioinformatics	Comp.	8.360410475802825	-57.315416984120915	12907
bae173b6047077255ec815c48a3bcee4027ef261	ensemble learning for the detection of facial dysmorphology	feature extraction support vector machines medical diagnostic imaging accuracy pediatrics genetics;median ensemble rule ensemble learning facial dysmorphology detection down syndrome chromosomal condition characteristic facial morphology texture patterns health management computer aided diagnosis system photography facial analysis texture facial feature extraction geometric facial feature extraction automatically located facial landmarks feature fusion feature selection multiple classifiers support vector machines random forests linear discriminant analysis;support vector machines biomedical optical imaging feature extraction image fusion learning artificial intelligence medical disorders medical image processing photography	Down syndrome is the most common chromosomal condition that presents characteristic facial morphology and texture patterns. The early detection of Down syndrome through an automatic, non-invasive and simple way is desirable and critical to provide the best health management to newborns. In this study, we propose such a computer-aided diagnosis system for Down syndrome from photography based on facial analysis with ensemble learning. First, geometric and texture facial features are extracted based on automatically located facial landmarks, followed by feature fusion and selection. Then multiple classifiers (i.e. support vector machines, random forests and linear discriminant analysis) are adopted to identify patients with Down syndrome. An accurate and reliable decision is finally achieved by optimally combining the outputs of these individual classifiers via ensemble learning that captures both the shared and complementary information from different classifiers. The best performance was achieved by using the median ensemble rule with 0.967 accuracy, 0.977 precision and 0.933 recall.	computer assisted diagnosis;down syndrome;early diagnosis;ensemble learning;entity name part qualifier - adopted;extraction;face;facial nerve diseases;facial recognition system;forests;galaxy morphological classification;infant, newborn;linear discriminant analysis;nonlinear system;patients;random forest;receiver operating characteristic;stiff-person syndrome;support vector machine	Qian Zhao;Naoufel Werghi;Kazunori Okada;Kenneth Rosenbaum;Marshall Summar;Marius George Linguraru	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6943700	computer vision;computer science;machine learning;pattern recognition;feature	Vision	34.43166938670742	-76.03688872090282	12924
d3a40771d99653b516bb31b4aa322faaee4d20c9	hypoglycaemia detection using fuzzy inference system with intelligent optimiser	multi objective optimisation;differential evolution;journal article;hypoglycaemia;fuzzy inference system	Hypoglycaemia is a medical term for a body state with a low level of blood glucose. It is a common and serious side effect of insulin therapy in patients with diabetes. In this paper, we propose a system model to measure physiological parameters continuously to provide hypoglycaemia detection for Type 1 diabetes mellitus (TIDM) patients. The resulting model is a fuzzy inference system (FIS). The heart rate (HR), corrected QT interval of the electrocardiogram (ECG) signal (QTc), change of HR, and change of QTc are used as the input of the FIS to detect the hypoglycaemic episodes. An intelligent optimiser is designed to optimise the FIS parameters that govern the membership functions and the fuzzy rules. The intelligent optimiser has an implementation framework that incorporates two wavelet mutated differential evolution optimisers to enhance the training performance. A multi-objective optimisation		Johnny C. Y. Lai;F. H. Frank Leung;Sai-Ho Ling	2014	Appl. Soft Comput.	10.1016/j.asoc.2013.12.015	differential evolution;mathematical optimization;simulation;computer science;artificial intelligence;machine learning	AI	10.82758647611494	-78.67938223657146	12950
aae54ef565326f0dc0b3b1e547e32429dc58fc9c	feature-based classifiers for somatic mutation detection in tumour–normal paired sequencing data	software;breast neoplasms;female;support vector machines;bayes theorem;models genetic;cluster analysis;exome;genome;artificial intelligence;algorithms;humans;neoplasms;polymorphism single nucleotide;mutation	MOTIVATION The study of cancer genomes now routinely involves using next-generation sequencing technology (NGS) to profile tumours for single nucleotide variant (SNV) somatic mutations. However, surprisingly few published bioinformatics methods exist for the specific purpose of identifying somatic mutations from NGS data and existing tools are often inaccurate, yielding intolerably high false prediction rates. As such, the computational problem of accurately inferring somatic mutations from paired tumour/normal NGS data remains an unsolved challenge.   RESULTS We present the comparison of four standard supervised machine learning algorithms for the purpose of somatic SNV prediction in tumour/normal NGS experiments. To evaluate these approaches (random forest, Bayesian additive regression tree, support vector machine and logistic regression), we constructed 106 features representing 3369 candidate somatic SNVs from 48 breast cancer genomes, originally predicted with naive methods and subsequently revalidated to establish ground truth labels. We trained the classifiers on this data (consisting of 1015 true somatic mutations and 2354 non-somatic mutation positions) and conducted a rigorous evaluation of these methods using a cross-validation framework and hold-out test NGS data from both exome capture and whole genome shotgun platforms. All learning algorithms employing predictive discriminative approaches with feature selection improved the predictive accuracy over standard approaches by statistically significant margins. In addition, using unsupervised clustering of the ground truth 'false positive' predictions, we noted several distinct classes and present evidence suggesting non-overlapping sources of technical artefacts illuminating important directions for future study.   AVAILABILITY Software called MutationSeq and datasets are available from http://compbio.bccrc.ca.	algorithm;bioinformatics;biopolymer sequencing;class;cluster analysis;communications satellite;computational problem;cross-validation (statistics);decision tree learning;diploid cell;exome;experiment;feature selection;genome;ground truth;logistic regression;machine learning;mammary neoplasms;massively-parallel sequencing;morphologic artifacts;nucleotides;random forest;scientific publication;somatic mutation;supervised learning;support vector machine;utility functions on indivisible goods;statistical cluster;triangulation	Jiarui Ding;Ali Bashashati;Andrew Roth;Arusha Oloumi;Kane Tse;Thomas Zeng;Gholamreza Haffari;Martin Hirst;Marco A. Marra;Anne Condon;Sam Aparicio;Sohrab P. Shah	2012		10.1093/bioinformatics/btr629	mutation;biology;support vector machine;computer science;bioinformatics;data mining;exome;cluster analysis;bayes' theorem;genetics;genome	Comp.	6.262846767289485	-52.467332163265986	12966
fba11af4713685f1cb5a1858fc8bb86e46bd417c	matching between physiological sensor and smartphone based on rr intervals time series	biomedical monitoring;time series assisted living biomedical telemetry electrocardiography geriatrics medical signal processing photoplethysmography smart phones telemedicine;cameras heart rate assisted living time series analysis biomedical monitoring;assisted living;heart rate;time series analysis;real data collection physiological sensor rr intervals time series ambient assisted living quality of life elderly population age related physical limitations personal smartphone multiple sensor devices automatic pairing instantaneous variability heart rate;cameras	Ambient assisted living is a key enabler to increase the quality of life, and its importance is going to increase in the next decade due to a significant increment in the number and in the percentage of the elderly population. Many solutions have been proposed to compensate the age related physical limitations and to guarantee autonomy and safety to the elderly people, allowing them to continue their life in their own environment. There are several open issues to solve in order to enable the practical implementation of these solutions. One of them regards the intrinsic difficulties to create a network between a personal smartphone and the correct physiological sensors, in the presence of multiple sensor devices, where the automatic choice of the correct sensor is not obvious. In this paper we propose a solution to this problem, based on an automatic pairing that relies on the instantaneous variability of the heart rate of each subject. We evaluate the technique by collecting real data and showing some promising performance accuracy results, opening new research opportunities in this area.	android;authentication protocol;autonomy;futures studies;heart rate variability;instantaneous phase;rapid refresh;sensor;smartphone;the quality of life;time series	Giorgio Quer;Matteo Danieletto	2015	2015 IEEE International Conference on Communication Workshop (ICCW)	10.1109/ICCW.2015.7247194	simulation;telecommunications;time series;statistics	Mobile	7.920375261253305	-86.32226846151775	12968
dbd55bac4bba051b35afecc91cbb4e133a404c49	the nip7 protein is required for accurate pre-rrna processing in human cells	hek293 cells;complex;cell nucleus structures;poly a u;poly u;rna processing post transcriptional;interdisciplinar;polyribosomes;nuclear proteins;down regulation;rna;rna precursors;ribosomal rna;humans;rna ribosomal;gene knockdown techniques;ribosomes;cell line	Eukaryotic ribosome biogenesis requires the function of a large number of trans-acting factors which interact transiently with the nascent pre-rRNA and dissociate as the ribosomal subunits proceed to maturation and export to the cytoplasm. Loss-of-function mutations in human trans-acting factors or ribosome components may lead to genetic syndromes. In a previous study, we have shown association between the SBDS (Shwachman-Bodian-Diamond syndrome) and NIP7 proteins and that downregulation of SBDS in HEK293 affects gene expression at the transcriptional and translational levels. In this study, we show that downregulation of NIP7 affects pre-rRNA processing, causing an imbalance of the 40S/60S subunit ratio. We also identified defects at the pre-rRNA processing level with a decrease of the 34S pre-rRNA concentration and an increase of the 26S and 21S pre-rRNA concentrations, indicating that processing at site 2 is particularly slower in NIP7-depleted cells and showing that NIP7 is required for maturation of the 18S rRNA. The NIP7 protein is restricted to the nuclear compartment and co-sediments with complexes with molecular masses in the range of 40S-80S, suggesting an association to nucleolar pre-ribosomal particles. Downregulation of NIP7 affects cell proliferation, consistently with an important role for NIP7 in rRNA biosynthesis in human cells.	anabolism;anatomical compartments;biologic development;carpal tunnel syndrome;cell proliferation;down-regulation;gene expression;genetic translation process;multi-compartment model;mutation;nip7 gene;ribosomal rna;ribosome subunits;ribosomes;sbds gene;shwachman syndrome;source-to-source compiler;streptococcus pyogenes rrna:prthr:pt:xxx:ord:probe;trans-activators;transcription, genetic;negative regulation of type iv pilus biogenesis;rrna processing	Luis G. Morello;Cédric Hesling;Patrícia P. Coltri;Beatriz A. Castilho;Ruth Rimokh;Nilson I. T. Zanchin	2011		10.1093/nar/gkq758	biology;molecular biology;rna;ribosomal rna;hek 293 cells;bioinformatics;downregulation and upregulation;nuclear protein;ribosome;polysome;genetics;cell culture	Comp.	5.951473037668887	-63.45421700821884	12969
f745a42c88fcc74be625d8ceffcf44299c02e897	image thresholding via a modified fuzzy c-means algorithm	analisis imagen;fuzzy c means algorithm;modelizacion;cluster algorithm;fuzzy c mean;analyse amas;algorithm performance;analisis estadistico;threshold detection;image databank;classification non supervisee;algoritmo borroso;imagen nivel gris;cartographie;probabilistic approach;modelisation;detection seuil;vecino mas cercano;histogram;cartografia;deteccion umbral;cluster analysis;histogramme;statistical analysis;resultado algoritmo;enfoque probabilista;approche probabiliste;fuzzy algorithm;banco imagen;clasificacion no supervisada;image niveau gris;banque image;analyse statistique;performance algorithme;pattern recognition;algorithme flou;unsupervised classification;cartography;plus proche voisin;nearest neighbour;k nearest neighbor;image analysis;analisis cluster;reconnaissance forme;reconocimiento patron;entropy method;histograma;modeling;grey level image;analyse image;variance;variancia	In this paper, a modified fuzzy c-means (FCM) algorithm named weighted fuzzy c-means (WFCM) algorithm for image thresholding is presented. The algorithm is developed by incorporating the spatial neighborhood information into the standard FCM clustering algorithm. The weight indicates the spatial influence of the neighboring pixels on the centre pixel, which is derived from the k-nearest neighbor (k-NN) algorithm and is modified in two aspects so as to improve its property in the WFCM algorithm. To speed up the algorithm, the iteration in FCM algorithm is carried out with the statistical gray level histogram of image instead of the conventional whole data of image. The performance of the algorithm is compared with those of an existing fuzzy thresholding algorithm and widely applied between variance and entropy methods. Experimental results on both synthetic and real images are given to demonstrate the proposed algorithm is effective and efficient. In addition, due to the neighborhood model, our method is more tolerant to noise.	algorithm;thresholding (image processing)	Yong Yang;Chongxun Zheng;Pan Lin	2004		10.1007/978-3-540-30463-0_74	image analysis;systems modeling;ramer–douglas–peucker algorithm;computer science;artificial intelligence;canopy clustering algorithm;histogram;mathematics;fsa-red algorithm;variance;cluster analysis;linde–buzo–gray algorithm;dinic's algorithm;k-medoids;k-nearest neighbors algorithm;algorithm;difference-map algorithm;statistics	EDA	45.793709116028495	-63.931259487585116	12972
1a495fa4808941912e468e37a7edb1b2c447d4a4	cranial thickness changes in early childhood		The neurocranium changes rapidly in early childhood to accommodate the developing brain. However, developmental disorders may cause abnormal growth of the neurocranium, the most common one being craniosynostosis, affecting about 1 in 2000 children. It is important to understand how the brain and neurocranium develop together to understand the role of the neurocranium in neurodevelopmental outcomes. However, the neurocranium is not as well studied as the human brain in early childhood, due to a lack of imaging data. CT is typically employed to investigate the cranium, but, due to ionizing radiation, may only be used for clinical cases. However, the neurocranium is also visible on magnetic resonance imaging (MRI). Here, we used a large dataset of MRI images from healthy children in the age range of 1 to 2 years old and extracted the neurocranium. A conformal geometry based analysis pipeline is implemented to determine a set of statistical atlases of the neurocranium. A growth model of the neurocranium will help us understand cranial bone and suture development with respect to the brain, which will in turn inform better treatment strategies for neurocranial disorders.	cranial electrotherapy stimulation;thickness (graph theory)	Niharika Gajawelli;Sean C. L. Deoni;Jie Shi;Holly Dirks;Marius George Linguraru;Marvin D. Nelson;Yalin Wang;Natasha Lepore	2017		10.1117/12.2286736	anatomy;cranial bone;early childhood;neurocranium;craniosynostosis;human brain;magnetic resonance imaging;medicine;skull	Vision	21.364927672772343	-79.93521383121319	12987
8021e48e698b57446eb6a0f0cd6e55c896cc57fe	topological active volumes	image tridimensionnelle;structure topologique;vision ordenador;characteristic;singularite;image segmentation;image processing;3d imaging;3d computer vision;procesamiento imagen;adjustment;topological structure;caracteristica;traitement image;reglage;computer vision;topological properties;reconstruction image;local structure;reconstruccion imagen;image reconstruction;segmentation image;singularidad;caracteristique;tridimensional image;object reconstruction;vision ordinateur;estructura local;structure locale;reglaje;estructura topologica;deformable model;3d reconstruction;imagen tridimensional;singularity	The topological active volumes (TAVs) model is a general model for 3D image segmentation. It is based on deformable models and integrates features of region-based and boundary-based segmentation techniques. Besides segmentation, it can also be used for surface reconstruction and topological analysis of the inside of detected objects. The TAV structure is flexible and allows topological changes in order to improve the adjustment to object’s local characteristics, find several objects in the scene, and identify and delimit holes in detected structures. This paper describes the main features of the TAV model and shows its ability to segment volumes in an automated manner.	ct scan;electron hole;fits;graphical user interface;image segmentation;pyramid (geometry)	Noelia Barreira;Manuel G. Penedo;Cástor Mariño;F. M. Ansia	2003		10.1007/978-3-540-45179-2_42	3d reconstruction;iterative reconstruction;stereoscopy;singularity;computer vision;topology;image processing;computer science;mathematics;geometry;image segmentation;characteristic	Vision	50.60510622396959	-60.234589059883064	12992
970981bd9bf5bb4fc34337c978d9858a22b73994	gradient regression for brain landmark localization on magnetic resonance imaging		Landmark localization in human brain from Magnetic Resonance Imaging (MRI) is primarily important for numerous medical analysis applications. Recently developed regression based (including deep networks) methods typically learn a mapping from input features to individual landmark positions or transform parameters. These methods neglect the geometric correlations among landmarks, thus resulting in inaccurate localization, especially for the parcellation functional regions whose boundaries are composed of a bunch of landmarks. In this paper, we build a shape energy for landmarks on 3D M-RI features and learn the gradient regression for the energy. Our method accelerates the iterative gradient calculation and accurately detect brain landmarks. We validate the algorithm on two localization tasks for two key points, anterior commissure (AC) and posterior commissure (PC), and for three functional regions on the OASIS TI-weighted MR data set. Experimental results demonstrate its efficiency and effectiveness by comparing with the state-of-the-art.		Yuzhuo Duan;Xin Fan;Hua Cheng;Huiying Kang	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451747	computer vision;anterior commissure;posterior commissure;artificial intelligence;computer science;medical imaging;human brain;pattern recognition;magnetic resonance imaging	Robotics	43.64766232917537	-78.55104110028388	12994
7e866b9fbd7a55db17c3c236ed764b4eee9f27fb	coupling dynamic programming with machine learning for horizon line detection	dynamic programming;horizon line detection;skyline extraction;sky segmentation	In this paper, we consider the problem of segmenting an image into sky and non-sky regions, typically referred to as horizon line detection or skyline extraction. Specifically, we present a new approach to horizon line detection by coupling machine learning with dynamic programming. Given an image, the Canny edge detector is applied first and keeping only those edges which survive over a wide range of thresholds. We refer to the surviving edges as Maximally Stable Extremal Edges (MSEEs). The number of edges is further reduced by classifying MSEEs into horizon and non-horizon edges using a Support Vector Machine (SVM) classifier. Dynamic programming is then applied on the horizon classified edges to extract the horizon line. Various local texture features and their combinations have been investigated in training the horizon edge classifier including SIFT, LBP, HOG, SIFT-LBP, SIFT-HOG, LBP-HOG and SIFT-LBP-HOG. We have also investigated various nodal costs in the context of dynamic programming	canny edge detector;dynamic programming;edge detection;global positioning system;local binary patterns;machine learning;planetary scanner;rover (the prisoner);scale-invariant feature transform;support vector machine	Touqeer Ahmad;George Bebis;Emma E. Regentova;Ara V. Nefian;Terrence Fong	2015	International Journal on Artificial Intelligence Tools	10.1142/s0218213015400187	computer vision;mathematical optimization;computer science;machine learning;dynamic programming;pattern recognition	AI	33.13839913368285	-55.37667965063507	12995
0c78021da48c15efba04f29681b024c7616d733c	a qualitative continuous model of cellular auxin and brassinosteroid signaling and their crosstalk	en continu;en continuo;modelo;auxina;auxin;auxine;brasinoesteroide;modele;plant growth substance;brassinosteroid;brassinosteroide;substancia crecimiento vegetal;models;substance croissance vegetal;continuous process	MOTIVATION Hormone pathway interactions are crucial in shaping plant development, such as synergism between the auxin and brassinosteroid pathways in cell elongation. Both hormone pathways have been characterized in detail, revealing several feedback loops. The complexity of this network, combined with a shortage of kinetic data, renders its quantitative analysis virtually impossible at present.   RESULTS As a first step towards overcoming these obstacles, we analyzed the network using a Boolean logic approach to build models of auxin and brassinosteroid signaling, and their interaction. To compare these discrete dynamic models across conditions, we transformed them into qualitative continuous systems, which predict network component states more accurately and can accommodate kinetic data as they become available. To this end, we developed an extension for the SQUAD software, allowing semi-quantitative analysis of network states. Contrasting the developmental output depending on cell type-specific modulators enabled us to identify a most parsimonious model, which explains initially paradoxical mutant phenotypes and revealed a novel physiological feature.   AVAILABILITY The package SQUADD is freely available via the Bioconductor repository at http://www.bioconductor.org/help/bioc-views/release/bioc/html/SQUADD.html.	akap13 protein, human;add-ons for firefox;assumed;auxins;bioconductor;boolean algebra;cell signaling;crosstalk;feedback;gene regulatory network;interaction;kinetics;logical connective;maximum parsimony (phylogenetics);modulation;modulator device component;noise shaping;occam's razor;phenotype;plant development;plant physiological phenomena;plant seeds;rendering (computer graphics);semiconductor industry;squad;confirmation - responselevel	Martial Sankar;Karen S. Osmont;Jakub Rolcik;Bojan Gujas;Danuše Tarkowská;Miroslav Strnad;Ioannis Xenarios;Christian S. Hardtke	2011	Bioinformatics	10.1093/bioinformatics/btr158	biology;botany;simulation;auxin	Comp.	5.945121782212776	-62.71509472438723	12997
8141c3f9eefa4a14586e8fe5cb0384d70d9e7db9	diagnostic process optimisation with evolutionary programming	congenital heart disease;medical records;cardiology;evolutionary programming;records management;genetic programming pediatrics cardiology databases health information management information systems cardiac disease decision support systems medical diagnostic imaging decision making;decision support system;medical information systems;decision support systems;genetic algorithms;records management cardiology genetic algorithms medical diagnostic computing medical information systems decision support systems cost benefit analysis;information system;maximally rationalised examinations diagnostic process optimisation evolutionary programming paediatric cardiology medical information system congenital heart disease diagnostic decision support system children maximal information patient risk highly invasive methods cost benefit ratio diapro diagnostic process management medical records database;medical diagnostic computing;cost benefit analysis	In paediatric cardiology a database and the supporting information system are indispensable in order to conduct more extensive and detailed investigations. Diagnostic decisions during the diagnostic process in congenital heart disease (CHD) must be supported by suitable decision support system. In the majority of children with CHD, the diagnosis should be possibly made before the complete diagnosis procedure has been accomplished, with maximal information (possible probability of correct diagnosis 1.00) and minimal risk for patient and doctor (the minimal needed number of steps, especially for high invasive methods) and cost benefit. We developed a tool called DIAPRO that supports all activities concerned with the management of the diagnosis process and the management of the medical records database, but its main strength is in the decision making support concerning the optimisation of the diagnostic process. It enables the paediatrician to perform cardiological examination in maximally rationalised manner.	decision support system;evolutionary programming;information system;mathematical optimization;maximal set	Peter Kokol;Vili Podgorelec;Ivan Malcic	1998		10.1109/CBMS.1998.701240	evolutionary programming;genetic algorithm;medicine;decision support system;computer science;cost–benefit analysis;artificial intelligence;data mining;information system;medical record	AI	2.533694966427896	-77.8939540808311	13010
14bac30621c9b7e164b9fa514a84430d736b7e8b	learning-based ventricle detection from cardiac mr and ct images	nuclear magnetic resonance imaging;cardiopathie;echo gradient;computerized axial tomography;heart disease;tomodensitometria;radiodiagnostic;medical imagery;apprentissage perceptif;heart;image segmentation;cardiology image segmentation medical image processing computerised tomography biomedical nmr;perceptive learning;cardiology;appareil circulatoire pathologie;biomedical nmr;experience;hemodynamique;hombre;exploracion;result;coeur;imageria rmn;aparato circulatorio patologia;radiodiagnostico;hemodynamics;tomodensitometrie;medical image;cardiovascular disease;corazon;medical image processing;eco gradiente;aprendizaje perceptivo;region of interest;human;computerised tomography;tecnica;hemodinamica;imagerie medicale;exploration;resultado;gradient echo;algorithms computer assisted instruction heart diseases heart ventricles humans image processing computer assisted magnetic resonance imaging myocardial contraction tomography x ray computed;imagerie rmn;resultat;imageneria medical;computed tomography image segmentation magnetic resonance imaging shape measurement image analysis heart histograms ultrasonic imaging positron emission tomography area measurement;experiencia;radiodiagnosis;cardiopatia;visual attention;high efficiency;medical diagnostic imaging learning based ventricle detection global histogram bell image intensity model attention map learned likelihood function endocardium boundaries location segmented region boundary application specific measures gradient echo magnetic resonance images input image segmentation cardiac mr images cardiac ct images;technique;homme	The objective of this work is to investigate the issue of automatically detecting regions of interest (ROI's) in medical images. It is assumed that the regions to be detected can be roughly segmented by a threshold based on a likelihood measure of the ROI, First, an analysis of the global histogram is used to compute a preliminary threshold that is likely near the optimal one. The histogram analysis is motivated by the analytical result of a bell image intensity model proposed in this work. Then, the preliminary threshold is used to segment the input image, resulting in an attention map, which contains an attention region that approximates the ROI as well as many spurious ones. Due to the nonoptimality of the preliminary threshold, it can happen that the attention region contains a part of, or more regions than, the ROI. Learning takes place in two stages: (1) learning for automatic selection of the preliminary threshold value and (2) learning for automatically selecting the ROI from the attention map while dynamically tuning the threshold according to the learned-likelihood function. Experiments have been conducted to approximately locate the endocardium boundaries of the left and right ventricles from gradient-echo magnetic resonance (MR) images. Cardiac computed tomography (CT) images have also been used for testing. The boundary of the segmented region provided by this algorithm is not very accurate and is meant to be used for further fine tuning based on other application-specific measures.	algorithm;assumed;ct scan;experiment;gradient;heart ventricle;histogram;medical imaging;region of interest;resonance;sensor;x-ray computed tomography	Juyang Weng;Ajit Singh;M. Y. Chiu	1997	IEEE Transactions on Medical Imaging	10.1109/42.611346	computer vision;radiology;medicine;exploration;computer science;hemodynamics;image segmentation;nuclear medicine;heart;region of interest	Vision	46.057246946739205	-79.35876137622719	13012
af6746227a53a95bed4cf9f8dc6327e1f2e3f9e3	dbtmee: a database of transcriptome in mouse early embryos	animals;mice;databases genetic;histones;internet;gene expression profiling;embryo mammalian	DBTMEE (http://dbtmee.hgc.jp/) is a searchable and browsable database designed to manipulate gene expression information from our ultralarge-scale whole-transcriptome analysis of mouse early embryos. Since integrative approaches with multiple public analytical data have become indispensable for studying embryogenesis due to technical challenges such as biological sample collection, we intend DBTMEE to be an integrated gateway for the research community. To do so, we combined the gene expression profile with various public resources. Thereby, users can extensively investigate molecular characteristics among totipotent, pluripotent and differentiated cells while taking genetic and epigenetic characteristics into consideration. We have also designed user friendly web interfaces that enable users to access the data quickly and easily. DBTMEE will help to promote our understanding of the enigmatic fertilization dynamics.	embryo;embryonic development;fertilization;gene expression profiling;specimen collection;transcriptome;usability;user interface;study of epigenetics	Sung-Joon Park;Katsuhiko Shirahige;Miho Ohsugi;Kenta Nakai	2015		10.1093/nar/gku1001	biology;the internet;bioinformatics;histone;gene expression profiling;genetics	Visualization	-1.0454110053024537	-60.80787053093856	13014
1b1a856b3586947a10844d6cf70fbefa280ee428	global localization of 3d anatomical structures by pre-filtered hough forests and discrete optimization	sensitivity and specificity;anatomical structure localization;data interpretation statistical;whole body imaging;imaging three dimensional;interest point detection;random forests;anatomic landmarks;image interpretation computer assisted;reproducibility of results;algorithms;regression analysis;pattern recognition automated;humans;hough regression;tomography x ray computed	The accurate localization of anatomical landmarks is a challenging task, often solved by domain specific approaches. We propose a method for the automatic localization of landmarks in complex, repetitive anatomical structures. The key idea is to combine three steps: (1) a classifier for pre-filtering anatomical landmark positions that (2) are refined through a Hough regression model, together with (3) a parts-based model of the global landmark topology to select the final landmark positions. During training landmarks are annotated in a set of example volumes. A classifier learns local landmark appearance, and Hough regressors are trained to aggregate neighborhood information to a precise landmark coordinate position. A non-parametric geometric model encodes the spatial relationships between the landmarks and derives a topology which connects mutually predictive landmarks. During the global search we classify all voxels in the query volume, and perform regression-based agglomeration of landmark probabilities to highly accurate and specific candidate points at potential landmark locations. We encode the candidates' weights together with the conformity of the connecting edges to the learnt geometric model in a Markov Random Field (MRF). By solving the corresponding discrete optimization problem, the most probable location for each model landmark is found in the query volume. We show that this approach is able to consistently localize the model landmarks despite the complex and repetitive character of the anatomical structures on three challenging data sets (hand radiographs, hand CTs, and whole body CTs), with a median localization error of 0.80 mm, 1.19 mm and 2.71 mm, respectively.		Rene Donner;Bjoern H. Menze;Horst Bischof;Georg Langs	2013		10.1016/j.media.2013.02.004	random forest;computer vision;computer science;machine learning;pattern recognition;mathematics;interest point detection;regression analysis	Vision	42.799714548462774	-77.8282677595121	13027
82a0d71b648218afed2d39d791602f543ad4be88	using fuzzy sets for coarseness representation in texture images	image features;fuzzy set;texture features;human subjects;membership function;visual features;image analysis;human perception	Texture is a visual feature frequently used in image analysis that has associated certain vagueness. However, the majority of the approaches found in the literature do not either consider such vagueness or they do not take into account human perception to model the related uncertainty. In this paper we model the concept of ”coarseness”, one of the most important textural features, by means of fuzzy sets and considering the way humans perceive this kind of texture. Specifically, we relate representative measures of coarseness with its presence degree. To obtain these ”presence degrees”, we collect assessments from polls filled by human subjects, performing an aggregation of such assessments. Thus, the membership function corresponding to the fuzzy set ”coarseness” is modelled by using as reference set the representative measures and the aggregated data.	fuzzy set;high-level programming language;image analysis;statistical classification;texture mapping;vagueness	Jesús Chamorro-Martínez;Elena Galán-Perales;Jose Manuel Soto-Hidalgo;Belén Prados-Suárez	2007		10.1007/978-3-540-72434-6_79	image texture;computer vision;machine learning;pattern recognition	AI	41.187342569651314	-62.738676866689126	13034
7ed9e3e3e32303e4540967aa28f19ff887b49cd5	trends in the production of scientific data analysis resources	software;authorship;medline;periodicals as topic;computational biology bioinformatics;internet;algorithms;databases factual;combinatorial libraries;computational biology;computer appl in life sciences;microarrays;bioinformatics	As the amount of scientific data grows, peer-reviewed Scientific Data Analysis Resources (SDARs) such as published software programs, databases and web servers have had a strong impact on the productivity of scientific research. SDARs are typically linked to using an Internet URL, which have been shown to decay in a time-dependent fashion. What is less clear is whether or not SDAR-producing group size or prior experience in SDAR production correlates with SDAR persistence or whether certain institutions or regions account for a disproportionate number of peer-reviewed resources. We first quantified the current availability of over 26,000 unique URLs published in MEDLINE abstracts/titles over the past 20 years, then extracted authorship, institutional and ZIP code data. We estimated which URLs were SDARs by using keyword proximity analysis. We identified 23,820 non-archival URLs produced between 1996 and 2013, out of which 11,977 were classified as SDARs. Production of SDARs as measured with the Gini coefficient is more widely distributed among institutions (.62) and ZIP codes (.65) than scientific research in general, which tends to be disproportionately clustered within elite institutions (.91) and ZIPs (.96). An estimated one percent of institutions produced 68% of published research whereas the top 1% only accounted for 16% of SDARs. Some labs produced many SDARs (maximum detected = 64), but 74% of SDAR-producing authors have only published one SDAR. Interestingly, decayed SDARs have significantly fewer average authors (4.33 +/- 3.06), than available SDARs (4.88 +/- 3.59) (p < 8.32 × 10-4). Approximately 3.4% of URLs, as published, contain errors in their entry/format, including DOIs and links to clinical trials registry numbers. SDAR production is less dependent upon institutional location and resources, and SDAR online persistence does not seem to be a function of infrastructure or expertise. Yet, SDAR team size correlates positively with SDAR accessibility, suggesting a possible sociological factor involved. While a detectable URL entry error rate of 3.4% is relatively low, it raises the question of whether or not this is a general error rate that extends to additional published entities.	abstract summary;accessibility;archive;bit error rate;classification;code;coefficient;entity;extraction;keyword;medline;persistence (computer science);published database;registries;scientific publication;uniform resource locator;web server	Jason Hennessey;Constantin Georgescu;Jonathan D. Wren	2014		10.1186/1471-2105-15-S11-S7	biology;the internet;dna microarray;computer science;bioinformatics;data science;data mining	OS	0.9386712359743997	-60.12435735342334	13050
898be344d921edcf9622eeeaf14522ceb970ded5	on approximate string matching of unique oligonucleotides	dna;approximate string matching;oligonucleotides;medicinsk genetik	The current research considers the approximate string matching search for important subsequences from DNA sequences, which is essential for numerous bioinformatics computation tasks. We tested several approximate string matching algorithms and furthermore developed one for DNA data. Run times of the algorithms are important, since the amount of data is very large.	approximate string matching;bioinformatics;computation;string searching algorithm	Heikki Hyyrö;Mauno Vihinen;Martti Juhola	2001	Studies in health technology and informatics	10.3233/978-1-60750-928-8-960	approximate string matching;commentz-walter algorithm;bioinformatics;theoretical computer science;mathematics;algorithm;string searching algorithm	Theory	-1.3015942781933645	-52.68071265580463	13069
5633284e8c2a1a96b3ddba214ac369a1ec76f336	challenge of normality evaluation by using micro-size tension measurement device in anterior cruciate ligament reconstruction	ligaments;sensors;reconstruction;biomechanics;cadaver knee normality evaluation microsize tension measurement device anterior cruciate ligament reconstruction knee ligament femur tibia excessive forward movement clinical sports medicine acl reconstruction knee injury knee function recovery knee bending normal tension pattern knee normality imitation bone imitation ligament micromeasurement system;joints;anterior cruciate ligament;orthopaedics;medical computing;bones;bone;tension measurement;patient treatment;tension measurement anterior cruciate ligament reconstruction;patient treatment biomechanics biomedical measurement bone mechanical variables measurement medical computing orthopaedics;strain;mechanical variables measurement;calibration;biomedical measurement;voltage measurement;ligaments bones sensors strain voltage measurement calibration joints	Anterior Cruciate Ligament (ACL) is one of the important ligaments in the knee. The ACL connects the femur and tibia, and has a role to prevent an excessive forward movement of the tibia. In the clinical sports medicine, ACL reconstruction is often preformed about the injured knee. The ACL reconstruction aims to recover the function of the knee. Although some studies have reported tension pattern of the reconstructed knee during knee bending, no reports have reported an evaluation method of a normal tension pattern. This study challenges to define the normality of the normal knee in considering with the tension pattern. The experiment used an imitation bone with imitation ligaments for measuring the tension pattern by using our developed micro-measurement system, and evaluated the normality of the tension pattern. A future work is to apply this evaluation to cadaver knees.		Shogo Kawaguchi;Kouki Nagamune;Yuichiro Nishizawa;Yuichi Hoshino;Tomoyuki Matsumoto;Seiji Kubo;Takehiko Matsushita;Ryosuke Kuroda;Masahiro Kurosaka	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378139	calibration;sensor;biomechanics;strain	Robotics	13.334023468179646	-84.20266164071745	13083
599baafb7182ee1bca684e4e38894efe01a6bb66	swath-ms in proteomics: current status	proteome quantification;sequential window acquisition;proteome identification;fragment ion spectra;swath ms;tandem mass spectrometry;drug design;protein quantification;computational biology;proteomes;protein identification;bioinformatics	Sequential window acquisition of all theoretical fragment ion spectra coupled to tandem mass spectrometry (SWATH-MS) is a new strategy for identification and quantification of proteins and proteomes proposed in 2012. SWATH-MS operates in unbiased data-independent acquisition (DIA) mode and combines the advantages of two widely used MS-based techniques: shotgun (high throughput) and SRM proteomics (high sensitivity and reproducibility). In this review, we provide an introduction of the SWATH-MS method, its application, and a perspective on the future of this powerful technology.	proteomics	Jing Wang;Quanhu Sheng;Shyr Yu	2015	I. J.  Computational Biology and Drug Design	10.1504/IJCBDD.2015.072074	tandem mass spectrometry;biology;chromatography;shotgun proteomics;bioinformatics;quantitative proteomics;analytical chemistry;proteome;drug design	Comp.	-0.7149503088426367	-57.269891146622314	13104
a8a78eca9761288464ec960a967bf73bf0e852df	a new definition of fuzzy contours in mammographic images	image segmentation;level set;mass edge fuzzy contour region of interest image mass segmentation geometric deformable model contour representation zero level set mammogram mammographic image characteristics;deformable models;tumours cancer fuzzy logic image segmentation mammography medical image processing;yttrium;image edge detection;decision support systems;decision support systems image segmentation image edge detection yttrium deformable models level set;contour representation mammography mass fuzyy contours geometric deformable models segmentation	"""In this paper, we give a new definition of fuzzy contours in mammographic images and we propose an approach to get them. This approach starts from region of interest images with initial contours delimiting masses provided by an expert radiologist, and then it involves three major stages: mass segmentation using geometric deformable models, contour representation and finally definition of fuzzy contours. For the first phase, the initial zero level set is performed by using the initial contour guided by the radiologist. After that, we propose a new representation of contours. Since the segmentation of masses in mammograms is a difficult problem due to the characteristics of mammographic image, and even mass edges can be identified differently by different radiologists and also by the same radiologist at a different time, we define an entire region that may contain the accurate edges, in which we assign to each pixel a membership value to the class """"Contour"""". We call that fuzzy contours."""	contour line;delimiter;fuzzy logic;pixel;radiology;region of interest	Marwa Hmida;Kamel Hamrouni;Bassel Solaiman	2015	2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2015.7367120	computer vision;decision support system;computer science;level set;yttrium;machine learning;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	41.78727720062669	-73.52199687766505	13111
f9ae8fadf6e5ee95794fe3a51258ea94866e0f3d	the process of 3d-printed skull models for the anatomy education		Objective The 3D printed medical models can come from virtual digital resources, like CT scanning. Nevertheless, the accuracy of CT scanning technology is limited, which is 1mm. In this situation, the collected data is not exactly the same as the real structure and there might be some errors causing the print to fail. This study presents a common and practical way to process the skull data to make the structures correctly. And then we make a skull model through 3D printing technology, which is useful for medical students to understand the complex structure of skull. Materials and Methods The skull data is collected by the CT scan. To get a corrected medical model, the computer-assisted image processing goes with the combination of five 3D manipulation tools: Mimics, 3ds Max, Geomagic, Mudbox and Meshmixer, to reconstruct the digital model and repair it. Subsequently, we utilize a low-cost desktop 3D printer, Ultimaker2, with polylactide filament (PLA) material to print the model and paint it based on the atlas. Result After the restoration and repairing, we eliminate the errors and repair the model by adding the missing parts of the uploaded data within 6 hours. Then we print it and compare the model with the cadaveric skull from frontal, left, right and anterior views respectively. The printed model can show the same structures and also the details of the skull clearly and is a good alternative of the cadaveric skull. Conclusion We present an available and cost-effective way to obtain a printed skull model from the original CT data, which has a considerable economic and social benefit for the medical education. The steps of the data processing can be performed easily. The cost for the 3D printed model is also low. The manipulation procedure presented in this study can be applied widely in processing skull data.	3d printing;autodesk 3ds max;autodesk mudbox;ct scan;circuit restoration;comparison of 3d printers;declaration (computer programming);desktop computer;fused filament fabrication;geomagic;image processing;printer (computing);programmable logic array	Zhen Shen;Yong Yao;Yi Xie;Chao Guo;Xiuqin Shang;Xisong Dong;Yuqing Li;Zhouxian Pan;Shi Chen;Hui Pan;Gang Xiong	2017	CoRR		computer science;computed tomography;image processing;atlas (anatomy);3d printing;skull;anatomy	HCI	37.979313325671	-86.11019252215348	13117
2c7a8def1d8483221cfe6cc433c726eb0a77bd99	inspection of 2-d objects using pattern matching method	vision ordenador;two dimensional shape;forme bidimensionnelle;inspection;computer vision;forma bidimensional;vision inspection;application industrielle;axis of least inertia;pattern matching;pattern recognition;industrial application;vision ordinateur;concordance forme;reconnaissance forme;polygon approximation;reconocimiento patron;inspeccion;aplicacion industrial	-A pattern matching scheme is developed for the inspection of objects in industrial environment. The inspection includes dimensional verification and shape matching which compares a 2-dimensional image of an object to a pattern image. The method proves to be computationally efficient and accurate for real time application. Pattern matching Vision inspection Axis of least inertia Polygon approximation	algorithmic efficiency;approximation;pattern matching	Min-Hong Han;Dongsig Jang;Joseph W. Foster	1989	Pattern Recognition	10.1016/0031-3203(89)90024-1	computer vision;inspection;automated x-ray inspection;computer science;pattern matching;pattern recognition;mathematics;computer graphics (images)	Vision	49.19759684849324	-59.25206968920527	13174
19b46a9f7dab6ac877fbfe28903285f3e6ec10e9	kinetic analysis and quantification of the dopamine transporter in the non-human primate brain with [11c]pe2i and [18f]fe-pe2i	dopamine transporter;kinetic analysis;non human primate	Purpose: [F]FE-PE2I is a novel radioligand for dopamine transporter (DAT) PET imaging. As comparedwith [C]PE2I it shows faster kinetics and more favorable metabolism, with less production of a metabolite with intermediate lipophilicity, which in the case of [C]PE2I has been shown to enter the rat brain. In this studywe aimed at comparing DAT quantificationwith [C]PE2I and [F]FE-PE2I in non-human primates, using kinetic and graphical analyses with the input function of both the parent and the metabolite, to assess the potential contribution of the metabolite.	dopamine;graphical user interface;intel dynamic acceleration;kinetics internet protocol;polyethylene terephthalate;transporter classification database	Andrea Varrone;Miklós Tóth;Carsten Steiger;Akihiro Takano;Denis Guilloteau;Masanori Ichise;Balázs Gulyás;Christer Halldin	2010	NeuroImage	10.1016/j.neuroimage.2010.04.137	reaction progress kinetic analysis	NLP	11.124089075471977	-66.20008266779587	13188
46b7f92d9db6a36dc646568037be8d1652ede87f	boundary detection in a hexagonal grid using energy minimization	vision ordenador;image processing;echantillonnage hexagonal;relaxation contour;edge detection;echantillonnage;procesamiento imagen;traitement image;relajacion;computer vision;deteccion contorno;sampling;detection contour;edge relaxation;hexagonal sampling;relaxation;vision ordinateur;energy minimization;muestreo;boundary detection	In this letter a new boundary detection algorithm for a hexagonal grid is described. We propose a hexagonal boundary label model modified from the square boundary label model defined by Geman et al. (1990) and the penalty patterns reasonable for the hexagonal grid. Because of smaller quantization errors and reasonable penalty patterns, the results of the hexagonal boundary label model are better than those for the square boundary label model.	energy minimization	In Gook Chun;Taeg Il Cho;Kyu Ho Park	1994	Pattern Recognition Letters	10.1016/0167-8655(94)90044-2	sampling;computer vision;mathematical optimization;edge detection;image processing;computer science;relaxation;mathematics;geometry;energy minimization	Vision	47.38046210720624	-64.10489752675517	13212
234fce6f066aa3cf49845a5ecb667480b4380a36	towards objective performance analysis for estimation of complex motion: analytic motion modeling, filter optimization, and test sequences	motion analysis;optimisation;image sequences motion estimation filtering theory optimisation;motion estimation;performance analysis motion analysis motion estimation filters testing image motion analysis image sequence analysis spatiotemporal phenomena image sequences image analysis;design optimization;image sequence;performance analysis;image sequence complex motion estimation objective performance analysis analytic motion modeling filter optimization test sequence spatiotemporal filter formulation;filtering theory;image sequences	This paper deals with several aspects towards a more objective performance analysis of low-level motion analysis. A generalized spatiotemporal filter formulation and motion modeling in image sequences is introduced and used to design optimal motion estimators and to perform analytic studies.	high- and low-level;mathematical optimization;profiling (computer programming)	Bernd Jähne;Christoph S. Garbe	2003		10.1109/ICIP.2003.1247184	computer vision;mathematical optimization;structure from motion;multidisciplinary design optimization;computer science;motion estimation;control theory;mathematics	Vision	52.48680201973487	-59.72549681398548	13213
3f48591aa3cf493496601aed80a4ce4729453c33	recurrent v1-v2 interaction for early visual information processing	context dependent;computer model;top down;computability theory	A majority of cortical areas are connected via feedforward and feedback fiber projections. The computational role of the descending feedback pathways at different processing stages remains largely unknown. We suggest a new computational model in which normalized activities of orientation selective contrast cells are fed forward to the next higher processing stage. The arrangement of input activation is matched against local patterns of curvature shape to generate activities which are subsequently fed back to the previous stage. Initial measurements that are consistent with the top-down generated context-dependent responses are locally enhanced. In all, we present a computational theory for recurrent processing in visual cortex in which the significance of measurements is evaluated on the basis of priors that are represented as contour code patterns. The model handles a variety of perceptual phenomena, such as e.g. bar texture stimuli, illusory contours, and grouping of fragmented shape outline.	computational model;context-sensitive language;feedforward neural network;gene regulatory network;information processing;theory of computation;top-down and bottom-up design	Heiko Neumann;Wolfgang Sepp	1999				ML	17.86083614762137	-74.42720101031608	13219
34f30e8bae3e6cce0e4091b29582cb732017e69c	beat-to-beat cardiac output inference using heart sounds	finkelstein model beat to beat cardiac output inference heart sounds oxygenation cardiovascular diseases nonlinear model two layer feed forward artificial neural network contractility mean arterial pressure systolic time intervals echocardiography;analytical models;algorithms cardiac output diagnosis computer assisted heart auscultation heart rate humans reproducibility of results sensitivity and specificity sound spectrography;feed forward;inference mechanisms;correlation estimation error echocardiography analytical models mathematical model heart rate;cardiac output;heart rate;cardiovascular disease;stroke volume;mathematical model;echocardiography;diseases;feedforward neural nets;patient monitoring;cardiovascular system;estimation error;correlation;mean arterial pressure;patient monitoring biomedical measurement cardiovascular system diseases echocardiography feedforward neural nets inference mechanisms medical signal processing;biomedical measurement;medical signal processing;body surface area;analytical model;artificial neural network;nonlinear model	Cardiac output (CO) change is the primary compensatory mechanism that responds to oxygenation demand. Its continuous monitoring has great potential for the diagnosis and management of cardiovascular diseases, both in hospital as well as in ambulatory settings. However, CO measurements are currently limited to hospital settings only. In this paper, we present an extension of the model proposed by Finkelstein for beat-to-beat CO assessment. We use a nonlinear model consisting of a two-layer feed-forward artificial neural network. In addition to demographic (body surface area and age) and physiological parameters (HR), surrogates of contractility, afterload and mean arterial pressure based on systolic time intervals (STIs), estimated from echocardiography and heart sounds are used as inputs to our models. The results showed that the proposed models — with echocardiography as reference — produce better estimations of stroke volume/CO than the Finkelstein model (12.83±10.66 ml vs 7.23±6.6 ml), as well as higher correlation (0.46 vs 0.82).	artificial neural network;body dysmorphic disorders;body surface;cardiac output;cardiovascular diseases;cell respiration;cerebrovascular accident;echocardiography;feedforward neural network;heart sounds;inference;nonlinear system;sexually transmitted diseases;stroke volume;surrogates	Ricardo Couceiro;Paulo de Carvalho;Rui Pedro Paiva;Jorge Henriques;Manuel Antunes;Isabel Quintal;Jens Mühlsteff	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091369	intensive care medicine;radiology;medicine;stroke volume;machine learning;circulatory system;remote patient monitoring;mathematical model;cardiac output;biological engineering;correlation;feed forward;artificial neural network;cardiology	EDA	13.576268693839314	-85.61545517184862	13222
275965a698d9cdad32121783b0cde52bc784d8ac	lesion detection and characterization in digital mammography by bezier histograms	digital radiography;glandula mamaria patologia;female;analisis sensibilidad;radiodiagnostic;analisis componente principal;medical imagery;curva bezier;tumor maligno;mastografia;radiografia numerica;mammary gland;diagnostico;hombre;segmentation;glandula mamaria;radiodiagnostico;histogram;digital mammography;hembra;medical diagnostics;histogramme;mammographie;radiographie numerique;courbe bezier;sensitivity analysis;principal component analysis;human;imagerie medicale;analyse composante principale;mammary gland diseases;analyse sensibilite;computing systems;tumeur maligne;glande mammaire;imageneria medical;radiodiagnosis;femelle;mammography;diagnosis;histograma;segmentacion;glande mammaire pathologie;malignant tumor;homme;bezier curve;diagnostic	Due to some important properties of B ezier splines, they have great potential use in computer-aided mammogram diagnosis. In this paper, B ezier splines are applied in both lesion detection and characterization processes, where lesion detection is achieved by segmentation using a natural threshold computed from B ezier smoothed histogram; and lesion characterization is achieved by measuring the tness between Gaussian and B ezier histograms of data projected on principal components of the segmented lesions. Experimental results show that this approach is e cient, easy to use, and can achieve high sensitivity.	b-spline;smoothing;spline (mathematics)	Hairong Qi;Wesley E. Snyder	1999		10.1117/12.348554	pathology;geography;surgery;cartography	Robotics	37.4438093267059	-74.78174346996485	13242
b8c6197334df872fdcc94f34d56a462644a4ebc0	a novel approach to multiclass psoriasis disease risk stratification: machine learning paradigm	multiclass;color features;texture features;machine learning;psoriasis skin disease;dermatology	The stage and grade of psoriasis severity is clinically relevant and important for dermatologists as it aids them lead to a reliable and an accurate decision making process for better therapy. This paper proposes a novel psoriasis risk assessment system (pRAS) for stratification of psoriasis severity from colored psoriasis skin images having Asian Indian ethnicity. Machine learning paradigm is adapted for risk stratification of psoriasis disease grades utilizing offline training and online testing images. We design four kinds of pRAS systems. It uses two kinds of classifiers (support vector machines (SVM) and decision tree (DT)) during training and testing phases and two kinds of feature selection criteria (Principal Component Analysis (PCA) and Fisher Discriminant Ratio (FDR)), thus, leading to an exhaustive comparison between these four systems.#R##N##R##N#Our database consisted of 848 psoriasis images with five severity grades: healthy, mild, moderate, severe and very severe, consisting of 383, 47, 245, 145, and 28 images respectively. The pRAS system computes 859 colored and grayscale image features. Using cross-validation protocol with K-fold procedure, the pRAS system utilizing the SVM with FDR combination with combined color and grayscale feature set gives an accuracy of 99.92%. Several performance evaluation parameters such as: feature retaining power, aggregated feature effect and system reliability is computed meeting our assumptions and hypothesis. Our results demonstrate promising results and pRAS system is able to stratify the psoriasis disease.	machine learning;programming paradigm	Vimal K. Shrivastava;Narendra D. Londhe;Rajendra S. Sonawane;Jasjit S. Suri	2016	Biomed. Signal Proc. and Control	10.1016/j.bspc.2016.04.001	medicine;pathology;computer science;artificial intelligence;machine learning;data mining	ML	34.67197660984121	-75.20579372641856	13245
bed479b92f3c2ce943ec946b875c831043a304cf	the pathway coexpression network: revealing pathway relationships		A goal of genomics is to understand the relationships between biological processes. Pathways contribute to functional interplay within biological processes through complex but poorly understood interactions. However, limited functional references for global pathway relationships exist. Pathways from databases such as KEGG and Reactome provide discrete annotations of biological processes. Their relationships are currently either inferred from gene set enrichment within specific experiments, or by simple overlap, linking pathway annotations that have genes in common. Here, we provide a unifying interpretation of functional interaction between pathways by systematically quantifying coexpression between 1,330 canonical pathways from the Molecular Signatures Database (MSigDB) to establish the Pathway Coexpression Network (PCxN). We estimated the correlation between canonical pathways valid in a broad context using a curated collection of 3,207 microarrays from 72 normal human tissues. PCxN accounts for shared genes between annotations to estimate significant correlations between pathways with related functions rather than with similar annotations. We demonstrate that PCxN provides novel insight into mechanisms of complex diseases using an Alzheimer's Disease (AD) case study. PCxN retrieved pathways significantly correlated with an expert curated AD gene list. These pathways have known associations with AD and were significantly enriched for genes independently associated with AD. As a further step, we show how PCxN complements the results of gene set enrichment methods by revealing relationships between enriched pathways, and by identifying additional highly correlated pathways. PCxN revealed that correlated pathways from an AD expression profiling study include functional clusters involved in cell adhesion and oxidative stress. PCxN provides expanded connections to pathways from the extracellular matrix. PCxN provides a powerful new framework for interrogation of global pathway relationships. Comprehensive exploration of PCxN can be performed at http://pcxn.org/.	alzheimer's disease;biological processes;cell adhesion;complement system proteins;databases;experiment;extracellular matrix;gene ontology term enrichment;gene expression profiling;gene regulatory network;genomics;inference;interaction;kegg;mental association;microarray;molecular profiling;oxidative stress;reactome: a database of reactions, pathways and biological processes.	Yered Pita-Juárez;Gabriel M. Altschuler;Sokratis Kariotis;Wenbin Wei;Katjusa Koler;Claire Green;Rudolph E. Tanzi;Winston A Hide	2018		10.1371/journal.pcbi.1006042	gene;genomics;computational biology;dna microarray;genetics;biology;gene expression profiling;protein interaction networks;kegg;proteomics	Comp.	4.766242891151828	-57.31556327471237	13246
78a2665b49b37e1da613b54afeba0968fb30ccbf	novelty-dependent learning and topological mapping	cognitive map;connectionist models;learning rate;distributed networks;novelty detection;fear conditioning;competitive learning;novelty dependent learning;self organized feature map;lateral inhibition;kohonen map;winner take all;connectionist module for competitive learning;neural network;topological mapping	Unsupervised topological ordering, similar to Kohonen’s (1982) Self-organizing feature map, was achieved in a connectionist module for competitive learning (a CALM Map) by internally regulating the learning rate and the size of the active neighborhood on the basis of input novelty. In this module winner-take-all competition and the 'activity bubble' are due to graded lateral inhibition between units. It tends to separate representations as far apart as possible, which leads to interpolation abilities and an absence of catastrophic interference when the interfering set of patterns forms an interpolated set of the initial data set. More than the Kohonen maps, these maps provide an opportunity for building psychologically and neurophysiologically motivated multimodular connectionist models. As an example, the dual pathway connectionist model for fear conditioning by Armony, Servan-Schreiber, Cohen, and LeDoux (1997) was rebuilt and extended with CALM maps. If the detection of novelty enhances memory encoding in a canonical circuit, such as the CALM map, this could explain the finding of large distributed networks for novelty detection (e.g. Knight & Scabini, 1998) in the brain. A self-organising connectionist map 2	activity recognition;catastrophic interference;competitive learning;connectionism;gene regulatory network;interference (communication);interpolation;lateral computing;lateral thinking;novelty detection;organizing (structure);self-organization;self-organizing map;topological sorting;unsupervised learning	R. Hans Phaf;Paul den Dulk;Adriaan G. Tijsseling;Ed Lebert	2001	Connect. Sci.	10.1080/09540090110085666	winner-take-all;fear conditioning;self-organizing map;lateral inhibition;cognitive map;computer science;artificial intelligence;machine learning;competitive learning;artificial neural network	ML	19.53854432144303	-66.37842530315089	13256
b4950aca069ec1a034048c8bbe989e7bdce8488a	algorithms for fuzzy segmentation	key words dynamic programming;greedy algorithms;dynamic program;segmentation;random noise;medical image;medical imaging;pattern recognition;greedy algorithm;thresholding;medical image segmentation;fuzzy pattern recognition;fuzzy segmentation	Fuzzy segmentation is an effective way of segmenting out objects in pictures containing both random noise and shading. This is illustrated both on mathematically created pictures and on some obtained from medical imaging. A theory of fuzzy segmentation is presented. To perform fuzzy segmentation, a ‘connectedness map’ needs to be produced. It is demonstrated that greedy algorithms for creating such a connectedness map are faster than the previously used dynamic programming technique. Once the connectedness map is created, segmentation is completed by a simple thresholding of the connectedness map. This approach is efficacious in instances where simple thresholding of the original picture fails.	dynamic programming;experiment;file spanning;greedy algorithm;ibm notes;intelligent platform management interface;medical imaging;noise (electronics);shading;thresholding (image processing);eric	Bruno M. Carvalho;C. Joe Gau;Gabor T. Herman	1999	Pattern Analysis & Applications	10.1007/s100440050016	medical imaging;computer vision;greedy algorithm;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;thresholding;image segmentation;scale-space segmentation	Vision	44.60167717282946	-70.20555830994668	13265
a62ce39aa052853006c8e71d90a81c0fa00fa2e6	seismocardiogram (scg) interpretation using neural networks	heart disease;overlearning issues;cardiac disease;normal patient;heart;neural networks;learning;time measurement;neural nets;neural network architectures;cardiology;diseased patient;acceleration;coronary artery disease;artificial neural networks;multilayered neural network;electrocardiography;scg recordings;artificial neural networks seismocardiograms coronary artery disease scg recordings multilayered neural network scg recordings normal patient diseased patient heart disease neural network architectures learning overlearning issues;neural nets cardiology electrocardiography medical diagnostic computing;neural networks acceleration electrocardiography seismic measurements time measurement cardiac disease supercomputers accelerometers heart frequency;frequency;accelerometers;multilayer neural network;medical diagnostic computing;early detection;supercomputers;seismic measurements;low risk;artificial neural network;neural network;seismocardiograms	Exercise seismocardiograms (SCG) were recorded in a population of 114 patients, 57 of which were diagnosed as having coronary artery disease. The remaining 57 were diagnosed as low-risk normals. Three 60-s SCG recordings were performed at rest, immediately after exercise, and after recovery. A multilayered neural network has been developed to accept the 48 input parameters derived from the three SCG recordings, and to produce an output value of zero is the input parameters correspond to a normal patient and an output value of one if the input parameters correspond to a diseased patient. Three SCG parameters have been identified as most sensitive indicators of heart disease. The relative performance of several neural-network architectures in detecting heart disease based on the selected SCG parameters is discussed. Optimal learning and overlearning issues are discussed. The work suggests that using artificial neural networks can be a useful tool in the interpretation of subtle changes in exercise SCGs necessary for the early detection of heart disease. >	artificial neural network	Marius O. Poliac;John M. Zanetty;David Salerno;George L. Wilcox	1991		10.1109/CBMS.1991.128981	acceleration;computer science;artificial intelligence;machine learning;frequency;accelerometer;heart;artificial neural network;time	NLP	15.550104310677686	-88.63809673645426	13278
f9c6db6d82cbd0ded2b90244727fc770cb767e7b	modellierung von entscheidungsproblemen in der lehre - ein erfahrungsbericht		Implants, microbeads and microcapsules suitable for injection into an animal body comprise cross-linked but physically-native albumin and a non-albumin substance, for example a steroid or an enzyme. The microcapsules are formed under mild conditions (not above 37 DEG C. and pH 4-10) from either a native albumin and a non-denaturing bi-functional cross-linking agent or a self-polymerizable albumin derivative. The method may be used to immobilize living cells such as yeast.		Karel Vejsada	2005		10.1007/3-540-32539-5_53	discrete mathematics;mathematics;albumin;steroid;enzyme;yeast;chromatography	Crypto	3.2087572529166812	-65.1307732529254	13282
1de01ef4006fc9d9f3c84de64857ab6fc534a87a	a system for processing handwritten bank checks automatically	neural network based reading;automation of banking systems;banking system;automated check reading;handwritten checks;knowledge acquisition;automated reading of handwritten information;reading unconstrained handwritten material;neural network	In the US and many other countries, bank checks are preprinted with the account number and the check number in special ink and format; as such, these two numeric fields can be easily read and processed using automated techniques. However, the amount fields on a filled-in check is usually read by human eyes, and involves significant time and cost, especially when one considers that over 50 billion checks are processed per annum in the US alone. The system described in this paper uses the scanned image of a bank check to &#8216;read&#8217; the check. It includes three main modules that allow for fully automated bank check processing. These three modules are described in the paper; they focus sequentially on: the detection of strings within the image; the segmentation and recognition of string in a feedback loop; and the post-processing issues that help to ensure higher accuracy of recognition. The major benefit of the integrated system is the ability to address the complex problem of reading handwritten bank checks by implementing efficient algorithms for each processing step. All modules have been implemented and subsequently tested for reading the value of the check using different image databases. Due to the particular requirements of this application, the system can be tuned to yield low levels of incorrect readings; this, in turn, leads to higher levels of rejection than the levels encountered in other handwritten recognition applications. A &#8216;rejected&#8217; check can be read subsequently by human eyes or other more advanced automated approaches. However, a check &#8216;read&#8217; incorrectly is more difficult to deal with, in terms of costs and time involved to rectify the mistake. As such, our architecture can be geared towards producing the most suitable balance between inaccurate readings and rejection level, in accordance with user preferences. The experimental results presented in the paper do not focus on the best possible results for a particular database of checks; instead, they show the benefits attained independently by each of the modules proposed.	algorithm;dna data bank ofjapan;database;feedback;image scanner;international bank account number;rejection sampling;requirement;string (computer science);user (computing);video post-processing	Rafael Palacios;Amar Gupta	2008	Image Vision Comput.	10.1016/j.imavis.2006.04.012	computer vision;speech recognition;computer science;machine learning;data mining;artificial neural network	DB	29.035608994202875	-68.67173255636997	13292
182197ab1badd5a34f21ef15ecb9983cb245403c	error corrective boosting for learning fully convolutional networks with limited data		Training deep fully convolutional neural networks (F-CNNs) for semantic image segmentation requires access to abundant labeled data. While large datasets of unlabeled image data are available in medical applications, access to manually labeled data is very limited. We propose to automatically create auxiliary labels on initially unlabeled data with existing tools and to use them for pre-training. For the subsequent fine-tuning of the network with manually labeled data, we introduce error corrective boosting (ECB), which emphasizes parameter updates on classes with lower accuracy. Furthermore, we introduce SkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that combines skip connections with the unpooling strategy for upsampling. The SD-Net addresses challenges of severe class imbalance and errors along boundaries. With application to whole-brain MRI T1 scan segmentation, we generate auxiliary labels on a large dataset with FreeSurfer and finetune on two datasets with manual annotations. Our results show that the inclusion of auxiliary labels and ECB yields significant improvements. SD-Net segments a 3D scan in 7 secs in comparison to 30 hours for the closest multi-atlas segmentation method, while reaching similar performance. It also outperforms the latest state-of-the-art F-CNN models.	artificial neural network;convolutional neural network;encoder;european chemicals bureau;freesurfer;image segmentation;secure digital;upsampling	Abhijit Guha Roy;Sailesh Conjeti;Debdoot Sheet;Amin Katouzian;Nassir Navab;Christian Wachinger	2017		10.1007/978-3-319-66179-7_27	pattern recognition;boosting (machine learning);upsampling;computer science;convolutional neural network;labeled data;segmentation;image segmentation;machine learning;brain segmentation;artificial intelligence	ML	30.37781245692302	-74.7985523357333	13299
6d5d44040151af64cc8e49718357a2d557806f95	mining biologically active patterns in metabolic pathways using microarray expression profiles	microarray data;frequent pattern;expression profile;supervised learning;long range correlation;microarray profiles;markov model;biological activity;markov models;finite mixture models;metabolic pathway;finite mixture model;em algorithm;model based clustering;metabolic pathways	We present a new probabilistic framework for analyzing a metabolic pathway with microarray expression profiles. Our purpose is to find biologically significant paths and patterns in a given metabolic pathway. Our approach first builds a Markov model using a graph structure of a known metabolic pathway, and then estimates parameters of a mixture of the Markov models using microarray data, based on an EM algorithm. In our experiments, we used a main pathway of glycolysis to evaluate the effectiveness of our method. We first measured the performance of our method comparing with that of another method, in a supervised learning manner, and found that our method significantly outperformed another method, which was trained by microarray data only. We further analyzed the trained models and obtained a number of new biological findings on frequent patterns (paths) and long-range correlations in a metabolic pathway.	expectation–maximization algorithm;experiment;gene regulatory network;markov chain;markov model;microarray;supervised learning	Hiroshi Mamitsuka;Yasushi Okuno;Atsuko Yamaguchi	2003	SIGKDD Explorations	10.1145/980972.980986	metabolic pathway;computer science;bioinformatics;machine learning;pattern recognition;markov model;supervised learning	ML	5.6565193095805535	-55.78592802191359	13304
ce22a79435a54fa64710b737a45ceee6aee16058	using isolated vowel sounds for classification of mild traumatic brain injury	brain;bioelectric potentials;medical signal detection;speech processing;speech;sport bioelectric potentials brain feature extraction health care injuries learning artificial intelligence medical signal detection speech speech processing;mild traumatic brain injury classification one class machine learning algorithm acoustic feature extraction vowel sound isolation boxing tournament athlete mobile device mild traumatic brain injury detection speech analysis sport;feature extraction;injuries;sport;learning artificial intelligence;speech accuracy feature extraction brain injuries jitter acoustics;concussion speech analysis predictive models health and safety;health care	Concussions are Mild Traumatic Brain Injuries (mTBI) that are common in contact sports and are often difficult to diagnose due to the delayed appearance of symptoms. This paper explores the feasibility of using speech analysis for detecting mTBI. Recordings are taken on a mobile device from athletes participating in a boxing tournament following each match. Vowel sounds are isolated from the recordings and acoustic features are extracted and used to train several one-class machine learning algorithms in order to predict whether an athlete is concussed. Prediction results are verified against the diagnoses made by a ringside medical team at the time of recording and performance evaluation shows prediction accuracies of up to 98%.	acoustic cryptanalysis;algorithm;brain implant;machine learning;mobile device;object type (object-oriented programming);performance evaluation;sensor;voice analysis	Michael Falcone;Nikhil Yadav;Christian Poellabauer;Patrick J. Flynn	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639136	simulation;speech recognition;feature extraction;computer science;speech;sport;speech processing;health care	Robotics	14.53205064234868	-88.31960860955016	13314
d3fd3783767ed708b726b58015a2a61fd2fb848a	fast vehicle detector for autonomous driving		This paper presents a fast vehicle detector which can be deployed on NVIDIA DrivePX2 under real-time constraints. The network predicts bounding boxes with different aspect ratio and scale priors from the specifically-designed prediction module given concatenated multi-scale feature map. A new data augmentation strategy is proposed to systematically generate a lot of vehicle training images whose appearance is randomly truncated so our detector could detect occluded vehicles better. Besides, we propose a non-region-based online hard example mining framework which performs fine-tuning by picking (1) hard examples and (2) detection results with insufficient IOU. Compared to other classical object detectors, this work achieves very competitive result in terms of average precision (AP) and computational speed. For the newly-defined vehicle class (car+bus) on VOC2007 test, our detector achieves 85.32 AP and runs at 48 FPS and 30 FPS on NVIDIA Titan X & GP106 (DrivePX2), respectively.	autonomous car;computation;concatenation;convolutional neural network;floating point systems;information retrieval;randomness;real-time clock;sensor;titan	Che-Tsung Lin;Patrisia Sherryl Santoso;Shu-Ping Chen;Hung-Jin Lin;Shang-Hong Lai	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.35	artificial intelligence;computer vision;prior probability;concatenation;object detection;feature extraction;computer science;detector	Vision	28.261743940299475	-53.50789540139242	13323
2016612c3c697238dfd9e11e19dc4bc39c229880	comparing genetic pathways variation of immunoinhibitory receptor lair-1 in murine vs human internal organs	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;microarrays;bioinformatics	Background Recent evidence suggests that leukocyte-associated immunoglobulin-like receptor-1 (LAIR-1) may play an important role in down-regulating immune activities upon collagen binding [1], and its defective expression or dysfunction is clinically associated with some autoimmune diseases [2-5], cancer [6-8] and viral infection [9-12]. The human genome encodes the counterpart to LAIR-1, soluble protein LAIR-2 [13], which also binds collagen and can interfere with LAIR-1/collagen interactions [14]. However, LAIR-2 has no homologue in mouse or rat [13]. To clarify the extrapolative credibility of a murine model to human disease, we compared LAIR-1 genetic pathways in internal organs of the two species.	interaction	Shuqiu Sun;Yan Jiao;Wei Wei;Arnold E. Postlethwaite;Weikuan Gu;Dianjun Sun	2014		10.1186/1471-2105-15-S10-P9	biology;dna microarray;computer science;bioinformatics	Comp.	6.229548415396193	-62.464290701402234	13325
3e4587c261c642ea328bb3031047cd4cea640c32	a supervised learning approach to predicting coronary heart disease complications in type 2 diabetes mellitus patients	decision support;coronary heart disease;supervised learning;classification performance;ga initialization technique;classification;supervised machine learning approach;supervised machine learning;type 2 diabetes mellitus;type 2 diabetes mellitus patients;classification performance supervised machine learning approach coronary heart disease complications type 2 diabetes mellitus patients genetic algorithm weighted k nearest neighbours risk factors ulster hospital ga initialization technique medical expert knowledge decision support system;risk factors;decision support system;medical information systems;supervised learning cardiac disease diabetes machine learning blood pressure genetic mutations hospitals genetic algorithms delta modulation cardiovascular diseases;decision support systems;medical information systems cardiovascular system classification decision support systems diseases genetic algorithms learning artificial intelligence;weighted k nearest neighbours;northern ireland;diseases;genetic algorithm;genetic algorithms;cardiovascular system;k nearest neighbour;expert knowledge;learning artificial intelligence;medical expert knowledge;ulster hospital;coronary heart disease complications	A supervised machine learning approach that incorporates genetic algorithms (GA) and weighted k-nearest neighbours (WkNN) was applied to classify type 2 diabetes mellitus (T2DM) patients according to the presence or absence of coronary heart disease (CHD) complications. The investigation was carried out by analyzing potential risk factors recorded at the Ulster Hospital in Northern Ireland. A GA initialization technique that integrates medical expert knowledge was compared with traditional data-driven GA initialization techniques. The results indicate that the incorporation of expert knowledge provides only a small improvement of CHD classification performance compared with models based on data-driven initialization techniques. This may be due to data incompleteness and noise or due to the beneficial effects of treatment, which masks the complication of CHD in the dataset. Further incorporation of expert knowledge at different levels of the GA need to be addressed to improve decision support in this domain	decision support system;genetic algorithm;image noise;k-nearest neighbors algorithm;machine learning;nsa product types;software release life cycle;supervised learning	Marisol Giardina;Francisco Azuaje;Paul J. McCullagh;Roy Harper	2006	Sixth IEEE Symposium on BioInformatics and BioEngineering (BIBE'06)	10.1109/BIBE.2006.253297	genetic algorithm;decision support system;computer science;artificial intelligence;machine learning;data mining	AI	6.330887847147345	-77.68994654186378	13338
fc4094705e5f3ebd6194ac69cdf420a29c8ff081	from visual to motor strategies: training in mental rotation of hands	putamen;motor imagery;parietal sulcus;mental rotation	Functional imaging studies on mental rotation of hands have consistently pointed to the importance of the motor network implying the use of motor simulations for task solving. There is some evidence that the putamen may be a critical modulator of processing egocentric spatial orientation in mental rotation of hands and implicit motor imagery strategies have been described involving hand motor areas. This recruitment of resources processing representations of the own body is used in therapeutic mental rotation training. However, studies are lacking that investigate training-induced changes on the neuronal level. We used functional MRI to study the effects of long-term training on the neuro-functional correlates of mental rotation of hands in healthy volunteers and compared the training group to a passive control group. From pre- to post training, we found a transition of activation from the anterior putamen in unskilled performance to the posterior putamen in skilled performance. We also found an increase in activation in motor cortices and the supramarginal gyrus after learning. By contrast, members of the control group showed no improvements in performance and no pre/post-test differences in cortical activity. In conclusion, these findings suggest that increased neural efficiency after training in mental rotation of hands manifests as a decrease in visual imagery in conjunction with increased recruitment of motor-related regions. This is consistent with the obtained behavioral effects depicting motor imagery mediating expertise in mental rotation of hands.	auditory recruitment;expectation propagation;functional imaging;guided imagery;medical imaging;modulation;modulator device component;motor cortex;neural oscillation;neuroimaging;neuronal plasticity;preprocessor;programming paradigm;simulation;space perception;spatial–temporal reasoning;structure of putamen;super paper mario;tellurium;test engineer	J. Berneiser;Georg Jahn;Matthias Grothe;M. Lotze	2018	NeuroImage	10.1016/j.neuroimage.2016.06.014	psychology;cognitive psychology;neuroscience;developmental psychology;mental rotation;communication;motor imagery	ML	18.047071008259092	-78.32867050607858	13340
ae4be3c70d2109ef1353b26aa17f85530d64c592	an /spl omega/-automata approach to the representation of bilevel images	image processing;indexing terms;markov chains spl omega automata bilevel image representation zero size image objects lines points image processing operations shift flip rotation complement boundary difference union intersection size;infinite word;image representation;finite automata;automata transducers image representation image resolution image processing image coding particle measurements size measurement computer science formal languages;automata theory;markov processes image representation automata theory;markov processes;markov chain	We use /spl omega/-automata (i.e., automata over infinite words) as a device for representing bilevel images. A major advantage of our approach, as opposed to using the conventional finite automata, lies in that /spl omega/-automata are capable of representing image objects of zero size, such as lines and points. To demonstrate the feasibility of our approach, we also show how a number of image processing operations, including shift, flip, rotation, complement, boundary, difference, union, intersection, and size, can be effectively carried out in the framework of /spl omega/-automata. In particular, the size of an image represented by an /spl omega/-automaton is measured based on the theory of Markov chains. In comparison with other automata-based image representation schemes reported in the literature, our approach is capable of supporting a richer set of operations, which can be performed on the automata directly and easily.	arabic numeral 0;automaton;binary image;complement system proteins;finite-state machine;hl7publishingsubsection <operations>;image processing;intersection of set of elements;markov chain;physical object	Yih-Kai Lin;Hsu-Chun Yen	2003	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/TSMCB.2003.811123	markov chain;mathematical optimization;combinatorics;discrete mathematics;index term;quantum finite automata;image processing;computer science;theoretical computer science;machine learning;automata theory;ω-automaton;mathematics;markov process;statistics	Vision	49.36453210314752	-64.14164109057472	13341
0536906fa0bc7eb7e56bf0c01b01d7dbbd25ff7d	semantic fusion of laser and vision in pedestrian detection	evaluation performance;peaton;deteccion blanco;procesamiento informacion;pedestrian safety;performance evaluation;modelo markov;poison control;injury prevention;evaluacion prestacion;contextual information;markov logic network;safety literature;fusion capteur;circuito logico;segmentation;data fusion;probabilistic approach;region interes;traffic safety;injury control;detection cible;home safety;detection objet;injury research;semantic information;safety abstracts;markov model;human factors;circuit logique;enfoque probabilista;approche probabiliste;fusion donnee;region of interest;pedestrian;occupational safety;pedestrian detection;safety;information processing;signal classification;semantic sensor fusion;classification signal;safety research;accident prevention;violence prevention;bicycle safety;spatial relationships;region interet;sensor fusion;modele markov;classification automatique;analisis semantico;traitement information;analyse semantique;fusion datos;automatic classification;poisoning prevention;logic circuit;target detection;falls;clasificacion automatica;ergonomics;segmentacion;suicide prevention;object detection;semantic analysis;interest region;pieton	Fusion of laser and vision in object detection has been accomplished by two main approaches: (1) independent integration of sensor-driven features or sensor-driven classifiers, or (2) a region of interest (ROI) is found by laser segmentation and an image classifier is used to name the projected ROI. Here, we propose a novel fusion approach based on semantic information, and embodied on many levels. Sensor fusion is based on spatial relationship of parts-based classifiers, being performed via a Markov logic network. The proposed system deals with partial segments, it is able to recover depth information even if the laser fails, and the integration is modeled through contextual information— characteristics not found on previous approaches. Experiments in pedestrian detection demonstrate the effectiveness of our method over data sets gathered in urban scenarios. & 2010 Elsevier Ltd. All rights reserved.	angularjs;experiment;http 404;image fusion;markov chain;markov logic network;minimum bounding box;object detection;pedestrian detection;region of interest;sensor	Luciano Oliveira;Urbano Nunes;Paulo Peixoto;Marco Dias Silva;Fernando Moita	2010	Pattern Recognition	10.1016/j.patcog.2010.05.014	computer vision;simulation;computer science;human factors and ergonomics;machine learning;sensor fusion;computer security	Vision	46.37166571546924	-59.13817365227718	13354
52b7f1323229c132ed0a79194d7cedc3b289ff37	evolutionary data purification for social media classification	social network services;support vector machines;training;semantics;training data;visualization;genetic algorithms	We present a novel algorithm for the semantic labeling of photographs shared via social media. Such imagery is diverse, exhibiting high intra-class variation that demands large training data volumes to learn representative classifiers. Unfortunately image annotation at scale is noisy resulting in errors in the training corpus that confound classifier accuracy. We show how evolutionary algorithms may be applied to select a 'purified' subset of the training corpus to optimize classifier performance. We demonstrate our approach over a variety of image descriptors (including deeply learned features) and support vector machines.	automatic image annotation;evolutionary algorithm;expectation propagation;high- and low-level;image quality;machine learning;mathematical optimization;purification of quantum state;social media;software release life cycle;supervised learning;support vector machine;visual descriptor	Stuart James;John P. Collomosse	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900039	support vector machine;computer vision;training set;genetic algorithm;visualization;computer science;machine learning;pattern recognition;data mining;semantics	Vision	22.399413468950563	-57.88018443185186	13355
74ecf0e6fe87bd5befd46536667654ae5c1d6565	lateral fall detection via events in linear prediction residual of acceleration		Lateral fall is a major cause of hip fractures in elderly people. An automatic fall detection algorithm can reduce the time to get medical help. In this paper, we propose a fall detection algorithm that detects lateral falls by identifying the events in the Linear Prediction (LP) residual of the acceleration experienced by the the body during a fall. The acceleration is measured by a triaxial accelerometer. The accelerometer is attached to an elastic band and is worn around the test subject’s waist. The LP residual is filtered using a Savitzky-Golay filter and the maximum peaks are identified as falls. The results indicate that the lateral falls can be detected using our algorithm with a sensitivity of 84% when falling from standing and 90% when falling from walking.	lateral thinking	Femina Hassan Aysha Beevi;Christian Fischer Pedersen;Stefan Wagner;Stefan Hallerstede	2014		10.1007/978-3-319-07596-9_22	control theory;residual;acceleration;linear prediction;inertial measurement unit;accelerometer;waist;physics	Vision	11.021725991453598	-85.43363865167447	13356
08931c549572af3a112dc50687ac9e7ae8d15953	machine learning approach for skill evaluation in robotic-assisted surgery	surgeon skill;classification;skill assessment;machine learning;robotic assisted surgery	Evaluating surgeon skill has predominantly been a subjective task. Development of objective methods for surgical skill assessment are of increased interest. Recently, with technological advances such as robotic-assisted minimally invasive surgery (RMIS), new opportunities for objective and automated assessment frameworks have arisen. In this paper, we applied machine learning methods to automatically evaluate performance of the surgeon in RMIS. Six important movement features were used in the evaluation including completion time, path length, depth perception, speed, smoothness and curvature. Different classification methods applied to discriminate expert and novice surgeons. We test our method on real surgical data for suturing task and compare the classification result with the ground truth data (obtained by manual labeling). The experimental results show that the proposed framework can classify surgical skill level with relatively high accuracy of 85.7%. This study demonstrates the ability of machine learning methods to automatically classify expert and novice surgeons using movement features for different RMIS tasks. Due to the simplicity and generalizability of the introduced classification method, it is easy to implement in existing trainers. .	complexity;decision support system;depth perception;ground truth;machine learning;minimally invasive education;personalization;risk management information systems;robot;test set	Mahtab Jahanbani Fard;Sattar Ameri;Ratna Babu Chinnam;Abhilash K. Pandya;Michael D. Klein;R. Darin Ellis	2016	CoRR		computer vision;simulation;biological classification;computer science;machine learning	AI	3.3209743614558427	-80.43940236872363	13370
1f73cf2493f9bf44f1fbd78301868cbadcf41b3d	mri-based visualisation of orbital fat deformation during eye motion		Orbital fat, or the fat behind the eye, plays an important role in eye movements. In order to gain a better understanding  of orbital fat mobility during eye motion, MRI datasets of the eyes of two healthy subjects were acquired respectively in  seven and fourteen different directions of gaze. After semi-automatic rigid registration, the Demons deformable registration  algorithm was used to derive time-dependent three-dimensional deformation vector fields from these datasets. Visualisation  techniques were applied to these datasets in order to investigate fat mobility in specific regions of interest in the first  subject. A qualitative analysis of the first subject showed that in two of the three regions of interest, fat moved half as  much as the embedded structures. In other words, when the muscles and the optic nerve that are embedded in the fat move, the  fat partly moves along with these structures and partly flows around them. In the second subject, a quantitative analysis  was performed which showed a relation between the distance behind the sciera and the extent to which fat moves along with  the optic nerve.  	molecular orbital	Charl P. Botha;Thijs de Graaf;Sander Schutte;Ronald Root;Piotr Wielopolski;Frans C. T. van der Helm;Huibert J. Simonsz;Frits H. Post	2008		10.1007/978-3-540-72630-2_13	computer vision	Vision	31.45854972749868	-84.09194303449145	13386
fc161ed14aef5cc3c31b4278539c0a40a5bc4b49	a histogram-based joint boosting classification for determining urban road	histograms;urban areas;roads;algorithms;image analysis;lighting;detection and identification systems;digital mapping	Road detection for inner-city scenarios remains a difficult problem due to the high complexity in scene layout with unmarked or weakly marked roads and poor lighting conditions. This paper introduces a novel method based on multi normalized-histogram with Joint Boosting algorithm to road recognition. The approach performs three modules in parallel that are the Image Segmentation, the Texton Maps and the new one Dispton Maps. The first one applies a combination of pre-filters with Watershed Transform to make the super-pixel. The last two perform a dense feature extraction based on 2D texture image and 3D disparity image to get appearance, shape and context information. At last, a discriminative model of road class is learned based on distribution of Textons and Disptons applied in Joint Boosting algorithm. The proposed work reports real experiments carried out in a challenging urban environment utilizing the modern KITTI benchmark for road areas in which meaningful evaluation can be done to illustrate the validity and application of this approach.	algorithm;benchmark (computing);binocular disparity;discriminative model;experiment;feature extraction;image segmentation;pixel;texton;watershed (image processing)	Giovani B. Vitor;Alessandro Corrêa Victorino;Janito V. Ferreira	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6958038	computer vision;geography;machine learning;pattern recognition	Vision	32.96717462561049	-53.11469697742163	13392
1ad6072b282ac174fe0ca2e2f3219943b32e6836	hierarchically structured multi-view features for mobile visual search	databases;elektroteknik och elektronik;communication systems;electrical engineering electronic engineering information engineering;visualization;servers;feature extraction;multiview feature database hierarchically structured multiview features mobile visual search graph model feature correspondence multiview images multiview geometric verification hierarchical structure properties query matching strategy;kommunikationssystem;mobile communication;robustness;visual databases feature extraction graph theory image retrieval mobile computing;visualization servers mobile communication robustness databases feature extraction	This paper presents an approach for using hierarchically structured multi-view features for mobile visual search. We utilize a graph model to describe the feature correspondences between multi-view images. To add features of images from new viewpoints, we designa level raising algorithm and the associated multi-view geometric verification, which are based on the properties of the hierarchical structure. With this approach, features from new viewpoints can be recursively added in an incremental fashion. Additionally, we designa query matching strategy which utilizes the advantage of the hierarchical structure. The experimental results show that our structure of the multi-view feature database can efficiently improve the performance of mobile visual search.	algorithm;recursion;server (computing);verification and validation;web search query	Xinrui Lyu;Haopeng Li;Markus Flierl	2014	2014 Data Compression Conference	10.1109/DCC.2014.43	computer vision;visualization;mobile telephony;feature extraction;computer science;machine learning;pattern recognition;communications system;server;robustness	Robotics	36.944031406836785	-56.26381234628027	13402
32ead65e9ffd0ecfe6825d100788e5afb9c6b118	modular neural network integrator for human recognition from ear images	wavelet analysis;image recognition;recognition process;2d wavelet analysis;biometrics access control;biometrics;neural net architecture;human recognition;wta modular neural network integrator human recognition ear images modular neural network architecture recognition process biometrics ear recognition 2d wavelet analysis global thresholding method sugeno measure winner takes all;modular neural network architecture;wavelet transforms;artificial neural networks;ear;ear recognition;modular neural network integrator;fingerprint recognition;feature extraction;global thresholding method;wta;ear images;wavelet transforms biometrics access control ear image recognition neural net architecture;face;humans;sugeno measure;winner take all;modular neural network;ear artificial neural networks feature extraction humans face fingerprint recognition image recognition;winner takes all	We propose a modular neural network architecture in order to make easy and fast the recognition process of the ear as a biometric. Comparing with other biometrics, ear recognition has one of the best performances, even when it has not received much attention. To improve the performance for ear recognition and make a comparison with other existing methods, we used the 2D wavelet analysis with Global Thresholding method, and Sugeno Measure and Winner-Takes-All (WTA) as modular neural network integrator. Recognition results achieved was up to 97%.	artificial neural network;biometrics;image compression;modular neural network;network architecture;performance;preprocessor;region of interest;thresholding (image processing);wavelet;weapon target assignment problem	Lizette Gutierrez;Patricia Melin;Miguel Lopez	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596633	winner-take-all;computer vision;computer science;artificial intelligence;machine learning;time delay neural network;artificial neural network	Vision	29.752179867560393	-65.44926670152563	13407
223fc5279135dfc4bda8cbeb4efe24256cd8962a	multi-modal region selection approach for training object detectors	mirflickr;automatic segmentation;multi modal;region selection;social network;large scale;self training;large scale dataset;object detection	Our purpose in this work is to boost the performance of object classifiers learned using the self-training paradigm. We exploit the multi-modal nature of tagged images found in social networks, to optimize the process of region selection when retraining the initial model. More specifically, the proposed approach uses a small number of manually labelled regions to train the initial object detection classifiers. Then, a large number of loosely tagged images, pre-segmented by an automatic segmentation algorithm, is used to enhance the initial training set with additional image regions. However, in contrast to the typical case of self-training where the image regions are selected based solely on how well they fit to the original classification model, our approach aims at optimizing this selection by making combined use of both visual and textual information. The experimental results show that the object detection classifiers generated using the proposed approach outperform the classifiers generated using the typical self-training paradigm.	algorithm;modal logic;object detection;programming paradigm;sensor;social network;test set	Elisavet Chatzilari;Spiros Nikolopoulos;Yiannis Kompatsiaris;Josef Kittler	2012		10.1145/2324796.2324803	computer vision;computer science;machine learning;multimodal interaction;pattern recognition;social network	Vision	30.68024133131309	-53.063537917805455	13410
78b4759c1b23f74f1da38887e5b291bf4c6fb7b6	stereo matching with nonparametric smoothness priors in feature space	feature extraction;robustness;graph cuts;computer vision;nonparametric statistics;layout;image features;point cloud;feature vector;graph theory;image segmentation;shape;feature space;sparse graph;pixel;stereo vision;graph cut	We propose a novel formulation of stereo matching that considers each pixel as a feature vector. Under this view, matching two or more images can be cast as matching point clouds in feature space. We build a nonparametric depth smoothness model in this space that correlates the image features and depth values. This model induces a sparse graph that links pixels with similar features, thereby converting each point cloud into a connected network. This network defines a neighborhood system that captures pixel grouping hierarchies without resorting to image segmentation. We formulate global stereo matching over this neighborhood system and use graph cuts to match pixels between two or more such networks. We show that our stereo formulation is able to recover surfaces with different orders of smoothness, such as those with high-curvature details and sharp discontinuities. Furthermore, compared to other single-frame stereo methods, our method produces more temporally stable results from videos of dynamic scenes, even when applied to each frame independently.	computer stereo vision;cut (graph theory);feature vector;glossary of computer graphics;image segmentation;pixel;point cloud;sparse graph code;sparse matrix;temporal logic	Brandon M. Smith;Li Zhang;Hailin Jin	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206793	computer vision;template matching;cut;feature vector;computer science;graph theory;machine learning;pattern recognition;mathematics	Vision	46.35497433255728	-52.20109779204549	13422
1bc0c82b3ac3e772fc41278ab04f88451be8d484	distinguishing computer graphics from natural images using convolution neural networks		This paper presents a deep-learning method for distinguishing computer generated graphics from real photographic images. The proposed method uses a Convolutional Neural Network (CNN) with a custom pooling layer to optimize current best-performing algorithms feature extraction scheme. Local estimates of class probabilities are computed and aggregated to predict the label of the whole picture. We evaluate our work on recent photo-realistic computer graphics and show that it outperforms state of the art methods for both local and full image classification.	aggregate data;algorithm;artificial neural network;boosting (machine learning);computer graphics;computer vision;convolution;convolutional neural network;deep learning;feature extraction;screenshot	Nicolas Rahmouni;Vincent Nozick;Junichi Yamagishi;Isao Echizen	2017	2017 IEEE Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2017.8267647	support vector machine;convolutional neural network;computer graphics;feature extraction;artificial neural network;graphics;contextual image classification;convolution;artificial intelligence;pattern recognition;computer science	Vision	24.97339966075313	-53.36732025681623	13424
06104d413dba0601ae3338ccdecc1236439b240f	influence of the multisine excitation amplitude design for biomedical applications using impedance spectroscopy	impedance;medical image processing electric impedance imaging;electrical impedance spectroscopy;data collection;time frequency;impedance broadband communication dispersion time frequency analysis impedance measurement accuracy;spectrum;electric impedance imaging;accuracy;electrical impedance spectroscopy biomedical applications multisine broadband excitation amplitude design noise to signal ratio impedance analyzer;medical image processing;conference report;impedance spectroscopy;impedance measurement;body composition dielectric spectroscopy humans;body composition;dispersion;biomedical application;broadband communication;time frequency analysis	Electrical Impedance Spectroscopy (EIS) is a powerful tool to collect data from many biological materials in a wide variety of applications. Body composition fluid or tissue and organ state monitoring are just some examples of these applications. While the classical EIS is based on frequency sweep, the EIS technique using broadband excitations allows to acquire simultaneous impedance spectrum data. The strength and weakness of broadband EIS relies on the fact that it enables multiple Electrical Bio-Impedance (EBI) data collection in a short measuring time but at the cost of losing impedance spectrum accuracy. In general, there is a relationship between the broadband excitation time/frequency properties and the final EBI's accuracy obtained. This paper studies the influence of the multisine broadband excitation amplitude's design over the EBI accuracy by means of the resultant Noise-to-Signal Ratio (NSR) obtained when measuring with a custom impedance analyzer. Theory has been supported by a set of validation experiments.	analyzer, device;antenna analyzer;body dysmorphic disorders;cardiography, impedance;characteristic impedance;data collection;excitation;experiment;external bus interface;lagrangian relaxation;linear programming relaxation;neutral sidebent rotated;nominal impedance;quantitative impedance;radio-frequency sweep;resultant;sample variance;signal-to-noise ratio;spectral density;electric impedance	Benjamin Sanchez;Ramon Bragós;Gerd Vandersteen	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090987	electronic engineering;time–frequency analysis;engineering;electrical engineering;physics;quantum mechanics;statistics	EDA	25.012207444933953	-86.5033947242493	13433
015fbb1baabe81475090966265d52a309d8bc75b	distinct mechanisms for multimodal integration and unimodal representation in spatial development	vision robotics mutimodalities learning gain field audio;psychology cognition learning artificial intelligence;psychology;computational modeling robot sensing systems cognition modulation;sensory alignment mechanism multimodal integration unimodal representation spatial development statistical learning developmental psychology attribute spatial cognition coherence conditional learning visual modality auditory modality;cognition;learning artificial intelligence	In this paper, we attempt to reconciliate two views of spatial development based on two mechanisms of statistical learning and of sensory alignment. Conflicting results in developmental psychology attribute either a developmental period to spatial cognition (Piaget). Besides, these results conflict with other researches in which infants do demonstrate good coordination and coherence across modalities (Gibsonian), even from restricted pre-natal experiences [1], [2]. In order to study both views, we present at first a simple model based on conditional learning which integrates visual and auditory modalities although it has some limitation regarding the number of degrees of freedom. In second, we propose then to use a sensory alignment mechanism, which allows the system to learn invariances in the world. In experiments with a robot head, we show the advantages of each strategy. We then discuss about the future possibilities of merging both models and their implications.	cognition;experience;experiment;machine learning;multimodal interaction;piaget's theory of cognitive development	Alex Pitti;Arnaud J. Blanchard;Matthieu Cardinaux;Philippe Gaussier	2012	2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL)	10.1109/DevLrn.2012.6400886	psychology;computer vision;artificial intelligence;communication	Robotics	20.22423739177839	-64.93643361555128	13451
718bca69a1c82be594f2cfc71734baa1769eb6a3	towards a low-cost point-of-care screening platform for electronic auscultation of vital body sounds		Analyzing pathological sounds is a simple test for understanding what is happening inside the body. This process of listening to the body is traditionally done using a Littmann stethoscope. Traditional stethoscopes are limited by inadequate sound amplification. Thus, if the sound of interest is of low amplitude, as is the case with biomedical sounds, detecting them is difficult. The electronic stethoscope (eStethoscope) platform is designed to facilitate doctors in the act of listening to body sounds. This is achieved by enhancing the functionality of traditional stethoscopes using electronic instrumentation. Additionally, using modern signal processing tools, the eStethoscope serves as a screening and monitoring aid to physicians and a learning aid for students. Preliminary results show that the signals acquired are distinguishable to have originated from their respective sources (heart, lung, and knee). The eStethoscope demonstrates the practicality of using low-cost instrumentation to obtain signal quality that is comparable to clinical grade signals, adding convenience to both the physician and the patient.	amplifier;sensor;signal processing	Uzair Mayat;Fayez Qureshi;Saad Ahmed;Yashodhan Athavale;Sridhar Krishnan	2017	2017 IEEE Canada International Humanitarian Technology Conference (IHTC)	10.1109/IHTC.2017.8058166	stethoscope;auscultation;human–computer interaction;wearable computer;signal processing;point of care;active listening;electronic stethoscope;electrical engineering;telemedicine;computer science	Visualization	10.478045699230874	-88.38152660897218	13454
2c11d0d0c974907a541ab3b8994ae9b98d3c48d6	an efficient fingerprint matching approach based on minutiae to minutiae distance using indexing with effectively lower time complexity	false matching ratio fingerprint feature fingerprint matching fingerprint based authentication system fingerprint template indexing false non match ratio;vectors fingerprint recognition feature extraction authentication indexing;authentication;false non match ratio;fingerprint matching;vectors;indexing;fingerprint recognition;feature extraction;fingerprint based authentication system;false matching ratio fingerprint matching approach minutiae to minutiae distance indexing time complexity fingerprint based person authentication system matching accuracy space complexity templates false nonmatch ratio;fingerprint feature;false matching ratio;image matching authorisation computational complexity fingerprint identification;fingerprint template	Fingerprint matching is the main module of fingerprint-based person authentication system. Accuracy of fingerprint matching is an important objective of this type authentication system. Multiple features are used for better matching accuracy but more features add more computational complexity as well as time and space complexity. In this paper, we proposed an approach of fingerprint based authentication system where fingerprint matching is carried out using spacial information (distance) of minutiae points only. This approach is simple and it needs very small space to store templates. We have used an indexing technique to speed up the matching process. In our experiment, we have used FVC2004 fingerprint data-set as input data and investigated the false non-match ratio and false matching ratio for DB2, DB3 and DB4 also.	authentication;computation;computational complexity theory;dspace;distance-vector routing protocol;euclidean distance;fingerprint recognition;minutiae;time complexity	Subhas Barman;Samiran Chattopadhyay;Debasis Samanta;Sujoy Bag;Goutam Show	2014	2014 International Conference on Information Technology	10.1109/ICIT.2014.46	computer vision;search engine indexing;feature extraction;computer science;pattern recognition;data mining;authentication;fingerprint recognition	Vision	35.503571625992855	-61.527518311133406	13513
099bdf0c70bc304a88292081b3fe3280fa4e8e16	reprint of 'model of unidirectional block formation leading to reentrant ventricular tachycardia in the infarct border zone of postinfarction canine hearts'	health research;uk clinical guidelines;myocardium;biological patents;animals;europe pubmed central;dogs;citation search;journal article;myocardial infarction;ventricular tachycardia;uk phd theses thesis;isthmus;life sciences;unidirectional block;myocardial contraction;wavefront curvature;humans;tachycardia ventricular;uk research reports;medical journals;models cardiovascular;activation mapping;europe pmc;biomedical research;bioinformatics	Background: When the infarct border zone is stimulated prematurely, a unidirectional block line (UBL) can form and lead to double-loop (figure-of-eight) reentrant ventricular tachycardia (VT) with a central isthmus. The isthmus is composed of an entrance, center, and exit. It was hypothesized that for certain stimulus site locations and coupling intervals, the UBL would coincide with the isthmus entrance boundary, where infarct border zone thickness changes from thin-to-thick in the travel direction of the premature stimulus wavefront. Method: A quantitative model was developed to describe how thin-to-thick changes in the border zone result in critically convex wavefront curvature leading to conduction block, which is dependent upon coupling interval. The model was tested in 12 retrospectively analyzed postinfarction canine experiments. Electrical activation was mapped for premature stimulation and for the first reentrant VT cycle. The relationship of functional conduction block forming during premature stimulation to functional block during reentrant VT was quantified. Results: For an appropriately placed stimulus, in accord with model predictions: (1) The UBL and reentrant VT isthmus lateral boundaries overlapped (error: 4.875.7 mm). (2) The UBL leading edge coincided with the distal isthmus where the center-entrance boundary would be expected to occur. (3) The mean coupling interval was 164.6711.0 ms during premature stimulation and 190.7720.4 ms during the first reentrant VT cycle, in accord with model calculations, which resulted in critically convex wavefront curvature with functional conduction block, respectively, at the location of the isthmus entrance boundary and at the lateral isthmus edges. Discussion: Reentrant VT onset following premature stimulation can be explained by the presence of critically convex wavefront curvature and unidirectional block at the isthmus entrance boundary when the premature stimulation interval is sufficiently short. The double-loop reentrant circuit pattern is a consequence of wavefront bifurcation around this UBL followed by coalescence, and then impulse propagation through the isthmus. The wavefront is blocked from propagating laterally away from the isthmus by sharp increases in border zone thickness, which results in critically convex wavefront curvature at VT cycle lengths. & 2015 Elsevier Ltd. All rights reserved.	bifurcation theory;border zone;coalescing (computer science);experiment;heart block;implantable cardioverter-defibrillator;infarction;lateral thinking;mobitz type ii atrioventricular block;norm (social);onset (audio);reentrancy (computing);reprint;software propagation;tachycardia;tachycardia, atrioventricular nodal reentry;tachycardia, ventricular;thickness (graph theory)	Edward J. Ciaccio;James Coromilas;Hiroshi Ashikaga;Daniel O. Cervantes;Andrew L. Wit;Nicholas S. Peters;Elliot R. McVeigh;Hasan Garan	2015		10.1016/j.compbiomed.2015.04.032	myocardial infarction;cardiology	AI	20.482207698940332	-82.55253320646847	13516
69ffb8af621e27ac419ad4dd3f06db49f3533d0e	layout error correction using deep neural networks		Layout analysis, mainly including binarization and text-line extraction, is one of the most important performance determining steps of an OCR system for complex medieval historical document images, which contain noise, distortions and irregular layouts. In this paper, we present a novel text-line error correction technique which include a VGG Net to classify non-text-line and adversarial network approach to obtain the layout bounding mask. The presented text-line error correction technique are applied to a collection of 15th century Latin documents, which achieved more than 75% accuracy for segmentation techniques.	artificial neural network;deep learning;distortion;error detection and correction;historical document;information extraction;neural networks;qr code	Srie Raam Mohan;Syed Saqib Bukhari;Andreas Dengel	2018	2018 13th IAPR International Workshop on Document Analysis Systems (DAS)	10.1109/DAS.2018.61	computer vision;error detection and correction;real-time computing;artificial neural network;image segmentation;historical document;computer science;convolution;artificial intelligence	Vision	37.04565990850238	-65.88589314816721	13531
7909c69dcf2c6e39c73690e5a8ec042581a2c80d	detection of moving cast shadows for object segmentation	moving object;image segmentation;standards;image sequences image segmentation motion estimation standards;shadow detection;motion estimation;indexing terms;object segmentation;object detection object segmentation light sources change detection algorithms iso standards mpeg 4 standard layout image sequences two dimensional displays shape;image sequence;iso mpeg 4 moving cast shadows detection object segmentation dominating scene background monocular video image sequence temporal integration shape estimation;image sequences	To prevent moving shadows being misclassified as moving objects or parts of moving objects, this paper presents an explicit method for detection of moving cast shadows on a dominating scene background. Those shadows are generated by objects moving between a light source and the background. Moving cast shadows cause a frame difference between two succeeding images of a monocular video image sequence. For shadow detection, these frame differences are detected and classified into regions covered and regions uncovered by a moving shadow. The detection and classification assume plane background and a nonnegligible size and intensity of the light sources. A cast shadow is detected by temporal integration of the covered background regions while subtracting the uncovered background regions. The shadow detection method is integrated into an algorithm for two-dimensional (2-D) shape estimation of moving objects from the informative part of the description of the international standard ISO/MPEG-4. The extended segmentation algorithm compensates first apparent camera motion. Then, a spatially adaptive relaxation scheme estimates a change detection mask for two consecutive images. An object mask is derived from the change detection mask by elimination of changes due to background uncovered by moving objects and by elimination of changes due to background covered or uncovered by moving cast shadows. Results obtained with MPEG-4 test sequences and additional sequences show that the accuracy of object segmentation is substantially improved in presence of moving cast shadows. Objects and shadows are detected and tracked separately.	algorithm;explicit and implicit methods;information;linear programming relaxation;shadow volume	Jürgen Stauder;Roland Mech;Jörn Ostermann	1999	IEEE Trans. Multimedia	10.1109/6046.748172	computer vision;index term;computer science;motion estimation;image segmentation;computer graphics (images)	Vision	48.72183936558844	-53.51213738976422	13537
3151c8ec432bc1a14288212487e2f682fd11b6b3	fast opposite weight learning rules with application in breast cancer diagnosis	computer aided diagnosis;receiver operator characteristic;false negative;breast cancer diagnosis;convergence rate;multilayer perceptron;classification;false positive rate;mammography;back propagation;opposition based learning;neural network	Classification of breast abnormalities such as masses is a challenging task for radiologists. Computer-aided Diagnosis (CADx) technology may enhance the performance of radiologists by assisting them in classifying patterns into benign and malignant categories. Although Neural Networks (NN) such as Multilayer Perceptron (MLP) have drawbacks, namely long training times, a considerable number of CADx systems employ NN-based classifiers. The reason being that they provide high accuracy when they are appropriately trained. In this paper, we introduce three novel learning rules called Opposite Weight Back Propagation per Pattern (OWBPP), Opposite Weight Back Propagation per Epoch (OWBPE), and Opposite Weight Back Propagation per Pattern in Initialization (OWBPI) to accelerate the training procedure of an MLP classifier. We then develop CADx systems for the diagnosis of breast masses employing the traditional Back Propagation (BP), OWBPP, OWBPE and OWBPI algorithms on MLP classifiers. We quantitatively analyze the accuracy and convergence rate of each system. The results suggest that the convergence rate of the proposed OWBPE algorithm is more than 4 times faster than that of the traditional BP. Moreover, the CADx systems which use OWBPE classifier on average yield an area under Receiver Operating Characteristic (ROC), i.e. Az, of 0.928, a False Negative Rate (FNR) of 9.9% and a False Positive Rate (FPR) of 11.94%.	artificial neural network;back pain;backpropagation;breast carcinoma;categories;classification;congenital abnormality;converge;feature extraction;film-type patterned retarder;hyperactive behavior;mammary neoplasms;memory-level parallelism;multilayer perceptron;neural tube defects;norm (social);performance;preprocessor;quad flat no-leads package;radiology;rate of convergence;receiver operating characteristic;reducing diet;rule (guideline);software propagation;algorithm;azimexon;cancer diagnosis	Fatemeh Saki;Amir Tahmasbi;Hamid Soltanian-Zadeh;Shahriar B. Shokouhi	2013	Computers in biology and medicine	10.1016/j.compbiomed.2012.10.006	biological classification;false positive rate;computer science;artificial intelligence;backpropagation;machine learning;pattern recognition;rate of convergence;multilayer perceptron;receiver operating characteristic;artificial neural network	ML	32.57810969698169	-74.45232175424815	13543
25472c00017e301639bd4c382019da78a88ba11b	viroblast: a stand-alone blast web server for flexible queries of multiple databases and user's datasets	nucleotides;amino acid sequence;web interface;similarity search	UNLABELLED ViroBLAST is a stand-alone BLAST web interface for nucleotide and amino acid sequence similarity searches. It extends the utility of BLAST to query against multiple sequence databases and user sequence datasets, and provides a friendly output to easily parse and navigate BLAST results. ViroBLAST is readily useful for all research areas that require BLAST functions and is available online and as a downloadable archive for independent installation.   AVAILABILITY http://indra.mullins.microbiol.washington.edu/blast/viroblast.php.		Wenjie Deng;David C. Nickle;Gerald H. Learn;Brandon Maust;James I. Mullins	2007	Bioinformatics	10.1093/bioinformatics/btm331	biology;nucleotide;computer science;bioinformatics;database;peptide sequence;user interface;world wide web	DB	-2.3764424713213765	-59.25545233450849	13552
96e733632640ce8bffcec53ba91a92eb23e38875	stress distribution of metatarsals during forefoot strike versus rearfoot strike: a finite element study	finite element analysis;forefoot strike;metatarsal stress;rearfoot strike	Due to the limitations of experimental approaches, comparison of the internal deformation and stresses of the human man foot between forefoot and rearfoot landing is not fully established. The objective of this work is to develop an effective FE modelling approach to comparatively study the stresses and energy in the foot during forefoot strike (FS) and rearfoot strike (RS). The stress level and rate of stress increase in the Metatarsals are established and the injury risk between these two landing styles is evaluated and discussed. A detailed subject specific FE foot model is developed and validated. A hexahedral dominated meshing scheme was applied on the surface of the foot bones and skin. An explicit solver (Abaqus/Explicit) was used to stimulate the transient landing process. The deformation and internal energy of the foot and stresses in the metatarsals are comparatively investigated. The results for forefoot strike tests showed an overall higher average stress level in the metatarsals during the entire landing cycle than that for rearfoot strike. The increase rate of the metatarsal stress from the 0.5 body weight (BW) to 2 BW load point is 30.76% for forefoot strike and 21.39% for rearfoot strike. The maximum rate of stress increase among the five metatarsals is observed on the 1st metatarsal in both landing modes. The results indicate that high stress level during forefoot landing phase may increase potential of metatarsal injuries.	bone structure of foot;cumulative trauma disorders;desert strike: return to the gulf;finite element method;hexahedron;human body weight;metatarsal bone structure;natural science disciplines;reed–solomon error correction;solver;structure of cisterna magna	Shudong Li;Yan Zhang;Yaodong Gu;James Ren	2017	Computers in biology and medicine	10.1016/j.compbiomed.2017.09.018	deformation (mechanics);computer science;foot bones;artificial intelligence;structural engineering;computer vision;finite element method;body weight;surgery;forefoot	HCI	28.43066513034355	-85.44273129926869	13570
26d942f50d1bd656e4800c8072c270ca1458b06b	fmri adaptation dissociates syntactic complexity dimensions	female;ucl;middle aged;reading;oxygen;male;discovery;theses;conference proceedings;superior temporal gyrus;image processing computer assisted;multi dimensional;frontal lobe;digital web resources;acoustic stimulation;ucl discovery;adult;magnetic resonance imaging;open access;adaptation physiological;cerebral cortex;ucl library;humans;photic stimulation;book chapters;open access repository;language;young adult;behavior;psycholinguistics;movement;ucl research;broca s area	The current fMRI adaptation study sought to elucidate the dimensions of syntactic complexity and their underlying neural substrates. For the first time with fMRI, we investigated repetition suppression (i.e., fMRI adaptation) for two orthogonal dimensions of sentence complexity: embedding position (right-branching vs. center-embedding) and movement type (subject vs. object). Two novel results were obtained: First, we found syntactic adaptation in Broca's area and second, this adaptation was structured. Anterior Broca's area (BA 45) selectively adapted to movement type, while posterior Broca's area (BA 44) demonstrated adaptation to both movement type and embedding position (as did left posterior superior temporal gyrus and right inferior precentral sulcus). The functional distinction within Broca's area is critical not only to an understanding of the functional neuroanatomy of language, but also to theoretical accounts of syntactic complexity, demonstrating its multi-dimensional nature. These results implicate that during syntactic comprehension, a large network of areas is engaged, but that only anterior Broca's area is selective to syntactic movement.	acclimatization;broca aphasia;business architecture;center embedding;cumulative trauma disorders;dimensions;groove;list comprehension;neuroanatomy;numerous;structure of precentral sulcus;superior temporal gyrus;syntactic predicate;zero suppression;fmri	Andrea Santi;Yosef Grodzinsky	2010	NeuroImage	10.1016/j.neuroimage.2010.03.034	psychology;movement;neuroscience;developmental psychology;young adult;magnetic resonance imaging;oxygen;language;psycholinguistics;communication;reading;cognitive science;behavior	ML	16.720210752866393	-77.42745952086442	13586
4c179e95ed50cb59798a1f17388b590951622183	detection of common mistakes in novice violin playing		Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musical instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset comprising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are generated from the same feature set with different scales, including two note-level representations and three segmentlevel representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vector machine for classification. Results show that the Fmeasures using different feature representations can vary up to 20% for two types of playing mistakes. It demonstrates the different sensitivities of each feature representation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed.	feature selection;linear discriminant analysis;onset (audio);principle of good enough;support vector machine	Yin-Jyun Luo;Li Su;Yi-Hsuan Yang;Tai-Shih Chi	2015			speech recognition;violin;computer science	AI	-0.5956070054058208	-89.32493830679213	13592
cb4c27beb705db5393f90241a70fd5267b44a99e	on the permanence of eeg signals for biometric recognition	databases;biometrics access control;biometrics;database;medical signal processing biometrics access control electroencephalography;electroencephalographic signal eeg signal biometric recognition brain signal clinical application biometric identifier permanence;database biometrics electroencephalography permanence;brain modeling;electroencephalography biometrics access control databases electrodes scalp feature extraction brain modeling;electrodes;feature extraction;permanence;electroencephalography;scalp	Brain signals have been investigated for more than a century in the medical field. However, despite the broad interest in clinical applications, their use as a biometric identifier has been only recently considered by the scientific community. In this paper, we focus on the permanence across time of brain signals, specifically of electroencephalographic (EEG) signals, issue of paramount importance for the deployment of brain-based biometric recognition systems in real life, not yet fully addressed. In particular, we speculate about the stability of EEG features by analyzing the recognition performance that can be achieved when comparing EEG signals acquired during different sessions. We carry out an extensive set of experimental tests, performed on several EEG-based biometric systems over a large database, comprising three recordings taken from 50 healthy subjects in resting state conditions, acquired in a time span of approximately one month and a half. The results confirm that a significant level of permanence can be guaranteed.	biometric device;biometrics;database;electroencephalography;handwritten biometric recognition;identifier;real life;resting state fmri;software deployment	Emanuele Maiorana;Daria La Rocca;Patrizio Campisi	2016	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2015.2481870	speech recognition;electroencephalography;feature extraction;computer science;electrode;artificial intelligence;object permanence;biometrics	Vision	17.878906366795864	-92.75897277169496	13602
33c668bbb158a4596d1750521a06e0ca0c130d52	adaptive fiducial-free registration using multiple point selection for real-time electromagnetically navigated endoscopy	sensors;endoscopy;navigation systems;endoscopes	This paper proposes an adaptive fiducial-free registration method that uses a multiple point selection strategy based on sensor orientation and endoscope radius information. To develop a flexible endoscopy navigation system, since we use an electromagnetic tracker with positional sensors to estimate bronchoscope movements, we must synchronize such tracker and pre-operative image coordinate systems using either marker-based or fiducial-free registration methods. Fiducial-free methods assume that bronchoscopes are operated along bronchial centerlines. Unfortunately, such an assumption is easily violated during interventions. To address such a tough assumption, we utilize an adaptive strategy that generates multiple points in terms of sensor measurements and bronchoscope radius information. From these generated points, we adaptively choose the optimal point, which is the closest to its assigned bronchial centerline, to perform registration. The experimental results from phantom validation demonstrate that our proposed adaptive strategy significantly improved the fiducial-free registration accuracy from at least 5.4 to 2.2 mm compared to current available methods. © (2014) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	fiducial marker;real-time clock	Xióngbiao Luó;Kensaku Mori	2014		10.1117/12.2043388	computer vision;simulation;sensor	Vision	41.2643855045213	-84.10210759484907	13606
a5028d07aa927f58c4913185c642cfea30ecc2f2	robustness of transcriptional regulation in yeast-like model boolean networks	sncf;transcriptional regulation;saccharomyces cerevisiae;ncf;articulo;cf;boolean function;degree distribution;gene expression;transcription regulation;boolean network;rf;dynamic properties	We investigate the dynamical properties of the transcriptional regulation of gene expression in the yeast Saccharomyces Cerevisiae within the framework of a synchronously and deterministically updated Boolean network model. By means of a dynamically determinant subnetwork, we explore the robustness of transcriptional regulation as a function of the type of Boolean functions used in the model that mimic the influence of regulating agents on the transcription level of a gene. We compare the results obtained for the actual yeast network with those from two different model networks, one with similar in-degree distribution as the yeast and random otherwise, and another due to Balcan et al., where the global topology of the yeast network is reproduced faithfully. We, surprisingly, find that the first set of model networks better reproduce the results found with the actual yeast network, even though the Balcan et al. model networks are structurally more similar to that of yeast. INTRODUCTION Recent advances in biotechnology allowed the accumulation of a vast amount of experimental data on intra-cellular processes, however, our knowledge on how a cell works remains incomplete [Lockhart & Winzeler, 2000; Barabasi & Olvai, 2004]. The key component of the functional organization in a cell is the regulation of gene expression. By now, interacting gene pairs for several organisms ∗corresponding author: mtugrul@ifisc.uib-csic.es and his present address: IFISC(Institute for CrossDisciplinary Physics and Complex Systems), UIB-CSIC, Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain have been identified with significant coverage [Bergmann et al., 2003]. In particular the set of regulatory interactions identified in Saccharomyces Cerevisiae are believed to be close to complete [Teixeira et al., 2006]. The activation/suppression dynamics in a cell evolves on a complex and inhomogeneous network of interactions. Therefore, the framework of graph theory serve as a powerful mathematical tool for studying the regulation of the gene expression on cellular level [Albert & Barabási, 2002; Bollobas, 1998; Milo et al., 2002; Colizza et al., 2006; Newman, 2001; Barabasi & Olvai, 2004]. The topology of the graph describing the gene regulatory network (GRN) is far from being random and has been studied for several organisms, in particular the budding yeast [Guelzim & et al., 2002; Nicholas & et al., 2004; Bergmann et al., 2003]. Deterministically and synchronously updated Boolean networks have been used widely as a model for regulatory dynamics [Kauffman, 1969; Aldana, 2003; Balcan & Erzan, 2007, 2006]. In this model, the expression levels of genes are discretized to take values 0 or 1 at each time step. Although it is a major oversimplification [Norrell et al., 2007], this approach has proven valuable in the context of gene regulation [Mendoza et al., 1999; Espinosa-Soto et al., 2004; Albert & Othmer, 2003]. The network topology of the yeast’s GRN is now believed to be unveiled to a large extent. However the nature of interactions, i.e., the rules that govern the dynamics, are not known in comparable detail. Accordingly, a statistical approach involving randomly assigned functions is relevant. Several classes of such functions have been investigated in the literature. The 1 unbiased choice is to pick random Boolean functions. On the other hand, it has been claimed that experimental data is consistent with a subset of Boolean functions where one of the output is fixed for a particular value of one of the inputs (canalizing functions) [Harris & et al., 2002]. It has also been suggested that a subset of canalizing functions (nested canalizing functions) is more appropriate for gene regulation dynamics on yeast [Kauffman et al., 2003]. A more recent study finds that two subclasses of the nested canalizing functions are actually dominant in the yeast [Nikolajewa et al., 2006]. The computational bottleneck in the analysis of Boolean network dynamics is the fact of that number of states increases exponentially with system size. This makes an exhaustive enumeration prohibitive, even if, in most cases, a fraction of the nodes can be left outside the analysis due to their irrelevance to the dynamics by virtue of either the topology or the choice of the function set [Socolar & Kauffman, 2003]. In this paper, we determine and use a strongly connected subset of the genes that dictates the network’s dynamical character and use a statistical approach to identify its robustness. The paper is organized as follows. In the Method section, we present the yeast’s gene regulation network, describe the employed Boolean dynamics, define the function classes used for setting the rules of the dynamics, propose a dynamically relevant subnetwork and the model networks used for comparison with yeast. In the Result section, we present and analysis of the yeast’s GRN dynamics, in particular exploring the robustness of the network under small perturbations, comparing the results for different types of functions and for the actual vs. model network topologies. We discuss our findings in the last section. METHOD & MODELS Transcriptional regulation of gene expression in a cell operates through transcription factors (TFs). These proteins bind the DNA on “promoter regions” (PRs) that act as the regulation centers of each gene. The details of this interaction can be very complex. In our study, as in past studies in the literature, we assume that effect of the TFs that regulate a certain gene can be summarized in a Boolean function whose inputs represent the presence or the absence of TFs and the output determines whether the gene is activated or inhibited for the given expression profile of the TF genes. The regulation dynamics evolves on a directed graph, whose nodes are the genes and a directed edge from A to B indicates that the product of A regulates B. The corresponding network for Saccharomyces Cerevisiae can be retrieved from YEASTRACT database [Teixeira et al., 2006] (www.yeastract.com). In order to be able to compare our results with past studies, we here consider an earlier version (2005) of the network including 4252 genes (with 146 TFs) with 12541 interactions. As explained below, we also consider two model networks, one with a similar in-degree distribution as the yeast network above and random otherwise, and another with a topology highly similar to that of yeast, which emerges from a null-model proposed earlier [Balcan et al., 2007]. The Boolean regulation dynamics on these networks is investigated by means of a synchronous and deterministic update of the network state as follows: Each node (gene) i has a state σi(t) at a particular time t where σi(t) is either 1 (on) or 0 (off). The network state S(t) is the set of individual node states: S(t) = {σ1(t), σ2(t), .., σN (t)}. σi(t + 1) is determined by the Boolean function Bi assigned to i, which is a function of the states of the neighbor nodes connected to i by incoming edges. We used four types of random function classes found in the literature as described below.	angular resolution (graph drawing);bianconi–barabási model;boolean network;c date and time functions;complex systems;degree distribution;deterministic algorithm;directed graph;discretization;dynamical system;emergence;gene expression profiling;graph theory;harris affine region detector;interaction;network model;network topology;notability in the english wikipedia;perturbation theory;project milo;randomness;relevance;samuel newman;socolar;strongly connected component;subnetwork;synchronization (computer science);transcription (software);tree accumulation;word lists by frequency;yeastract;zero suppression	Murat Tugrul;Alkan Kabakçioglu	2010	I. J. Bifurcation and Chaos	10.1142/S0218127410026228	bioinformatics;transcriptional regulation	Comp.	5.851640150418354	-59.110058767253456	13611
f320d2b36b6201d881875844f57f45450985f967	machine learning techniques applied to the diagnosis of acute abdominal pain	machine learning;abdominal pain	Without Abstract	machine learning	Christian Ohmann;Qianyi Yang;Vassilis Moustakis;Konrad Lang;P. J. van Elk	1995		10.1007/3-540-60025-6_144	computer science;machine learning	AI	14.771813344658208	-87.94049160531203	13613
0f0a25d3be0d50a134f6f68e6a82bd8a2f668882	swiden: convolutional neural networks for depiction invariant object recognition	object category recognition;deep learning;convolutional neural networks;depiction invariance	Current state of the art object recognition architectures achieve impressive performance but are typically specialized for a single depictive style (e.g. photos only, sketches only). In this paper, we present SwiDeN: our Convolutional Neural Network (CNN) architecture which recognizes objects regardless of how they are visually depicted (line drawing, realistic shaded drawing, photograph etc.). In SwiDeN, we utilize a novel `deep' depictive style-based switching mechanism which appropriately addresses the depiction-specific and depiction-invariant aspects of the problem. We compare SwiDeN with alternative architectures and prior work on a 50-category Photo-Art dataset containing objects depicted in multiple styles. Experimental results show that SwiDeN outperforms other approaches for the depiction-invariant object recognition problem.	convolutional neural network;line drawing algorithm;neural networks;norm (social);outline of object recognition;shading	Ravi Kiran Sarvadevabhatla;Shiv Surya;Srinivas S. S. Kruthiventi;R. Venkatesh Babu	2016		10.1145/2964284.2967208	computer vision;computer science;artificial intelligence;machine learning;pattern recognition;deep learning	Vision	23.67795171677077	-53.54055556830628	13633
76f5bae6f1fa0c95c3e73fc1c4b3b2ae7b514a80	automated 3d geometric reasoning in computer assisted joint reconstructive surgery	computer graphics technologies;orthopedic surgery;virtual model;fuzzy logic automated 3d geometric reasoning computer assisted joint reconstructive surgery computer graphics technologies preoperative planning intraoperative navigation tumor surgery application caos system orthosys ct images anatomical landmark patient model virtual model prosthesis positioning;computer graphics;tumours;computer assisted joint reconstructive surgery;prosthetics;orthosys;intraoperative navigation;anatomical landmark;computer graphic;medical computing;prosthesis positioning;fuzzy logic;bones;3d model;three dimensional displays;image reconstruction;bones neoplasms prosthetics orthopedic surgery image reconstruction joints computer graphics technology planning navigation application software;knee;geometric reasoning;computerised tomography;surgery;tumors;patient model;tumours computer graphics computerised tomography image reconstruction medical computing prosthetics surgery;caos system;ct images;tumor surgery application;preoperative planning;medial axis;automated 3d geometric reasoning	Computer Assisted Orthopedic Surgery (CAOS) employing information and computer graphics technologies for preoperative planning, intraoperative navigation, and for guiding or performing surgical interventions, has received very little attention for bone tumor surgery applications. We have developed a CAOS system called OrthoSYS, driven by geometric reasoning algorithms to visualize tumor size, shape, and plan for resection according to the tumor's spread, starting from a 3D model reconstructed from CT images. Anatomical landmarks on bone are automatically identified and labeled, useful for registering patient model with virtual model during surgery and also as a reference for tumor resection and prosthesis positioning. The thickness of bone stock remaining after tumor resection is automatically analyzed to choose the best modular stem and fix the prosthesis. A method for prosthesis components selection using fuzzy logic has been developed to assist the surgeons. The medial axis of the long bones and anatomical landmarks are used for positioning the prosthesis in virtual planning and verification in the intraoperative stage. A set of anatomical metrics have been developed to measure the effectiveness of the prosthetic replacement of bone.	3d modeling;algorithm;apache axis;ct scan;computer graphics;fuzzy logic;medial graph;thickness (graph theory)	K. Subburaj;Bhallamudi Ravi;Manish Agarwal	2009	2009 IEEE International Conference on Automation Science and Engineering	10.1109/COASE.2009.5234143	computer vision;engineering;biological engineering;surgery	Robotics	38.79921744640189	-84.85975806727077	13657
17faa70179b5b769cd7fce0254a13c3057b12dfc	microbesonline: an integrated portal for comparative and functional genomics	genes;software;evolutionary history;microbiology comparative genomics functional genomics evolution bioremediation;escherichia coli;complete genome;comparative analysis;expression profile;saccharomyces cerevisiae;structural chemical analysis;databases nucleic acid;comparative genomics;biological pathways;databases genetic;chemical analysis;bioinformatics bioremediation comparative genomics functional genomics microarrays stress response;earth sciences;fungi;internet;protein structure tertiary;phylogenetic tree;knowledge management and preservation prokaryotic genomes database tool annotation resource genes;functional genomics;comparative and functional genome analysis phylogenetic context multi species genome browser operon and regulon prediction methods gene and species phylogeny browser workbench for sequence analysis;genome bacterial;life sciences;gene family;algorithms;source code;sequence analysis;bacteria;basic biological sciences;metabolic pathway;computational biology;desulfovibrio;information storage and retrieval;trees phylogenetic profile metabolic pathways operon predictions microbial genome;gene expression profiling;oligonucleotide array sequence analysis;documentation;functionals;databases protein	Since 2003, MicrobesOnline (http://www.microbesonline.org) has been providing a community resource for comparative and functional genome analysis. The portal includes over 1000 complete genomes of bacteria, archaea and fungi and thousands of expression microarrays from diverse organisms ranging from model organisms such as Escherichia coli and Saccharomyces cerevisiae to environmental microbes such as Desulfovibrio vulgaris and Shewanella oneidensis. To assist in annotating genes and in reconstructing their evolutionary history, MicrobesOnline includes a comparative genome browser based on phylogenetic trees for every gene family as well as a species tree. To identify co-regulated genes, MicrobesOnline can search for genes based on their expression profile, and provides tools for identifying regulatory motifs and seeing if they are conserved. MicrobesOnline also includes fast phylogenetic profile searches, comparative views of metabolic pathways, operon predictions, a workbench for sequence analysis and integration with RegTransBase and other microbial genome resources. The next update of MicrobesOnline will contain significant new functionality, including comparative analysis of metagenomic sequence data. Programmatic access to the database, along with source code and documentation, is available at http://microbesonline.org/programmers.html.	archaea;desulfovibrio vulgaris;documentation;functional genomics;fungi;gene expression profiling;gene family;genome;genomic structural variation;metagenomics;microarray;microbesonline;one thousand;operon;phylogenetic profiling;phylogenetic tree;phylogenetics;qualitative comparative analysis;sequence analysis;shewanella;trees (plant);workbench	Paramvir S. Dehal;Marcin P. Joachimiak;Morgan N. Price;John T. Bates;Jason K. Baumohl;Dylan Chivian;Greg D. Friedland;Katherine H. Huang;Keith Keller;Pavel S. Novichkov;Inna Dubchak;Eric John Alm;Adam Paul Arkin	2010		10.1093/nar/gkp919	functional genomics;biology;metabolic pathway;phylogenetic tree;bacteria;documentation;bioinformatics;gene family;sequence analysis;gene expression profiling;comparative genomics;escherichia coli;genetics;source code	Comp.	-0.7736174553776343	-59.45734198820421	13664
b4ea64b01fe81fa71960163260c9796ff3f0d7f7	electre tri-c, a multiple criteria decision aiding sorting model applied to assisted reproduction	multiple criteria decision aid;risk of multiple pregnancies;decision aid;infertility;assisted reproductive technology;assisted reproduction;multiple criteria decision aiding;sorting problems;electre tri c;assisted reproduction technology	OBJECTIVE The aim of this paper is to apply an informatics tool for dealing with a medical decision aiding problem to help infertile couples to become parents, when using assisted reproduction.   METHODS A multiple criteria decision aiding method for sorting or ordinal classification problems, called Electre Tri-C, was chosen in order to assign each couple to an embryo-transfer category. The set of categories puts in evidence a way for increasing the single pregnancy rate, while minimizing multiple pregnancies. The decision aiding sorting model was co-constructed through an interaction process between the decision aiding analysts and the medical experts.   RESULTS According to the sample used in this study, the Electre Tri-C method provides a unique category in 86% of the cases and it achieves a sorting accuracy of 61%. Retrospectively, the medical experts do agree that some of their judgments concerning the number of embryos to transfer back to the uterus of the woman could be different according to these results. The current ART methodology achieves a single pregnancy rate of 47% and a twin pregnancy rate of 14%. Thus, this informatics tools may play an important role for supporting ART medical decisions, aiming to increase the single pregnancy rate, while minimizing multiple pregnancies.   LIMITATIONS Building the set of criteria comprises a part of arbitrariness and imperfect knowledge, which require time and expertise to be refined. Among them, three criteria are modeled by means of a holistic classification procedure by the medical experts.	categories;embryo;holism;infertility;informatics (discipline);judgment;level of measurement;pregnancy rate;sorting;tri-level sync;triangular function;trimipramine	José Rui Figueira;Juscelino Almeida Dias;Sara Matias;Bernard Roy;M. J. Carvalho;C. E. Plancha	2011	International journal of medical informatics	10.1016/j.ijmedinf.2010.12.001	assisted reproductive technology;artificial intelligence;operations research	ML	2.383641500182409	-79.21372235182713	13673
7686f369b4b31acf3ce430651ed332c5d584bab7	a novel approach for video text detection and recognition based on a corner response feature map and transferred deep convolutional neural network		The text presented in videos contains important information for content analysis, indexing, and retrieval of videos. The key technique for extracting this information is to find, verify, and recognize video text in various languages and fonts against complex backgrounds. In this paper, we propose a novel method that combines a corner response feature map and transferred deep convolutional neural networks for detecting and recognizing video text. First, we use a corner response feature map to detect candidate text regions with a high recall. Next, we partition the candidate text regions into candidate text lines by projection analysis using two alternative methods. We then construct classification networks transferred from VGG16, ResNet50, and InceptionV3 to eliminate false positives. Finally, we develop a novel fuzzy c-means clustering-based separation algorithm to obtain a clean text layer from complex backgrounds so that the text is correctly recognized by commercial optical character recognition software. The proposed method is robust and has good performance on video text detection and recognition, which was evaluated on three publicly available test data sets and on the high-resolution test data set we constructed.	algorithm;artificial neural network;cluster analysis;convolutional neural network;image resolution;optical character recognition;robustness (computer science);sensor;test data	Wei Lu;Hongbo Sun;Jinghui Chu;Xiangdong Huang;Jiexiao Yu	2018	IEEE Access	10.1109/ACCESS.2018.2851942	optical character recognition;convolutional neural network;feature extraction;search engine indexing;cluster analysis;artificial neural network;distributed computing;computer science;content analysis;test data;artificial intelligence;pattern recognition	Vision	36.08148707871366	-64.46596009758576	13679
e9e50f0b064b841efafa83d6c0b5b8592776b2dc	"""corrigendum to """"real-time line detection through an improved hough transform voting scheme"""" [pattern recognition 41 (1) 299-314]"""	real time;line detection;pattern recognition;hough transform	Clarification A vertical line in the original imagewould cause a division by zero operation that is not dealtwithin Algorithm2, line 20 ( / m′ =xu/ √ 1− x2 v in Eq. (14)). In such a case, however, the / m′ is zero, and 2 and (Eq. (13)) are also equal to zero. Thus, in practice, division by zero never actually happens. Acknowledgments: The authors would like to thank Ian (from University of Waterloo) for asking this.	division by zero;edge detection;hough transform;line level;pattern recognition;real-time transcription;timeline;vertical bar	Leandro A. F. Fernandes;Manuel Menezes de Oliveira Neto	2008	Pattern Recognition	10.1016/j.patcog.2008.04.007	hough transform;computer vision;speech recognition;computer science;pattern recognition	Vision	38.629778112964104	-65.16643854995101	13748
e0c1f0a86bd76a45e94d11a1a0a3d575c7e47f98	face sketch matching using speed up robust feature descriptor		In this paper, face sketch and face image matching problem is presented. Matching of the sketch with face is crucial for the law enforcement applications and received attention of the researchers in the recent years. Face sketch and face images are the two different modality representations of the same face. Face sketch is drawn based on description given by the witness when no other source of information is available about the suspect. Matching of the sketch with the face image is challenging problem due to the visual difference between a face sketch and face image. To find the potential suspect, the sketch is compared with the different face images. Speed Up Robust Feature (SURF) descriptor used for matching similarity between sketch and the face image. The result shows that, SURF descriptor gives better result as compared to Scale Invariant Feature Transform descriptor for the viewed and forensic sketches.	feature model;sketch	N. K. Bansode;P. K. Sinha	2016		10.1007/978-981-10-4859-3_38	computer vision;pattern recognition	Vision	35.97681790150339	-55.99548065969181	13758
0796b79ca0f15942d7d86729898c83cb6364045b	applying matrix factorization in data reconstruction for heart disease patient classification	matrix factorization;data mining;classification;classification data mining matrix factorization machine learning;machine learning;pattern classification cardiology data mining diseases health care matrix decomposition numerical analysis patient diagnosis;diseases heart matrix decomposition matrix converters machine learning algorithms support vector machines data mining;patient datasets matrix factorization data reconstruction heart disease patient classification health illnesses heart disease diagnosis data mining based technique classification performance diagnosis performance	Heart disease is one of the most severe health illnesses. Developing accurate and efficient methods to diagnose heart disease is crucial in providing good heart healthcare to patients. In this paper, a data mining based technique for diagnosing heart disease is introduced, in which heart disease related patient data sets are utilized. A matrix factorization based technique for missing data reconstruction is presented. Numerical results show that recovery data sets are able to achieve reliable diagnosis or classification performance comparable to using original completed patient datasets.	algorithm;data mining;machine learning;missing data;numerical method;the matrix	Huaxia Wang;Yu-Dong Yao;Wei Qian;Fleming Lure	2015	2015 17th International Conference on E-health Networking, Application & Services (HealthCom)	10.1109/HealthCom.2015.7454506	computer science;data science;pattern recognition;data mining	ML	5.117072996071567	-76.60710714183617	13763
aaa71508a92aca729ed82bd950a863b894b4b690	health internet of things: metrics and methods for efficient data transfer	internet of things;mobile applications;health information systems	The rapid development of modern Information and Communication Technologies (ICTs) in recent years and their introduction into people’s daily lives worldwide, has led to new circumstances at all levels of the social environment. In health care in particular, sensors and data links offer potential for constant monitoring of patient’s symptoms and needs, in real time, enabling physicians to diagnose and monitor health problems wherever the patient is, either at home or outdoors. However, the use of Internet of Things concepts in the health domain does not come without extra data and therefore a data transfer cost overheads. To deal with these overheads, novel metrics, and methods are introduced in an attempt to maximize the capabilitie s and widen acceptance/usage provided by the Internet of Things. Without losing its general ity, the method discussed is experimentally evaluated in the paradigm of the Health domain. The focus is on the need for an overview of availabl e data formats and transmission methods and selection of the optimal combi nation, which can result to reduction/minim ization of costs. An analytic methodol ogy is presented backed with theoretical metrics and eval uated experimentally. 2012 Elsevier B.V. All rights reserved.	eval;experiment;internet of things;maxima and minima;programming paradigm;sensor	Mersini Paschou;Evangelos Sakkopoulos;Efrosini Sourla;Athanasios K. Tsakalidis	2013	Simulation Modelling Practice and Theory	10.1016/j.simpat.2012.08.002	health informatics;simulation;computer science;engineering;electrical engineering;artificial intelligence;operating system;data mining;computer security;internet of things;computer network	AI	4.1742181954899475	-89.09037575204272	13767
d2bc9aa04c8efc288dfeb51974b963d462a8b6d1	a spatial-pyramid scene categorization algorithm based on locality-aware sparse coding	histograms;image recognition;support vector machines;encoding dictionaries histograms training feature extraction image recognition support vector machines;training;feature extraction;dictionaries;transforms image classification image coding natural scenes support vector machines;spatial pyramid pooling model spatial pyramid scene categorization algorithm locality aware sparse coding scene recognition natural scene images locality constrained sparse coding lcsp linear svm end to end model dense sift descriptor codeword distribution;encoding	Scene recognition has a wide range of applications, such as object recognition and detection, content-based image indexing and retrieval, and intelligent vehicle and robot navigation. In particular, natural scene images tend to be very complex and are difficult to analyze due to changes of illumination and transformation. In this study, we investigate a novel model to learn and recognize scenes in nature by combining locality constrained sparse coding (LCSP), Spatial Pyramid Pooling, and linear SVM in end-to-end model. First, interesting points for each image in the training set are characterized by a collection of local features, known as codewords, obtained using dense SIFT descriptor. Each codeword is represented as part of a topic. Then, we employ LCSP algorithm to learn the codeword distribution of those local features from the training images. Next, a modified Spatial Pyramid Pooling model is employed to encode the spatial distribution of the local features. For the final stage, a linear SVM is employed to classify local features encoded by Spatial Pyramid Pooling. Experimental evaluations on several benchmarks well demonstrate the effectiveness and robustness of the proposed method compared to several state-of-the-art visual descriptors.	algorithm;benchmark (computing);categorization;code word;codebook;encode;end-to-end principle;locality of reference;neural coding;outline of object recognition;pyramid (geometry);robotic mapping;scale-invariant feature transform;sparse matrix;test set;visual descriptor	Dang Duy Thang;Shintami Chusnul Hidayati;Yung-Yao Chen;Wen-Huang Cheng;Shih-Wei Sun;Kai-Lung Hua	2016	2016 IEEE Second International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2016.93	computer vision;computer science;machine learning;pattern recognition	Vision	32.701811267905306	-53.88209910780148	13775
c3864df8015027ba2fdf5407bf8a6a72da2b0324	unsupervised segmentation of dual-echo mr images by a sequentially learned gaussian mixture model	sequentially learned gaussian mixture model;unsupervised learning;brain;image segmentation;gaussian processes;biomedical nmr;unsupervised segmentation;aging;prior knowledge;global spatial relationship;global spatial relationship unsupervised segmentation dual echo mr images sequentially learned gaussian mixture model brain tissues density distributions mri echo finite gaussian mixture model expectation maximization;brain biomedical nmr medical image processing image segmentation unsupervised learning gaussian processes;brain tissues;artificial neural networks;brain modeling;mr imaging;gaussian mixture model;expectation maximization;streaming media;medical image processing;magnetic resonance imaging;pixel;information processing;intelligent systems;mri echo;dual echo mr images;image segmentation pixel brain modeling magnetic resonance imaging aging artificial neural networks intelligent systems information processing australia streaming media;spatial relationships;brain tissue;em algorithm;finite gaussian mixture model;density distributions;australia	This paper proposes a method for unsupervised segmentation of brain tissues from dual-echo MR images without any prior knowledge about the number of tissues and their density distributions on each MRI echo. The brain tissues are described by a Finite Gaussian Mixture Model (FGMM). The FGMM parameters are learned by sequentially applying the Expectation Maximization (EM) algorithm to a stream of data sets which are specifically organized according to the global spatial relationship of the brain tissues. Preliminary results on actual MRI slices have shown the method to be prornising.	expectation–maximization algorithm;mixture model;unsupervised learning	Wanqing Li;Mark W. Morrison;Yianni Attikiouzel	1995		10.1109/ICIP.1995.537700	unsupervised learning;computer vision;expectation–maximization algorithm;information processing;computer science;magnetic resonance imaging;machine learning;pattern recognition	ML	43.47305356477544	-75.73174828272782	13885
9cda3e56cec21bd8f91f7acfcefc04ac10973966	periocular biometrics: databases, algorithms and directions	databases;eyebrows;image segmentation;iris recognition;iris recognition face recognition feature extraction image classification;feature extraction;surgery;databases feature extraction iris recognition face eyebrows surgery image segmentation;face;plastic surgery periocular biometrics independent modality facial region in eye vicinity eyelids lashes eyebrows iris texture face images iris images feature extraction gender classification ethnicity classification gender transformation;soft biometrics periocular biometrics databases segmentation features	Periocular biometrics has been established as an independent modality due to concerns on the performance of iris or face systems in uncontrolled conditions. Periocular refers to the facial region in the eye vicinity, including eyelids, lashes and eyebrows. It is available over a wide range of acquisition distances, representing a trade-off between the whole face (which can be occluded at close distances) and the iris texture (which do not have enough resolution at long distances). Since the periocular region appears in face or iris images, it can be used also in conjunction with these modalities. Features extracted from the periocular region have been also used successfully for gender classification and ethnicity classification, and to study the impact of gender transformation or plastic surgery in the recognition performance. This paper presents a review of the state of the art in periocular biometric research, providing an insight of the most relevant issues and giving a thorough coverage of the existing literature. Future research trends are also briefly discussed.	algorithm;biometrics;data acquisition;database;modality (human–computer interaction);uncontrolled format string	Fernando Alonso-Fernandez;Josef Bigün	2016	2016 4th International Conference on Biometrics and Forensics (IWBF)	10.1109/IWBF.2016.7449688	face;computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;iris recognition;geometry;image segmentation	Vision	29.69260968700407	-61.63868935693712	13922
d9c417622ab5f23ee9d00461c5e68b0be1769e52	fast approximated sift	vision ordenador;repetabilite;image processing;repetibilidad;procesamiento imagen;espacio escala;region interes;cost analysis;traitement image;analisis costo;computer vision;analyse cout;histogram;scale space;histogramme;estructura datos;vision ordinateur;structure donnee;region interet;histograma;data structure;repeatability;espace echelle;interest region	We propose a considerably faster approximation of the well known SIFT method. The main idea is to use efficient data structures for both, the detector and the descriptor. The detection of interest regions is considerably speed-up by using an integral image for scale space computation. The descriptor which is based on orientation histograms, is accelerated by the use of an integral orientation histogram. We present an analysis of the computational costs comparing both parts of our approach to the conventional method. Extensive experiments show a speed-up by a factor of eight while the matching and repeatability performance is decreased only slightly.	adaboost;approximation algorithm;c++;computation;data structure;experiment;fast fourier transform;repeatability;scale space;scale-invariant feature transform;speedup;whole earth 'lectronic link	Michael Grabner;Helmut Grabner;Horst Bischof	2006		10.1007/11612032_92	computer vision;repeatability;scale space;simulation;data structure;image processing;gloh;computer science;cost–benefit analysis;histogram	Vision	46.613770892685416	-59.11189489531281	13926
2656e37d057b6003d1e9d369880338da36e14aef	introduction to knowledge discovery in medical databases and use of reliability analysis in data mining	databases;reliability;database optimization knowledge discovery medical databases reliability analysis data mining dm algorithms database attributes;algorytmy;diabetes;reliability data mining medical information systems;data analysis;analiza danych;reliability databases data models medical diagnostic imaging diabetes breast cancer knowledge discovery;algorithms;breast cancer;medical diagnostic imaging;data models;knowledge discovery	Data mining (DM) is a collection of algorithms that are used to find some novel, useful and interesting knowledge in databases. DM algorithms are based on applied fields of mathematics and informatics, such as mathematical statistics, probability theory, information theory, neural networks. Some methods of these fields can be used to find hidden relation between data, what can be used to create models that predict some behavior or describe some common properties of analyzed objects. In this paper, we combine methods of DM with tools of reliability analysis to investigate importance of individual database attributes. Results of such investigation can be used in database optimization because it allows identifying attributes that are not important for purposes for which the database is used. Our approach is based on some coincidence between the key terms of DM and reliability analysis.	algorithm;artificial neural network;data mining;database;informatics;information theory;mathematical optimization;reliability engineering	Elena N. Zaitseva;Miroslav Kvassay;Vitaly G. Levashenko;Jozef Kostolny	2015	2015 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2015F327	data modeling;database theory;computer science;data science;breast cancer;data mining;reliability;database;knowledge extraction;data analysis;information retrieval;database testing;statistics	ML	1.2590837909204589	-75.27209816725694	13932
9c37097dfb55a8e0e79e50f763077e769292f0f0	chip-array 2: integrating multiple omics data to construct gene regulatory networks	pou5f1 protein mouse;conference_paper;transcription factors;histones;chromatin;transcriptome;article	Transcription factors (TFs) play an important role in gene regulation. The interconnections among TFs, chromatin interactions, epigenetic marks and cis-regulatory elements form a complex gene transcription apparatus. Our previous work, ChIP-Array, combined TF binding and transcriptome data to construct gene regulatory networks (GRNs). Here we present an enhanced version, ChIP-Array 2, to integrate additional types of omics data including long-range chromatin interaction, open chromatin region and histone modification data to dissect more comprehensive GRNs involving diverse regulatory components. Moreover, we substantially extended our motif database for human, mouse, rat, fruit fly, worm, yeast and Arabidopsis, and curated large amount of omics data for users to select as input or backend support. With ChIP-Array 2, we compiled a library containing regulatory networks of 18 TFs/chromatin modifiers in mouse embryonic stem cell (mESC). The web server and the mESC library are publicly free and accessible athttp://jjwanglab.org/chip-array.	cell (microprocessor);compiler;drosophila <fruit fly, genus>;gene expression regulation;gene regulatory networks;gene regulatory network;interaction;medical transcription;motif;mouse embryonic stem cells;omics;server (computing);transcription factor;transcription (software);transcription, genetic;web server;world wide web;study of epigenetics	Panwen Wang;Jing Qin;Yiming Qin;Yun Zhu;Lily Yan Wang;Mulin Jun Li;Michael Q. Zhang;Junwen Wang	2015		10.1093/nar/gkv398	biology;molecular biology;chromatin;transcriptome;bioinformatics;histone;genetics;transcription factor	Comp.	-0.3758246826770445	-59.474955446259386	13933
0872172f05effaab8cfb391c7f8f1ca7810cff13	a fast and robust matching framework for multimodal remote sensing image registration		1 Abstract: While image registration has been studied in remote sensing community for decades, registering multimodal data [e.g., optical, light detection and ranging (LiDAR), synthetic aperture radar (SAR), and map] remains a challenging problem because of significant nonlinear intensity differences between such data. To address this problem, we present a novel fast and robust matching framework integrating local descriptors for multimodal registration. In the proposed framework, a local descriptor (such as Histogram of Oriented Gradient (HOG), Local Self-Similarity or Speeded-Up Robust Feature) is first extracted at each pixel to form a pixel-wise feature representation of an image. Then we define a similarity measure based on the feature representation in frequency domain using the Fast Fourier Transform (FFT) technique, followed by a template matching scheme to detect control points between images. We also propose a novel pixel-wise feature representation using orientated gradients of images, which is named channel features of orientated gradients (CFOG). This novel feature is an extension of the pixel-wise HOG descriptor, and outperforms that both in matching performance and computational efficiency. The major advantages of the proposed framework include (1) structural similarity representation using the pixel-wise feature description and (2) high computational efficiency due to the use of FFT. Moreover, we design an automatic registration system for very large-size multimodal images based on the proposed framework. Experimental results	aperture (software);computation;fast fourier transform;gradient;image registration;multimodal interaction;nonlinear system;pixel;self-similarity;similarity measure;structural similarity;synthetic intelligence;template matching	Yuanxin Ye;Lorenzo Bruzzone;Jie Shan;Francesca Bovolo;Qing Zhu	2018	CoRR		fast fourier transform;pixel;frequency domain;artificial intelligence;similarity measure;remote sensing;pattern recognition;synthetic aperture radar;image registration;ranging;computer science;template matching	Vision	39.27774780007215	-56.27512293601066	13960
7182acfe0e6ffeda7df437ec951e0d702bb94eb5	loop problem in proteins: developments on monte carlo simulated annealing approach	dihedral angle;simulated annealing;factor h;monte carlo simulation;hydrogen bond	Calculations of loop segments in bovine pancreatic trypsin inhibitor starting from random conformations are more efficient, reproducible, and reliable due to several program enhancements. Monte Carlo simulated annealing (MCSA) calculations of a five-residue a-helix N-terminus segment (H5) and @-strand segment (B5) in this study are compared to the corresponding loop calculations in our previous study. Characteristics of the calculations are: the lowest final total energy conformations (LECs) are within 5 kcal/mol; the average backbone deviations of the computed segments from the native X-ray conformations are 0.43 -t 0.15 A for H5 and 0.68 -t 0.20 A for B5; and all the native backbone-backbone hydrogen bonds (H bonds) are present in the best LECs. Compared to the previous study, the H5 and B5 calculations are about 3 and 24 times more efficient, respectively. In the analysis of the best H5 simulated annealing run, backbone-backbone H bonds appear between RT = 4 and 70 kcal/mol, where RT is the Boltzmann temperature factor. H bonds that involve side chains appear in the RT = 1-10 kcal/mol range. Program enhancements implemented are varying main chain versus side chain dihedral angle selection rate, varying +/I,!J and xl/x2 dihedral angles in pairs, the use of main chain and side chain rotamer libraries, and varying the location of the segment origin.	hydrogen;hydrogen-like atom;internet backbone;library (computing);monte carlo method;simulated annealing	Louis Carlacci;S. Walter Englander	1996	Journal of Computational Chemistry	10.1002/(SICI)1096-987X(199606)17:8%3C1002::AID-JCC9%3E3.0.CO;2-Y	crystallography;mathematical optimization;chemistry;simulated annealing;computational chemistry;factor h;dihedral angle;hydrogen bond;monte carlo method	Comp.	12.316684100589725	-60.416614651169155	13962
53a426612183c5d85c8cdeff2554c44e9160a076	a bsn based service for post-surgical knee rehabilitation at home	earth magnetic field;developed solution;unassisted rehabilitation;post-surgical knee rehabilitation;case study;anterior cruciate ligament;biomechanical parameter;wider application field;knee rehabilitation;patient movement;magnetic data;inertial measurement unit	"""The paper illustrates a prototype of an end-to-end service to support unassisted rehabilitation of motor functions. The core of the developed solution is a system able to analyze the patient movements during the execution of the prescribed exercises. The motion analysis relies on real-time evaluations of biomechanical parameters, derived from inertial and magnetic data provided by a wireless body sensor network, worn by the patient while he is doing his exercises. The method uses an approach based on complementary filters and addresses a number of challenges, such as compensating the incorrect positioning of motes and managing perturbations of the Earth magnetic field. Besides, the solution provides the patient with a number of \""""coaching functions\"""", aimed at helping him in getting the best from his training at home, and with a videoconferencing tool to be used whenever the direct evaluation of the therapist is needed. Although this system can have a wider application field, in this work the focus is on the knee rehabilitation after the anterior cruciate ligament (ACL) reconstruction, in order to demonstrate the suitability of this solution to address specific clinical requirements. Preliminary results on this case study are provided."""		Roberto Nerino;Laura Contin;Walter Gonçalves da Silva Pinto;Giuseppe Massazza;Maria Vittoria Actis;Patrizia Capacchione;Antonio Chimienti;Giuseppe Pettiti	2013			simulation;physical medicine and rehabilitation;engineering;physical therapy	HCI	10.510071757758826	-86.8179276290916	13968
733cd80b4cbeb79323ef2b9a6d27c75853e1d0e0	spiking neurons can learn to solve information bottleneck problems and extract independent components	extraction information;prediccion;calcul neuronal;optimisation;neural computation;brain;sensory processing;neurone impulsionnel;information sources;high dimensionality;firing rate;taux decharge;optimizacion;learning;taux tirs;information extraction;information source;source information;blind source separation;62m20;neurona pulsante;65kxx;optimization method;independent component analysis;metodo optimizacion;aprendizaje;49xx;apprentissage;cerebro;feedback;spiking neurons;spiking neuron;cerveau;regle apprentissage;separacion senal;methode optimisation;analyse composante independante;curso agua;optimization;separation source;information bottleneck;cours eau;reseau neuronal;boucle reaction;analisis componente independiente;retroalimentacion;source separation;prediction;independent component;extraccion informacion;red neuronal;computacion neuronal;fuente informacion;neural network;stream	Independent component analysis (or blind source separation) is assumed to be an essential component of sensory processing in the brain and could provide a less redundant representation about the external world. Another powerful processing strategy is the optimization of internal representations according to the information bottleneck method. This method would allow extracting preferentially those components from high-dimensional sensory input streams that are related to other information sources, such as internal predictions or proprioceptive feedback. However, there exists a lack of models that could explain how spiking neurons could learn to execute either of these two processing strategies. We show in this article how stochastically spiking neurons with refractoriness could in principle learn in an unsupervised manner to carry out both information bottleneck optimization and the extraction of independent components. We derive suitable learning rules, which extend the well-known BCM rule, from abstract information optimization principles. These rules will simultaneously keep the firing rate of the neuron within a biologically realistic range.	assumed;blind signal separation;independent component analysis;information bottleneck method;mathematical optimization;neuron;neurons;rule (guideline);sensory process;source separation;spiking neural network;unsupervised learning;whole earth 'lectronic link	Stefan Klampfl;Robert A. Legenstein;Wolfgang Maass	2009	Neural Computation	10.1162/neco.2008.01-07-432	independent component analysis;information bottleneck method;prediction;computer science;artificial intelligence;machine learning;feedback;blind signal separation;communication;stream;information extraction;cerebro;artificial neural network;models of neural computation	ML	21.41234697495423	-70.47918393832809	13970
0e2bece1b23ea332c6c2b6bc0a698cf7cb4e6c16	3-d object recognition using adaptive scale megi	object recognition;coarsest representation 3d object recognition adaptive scale megi adaptive matching shapes representation matching score;object recognition shape position measurement data mining equations pressing computational modeling computer simulation humans	We propose a method for recognition of a 3-D object using multi scale description of the object and adaptive matching. MEGI model is a description model to represent arbitrary shapes. However, many MEGI elements are necessary to represent uneven or curved surfaces with accuracy, so it is di cult to use them for recognition. As a solution, we make a tree which corresponds multi scale description of the object. While tracing the tree from the root which corresponds the coarsest representation to leafs, a matching algorithm presented in this paper assigns a di erent scale to each part of the object adaptively and estimates the matching score effectively.	algorithm;computation;data structure;outline of object recognition;robustness (computer science);tree (data structure)	Hiroshi Matsuo;Junichiro Funabashi;Akira Iwata	1996		10.1109/ICPR.1996.547245	computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;3d single-object recognition	Vision	41.47532189785922	-58.23346742263714	14005
0762537106f210ee0c1c4358674a92dc4e28d469	intelligent computing for automated biometrics, criminal and forensic applications	criminal practice;intelligent computing;novel method;ear image;face recognition;automated biometrics;computer vision;forensic application;computer vision system;human identification;cases human identification biometrics;human recognition system	criminal practice;intelligent computing;novel method;ear image;face recognition;automated biometrics;computer vision;forensic application;computer vision system;human identification;cases human identification biometrics;human recognition system	biometrics	Michal Choras	2007		10.1007/978-3-540-74171-8_119	computer vision;computer security	Crypto	30.774731795082957	-62.724748524274915	14018
1ff3ae37f0a3b80670be602503f09852129b48fd	color space analysis of mutual illumination	histograms;object recognition;vision ordenador;shape from shading;image processing;illumination mutuelle;color space;reflectivity;interreflection;singular value decomposition;image;color histogram;indexing terms;computer vision;vision couleur;rgb triples;geometrical information;shape;histogramme couleur;image color analysis;light intensity;pixel;rgb triples color space analysis mutual illumination image light intensity interreflection singular value decomposition geometrical information;color space analysis;lighting computer vision shape pixel light sources image color analysis object recognition reflectivity singular value decomposition histograms;diffuse reflectance;vision ordinateur;lighting;mutual illumination;interreflexion;light sources;color vision	Mutual illumination occurs when light reflected from one surface impinges on a second one. The resulting additional illumination incident on the second surface affects both the color and intensity of the light reflected from it. As a consequence, the image of a surface in the presence of mutual illumination differs from what it otherwise would have been in the absence of mutual illumination. Unaccounted for mutual illumination can easily confuse methods that rely on intensity or color such as shape-from-shading or color-based object recognition. In this correspondence, we introduce an algorithm that removes mutual illumination effects from images. The domain is that of previously-segmented images of convex surfaces of uniform color and diffuse reflectance where for each surface the interreflection occurs mainly from one other surface and can he accurately accounted for within a one-bounce model. The algorithm is based on a singular value decomposition of the colors coming from each surface. Geometrical information about where on the surface the colors emanate from is not required. The RGB triples from a single convex surface experiencing interreflection fall in a plane; intersecting the planes generated from two interreflecting surfaces results in a unique interreflection color. Each pixel can then he factored into its interreflection and no-interreflection components so that a complete no-interreflection image is produced.	algorithm;color space;global illumination;illumination (image);mutual information;outline of object recognition;photometric stereo;pixel;shading;singular value decomposition	Brian V. Funt;Mark S. Drew	1993	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.250838	color histogram;computer vision;index term;photometric stereo;image processing;shape;computer science;cognitive neuroscience of visual object recognition;image;lighting;histogram;mathematics;reflectivity;color vision;photon mapping;color space;singular value decomposition;pixel;diffuse reflection;computer graphics (images)	Vision	53.205995315466	-56.15025239102627	14027
f8660a282e5fbff5a04c56da68dbe5dfd5263452	large baseline matching of scale invariant features	vision ordenador;decomposition valeur singuliere;singular value decomposition;computer vision;invariante;vision ordinateur;decomposicion valor singular;invariant;invariant feature	The problem of feature points matching between pair of views of the scene is one of the key problems in computer vision, because of the number of applications. In this paper we discuss an alternative version of an SVD matching algorithm earlier proposed in the literature. In the version proposed the original algorithm has been modified for coping with large baselines. The claim of improved performances for larger baselines is supported by experimental evidence.	baseline (configuration management)	Elisabetta Delponte;Francesco Isgrò;Francesca Odone;Alessandro Verri	2005		10.1007/11553595_97	computer vision;machine learning;invariant;pattern recognition;mathematics;singular value decomposition	Vision	46.167623409442335	-58.801729715986944	14028
26b15e3927133b1d3543036a443ef69cbf75ad3a	characterisation and automatic detection of lymph nodes on mr colorectal images		Abstract: Colorectal cancer is the second most common cause of death in Western countries. It is often curable by chemoradiotherapy and/or surgery; however, accurate staging has a significant impact on patient management and outcome. Numerous clinical reports attest to the fact that staging is not currently satisfactory, and so more precise methods are required for effective treatment. The three major components of disease staging are tumour size; whether or not there is distal metastatic spread; and the extent of lymph node involvement. Of these, the latter is currently by far the hardest to quantify, and it is the subject of this paper. Lymph nodes are distributed throughout the mesorectal fascia that envelops the colorectum. In practice, they are detected and assessed by clinicians using properties such as their size and shape. We are not aware of any previous image analysis approach for colorectal images that makes this subjective approach more scientific. To aid precise staging and surgery, we have developed methods that characterises lymph nodes by extracting implicit properties as computed from magnetic resonance colorectal images. We first learn the probability density function (PDF) of the intensities of the mesorectal fascia and find that it closely approximates a Gaussian distribution. The parameters of a Gaussian, fitted to the PDF, were estimated and the mean intensity of a lymph node candidate was compared with it. The fitting provides an explicit criterion for a region to be classed as a lymph node: namely, it is an outlier of the Gaussian distribution. As a key part of this process, we need to segment the boundaries of the mesorectal fascia, which is enclosed by two closed contours. Clinicians recognise the outer contour as thin edges. Since the thin edges are often ambiguous and disconnected, differentiating them from neighbouring tissues is a non-trivial problem; the surrounding tissues have no significant difference from the mesorectal fascia in both intensity and texture. We employed a level set method to segment three sets of objects: the mesorectal fascia, the colorectum, and lymph node candidates. Our segmentation results led us to build a PDF and to use it for the criterion that we propose. The whole process of implementation of our methods is automatic including the lookup of lymph candidates. The results of clinical cases are summarised in the paper.	disk staging;image analysis;lookup table;palmar aponeurosis;portable document format;resonance	Jeong-Gyoo Kim;Michael Brady	2008			acceleration;control theory;artificial intelligence;computer vision;computer science;motor drive;control theory	Vision	38.79163110911125	-80.58995948902756	14063
23b8c961463a5b78d3c16207e522b77c150bd3a8	propagation of electrical field in the brain using electrical intra-cerebral stimulations	deep brain stimulation;medical disorders boundary elements methods electroencephalography;boundary element method;electric stimulation;brain models electrodes power harmonic filters frequency measurement electroencephalography harmonic analysis;numerical method;dipole model electrical field propagation electrical intra cerebral stimulations drug resistant partial epilepsy intracerebral electrical stimulation deep brain stimulation epileptic volume source localization problem current density distribution in vivo electrical stimulation propagation model skull stereo eeg data dipolar model infinite homogeneous medium numerical method boundary element method bem;source localization;brain models;electric field;frequency measurement;medical disorders;electrodes;power harmonic filters;electroencephalography;drug resistance;dipole moment;adult brain deep brain stimulation electrodes electroencephalography female humans male;boundary elements methods;current density;harmonic analysis	For drug resistant partial epilepsy, intra-cerebral electrical stimulation (Deep Brain Stimulation — DBS) constitutes one of the means to locate epileptic volume. This paper investigates, in the framework of source localization problem, the propagation of the electrical field and current density distribution induced in the brain during in vivo electrical stimulation. There are three objectives in this work: to validate the propagation model for different large frequencies, to highlight the problem of the close field with the DBS source and to show the influence of the proximity to the skull on the results. We compared the Stereo-EEG data, recorded during DBS, with those obtained using: (i) the simplest model, the dipolar model in an infinite homogeneous medium, (ii) a more realistic approach with a numerical method, the Boundary Element Method (BEM). Studies on ten subjects with 234 stimulations showed that the dipole model could be used in the brain far from the skull in direction of dipole moment but that BEM was more appropriate close to the skull.	boundary element method;deep brain stimulation;direct-broadcast satellite;electric stimulation technique;electroencephalography;epilepsy;epilepsy, partial, motor;functional electrical stimulation;numerical method;software propagation;video-in video-out;dipole moment	Janis Hofmanis;Valérie Louis-Dorr;Thierry Cecchin;Olivier Caspary;Laurent Koessler	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090966	electronic engineering;boundary element method;neuroscience;drug resistance;electroencephalography;numerical analysis;engineering;electrode;electric field;harmonic analysis;biological engineering;physics;current density;quantum mechanics	Robotics	22.08626882077847	-83.93117392205261	14071
ea152c83ecaa2d1e7feb6c7cff22243828ec7663	a second level neural network trigger in the h1 experiment at hera	neural network	At the HERA e-p collider the expected machine background rates are typically 105 times higher than the rates from physics. The greatest challenge in the trigger is finding methods which suppress the machine background without using lengthy pattern recognition algorithms with prohibitive computing times. This task is optimally suited to a neural network solution. Our present investigations show that feed-forward networks, trained on the topological energy sums from the H1 calorimeter and on first level tracking trigger information, provide an additional background suppression factor compared to the traditional method which operates with fixed thresholds. The neural network algorithm — implemented by special purpose fast matrix-vector multiplier chips at the second level of the trigger — allows the lowering of the first level thresholds which is shown to be important to get good efficiencies for specific physics event classes.	artificial neural network	Jürgen Fent;Andrea Gruber;Christian Kiesling;Horst Oberlack;P. Ribarics	1992	Int. J. Neural Syst.	10.1142/S0129065792000620	real-time computing;simulation;computer science;artificial intelligence;machine learning;artificial neural network	NLP	18.52175745276599	-64.60057068964824	14079
df219a13011f0aa8dcee4923156f8a64aba91762	modeling protective anti-tumor immunity via preventative cancer vaccines using a hybrid agent-based and delay differential equation approach	cell cycle and cell division;t lymphocytes cytotoxic;cancer vaccines;antigen presenting cells;models immunological;antigens;antigens neoplasm;humans;differential equations;neoplasms;computational biology;lymph nodes;cell death;immune response;memory	A next generation approach to cancer envisions developing preventative vaccinations to stimulate a person's immune cells, particularly cytotoxic T lymphocytes (CTLs), to eliminate incipient tumors before clinical detection. The purpose of our study is to quantitatively assess whether such an approach would be feasible, and if so, how many anti-cancer CTLs would have to be primed against tumor antigen to provide significant protection. To understand the relevant dynamics, we develop a two-compartment model of tumor-immune interactions at the tumor site and the draining lymph node. We model interactions at the tumor site using an agent-based model (ABM) and dynamics in the lymph node using a system of delay differential equations (DDEs). We combine the models into a hybrid ABM-DDE system and investigate dynamics over a wide range of parameters, including cell proliferation rates, tumor antigenicity, CTL recruitment times, and initial memory CTL populations. Our results indicate that an anti-cancer memory CTL pool of 3% or less can successfully eradicate a tumor population over a wide range of model parameters, implying that a vaccination approach is feasible. In addition, sensitivity analysis of our model reveals conditions that will result in rapid tumor destruction, oscillation, and polynomial rather than exponential decline in the tumor population due to tumor geometry.	agent-based model;anatomic node;anatomical compartments;biomarkers, tumor;cell proliferation;cytotoxic t-lymphocytes;dynamic data exchange;helper-inducer t-lymphocyte;interaction;lymph node tissue;memory disorders;multi-compartment model;neoplasms;polynomial;pool (computer science);population;speech delay;stage iv ovarian carcinoma;time complexity;tumor antigens;vaccination;autologous epstein-barr virus-specific cytotoxic t lymphocytes (cell);lymph nodes	Peter S. Kim;Peter P. Lee	2012		10.1371/journal.pcbi.1002742	programmed cell death;biology;antigen-presenting cell;immune system;virology;antigen;immunology;memory;differential equation	Metrics	8.446639233311357	-68.47592517155368	14159
c3cbcc5d26a49a84c41db1c6f2fdca000a32cad3	validation of elastic registration algorithms based on adaptive irregular grids for medical applications	computed tomography;magnetism;clinical application;quality measurement;adaptive irregular grids;attenuation correction;signal attenuation;positron emission tomography;medical image registration;mr imaging;irregular grid;medical image;magnetic resonance;region of interest;algorithms;validation;ground truth;medical application;quality measures;rigid registration;pulmonary embolism;elastic transformations	Elastic registration of medical images, i.e. finding a non-affine transformation such that corresponding image structures correctly align, is an active field of current research. Registration algorithms have to be validated in order to show that they fulfill the requirements of a particular clinical application. Furthermore, validation strategies compare the performance of different registration algorithms and can hence judge which algorithm is best suited to meet the requirements of a target application. In the literature, validation strategies for rigid registration algorithms have been analyzed. For a known ground truth they assess the displacement error at a few isolated landmarks. This approach is not sufficient for elastic transformations described by a huge number of parameters and the non-linear inter-landmark behaviour. Hence we consider the displacement error averaged over all pixels in the whole image or in a region-of-interest of clinical relevance. Using artificially, but realistically deformed images of the application domain, we use this quality measure to analyze an elastic registration based on transformations defined on adaptive irregular grids for the following clinical applications: Magnetic Resonance (MR) images of freely moving joints for orthopedic investigations, thoracic Computed Tomography (CT) images for the detection of pulmonary embolisms, and transmission images as used for the attenuation correction and registration of independently acquired Positron Emission Tomography (PET) and CT images. The definition of a region-of-interest allows to restrict the analysis of the registration accuracy to clinically relevant image areas. The behaviour of the displacement error as a function of the number of transformation control points and their placement can be used for identifying the best strategy for the initial placement of the control points allowing the same registration accuracy to be achieved with significantly less control points than using a regular control point arrangement.	algorithm;elastic matching	Astrid Franz;Ingwer C. Carlsen;Steffen Renisch;Hans-Aloys Wischmann	2006		10.1117/12.650577	computer vision;geography;image registration;biological engineering;medical physics	Vision	41.00979245972501	-82.66691297465117	14161
097ef03f042e31e6c0f4640ff490c981c912f492	parcellation of infant surface atlas using developmental trajectories of multidimensional cortical attributes	parcellation;infant;local gyrification;atlas;surface area	Cortical surface atlases, equipped with anatomically and functionally defined parcellations, are of fundamental importance in neuroimaging studies. Typically, parcellations of surface atlases are derived based on the sulcal-gyral landmarks, which are extremely variable across individuals and poorly matched with microstructural and functional boundaries. Cortical developmental trajectories in infants reflect underlying changes of microstructures, which essentially determines the molecular organization and functional principles of the cortex, thus allowing better definition of developmentally, microstructurally, and functionally distinct regions, compared to conventional sulcal-gyral landmarks. Accordingly, a parcellation of infant cortical surface atlas was proposed, based on the developmental trajectories of cortical thickness in infants, revealing regional patterning of cortical growth. However, cortical anatomy is jointly characterized by biologically-distinct, multidimensional cortical attributes, i.e., cortical thickness, surface area, and local gyrification, each with its distinct genetic underpinning, cellular mechanism, and developmental trajectories. To date, the parcellations based on the development of surface area and local gyrification is still missing. To bridge this critical gap, for the first time, we parcellate an infant cortical surface atlas into distinct regions based solely on developmental trajectories of surface area and local gyrification, respectively. For each cortical attribute, we first nonlinearly fuse the subject-specific similarity matrices of vertices' developmental trajectories of all subjects into a single matrix, which helps better capture common and complementary information of the population than the conventional method of simple averaging of all subjects' matrices. Then, we perform spectral clustering based on this fused matrix. We have applied our method to parcellate an infant surface atlas using the developmental trajectories of surface area and local gyrification from 35 healthy infants, each with up to 7 time points in the first two postnatal years, revealing biologically more meaningful growth patterning than the conventional method.	anatomic structures;atlases;cervical atlas;cluster analysis;fuse device component;infant;neuroimaging;nonlinear system;spectral clustering;thickness (graph theory);statistical cluster	Gang Li;Li Wang;John H. Gilmore;Weili Lin;Dinggang Shen	2015	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-24574-4_65	atlas;surface area;mathematics	Visualization	22.929519791489547	-77.91387544142576	14177
6d304592eb0dfdec5a9e770cb94316acc454d879	modeling sequence and function similarity between proteins for protein functional annotation	health research;uk clinical guidelines;biological patents;protein function;sequence similarity;europe pubmed central;citation search;biostatistics;uk phd theses thesis;life sciences;sequence alignment;multiple sequence alignment;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	"""A common task in biological research is to predict function for proteins by comparing sequences between proteins of known and unknown function. This is often done using pair-wise sequence alignment algorithms (e.g. BLAST). A problem with this approach is the assumption of a simple equivalence between a minimum sequence similarity threshold and the function similarity between proteins. This assumption is based on the binary concept of homology in that proteins are or not homologous. The relationship between sequence and function however is more complex as well as pertinent for predicting protein function, e.g. evaluating BLAST alignments or developing training sets for profile models based on functional rather than homologous groupings. Our motivation for this study was to model sequence and function similarity between proteins to gain insights into the """"sequence-function similarity relationship between proteins for predicting function. Using our model we found that function similarity generally increases with sequence similarity but with a high degree of variability. This result has implications for pair-wise approaches in that it appears sequence similarity must be very high to ensure high function similarity. Profile models which enable higher sensitivity are a potential solution. However, multiple sequences alignments (a necessary prerequisite) are a problem in that current algorithms have difficulty aligning sequences with very low sequence similarity, which is common in our data set, or are intractable for high numbers of sequences. Given the importance of predicting protein function and the need for multiple sequence alignments, algorithms for accomplishing this task should be further refined and developed."""	algorithm;annotation;blast;homologous gene;protein function prediction;relevance;sequence alignment;spatial variability;turing completeness	Roger Higdon;Brenton Louie;Eugene Kolker	2010	Proceedings of the ... International Symposium on High Performance Distributed Computing	10.1145/1851476.1851548	multiple sequence alignment;computer science;bioinformatics;data science;sequence analysis;sequence alignment;data mining;biostatistics;structural classification of proteins database	Comp.	4.4533322896661724	-55.38891817706381	14178
9a9aae77a062b6a6212ca70a614184c5b8b79889	self-reported well-being score modelling and prediction: proof-of-concept of an approach based on linear dynamic systems		Assessment and recognition of perceived well-being has wide applications in the development of assistive healthcare systems for people with physical and mental disorders. In practical data collection, these systems need to be less intrusive, and respect users' autonomy and willingness as much as possible. As a result, self-reported data are not necessarily available at all times. Conventional classifiers, which usually require feature vectors of a prefixed dimension, are not well suited for this problem. To address the issue of non-uniformly sampled measurements, in this study we propose a method for the modelling and prediction of self-reported well-being scores based on a linear dynamic system. Within the model, we formulate different features as observations, making predictions even in the presence of inconsistent and irregular data. We evaluate the proposed method with synthetic data, as well as real data from two patients diagnosed with cancer. In the latter, self-reported scores from three well-being-related scales were collected over a period of approximately 60 days. Prompted each day, the patients had the choice whether to respond or not. Results show that the proposed model is able to track and predict the patients' perceived well-being dynamics despite the irregularly sampled data.		Xinyang Li;Riccardo Poli;Gaetano Valenza;Enzo Pasquale Scilingo;Luca Citi	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037292	computer science;data collection;well-being;dynamical system;proof of concept;machine learning;feature vector;signal processing;artificial intelligence;synthetic data	Visualization	4.23853538501928	-81.87917619645057	14215
81f96d82cceb9954b0c50eb6113b538e3d0c458c	dominant frequencies of resting human brain activity as measured by the electrocorticogram	female;brain waves;male;rest;brain mapping;adult;biological clocks;cerebral cortex;humans;electroencephalography	The brain's spontaneous, intrinsic activity is increasingly being shown to reveal brain function, delineate large scale brain networks, and diagnose brain disorders. One of the most studied and clinically utilized types of intrinsic brain activity are oscillations in the electrocorticogram (ECoG), a relatively localized measure of cortical synaptic activity. Here we objectively characterize the types of ECoG oscillations commonly observed over particular cortical areas when an individual is awake and immobile with eyes closed, using a surface-based cortical atlas and cluster analysis. Both methods show that [1] there is generally substantial variability in the dominant frequencies of cortical regions and substantial overlap in dominant frequencies across the areas sampled (primarily lateral central, temporal, and frontal areas), [2] theta (4-8 Hz) is the most dominant type of oscillation in the areas sampled with a mode around 7 Hz, [3] alpha (8-13 Hz) is largely limited to parietal and occipital regions, and [4] beta (13-30 Hz) is prominent peri-Rolandically, over the middle frontal gyrus, and the pars opercularis. In addition, the cluster analysis revealed seven types of ECoG spectral power densities (SPDs). Six of these have peaks at 3, 5, 7 (narrow), 7 (broad), 10, and 17 Hz, while the remaining cluster is broadly distributed with less pronounced peaks at 8, 19, and 42 Hz. These categories largely corroborate conventional sub-gamma frequency band distinctions (delta, theta, alpha, and beta) and suggest multiple sub-types of theta. Finally, we note that gamma/high gamma activity (30+ Hz) was at times prominently observed, but was too infrequent and variable across individuals to be reliably characterized. These results should help identify abnormal patterns of ECoG oscillations, inform the interpretation of EEG/MEG intrinsic activity, and provide insight into the functions of these different oscillations and the networks that produce them. Specifically, our results support theories of the importance of theta oscillations in general cortical function, suggest that alpha activity is primarily related to sensory processing/attention, and demonstrate that beta networks extend far beyond primary sensorimotor regions.	brain diseases;categories;cervical atlas;cluster analysis;east pole–west pole divide;electrocorticogram;electroencephalography;eye;frequency band;frontal lobe gyrus;gamma correction;hertz (hz);large scale brain networks;lateral computing;lateral thinking;magnetoencephalography;middle frontal gyrus structure;neural oscillation;occipital lobe;pdf/a;parietal lobe;precondition;rest;sampling - surgical action;sensory process;spatial variability;spontaneous order;structure of frontal pole;synaptic package manager;theory;density	David M. Groppe;Stephan Bickel;Corey J. Keller;Sanjay K. Jain;Sean T. Hwang;Cynthia Harden;Ashesh D. Mehta	2013	NeuroImage	10.1016/j.neuroimage.2013.04.044	psychology;neuroscience;developmental psychology;electroencephalography;rest;brain mapping;communication	ML	19.725245982694794	-77.18323077641605	14228
ce99475c3ee85da967a4e9b60e3f83fd53c969f8	exploiting sparsity for image-based object surface anomaly detection	industrial surface defect detection image based object surface anomaly detection quality control industrial processes manufacturing processes image reconstruction sparse representation image pixel sparse approximation;quality control and automated inspection;quality control and automated inspection parameter optimization sparse reconstruction anomaly detection defect detection;anomaly detection;dictionaries image reconstruction training quality control inspection optimization metals;sparse reconstruction;quality control image recognition image reconstruction;parameter optimization;defect detection	The anomaly detection task plays an important role in quality control in many industrial or manufacturing processes. However, in many such processes, anomaly detection is done visually by human experts who have in-depth knowledge and vast experience on a product in order to perform well in the detection task. In this paper, we present an approach that (i) identifies anomalies in an image based on the sparse residuals (or errors) obtained during image reconstruction using sparse representation and (ii) learns the threshold to classify an image pixel based on its residual value. The intuitions for our proposed sparse approximation driven approach are, namely: (i) anomalies are infrequent and (ii) anomalies are unwanted portions of an image reconstruction. Empirical results on a real-world image dataset for an industrial surface defect detection task are used to demonstrate the feasibility of our proposed approach.	anomaly detection;iterative reconstruction;pixel;software bug;sparse approximation;sparse matrix;world file	Woon Huei Chai;Shen-Shyang Ho;Chi-Keong Goh	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472024	computer vision;anomaly detection;feature detection;computer science;machine learning;pattern recognition	Robotics	48.29009352598344	-66.50015834479886	14237
882df5c14316f09dc67255c17daac6081b258165	utilizing multiscale local binary pattern for content-based image retrieval		With the development of different image capturing devices, huge amount of complex images are being produced everyday. Easy access to such images requires proper arrangement and indexing of images which is a challenging task. The field of Content-Based Image Retrieval (CBIR) deals with finding solutions to such problems. This paper proposes a CBIR technique through multiscale Local Binary Pattern (LBP). Instead of considering consecutive neighbourhood pixels, Local Binary Pattern of different combinations of eight neighbourhood pixels is computed at multiple scales. The final feature vector is constructed through Gray Level Co-occurrence Matrix (GLCM). Advantage of the proposed multiscale LBP scheme is that it overcomes the limitations of single scale LBP and acts as more robust feature descriptor. It efficiently captures large scale dominant features of some textures which single scale LBP fails to do and also overcomes some of the limitations of other multiscale LBP techniques. Performance of the proposed technique is tested on five benchmark datasets, namely, Corel-1K, Olivia-2688, Corel-5K, Corel-10K, and GHIM-10K and measured in terms of precision and recall. The experimental results demonstrate that the proposed method outperforms other multiscale LBP techniques as well as some of the other state-of-the-art CBIR methods.	benchmark (computing);binary pattern (image generation);co-occurrence matrix;content-based image retrieval;feature vector;local binary patterns;pixel;precision and recall;visual descriptor	Prashant Srivastava;Ashish Khare	2017	Multimedia Tools and Applications	10.1007/s11042-017-4894-4	computer vision;pixel;computer science;precision and recall;artificial intelligence;local binary patterns;search engine indexing;content-based image retrieval;pattern recognition;image retrieval;feature vector;neighbourhood (mathematics)	Vision	37.00670143388895	-59.49697015034507	14243
f81311ce9bb3ea8f478f22e608f501b33258e1a2	evaluating image processing algorithms that predict regions of interest	algorithm comparisons;human performance;bottom up;image processing;regions of visual interest;context free;eye movements;region of interest;eye movement	Several bottom-up, context-free, algorithms for the detection of regions of interest in pictures were analyzed, evaluated and compared. Our aim is to develop new criteria related to human performance for these algorithms and perhaps to be able to design more biologically plausible perceptive machines. We introduce the statistical and computational platform we have been using to compare sequences of regions of interest, both biological (eye movements) and arti®cial (algorithms). Ó 1998 Elsevier Science B.V. All rights reserved.	algorithm;bottom-up parsing;computation;context-free grammar;context-free language;human reliability;image processing;region of interest;top-down and bottom-up design	Claudio M. Privitera;Lawrence W. Stark	1998	Pattern Recognition Letters	10.1016/S0167-8655(98)00077-4	computer vision;simulation;image processing;computer science;eye movement;computer graphics (images)	AI	41.40298447107006	-65.345710056655	14264
7e331137084f8873744092574b9dffe9da415dd8	use of the fractal dimension for the analysis of electroencephalographic time series	non linear analysis;real time;time series;phase space;fractal dimension;temporal resolution;spectral method;time domain;electroencephalogram	 Electroencephalogram (EEG) traces corresponding to different physiopathological conditions can be characterized by their fractal dimension, which is a measure of the signal complexity. Generally this dimension is evaluated in the phase space by means of the attractor dimension or other correlated parameters. Nevertheless, to obtain reliable values, long duration intervals are needed and consequently only long-term events can be analysed; also much calculation time is required. To analyse events of brief duration in real-time mode and to apply the results obtained directly in the time domain, thus providing an easier interpretation of fractal dimension behaviour, in this work we optimize and propose a new method for evaluating the fractal dimension. Moreover, we study the robustness of this evaluation in the presence of white or line noises and compare the results with those obtained with conventional spectral methods. The non-linear analysis carried out allows us to investigate relevant EEG events shorter than those detectable by means of other linear and non-linear techniques, thus achieving a better temporal resolution. An interesting link between the spectral distribution and the fractal dimension value is also pointed out.	ablepharon;alpha rhythm;electroencephalography;epilepsy;fast fourier transform;fourier analysis;fractal analysis;fractal dimension;frequency band;gain;increment;license;noise (electronics);nonlinear system;normal statistical distribution;real-time clock;rest;seizures;sensor;spectral density;spectral method;time series;tracing (software)	P. Agostino Accardo;M. Affinito;M. Carrozzi;F. Bouquet	1997	Biological Cybernetics	10.1007/s004220050394	multifractal system;correlation dimension;mathematical analysis;fractal analysis;time domain;temporal resolution;phase space;calculus;time series;mathematics;fractal dimension;quantum mechanics;statistics;spectral method	ML	20.226894430601597	-86.28162697995782	14288
dfaac57551214fe44f336bc675b3f4d3b86f9eb2	small camera movements as a means of reducing the amount of broken and false detected lines in hough transform	real time;line detection;hough transform	Abstract   This letter shows how one of the most unwanted problems of Hough transforms for straight line detection, namely the breaking and replication of lines, can be easily overcome by introducing small camera movements. The reported technique is particularly suitable for application in systems capable of on-line, real-time Hough transform processing. Experimental results are presented that substantiate the effectiveness of the proposed approach.	hough transform	Luciano da Fontoura Costa	1996	Real-Time Imaging	10.1006/rtim.1996.0018	hough transform;computer vision;computer science;computer graphics (images)	Vision	39.540850691819124	-65.78445551437453	14299
65ec22b474c8ab6e65ea4ed8d77f2a089e3471f0	torsion-mediated interaction between adjacent genes	dna;health research;uk clinical guidelines;dna transcription;biological patents;europe pubmed central;polymerases;gene regulation;citation search;invertebrate genomics;gene expression;prokaryotic cells;uk phd theses thesis;drosophila melanogaster;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	DNA torsional stress is generated by virtually all biomolecular processes involving the double helix, in particular transcription where a significant level of stress propagates over several kilobases. If another promoter is located in this range, this stress may strongly modify its opening properties, and hence facilitate or hinder its transcription. This mechanism implies that transcribed genes distant of a few kilobases are not independent, but coupled by torsional stress, an effect for which we propose the first quantitative and systematic model. In contrast to previously proposed mechanisms of transcriptional interference, the suggested coupling is not mediated by the transcription machineries, but results from the universal mechanical features of the double-helix. The model shows that the effect likely affects prokaryotes as well as eukaryotes, but with different consequences owing to their different basal levels of torsion. It also depends crucially on the relative orientation of the genes, enhancing the expression of eukaryotic divergent pairs while reducing that of prokaryotic convergent ones. To test the in vivo influence of the torsional coupling, we analyze the expression of isolated gene pairs in the Drosophila melanogaster genome. Their orientation and distance dependence is fully consistent with the model, suggesting that torsional gene coupling may constitute a widespread mechanism of (co)regulation in eukaryotes.	basal (phylogenetics);interference (communication);prokaryote;torsion (gastropod);transcription (software);transcription, genetic;video-in video-out	Sam Meyer;Guillaume Beslon	2014		10.1371/journal.pcbi.1003785	biology;regulation of gene expression;gene expression;bioinformatics;transcription;genetics;dna;polymerase	Comp.	6.359384451158914	-63.43154134263128	14300
22e08218c9de21efc3e56cc9b2dca8d6419d2ccf	a web-based system to search for and view genes with sequence variations among geographically distributed populations			population	Teruyoshi Hishiki;Takuro Tamura	2012			web application;gene;bioinformatics;biology	Networks	-0.02622086483603398	-62.905581828726916	14307
78b89a75f2712d50b8e4f190a290e3a194c5264e	robust skin color segmentation using a 2d plane of rgb color space	analisis componente principal;piel;illumination;color space;peau;color constancy;skin color detection;skin;color histogram;intelligence quotient;skin color;histogram;color segmentation;histogramme;distribution temporelle;cociente intelectual;principal component analysis;analyse composante principale;invariante;espace chromatique;espacio cromatico;quotient intellectuel;imagen color;histograma;eclairement;image couleur;invariant;distribucion temporal;color image;alumbrado;time distribution	This research features a new method for skin color segmentation using a 2D plane in the RGB color space. The RGB color values of the input color image do not need to be converted into HSI or YIQ color coordinates that have popularly been used for color segmentation. We have observed an important fact that skin colors in the RGB color space are approximately distributed in a linear fashion. Based on this fact, we have applied PCA (Principal Component Analysis) techniques to RGB values of skin colors from a set of training images. We detect skin regions by the lookup of skin color histogram computed based on a 2D color plane of which two axes correspond to two directions with smallest spread of skin colors. The proposed 2D color plane for color histogram lookup has an advantage over HS or IQ color planes. By using this plane, the problem of color constancy is much relieved. A learned color histogram contains most skin colors detected in the input images and at the same time, the distribution of skin colors in the plane is invariant compared to those in the HS or IQ planes. We have evaluated the performance of the proposed method by comparing with the performance of color histogram lookup methods based on HS or IQ color plane. The experimental results show that the performance of our method is robust to illumination changes.	color depth;color histogram;color image;color space;horizontal situation indicator;image segmentation;lookup table;principal component analysis	Juneho Yi;Jiyoung Park;Jongsun Kim;Jongmoo Choi	2003		10.1007/978-3-540-39737-3_52	color histogram;rgb color model;computer vision;icc profile;color model;color quantization;hsl and hsv;3d lookup table;intelligence quotient;lightness;color normalization;color depth;color image;computer science;rgb color space;invariant;high color;histogram;skin;color balance;color space;color constancy;spectral color;principal component analysis;computer graphics (images)	Vision	44.25556452164845	-60.78467259880005	14328
a7805e39439e72f5132773668f9ccf2fa8f2b52c	visuotactile learning and body representation: an erp study with rubber hands and rubber objects	touch;analysis of variance;attentional bias;proprioception;visual perception;vibration	We studied how the integration of seen and felt tactile stimulation modulates somatosensory processing, and investigated whether visuotactile integration depends on temporal contiguity of stimulation, and its coherence with a preexisting body representation. During training, participants viewed a rubber hand or a rubber object that was tapped either synchronously with stimulation of their own hand, or in an uncorrelated fashion. In a subsequent test phase, somatosensory event-related potentials (ERPs) were recorded to tactile stimulation of the left or right hand, to assess how tactile processing was affected by previous visuotactile experience during training. An enhanced somatosensory N140 component was elicited after synchronous, compared with uncorrelated, visuotactile training, irrespective of whether participants viewed a rubber hand or rubber object. This early effect of visuotactile integration on somatosensory processing is interpreted as a candidate electro-physiological correlate of the rubber hand illusion that is determined by temporal contiguity, but not by preexisting body representations. ERP modulations were observed beyond 200 msec poststimulus, suggesting an attentional bias induced by visuotactile training. These late modulations were absent when the stimulation of a rubber hand and the participant's own hand was uncorrelated during training, suggesting that preexisting body representations may affect later stages of tactile processing.	erp;homophobia;illusions	Clare Press;Cecilia Heyes;Patrick Haggard;Martin Eimer	2008	Journal of Cognitive Neuroscience	10.1162/jocn.2008.20022	psychology;neuroscience;developmental psychology;analysis of variance;visual perception;vibration;proprioception;communication;social psychology;attentional bias	HCI	15.96651461771384	-77.12264623198368	14331
c1d2256540b929d3c6e240e62f509bd2332249a0	research of determination method of starch and protein content in buckwheat by mid-infrared spectroscopy	bp neural network;buckwheat;mid-infrared spectroscopy;pca;protein content;starch content	Starch and proteins are the vital nutrients in buckwheat, to achieve the fast detection of the buckwheat internal composition has an important theoretical significance and application value for buckwheat breeding, processing and other steps. In the paper, forty buckwheat samples from different origins have been selected. The starch and protein content of buckwheat was determined, and the mid-infrared transmission spectrum of buckwheat has been obtained using Fourier mid-infrared spectroscopy. Forty samples were randomly divided into the prediction set and validation set with 30 and 10 samples respectively. After smoothing preprocessing, the prediction models of buckwheat starch and protein content have been established using the combination method of principal component analysis and artificial neural network, finally the models have been verified. The results showed that the correlation coefficient between the prediction value and measurement value of buckwheat starch content is 0.9029, and the relative error is smaller and its mean value is 2.33%, the method of the buckwheat starch content prediction is feasible. But the prediction for buckwheat protein content is not ideal, need to be further studied.	approximation error;artificial neural network;coefficient;cooperative breeding;preprocessor;principal component analysis;randomness;smoothing	Fenghua Wang;Ju Yang;Hailong Zhu;Zhiyong Xi	2012		10.1007/978-3-642-36137-1_30	starch;fourier transform;principal component analysis;smoothing;mid infrared spectroscopy;spectroscopy;correlation coefficient;approximation error;biological system;mathematics	Comp.	14.693808910095461	-56.255149448976475	14337
de46d541b47989cbe1f78026e5ab11de2ea17c55	computer assisted detection of liver neoplasm (cadln)	liver;treatment planning computer assisted detection liver neoplasm radiologists neoplasm images image processing modules multifunctional image processing algorithm computer generated volumetric data serial imaging neoplasm growth progression neoplasm growth regression computerised tomography;tumours;algorithms cluster analysis humans image processing computer assisted liver neoplasms radiography abdominal tomography x ray computed;medical image processing;image registration;computerised tomography;liver computed tomography image segmentation tumors cancer filtering neoplasms;tumours computerised tomography image registration liver medical image processing	To date, radiologists evaluate neoplasm images manually. Currently there is wide spread attention for developing image processing modules to detect and measure early stage neoplasm growth in liver. We report the fundamentals associated with the development of a multifunctional image processing algorithm useful to measure early growth of neoplasm and the volume of liver. Using CADLN, a radiologist will be able to compare computer generated volumetric data in serial imaging of the patients over time, that eventually will enable assessing progression or regression of neoplasm growth and help in treatment planning.	algorithm;color gradient;image processing;limited stage (cancer stage);liver neoplasms;multi-function printer;patients;radiology	Shrinivas Bhosale;Ashish Aphale;Isaac G. Macwan;Miad Faezipour;Priya Bhosale;Prabir Patra	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346228	computer vision;radiology;medicine;pathology;computer science;image registration;medical physics	Robotics	35.64995941910536	-79.7066745134276	14341
1ed2aea64ce962c768ffae16f2c2530cd90e4d6f	a novel methodology for assessing the bounded-input bounded-output instability in qt interval dynamics: application to clinical ecg with ventricular tachycardia	female;ecg;history;middle aged;bounded input bounded output bibo stability;male;ventricular tachycardia vt;rail to rail inputs;electrocardiography rail to rail inputs mathematical model history stability analysis equations poles and zeros;medical disorders;qt interval qti;signal processing computer assisted;poles and zeros;premature activation frequency bounded input bounded output instability qt interval dynamics clinical ecg ventricular tachycardia bibo stability minecg;electrocardiography;medical disorders electrocardiography;stability analysis;mathematical model;algorithms;action potentials aged algorithms electrocardiography female humans male middle aged models cardiovascular signal processing computer assisted tachycardia ventricular;humans;action potentials;tachycardia ventricular;ventricular tachycardia vt bounded input bounded output bibo stability ecg qt interval qti repolarization dynamics;models cardiovascular;repolarization dynamics;aged	The goal of this paper is to present a new methodology for assessing the bounded-input bounded-output (BIBO) stability in QT interval (QTI) dynamics from clinical ECG. The ECG recordings were collected from 15 patients who experienced ventricular tachycardia (VT). Ten-minute-long ECG recordings extracted immediately before the onset of a chosen VT, one per patient, were assembled into a VT group, while the control group comprised 10-min-long ECGs extracted 1 h before VT onset and at least 1 h after any prior arrhythmic event. Each 10-min recording was subdivided into 1-min ECG recordings (minECGs). The QTI dynamics of each minECG was defined as a function of several prior QTIs and RR intervals; the BIBO stability of this function was then assessed in the z-domain. The number of minECGs with unstable QTI dynamics (Nus) and the frequency of premature activations (PA), fPA, were counted for each ECG recording and were compared between the VT and control groups. The results show that the present methodology successfully captured the instability in QTI dynamics leading to VT onset in the studied population. Significantly larger Nus was found in the VT group compared against the control and a positive correlation between Nus and fPA was identified in both groups.	atrial premature complexes;bibo stability;control theory;extraction;implantable cardioverter-defibrillator;instability;large;onset (audio);patients;qt interval feature (observable entity);qti;qt (software);rapid refresh;tachycardia;tachycardia, ventricular;unstable medical device problem	Xiaozhong Chen;Natalia A. Trayanova	2012	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2011.2170837	pole–zero plot;electronic engineering;von neumann stability analysis;neuroscience;engineering;mathematical model;control theory;mathematics;action potential;statistics;cardiology	Visualization	18.607467930934973	-85.39636385193218	14343
888aedb19f59b0276df8ebc06c5402c2ebd143d2	madmx: a strategy for maximal dense motif extraction	motif discovery;motifs extraction;algorithms	"""We develop, analyze, and experiment with a new tool, called MADMX, which extracts frequent motifs from biological sequences. We introduce the notion of density to single out the """"significant"""" motifs. The density is a simple and flexible measure for bounding the number of don't cares in a motif, defined as the fraction of solid (i.e., different from don't care) characters in the motif. A maximal dense motif has density above a certain threshold, and any further specialization of a don't care symbol in it or any extension of its boundaries decreases its number of occurrences in the input sequence. By extracting only maximal dense motifs, MADMX reduces the output size and improves performance, while enhancing the quality of the discoveries. The efficiency of our approach relies on a newly defined combining operation, dubbed fusion, which allows for the construction of maximal dense motifs in a bottom-up fashion, while avoiding the generation of nonmaximal ones. We provide experimental evidence of the efficiency and the quality of the motifs returned by MADMX."""	bottom-up parsing;don't-care term;maximal set;partial template specialization;programming tool;sequence motif	Roberto Grossi;Andrea Pietracaprina;Nadia Pisanti;Geppino Pucci;Eli Upfal;Fabio Vandin	2011	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2010.0177	computer science;bioinformatics;machine learning;data mining;mathematics	Comp.	-0.10007640976543085	-54.17813716950652	14355
c172f90d014e245c5a46db1866b18d7f5bd8345a	arterial input function: relevance of eleven analytical models in dce-mri studies	tissues;haemodynamics;multiexponential gamma function;gamma function;magnetic resonance image;microcirculatory parameters;dynamic contrast enhanced;q factor dynamic contrast enhanced mri arterial input function mathematical modeling multiexponential gamma function microcirculatory parameters tissues biomedical image processing magnetic resonance imaging;medical image processing;signal processing;arterial input function;magnetic resonance imaging;analytical models testing protocols magnetic resonance imaging biomedical measurements delay mathematical model biomedical imaging motion measurement volume measurement;mathematical model;biomedical image processing;dynamic contrast enhanced mri;signal processing biomedical image processing magnetic resonance imaging modeling q factor;mathematical modeling;q factor biomedical mri blood vessels haemodynamics medical image processing physiological models;modeling;physiological models;blood vessels;analytical model;q factor;biomedical mri	"""The assessment of the micro circulatory parameters in tissues by dynamic contrast enhanced MRI (DCE- MRI) requires the knowledge of the arterial input function (AIF). In several situations the estimation of the true AIF can be improved by a mathematical modeling. However, the mathematical modeling can introduce wrong a priori and generate artifacts. It is then necessary to verify its coherence. Several models are proposed in the literature to fit the AIF. In this study, several models were tested. The test consisted in the assessment of the ability of the models to describe a large range of experimental AIF, extracted of a DCE-MRI multicentric study. Simple models, such as exponential or gamma variate functions were not able to correctly describe all the measured AIF. Conversely, the AIF data were correctly described by multi-phase models and especially by an original function based on a """"mutli-exponential gamma"""" function."""	contrast ratio;gamma correction;mathematical model;relevance;time complexity	Daniel Balvay;Yannick Ponvianne;Michel Claudon;Charles-André Cuenod	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4541067	radiology;medicine;computer science;magnetic resonance imaging;mathematical model;mathematics;nuclear magnetic resonance;medical physics	Arch	44.76610654737018	-85.10754497732688	14386
23ebbb1b00a72a2cbff2ba5eb7c1ec5b3283a8b6	an initial performance evaluation of unsupervised learning with alias	multiprocessor interconnection networks;unsupervised learning;three layer hierarchical network;performance evaluation;image processing;supervised learning;neural nets;rotational invariance;unsupervised learning alias adaptive learning image analysis system anomalous features automatic classification collective learning systems theory learning cells massively parallel networks nonlearning cells parallel processing image processing engine performance evaluation rotational invariance scale invariant detection three layer hierarchical network translation;hierarchical networks;alias;learning cells;learning systems;nonlearning cells;learning system;rotation invariance;performance evaluation computerised picture processing learning systems multiprocessor interconnection networks neural nets;translation;scale invariant detection;parallel processing image processing engine;adaptive learning;collective learning systems theory;computerised picture processing;image analysis;automatic classification;anomalous features;parallel processing;scale invariance;massively parallel networks;adaptive learning image analysis system	Based on collective learning systems theory and a versatile general-purpose architecture for massively parallel networks of processors, a transputer-based parallel-processing image-processing engine comprising a three-layer hierarchical network of 32 learning cells and 33 nonlearning cells has been applied to a difficult image-processing task: the detection of anomalous features in otherwise normal images. Known as ALIAS (adaptive learning image analysis system), this engine is currently being constructed and tested. ALIAS is limited to the translation and scale-invariant detection of anomalies. Future enhancements will include rotational invariance as well as the automatic classification of images. An experiment with unsupervised learning indicates excellent detection of anomalies which are square sections of the image shifted horizontally or vertically with respect to the original image. Supervised learning, to be implemented in the near future, will allow ALIAS to be conditioned to accept or reject specific anomalous features (either normal or abnormal), as appropriate	performance evaluation;unsupervised learning	Peter Bock;H. Holz;Richard M. Rovner;C. J. Kocinski	1990		10.1109/IJCNN.1990.137606	translation;unsupervised learning;parallel processing;computer vision;image analysis;image processing;rotational invariance;computer science;theoretical computer science;machine learning;scale invariance;supervised learning;adaptive learning;alias;artificial neural network	Vision	23.548655430607997	-62.75676767611842	14387
ed92d76d7ff91a91b6a3a2d0bb31c8da4b40221b	design of microfluidic biochips (dagstuhl seminar 15352)	cyber physical integration microfluidic biochip computer aided design hardware and software co design test verification;004	Advances in microfluidic technologies have led to the emergence of biochip devices for automating laboratory procedures in biochemistry and molecular biology. Corresponding systems are revolutionizing a diverse range of applications, e.g.~air quality studies, point-of-care clinical diagnostics, drug discovery, and DNA sequencing -- with an increasing market. However, this continued growth depends on advances in chip integration and design-automation tools. Thus, there is a need to deliver the same level of Computer-Aided Design (CAD) support to the biochip designer that the semiconductor industry now takes for granted. The goal of the seminar was to bring together experts in order to present and to develop new ideas and concepts for design automation algorithms and tools for microfluidic biochips. #R##N#This report documents the program and the outcomes of this endeavor.		Krishnendu Chakrabarty;Tsung-Yi Ho;Robert Wille	2015	Dagstuhl Reports	10.4230/DagRep.5.8.34	engineering;nanotechnology;biological engineering	HCI	1.9516038752392173	-68.56487678170089	14393
909d4457d21378fb3fcdb5a05227cf48615b9b99	vocal fold pathology detection using modified wavelet-like features and support vector machines	acoustic signal processing;diseases;feature extraction;learning (artificial intelligence);medical signal processing;patient diagnosis;signal classification;support vector machines;wavelet transforms;svm;acoustic analysis;acoustic speech signal;classification system;input audio data;machine learning methods;neural networks;pseudowavelets;vocal pathology diagnostic method;wavelet-like transform;pathology;speech	Acoustic analysis is a perspective vocal pathology diagnostic method that can complement (and in some cases replace) other methods, based on direct vocal fold observation. There are different approaches and algorithms for feature extraction from acoustic speech signal and for making decision on their basis. While the second stage implies a choice of a variety of machine learning methods (SVMs, neural networks, etc), the first stage plays crucial part in performance and accuracy of the classification system, providing much more creativity in development of different feature extraction methods. In this paper we present initial study of feature extraction based on wavelets and pseudo-wavelets in the task of vocal pathology diagnostic. A new type of feature vector, based on continuous wavelet and wavelet-like transform of input audio data is proposed. Support vector machine was used as a classifier for testing the feature extraction procedure. The results of our experimental study are shown.	acoustic cryptanalysis;algorithm;artificial neural network;coefficient;continuous wavelet;experiment;feature extraction;feature vector;machine learning;multiclass classification;support vector machine;wavelet transform	Peter D. Kukharchik;D. Martynov;Igor E. Kheidorov;O. Kotov	2007	2007 15th European Signal Processing Conference		speech recognition;feature vector;feature extraction;computer science;machine learning;pattern recognition;feature	ML	16.684742251847435	-90.37478456937387	14411
39954484ea3bc111f0e6371329f76675dc14fb69	transcription factor and microrna-regulated network motifs for cancer and signal transduction networks	biological patents;simulation and modeling;biomedical journals;text mining;europe pubmed central;systems biology;citation search;physiological cellular and medical topics;citation networks;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Molecular networks are the basis of biological processes. Such networks can be decomposed into smaller modules, also known as network motifs. These motifs show interesting dynamical behaviors, in which co-operativity effects between the motif components play a critical role in human diseases. We have developed a motif-searching algorithm, which is able to identify common motif types from the cancer networks and signal transduction networks (STNs). Some of the network motifs are interconnected which can be merged together and form more complex structures, the so-called coupled motif structures (CMS). These structures exhibit mixed dynamical behavior, which may lead biological organisms to perform specific functions. In this study, we integrate transcription factors (TFs), microRNAs (miRNAs), miRNA targets and network motifs information to build the cancer-related TF-miRNA-motif networks (TMMN). This allows us to examine the role of network motifs in cancer formation at different levels of regulation, i.e. transcription initiation (TF → miRNA), gene-gene interaction (CMS), and post-transcriptional regulation (miRNA → target genes). Among the cancer networks and STNs we considered, it is found that there is a substantial amount of crosstalking through motif interconnections, in particular, the crosstalk between prostate cancer network and PI3K-Akt STN. To validate the role of network motifs in cancer formation, several examples are presented which demonstrated the effectiveness of the present approach. A web-based platform has been set up which can be accessed at: http://ppi.bioinfo.asia.edu.tw/pathway/ . It is very likely that our results can supply very specific CMS missing information for certain cancer types, it is an indispensable tool for cancer biology research.	crosstalk;medical transcription;merge;micrornas;motif;preparation;prostatic neoplasms;search algorithm;signal transduction;small;social network;streptonigrin;super-twisted nematic display;transcription factor;topic maps;transcription (software);transcription initiation;transcription, genetic;transcriptional regulation;transduction (machine learning);web application	Wen-Tsong Hsieh;Ke-Rung Tzeng;Jin-Shuei Ciou;Jeffrey J. P. Tsai;Nilubon Kurubanjerdjit;Chien-Hung Huang;Ka-Lok Ng	2015		10.1186/1752-0509-9-S1-S5	biology;text mining;medical research;computer science;bioinformatics;network motif;data science;data mining;systems biology	Comp.	5.430898207986821	-58.42491731683916	14412
e8ed213cfaf23ac07c1902244250a86422897b20	a robust 3d face recognition algorithm using passive stereo vision	face recognition;phase- only correlation;facial expression;3d face;biometrics;iterative closest point;stereo vision	The recognition performance of the conventional 3D face recognition algorithm using ICP (Iterative Closest Point) is degraded for the 3D face data with expression changes. Addressing this problem, we consider the use of the expression-invariant local regions of a face. We find the expression-invariant regions through the distance analysis between 3D face data with the neutral expression and smile, and propose a robust 3D face recognition algorithm using passive stereo vision. We demonstrate efficient recognition performance of the proposed algorithm compared with the conventional ICP-based algorithm through the experiment using a stereo face image database which includes the face images with expression changes. key words: biometrics, face recognition, 3D face, facial expression, phaseonly correlation	algorithm;biometrics;facial recognition system;iterative closest point;iterative method;stereopsis;three-dimensional face recognition	Akihiro Hayasaka;Koichi Ito;Takafumi Aoki;Hiroshi Nakajima;Koji Kobayashi	2009	IEICE Transactions		facial recognition system;computer vision;face detection;object-class detection;computer science;stereopsis;machine learning;pattern recognition;three-dimensional face recognition;facial expression;iterative closest point;biometrics	Vision	42.556921148041	-52.875204710505834	14419
9b9dd818a2f292b95274d380d24b9ac3f2d578b1	dental classification for periapical radiograph based on multiple fuzzy attribute	dental classification;dentistry;dental image;personal identification system dental classification periapical radiograph multiple fuzzy attribute classification method dental image speculative classification periapical dental radiographs university of indonesia;image segmentation;multiple fuzzy attribute;integral projection;personal identification system;personal identification;image classification;radiography;fuzzy logic;accuracy;periapical radiograph;university of indonesia;feature extraction;medical image processing;integral projection dental classification fuzzy inference feature extraction personal identification;radiography dentistry image classification medical image processing;fuzzy inference;classification method;teeth;periapical dental radiographs;teeth radiography dentistry accuracy feature extraction image segmentation fuzzy logic;speculative classification	Dental classification for periapical radiograph based on multiple fuzzy attribute is proposed, where each tooth is analyzed based on multiple criteria such as area/perimeter ratio and width/height ratio. A classification method on special type of dental image called periapical radiograph is studied and classification is done without speculative classification (in case of ambiguous object), therefore an accurate and assistive result can be obtained due to its capability to handle ambiguous tooth. Experiment results on 78 periapical dental radiographs from University of Indonesia indicates 82.51% total classification accuracy and 84.29% average classification rate per input radiograph. The proposed classification method is planned to be implemented as a submodule for an under developing dental based personal identification system.	perimeter;radiography;speculative execution	Martin Leonard Tangel;Chastine Fatichah;Fei Yan;Janet Pomares Betancourt;Muhammad Rahmat Widyanto;Fangyan Dong;Kaoru Hirota	2013	2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)	10.1109/IFSA-NAFIPS.2013.6608417	computer vision;pathology;dentistry	Vision	35.41774574416542	-75.58392875607997	14427
3ce92cac0f3694be2f2918bf122679c6664a1e16	deep relative attributes	visualization;learning systems;feature extraction;neural networks;measurement;machine learning;object recognition	Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn increasingly complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a deep neural network architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) is adopted to learn the features by including an additional layer (ranking layer) that learns to rank the images based on these features. We adopt an appropriate ranking loss to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various coarse and fine-grained datasets. Our qualitative results along with the visualization of the saliency maps show that the network is able to learn effective features for each specific attribute. Source code of the proposed method is available at https://github.com/yassersouri/ghiaseddin.	artificial neural network;baseline (configuration management);computer;convolutional neural network;deep learning;end-to-end principle;feature learning;map;network architecture;ranking (information retrieval)	Yaser Souri;Erfan Noury;Ehsan Adeli-Mosabbeb	2016		10.1007/978-3-319-54193-8_8	computer vision;computer science;machine learning;pattern recognition;data mining	AI	23.548615288092247	-52.99154424495451	14432
b5cf931cf0bd606575bc793c0c8ec6d913d08bc6	geometric primitive feature extraction - concepts, algorithms, and applications	drntu engineering computer science and engineering computing methodologies pattern recognition;drntu engineering computer science and engineering computing methodologies image processing and computer vision;thesis	This thesis presents important insights and concepts related to the topic of the extraction of geometric primitives from the edge contours of digital images. Three specific problems related to this topic have been studied, viz., polygonal approximation of digital curves, tangent estimation of digital curves, and ellipse fitting anddetection from digital curves. For the problem of polygonal approximation, two fundamental problems have been addressed. First, the nature of the performance evaluation metrics in relation to the local and global fitting characteristics has been studied. Second, an explicit error bound of the error introduced by digitizing a continuous line segment has been derived and used to propose a generic non-heuristic parameter independent framework which can be used in several dominant point detection methods. For the problem of tangent estimation for digital curves, a simple method of tangent estimation has been proposed. It is shown that the method has a definite upper bound of the error for conic digital curves. It has been shown that the method performs better than almost all (seventy two) existing tangent estimation methods for conic as well as several non-conic digital curves. For the problem of fitting ellipses on digital curves, a geometric distance minimization model has been considered. An unconstrained, linear, non-iterative, and numerically stable ellipse fitting method has been proposed and it has been shown that the proposed method has better selectivity for elliptic digital curves (high true positive and low false positive) as compared to several other ellipse fitting methods. For the problem of detecting ellipses in a set of digital curves, several innovative and fast pre-processing, grouping, and hypotheses evaluation concepts applicable for digital curves have been proposed and combined to form an ellipse detection method.	algorithm;approximation;curve fitting;digital image;euclidean distance;feature extraction;geometric primitive;heuristic;iterative method;numerical analysis;numerical stability;performance evaluation;preprocessor;selectivity (electronic);sensor;viz: the computer game	Dilip K. Prasad	2013	CoRR		computer vision;mathematical optimization;geometric design;computer science;theoretical computer science;machine learning;mathematics;geometry;statistics	Vision	49.459981716370464	-62.35408837030203	14433
c48155d7a22b36b4e954e03fc8607828d4e3b299	generative adversarial network for visualizing convolutional network		The convolutional neural network (CNN) is one of the most powerful models that has been achieving state-of-the art performance on a variety of computer vision tasks. However, their models are often considered as black-boxes and their lack of interpretability are considered to be a major problem. For example, in applications where the high interpretability is important, getting the predictions is not enough and we also need understanding of why these predictions are made. In that sense, understanding how their models work can build trust with users and make them more human-oriented. In this paper, we introduce a new visualization framework based on generative adversarial network (GAN) to give an insight into the neuron activities and what the CNN has learned. Our method is very simple, yet can produce recognizable and interpretable visualizations. Applied our method to AlexNet, these visualizations help to understand how the CNN works internally.	artificial neural network;black box;computer vision;convolutional neural network;neuron	Masayuki Kobayashi;Masanori Suganuma;Tomoharu Nagao	2017	2017 IEEE 10th International Workshop on Computational Intelligence and Applications (IWCIA)	10.1109/IWCIA.2017.8203577	machine learning;generative grammar;visualization;convolutional neural network;computer science;computer vision;adversarial system;interpretability;artificial intelligence	Vision	22.086547466591792	-53.27647232238107	14435
2cd8ac1da70dea447d2339b99786d02490e3b1fb	a variational framework for joint image registration, denoising and edge detection	edge detection;piecewise smooth;non rigid image registration;image registration;image denoising	In this paper we propose a new symmetrical framework that solves image denoising, edge detection and non–rigid image registration simultaneously. This framework is based on the Ambrosio–Tortorelli approximation of the Mumford–Shah model. The optimization of a global functional leads to decomposing the image into a piecewise–smooth representative, which is the denoised intensity function, and a phase field, which is the approximation of the edge-set. At the same time, the method seeks to register two images based on the segmentation results. The key idea is that the edge set of one image should be transformed to match the edge set of the other. The symmetric non–rigid transformations are estimated simultaneously in two directions. One consistency functional is designed to constrain each transformation to be the inverse of the other. The optimization process is guided by a generalized gradient flow to guarantee smooth relaxation. A multi–scale implementation scheme is applied to ensure the efficiency of the algorithm. We have performed preliminary medical evaluation on T1 and T2 MRI data, where the experiments show encouraging results.	algorithm;approximation;edge detection;experiment;gradient;image registration;linear programming relaxation;mathematical optimization;noise reduction;point process;variational principle	Jingfeng Han;Benjamin Berkels;Martin Rumpf;Joachim Hornegger;Marc Droske;Michael Fried;Jasmin Scorzin;Carlo Schaller	2006		10.1007/3-540-32137-3_50	computer vision;mathematical optimization;feature detection;edge detection;image gradient;pattern recognition;non-local means	Vision	51.692462521574825	-72.04242395202724	14449
7558a7ce53b785d21eef9318f11c0dbab4f9c0f4	deep convolutional neural networks for facial expression recognition		Facial expression recognition is a very active research topic due to its potential applications in the many fields such as human-robot interaction, human-machine interfaces, driving safety, and health-care. Despite of the significant improvements, facial expression recognition is still a challenging problem that wait for more and more accurate algorithms. This article presents a new model that is capable of recognizing facial expression by using deep Convolutional Neural Network (CNN). The CNN model is generated by using Caffe in Digits environment. Moreover, it is trained and tested on NVIDIA Tegra TX1 embedded development platform including a 250 Graphics Processing Unit (GPU) CUDA cores and Quadcore ARM Cortex A57 processor. The proposed model is applied to address the facial expression problem on the publicly available two expression databases, the JAFFE database and the Cohn-Kanade database.	convolutional neural network	Aysegül Uçar	2017		10.1109/INISTA.2017.8001188	caffè;convolutional neural network;graphics processing unit;feature extraction;facial recognition system;three-dimensional face recognition;facial expression;cuda;speech recognition;computer science	Vision	24.669410353176936	-59.84739673283184	14497
ed0cb81cdaac397c7d15ba2a600ad37a9abee18a	investigating cancer-related proteins specific domain interactions and differential protein interactions caused by alternative splicing	splicing;alternative splicing;liver;tumor suppressor;cancer;domain domain interactions;tumours;statistical significance;protein protein interactions;genetics;medical computing;gene expression;onco protein;sensitivity;internet;tumor suppressor protein;proteins;indexation;molecular biophysics;liver cancer;protein protein interaction;web based platforms cancer related proteins specific domain interactions differential protein interactions disease proteins protein protein interaction data multiple functional domains domain information biomedical problems domain domain interaction model oncoproteins tumor suppressor proteins cross validation testing f1 measurement alternative splicing mechanism domain removal effects liver cancer related isoforms gene onotology gene expression level jaccard index;tumors;diseases;cross validation;protein interaction;proteins cancer liver splicing tumors diseases sensitivity;physiological models;liver cancer protein protein interactions domain domain interactions tumor suppressor protein onco protein alternative splicing;biomedical measurement;biochemistry;tumours biochemistry biomedical measurement cancer genetics internet liver medical computing molecular biophysics physiological models proteins	A strategy to gain a better understanding of disease proteins is to consider the interactions of these proteins by making use of the protein-protein interaction (PPI) data. It is known that proteins are composed of multiple functional domains. In this article, domain information is introduced to study two biomedical problems. For the first problem, a one to one domain-domain interaction (DDI) model is proposed to obtain specific sets of DDI for onco-proteins and tumor suppressor proteins respectively. Three specific sets of DDI, i.e. oncoprotein and oncoprotein, tumor suppressor protein and tumor suppressor protein, and oncoprotein and tumor suppressor protein, are derived from their PPI. Cross-validation test is conducted to benchmark the prediction sensitivity, specificity and F1 measure. It is found that the oncoprotein and cancer protein DDI set achieved a 74% and 84% F1 measure respectively. This indicates the feasibility of applying DDI model for cancer protein PPI studies, which can contribute to biomedical study Secondly, it is suggested that PPI, which is mediated by DDI, may be affected due to domain removal through the alternative splicing mechanism. Domain removal effects on liver cancer isoforms¡¦ PPI are studied. It is found that certain liver cancer-related isoforms mediated differential PPI. Information are integrated from three pieces of information, i.e. Gene Onotology (GO), gene expression level and PubMed, to provide supporting details for the findings. The Jaccard index is used to rank statistical significant DDI with the above combinations. Finally, web-based platforms have been set up to display the results of the two studied biomedical problems, i.e. http://ppi.bioinfo.asia.edu.tw/TsgOcgppi/ and http://ppi.bioinfo.asia.edu.tw/hccas/search.	acceptance testing;benchmark (computing);f1 score;gene ontology;interaction;jaccard index;pixel density;pubmed;sensitivity and specificity;web application	Yu-Liang Lee;Jie-Wei Weng;Wen-Chin Chiang;Jason Yao-Tsu Lin;Ka-Lok Ng;Jeffrey J. P. Tsai;Chi-Ying F. Huang	2011	2011 IEEE 11th International Conference on Bioinformatics and Bioengineering	10.1109/BIBE.2011.12	protein–protein interaction;biology;molecular biology;bioinformatics;genetics;molecular biophysics	Comp.	8.015932521905997	-59.996429742869566	14503
6af5af2fbe9725c4a255dc836c8df751ee44b9a1	a simple and scalable receiver model in molecular communication systems	markovian model;scalable simulation;diffusion	This paper shows a simple although reliable receiver model for diffusion-based molecular communication systems. Indeed, the complexity of molecular communications system, involving a massive number of interacting entities, makes scalability a fundamental property of simulators and modeling tools. A sample scenario is that of targeted drug delivery systems, which makes use of biological nanomachines close to a biological target, able to release molecules in a diseased area. The proposed model tackles the time needed for analyzing such a system by the introduction of an equivalent markovian queuing model, which reproduces the aggregate behavior of thousands of receptors spread over the receiver surface. Our results demonstrate that the proposed approach substantially matches simulation results achieved through detailed simulations of a large number of receivers by means of BiNS2 simulator, although the time taken for obtaining the results is order of magnitudes lower than the simulation time. We believe that this model is the precursor of novel models based on similar principles that allow realizing reliable simulations of body-wide systems.	aggregate data;entity;interaction;nanorobotics;queueing theory;scalability;simulation	Luca Felicetti;Mauro Femminella;Gianluca Reali	2016		10.1145/2967446.2967475	simulation;computer science;theoretical computer science;distributed computing	HPC	4.049218441049569	-68.8500485973986	14512
24cbc8195bf8f86502f0158b0dff6d3ce3042bc5	content-based obscene video recognition by combining 3d spatiotemporal and motion-based features	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	In this article, a new method for the recognition of obscene video contents is presented. In the proposed algorithm, different episodes of a video file starting by key frames are classified independently by using the proposed features. We present three novel sets of features for the classification of video episodes, including (1) features based on the information of single video frames, (2) features based on 3D spatiotemporal volume (STV), and (3) features based on motion and periodicity characteristics. Furthermore, we propose the connected components’ relation tree to find the spatiotemporal relationship between the connected components in consecutive frames for suitable features extraction. To divide an input video into video episodes, a new key frame extraction algorithm is utilized, which combines color histogram of the frames with the entropy of motion vectors. We compare the results of the proposed algorithm with those of other methods. The results reveal that the proposed algorithm increases the recognition rate by more than 9.34% in comparison with existing methods.	algorithm;color histogram;connected component (graph theory);digital video;key frame;list of sega arcade system boards;quasiperiodicity	Alireza Behrad;Mehdi Salehpour;Meraj Ghaderian;Mahmoud Saiedi;Mahdi Nasrollah Barati	2012	EURASIP J. Image and Video Processing	10.1186/1687-5281-2012-23	video compression picture types;reference frame;computer vision;computer science;archaeology;video tracking;pattern recognition;block-matching algorithm;multimedia;motion compensation;biometrics	Vision	39.0387352025157	-52.39943251097912	14524
6a4fbda241d374468e4f9a1af638ce6d59c7cedc	hierarchical chunking during learning of visuomotor sequences	laboratories neuroscience computational intelligence delay automobiles manuals physiology cognitive science psychiatry biomedical imaging;sequence learning;visuomotor sequence learning hierarchical chunking;visual cues;learning artificial intelligence	It is well known that learning a sequential skill involves chaining a number of primitive actions together into chunks. We describe three different experiments using an explicit visuomotor sequence learning paradigm called the m times n task. The m times n task enables hierarchical learning of sequences by presenting m elements of the sequence at a time (called the set). The entire sequence to be learned is composed of n such sets and is called a hyperset. In the first experiment, we showed the chunking phenomenon while learning a sequence as opposed to following randomly generated visual cues. We further explored the nature of chunking across sets using complex sequences in the second experiment. Finally, we investigated effector dependence of the chunking patterns in the third experiment. Our results point out the facilitating factors for chunk formation in visuomotor sequence learning.	experiment;procedural generation;programming paradigm;response time (technology);shallow parsing	Krishna P. Miyapuram;Raju S. Bapi;Chandrasekhar V. S. Pammi;Ahmed;Kenji Doya	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246688	sequence learning;sensory cue;computer science;artificial intelligence;machine learning;chunking	Vision	18.5548576863926	-66.1720102006049	14525
830af27fb6ccdc06517ae591ca50c8c63912851a	revealing the binding process of new 3-alkylpyridine marine alkaloid analogue antimalarials and the heme group: an experimental and theoretical investigation		Synthetic 3-alkylpyridine marine alkaloid (3-APA) analogues have shown good antimalarial activity against Plasmodium falciparum. However, despite their structural originality, their molecular target was unknown. Herein, we report a proposal for the antimalarial mechanism of action of 3-APA analogues through interference with the process of hemozoin (Hz) formation. The interaction between 3-APA analogues and heme groups was investigated employing an in silico approach and biophysical techniques such as ultraviolet-visible light (UV-vis) titration and electrospray ionization-mass spectrometry (ESI-MS). The in silico approach was performed based on pure ab initio electronic structure methods in order to obtain insights at the molecular level concerning the binding process of antimalarial drugs at their target site, the heme group. In silico results showed that the formation of heme:3-APA complexes at a molecular ratio of 2:1 are more stable than 1:1 complexes. These results were further confirmed by experimental techniques, such as UV-vis and high-resolution mass spectrometry (ESI-TOF), for two of the most active 3-APA analogues.	1:1 pixel mapping;ab initio quantum chemistry methods;alkaloid identified:prid:pt:urine:nom:tlc;alkaloids;antimalarials;british undergraduate degree classification;design of experiments;electronic structure;heme;hertz (hz);image resolution;interference (communication);light, visible;marine ecosystem;plasmodium falciparum;spectrometry;spectrometry, mass, electrospray ionization;synthetic intelligence;titration method;hemozoin	Renato M. Ribeiro-Viana;Anna P. Butera;Eliziane S. Santos;César A. Tischer;Rosemeire B. Alves;Rossimiriam Pereira de Freitas;Luciana Guimaraes;Fernando de Pilla Varotti;Gustavo H. R. Viana;Clebio S. Nascimento	2016	Journal of chemical information and modeling	10.1021/acs.jcim.5b00742	biochemistry;stereochemistry;chemistry;organic chemistry	Comp.	10.081660955876442	-62.69121518464694	14527
6c03972bd9f8685b7d85fa21aa863ed2df124619	an iterative convex hull approach for image segmentation and contour extraction	contour;projection;digital image;convex hull;extraction	The contours and segments of objects in digital images have many important applications. Contour extractions of gray images can be converted into contour extractions of binary images. This paper presents a novel contour-extraction algorithm for binary images and provides a deduction theory for this algorithm. First, we discuss the method used to construct convex hulls of regions of objects. The contour of an object evolves from a convex polygon until the exact boundary is obtained. Second, the projection methods from lines to objects are studied, in which, a polygon iteration method is presented using linear projection. The result of the iteration is the contour of the object region. Lastly, addressing the problem that direct projections probably cannot find correct projection points, an effective discrete ray-projection method is presented. Comparisons with other contour deformation algorithms show that the algorithm in the present paper is very robust with respect to the shapes of the object regions. Numerical tests show that time consumption is primarily concentrated on convex hull computation, and the implementation efficiency of the program can satisfy the requirement of interactive operations.	active contour model;contour line;convex hull;image segmentation;iterative method	Jian Zhao;Jian An	2012	IJPRAI	10.1142/S0218001412550130	computer vision;mathematical optimization;extraction;convex combination;projection;convex hull;dykstra's projection algorithm;mathematics;geometry;convex set;digital image	Vision	48.75782816733846	-63.19716860044452	14529
0b95eb133b79d2ae210283e645915047d06b758f	the edge-driven dual-bootstrap iterative closest point algorithm for multimodal retinal image registration	surgical planning;eye;image processing;bootstrap;procesamiento imagen;traitement image;algorithme;registro imagen;algorithm;recalage image;retina;image registration;image sequence;retine;iterative closest point;iterative closest point algorithm;retinal imaging;diagnostics and therapeutics;structural similarity;diseases and disorders;algoritmo	Red-free (RF) fundus retinal images and fluorescein angiogram (FA) sequence are often captured from an eye for diagnosis and treatment of abnormalities of the retina. With the aid of multimodal image registration, physicians can combine information to make accurate surgical planning and quantitative judgment of the progression of a disease. The goal of our work is to jointly align the RF images with the FA sequence of the same eye in a common reference space. Our work is inspired by Generalized Dual-Bootstrap Iterative Closest Point (GDB-ICP), which is a fully-automatic, feature-based method using structural similarity. GDB-ICP rank-orders Lowe keypoint matches and refines the transformation computed from each keypoint match in succession. Albeit GDB-ICP has been shown robust to image pairs with illumination difference, the performance is not satisfactory for multimodal and some FA pairs which exhibit substantial non-linear illumination changes. Our algorithm, named Edge-Driven DBICP, modifies generation of keypoint matches for initialization by extracting the Lowe keypoints from the gradient magnitude image, and enriching the keypoint descriptor with global-shape context using the edge points. Our dataset consists of 61 randomly selected pathological sequences, each on average having two RF and 13 FA images. There are total of 4985 image pairs, out of which 1323 are multimodal pairs. Edge-Driven DBICP successfully registered 93% of all pairs, and 82% multimodal pairs, whereas GDB-ICP registered 80% and 40%, respectively. Regarding registration of the whole image sequence in a common reference space, Edge-Driven DBICP succeeded in 60 sequences, which is 26% improvement over GDB-ICP.	algorithm;image registration;iterative closest point;iterative method;multimodal interaction	Chia-Ling Tsai;Chun-Yi Li;Gehua Yang	2008		10.1117/12.770840	computer vision;image processing;image registration;structural similarity;optics;iterative closest point	Vision	41.42827466289575	-81.75050003158206	14530
4f2b88450dc2c4e922cc6e7199c9d2020a9c09c4	evaluating the effects of image compression in moiré-pattern-based face-spoofing detection	video coding data compression face recognition;moire pattern based face spoofing detection jpeg image compression h 264 avc frequency domain peak detection digital grids moire patterns image compression degradation automatic spoofing detection face recognition biometric systems;cameras image coding face quantization signal transform coding databases discrete fourier transforms;face recognition biometrics face spoofing detection	Face-recognition biometric systems have been shown unreliable under the presence of face-spoofing images, creating the need for automatic spoofing detection. In this paper, the effect of image compression degradation on face-spoofing detection is evaluated, based on an algorithm that searches for Moiré patterns due to the overlap of the digital grids, through peak detection in the frequency domain. The proposed spoofing-detection algorithm performance is evaluated subject to H.264/AVC and JPEG image compression for an image database of facial shots under several conditions.	algorithm;biometrics;elegant degradation;facial recognition system;h.264/mpeg-4 avc;image compression;jpeg	Diogo C. Garcia;Ricardo L. de Queiroz	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351727	data compression;computer vision;speech recognition;object-class detection;pattern recognition;three-dimensional face recognition	Robotics	43.04899310864664	-59.68914169086896	14578
83fe3deccefac9315cb2d2969419582e93666652	efficient algorithm for automatic road sign recognition and its hardware implementation	detection;classification;recognition;hardware implementation	The automatic detection of road signs is an application that alerts the vehicle’s driver of the presence of signals and invites him to react on time in the aim to avoid potential traffic accidents. This application can thus improve the road safety of persons and vehicles traveling in the road. Several techniques and algorithms allowing automatic detection of road signs are developed and implemented in software and do not allow embedded application. We propose in this work an efficient algorithm and its hardware implementation in an embedded system running in real time. In this paper we propose to implement the application of automatic recognition of road signs in real time by optimizing the techniques used in different phases of the recognition process. The system is implemented in a Virtex4 FPGA family which is connected to a camera mounted in the moving vehicle. The system can be integrated into the dashboard of the vehicle. The performance of the system shows a good compromise between speed and efficiency.	algorithm;artificial neural network;clock rate;embedded system;field-programmable gate array;median filter;serialization;synaptic package manager;synaptic weight;virtex (fpga)	Chokri Souani;Hassene Faiedh;Kamel Besbes	2013	Journal of Real-Time Image Processing	10.1007/s11554-013-0348-z	embedded system;simulation;biological classification;computer security	Robotics	25.627398295862694	-60.86239504015985	14581
52d060dc65d597ecab8971cea694e107e9b5c8d6	significance tests and statistical inequalities for segmentation by region growing on graph	bottom up;medical image;region growing;region merging;decision rule	Bottom-up segmentation methods merge similar neighboring regions according to a decision rule and a merging order. In this paper, we propose a contribution for each of these two points. Firstly, under statistical hypothesis of similarity, we provide an improved decision rule for region merging based on significance tests and the recent statistical inequality of McDiarmid. Secondly, we propose a dynamic merging order based on our merging predicate. This last heuristic is justified by considering an energy minimisation framework. Experimental results on both natural and medical images show the validity of our method.	algorithm;heuristic;medical imaging;region growing;significance arithmetic;social inequality;software framework;sorting;top-down and bottom-up design	Guillaume Née;Stéphanie Jehan-Besson;Luc Brun;Marinette Revenu	2009		10.1007/978-3-642-03767-2_114	computer science;machine learning;pattern recognition;top-down and bottom-up design;data mining;decision rule;mathematics;region growing	Vision	44.88286391563463	-69.3538268487845	14582
6e7abe92ee86b8cb2114138593a2f95aeaa07ebd	shape representation and recognition based on invariant unary and binary relations	object representation;graph theory;representation;object recognition;teoria grafo;algorithm performance;image processing;edge detection;procesamiento imagen;transformacion;pairing;theorie graphe;graph matching;traitement image;reference point;3d object recognition;deteccion contorno;algorithme;barycentric coordinate;algorithm;detection contour;transformation invariance;shape representation;resultado algoritmo;graph representation;attributed relational graph;performance algorithme;pattern recognition;binary relation;invariante;reconnaissance forme;transformation;emparejamiento;reconocimiento patron;appariement;invariant;representacion;algoritmo	The problem of transformation invariant object recognition is considered. We develop a projective transformation invariant representation for both scene and model which facilitates an attributed relational graph object matching based only on unary and binary relations. The unary and binary measurements used for matching are derived from sets of reference points such as corners and bi-tangent points which are stable under the various transformations considered. Each set of reference points is used to generate a distinct barycentric coordinate basis system associated with one node of the object graph representation. We show that barycentric coordinates of the reference image points can be made invariant under any arbitrary projective transformation. The conditions that must hold for a basis to be valid are stated. We illustrate the construction of the barycentric coordinate systems for the affine and perspective transformations. For the object and scene representation we use the barycentric coordinates of the reference points generating the barycentric coordinate system, together with auxiliary measurements such as colour and texture as the node's unary measurements. For binary measurements we use the product of the barycentric coordinate system for one node with the inverse of the barycentric coordinate system associated with another node. The unary and binary relations provide an orthogonal decomposition of the shape being matched. They are used in a relaxation process to detect instances of objects consistent with a given model. We demonstrate the proposed methodology of projective transformation invariant object representation on several examples. First we illustrate the stability of the shape representation in terms of unary relations both visually and numerically. We then experimentally demonstrated the invariance of binary relations on a star-like object. We show experimentally that the binary relations derived are invariant. The final example demonstrates the proposed approach as a tool for 3D object recognition. The aim is to recognize 3D objects in terms of planar faces. A hexagonal model shape is hypothesized in the image. The only instance of the hypothesized model is successfully recovered.	unary operation	Graham A Colditz;Josef Kittler	1999	Image Vision Comput.	10.1016/S0262-8856(98)00131-0	transformation;computer vision;discrete mathematics;edge detection;topology;image processing;computer science;graph theory;cognitive neuroscience of visual object recognition;invariant;pairing;binary relation;unary function;mathematics;geometry;graph;representation;matching	Vision	49.12330579622212	-60.052401113719284	14586
d3cac764ece8ca7c3b98de64cee046ccce6f33b5	comparative evaluation of voxel similarity measures for affine registration of diffusion tensor mr images	log euclidean distance;diffusion tensor images;mr images;image processing;cost function;sum of squared differences;diffusion mode based similarity;diffusion tensor image;voxel similarity;affine registration;statistical significance;euclidean distance;voxel;tensile stress biomedical measurements diffusion tensor imaging hospitals protocols magnetic resonance imaging cost function image registration image processing biomedical engineering;diffusion profiles;medical image processing biomedical mri image registration;mr imaging;medical image processing;image registration;sum of squared difference;local minima;similarity measure;sum of squared differences voxel affine registration diffusion tensor mr images diffusion tensor image voxel similarity euclidean distance log euclidean distance diffusion profiles diffusion mode based similarity;diffusion tensor;biomedical mri	Deriving an accurate cost function for tensor valued data has been one of the main difficulties in diffusion tensor image (DTI) registration. In this work, we evaluate and compare five voxel similarity measures: Euclidean distance (ED), Log-Euclidean distance (LOG), distance based on diffusion profiles (DP), diffusion mode based similarity (MBS), and multichannel version of sum of squared differences (SSD). In evaluation we used an optimization-independent evaluation protocol to assess the capture range, the number of local minima, and cyclic registrations to evaluate consistency. Statistically significant differences were observed: DP and MBS were found to be the most consistent similarity measures, ED had the least number of local minima, and SSD was inferior to other similarity measures in all evaluations.	euclidean distance;image registration;loss function;mathematical optimization;maxima and minima;solid-state drive;voxel	Mika Pollari;Tuomas Neuvonen;Mikko Lilja;Jyrki Lötjönen	2007	2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2007.356965	diffusion mri;computer vision;topology;radiology;image processing;computer science;image registration;maxima and minima;euclidean distance;mathematics;geometry;statistical significance;voxel	Vision	44.38906374790275	-79.03073452095113	14587
40a85cf1b7ec0dcb63c385cb0a63d914bd5a33e2	discrete heat kernel smoothing in irregular image domains		We present the discrete version of heat kernel smoothing on graph data structure. The method is used to smooth data in an irregularly shaped domains in 3D images. New statistical properties of heat kernel smoothing are derived. As an application, we show how to filter out noisy data in the lung blood vessel trees obtained from computed tomography. The method can be further used in representing the complex vessel trees parametrically as a linear combination of basis functions and extracting the skeleton representation of the trees.	basis function;blood vessel tissue;ct scan;data structure;estimated;graph (abstract data type);graph - visual representation;kernel;laplacian matrix;newton;signal-to-noise ratio;smoothing (statistical technique);structure of parenchyma of lung;tomography, emission-computed;trees (plant);united states national institutes of health;tomography	Moo K. Chung;Yanli Wang;Guorong Wu	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513450	skeleton (computer programming);computer vision;kernel (linear algebra);noisy data;graph (abstract data type);heat kernel;linear combination;mathematical optimization;smoothing;basis function;artificial intelligence;computer science	Vision	48.6296311860908	-77.07582377273272	14595
748dcdf5c873173803cd24ec39ba11633aa752a7	in silico white blood cell: mechanisms underlying leukocyte rolling and adhesion during inflammation	white blood cell;biological model;agent based;systems biology;adhesion;inflammation;system biology;leukocyte;in silico	We have used the synthetic modeling method to construct a multilevel, agent oriented, in silico analogue of an in vitro experimental system for studying leukocyte rolling, activation, and adhesion during inflammatory conditions. We specify capabilities that the envisioned analogues must have to achieve long-term goals. Here, we report progress towards our goal of using variants of this model as experimental systems for exploring the potential role of hypothesized mechanisms that are thought to mediate leukocyte rolling, activation, and adhesion. We focus initially on diffusion and clustering events of the LFA-1 integrin receptor on the leukocyte membrane during rolling and adhesion.	cluster analysis;experimental system;synthetic intelligence	Jonathan Tang;C. Anthony Hunt	2008			computer science;adhesion;models of abnormality;systems biology	ML	6.504411500692411	-67.20130097410112	14615
08dd99e0022a7077cd529c3970535b848e3f561d	a predictive network architecture for a robust and smooth robot docking behavior		Robots and living beings exhibit latencies in their sensorimotor processing due to mechanical and electronic or neural processing delays. A reaction typically occurs to input stimuli of the past. This is critical not only when the environment changes (e.g. moving objects) but also when the agent itself moves. An agent that does not predict while moving may need to remain static between sensory input acquisition and output response to guarantee that the response is appropriate to the percept. We propose a biologically-inspired learning model of predictive sensorimotor integration to compensate for this latency. In this model, an Elman network is developed for sensory prediction and sensory filtering; a Continuous Actor-Critic Learning Automaton (CACLA) is trained for continuous action generation. For a robot docking experiment, this architecture improves the smoothness of the robot’s sensory input and therefore results in a faster and more accurate continuous approach behavior. 1 Predictive Visual Processing In the sensorimotor cycle of a robot, there usually exists a temporal delay mainly contributed by the processing time of sensors, transmission time of signals and mechanical latency. For example, because few object recognition programs can recognize the identity of human faces from visual inputs in real time, the running speed of human following behavior of robots based on object recognition should be slower than a normal walking speed of human beings, if it keeps searching a person in image sequences in a short time-scale. A simple predictive mechanism, The original publication is available at www.springerlink.com such as a Kalman Filter, can solve this problem by predicting the movement of a person as soon as he/she has been identified (e.g. [1]). However, since a Kalman Filter is based on a linearity assumption, it does not consider more complicated movements with e.g. non-linear influence from context. Such problems may be tackled by neural networks, which can learn to predict percepts in a general dynamic environment. Sensory prediction is of great benefit to dynamic robot behaviors such as obstacle avoidance, visually guided navigation, reaching, visual search, and rapid decision-making under uncertainty, since these kinds of behavior highly rely on current sensory information. In these scenarios with non-linear dynamics, a developmental sensory prediction is needed to learn to compensate for the latency of the sensorimotor cycle. A second reason for employing predictive mechanisms is that the sensory inputs are often noisy and inaccurate in a real environment, which may lead to failure of robot behaviors. In that case, a predictive sensory module can compare its prediction based on previous short-term sensory percepts to the current sensory value. A noisy sensor value can thereby be identified and an action adjustment executed. A severe case may be due to sensor failure caused by hardware problems or a change of environment (e.g. lighting conditions in cameras). In such cases, an embodied predictive sensory module can act as a filter to recursively estimate the incoming percepts. Mechanisms of prediction have also been found in human perception. For example, in visual motion perception humans keep track of a moving object by observing saliency (i.e. the most occurring features) from the visual processing in the thalamus and visual cortex [2]. This object feature selection, on the other hand, also provides a cue in the early visual cortex to predict upcoming sensory data via recurrent and top-down connections based on the prior noisy sensory information [3, 4]. For instance, the flash-lag effect [5] shows that a perceptual prediction may exist based on visual motion cues and it extrapolates the forthcoming visual information [6]. In a similar manner as for motion, it is proposed that specialized cue detectors, such as colors or shapes, also predictively code the evidence of preferred features according to an internal model [7]. Research shows that prediction in the visual cortex is not hard-wired, but rather a process learned by a flexible system whose contingencies are adapted in different environments [8, 9]. These biological findings suggest that an adaptable predictive sensory module is beneficial for faster and more natural robot behaviors. From a neural modeling point of view, to realize a sensory prediction function and to embody it in a robot, recurrent connectivity is one option due to its capability to store previous dynamical activities and to represent a short-term memory of previously perceived visual information. Recurrent connections – horizontal excitatory and inhibitory connections in neuroanatomy – are found throughout visual areas [10, 11] and they account the neural activity changes during responses to object movements [12]. Furthermore, functions of de-noising and filtering during the unfolding of a spatiotemporal sequence can be performed by the same recurrent connectivity. The fluctuation caused by the sensory error can be efficiently smoothed by recurrent loops, which is consistent with neuroanatomical findings that inhibitory feedback connections suppress neural population-rate fluctuations [13]. These two properties of recurrent connections – prediction and filtering – fulfill our requirements for building up a sensory module that supports robust robot behavior. 2 Sensory Prediction Prediction within an autonomous cognitive robot can happen on various levels, but in this paper we only consider predictive sensory models, i.e. a system predicting sensory signals given the current and previous sensory states. Since prediction of the complete raw sensory percept (e.g. all pixels of the camera image) is not desirable and would be very difficult for an autonomous robot, it is advisable to predict only few features extracted from the sensory percepts [14, 15] as human perception does. This can be implemented as a learnt non-linear mapping of sensory representations to predict the forthcoming sensory flow. In previous research, prediction helped to e.g. determine object affordance [16], to make a robot avoid moving humans [17] or to steer a mobile robot [18, 19]. We have argued previously that any cortical area should compensate for its own processing delay via prediction [20]. Such local predictions may be easier to implement than prediction of the entire system response, and enable the mixed hierarchical and parallel processing in the (visual) cortex, including shortcut connections, while keeping the representations in all areas temporally aligned. Furthermore, prediction of restricted sensory percepts (instead of prediction of the next action) may be generalizable to many contexts and actions, to allow for latent learning, in which knowledge for specific tasks is acquired while other tasks are being performed. A well-known method for tracking and prediction is the Kalman filter [21]. It is built on linear operators and models a Markov chain with Gaussian noise with the assumption of a linear dynamic system. Other improved/adaptive Kalman filters have been proposed [22, 23]. Neural networks can learn universal function	automaton;autonomous robot;cognitive robotics;color;content-control software;docking (molecular);dynamical system;extrapolation;feature selection;humans;kalman filter;keyboard shortcut;long short-term memory;markov chain;mobile robot;network architecture;neural networks;neural ensemble;nonlinear system;obstacle avoidance;outline of object recognition;parallel computing;pixel;processing delay;quantum fluctuation;recurrent neural network;recursion;reinforcement learning;requirement;sensor;sinc function;smoothing;top-down and bottom-up design;unfolding (dsp implementation);utm theorem	Junpei Zhong;Cornelius Weber;Stefan Wermter	2012	Paladyn	10.2478/s13230-013-0106-8	simulation;artificial intelligence	Robotics	18.639983158955236	-65.49879770570529	14623
016068e588851b0ba021fdfc7bd3a9eedf0e595c	co-expression module analysis reveals biological processes, genomic gain, and regulatory mechanisms associated with breast cancer progression	breast neoplasms;female;simulation and modeling;disease progression;systems biology;signal transduction;gene regulatory networks;physiological cellular and medical topics;transcription factors;models biological;gene expression data;relapse free survival;computational biology bioinformatics;gene expression;risk factors;cluster analysis;ppar alpha;transcription factor;gene expression regulation neoplastic;cell proliferation;algorithms;humans;signaling pathway;cell adhesion;gene expression pattern;breast cancer;tumor progression;oligonucleotide array sequence analysis;immune response;biological process;bioinformatics;gene ontology	Gene expression signatures are typically identified by correlating gene expression patterns to a disease phenotype of interest. However, individual gene-based signatures usually suffer from low reproducibility and interpretability. We have developed a novel algorithm Iterative Clique Enumeration (ICE) for identifying relatively independent maximal cliques as co-expression modules and a module-based approach to the analysis of gene expression data. Applying this approach on a public breast cancer dataset identified 19 modules whose expression levels were significantly correlated with tumor grade. The correlations were reproducible for 17 modules in an independent breast cancer dataset, and the reproducibility was considerably higher than that based on individual genes or modules identified by other algorithms. Sixteen out of the 17 modules showed significant enrichment in certain Gene Ontology (GO) categories. Specifically, modules related to cell proliferation and immune response were up-regulated in high-grade tumors while those related to cell adhesion was down-regulated. Further analyses showed that transcription factors NYFB, E2F1/E2F3, NRF1, and ELK1 were responsible for the up-regulation of the cell proliferation modules. IRF family and ETS family proteins were responsible for the up-regulation of the immune response modules. Moreover, inhibition of the PPARA signaling pathway may also play an important role in tumor progression. The module without GO enrichment was found to be associated with a potential genomic gain in 8q21-23 in high-grade tumors. The 17-module signature of breast tumor progression clustered patients into subgroups with significantly different relapse-free survival times. Namely, patients with lower cell proliferation and higher cell adhesion levels had significantly lower risk of recurrence, both for all patients (p = 0.004) and for those with grade 2 tumors (p = 0.017). The ICE algorithm is effective in identifying relatively independent co-expression modules from gene co-expression networks and the module-based approach illustrated in this study provides a robust, interpretable, and mechanistic characterization of transcriptional changes.	8q21;antivirus software;biological processes;categories;cell adhesion;cell proliferation;color gradient;disease phenotype;enterprise test software;gene ontology term enrichment;gene co-expression network;gene expression programming;gene regulatory network;information retrieval facility;iterative reconstruction;iterative method;mammary neoplasms;maximal set;ppara gene;patients;silo (dataset);transcription factor;transcription (software);transcription, genetic;transcriptional activation;tumor progression;algorithm;elk-1 protein;pediatric intracranial germ cell brain tumor	Zhiao Shi;Catherine K. Derow;Bing Zhang	2010		10.1186/1752-0509-4-74	cancer research;biology;bioinformatics;genetics;systems biology;signal transduction;transcription factor	Comp.	6.886129918293201	-55.81775833510744	14633
4855d7e8c7ef857245735d4d42535087fb9e65c3	eyebrow shape analysis by using a modified functional curve procrustes distance	shape contours eyebrow shape analysis modified functional curve procrustes distance automatic recognition frontal face images eyebrow curves cubic splines landmark points multidimensional scaling method;eyebrows shape splines mathematics measurement face recognition face feature extraction;splines mathematics face recognition shape recognition;shape recognition;splines mathematics;face recognition	To tackle the problem of automatic recognition of human eyebrow, a novel approach for shape analysis based on frontal face images is proposed in this paper. First, eyebrow curves are acquired by fitting cubic splines based on landmark points. Next, we propose to use a modified functional curve procrustes distance to measure the similarities among the cubic splines, and finally a multidimensional scaling method is adopted to evaluate the effectiveness of the distance. This work extends previous work in analyzing the eyebrow for both human and machine recognition by providing a framework based on shape contours. Further this work demonstrates the effectiveness of eyebrow shape for discrimination when teamed with the appropriate metric distance.	cubic function;image scaling;intrinsic dimension;landmark point;multidimensional scaling;procrustes analysis;shape analysis (digital geometry);spline (mathematics);statistical classification	Yishi Wang;Cuixian Chen;A. Midori Albert;Yaw Chang;Karl Ricanek	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712741	facial recognition system;computer vision;computer science;pattern recognition;mathematics;geometry	Vision	41.37386056236802	-57.26221080247054	14641
324100db295a01db33542beaad69f44fd60e117c	the protein-small-molecule database, a non-redundant structural resource for the analysis of protein-ligand binding	ligando;proteine liaison;analisis estructural;small molecule;database;ressource;base dato;ligand binding;fixation ligand;ligand;proteina enlace;base de donnees;molecule petite;binding protein;analyse structurale;structural analysis;fijacion ligando;recurso;molecula pequena;resource	MOTIVATION An enabling resource for drug discovery and protein function prediction is a large, accurate and actively maintained collection of protein/small-molecule complex structures. Models of binding are typically constructed from these structural libraries by generalizing the observed interaction patterns. Consequently, the quality of the model is dependent on the quality of the structural library. An ideal library should be non-biased and comprehensive, contain high-resolution structures and be actively maintained.   RESULTS We present a new protein/small-molecule database (the PSMDB) that offers a non-redundant set of holo PDB complexes. The database was designed to allow frequent updates through a fully automated process without manual annotation or filtering. Our method of database construction addresses redundancy at both the protein and the small-molecule level. By efficiently handling structures with covalently bound ligands, we allow our database to include a larger number of structures than previous methods. Multiple versions of the database are available at our web site, including structures of split complexes--the proteins without their binding ligands and the non-covalently bound ligands within their native coordinate frame.   AVAILABILITY http://compbio.cs.toronto.edu/psmdb		Izhar Wallach;Ryan H. Lilien	2009	Bioinformatics	10.1093/bioinformatics/btp035	small molecule;bioinformatics;structural analysis;ligand;protein structure database;ligand;binding protein;resource	Comp.	-3.9968119191362015	-56.06911619105727	14648
18d839ebaf48430102263b6a00a5ab7a47c74c7d	tracking virus particles in fluorescence microscopy images via a particle kalman filter	degradation;insulation life;kalman filters;microscopy;computation time virus particle tracking fluorescence microscopy image sequence particle kalman filter fluorescent particle tracking quantitative characterization dynamical process probabilistic tracking sample generation elliptical approximation gaussian density sample weighting image likelihood image support multidimensional synthetic image data real microscopy image data particle tracking performance accuracy;three dimensional displays;probability approximation theory biomedical optical imaging fluorescence spectroscopy image sequences kalman filters medical image processing microorganisms object tracking optical microscopy particle filtering numerical methods;probabilistic logic;virus particles biomedical imaging microscopy images tracking;image sequences;kalman filters microscopy three dimensional displays image sequences probabilistic logic insulation life degradation	Tracking fluorescent particles in microscopy image sequences is pivotal in obtaining quantitative characterizations of the dynamical processes underlying these fluorescent structures. We have developed a probabilistic tracking approach that combines the Kalman filter with principles of the particle filter. To generate samples, we use an elliptical approximation of a Gaussian density. Each sample is weighted according to an image likelihood and the image support. The performance of our tracking approach has been evaluated using multi-dimensional synthetic as well as real microscopy image data. The approach yields a more accurate performance at very competitive computation times compared to previous probabilistic approaches.	approximation;computation;kalman filter;particle filter;synthetic data	William J. Godinez;Karl Rohr	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7163928	kalman filter;computer vision;degradation;microscopy;mathematics;probabilistic logic	Vision	52.674052535404336	-76.10332381611929	14668
cbf64b8db341b21ce65fd7e1118994e539169961	scene classification by feature co-occurrence matrix	会议论文	Classifying scenes (such as mountains, forests) is not an easy task owing to their variability, ambiguity, and the wide range of illumination and scale conditions that may apply. Bag of features (BoF) model have achieved impressive performances in many famous databases(such as the 15 scene dataset). A main drawback of the BoF model is it disregards all information about the spatial layout of the features, leads to a limited descriptive ability. In this paper, we use co-occurrence matrix to implant the spatial relations between local features, and demonstrate that feature co-occurrence matrix (FCM) is a potential discriminative character to scenes classification. We propose three FCM based image representations for scenes classification. The experimental results show that, under equal protocol, the proposed method outperforms BoF model and Spatial Pyramid (SP) model and achieves a comparable performance to the state-of-the-art.	cluster analysis;co-occurrence matrix;codebook;document-term matrix;fuzzy cognitive map;heart rate variability;ibm notes;performance	Haitao Lang;Yuyang Xi;Jianying Hu;Liang Du;Haibin Ling	2014		10.1007/978-3-319-16628-5_36	computer science;feature	Vision	32.55567998757826	-52.995458505135296	14696
d2b6a0fd3a266f44a1310cff0c0e761e2335558b	are fuzzy definitions of basic attributes of image objects really useful?	image segmentation;binary image;indexing terms;fuzzy set theory;computer vision;feature based methods fuzzy definitions basic attributes image objects computer vision applications thresholding segmentation crisp object boundaries crisp decisions object boundaries intensity based image analysis;fuzzy set theory image segmentation;image analysis;image segmentation image color analysis image texture analysis computer vision gray scale application software fuzzy sets fuzzy reasoning geometry inspection	Computer vision applications often involve measuring properties of objects in images. Typically, thresholding or segmentation techniques are used to obtain crisp object boundaries before object properties are computed. In this correspondence, we explore the possibility of using fuzzy definitions for measuring object properties without having to make crisp decisions about object boundaries prematurely. We present theorems which indicate that the use of fuzzy definitions to measure properties in intensity-based image analysis almost always gives accurate results. We also present experimental evidence and reasoning which show that fuzzy definitions are not always useful in feature-based methods.	fuzzy logic	Swarup Medasani;Raghu Krishnapuram;James M. Keller	1999	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.769756	image texture;computer vision;feature detection;image analysis;index term;binary image;fuzzy classification;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;fuzzy set;image segmentation;scale-space segmentation	Vision	42.60551722989571	-67.33199351815506	14700
beecd76e0571924dd383e729576025a2a02110c9	motion segmentation by multistage affine classification	metodo cuadrado menor;methode moindre carre;estimation mouvement;image segmentation;image processing;least squares method;video signal processing;real video frames motion segmentation multistage affine classification dominant motion block based affine modeling adaptive k means clustering step merging step;estimacion movimiento;procesamiento imagen;image classification;motion estimation;segmentation;classification;traitement image;algorithme;algorithm;motion segmentation;performance improvement;merging;motion segmentation computer vision motion estimation image segmentation clustering algorithms merging parameter estimation testing labeling laboratories;clasificacion;k means clustering;segmentacion;merging motion estimation image segmentation image classification video signal processing;algoritmo	We present a multistage affine motion segmentation method that combines the benefits of the dominant motion and block-based affine modeling approaches. In particular, we propose two key modifications to a recent motion segmentation algorithm developed by Wang and Adelson (1994). 1) The adaptive k-means clustering step is replaced by a merging step, whereby the affine parameters of a block which has the smallest representation error, rather than the respective cluster center, is used to represent each layer; and 2) we implement it in multiple stages, where pixels belonging to a single motion model are labeled at each stage. Performance improvement due to the proposed modifications is demonstrated on real video frames.	algorithm;cluster analysis;frame (physical object);frame (video);k-means clustering;multistage amplifier;pixel;tracer;benefit;biologic segmentation;statistical cluster	George Borshukov;Gozde Bozdagi Akar;Yücel Altunbasak;A. Murat Tekalp	1997	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.641420	computer vision;contextual image classification;image processing;biological classification;quarter-pixel motion;computer science;machine learning;pattern recognition;motion estimation;harris affine region detector;mathematics;image segmentation;affine shape adaptation;segmentation;least squares;k-means clustering	Vision	48.502250586948804	-54.31374182937179	14712
643bbe8450a52ff474b8194a2c95097d02387610	the influence of culture on memory	fmri;cognition;culture;strategies;memory	The study of cognition across cultures offers a useful approach to both identifying bottlenecks in information processing and suggesting culturespecific strategies to alleviate these limitations. The recent emphasis on applying cognitive neuroscience methods to the study of culture further aids in specifying which processes differ cross-culturally. By localizing cultural differences to distinct neural regions, the comparison of cultural groups helps to identify candidate information processing mechanisms that can be made more efficient with augmented cognition and highlights the unique solutions that will be required for different groups of information processors.	augmented cognition;central processing unit;cognitive tutor;failure;information processing;organizing (structure)	Angela H. Gutchess;Aliza J. Schwartz;Aysecan Boduroglu	2011		10.1007/978-3-642-21852-1_9	psychology;developmental psychology;communication;social psychology;augmented cognition	ML	18.7319880723619	-76.89272768172466	14739
67e8739445512a3af41f4fd8d6769f40bf23fd9c	an improved grabcut using a saliency map	visual saliency;manuals;image segmentation;visual saliency grabcut image segmentation;graph cut saliency map grabcut method interactive image segmentation method target visual saliency;image segmentation graph theory;grabcut;accuracy;visualization;image color analysis;image segmentation accuracy image color analysis manuals visualization robustness conferences;robustness;conferences	The GrabCut, which uses the graph-cut iteratively, is popularly used as an interactive image segmentation method since it can produce the globally optimal result. However, since the initialization of the GrabCut is roughly performed by the manual interaction, the accuracy of the segmentation result is not guaranteed when the user defines an inaccurate guide. To solve this problem, in this paper, we present an improved GrabCut method which uses a visual saliency of the target region for the effective initialization. Experimental results demonstrate that the proposed method provides more accurate segmentation results compared with the conventional method.	cut (graph theory);grabcut;image segmentation;maxima and minima	Kwang-Shik Kim;Yeo-Jin Yoon;Mun-Cheon Kang;Jee-Young Sun;Sung-Jea Ko	2014	2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2014.7031272	computer vision;computer science;grabcut;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	46.17195296077426	-70.02868100467086	14748
8006924a99b476527f06a41d36e862c11b9e1671	understanding chromatin structure: efficient computational implementation of polymer physics models		In recent years the development of novel technologies, as Hi-C or GAM, allowed to investigate the spatial structure of chromatin in the cell nucleus with a constantly increasing level of accuracy. Polymer physics models have been developed and improved to better interpret the wealth of complex information coming from the experimental data, providing highly accurate understandings on chromatin architecture and on the mechanisms regulating genome folding. To investigate the capability of the models to explain the experiments and to test their agreement with the data, massive parallel simulations are needed and efficient algorithms are fundamental. In this work, we consider general computational Molecular Dynamics (MD) techniques commonly used to implement such models, with a special focus on the Strings u0026 Binders Switch polymer model. By combining this model with machine learning computational approaches, it is possible to give an accurate description of real genomic loci. In addition, it is also possible to make predictions about the impact of structural variants of the genomic sequence, which are known to be linked to severe congenital diseases.		Simona Bianco;Carlo Annunziatella;Andrea Esposito;Luca Fiorillo;Mattia Conte;Raffaele Campanile;Andrea M. Chiariello	2018		10.1007/978-3-030-10549-5_53	molecular dynamics;parallel computing;experimental data;genome;computer science;chromatin;congenital diseases;polymer physics	ML	5.516642953332652	-67.31885128931953	14751
487e9b347db84fdd6a0bcb230226ccff52c8a6a1	earlier detection of alzheimer disease using n-fold cross validation approach	alzheimer disease (ad);hybrid wrapper filtering approach;lucy–richardson approach;n-fold crosses validation;prolong adaptive exclusive analytical atlas	According to the recent study, world-wide 40 million patients are affected by Alzheimer disease (AD) because it is one of the dangerous neurodegenerative disorders. This AD disease has less symptoms such as short term memory loss, mood swings, problem with language understanding and behavioral issues. Due to these low symptoms, AD disease is difficult to recognize in the early stage. So, the automated computer aided system need to be developed for recognizing the AD disease for minimizing the mortality rate. Initially, brain MRI image is collected from patients which are processed by applying different processing steps such as noise removal, segmentation, feature extraction, feature selection and classification. The captured MRI image has noise that is eliminated by applying the Lucy–Richardson approach which examines the each pixel in the image and removes the Gaussian noise which also eliminates the blur image. After eliminating the noise pixel from the image, affected region is segmented by Prolong adaptive exclusive analytical Atlas approach. From the segmented region, different GLCM statistical features are extracted and optimal features subset is selected by applying the hybrid wrapper filtering approach. This selected features are analyzed by N-fold cross validation approach which recognizes the AD related features successfully. Then the efficiency of the system is evaluated with the help of MATLAB based experimental results, in which Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset images are utilized for examining the efficiency in terms of sensitivity, specificity, ROC curve and accuracy.	alzheimer's disease;amendment;breast feeding, exclusive;coefficient;computation (action);consent forms;cross infection;cross reactions;cross-validation (statistics);declaration (computer programming);feature extraction;feature selection;gaussian blur;google map maker;helsinki declaration;limited stage (cancer stage);matlab;memory disorders;mixture model;mood disorders;mutual information;natural language understanding;neurodegenerative disorders;neuroimaging;normal statistical distribution;patients;pixel;receiver operating characteristic;requirement;richardson number;sensitivity and specificity;silo (dataset);statistical classification;sternarchogiton preto;subgroup;standards characteristics	R. Sampath;J. Indumathi	2018	Journal of Medical Systems	10.1007/s10916-018-1068-5	filter (signal processing);cross-validation;data mining;feature selection;gaussian noise;pixel;feature extraction;neuroimaging;segmentation;artificial intelligence;pattern recognition;medicine	ML	34.25161154478826	-77.22028311912881	14762
d171dc5f031ae3365e1700e6e1dfc1cb1ee84da6	hearing shapes: event-related potentials reveal the time course of auditory–visual sensory substitution		In auditory–visual sensory substitution, visual information (e.g., shape) can be extracted through strictly auditory input (e.g., soundscapes). Previous studies have shown that image-to-sound conversions that follow simple rules [such as the Meijer algorithm; Meijer, P. B. L. An experimental system for auditory image representation. Transactions on Biomedical Engineering, 39, 111–121, 1992] are highly intuitive and rapidly learned by both blind and sighted individuals. A number of recent fMRI studies have begun to explore the neuroplastic changes that result from sensory substitution training. However, the time course of cross-sensory information transfer in sensory substitution is largely unexplored and may offer insights into the underlying neural mechanisms. In this study, we recorded ERPs to soundscapes before and after sighted participants were trained with the Meijer algorithm. We compared these posttraining versus pretraining ERP differences with those of a control group who received the same set of 80 auditory/visual stimuli but with arbitrary pairings during training. Our behavioral results confirmed the rapid acquisition of cross-sensory mappings, and the group trained with the Meijer algorithm was able to generalize their learning to novel soundscapes at impressive levels of accuracy. The ERP results revealed an early cross-sensory learning effect (150–210 msec) that was significantly enhanced in the algorithm-trained group compared with the control group as well as a later difference (420–480 msec) that was unique to the algorithm-trained group. These ERP modulations are consistent with previous fMRI results and provide additional insight into the time course of cross-sensory information transfer in sensory substitution.	acoustic evoked brain stem potentials;auditory perceptual disorders;complement system proteins;erp;eighty;electroencephalography;experiment;experimental system;extraction;how true feel sluggish right now;mental association;modality (human–computer interaction);rule (guideline);sensory substitution;algorithm;fmri	Christian Graulty;Orestis Papaioannou;Phoebe Bauer;Michael A. Pitts;Enriqueta Canseco-Gonzalez	2018	Journal of Cognitive Neuroscience	10.1162/jocn_a_01210	cognitive psychology;neuroplasticity;visual sensory;information transfer;psychology;event-related potential;sensory substitution	ML	14.959176686057361	-77.36283390299472	14765
a6e1ccf8fcdd40cd062caa6c3285d2fdb7909be9	robust sign language recognition by combining manual and non-manual features based on conditional random field and support vector machine	boostmap embedding;sign language recognition;conditional random field;support vector machine	The sign language is composed of two categories of signals: manual signals such as signs and fingerspellings and non-manual ones such as body gestures and facial expressions. This paper proposes a new method for recognizing manual signals and facial expressions as non-manual signals. The proposed method involves the following three steps: First, a hierarchical conditional random field is used to detect candidate segments of manual signals. Second, the BoostMap embedding method is used to verify hand shapes of segmented signs and to recognize fingerspellings. Finally, the support vector machine is used to recognize facial expressions as non-manual signals. This final step is taken when there is some ambiguity in the previous two steps. The experimental results indicate that the proposed method can accurately recognize the sign language at an 84% rate based on utterance data.	active appearance model;conditional random field;language identification;meltwater entrepreneurial school of technology;real-time clock;support vector machine	Hee-Deok Yang;Seong-Whan Lee	2013	Pattern Recognition Letters	10.1016/j.patrec.2013.06.022	support vector machine;speech recognition;computer science;machine learning;pattern recognition;conditional random field	AI	30.90450511044886	-60.45279779987395	14766
0ba38f6dcb5d1c8f75afb386a51f57d5bf704973	on correlation		Correlation is a measure of the strength of a relationship between two variables. Correlations do not indicate causality and are not used to make predictions; rather they help identify how strongly and in what direction two variables covary in an environment. In the context of nutrient criteria development, correlation analysis is a powerful tool to explore which variables may be strongly related to nutrient concentrations.	causality	Sibasis Mukherjee	2003	Journal of Quantitative Linguistics	10.1076/jqul.10.2.187.16716		Logic	13.996353922537908	-56.356998417054186	14782
ae557167eb8a62616a8ed0ac974b4df8fe3c3299	compass: a program for generating serial samples under an infinite sites model	modelo;echantillon;modele;sample;biologiska vetenskaper;biological sciences;models;muestra;sitio;site	SUMMARY The program COMPASS can generate samples that have been collected at various points in time from a population that is evolving according to a Wright-Fisher model. The samples are generated using coalescence simulations permitting various demographic scenarios and the program uses an infinite sites model to generate polymorphism data for the samples. By generating serially sampled population-genetic data, COMPASS allows investigating properties of polymorphism data that has been collected at different time points, and aid in making inference from ancient polymorphism data.   AVAILABILITY The program and the manual are available at: http://www.egs.uu.se/evbiol/Research/JakobssonLab/compass.html   CONTACT mattias.jakobsson@ebc.uu.se.	compass;coalescing (computer science);inference;simulation;wright stain	Mattias Jakobsson	2009	Bioinformatics	10.1093/bioinformatics/btp534	sample;statistics	Comp.	-3.052044582658332	-55.59338927945189	14784
c9ad7d96944fbc5a81729c31f6cb3a72b2f24acf	computer-aided sleep staging using complete ensemble empirical mode decomposition with adaptive noise and bootstrap aggregating	bagging;ceemdan;ensemble methods;sleep scoring;eeg	Computer-aided sleep staging based on single channel electroencephalogram (EEG) is a prerequisite for a feasible low-power wearable sleep monitoring system. It can also eliminate the burden of the clinicians during analyzing a high volume of data by making sleep scoring less onerous, time-consuming and error-prone. Most of the prior studies focus on multichannel EEG based methods which hinder the aforementioned goals. Among the limited number of single-channel based methods, only a few yield good performance in automatic sleep staging. In this article, a single-channel EEG based method for sleep staging using recently introduced Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) and Bootstrap Aggregating (Bagging) is proposed. At first, EEG signal segments are decomposed into intrinsic mode functions. Higher order statistical moments computed from these nsemble methods functions are used as features. Bagged decision trees are then employed to classify sleep stages. This is the first time that CEEMDAN is employed for automatic sleep staging. Experiments are carried out using the well-known Sleep-EDF database and the results show that the proposed method is superior as compared to the state-of-the-art methods in terms of accuracy. In addition, the proposed scheme gives high detection accuracy for sleep stages S1 and REM. © 2015 Elsevier Ltd. All rights reserved.	bootstrap aggregating;cognitive dimensions of notations;decision tree;disk staging;earliest deadline first scheduling;electroencephalography;hilbert–huang transform;low-power broadcasting;sleep (system call);wearable computer;whole earth 'lectronic link	Ahnaf Rashik Hassan;Mohammed Imamul Hassan Bhuiyan	2016	Biomed. Signal Proc. and Control	10.1016/j.bspc.2015.09.002	speech recognition;bootstrap aggregating;computer science;machine learning;pattern recognition;ensemble learning	AI	17.537676395662963	-90.07046736722525	14811
9fbc6c38d87f7ba2ab237e59f16cad9e36d9209e	accurate object recognition in the underwater images using learning algorithms and texture features	underwater object recognition;image segmentation;chain coding;back propagation neural network;texture parameters;deep learning;morphological operators	Underwater image processing is very challenging due to its environmental conditions and poor sunlight. Images captured from the ocean using autonomous vehicles are often non-uniformly illuminated and contain noise due to the underlying environment. Object recognition is a challenging task under water due to the variation in the environment, target shape and orientation. Traditional algorithms based on spatial information may not lead to accurate segmentation as the intensity variation is often less in underwater images. Texture information representing the characteristics of the object is needed. Statistical features like autocorrelation, sum average, sum variance and sum entropy were extracted. These were fed as input to learning algorithms and training was done to effectively classify the object of interest and background. Chain coding was further applied for object recognition. The proposed methodology achieved a maximum classification accuracy of 96%.	algorithm;artificial neural network;autocorrelation;autonomous robot;color;database;deep learning;earth system science;feature model;image processing;machine learning;mathematical morphology;object detection;outline of object recognition;pixel;test data	K Srividhya;M M. RamyaM.	2017	Multimedia Tools and Applications	10.1007/s11042-017-4459-6	artificial intelligence;image processing;computer vision;computer science;autocorrelation;spatial analysis;3d single-object recognition;deep learning;pattern recognition;image segmentation;algorithm;cognitive neuroscience of visual object recognition;underwater	Vision	30.78232332749819	-55.287469212427276	14824
96aa5cee1dc5903f3afb72495549aa1767bea9ee	two-dimensional object alignment based on the robust oriented hausdorff similarity measure	adaptacion;object recognition;image processing;object matching two dimensional object alignment 2d images oriented hausdorff similarity measure robust object alignment hough transform distance transform;binary image;etude experimentale;image matching;hausdorff dimension;high definition video pollution measurement noise robustness statistics image edge detection statistical distributions two dimensional displays computer vision object recognition object detection;procesamiento imagen;transformacion desplazamiento;transformation deplacement;fotografia aerea;transformacion hough;traitement image;similitude;distance measurement;photographie aerienne;medicion distancia;adaptation;similarity;image binaire;displacive transformation;hough transforms object recognition image matching;imagen binaria;hough transforms;hough transformation;hough transform;transformation hough;dimension hausdorff;similitud;distance transform;similarity measure;estudio experimental;aerial photography;mesure de distance	This paper proposes an oriented Hausdorff similarity (OHS) measure for robust object alignment. The OHS measure is introduced by replacing the distance concept of conventional Hausdoff distance (HD) algorithms by the similarity concept of the Hough transform (HT). The proposed algorithm can be considered as the modified directed HT using the distance transform (DT). The orientation information at each pixel is also used to remove incorrect correspondences.	algorithm;alignment;distance transform;hausdorff dimension;hough transform;hypertensive disease;medical records, problem-oriented;oracle http server;pixel;similarity measure	Dong-Gyu Sim;Rae-Hong Park	2001	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.908541	hough transform;computer vision;topology;image processing;computer science;mathematics;geometry;aerial photography	Vision	42.90582477489063	-58.48393733158263	14833
50e7caf5e4febad74ce3cd5336bfac5863da2f5a	phylomedb v3.0: an expanding repository of genome-wide collections of trees, alignments and phylogeny-based orthology and paralogy predictions	genes;genomics;phylogeny;seeds;molecular sequence annotation;databases genetic;genome sequencing;trees plant;reconstructive surgical procedures;genome;humans;sequence alignment;peas dietary	The growing availability of complete genomic sequences from diverse species has brought about the need to scale up phylogenomic analyses, including the reconstruction of large collections of phylogenetic trees. Here, we present the third version of PhylomeDB (http://phylomeDB.org), a public database for genome-wide collections of gene phylogenies (phylomes). Currently, PhylomeDB is the largest phylogenetic repository and hosts 17 phylomes, comprising 416,093 trees and 165,840 alignments. It is also a major source for phylogeny-based orthology and paralogy predictions, covering about 5 million proteins in 717 fully-sequenced genomes. For each protein-coding gene in a seed genome, the database provides original and processed alignments, phylogenetic trees derived from various methods and phylogeny-based predictions of orthology and paralogy relationships. The new version of phylomeDB has been extended with novel data access and visualization features, including the possibility of programmatic access. Available seed species include model organisms such as human, yeast, Escherichia coli or Arabidopsis thaliana, but also alternative model species such as the human pathogen Candida albicans, or the pea aphid Acyrtosiphon pisum. Finally, PhylomeDB is currently being used by several genome sequencing projects that couple the genome annotation process with the reconstruction of the corresponding phylome, a strategy that provides relevant evolutionary insights.	access network;annotation;antigens, cd30;blast;base excision repair;candida albicans;classification research group;collections (publication);data access;database;gene prediction;homology (biology);imagery;largest;pathogenic organism;phylogenetic tree;phylomedb;repository;trees (plant);whole genome sequencing	Jaime Huerta-Cepas;Salvador Capella-Gutiérrez;Leszek P. Pryszcz;Ivan Denisov;Diego Kormes;Marina Marcet-Houben;Toni Gabaldón	2011		10.1093/nar/gkq1109	biology;dna sequencing;genomics;bioinformatics;gene;sequence alignment;genetics;genome	Comp.	-1.063574762835322	-59.68362554355876	14841
7ffb886aba25c66efcff22acb19a0eef314fa0d8	exploring the inhibitory potential of bioactive compound from luffa acutangula against nf-κb - a molecular docking and dynamics approach	nf κb;dynamics;nf źb;molecular docking;dft;dhma	Nuclear factor kappa B (NF-κB) is a transcription factor, plays a crucial role in the regulation of various physiological processes such as differentiation, cell proliferation and apoptosis. It also coordinates the expression of various soluble proinflammatory mediators like cytokines and chemokines. The 1, 8-dihydroxy-4-methylanthracene-9, 10-dione (DHMA) was isolated from Luffa acutangala and its in vitro cytotoxic activity against NCI-H460 cells was reported earlier. It also effectively induces apoptosis through suppressing the expression NF-κB protein. Based on experimental evidence, the binding affinity of compound 1 with NF-κB p50 (monomer) and NF-κB-DNA was investigated using molecular docking and its stability was confirmed through molecular dynamic simulation. The reactivity of the compound was evaluated using density functional theory (DFT) calculation. From the docking results, we noticed that the hydroxyl group of DHMA forms hydrogen bond interactions with polar and negatively charged amino acid Tyr57 and Asp239 and the protein-ligand complex was stabilized through pi-pi stacking with the help of polar amino acid His114, which plays a key role in binding of NF-κB to DNA at a site of DNA-binding region (DBR). The result indicates that the isolated bioactive compound DHMA might have altered the binding affinity between DNA and NF-κB. These findings suggest that potential use of DHMA in cancer chemoprevention and therapeutics.		Vanajothi Ramar;Srinivasan Pappu	2016	Computational biology and chemistry	10.1016/j.compbiolchem.2016.03.006	biochemistry;stereochemistry;dynamics;chemistry;docking;bioinformatics;discrete fourier transform;computational chemistry;mathematics	Comp.	8.624276149893085	-62.34445564761272	14844
e0067a9c3f30aab46d9e6f8844dc36e83c85869f	automatic brain tumor detection and segmentation using u-net based fully convolutional networks		A major challenge in brain tumor treatment planning and quantit ative evaluation is determination of the tumor extent. The noninva s ve magnetic resonance imaging (MRI) technique has emerged as a front-line diag nostic tool for brain tumors without ionizing radiation. Manual segmentation of brain t umor extent from 3D MRI volumes is a very time-consuming task and the performance is highly relied on operator’s experience. In this context, a reliable fully automatic segmentation method for the brain tumor segmentation is necessary for an efficient measurement of the tumor extent. In this study, w e propose a fully automatic method for brain tumor segmentation, which is develo ped using U-Net based deep convolutional networks. Our method was evalu ated on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade brain tumor and 54 low-grade tumor cases. Cro svalidation has shown that our method can obtain promising segmentation effi-	convolution;cross-validation (statistics);data validation;experiment;ground truth;hyperbolic geometric graph;image segmentation;interference (communication);multimodal interaction;resonance	Hao Dong;Guang Yang;Fangde Liu;Yuanhan Mo;Yike Guo	2017		10.1007/978-3-319-60964-5_44	computer vision;computer science;radiation treatment planning;image segmentation;magnetic resonance imaging;segmentation;artificial intelligence;brain tumor	Vision	31.530623722264842	-76.16745010318641	14858
11a69567ebe4b0e5e91f764a23e22711bac8fe0f	a wireless body area sensor network for posture detection	human computer interaction;application software;ambient intelligence;wearable sensors;design optimization;sensor network;emerging technology;computerized monitoring;interactive application;wireless sensor networks intelligent sensors application software wearable sensors computer interfaces human computer interaction ambient intelligence face detection design optimization computerized monitoring;human body;power consumption;face detection;computer interfaces;wireless sensor networks;intelligent sensors;human computer interface	Body Area Sensor Networks (BASN) are an emerging technology enabling the design of natural Human Computer Interfaces (HCI) in the context of Ambient Intelligence. This class of interactive applications poses new challenges on sensor network design that are hard to be faced using traditional solutions optimized for environmental monitoringlike applications. In this paper we present a novel solution for wireless and wearable posture recognition based on a custom-designed wireless body area sensor network, called WiMoCA. Nodes of the network, mounted on different parts of the human body, exploit tri-axial accelerometers to detect body postures. Afterwards we discuss results of interactive performance and power consumption optimizations required to match application constraints.	ambient intelligence;human computer;human–computer interaction;network planning and design;poor posture;sensor node;triangular function;wearable computer	Elisabetta Farella;Augusto Pieracci;Luca Benini;Andrea Acquaviva	2006	11th IEEE Symposium on Computers and Communications (ISCC'06)	10.1109/ISCC.2006.21	embedded system;simulation;wireless sensor network;human–computer interaction;computer science;body area network;key distribution in wireless sensor networks;mobile wireless sensor network;visual sensor network	Mobile	2.3603036773424604	-89.91809551259107	14877
8867a8c7eb8b21774d25556632f7cf7036f37f48	enviz: a cytoscape app for integrated statistical analysis and visualization of sample-matched data with multiple data types	software;breast neoplasms;female;data interpretation statistical;computer graphics;cell cycle;humans;gene expression profiling	ENViz (Enrichment Analysis and Visualization) is a Cytoscape app that performs joint enrichment analysis of two types of sample matched datasets in the context of systematic annotations. Such datasets may be gene expression or any other high-throughput data collected in the same set of samples. The enrichment analysis is done in the context of pathway information, gene ontology or any custom annotation of the data. The results of the analysis consist of significant associations between profiled elements of one of the datasets to the annotation terms (e.g. miR-19 was associated to the cell-cycle process in breast cancer samples). The results of the enrichment analysis are visualized as an interactive Cytoscape network.	annotation;cytoscape;gene expression;gene ontology term enrichment;gene regulatory network;high-throughput computing;imagery;mammary neoplasms;mental association;throughput	Israel Steinfeld;Roy Navon;Michael L. Creech;Zohar Yakhini;Anya Tsalenko	2015		10.1093/bioinformatics/btu853	biology;computer science;bioinformatics;cell cycle;data mining;gene expression profiling;computer graphics;world wide web;genetics	Visualization	-2.3670936210750906	-58.35161547187207	14880
6e33950353d28384bd9c5f564ba716484a3539c2	an effective approach for generating a three-cys2his2 zinc-finger-dna complex model by docking	dna;software;zinc fingers;zinc finger protein;x ray crystallography;dna binding proteins;complex structure;geometric distribution;sp1 transcription factor;transcription factors;zinc finger;computational biology bioinformatics;root mean square deviation;models molecular;nuclear magnetic resonance biomolecular;algorithms;difference set;humans;combinatorial libraries;crystallography x ray;computer appl in life sciences;microarrays;bioinformatics	Determination of protein-DNA complex structures with both NMR and X-ray crystallography remains challenging in many cases. High Ambiguity-Driven DOCKing (HADDOCK) is an information-driven docking program that has been used to successfully model many protein-DNA complexes. However, a protein-DNA complex model whereby the protein wraps around DNA has not been reported. Defining the ambiguous interaction restraints for the classical three-Cys2His2 zinc-finger proteins that wrap around DNA is critical because of the complicated binding geometry. In this study, we generated a Zif268-DNA complex model using three different sets of ambiguous interaction restraints (AIRs) to study the effect of the geometric distribution on the docking and used this approach to generate a newly reported Sp1-DNA complex model. The complex models we generated on the basis of two AIRs with a good geometric distribution in each domain are reasonable in terms of the number of models with wrap-around conformation, interface root mean square deviation, AIR energy and fraction native contacts. We derived the modeling approach for generating a three-Cys2His2 zinc-finger-DNA complex model according to the results of docking studies using the Zif268-DNA and other three crystal complex structures. Furthermore, the Sp1-DNA complex model was calculated with this approach, and the interactions between Sp1 and DNA are in good agreement with those previously reported. Our docking data demonstrate that two AIRs with a reasonable geometric distribution in each of the three-Cys2His2 zinc-finger domains are sufficient to generate an accurate complex model with protein wrapping around DNA. This approach is efficient for generating a zinc-finger protein-DNA complex model for unknown complex structures in which the protein wraps around DNA. We provide a flowchart showing the detailed procedures of this approach.	boat dock;computed tomography scanning systems;crystallography;docking (molecular);docking -molecular interaction;flowchart;haddock;mean squared error;plant roots;staphylococcal protein a;wrapping (graphics)	Chun-Chi Chou;M. Rajasekaran;Chinpan Chen	2009		10.1186/1471-2105-11-334	biology;zinc finger;searching the conformational space for docking;bioinformatics;genetics;x-ray crystallography	Comp.	10.71415056396201	-59.13357274438026	14886
5cc682234169077eb79ff530186ce390f72198c1	construction of an interactive system aims to extract expert knowledge about the condition cultured corneal endothelial cells	eye;interactive genetic algorithm iga expert system corneal endothelial cells;corneal endothelial cells;sociology statistics cells biology search problems expert systems genetic algorithms computational modeling;medical expert systems;interactive genetic algorithm iga;diagnostic expert systems;genetic algorithms;interactive system iga interactive genetic algorithm condition cultured corneal endothelial cells expert knowledge;medical expert systems diagnostic expert systems eye genetic algorithms medical diagnostic computing;medical diagnostic computing;expert system	We aim to construct an expert system for diagnosing the health of corneal endothelial cells. To construct the proposed system, we first constructed a system that confirms whether experts use the same criteria to diagnose the condition of cells. In the constructed system, an expert interacts with a computer that generates images of cells by simulation. These images describe cells that are in the best condition, according to expert diagnosis. By comparing the results from multiple experts, we can elucidate whether experts use the same criterion for diagnosis. The proposed system is composed of an interactive genetic algorithm (IGA) and involves the simulation of cells. We confirmed the system operated normally through operational experiments. In another experiment, conducted with no experts, we confirmed that this system could generate images demonstrating a predetermined feature value.	computer;experiment;expert system;genetic algorithm;in-game advertising;interactive evolutionary computation;interactivity;simulation	Tomoyuki Hiroyasu;Kiyofumi Uehori;Utako Yamamoto;Misato Tanaka	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.312	simulation;genetic algorithm;computer science;artificial intelligence;data mining;expert system	Robotics	3.7137817675890785	-79.69245296502524	14888
310afaf9645215aca390895dc035f2ad1052d6bb	a convolutional neural network for gait recognition based on plantar pressure images		This paper proposed a novel gait recognition method that is based on plantar pressure images. Different from many conventional methods where hand-crafted features are extracted explicitly. We utilized Convolution Neural Network (CNN) for automatic feature extraction as well as classification. The peak pressure image (PPI) generated from the time series of plantar pressure images is used as the characteristic image for gait recognition in this study. Our gait samples are collected from 109 subjects under three kinds of walking speeds, and for each subject total 18 samples are gathered. Experimental results demonstrate that the designed CNN model can obtain very high classification accuracy as compared to many traditional methods.	convolutional neural network;pedobarography	Yanlin Li;Dexiang Zhang;Jun Zhang;Li-Na Xun;Qing Yan;Jingjing Zhang;Qingwei Gao;Yi Xia	2017		10.1007/978-3-319-69923-3_50	convolutional neural network;feature extraction;gait;preferred walking speed;computer vision;computer science;artificial intelligence;pattern recognition	Vision	17.194127304260274	-91.39197192180903	14908
1c747481e2a67b8301f3ee9e24c051e0ec84c0d9	efficient use of accessibility in microrna target prediction	genes;rna interference;animals;sequences;data interpretation statistical;tool;expression;binding sites;nucleic acid hybridization;recognition;site accessibility;identification;drosophila melanogaster;mirna;3 untranslated regions;nucleic acid conformation;algorithms;humans;computational biology;micrornas;microrna	Considering accessibility of the 3'UTR is believed to increase the precision of microRNA target predictions. We show that, contrary to common belief, ranking by the hybridization energy or by the sum of the opening and hybridization energies, used in currently available algorithms, is not an efficient way to rank predictions. Instead, we describe an algorithm which also considers only the accessible binding sites but which ranks predictions according to over-representation. When compared with experimentally validated and refuted targets in the fruit fly and human, our algorithm shows a remarkable improvement in precision while significantly reducing the computational cost in comparison with other free energy based methods. In the human genome, our algorithm has at least twice higher precision than other methods with their default parameters. In the fruit fly, we find five times more validated targets among the top 500 predictions than other methods with their default parameters. Furthermore, using a common statistical framework we demonstrate explicitly the advantages of using the canonical ensemble instead of using the minimum free energy structure alone. We also find that 'naïve' global folding sometimes outperforms the local folding approach.	3' untranslated regions;accessibility;algorithmic efficiency;binding sites;computation;default;drosophila <fruit fly, genus>;energy, physics;experiment;fruit fly, queensland;naivety;nucleic acid hybridization;top500;algorithm;free energy	Ray M. Marín;Jiří Vaníček	2011		10.1093/nar/gkq768	biology;bioinformatics;genetics;microrna	NLP	11.34055485153109	-59.848928082183825	14922
5c6738889674c9922abb334ab7d972a305a7346b	apprentissage a contrario et architecture efficace pour la détection d'évènements visuels significatifs	vision par ordinateur methodes a contrario apprentissage statistique segmentation reconnaissance d objets anytime en francais	To ensure the robustness of a detection algorithm, it is important to get a close control of the false alarms it may produce. Because of the great variability of natural images, this task is very difficult in computer vision, and most methods have to rely on a priori chosen parameters. This limits the validity and applicability of the resulting algorithms. Recently, by searching for structures for which some properties are very unlikely to be due to chance, the a contrario statistical approach has proved successful to provide parameterless detection algorithms with a bounded expected number of false alarms. However, existing applications rely on a purely analytical framework that requires a big modeling effort, makes it difficult to use heterogeneous features and limits the use of data-driven search heuristics. In this thesis, we propose to overcome these restrictions by using statistical learning for quantities that cannot be computed analytically. The interest of this approach is demonstrated through three applications : segment detection, segmentation into homogeneous regions, and object matching from a database of pictures. For the two first ones, we show that robust decision thresholds can be learned from white noise images. For the last one, we show that only a few examples of natural images that do not contain the database objects are sufficient to learn accurate decision thresholds. Finally, we notice that the monotonicity of a contrario reasoning enables an incremental integration of partial data. This property leads us to propose an architecture for object detection which has an “anytime” behavior : it provides results all along its execution, the most salient first, and thus can be constrained to run in limited time. Mots-clés : Computer vision, a contrario reasoning, statistical learning, segmentation, object detection, anytime	anytime algorithm;computer vision;heuristic (computer science);linear algebra;machine learning;object detection;spatial variability;white noise	Nicolas Burrus	2008			humanities;computer science	ML	47.07380565548536	-58.43496865487901	14927
559891c7e6c9e1070ced1710d0de11d109d09728	tracing regulatory routes in metabolism using generalised supply-demand analysis	simulation and modeling;lactococcus lactis;metabolic modelling;metabolic networks and pathways;systems biology;generalised supply demand analysis;physiological cellular and medical topics;pyruvic acid;models biological;metabolic control analysis;arabidopsis thaliana;computational biology bioinformatics;arabidopsis;amino acids;algorithms;article;bioinformatics	Generalised supply-demand analysis is a conceptual framework that views metabolism as a molecular economy. Metabolic pathways are partitioned into so-called supply and demand blocks that produce and consume a particular intermediate metabolite. By studying the response of these reaction blocks to perturbations in the concentration of the linking metabolite, different regulatory routes of interaction between the metabolite and its supply and demand blocks can be identified and their contribution quantified. These responses are mediated not only through direct substrate/product interactions, but also through allosteric effects. Here we subject previously published kinetic models of pyruvate metabolism in Lactococcus lactis and aspartate-derived amino acid synthesis in Arabidopsis thaliana to generalised supply-demand analysis. Multiple routes of regulation are brought about by different mechanisms in each model, leading to behavioural and regulatory patterns that are generally difficult to predict from simple inspection of the reaction networks depicting the models. In the pyruvate model the moiety-conserved cycles of ATP/ADP and NADH/NAD + allow otherwise independent metabolic branches to communicate. This causes the flux of one ATP-producing reaction block to increase in response to an increasing ATP/ADP ratio, while an NADH-consuming block flux decreases in response to an increasing NADH/NAD + ratio for certain ratio value ranges. In the aspartate model, aspartate semialdehyde can inhibit its supply block directly or by increasing the concentration of two amino acids (Lys and Thr) that occur as intermediates in demand blocks and act as allosteric inhibitors of isoenzymes in the supply block. These different routes of interaction from aspartate semialdehyde are each seen to contribute differently to the regulation of the aspartate semialdehyde supply block. Indirect routes of regulation between a metabolic intermediate and a reaction block that either produces or consumes this intermediate can play a much larger regulatory role than routes mediated through direct interactions. These indirect routes of regulation can also result in counter-intuitive metabolic behaviour. Performing generalised supply-demand analysis on two previously published models demonstrated the utility of this method as an entry point in the analysis of metabolic behaviour and the potential for obtaining novel results from previously analysed models by using new approaches.	adenosine diphosphate;adenosine triphosphate;amino acids;aspartic acid;automated theorem proving;entry point;gene regulatory network;interaction;isoenzymes;kinetics;lactococcus lactis;large;lysine;metabolic process, cellular;nadh;network access device;nicotinamide adenine dinucleotide (nad);pyruvate metabolism pathway;pyruvates;reflow soldering;scientific publication;threonine;usb on-the-go	Carl D. Christensen;Jan-Hendrik S. Hofmeyr;Johann M. Rohwer	2015		10.1186/s12918-015-0236-1	metabolic control analysis;biology;biochemistry;biotechnology;bioinformatics;systems biology	Comp.	7.859465118687552	-62.75743267250702	14928
5301e59acaa845dcaf1d6a05414906d2cfcc93ce	brain mechanisms underlying cue-based memorizing during free viewing of movie memento	cued-recall;naturalistic stimulus;neurocinematics;pattern analysis;schema;fmri	"""How does the human brain recall and connect relevant memories with unfolding events? To study this, we presented 25 healthy subjects, during functional magnetic resonance imaging, the movie 'Memento' (director C. Nolan). In this movie, scenes are presented in chronologically reverse order with certain scenes briefly overlapping previously presented scenes. Such overlapping """"key-frames"""" serve as effective memory cues for the viewers, prompting recall of relevant memories of the previously seen scene and connecting them with the concurrent scene. We hypothesized that these repeating key-frames serve as immediate recall cues and would facilitate reconstruction of the story piece-by-piece. The chronological version of Memento, shown in a separate experiment for another group of subjects, served as a control condition. Using multivariate event-related pattern analysis method and representational similarity analysis, focal fingerprint patterns of hemodynamic activity were found to emerge during presentation of key-frame scenes. This effect was present in higher-order cortical network with regions including precuneus, angular gyrus, cingulate gyrus, as well as lateral, superior, and middle frontal gyri within frontal poles. This network was right hemispheric dominant. These distributed patterns of brain activity appear to underlie ability to recall relevant memories and connect them with ongoing events, i.e., """"what goes with what"""" in a complex story. Given the real-life likeness of cinematic experience, these results provide new insight into how the human brain recalls, given proper cues, relevant memories to facilitate understanding and prediction of everyday life events."""	angularjs;electroencephalography;focal (programming language);fingerprint;frame (physical object);gyrus cinguli;hemodynamics;key frame;lateral thinking;memento pattern;pattern recognition;real life;representation (action);resonance;structure of angular gyrus;structure of precuneus;unfolding (dsp implementation)	Janne Kauttonen;Yevhen Hlushchuk;Iiro P. Jääskeläinen;Pia Tikka	2018	NeuroImage	10.1016/j.neuroimage.2018.01.068	angular gyrus;cognitive psychology;precuneus;brain activity and meditation;psychology;functional magnetic resonance imaging;schema (psychology);gyrus;recall;memorization	ML	18.217442705538836	-76.10917508438578	14936
62696e80d13ea0d7fcbb4865cfc84c34c646829c	neural modelling of cognitive disinhibition and neurotransmitter dysfunction in ocd	anxiety disorder;negative priming;recurrent network;serotonin;recurrent neural network;dopamine;obsessive compulsive disorder	In this paper an Elman recurrent neural network model of obsessive-compulsive disorder (OCD) is developed to provide a simulation of the relationship between the cognitive disinhibition and serotonin/dopamine dysfunction that characterize this disorder. Cognitive disinhibition in OCD is apparent when OCD patients are compared with other anxiety disorder patients on a Temporal Stroop test, with OCD patients showing reduced negative priming. Alterations of the color gain parameter, the context gain parameter, and maximum cycle number were made in order to simulate changes in monoamine neutransmitter function. The recurrent network model was able to simulate reduced cognitive inhibition as well as serotonergic and dopaminergic dysfunction in OCD.		Jacques Ludik;Dan J. Stein	1996		10.1007/3-540-61510-5_150	dopamine;computer science;recurrent neural network;machine learning	HCI	15.723102289857403	-75.06881538025674	14939
b135b242357bd14c5383d056a606af1f32db2691	cooperative use of parallel processing with time or frequency-domain filtering for shape recognition	computer vision;endoscopes;filtering theory;frequency-domain analysis;medical image processing;time-domain analysis;wce images;blob objects;computer vision applications;frequency-domain filtering;identified filtering procedure;medical wireless capsule endoscopy images;parallel signal processing framework;shape recognition;time-domain filtering;tubular objects;convolution;frequency-domain filtering;object shape recognition;parallel processing;wireless capsule endoscopy	For many computer vision applications, detection of blobs and/or tubular structures in images are of great importance. In this paper, we have developed a parallel signal processing framework for speeding up the detection of blob and tubular objects in images. We identified filtering procedure as being responsible for up to 98% of the global processing time, in the used blob or tubular detector functions. We show that after a certain dimension of the filter it is beneficial to combine frequency-domain techniques with parallel processing to develop faster signal processing algorithms. The proposed framework is applied to medical wireless capsule endoscopy (WCE) images, where blob and/or tubular detectors are useful in distinguishing between abnormal and normal images.	algorithm;computer vision;parallel computing;sensor;signal processing	Carlos Graca;Gabriel Falcão Paiva Fernandes;Sunil Kumar;Isabel N. Figueiredo	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		computer vision;speech recognition;pattern recognition	Vision	39.93643911020094	-74.81169060605772	14952
35e8cd7cc66f865ebaac429e534152e9c3606d4e	modeling and analysis of heterogeneous regulation in biological networks	learning algorithm;learning model;gene expression;feedback loop;hidden variables;polynomial time;high throughput;experimental measurement;literature survey;biological network;modeling and analysis	In this study, we propose a novel model for the representation of biological networks and provide algorithms for learning model parameters from experimental data. Our approach is to build an initial model based on extant biological knowledge and refine it to increase the consistency between model predictions and experimental data. Our model encompasses networks which contain heterogeneous biological entities (mRNA, proteins, metabolites) and aims to capture diverse regulatory circuitry on several levels (metabolism, transcription, translation, post-translation and feedback loops, among them). Algorithmically, the study raises two basic questions: how to use the model for predictions and inference of hidden variables states, and how to extend and rectify model components. We show that these problems are hard in the biologically relevant case where the network contains cycles. We provide a prediction methodology in the presence of cycles and a polynomial time, constant factor approximation for learning the regulation of a single entity. A key feature of our approach is the ability to utilize both high-throughput experimental data, which measure many model entities in a single experiment, as well as specific experimental measurements of few entities or even a single one. In particular, we use together gene expression, growth phenotypes, and proteomics data. We tested our strategy on the lysine biosynthesis pathway in yeast. We constructed a model of more than 150 variables based on an extensive literature survey and evaluated it with diverse experimental data. We used our learning algorithms to propose novel regulatory hypotheses in several cases where the literature-based model was inconsistent with the experiments. We showed that our approach has better accuracy than extant methods of learning regulation.	algorithm;anabolism;approximation;biological network;cycle (graph theory);electronic circuit;entity;experiment;feedback;gene expression;gene regulatory network;genetic heterogeneity;hidden variable theory;high-throughput computing;inference;lysine;machine learning;phenotype;polynomial;proteomics;throughput;time complexity;transcription (software)	Irit Gat-Viks;Amos Tanay;Ron Shamir	2004	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2004.11.1034	high-throughput screening;time complexity;biology;biological network;gene expression;computer science;bioinformatics;data science;machine learning;feedback loop;genetics;hidden variable theory	Comp.	5.952155143622492	-58.742847634349864	14969
1e9b7bd01affcff972b62bf36c20d86b0734e4d4	a model for the self-organization of vesicular flux and protein distributions in the golgi apparatus	golgi apparatus;models biological;proteins	The generation of two non-identical membrane compartments via exchange of vesicles is considered to require two types of vesicles specified by distinct cytosolic coats that selectively recruit cargo, and two membrane-bound SNARE pairs that specify fusion and differ in their affinities for each type of vesicles. The mammalian Golgi complex is composed of 6-8 non-identical cisternae that undergo gradual maturation and replacement yet features only two SNARE pairs. We present a model that explains how distinct composition of Golgi cisternae can be generated with two and even a single SNARE pair and one vesicle coat. A decay of active SNARE concentration in aging cisternae provides the seed for a cis[Formula: see text]trans SNARE gradient that generates the predominantly retrograde vesicle flux which further enhances the gradient. This flux in turn yields the observed inhomogeneous steady-state distribution of Golgi enzymes, which compete with each other and with the SNAREs for incorporation into transport vesicles. We show analytically that the steady state SNARE concentration decays exponentially with the cisterna number. Numerical solutions of rate equations reproduce the experimentally observed SNARE gradients, overlapping enzyme peaks in cis, medial and trans and the reported change in vesicle nature across the Golgi: Vesicles originating from younger cisternae mostly contain Golgi enzymes and SNAREs enriched in these cisternae and extensively recycle through the Endoplasmic Reticulum (ER), while the other subpopulation of vesicles contains Golgi proteins prevalent in older cisternae and hardly reaches the ER.	anatomical compartments;biologic development;endoplasmic reticulum;experiment;flux;golgi apparatus;gradient;mammals;medial graph;recycling;snap receptor;self-organization;solutions;source-to-source compiler;steady state;tissue membrane;transport vesicles;vesicle (morphologic abnormality);positive regulation of er to golgi vesicle-mediated transport;vesicle coat	Iaroslav Ispolatov;Anne Müsch	2013		10.1371/journal.pcbi.1003125	biology;biochemistry;cell biology;golgi apparatus;secretory pathway;genetics	ML	7.990777671169991	-64.19099504690173	14980
b80ddc0aab5369a652066596e6ba2a2664f4c4f0	intrinsic mode entropy for nonlinear discriminant analysis	oscillations;entropy time measurement data mining signal analysis time series analysis signal processing signal processing algorithms robustness senior citizens stability;low frequency;postural stability;intrinsic mode entropy;time series;sample entropy computing;time series entropy physiology signal processing;empirical mode decomposition emd;nonlinear time series analysis;physiological signals;nonlinear discriminant analysis;discriminant analysis;posture;nonlinear systems;physiology;intrinsic mode function;signal processing;imen;nonlinear dynamics;empirical mode decomposition method;biomedical signal processing;empirical mode decomposition method intrinsic mode entropy imen nonlinear discriminant analysis time series physiological signals nonlinear dynamics sample entropy computing sampen;posture biomedical signal processing empirical mode decomposition emd entropy nonlinear systems nonlinear time series analysis physiological signals;entropy;nonlinear system;cumulant;sampen;empirical mode decomposition	Several methods of measuring entropy of time series have been developed and applied on physiological signals in order to distinguish data sets according to their underlying nonlinear dynamics. These methods are not well adapted for studying the time series in different scales, in the presence of dominant local trends and low-frequency components. In this letter, intrinsic mode entropy (IMEn) is proposed as an entropy measure over multiple oscillation levels. Robustness to local trends is ensured with this new measure, enabling an efficient characterization of the underlying nonlinear dynamics of the time series considered. IMEn is obtained by computing the Sample Entropy (SampEn) of the cumulative sums of the intrinsic mode functions extracted by the empirical mode decomposition method. An example of an application of IMEn is then presented, with the method able to successfully discriminate between two groups of subjects (elderly versus control) for signals of postural stability	automatic control;entropy (information theory);hilbert–huang transform;linear discriminant analysis;nonlinear system;sample entropy;signal processing;time series	Hassan Amoud;Hichem Snoussi;David J. Hewson;Michel Doussot;Jacques Duchêne	2007	IEEE Signal Processing Letters	10.1109/LSP.2006.888089	econometrics;entropy;transfer entropy;nonlinear system;hilbert–huang transform;machine learning;time series;mathematics;low frequency;oscillation;statistics;cumulant	ML	19.726013056656033	-86.0616060983626	14988
078217576d9b514beef42dff6a5cc4dcb70842ae	a whole genome long-range haplotype (wglrh) test for detecting imprints of positive selection in human populations	evolutionary history;positive selection;high density;allele frequency;large scale;long range;african american;high frequency	MOTIVATION The identification of signatures of positive selection can provide important insights into recent evolutionary history in human populations. Current methods mostly rely on allele frequency determination or focus on one or a small number of candidate chromosomal regions per study. With the availability of large-scale genotype data, efficient approaches for an unbiased whole genome scan are becoming necessary.   METHODS We have developed a new method, the whole genome long-range haplotype test (WGLRH), which uses genome-wide distributions to test for recent positive selection. Adapted from the long-range haplotype (LRH) test, the WGLRH test uses patterns of linkage disequilibrium (LD) to identify regions with extremely low historic recombination. Common haplotypes with significantly longer than expected ranges of LD given their frequencies are identified as putative signatures of recent positive selection. In addition, we have also determined the ancestral alleles of SNPs by genotyping chimpanzee and gorilla DNA, and have identified SNPs where the non-ancestral alleles have risen to extremely high frequencies in human populations, termed 'flipped SNPs'. Combining the haplotype test and the flipped SNPs determination, the WGLRH test serves as an unbiased genome-wide screen for regions under putative selection, and is potentially applicable to the study of other human populations.   RESULTS Using WGLRH and high-density oligonucleotide arrays interrogating 116 204 SNPs, we rapidly identified putative regions of positive selection in three populations (Asian, Caucasian, African-American), and extended these observations to a fourth population, Yoruba, with data obtained from the International HapMap consortium. We mapped significant regions to annotated genes. While some regions overlap with genes previously suggested to be under positive selection, many of the genes have not been previously implicated in natural selection and offer intriguing possibilities for further study.   AVAILABILITY the programs for the WGLRH algorithm are freely available and can be downloaded at http://www.affymetrix.com/support/supplement/WGLRH_program.zip.	algorithm;alleles;antivirus software;electronic signature;genome scan;genotype determination;haplotypes;hearing loss, high-frequency;imprinting (psychology);international hapmap project;linkage disequilibrium;natural selection;pan troglodytes;population;positive selection;sensor;single nucleotide polymorphism	Chun Zhang;Dione K. Bailey;Tarif Awad;Guoying Liu;Guoliang Xing;Manqiu Cao;Venu Valmeekam;Jacques Retief;Hajime Matsuzaki;Margaret Taub;Mark Seielstad;Giulia C. Kennedy	2006	Bioinformatics	10.1093/bioinformatics/btl365	biology;bioinformatics;allele frequency;high frequency;genetics;directional selection	Comp.	2.982120840162991	-53.51220973030235	14997
0c9a6dc86ecb9dd2adc147a693a1d65b745b430a	object recognition using tactile measurements: kernel sparse coding methods	tactile sensors data structures dexterous manipulators object recognition;tactile measurement dexterous robot joint sparse coding kernel sparse coding object recognition;encoding kernel silicon object recognition sensors training robots;multifinger tactile sequence data classification problem object recognition tactile measurement kernel sparse coding method dexterous robot fine motor control assistance surgery undersea welding mechanical manipulation tactile sensor robotic fingertip dynamic time warping method tactile data representation	Dexterous robots have emerged in the last decade in response to the need for fine-motor-control assistance in applications as diverse as surgery, undersea welding, and mechanical manipulation in space. Crucial to the fine operation and contact environmental perception are tactile sensors that are fixed on the robotic fingertips. These can be used to distinguish material texture, roughness, spatial features, compliance, and friction. In this paper, we regard the investigated tactile data as time sequences, of which dissimilarity can be evaluated by the popular dynamic time warping method. A kernel sparse coding method is therefore developed to address the tactile data representation and classification problem. However, the naive use of sparse coding neglects the intrinsic relation between individual fingers, which simultaneously contact the object. To tackle this problem, we develop a joint kernel sparse coding model to solve the multifinger tactile sequence classification problem. In this model, the intrinsic relations between fingers are explicitly taken into account using the joint sparse coding, which encourages all of the coding vectors to share the same sparsity support pattern. The experimental results show that the joint sparse coding achieves better performance than conventional sparse coding.	data (computing);dynamic time warping;feature model;image warping;kernel (operating system);neural coding;outline of object recognition;robot welding;sparse matrix;statistical classification;tactile sensor	Huaping Liu;Di Guo;Fuchun Sun	2016	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2016.2514779	computer vision;computer science;machine learning	Robotics	34.48541841664226	-55.69571509482441	15021
6b6f7242c35cf3dbadcfc8c318519f8b80117f28	blood electrolyte homeostasis of rat after high-intensive swimming training	magnesium;animals;biomedical measurements;rats;high intensive swimming training;cardiology;blood electrolyte homeostasis;blood pressure;testing;sports medicine;ca blood electrolyte homeostasis rat high intensive swimming training sports medicine neuronal activity cardiac excitability neuromuscular transmission muscular contraction vasomotor tone blood pressure sweat urine excretion mg;calcium;muscular contraction;gases;sweat;cardiac excitability;magnesium animals analysis of variance calcium neuromuscular blood pressure rats biomedical measurements gases testing;electrolytes;urine excretion;ica2 img2;anion gap;ca;mg;analysis of variance;neuromuscular;vasomotor tone;rat;ca2;neuromuscular transmission;neurophysiology;bioelectric phenomena;blood pressure measurement;ica2 img2 mg2 ca2;mg2;potassium;athletic performance;neurophysiology bioelectric phenomena blood pressure measurement calcium cardiology electrolytes magnesium muscle;neuronal activity;muscle;physical performance	In sports medicine, very little attention has been given to magnesium compared with potassium and calcium. Magnesium ions (Mg<sup>2+</sup>) play a central role of neuronal activity, cardiac excitability, neuromuscular transmission, muscular contraction, vasomotor tone, and blood pressure significantly related to physical performance. Exercise is a potent stressor that appears to lead to magnesium depletion through alterations on blood magnesium levels as well as increased sweat and urine excretion. After exhausted swimming (3-4 hrs) in rats, the iCa<sup>2+</sup>/iMg<sup>2+</sup> were significantly decreased. The Na<sup>+</sup>, iCa<sup>2+</sup>, iMg<sup>2+</sup>, and anion gap were significantly increased. These data suggest that exercise could alter blood iMg<sup>2+</sup>, iCa<sup>2+</sup> and the ratio of iCa<sup>2+</sup>/iMg<sup>2+</sup> and point to important uses for iMg<sup>2+</sup> and the ratio of iCa<sup>2+</sup>/iMg<sup>2+</sup> during the training and examination of athletic performance in sports medicine.	depletion region;homeostasis;numerical aperture	Seol-Hee Jeon;Mun-Young Lee;Shang-Jin Kim;Md. Mizanur Rahman;Gi-Beum Kim;Jin-Shang Kim;Hyung-Sub Kang	2008	2008 International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2008.368	endocrinology;sports medicine;medicine;calcium;blood pressure;magnesium;neurophysiology;diabetes mellitus;surgery	DB	17.388656845198458	-83.27119526344877	15028
4e875bf07528232479ff5588867ec575ad25b259	identification of a functional connectome for long-term fear memory in mice	animals;mice;brain;immunohistochemistry;nerve net;fear;mice mutant strains;memory	Long-term memories are thought to depend upon the coordinated activation of a broad network of cortical and subcortical brain regions. However, the distributed nature of this representation has made it challenging to define the neural elements of the memory trace, and lesion and electrophysiological approaches provide only a narrow window into what is appreciated a much more global network. Here we used a global mapping approach to identify networks of brain regions activated following recall of long-term fear memories in mice. Analysis of Fos expression across 84 brain regions allowed us to identify regions that were co-active following memory recall. These analyses revealed that the functional organization of long-term fear memories depends on memory age and is altered in mutant mice that exhibit premature forgetting. Most importantly, these analyses indicate that long-term memory recall engages a network that has a distinct thalamic-hippocampal-cortical signature. This network is concurrently integrated and segregated and therefore has small-world properties, and contains hub-like regions in the prefrontal cortex and thalamus that may play privileged roles in memory expression.	connectome;global network;memory disorders;memory, long-term;prefrontal cortex;small-world experiment;thalamic structure;usb hub	Anne L. Wheeler;Cátia M. Teixeira;Afra H. Wang;Xuejian Xiong;Natasa Kovacevic;Jason P. Lerch;Anthony Randal McIntosh;John Parkinson;Paul W. Frankland	2013		10.1371/journal.pcbi.1002853	immunohistochemistry;memory consolidation;memory	ML	18.653906243024288	-77.47146728445145	15033
524a091ee53c22db69b1780f318de927fb8d934a	neuromodulation of early multisensory interactions in the visual cortex	settore m psi 02 psicobiologia e psicologia fisiologica	Merging information derived from different sensory channels allows the brain to amplify minimal signals to reduce their ambiguity, thereby improving the ability of orienting to, detecting, and identifying environmental events. Although multisensory interactions have been mostly ascribed to the activity of higher-order heteromodal areas, multisensory convergence may arise even in primary sensory-specific areas located very early along the cortical processing stream. In three experiments, we investigated early multisensory interactions in lower-level visual areas, by using a novel approach, based on the coupling of behavioral stimulation with two noninvasive brain stimulation techniques, namely, TMS and transcranial direct current stimulation (tDCS). First, we showed that redundant multisensory stimuli can increase visual cortical excitability, as measured by means of phosphene induction by occipital TMS; such physiological enhancement is followed by a behavioral facilitation through the amplification of signal intensity in sensory-specific visual areas. The more sensory inputs are combined (i.e., trimodal vs. bimodal stimuli), the greater are the benefits on phosphene perception. Second, neuroelectrical activity changes induced by tDCS in the temporal and in the parietal cortices, but not in the occipital cortex, can further boost the multisensory enhancement of visual cortical excitability, by increasing the auditory and tactile inputs from temporal and parietal regions, respectively, to lower-level visual areas.	accommodation phosphene disorder;cerebral cortex;convergence (action);deep brain stimulation;experiment;inductive reasoning;interaction;neuromodulation (medicine);occipital region trauma;occipital lobe;parietal lobe;sensor;transcranial direct current stimulation;transcranial magnetic stimulation;vergence;benefit;facilitation	Silvia Convento;Giuseppe Vallar;Chiara Galantini;Nadia Bolognini	2013	Journal of Cognitive Neuroscience	10.1162/jocn_a_00347	psychology;cognitive psychology;neuroscience;communication	ML	17.412738296403457	-77.77397595202362	15045
cc9cbcdd1a229fa79e2c5d5010ff3cd858365bc2	determination of the curling behavior of a preformed cochlear implant electrode array	nonlinear least squares;image processing;perimodiolar electrode array;curling behavior;minimally invasive surgery;electrode carrier;simulation model;cochlear implant;image guided surgery	Accurate insertion of a cochlear implant electrode array into the cochlea’s helical shape is a crucial step for residual hearing preservation. In image-guided surgery, especially using an automated insertion tool, the overall accuracy of the operative procedure can be improved by adapting the electrode array’s intracochlear movement to the individual cochlear shape. The curling characteristic of a commercially available state-of-the-art preformed electrode array (Cochlear Ltd. Contour AdvanceTM Electrode Array) was determined using an image-processing algorithm to detect its shape in series of images. An automatic image-processing procedure was developed using Matlab and the Image Processing Toolbox (MathWorks, Natick, Massachusetts, USA) to determine the complete curvature of the electrode array by identifying the 22 platinum contacts of the electrode. A logarithmic spiral was used for a comprehensive mathematical description of the shape of the electrode array. A fitting algorithm for nonlinear least-squares problems was used to provide a complete mathematical description of the electrode array. The system was tested for curling behavior as a function of stylet extraction using nine Contour Advance Research Electrodes (RE) and additionally for nine Contour Advance Practice Electrodes (PE). All arrays show a typical pattern of curling with adequate predictability after the first 2 or 3 millimeters of stylet extraction. Although non-negligible variations in the overall curling behavior were detected, the electrode arrays show a characteristic movement due to the stylet extraction and only vary minimally after this initial phase. These results indicate that the risk of intracochlear trauma can be reduced if the specific curling behavior of the electrode carrier is incorporated into the insertion algorithm. Furthermore, the determination of the curling behavior is an essential step in computer-aided cochlear implant electrode development. Experimental data are required for accurate evaluation of the simulation model.	acclimatization;biologic preservation;circuit complexity;clinical act of insertion;cochlear implant;cochlear implant procedure;computability;contour line;cumulative trauma disorders;electrodes, implanted;image processing;insertion mutation;ion implantation;ion-selective electrodes;least squares;matlab;mathematics;mechatronics;non-linear least squares;nonlinear system;operative surgical procedures;personalization;platinum;scala tympani;series and parallel circuits;silicones;simulation;stylet, device;surgery, image-guided;surgical replantation;through-hole technology;algorithm;electrode;millimeter	Thomas S. Rau;Omid Majdani;Andreas Hussong;Thomas Lenarz;Martin Leinung	2010	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-010-0520-x	computer vision;image processing;computer science;simulation modeling;non-linear least squares;audiology;surgery	Robotics	25.921908342962954	-87.38405527000866	15085
5dbe54dd6a1dc86df1526a194a326319f0ca9d73	refinement-cut: user-guided segmentation algorithm for translational science	sensitivity and specificity;computer systems;image enhancement;translational medical research;image interpretation computer assisted;magnetic resonance imaging;reproducibility of results;algorithms;pattern recognition automated;humans;subtraction technique;user computer interface;tomography x ray computed	In this contribution, a semi-automatic segmentation algorithm for (medical) image analysis is presented. More precise, the approach belongs to the category of interactive contouring algorithms, which provide real-time feedback of the segmentation result. However, even with interactive real-time contouring approaches there are always cases where the user cannot find a satisfying segmentation, e.g. due to homogeneous appearances between the object and the background, or noise inside the object. For these difficult cases the algorithm still needs additional user support. However, this additional user support should be intuitive and rapid integrated into the segmentation process, without breaking the interactive real-time segmentation feedback. I propose a solution where the user can support the algorithm by an easy and fast placement of one or more seed points to guide the algorithm to a satisfying segmentation result also in difficult cases. These additional seed(s) restrict(s) the calculation of the segmentation for the algorithm, but at the same time, still enable to continue with the interactive real-time feedback segmentation. For a practical and genuine application in translational science, the approach has been tested on medical data from the clinical routine in 2D and 3D.	algorithm;image analysis;post-translational protein processing;real-time clock;real-time web;segmentation action;semiconductor industry;translational research;biologic segmentation	Jan Egger	2014		10.1038/srep05164	computer vision;simulation;computer science;theoretical computer science;magnetic resonance imaging;segmentation-based object categorization;image segmentation;scale-space segmentation	Robotics	37.52022717220355	-83.25924934674065	15099
22e1aefbc68ec882cb684dc4646b187f14b45dd0	improving human brain mapping via joint inversion of brain electrodynamics and the bold signal	goodness of fit;cost function;blood oxygen level dependent;independent component analysis;human brain mapping;inverse method;functional magnetic resonance images;electroencephalography	We present several methods to improve the resolution of human brain mapping by combining information obtained from surface electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) of the same participants performing the same task in separate imaging sessions. As an initial step in our methods we used independent component analysis (ICA) to obtain task-related sources for both EEG and fMRI. We then used that information in an integrated cost function that attempts to match both data sources and trades goodness of fit in one regime for another. We compared the performance and drawbacks of each method in localizing sources for a dual visual evoked response experiment, and we contrasted the results of adding fMRI information to simple EEG-only inversion methods. We found that adding fMRI information in a variety of ways gives superior results to classical minimum norm source estimation. Our findings lead us to favor a method which attempts to match EEG scalp dynamics along with voxel power obtained from ICA-processed blood oxygenation level dependent (BOLD) data; this method of joint inversion enables us to treat the two data sources as symmetrically as possible.	cell respiration;data sources;dual;electroencephalography phase synchronization;independent computing architecture;independent component analysis;loss function;magnetic resonance imaging;outline of brain mapping;visual evoked cortical potential;voxel;blood oxygen level dependent;fmri	Kevin Scott Brown;Stephanie Ortigue;Scott T. Grafton;Jean M. Carlson	2010	NeuroImage	10.1016/j.neuroimage.2009.10.011	psychology;independent component analysis;speech recognition;electroencephalography;artificial intelligence;goodness of fit;communication	ML	17.39600017711311	-80.4186685891746	15110
ac1fde5f0b08a444f3f63e9b5e3265c4836fbb95	deducing molecular similarity using voronoi binding sites	biological macromolecule;diagramme voronoi;etude theorique;integracion numerica;estudio comparativo;isomero geometrico;ligand binding;binding site;macromolecula biologica;fixation ligand;geometrical isomer;etude comparative;site fixation;macromolecule biologique;numerical integration;structure moleculaire;comparative study;estudio teorico;theoretical study;isomere geometrique;diagrama voronoi;integration numerique;fijacion ligando;estructura molecular;sitio fijacion;voronoi diagram;molecular structure	We have devised a new measure of molecular similarity with respect to given simple partitions of space into regions. The similarity is determined by numerical integration of the difference in the optimal interaction between the two molecules and the regions over a large range of interaction parameter values. Compounds differing in empirical formula are differentiated by a single infinite region; cis/trans or ortho/meta/para isomers are distinguishable by two adjacent regions that are half-spaces; and stereoisomers require five regions. This can be viewed as a natural classification of isomers. The concept can also be applied to drug binding studies to determine which molecules may bind alike in a given biological receptor and to elucidate a necessary starting geometry when a binding site is modeled for inhibitors whose experimental binding energies are different.	binding sites;chemical similarity;energy, physics;isomerism;ligand binding domain;numerical analysis;numerical integration;population parameter;source-to-source compiler;stereoisomers;substance abuse detection	Mary P. Bradley;Wendy Richardson;Gordon M. Crippen	1993	Journal of chemical information and computer sciences	10.1021/ci00015a014	stereochemistry;chemistry;voronoi diagram;molecule;numerical integration;binding site;comparative research;mathematics;ligand;mineralogy;quantum mechanics	Comp.	14.437183148071105	-61.22608643551827	15119
2024132d4a0e642e24047ada882d397b99f95c6e	evaluation of changes in t-wave alternans induced by 60-days of immobilization by head-down bed-rest		Prolonged exposure to weightlessness is known to induce cardiovascular deconditioning. The aim of this work was to assess changes in T-wave alternans (TWA) induced by 60 days exposure to simulated microgravity using the −6° head-down bed-rest model (HDBR) and the potential effectiveness of a jump training countermeasure. We hypothesized that TWA could be able to reflect these changes, if they exist. Twenty-four healthy men were recruited at the German Aerospace Center (12 in control group, 12 with applied countermeasure), from which ECG signals were acquired before (PRE), at day 21 of HDBR (HDT21), before the end of HDBR (HDT57), and the day after its conclusion (POST). The index of average alternans (IAA), quantifying the average TWA amplitude, was computed using a fully automated algorithm based on periodic component analysis and the Laplacian likelihood ratio method. A significant increase in HR was found at HDT57 and POST compared to PRE. The IAA showed an increasing trend at POST (median (25th;75th percentile): 0.467(0.381;0.574) μV) compared to PRE (0.337(0.204;0.437) μΥ p=0.18) only in the control group, although no significant differences along HDBR were found. In conclusion, long-term exposure to simulated microgravity did not induce significant alterations in electrical instability measured in terms of nocturnal TWA.	algorithm;immobiliser;instability;simulation	Alba Martín;Violeta Monasterio;Pablo Laguna;Juan Pablo Martínez;Enrico G. Caiani	2017	2017 Computing in Cardiology (CinC)		cardiovascular deconditioning;percentile;t wave alternans;bed rest;weightlessness;internal medicine;cardiology;medicine	HCI	16.440725058309557	-84.39001770205064	15136
c858b81b7cc5a4ab9d6f055efde9bef3c5b0e4c6	surface area under the motion curve as a new tool for gait recognition		The main goal of Motion Capture based modeling is to understand the essence of human gait phenomenon. Actually the follow- ing methods can propose to recognize gaits: The Dynamic Time Warping. method is based on dynamic programming and is widely used for differ- ent time-series comparison applications (like voice recognition. Spectrum analysis of the motion signal leads to interesting results concerning per- son identification The proposed method for gait recognition that uses var- ious techniques of comparing the surface areas under the motion curves offers very accurate results. One of the most challenging problems in markerless motion capture, but offering a widespread application poten- tial is that of estimation of body pose from single video sequence.		Ryszard Klempous	2013		10.1007/978-3-642-53862-9_26	computer vision;simulation	Vision	47.33478256006507	-54.26187692597829	15175
f56c29616ac621bbf358abdb1fae19c1529fb60c	3d deep learning for detecting pulmonary nodules in ct scans		Objective To demonstrate and test the validity of a novel deep-learning-based system for the automated detection of pulmonary nodules.   Materials and Methods The proposed system uses 2 3D deep learning models, 1 for each of the essential tasks of computer-aided nodule detection: candidate generation and false positive reduction. A total of 888 scans from the LIDC-IDRI dataset were used for training and evaluation.   Results Results for candidate generation on the test data indicated a detection rate of 94.77% with 30.39 false positives per scan, while the test results for false positive reduction exhibited a sensitivity of 94.21% with 1.789 false positives per scan. The overall system detection rate on the test data was 89.29% with 1.789 false positives per scan.   Discussion An extensive and rigorous validation was conducted to assess the performance of the proposed system. The system demonstrated a novel combination of 3D deep neural network architectures and demonstrates the use of deep learning for both candidate generation and false positive reduction to be evaluated with a substantial test dataset. The results strongly support the ability of deep learning pulmonary nodule detection systems to generalize to unseen data. The source code and trained model weights have been made available.   Conclusion A novel deep-neural-network-based pulmonary nodule detection system is demonstrated and validated. The results provide comparison of the proposed deep-learning-based system over other similar systems based on performance.		Ross Gruetzemacher;Ashish Gupta;David B. Paradice	2018	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocy098	data mining;radiology;deep learning;medicine;artificial intelligence	AI	31.64228925568416	-76.30037900923479	15185
e2e1bb7a0c698ffce632903f5d846d799366c1cf	arabic spotted words recognition system based on hmm approach to control a didactic manipulator	parallel port interface;hidden markov models control systems automatic control robotics and automation automatic speech recognition application software manipulators robustness cepstral analysis microcomputers;manipulators;didactic manipulator control;cepstral coefficient;personal computer;hidden markov model;degree of freedom;speech recognition hidden markov models manipulators;hmm;stepper motor;voice command system;vocal phrase;automatic speech recognition;robot arm;hidden markov models;vocal phrase arabic spotted words recognition hmm didactic manipulator control voice command system robot arm hidden markov model cepstral coefficient automatic speech recognition parallel port interface stepper motor;word recognition;speech recognition;real time application;arabic spotted words recognition	A voice command system for a robot arm is designed as a part of a research project. The methodology adopted is based on a spotted words recognition system based on a robust HMM (Hidden Markove Model) technique with cepstral coefficients as parameters used in automatic speech recognition system. To implement the approach on a real-time application, a Personal Computer parallel port interface was designed to control the movement of a set of stepper motors. The user can control the movements of five degree of freedom (DOF) for a robot arm using a vocal phrase containing spotted words. Other applications are proposed.	cepstrum;coefficient;didactic organisation;hidden markov model;parallel port;personal computer;real-time computing;real-time transcription;robot;robotic arm;speech recognition	Mohamed Fezari;Hamza Attoui;Mouldi Bedda	2008	2008 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2008.4493648	speech recognition;robotic arm;word recognition;computer science;artificial intelligence;machine learning;degrees of freedom;hidden markov model	Robotics	-2.7280510012931054	-82.83626759262536	15215
7b7802ed891276b4b4df606f5ab9f43799a4e926	a pca-based thresholding strategy for group studies of brain connectivity - with applications to resting state fmri	graph theory;alzheimers disease pca based thresholding strategy resting state fmri functional brain connectivity bold fmri recordings graph representations binary graphs correlation value thresholding pairwise time series brain locations data driven approach subject specific graphs simple synthetic graphs healthy elderly people genetic risk;bold fmri recordings;brain;correlation senior citizens genetics alzheimer s disease time series analysis educational institutions time measurement;senior citizens;fmri;time measurement;graph representations;correlation value thresholding;alzheimers disease fmri functional brain connectivity graph analysis graph thresholding;genetic risk;alzheimers disease;pairwise time series;genetics;healthy elderly people;graph thresholding;time series analysis;resting state fmri;medical image processing;principal component analysis;pca based thresholding strategy;alzheimer s disease;graph analysis;diseases;correlation;subject specific graphs;binary graphs;simple synthetic graphs;brain locations;principal component analysis biomedical mri brain diseases graph theory medical image processing;functional brain connectivity;biomedical mri;data driven approach	Functional brain connectivity can be measured from BOLD fMRI recordings using graph representations incorporating inter-regional similarity of time courses. The resulting binary graphs are commonly constructed using thresholding of correlation values between pairwise time series anchored at their corresponding brain locations, representing the nodes in the graph. In this work we propose a new data driven approach to the challenge of selecting correlation threshold levels in group studies. Our approach addresses a collection of subject-specific graphs, thresholded at different levels. It combines information such that variability is preserved, and detection of sub-groups in the sample, having specific properties, is facilitated. The method is illustrated on simple synthetic graphs, and also tested on data from a resting state fMRI study of healthy elderly people with varying genetic risk of Alzheimers disease.	resting state fmri;spatial variability;synthetic intelligence;thresholding (image processing);time series	Erik A. Hanson;Erling Tjelta Westlye;Arvid Lundervold	2014	2014 Southwest Symposium on Image Analysis and Interpretation	10.1109/SSIAI.2014.6806029	power graph analysis;computer science;artificial intelligence;graph theory;machine learning;time series;pattern recognition;mathematics;correlation;resting state fmri;time;principal component analysis	ML	23.560527369850536	-78.84801950173343	15296
8531ff13a352f8c73912a151db24a5dd486ce89e	classamp: a prediction tool for classification of antimicrobial peptides	sequence features;svm antibacterial antifungal antimicrobial antiviral prediction algorithm random forests;antiviral;antimicrobial peptide classification;drugs;biology computing;peptides;support vector machines antibacterial activity biology computing drugs molecular biophysics proteins random sequences;antiviral activity;support vector machines;protein sequence;random sequences;training;antimicrobial;antifungal activity;drug discovery programs;random forests;algorithms anti infective agents peptides support vector machines;anti bacterial support vector machines anti fungal radio frequency peptides predictive models training;radio frequency;antiinfective agents;proteins;antibacterial activity;anti bacterial;prediction algorithm;molecular biophysics;antifungal;antibacterial;anti fungal;predictive models;svm;classamp;antiviral activity classamp antimicrobial peptide classification antiinfective agents sequence features drug discovery programs random forests support vector machines svm protein sequence antibacterial activity antifungal activity	Antimicrobial peptides (AMPs) are gaining popularity as anti-infective agents. Information on sequence features that contribute to target specificity of AMPs will aid in accelerating drug discovery programs involving them. In this study, an algorithm called ClassAMP using Random Forests (RFs) and Support Vector Machines (SVMs) has been developed to predict the propensity of a protein sequence to have antibacterial, antifungal, or antiviral activity. ClassAMP is available at http://www.bicnirrh.res.in/classamp/.	algorithm;anti-bacterial agents;antifungal agents;antimicrobial cationic peptides;antiviral agents;drug discovery;random forest;sensitivity and specificity;staphylococcal protein a;support vector machine	Shaini Joseph;Shreyas Karnik;Pravin Nilawe;Vaidyanathan K. Jayaraman;Susan Idicula-Thomas	2012	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2012.89	biology;support vector machine;computer science;bioinformatics;machine learning;microbiology;molecular biophysics	Comp.	8.93695704334677	-56.137713915668776	15311
f55ccea1106da4342e5c3f887f921176a2baa412	application of higher-order spectra for automated grading of diabetic maculopathy	qa75 electronic computers computer science	Diabetic macular edema (DME) is one of the most common causes of visual loss among diabetes mellitus patients. Early detection and successive treatment may improve the visual acuity. DME is mainly graded into non-clinically significant macular edema (NCSME) and clinically significant macular edema according to the location of hard exudates in the macula region. DME can be identified by manual examination of fundus images. It is laborious and resource intensive. Hence, in this work, automated grading of DME is proposed using higher-order spectra (HOS) of Radon transform projections of the fundus images. We have used third-order cumulants and bispectrum magnitude, in this work, as features, and compared their performance. They can capture subtle changes in the fundus image. Spectral regression discriminant analysis (SRDA) reduces feature dimension, and minimum redundancy maximum relevance method is used to rank the significant SRDA components. Ranked features are fed to various supervised classifiers, viz. Naive Bayes, AdaBoost and support vector machine, to discriminate No DME, NCSME and clinically significant macular edema classes. The performance of our system is evaluated using the publicly available MESSIDOR dataset (300 images) and also verified with a local dataset (300 images). Our results show that HOS cumulants and bispectrum magnitude obtained an average accuracy of 95.56 and 94.39 % for MESSIDOR dataset and 95.93 and 93.33 % for local dataset, respectively.	abnormality of the macula;adaboost;area under curve;bispectrum;classification;diabetes mellitus;diabetic neuropathies;extraction;exudate;histopathologic grade;icl direct machine environment;linear discriminant analysis;macular retinal edema;naive bayes classifier;patients;projections and predictions;radon;relevance;robustness (computer science);silo (dataset);supervised learning;support vector machine;viz: the computer game;wavelet analysis;wavelet transform;diabetic macular edema;hearing impairment	Muthu Rama Krishnan Mookiah;U. Rajendra Acharya;Vinod Chandran;Roshan Joy Martis;Jen-Hong Tan;Joel E. W. Koh;Chua Kuang Chua;Louis Tong;Augustinus Laude	2015	Medical & Biological Engineering & Computing	10.1007/s11517-015-1278-7	computer vision;speech recognition;medicine;computer science;engineering;mathematics;optics;physics	Vision	32.380521462445984	-77.5319523353179	15329
8a530e792d1555ac0e18dec3c2db5b4a0ac403bc	image filtering techniques and vlsi architectures for efficient data extraction in shell rendering	vlsi architectures;image filtering;filtering;image preprocessing;search engine;range query;interactive data reduction;real time data reduction;search engines;volume rendering;very large scale integration;ultrasonic imaging;real time;biomedical nmr;image filtering techniques;image classification;3d imagery;real time data;data mining;classification;chip;3d mri studies image filtering techniques vlsi architectures data extraction shell rendering volume rendering interactive data reduction real time data reduction pci based search engine full custom vlsi chip classification opacity assignment multi spectral voxel data image preprocessing interactive inspection procedures 3d imagery 3d ultrasonics;opacity assignment;3d mri studies;feedback;data extraction;application specific integrated circuits;feature extraction;medical image processing;magnetic resonance imaging;shell rendering;data visualization;vlsi;interactive inspection procedures;volume visualization;hardware design;multi spectral voxel data;digital signal processing chips;pci based search engine;search problems;data reduction;multi spectral;numerical experiment;rendering computer graphics;3d ultrasonics;biomedical ultrasonics;filtering theory;feature extraction vlsi rendering computer graphics biomedical ultrasonics biomedical nmr image classification application specific integrated circuits digital signal processing chips search problems medical image processing data reduction filtering theory;filtering very large scale integration data mining rendering computer graphics data visualization hardware search engines magnetic resonance imaging ultrasonic imaging feedback;hardware;vlsi architecture;full custom vlsi chip	This paper presents an approach to volume rendering based on real-time and interactive data reduction prior to volume visualization. The underlying hardware design of a PCI based search engine is introduced including the architecture of a full custom VLSI chip that based on combinations of general range queries performs a real-time classification/opacity assignment of the multi-spectral voxel data. Various image preprocessing techniques are presented. These techniques are used to enhance the potential of the subsequent real-time data extraction, In particular, we describe how the real-time data extraction facility can be utilized to develop interactive inspection procedures for 3D imagery. The numerical experiments include 3D ultrasonics and 3D MRI studies.	very-large-scale integration	Bjørn Olstad;Erik Steen;Arne Halaas	1995		10.1109/ICIP.1995.537427	computer vision;computer science;theoretical computer science;magnetic resonance imaging;very-large-scale integration;data visualization;search engine;computer graphics (images)	EDA	41.63200892905884	-88.87575817009218	15360
aa64f90fa47cd2b457925229da03399ff7ad4b5e	errors with manual phenotype validation: case study and implications				Polina V. Kukhareva;Catherine J. Staes;Tyler J. Tippetts;Phillip B. Warner;David Shields;Heather Mueller;Kevin Noonan;Kensaku Kawamoto	2015			cancer research;phenotype;biology	HCI	1.1685284866963703	-64.212766943357	15365
af84af080b7785c6421169083cdc2624b90ae252	workshop introduction		Electron cryo-microscopy (cryoEM) is a rapidly maturing methodology in structural biology, which now enables the determination of 3D structures of molecules, macromolecular complexes and cellular components at resolutions as high as 3.5Å, bridging the gap between light microscopy and X-ray crystallography/NMR. In recent years structures of many complex molecular machines have been visualized using this method. Single particle reconstruction, the most widely used technique in cryoEM, has recently demonstrated the capability of producing structures at resolutions approaching those of X-ray crystallography, with over a dozen structures at better than 5 Å resolution published to date. This method represents a significant new source of experimental data for molecular modeling and simulation studies. CryoEM derived maps and models are archived through EMDataBank.org joint deposition services to the EM Data Bank (EMDB) and Protein Data Bank (PDB), respectively. CryoEM maps are now being routinely produced over the 3 - 30 Å resolution range, and a number of computational groups are developing software for building coordinate models based on this data and developing validation techniques to better assess map and model accuracy. In this workshop we will present the results of the first cryoEM modeling challenge, in which computational groups were asked to apply their tools to a selected set of published cryoEM structures. We will also compare the results of the various applied methods, and discuss the current state of the art and how we can most productively move forward.	archive;bridging (networking);chemical vapor deposition;computation;computed tomography scanning systems;cryoelectron microscopy;crystallography;de novo transcriptome assembly;emdatabank.org;electron;light microscopy;map;map analysis;molecular modelling;physical restraint equipment (device);protein data bank;registration;scientific publication;simulation;single particle analysis;united states national institutes of health;funding grant;molecular modeling	Steven J. Ludtke;Catherine L. Lawson;Gerard J. Kleywegt;Helen M. Berman;Wah Chiu	2011	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing			Comp.	12.820808877411707	-60.36814961657878	15372
228de021449678a97656b6f3f367c4f45fb28dab	copasi - a complex pathway simulator	modelizacion;software;plataforma;interfaz grafica;optimisation;simulation and modeling;software tool;optimizacion;langage c;logiciel;reaction biochimique;graphical interface;sistema;stochastic simulation;random number generation;simulation;bioinformatique;simulacion;platform;original document;analisis matematico;mathematical analysis;document original;modelisation;c language;system;generation nombre aleatoire;documento original;system biology;logicial;optimization;source code;systeme;bioinformatica;biochemical reaction;analyse mathematique;modeling;interface graphique;hybrid determinist;plateforme;generacion numero aleatorio;lenguaje c;reaccion bioquimica;open source;bioinformatics	MOTIVATION Simulation and modeling is becoming a standard approach to understand complex biochemical processes. Therefore, there is a big need for software tools that allow access to diverse simulation and modeling methods as well as support for the usage of these methods.   RESULTS Here, we present COPASI, a platform-independent and user-friendly biochemical simulator that offers several unique features. We discuss numerical issues with these features; in particular, the criteria to switch between stochastic and deterministic simulation methods, hybrid deterministic-stochastic methods, and the importance of random number generator numerical resolution in stochastic simulation.   AVAILABILITY The complete software is available in binary (executable) for MS Windows, OS X, Linux (Intel) and Sun Solaris (SPARC), as well as the full source code under an open source license from http://www.copasi.org.	biochemical processes;copasi;executable;linux;microsoft windows;numerical analysis;open-source license;open-source software;random number generation;sparc;simulation;source code;usability	Stefan Hoops;Sven Sahle;Ralph Gauges;Christine Lee;Jürgen Pahle;Natalia Simus;Mudita Singhal;Liang Xu;Pedro Mendes;Ursula Kummer	2006	Bioinformatics	10.1093/bioinformatics/btl485	simulation;systems modeling;random number generation;computer science;bioinformatics;stochastic simulation;graphical user interface;system;platform;systems biology;algorithm;source code	Comp.	-4.12164749777622	-54.95704612442273	15391
3aed2587078ca97d110c068f00f369b6ae74bb75	development of automatic filtering system for individually unpleasant data detected by pupil-size change	databases;support vector machines;a classifier;information filtering;pupil size;emotion classification automatic filtering system individually unpleasant data detection pupil size change individual unpleasant emotion multimedia database support vector machine classifier single trial data emotional pictures picture database unpleasant information filtering bag of features scheme image search;image classification;emotion recognition;support vector machines pupil size emotion database search a classifier;wavelet transforms;accuracy;visualization;emotion;feature extraction;multimedia databases;visual databases emotion recognition image classification image retrieval information filtering multimedia databases support vector machines;database search;visual databases;support vector machines feature extraction wavelet transforms accuracy visualization databases;image retrieval	We proposed an automatic filtering system to classify individual unpleasant emotions represented by pupil-size change and to remove similar images from a multimedia database. The support vector machines classifier was applied to single-trial data of the pupil size and indicated the possibility of the accurate judgment of individually unpleasant states immediately after looking at emotional pictures. We then constructed the framework to automatically filter such unpleasant information from a picture database, using the bag of features scheme to search for similar images.	support vector machine	Koji Kashihara;Momoyo Ito;Minoru Fukumi	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6084180	computer vision;visualization;emotion;feature extraction;image retrieval;computer science;machine learning;pattern recognition;data mining	Robotics	36.51732170640466	-63.19720597123752	15396
5937916332af16c5d978159e329e3bddf1e41f28	a fully automated calibration method for an optical see-through head-mounted operating microscope with variable zoom and focus	augmented reality biomedical optical imaging calibration patient treatment;indexing terms;three dimensional;algorithms calibration equipment design equipment failure analysis head protective devices humans image enhancement image interpretation computer assisted imaging three dimensional microscopy microsurgery optics reproducibility of results sensitivity and specificity surgery computer assisted user computer interface;computer aided surgery;image guided therapy;field of view;medical device;augmentation graphics fully automated calibration optical see through head mounted operating microscope image guided therapy medical devices augmented reality visualization video see through systems semitransparent mirrors modified endoscopes binocular varioscope ms ar;patient treatment;head mounted display augmented reality computer aided surgery;biomedical optical imaging;camera calibration;augmented reality;frame of reference;optical microscopy calibration biomedical optical imaging focusing optical devices delay medical treatment displays biomedical imaging augmented reality;calibration;head mounted display	Ever since the development of the first applications in image-guided therapy (IGT), the use of head-mounted displays (HMDs) was considered an important extension of existing IGT technologies. Several approaches to utilizing HMDs and modified medical devices for augmented reality (AR) visualization were implemented. These approaches include video-see through systems, semitransparent mirrors, modified endoscopes, and modified operating microscopes. Common to all these devices is the fact that a precise calibration between the display and three-dimensional coordinates in the patient's frame of reference is compulsory. In optical see-through devices based on complex optical systems such as operating microscopes or operating binoculars-as in the case of the system presented in this paper-this procedure can become increasingly difficult since precise camera calibration for every focus and zoom position is required. We present a method for fully automatic calibration of the operating binocular Varioscope/spl trade/ M5 AR for the full range of zoom and focus settings available. Our method uses a special calibration pattern, a linear guide driven by a stepping motor, and special calibration software. The overlay error in the calibration plane was found to be 0.14-0.91 mm, which is less than 1% of the field of view. Using the motorized calibration rig as presented in the paper, we were also able to assess the dynamic latency when viewing augmentation graphics on a mobile target; spatial displacement due to latency was found to be in the range of 1.1-2.8 mm maximum, the disparity between the true object and its computed overlay represented latency of 0.1 s. We conclude that the automatic calibration method presented in this paper is sufficient in terms of accuracy and time requirements for standard uses of optical see-through systems in a clinical environment.	augmented reality;binocular disparity;binocular vision;calibration;camera resectioning;displacement mapping;endoscopes;graphics;head-mounted display;imagery;information governance;medical devices;microscope device component;newton's method;operating room;operating tables;overlay device component;patients;psychologic displacement;receiver operating characteristic;requirement;spironolactone;stepping level	Michael Figl;Christopher Ede;Johann Hummel;Felix Wanschitz;Rolf Ewers;Helmar Bergmann;Wolfgang Birkfellner	2005	IEEE Transactions on Medical Imaging	10.1109/TMI.2005.856746	frame of reference;three-dimensional space;computer vision;augmented reality;calibration;camera resectioning;simulation;index term;field of view;computer science;optical head-mounted display;computer graphics (images)	Visualization	40.976354640140315	-85.84545293348067	15403
6c2602a51dfb6104bf438ba0b4e2b2379de168d0	performance validation of neural network based 13c nmr prediction using a publicly available data source	mean deviation;mean error;chemical shift;neural network;open source	"""The validation of the performance of a neural network based 13C NMR prediction algorithm using a test set available from an open source publicly available database, NMRShiftDB, is described. The validation was performed using a version of the database containing ca. 214,000 chemical shifts as well as for two subsets of the database to compare performance when overlap with the training set is taken into account. The first subset contained ca. 93,000 chemical shifts that were absent from the ACD\CNMR DB, the """"excluded shift set"""" used for training of the neural network and the ACD\CNMR prediction algorithm, while the second contained ca. 121,000 shifts that were present in the ACD\CNMR DB training set, the """"included shift set"""". This work has shown that the mean error between experimental and predicted shifts for the entire database is 1.59 ppm, while the mean deviation for the subset with included shifts is 1.47 and 1.74 ppm for excluded shifts. Since similar work has been reported online for another algorithm we compared the results with the errors determined using Robien's CNMR Neural Network Predictor using the entire NMRShiftDB for program validation."""		K. A. Blinov;Y. D. Smurnyy;M. E. Elyashberg;T. S. Churanova;M. Kvasha;Christoph Steinbeck;B. A. Lefebvre;A. J. Williams	2008	Journal of chemical information and modeling	10.1021/ci700363r	chemical shift;computer science;machine learning;data mining;database;mean squared error;absolute deviation;artificial neural network;statistics	Vision	10.721158965342028	-55.69294115746974	15404
a3a2db9c179e2e942837022d92db33d8f08106da	automatic quantification of myocardial infarction from delayed enhancement mri	myocardium;automatic;per slice analysis automatic quantification myocardial infarction mri delay image enhancement image segmentation magnetic resonance imaging tissue gaussian mixture model spatial weighted fuzzy clustering infarct core image thresholding global volumetric analysis;pattern clustering;biological tissues;image segmentation;gaussian processes;viability;fuzzy set theory;automatic myocardium mri viability;image enhancement;myocardial infarct;medical image processing;mri;clinical practice;myocardium image segmentation silicon magnetic resonance imaging helium heart manuals;delayed enhancement;pattern clustering biological tissues biomedical mri fuzzy set theory gaussian processes image enhancement image segmentation medical image processing;biomedical mri	In this study, we developed a novel method for a fullyautomatic segmentation and quantification of myocardialinfarction from delay enhancement - magnetic resonanceimaging. Hyper-enhanced region corresponding to the infarctcore is separated from normal tissue via a Gaussian mixturemodel. Then, peri-infarct area is determined by spatial-weightedfuzzy clustering. No-reflow area inside the infarct core is alsodefined by an improved thresholding. Results of quantificationare presented in per-slice and global volumetric analysis, as wellas in bull's eye of 16-segment model. Promising results have beenachieved in quantification and segmentation of myocardialinfarct. Our software could be used in clinical practice after acomplete validation.	cluster analysis;reflow soldering;thresholding (image processing)	Vanya V. Valindria;Marion Angue;Nicolas Vignon;Paul Michael Walker;Alexandre Cochet;Alain Lalande	2011	2011 Seventh International Conference on Signal Image Technology & Internet-Based Systems	10.1109/SITIS.2011.83	myocardial infarction;computer vision;computer science;magnetic resonance imaging;machine learning;gaussian process;mathematics;fuzzy set;image segmentation;automatic transmission;statistics	EDA	40.340432998853636	-77.48459816035934	15420
1c569bafc88c03ff01758f5e4d21e9a325d33773	telesurgery via unmanned aerial vehicle (uav) with a field deployable surgical robot	b teleoperation general;bb sensors for teleoperation	Robotically assisted surgery stands to further revolutionize the medical field and provide patients with more effective healthcare. Most robotically assisted surgeries are teleoperated from the surgeon console to the patient where both ends of the system are located in the operating room. The challenge of surgical teleoperation across a long distance was already demonstrated through a wired communication network in 2001. New development has shifted towards deploying a surgical robot system in mobile settings and/or extreme environments such as the battlefield or natural disaster areas with surgeons operating wirelessly. As a collaborator in the HAPs/MRT (High Altitude Platform/Mobile Robotic Telesurgery) project, The University of Washington surgical robot was deployed in the desert of Simi Valley, CA for telesurgery experiments on an inanimate model via wireless communication through an Unmanned Aerial Vehicle (UAV). The surgical tasks were performed telerobotically with a maximum time delay between the surgeon's console (master) and the surgical robot (slave) of 20 ms for the robotic control signals and 200 ms for the video stream. This was our first experiment in the area of Mobile Robotic Telesurgery (MRT). The creation and initial testing of a deployable surgical robot system will facilitate growth in this area eventually leading to future systems saving human lives in disaster areas, on the battlefield or in other remote environments.	aerial photography;broadcast delay;collaborator;experiment;medial rostrotemporal auditory cortex;natural disasters;operating room;patients;remote surgery;robot;streaming media;telecommunications network;telerobotics;unmanned aerial vehicle	Mitchell J. H. Lum;Jacob Rosen;Hawkeye H. I. King;Diana C. W. Friedman;Gina Donlin;Ganesh Sankaranarayanan;Brett M. Harnett;Lynn Huffman;Charles R. Doarn;Timothy J. Broderick;Blake Hannaford	2007	Studies in health technology and informatics		control engineering;embedded system;simulation;engineering	Robotics	5.387520373644507	-91.70465261539458	15431
1f2cd75e2bb172ab5f1f34b068d2cf1b754251ad	a hybrid blob-slice model for accurate and efficient detection of fluorescence labeled nuclei in 3d	animals;experimental method;caenorhabditis elegans;mice;imaging three dimensional;cell nucleus;real time;single cell analysis;large data sets;image processing computer assisted;computational biology bioinformatics;complex data;level of detail;single cell;error correction;microscopy fluorescence;error rate;algorithms;image analysis;shape modeling;combinatorial libraries;high throughput;computer appl in life sciences;open source software;microarrays;bioinformatics;volume data	To exploit the flood of data from advances in high throughput imaging of optically sectioned nuclei, image analysis methods need to correctly detect thousands of nuclei, ideally in real time. Variability in nuclear appearance and undersampled volumetric data make this a challenge. We present a novel 3D nuclear identification method, which subdivides the problem, first segmenting nuclear slices within each 2D image plane, then using a shape model to assemble these slices into 3D nuclei. This hybrid 2D/3D approach allows accurate accounting for nuclear shape but exploits the clear 2D nuclear boundaries that are present in sectional slices to avoid the computational burden of fitting a complex shape model to volume data. When tested over C. elegans, Drosophila, zebrafish and mouse data, our method yielded 0 to 3.7% error, up to six times more accurate as well as being 30 times faster than published performances. We demonstrate our method's potential by reconstructing the morphogenesis of the C. elegans pharynx. This is an important and much studied developmental process that could not previously be followed at this single cell level of detail. Because our approach is specialized for the characteristics of optically sectioned nuclear images, it can achieve superior accuracy in significantly less time than other approaches. Both of these characteristics are necessary for practical analysis of overwhelmingly large data sets where processing must be scalable to hundreds of thousands of cells and where the time cost of manual error correction makes it impossible to use data with high error rates. Our approach is fast, accurate, available as open source software and its learned shape model is easy to retrain. As our pharynx development example shows, these characteristics make single cell analysis relatively easy and will enable novel experimental methods utilizing complex data sets.	computation;developmental process;dudebro: my shit is fucked up so i got to shoot/slice you ii: it's straight-up dawg time;error detection and correction;fluorescence;heart rate variability;image analysis;image plane;level of detail;morphogenesis;open-source software;performance;pharyngeal structure;scalability;scientific publication;throughput;tracer;zebrafish;pharynx development	Anthony Santella;Zhuo Du;Sonja Nowotschin;Anna-Katerina Hadjantonakis;Zhirong Bao	2010		10.1186/1471-2105-11-580	high-throughput screening;image analysis;error detection and correction;dna microarray;word error rate;computer science;bioinformatics;theoretical computer science;level of detail;complex data type;computer graphics (images)	Visualization	48.2717543329154	-86.37147179065894	15443
c8550f9b2878f27c784e448b5a82ec5643bc5d3e	data-driven synthetic cerebrovascular models for validation of segmentation algorithms		We introduce a novel method to generate biologically grounded synthetic cerebrovasculature models in a datadriven fashion. First, the centerlines of vascular filaments embedded in an acquired imaging volume are obtained by a segmentation algorithm. That imaging volume is reconstructed from a graph encoding of the centerline (i.e., generating the model’s ground truth) and the segmentation algorithm is applied to the resultant volume. As the location and characteristics of the vasculature embedded in this volume are known,theaccuracyofthesegmentationalgorithmcanbeassessed. Moreover, because the synthetic volume was reconstructed directly from biological data, an assessment is made on embedded filaments that are representative of the topologicalandgeometricalcharacteristicsofthedataset. Webelieve thatsuchmodels will provide the means necessary for the enhanced evaluation of vascular segmentation algorithms.	blood supply aspects;embedded system;embedding;graph - visual representation;ground truth;resultant;segmentation action;synthetic intelligence;algorithm;biologic segmentation	Michael R. Nowak;Yoonsuck Choe	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513456	computer vision;iterative reconstruction;biological data;image segmentation;ground truth;algorithm;artificial intelligence;graph;computer science;segmentation	Visualization	37.23361224232155	-84.04326188345983	15449
2176b4d9bf79d5a12d5a13c3be69eeb93b9c79b2	skin prick test digital imaging system with manual, semiautomatic, and automatic wheal edge detection and area measurement		A novel biomedical instrument for supporting the physician in Skin Prick Test analysis was designed, developed, characterized, and is now ready for clinical trials. Skin Prick Test is the gold standard front-end analysis for diagnosis of allergies in human subjects. The forearm skin is punctured with different allergens and the resulting reaction wheals are analyzed and compared to standard reaction, with larger areas corresponding to stronger allergy to the specific allergen. The wheals inspection and allergy diagnosis are performed, visually and subjectively, by the Medical Doctor. This procedure is laborious and somehow unreliable, being subject to variability both intra- and inter-operator because the doctor subjectivity in detecting and measuring the wheals is significant. Registration of the exam result is rarely available in a digital format, useful for data saving, transmission, retrieval and comparative analyses. Many of the above criticalities of the actual Prick Test manual practice are addressed and resolved by the proposed biomedical instrumentation that makes use of digital image-processing and data storage. In this work, we present a prototype of wheal measurement system, designed, developed and characterized to specifically measure geometry and areas of the allergic reaction wheals in Prick Test clinical exams. After software developments from previous version of the instrument, the wheal-meter now allows manual, semi-automatic, automatic, and fully-automatic operating conditions always providing digital exams output.	color;computer data storage;digital image processing;digital imaging;edge detection;experiment;heart rate variability;iterative reconstruction;prototype;qualitative comparative analysis;repeatability;semiconductor industry;sensor;system of measurement;thresholding (image processing)	Cesare Svelto;Matteo Matteucci;Alexey Pniov;Lorenzo Pedotti	2018	Multimedia Tools and Applications	10.1007/s11042-018-5823-x	forearm skin;computer science;computer vision;medical physics;artificial intelligence;digital imaging;edge detection	Graphics	37.83170564983271	-86.83356454698442	15472
486a155b0fba9262586996394b9faf5b254f14c2	integration of a prognostic gene module with a drug sensitivity module to identify drugs that could be repurposed for breast cancer therapy	drug sensitivity module;prognostic associated module;breast cancer;drug reposition	BACKGROUND Efficiently discovering low risk drugs is important for drug development. However, the heterogeneity in patient population complicates the prediction of the therapeutic efficiency. Drug repositioning aiming to discover new indications of known drugs provides a possible gateway.   METHOD We introduce a novel computational method to identify suitable drugs by using prognosis information of patients. First, we identify prognostic related gene modules, Prognostic Gene Ontology Module (PGOMs), by incorporating multiple functional annotations. Then, we build the drug sensitivity modules based on gene expressions and drug activity patterns. Finally, we analyze the potential effects of drugs on prognostic gene modules and establish the links between PGOMs and drugs.   RESULT AND DISCUSSION With PGOMs generated based on the patient outcome, FDA approved drugs for breast cancer treatment have been successfully identified on one hand; several drugs that have not been approved by FDA, such as Etoposide, have found to strongly associate with the outcome on the other hand. With PGOMs generated based on the patient ER status, Tamoxifen and Exemestane rank at the top of the drug list, suggesting that they may be more specific to ER status of breast cancer. Especially, the rank difference of Exemestane in ER+ group and ER- group is very large, demonstrating that Exemestane may be more specific to ER+ breast cancer and would cause side-effect to ER- breast cancer patients. Our method can not only identify the drugs that could be repurposed for breast cancer therapy, but also can reveal their effective pharmacological mechanisms.		Lida Zhu;Juan Liu	2015	Computers in biology and medicine	10.1016/j.compbiomed.2014.12.019	medicine;pathology;breast cancer	Comp.	6.683508410212271	-56.29256060482856	15476
3b093fa1eb691cc8a4a012c14673a45cbc1a4fd9	establishing a statistical link between network oscillations and neural synchrony	animals;models neurological;simulation and modeling;mice;cortical synchronization;signaling networks;male;macaca mulatta;pyramidal cells;synaptic transmission;signal filtering;cells;feedback physiological;cultured;feedback;cells cultured;electrode potentials;biological clocks;neurological;models statistical;nerve net;humans;generalized linear model;physiological;neurons;statistical;action potentials;computer simulation;models	"""Pairs of active neurons frequently fire action potentials or """"spikes"""" nearly synchronously (i.e., within 5 ms of each other). This spike synchrony may occur by chance, based solely on the neurons' fluctuating firing patterns, or it may occur too frequently to be explicable by chance alone. When spike synchrony above chances levels is present, it may subserve computation for a specific cognitive process, or it could be an irrelevant byproduct of such computation. Either way, spike synchrony is a feature of neural data that should be explained. A point process regression framework has been developed previously for this purpose, using generalized linear models (GLMs). In this framework, the observed number of synchronous spikes is compared to the number predicted by chance under varying assumptions about the factors that affect each of the individual neuron's firing-rate functions. An important possible source of spike synchrony is network-wide oscillations, which may provide an essential mechanism of network information flow. To establish the statistical link between spike synchrony and network-wide oscillations, we have integrated oscillatory field potentials into our point process regression framework. We first extended a previously-published model of spike-field association and showed that we could recover phase relationships between oscillatory field potentials and firing rates. We then used this new framework to demonstrate the statistical relationship between oscillatory field potentials and spike synchrony in: 1) simulated neurons, 2) in vitro recordings of hippocampal CA1 pyramidal cells, and 3) in vivo recordings of neocortical V4 neurons. Our results provide a rigorous method for establishing a statistical link between network oscillations and neural synchrony."""	action potentials;action potential;ca1 field;cognition;computation (action);generalized linear model;hippocampus (brain);in vitro [publication type];mental processes;neural oscillation;neuron;neurons;point process;pyramidal cells;randomness;relevance;scientific publication;spike-triggered average;synchronization (computer science);video-in video-out;virtual synchrony	Pengcheng Zhou;Shawn D. Burton;Adam C. Snyder;Matthew A. Smith;Nathaniel N. Urban;Robert E. Kass	2015		10.1371/journal.pcbi.1004549	computer simulation;filter;neuroscience;computer science;machine learning;standard electrode potential;generalized linear model;feedback;action potential;culture;neurotransmission	ML	19.77847967314737	-73.75593919306442	15498
e1506c228c1c376e7862bf46dae174e95d08b9f0	hemodynamic analysis of surgical correction for patient-specific aortic coarctation with aortic arch hypoplasia by end-to-side anastomosis	heart;biomedical imaging;correction procedure hemodynamic analysis surgical correction patient specific aortic coarctation aortic arch hypoplasia end to side anastomosis coarctation of the aorta coa congenital heart defects aah end to side aortic anastomosis esaa surgical procedure computational fluid dynamics cfd patient specific models pressure drop energy loss el uniformed velocity profile blood flow distribution patient recovery cardiac function;computational fluid dynamics;hemodynamics;surgery blood vessels cardiovascular system computational fluid dynamics haemodynamics medical computing medical disorders paediatrics physiological models;computational modeling;surgery;surgery hemodynamics computational fluid dynamics heart computational modeling biomedical imaging;end to side aortic anastomosis computational fluid dynamics aortic coarctation hemodynamics	Coarctation of the Aorta (CoA) is one of the most serious congenital heart defects in newborns. Approximately 30% of cases of CoA associated with aortic arch hypoplasia (AAH). The end-to-side aortic anastomosis (ESAA) is an effective surgical procedure to be performed for correction of CoA with AAH. In this study, the approach of computational fluid dynamics (CFD) was used to investigate the hemodynamics in patient-specific models of CoA with aortic arch hypoplasia before and after the procedure of ESAA. The results showed that the pressure drop and energy loss (EL) were significantly decreased after the surgical correction was done. Uniformed velocity profile and blood flow distribution were surgically created for patient's recovery of cardiac function. The approach of CFD can be used to disclose and evaluate the hemodynamics of CoA and its correction procedure.	computational fluid dynamics;gnu arch;hemodynamics;velocity (software development)	Le Mao;Jinlong Liu;Haifa Hong;Qi Sun;Junrong Huang;Jinfen Liu;Zhongqun Zhu;Qian Wang	2014	2014 7th International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2014.7002816	medical imaging;medicine;computational fluid dynamics;hemodynamics;anesthesia;computational model;heart;surgery;cardiology	Robotics	30.275353096053774	-85.65139985441344	15534
d573d8d8ec2d0262061cf3c49662fd815935eb78	database-guided segmentation of anatomical structures with complex appearance	optimisation;image segmentation;ultrasound;inference mechanisms;perceptual grouping;medical expert systems;shape image segmentation biomedical imaging medical diagnostic imaging spatial databases ultrasonic imaging heart anatomical structure image databases data systems;a priori knowledge;ultrasound heart image database guided segmentation anatomical structure perceptual grouping task clustering variational approach optimization criterion complex structure appearance data driven paradigm expert annotation large medical database learning problem global rigid transformation shape inference sample based representation joint distribution feature selection mechanism;variational approach;medical information systems;feature extraction;learning problems;feature selection;learning artificial intelligence;visual databases medical information systems image segmentation optimisation medical expert systems feature extraction learning artificial intelligence inference mechanisms;visual databases	"""The segmentation of anatomical structures has been traditionally formulated as a perceptual grouping task, and solved through clustering and variational approaches. However, such strategies require the a priori knowledge to be explicitly defined in the optimization criterion, e.g., """"high-gradient border"""", """"smoothness""""', or """"similar intensity or texture"""". This approach is limited by the validity of underlying assumptions and cannot capture complex structure appearance. This paper introduces database-guided segmentation as a new data-driven paradigm that directly exploits expert annotation of interest structures in large medical databases. Segmentation is formulated as a two-step learning problem. The first step is structure detection where we learn how to discriminate between the object of interest and background. The resulting classifier based on a boosted cascade of simple features also provides a global rigid transformation of the structure. The second step is shape inference where we use a sample-based representation of the joint distribution of appearance and shape annotations. To learn the association between the complex appearance and shape we propose a feature selection mechanism and the corresponding metric. We show that the selected features are better than using directly the appearance and illustrate the performance of the proposed method on a large set of ultrasound heart images."""	algorithm;cluster analysis;curve fitting;database;encode;feature selection;gradient;mathematical optimization;programming paradigm;real-time clock;shape context;simple features;variational principle	Bogdan Georgescu;Xiang Sean Zhou;Dorin Comaniciu;Alok Gupta	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.119	computer vision;a priori and a posteriori;feature extraction;computer science;machine learning;pattern recognition;ultrasound;image segmentation;feature selection	Vision	43.76728187557774	-76.52366535569077	15564
2170c966435d55e80417a50162629fb881a7ac84	a novel cnn segmentation framework based on using new shape and appearance features		To improve the accuracy of segmenting medical images from different modalities we propose to integrate three types of comprehensive quantitative image descriptors with a deep 3D convolutional neural network. The descriptors include: (i) a Gibbs energy for a prelearned 7th-order Markov-Gibbs random field (MGRF) model of visual appearance, (ii) a relearned adaptive shape prior model, and a first-order conditional random field model of visual appearance of regions at each current stage of segmentation. The neural network fuses the computed descriptors, together with the raw image data, for obtaining the final voxel-wise probabilities of the goal regions. Quantitative assessment of our framework in terms of Dice similarity coefficients, 95-percentile bidirectional Hausdorff distances, and percentage volume differences confirms the high accuracy of our model on 95 CT lung images $(98.37\pm_{0.68}\%,2.79\pm_{1.32}\ mm,3.94\pm_{2.11}\%)$ and 95 diffusion weighted kidney MRI $(96.65\pm_{2.15}\%,4.32\pm_{3.09}\ mm,5.61\pm_{3.37}\%)$, respectively.		Ahmed Soliman;Ahmed Shaffie;Mohammed Ghazal;Georgy L. Gimel'farb;Robert Keynton;Ayman El-Baz	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451534	random field;convolutional neural network;artificial neural network;image segmentation;pattern recognition;artificial intelligence;conditional random field;segmentation;computer science;visual appearance;hausdorff space	Vision	31.373181311096133	-75.87982204602103	15599
46404746fdbbc5104d6e65947573aac10e8c87ea	group-wise fmri activation detection on corresponding cortical landmarks	cortical landmarks;dti;fmri;group-wise activation detection	Group-wise activation detection in task-based fMRI has been widely used because of its robustness to noises and statistical power to deal with variability of individual brains. However, current group-wise fMRI activation detection methods typically rely on the spatial alignment established by coregistration of individual brains' fMRI images into the same template space, which has difficulty in dealing with the remarkable anatomic variation of different brains. As a consequence, the resulted misalignment among multiple brains could substantially degrade the accuracy and specificity of group-wise fMRI activation detection. To address these challenges, this paper presents a novel methodology to detect group-wise fMRI activation based on a publicly released dense map of DTI-derived structural cortical landmarks, which possess intrinsic correspondences across individuals and populations. The basic idea here is that a first-level general linear model (GLM) analysis is performed on fMRI signals of each corresponding cortical landmark in each individual brain's own space, and then the single-subject effect size of the same landmark from a group of subjects are statistically integrated and assessed at the group level using the mixed-effects model. As a result, the consistently activated cortical landmarks are determined and declared group-wisely in response to external block-based stimuli. Our experimental results demonstrated that the proposed approach can map meaningful group-wise activation patterns on the atlas of cortical landmarks without image registration between subjects and spatial smoothing.		Jinglei Lv;Dajiang Zhu;Xintao Hu;Xin Zhang;Tuo Zhang;Junwei Han;Lei Guo;Tianming Liu	2013	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-40763-5_82	computer vision;artificial intelligence	ML	42.268947081638245	-77.36966392540437	15603
7a0854513d38306dab4ec07f9bce3b195636c593	anatomical curve identification	biological patents;biomedical journals;text mining;europe pubmed central;citation search;shape analysis;citation networks;p splines;research articles;smoothing;abstracts;open access;life sciences;principal curves;clinical guidelines;principal components;full text;anatomy;rest apis;change point;orcids;europe pmc;biomedical research;bioinformatics;literature search	Methods for capturing images in three dimensions are now widely available, with stereo-photogrammetry and laser scanning being two common approaches. In anatomical studies, a number of landmarks are usually identified manually from each of these images and these form the basis of subsequent statistical analysis. However, landmarks express only a very small proportion of the information available from the images. Anatomically defined curves have the advantage of providing a much richer expression of shape. This is explored in the context of identifying the boundary of breasts from an image of the female torso and the boundary of the lips from a facial image. The curves of interest are characterised by ridges or valleys. Key issues in estimation are the ability to navigate across the anatomical surface in three-dimensions, the ability to recognise the relevant boundary and the need to assess the evidence for the presence of the surface feature of interest. The first issue is addressed by the use of principal curves, as an extension of principal components, the second by suitable assessment of curvature and the third by change-point detection. P-spline smoothing is used as an integral part of the methods but adaptations are made to the specific anatomical features of interest. After estimation of the boundary curves, the intermediate surfaces of the anatomical feature of interest can be characterised by surface interpolation. This allows shape variation to be explored using standard methods such as principal components. These tools are applied to a collection of images of women where one breast has been reconstructed after mastectomy and where interest lies in shape differences between the reconstructed and unreconstructed breasts. They are also applied to a collection of lip images where possible differences in shape between males and females are of interest.	adaptation;breast;dimensions;floor and ceiling functions;interpolation imputation technique;lip structure;mastectomy;photogrammetry;smoothing (statistical technique);smoothing spline;t-spline;trunk structure	Adrian W. Bowman;Stanislav Katina;Joanna Smith;Denise Brown	2015		10.1016/j.csda.2014.12.007	computer vision;text mining;artificial intelligence;data mining;shape analysis;statistics;smoothing;principal component analysis	Vision	42.506446321737094	-80.10301370176578	15618
aab540d9e01e220117b92064c5f8793dc46cd36f	the 2017 davis challenge on video object segmentation		We present the 2017 DAVIS Challenge, a public competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC [1] and PASCAL VOC [2], which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation [3]), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset and define the evaluation metrics of the competition.	benchmark (computing);cvpr	Jordi Pont-Tuset;Federico Perazzi;Sergi Caelles;Pablo Andrés Arbeláez;Alexander Sorkine-Hornung;Luc Van Gool	2017	CoRR		computer vision;simulation;computer science;machine learning;computer graphics (images)	Vision	24.717662340716778	-55.6865025068492	15655
858a0e73c8eb4d921c3d44ff3490b8a7dbbd91dd	differential oestrogen receptor binding is associated with clinical outcome in breast cancer	genomics;cancer;protein binding;nucleic acid;gene expression regulation;breast cancer;drug resistance;regulatory sequences;receptors;genetics;cell line;survival analysis;receptor binding	This paper, which maps ERα binding via ChIP-seq in tumour tissue from twenty ER+ breast cancer patients, relies on a concurrently developed Bioconductor package, DiffBind, which provides a framework for quantitative differential analysis of protein/DNA binding events. Here we use DiffBind to identify ERα sites significantly differentially bound between those found in tumours from patients with good prognosis vs. those with poor prognosis and metastases. Gene signatures that predict clinical outcome in ER+ disease, validated in publically available breast cancer gene expression datasets, are derived from these sites. These signatures are enriched for genes with relevant proximal cis-regulatory events. Statistical characterization of differentially bound ERα sites enables further downstream analysis, including identification of a differentially enriched motif for the transcription factor FoxA1. Further differential analysis in five ER+ breast cancer cell lines shows how ERα binding is extensively shifted in tamoxifen-resistance, with the FoxA1 motif enriched proximal to ERα binding sites differentially bound in cells resistant to treatment. Analysis of FoxA1 binding at mitogen-induced ERα sites demonstrates that the observed differential ER binding program is not due to the selection of a rare subpopulation of cells, but rather to the FoxA1-mediated reprogramming of ER binding on a rapid time scale. Focusing our analysis on differential binding in primary tumour material allows us to show the plasticity of ERα binding capacity, with distinct combinations of cis-regulatory elements linked with the different clinical outcomes. These techniques are applicable to other cancers (and indeed other diseases) where master transcription factor regulators are known.		Rory Stark	2012		10.1007/978-3-642-29627-7_30	biology;bioinformatics;immunology	Logic	6.316167440185987	-56.62172381493341	15691
4d392778ecae18dfde1d85831b24f285b3d0f612	an optimized image segmentation approach based on boltzmann machine		ABSTRACTImage segmentation with complex background is a tedious task. In our study, a convex spline is constructed based on Good Features to Track (GF2T) method’s region-based salient feature (i.e., corner) set. For an optimized edge-based segmentation, an ellipse shape prior based on this convex spline is useful in edge regularization procedure with region-based features. This kind of optimization is achieved by Boltzmann machine (BM) to automatically form an elliptical foreground mask of the GrabCut method. We demonstrated our approach’s usability through traveling salesman problem (TSP), thus, we consider that the TSP’s valid tour’s path solved by BM can be taken as an optimized convex spline for edge-based segmentation. In our experiments, proposed BM-based approach has the performance improvement of segmentation to stand-alone GF2T as 29.79% improvement based on bounding boxes evaluation and as 38.67% improvement based on the overlapping pixel regions for a quantitative evaluation via objective metrics.		Bahadir Karasulu	2017	Applied Artificial Intelligence	10.1080/08839514.2018.1444849	data mining;algorithm;pixel;grabcut;travelling salesman problem;image segmentation;regularization (mathematics);computer science;boltzmann machine;spline (mathematics);segmentation	AI	45.47142187798849	-69.85676840035218	15717
2c06c65c56f2d2c79a8b13c19da376aba52f043e	oranges-in-a-box simulations help to classify brain tissues from mri			computer simulation	Hugo G. Schnack	2007	ERCIM News		data mining;computer science	NLP	30.41842074429759	-79.16145331819114	15720
15fc29f5af1cf1feb4c35dd08ea17192e28c8d01	scale adaptive supervoxel segmentation of rgb-d image	image segmentation;object segmentation;three dimensional displays;image color analysis;merging;clustering algorithms;entropy	Superpixels are perceptually meaningful atomic regions that can effectively capture image features. We propose a novel scale adaptive supervoxel segmentation algorithm for RGB-D images, i.e., small supervoxels in content-dense regions (e.g., with high intensity or color variation) and large supervoxels in content-sparse regions. Among various methods for computing uniform superpixels, simple linear iterative clustering (SLIC) is popular due to its simplicity and high performance. We extend SLIC to generate small evenly distributed supervoxels in 3D space as PDS-SLIC, reducing the numbers of nodes. Then we use supervoxels as the nodes to construct a graph and setting the adaptive threshold for supervoxel merging. Experiments on NYU Depth Dataset V2 show that our proposed method outperforms state-of-the-art methods.	algorithm;cluster analysis;iterative method;simulation;sparse matrix	Peng Xu;Jie Li;Juan Yue;Xia Yuan	2016	2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2016.7866506	computer vision;entropy;computer science;machine learning;pattern recognition;mathematics;image segmentation;cluster analysis	Robotics	46.075904324424755	-68.36312232552348	15723
478efa1f1a71df7f4ec2fa9ac5cb1cfc32544ba8	cnn universal machine as classificaton platform: an art-like clustering algorithm	cellular nonlinear network;cluster algorithm;classification;analogic algorithm;hardware implementation;adaptive resonance theory;artificial neural network	Fast and robust classification of feature vectors is a crucial task in a number of real-time systems. A cellular neural/nonlinear network universal machine (CNN-UM) can be very efficient as a feature detector. The next step is to post-process the results for object recognition. This paper shows how a robust classification scheme based on adaptive resonance theory (ART) can be mapped to the CNN-UM. Moreover, this mapping is general enough to include different types of feed-forward neural networks. The designed analogic CNN algorithm is capable of classifying the extracted feature vectors keeping the advantages of the ART networks, such as robust, plastic and fault-tolerant behaviors. An analogic algorithm is presented for unsupervised classification with tunable sensitivity and automatic new class creation. The algorithm is extended for supervised classification. The presented binary feature vector classification is implemented on the existing standard CNN-UM chips for fast classification. The experimental evaluation shows promising performance after 100% accuracy on the training set.		Dávid Bálya	2003	International journal of neural systems	10.1142/S0129065703001807	computer vision;biological classification;computer science;artificial intelligence;adaptive resonance theory;machine learning;linear classifier;artificial neural network	ML	26.993076692976825	-57.911124279843314	15743
ecc9699efc102ab33554b07bc4b6429a38583dbd	automatic segmentation of prostate boundaries in transrectal ultrasound (trus) imaging	contrast enhanced;image processing;and brachytherapy;image contrast enhancement;ultrasound;automatic segmentation;prostate boundary;segmentation;signal attenuation;computing systems;ultrasonography;denoising;direct search;noise removal;prostate	An automatic segmentation method for detecting the prostate boundary in transrectal ultrasound (TRUS) images was developed. The TRUS images were preprocessed by using an adaptive directional filtering and an automatic attenuation compensation for noise removal and contrast enhancement. A directional search strategy was used to locate key-points on the prostate boundary. The prostate contour was interpolated from the key-points under the supervision of a morphological prostate boundary model, which had been trained from prior manual segmentation of a large number of TRUS images. A new prostate center was calculated based on the intermediate segmentation result. The algorithm is reiterated until the prostate boundary and center reach a stable state. The overall performance of the method was compared to manual segmentation of an expert radiologist. About 78% out of 282 TRUS images (excluding base and apex slices) from three types of ultrasound machine (Acuson, Siemens, and B&K) were correctly delineated. The segmentation error was 0.9 mm averaged on 30 selected images, 10 for each type of machine. The computation time for a typical series of TRUS images is approximately 1 minute on a Pentium-II computer.© (2002) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	medical ultrasound	Haisong Liu;Gang Cheng;Deborah J. Rubens;John G. Strang;Lydia Liao;Ralph Brasacchio;Edward Messing;Yan Yu	2002		10.1117/12.467183	computer vision;radiology;medicine;medical physics	Vision	39.05553158987277	-80.48015201568823	15754
b74424e76e8e1c25634fead6eadfd4aa385afaa7	simultaneous reconstruction of outer boundary shape and admittivity distribution in electrical impedance tomography	complete electrode model electrical impedence tomography model inaccuracies output least squares shape derivative unknown boundary shape;output least squares;model inaccuracies;electrical impedence tomography;35r30;35j25;shape derivative;electrical impedance tomography;complete electrode model;unknown boundary shape;65n21	The aim of electrical impedance tomography is to reconstruct the admittivity distribution inside a physical body from boundary measurements of current and voltage. Due to the severe ill-posedness of the underlying inverse problem, the functionality of impedance tomography relies heavily on accurate modelling of the measurement geometry. In particular, almost all reconstruction algorithms require the precise shape of the imaged body as an input. In this work, the need for prior geometric information is relaxed by introducing a Newton-type output least squares algorithm that reconstructs the admittivity distribution and the object shape simultaneously. The method is built in the framework of the complete electrode model and is based on the Frechet derivative of the corresponding current-to-voltage map with respect to the object boundary shape. The functionality of the technique is demonstrated via numerical experiments with simulated measurement data.	nominal impedance;tomography	Jérémi Dardé;Nuutti Hyvönen;Aku Seppänen;Stratos Staboulis	2013	SIAM J. Imaging Sciences	10.1137/120877301	mathematical optimization;electrical impedance tomography;mathematics;geometry	Theory	51.70645516834845	-81.13730050754427	15756
47b9bc3850d30118b29650cc018d3262c2e00dff	binarization of badly illuminated document images through shading estimation and compensation	shading estimation;document images;illuminated docu;segments text;estimated shading variation;text document;document image binarization tech;document contrast;text size;illuminated document image;binary text im;document image;image segmentation;text analysis;background noise;polynomials	This paper presents a document image binarization technique that segments text from badly illuminated document images. Based on the observations that text documents normally lie over a planar or smoothly curved surface and have a uniformly colored background, badly illuminated document images are binarized by using a smoothing polynomial surface, which estimates the shading variation and compensates the shading degradation based on the estimated shading variation. Badly illuminated document images are accordingly binarized through the global thresholding of the compensated document images. Compared with the reported methods, the proposed technique is tolerant to the variations in text size and document contrast. At the same time, it is much faster and able to produce a binary text image with little background noise.	ascii art;binary image;elegant degradation;polynomial;shading;smoothing;thresholding (image processing)	S. J. Lu;Chew Lim Tan	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.74	computer vision;text mining;computer science;pattern recognition;background noise;image segmentation;polynomial;computer graphics (images)	Vision	38.41917925419487	-66.2588231734446	15783
322999addbbdda6221af962c9326980e69d0f1e5	dr.seq: a quality control and analysis pipeline for droplet sequencing		MOTIVATION Drop-seq has recently emerged as a powerful technology to analyze gene expression from thousands of individual cells simultaneously. Currently, Drop-seq technology requires refinement and quality control (QC) steps are critical for such data analysis. There is a strong need for a convenient and comprehensive approach to obtain dedicated QC and to determine the relationships between cells for ultra-high-dimensional datasets.   RESULTS We developed Dr.seq, a QC and analysis pipeline for Drop-seq data. By applying this pipeline, Dr.seq provides four groups of QC measurements for given Drop-seq data, including reads level, bulk-cell level, individual-cell level and cell-clustering level QC. We assessed Dr.seq on simulated and published Drop-seq data. Both assessments exhibit reliable results. Overall, Dr.seq is a comprehensive QC and analysis pipeline designed for Drop-seq data that is easily extended to other droplet-based data types.   AVAILABILITY AND IMPLEMENTATION Dr.seq is freely available at: http://www.tongji.edu.cn/∼zhanglab/drseq and https://bitbucket.org/tarela/drseq   CONTACT yzhang@tongji.edu.cn   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.		Xiao Huo;Sheng'en Hu;Chengchen Zhao;Yong Zhang	2016	Bioinformatics	10.1093/bioinformatics/btw174	computer science;bioinformatics	Comp.	-1.5077291940151707	-56.84468656112511	15828
d15d09dab0e64ea7b3175919b06b818000fb156f	a hierarchical regression approach for unconstrained face analysis	hierarchical regression;head pose estimation;facial feature detection;unconstrained face analysis;d rf	Head pose and facial feature detection are important for face analysis. However, many studies reported good results in constrained environment, the performance could be decreased due to the high variations in facial appearance, poses, illumination, occlusion, expression and make-up. In this paper, we propose a hierarchical regression approach, Dirichlet-tree enhanced random forests (D-RF) for face analysis in unconstrained environment. D-RF introduces Dirichlet-tree probabilistic model into regression RF framework in the hierarchical way to achieve the efficiency and robustness. To eliminate noise influence of unconstrained environment, facial patches extracted from face area are classified as positive or negative facial patches, only positive facial patches are used for face analysis. The proposed hierarchical D-RF works in two iterative procedures. First, coarse head pose is estimated to constrain the facial features detection, then the head pose is updated based on the estimated facial features. Second, the facial feature localization is refined based on the updated head pose. In order to further improve the efficiency and robustness, multiple probabilitic models are learned in leaves of the D-RF, i.e. the patch’s classification, the head pose probabilities, the locations of facial points and face deformation models (FDM). Moreover, our algorithm takes a composite weight voting method, where each patch extracted from the image can directly cast a vote for the head pose or each of the facial features. Extensive experiments have been done with different publicly available databases. The experimental results demonstrate that the proposed approach is robust and efficient for head pose and facial feature detection.	multilevel model	Yuanyuan Liu;Jingying Chen;Cunjie Shan;Zhiming Su;Pei Cai	2015	IJPRAI	10.1142/S021800141556011X	computer vision;speech recognition;multilevel model;pattern recognition;face hallucination;statistics	Vision	42.75679791149368	-52.38485653907627	15837
76251c0bc3a4685c433060cf0505cefe378f549a	a belief rule-based expert system to assess suspicion of acute coronary syndrome (acs) under uncertainty		Acute coronary syndrome (ACS) is responsible for the obstruction of coronary arteries, resulting in the loss of lives. The onset of ACS can be determined by looking at the various signs and symptoms of a patient. However, the accuracy of ACS determination is often put into question since there exist different types of uncertainties with the signs and symptoms. Belief rule-based expert systems (BRBESs) are widely used to capture uncertain knowledge and to accomplish the task of reasoning under uncertainty by employing belief rule base and evidential reasoning. This article presents the process of developing a BRBES to determine ACS predictability. The BRBES has been validated against the data of 250 patients suffering from chest pain. It is noticed that the outputs created from the BRBES are more dependable than that of the opinion of cardiologists as well as other two expert system tools, namely artificial neural networks and support vector machine. Hence, it can be argued that the BRBES is capable of playing an important role in decision making as well as in avoiding costly laboratory investigations. A procedure to train the system, allowing its enhancement of performance, is also presented.	expert system;logic programming	Mohammad Shahadat Hossain;Saifur Rahaman;Rashed Mustafa;Karl Andersson	2018	Soft Comput.	10.1007/s00500-017-2732-2	machine learning;artificial neural network;artificial intelligence;chest pain;computer science;rule-based system;support vector machine;acute coronary syndrome;evidential reasoning approach;predictability;data mining;expert system	Robotics	4.975341202861844	-78.5897781369959	15840
2c15a7acfa2b85732882d5131bb61e0264c7e887	idba-mt: de novo assembler for metatranscriptomic data generated from next-generation sequencing technology	dynamic programming;genomic rearrangements;computational molecular biology;metagenomics;algorithms;next generation sequencing;article;alignment	High-throughput next-generation sequencing technology provides a great opportunity for analyzing metatranscriptomic data. However, the reads produced by these technologies are short and an assembling step is required to combine the short reads into longer contigs. As there are many repeat patterns in mRNAs from different genomes and the abundance ratio of mRNAs in a sample varies a lot, existing assemblers for genomic data, transcriptomic data, and metagenomic data do not work on metatranscriptomic data and produce chimeric contigs, that is, incorrect contigs formed by merging multiple mRNA sequences. To our best knowledge, there is no assembler designed for metatranscriptomic data. In this article, we introduce an assembler called IDBA-MT, which is designed for assembling reads from metatranscriptomic data. IDBA-MT produces much fewer chimeric contigs (reduce by 50% or more) when compared with existing assemblers such as Oases, IDBA-UD, and Trinity.	assembly language;biopolymer sequencing;chimera organism;genome;massively-parallel sequencing;metagenomics;reading (activity);throughput;trinity;urban dictionary	Henry C. M. Leung;Siu-Ming Yiu;John Parkinson;Francis Y. L. Chin	2013	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2013.0042	biology;dna sequencing;bioinformatics;dynamic programming;genetics;metagenomics;algorithm	Comp.	-0.4981834769246008	-54.315491763184085	15858
2295b6afa33dc5889f4ca7cf7773cbab7bc51e67	image segmentation using normalized cuts and efficient graph-based segmentation	image segmentation;normalized cuts;region adjacency graph;efficient graph based	"""In this paper we propose an hybrid segmentation algorithm which incorporates the advantages of the efficient graph based segmentation and normalized cuts partitioning algorithm. The proposed method requires low computational complexity and is therefore suitable for realtime image segmentation processing. Moreover, it provides effective and robust segmentation. For that, our method consists first, at segmenting the input image by the """"Efficient Graph-Based"""" segmentation. The segmented regions are then represented by a graph structure. As a final step, the normalized cuts partitioning algorithm is applied to the resulting graph in order to remove non-significant regions. In the proposed method, the main computational cost is the efficient graph based segmentation cost since the computational cost of partitioning regions using the Ncut method is negligibly small. The efficiency of the proposed method is demonstrated through a large number of experiments using different natural scene images."""		Narjes Doggaz;Imene Ferjani	2011		10.1007/978-3-642-24088-1_24	graph cuts in computer vision;computer vision;range segmentation;computer science;grabcut;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;connected-component labeling	Vision	45.81111236574253	-68.7223161508946	15863
4d0ec4e0f873642adb6e2b37fa39574e4441abf0	genomic characterization of large heterochromatic gaps in the human genome assembly	heterochromatin;chromosome mapping;sequence analysis dna;genome human;dna satellite;chromosomes human y;humans;molecular sequence data;base sequence	The largest gaps in the human genome assembly correspond to multi-megabase heterochromatic regions composed primarily of two related families of tandem repeats, Human Satellites 2 and 3 (HSat2,3). The abundance of repetitive DNA in these regions challenges standard mapping and assembly algorithms, and as a result, the sequence composition and potential biological functions of these regions remain largely unexplored. Furthermore, existing genomic tools designed to predict consensus-based descriptions of repeat families cannot be readily applied to complex satellite repeats such as HSat2,3, which lack a consistent repeat unit reference sequence. Here we present an alignment-free method to characterize complex satellites using whole-genome shotgun read datasets. Utilizing this approach, we classify HSat2,3 sequences into fourteen subfamilies and predict their chromosomal distributions, resulting in a comprehensive satellite reference database to further enable genomic studies of heterochromatic regions. We also identify 1.3 Mb of non-repetitive sequence interspersed with HSat2,3 across 17 unmapped assembly scaffolds, including eight annotated gene predictions. Finally, we apply our satellite reference database to high-throughput sequence data from 396 males to estimate array size variation of the predominant HSat3 array on the Y chromosome, confirming that satellite array sizes can vary between individuals over an order of magnitude (7 to 98 Mb) and further demonstrating that array sizes are distributed differently within distinct Y haplogroups. In summary, we present a novel framework for generating initial reference databases for unassembled genomic regions enriched with complex satellite DNA, and we further demonstrate the utility of these reference databases for studying patterns of sequence variation within human populations.	bibliographic database;databases;description;fourteen;genome assembly sequence;haplogroup;high-throughput computing;largest;megabyte;numerous;population;repetitive region;satellite viruses;tandem repeat sequences;throughput;variation (genetics);algorithm	Nicolas Altemose;Karen H. Miga;Mauro Maggioni;Huntington F. Willard	2014		10.1371/journal.pcbi.1003628	biology;heterochromatin;bioinformatics;genetics	Comp.	2.718113108820509	-59.410896074587626	15877
30c95c2d595f7e09698231d3d40cff0f4bbe0c36	transformation based walking speed normalization for gait recognition	biometrics;gait recognition;identification of persons;computer vision;pattern recognition	Humans are able to recognize small number of people they know well by the way they walk. This ability represents basic motivation for using human gait as the means for biometric identification. Such biometric can be captured at public places from a distance without subject’s collaboration, awareness or even consent. Although current approaches give encouraging results, we are still far from effective use in practical applications. In general, methods set various constraints to circumvent the influence factors like changes of view, walking speed, capture environment, clothing, footwear, object carrying, that have negative impact on recognition results. In this paper we investigate the influence of walking speed variation to different visual based gait recognition approaches and propose normalization based on geometric transformations, which mitigates its influence on recognition results. With the evaluation on MoBo gait dataset we demonstrate the benefits of using such normalization in combination with different types of gait recognition approaches.		Jure Kovac;Peter Peer	2013	TIIS	10.3837/tiis.2013.11.008	computer vision;simulation;computer science;biometrics	Vision	35.02704972744549	-56.495140301106076	15883
c9ce303b43a7e9ee4eecdb58cd228c27974c17d9	interactive constraint satisfaction and its application to visual object recognition	constraint satisfaction;visual object recognition		constraint satisfaction;outline of object recognition	Evelina Lamma;Michela Milano;Rita Cucchiara;Paola Mello;Massimo Piccardi	1998			cognitive neuroscience of visual object recognition;machine learning;computer vision;constraint satisfaction;artificial intelligence;computer science	Vision	29.768629357913326	-57.40351881302185	15890
a5d4205593d6c87b77b75808f789d10d016b78a6	functional activation of the cerebral cortex related to sensorimotor adaptation of reactive and voluntary saccades	fmri;voluntary saccades;multi voxel pattern classification;saccadic adaptation;reactive saccades	Potentially dangerous events in the environment evoke automatic ocular responses, called reactive saccades. Adaptation processes, which maintain saccade accuracy against various events (e.g. growth, aging, neuro-muscular lesions), are to date mostly relayed to cerebellar activity. Here we demonstrate that adaptation of reactive saccades also involves cerebral cortical areas. Moreover, we provide the first identification of the neural substrates of adaptation of voluntary saccades, representing the complement to reactive saccades for the active exploration of our environment. An fMRI approach was designed to isolate adaptation from saccade production: an adaptation condition in which the visual target stepped backward 50 ms after saccade termination was compared to a control condition where the same target backstep occurred 500 ms after saccade termination. Subjects were tested for reactive and voluntary saccades in separate sessions. Multi-voxel pattern analyses of fMRI data from previously-defined regions of interests (ROIs) significantly discriminated between adaptation and control conditions for several ROIs. Some of these areas were revealed for adaptation of both saccade categories (cerebellum, frontal cortex), whereas others were specifically related to reactive saccades (temporo-parietal junction, hMT+/V5) or to voluntary saccades (medial and posterior areas of intra-parietal sulcus). These findings critically extend our knowledge on brain motor plasticity by showing that saccadic adaptation relies on a hitherto unknown contribution of the cerebral cortex.	acclimatization;categories;cerebral cortex;complement system proteins;cortical cell layer of the cerebellum;groove;medial graph;muscle;parietal lobe;saccades;voxel;fmri;interest	Peggy Gerardin;Aline Miquée;Christian Urquizar;Denis Pélisson	2012	NeuroImage	10.1016/j.neuroimage.2012.03.037	psychology;neuroscience;developmental psychology;communication	ML	18.435708385052166	-78.79568643878699	15894
1cd9dba357e05c9be0407dc5d477fd528cfeb79b	model-driven simulations for deep convolutional neural networks		The use of simulated virtual environments to train deep convolutional neural networks (CNN) is a currently active practice to reduce the (real)data-hungriness of the deep CNN models, especially in application domains in which large scale real data and/or groundtruth acquisition is difficult or laborious. Recent approaches have attempted to harness the capabilities of existing video games, animated movies to provide training data with high precision groundtruth. However, a stumbling block is in how one can certify generalization of the learned models and their usefulness in real world data sets. This opens up fundamental questions such as: What is the role of photorealism of graphics simulations in training CNN models? Are the trained models valid in reality? What are possible ways to reduce the performance bias? In this work, we begin to address theses issues systematically in the context of urban semantic understanding with CNNs. Towards this end, we (a) propose a simple probabilistic urban scene model, (b) develop a parametric rendering tool to synthesize the data with groundtruth, followed by (c) a systematic exploration of the impact of level-of-realism on the generality of the trained CNN model to real world; and domain adaptation concepts to minimize the performance bias.	artificial neural network;computation;computer simulation;computer vision;convolutional neural network;domain adaptation;domain model;emoticon;experiment;graphics pipeline;model-driven integration;monte carlo;object detection;performance tuning;pixel;principle of good enough;rendering (computer graphics);sampling (signal processing);stumbleupon;virtual reality;virtual world	V. S. R. Veeravasarapu;Constantin A. Rothkopf;Visvanathan Ramesh	2016	CoRR		computer vision;simulation;computer science;artificial intelligence;machine learning;mathematics	ML	23.200900199327204	-53.44512310275147	15917
1d113af676f509890a2ce2e8b613efd870aeede9	virtual fragment screening: discovery of histamine h3 receptor ligands using ligand-based and protein-based molecular fingerprints		Virtual fragment screening (VFS) is a promising new method that uses computer models to identify small, fragment-like biologically active molecules as useful starting points for fragment-based drug discovery (FBDD). Training sets of true active and inactive fragment-like molecules to construct and validate target customized VFS methods are however lacking. We have for the first time explored the possibilities and challenges of VFS using molecular fingerprints derived from a unique set of fragment affinity data for the histamine H(3) receptor (H(3)R), a pharmaceutically relevant G protein-coupled receptor (GPCR). Optimized FLAP (Fingerprints of Ligands and Proteins) models containing essential molecular interaction fields that discriminate known H(3)R binders from inactive molecules were successfully used for the identification of new H(3)R ligands. Prospective virtual screening of 156,090 molecules yielded a high hit rate of 62% (18 of the 29 tested) experimentally confirmed novel fragment-like H(3)R ligands that offer new potential starting points for the design of H(3)R targeting drugs. The first construction and application of customized FLAP models for the discovery of fragment-like biologically active molecules demonstrates that VFS is an efficient way to explore protein-fragment interaction space in silico.	binding (molecular function);computer simulation;customize;drug discovery;experiment;fingerprint;histamine h2 antagonists;interactome;ligands;parathyroid hormone receptor;physical inactivity;processor affinity;prospective search;thrombocytopenia;virtual screening;cellular targeting	Francesco Sirci;Enade P. Istyastono;Henry F. Vischer;Albert J. Kooistra;Saskia Nijmeijer;Martien Kuijer;Maikel Wijtmans;Raimund Mannhold;Rob Leurs;Iwan J. P. de Esch;Chris de Graaf	2012	Journal of chemical information and modeling	10.1021/ci3004094	stereochemistry;chemistry;bioinformatics;combinatorial chemistry	Comp.	9.73935090833345	-59.26181712398618	15920
1c10c79ed8025f4ccc86ff139803f0318829c335	modeling subtilin production in bacillus subtilis using stochastic hybrid systems	genetique;chaine markov;cadena markov;proteine;sistema hibrido;modelo markov;genetica;systeme discret;modelo hibrido;stochastic hybrid system;continuous system;algoritmo genetico;bacillus subtilis;modele hybride;genetics;hybrid model;systeme continu;stochastic system;systeme incertain;markov model;sistema continuo;genetic network;hybrid system;algorithme genetique;genetic algorithm;proteina;conmutador;modele markov;sistema discreto;sistema estocastico;protein;sistema incierto;uncertain system;systeme stochastique;discrete system;commutateur;selector switch;systeme hybride;markov chain	The genetic network regulating the biosynthesis of subtilin in Bacillus subtilis is modeled as a stochastic hybrid system. The continuous state of the hybrid system is the concentrations of subtilin and various regulating proteins, whose productions are controlled by switches in the genetic network that are in turn modeled as Markov chains. Some preliminary results are given by both analysis and simulations. 1 Background of Subtilin Production In order to survive, bacteria develop a number of strategies to cope with harsh environmental conditions. One of the survival strategies employed by bacteria is the release of antibiotics to eliminate competing microbial species in the same ecosystem [15]. It is observed that the production of antibiotics in the cells is affected by not only the environmental stimuli (e.g. nutrient levels, aeration, etc.) but also the local population density of their own species [12]. Therefore, the physiological states of the cell and the external signals both contribute to the regulation of antibiotic synthesis. Our study focuses on the subtilin, an antibiotic produced by Bacillus subtilis ATCC 6633, because the genetics of subtilin is known and its biosynthetic pathways are well characterized [2, 7, 11]. We briefly describe the production process of subtilin in B. subtilis. It is shown in [19] that the production is controlled by two independent mechanisms. When the foods are abundant, the population proliferates and the cells produce very little amount (non-lethal dose) of subtilin. However, when the foods become scarce, the production of subtilin picks up as follows. First, sigma-H (SigH), a sigma factor that regulates gene expression, enables the production of SpaRK (SpaR and SpaK) proteins by binding to the promoter regions of their genes (spaR and spaK). The membrane-bound SpaK protein senses the extracellular subtilin accumulating in the environment as the cell colony becomes large, and activates the SpaR protein. The activated SpaR (SpaR∼p) in turn directs the productions of the subtilin structural peptide SpaS, the biosynthesis complex SpaBTC which modifies SpaS to yield the final product subtilin, and the immunity machinery SpaIFEG which protects the cell against the killing effect of ? This research is partially supported by the National Science Foundation under Grant No. EIA-0122599. Fig. 1. Schematic representation of subtilin biosynthesis, immunity, and regulation in Bacillus subtilis. Subtilin prepeptide SpaS is modified, cleaved, and translocated across the cell membrane by the subtilin synthetase complex SpaBTC. The genes of subtilin are organized in an operon-like structure (spaBTC, spaS, spaIFEG, and spaRK) so that each functional unit is transcribed together. The extra-cellular subtilin functions as pheromone that regulates its own synthesis via an autoinduction feedback pathway. subtilin. See Fig. 1 for a schematic representation of the biosynthesis process of subtilin in B. subtilis. In this work, a simplified version of the production process is adopted for ease of study. Namely, we ignore the dynamics in the post-translational processing of SpaS by SpaBTC to form mature subtilin and the signal transduction between SpaK and SpaR. Hence, the amount of SpaS is assumed to be equivalent to the amount of subtilin released by the cell and the SpaK and SpaR proteins are considered as one protein species. From the above description, a dynamical model of subtilin production consists of two parts: discrete events and continuous dynamics. The discrete events include the initiations and terminations of transcriptions of various genes due to the binding and unbinding of their transcription regulators to their promoter regions, while the continuous dynamics include the accumulations and degradations of the protein species after the expressions of their genes are being switched on and off, respectively. Thus, hybrid systems can be a suitable choice for such a model. Furthermore, in cellular networks involving coupled genetic and biochemical reactions, the expressions of genes are intrinsically non-deterministic, as is evidenced by the random fluctuations (noise) in the concentrations of protein species in the cell population, even in an isogenic culture [6]. One reason for the stochasticity in gene expressions is the small copy number of interacting molecular species (e.g. regulatory proteins and genes) in the relatively large cell volume [13, 14, 16], since in a chemical system with extremely low concentrations of reacting species, a reaction (collision of the reacting molecules) occurs in a short time interval and is best viewed as a probabilistic event [9]. This is also the case in our example, as an individual B. subtilis cell has only one copy of each spa gene for the corresponding spa protein. Techniques such as stochastic differential equations and Monte Carlo algorithms are often used to model and simulate such biological systems with noise (see [17] for a review). Although stochastic algorithms are computationally involved, they produce a more realistic and complete description of the time-dependent behavior of biochemical systems than deterministic algorithms. In this paper, we adopt such a stochastic point of view and propose a stochastic hybrid systems approach to analyze the dynamics of the spa genes in the subtilin system. We also present simulation results of the subtilin regulatory network to demonstrate the distinctive behaviors of the systems using the deterministic and stochastic modeling formalisms. 2 Stochastic Hybrid Systems A framework first proposed by the engineering community to model systems that exhibit both continuous dynamics and discrete state changes, hybrid systems have in recent years found increasingly wide applications in many practical fields. In particular, applications of hybrid systems in modeling of biological systems can be found in [8, 1], to name a few. However, most of the models proposed in the literature so far are deterministic, and are not suitable to model system with inherent randomness. This is the case, for example, in cellular processes modeling, especially when the number of participating cells is not large enough and random fluctuations within single cells cannot be ignored. Hence one needs to extend the framework to a more general class of hybrid systems with built-in randomness, namely, stochastic hybrid systems. There have already been some work on stochastic hybrid systems (see, for example, [10, 3]). In this section we shall present a simple model suitable for our main example, the production of subtilin, to be given in the next section. Basically, the state of the stochastic hybrid system consists of two parts, a continuous one and a discrete one. The discrete state has a finite number of possible values, and evolves randomly according to a Markov chain. The continuous state, on the other hand, takes value in a certain Euclidean space, and evolves deterministically according to some ordinary differential equations. The dynamics of the discrete and the continuous states are coupled in the following sense: on one hand, the transition probabilities of the Markov chain for the discrete state depend on the value of the continuous state at the moment of jump; on the other hand, the differential equations governing the evolution of the continuous state are different when the discrete state takes on different values. More formally, we consider the following model of a stochastic hybrid system. Its state (q, x) consists of a discrete part (mode) q taking values in a finite set Q and a continuous part x taking values in a Euclidean space R, n ≥ 1. Both q and x are functions of time t and their dynamics are specified in the following. – (Discrete Dynamics) The discrete state q follows a Markov chain that jumps at epochs t = 0, ∆, 2∆, . . . of constant interval ∆ > 0 with transition proba-	biological system;c date and time functions;deterministic algorithm;dynamical system;ecosystem;emoticon;epoch (reference date);execution unit;gene regulatory network;hybrid system;interaction;markov chain;monte carlo method;network switch;randomness;spark;schematic;simulation;stochastic modelling (insurance);stochastic process;transcription (software);transduction (machine learning);xfig	Jianghai Hu;Wei Chung Wu;S. Shankar Sastry	2004		10.1007/978-3-540-24743-2_28	markov chain;genetic algorithm;computer science;artificial intelligence;discrete system;markov model;algorithm;statistics;hybrid system	Logic	6.890535425049023	-65.33452221075038	15922
0abcb75650110d2bd5fda18c6f27f0065369941b	cardiac arrhythmia classification using a combination of quadratic spline-based wavelet transform and artificial neural classification network		The authors present the use of Wavelet Transform, using a quadratic spline function, and Probabilistic Neural Network (PNN) to classify 8 heartbeat conditions. The process consists of four mains stages. The first part consists of preprocessing a nd f iltering selected ECG l ead II (D II) data re gisters f rom t he PhysioNet repository. The filtered signal is fed to a w avelet transform process using a quadratic s pline f unction, to obtain a f eature v ector. T he r esults ar e transferred to a Probabilistic Neural Network algorithm for heartbeat classification. Finally, the algorithm is tested with confusion matrices to determine classification accuracy. The algorithm yielded a 9 1.5%, 90.3% and 95.5% classification accu racy f or au ricular f ibrillation, s inoauricular h eart block an d p aroxysmal atrial fibrillation conditions respectively. The lower scores were obtained for p remature at rial co ntraction and premature v entricular co ntraction conditions (75.5% and 69.9% respectively). However, considering the validation test conditions, the results suggest the algorithm is suitable for on-line classification of heartbeat conditions as part of a DSP-based Holter device.	acceptance testing;algorithm;confusion matrix;online and offline;preprocessor;probabilistic neural network;spline (mathematics);wavelet transform	Jose Antonio Gutierrez Gnecchi;Rodrigo Morfin Magaña;Adriana Del Carmen Tellez Anguiano;Daniel Lorias Espinoza;Enrique Reyes-Archundia;Obeth Hernandez Diaz	2014			confusion;wavelet transform;digital signal processing;quadratic equation;probabilistic neural network;mathematics;spline (mathematics);matrix (mathematics);heartbeat;pattern recognition;artificial intelligence	ML	17.18127521171345	-90.90315961953046	15933
0997ee7c9f0672409d76d444679f797f7406a773	multi-scale surface descriptors	local shape descriptor;protein surface analysis;differential curvature;shape descriptor;image matching;multiscale surface mesh descriptor;infinitesimal neighborhood;anisotropy;surface fitting;shape matching curvature descriptors npr stylized rendering;statistical approximation;shape recognition;data mining;npr;approximation theory;stylized rendering;data visualisation;proteins;shape;statistical analysis;shape matching;visual analysis;surface fitting approximation theory curve fitting data visualisation image matching mesh generation rendering computer graphics shape recognition statistical analysis;data visualization;anisotropic magnetoresistance;descriptors;stylized rendering multiscale surface mesh descriptor local shape descriptor data visualization shape matching infinitesimal neighborhood quadratic surface fitting differential curvature anisotropy statistical approximation protein surface analysis;robustness;curvature;approximation methods;curve fitting;quadratic surface fitting;rendering computer graphics;mesh generation;shape surface fitting proteins anisotropic magnetoresistance application software mesh generation computational efficiency data visualization data analysis mathematics;noise;surface analysis	Local shape descriptors compactly characterize regions of a surface, and have been applied to tasks in visualization, shape matching, and analysis. Classically, curvature has be used as a shape descriptor; however, this differential property characterizes only an infinitesimal neighborhood. In this paper, we provide shape descriptors for surface meshes designed to be multi-scale, that is, capable of characterizing regions of varying size. These descriptors capture statistically the shape of a neighborhood around a central point by fitting a quadratic surface. They therefore mimic differential curvature, are efficient to compute, and encode anisotropy. We show how simple variants of mesh operations can be used to compute the descriptors without resorting to expensive parameterizations, and additionally provide a statistical approximation for reduced computational cost. We show how these descriptors apply to a number of uses in visualization, analysis, and matching of surfaces, particularly to tasks in protein surface analysis.	algorithmic efficiency;approximation;encode;hl7publishingsubsection <operations>;imagery;matching;membrane proteins;numerous;polygon mesh;shape context	Gregory Cipriano;George N. Phillips;Michael Gleicher	2009	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2009.168	magnetoresistance;mesh generation;computer vision;mathematical optimization;shape;noise;surface weather analysis;mathematics;geometry;curvature;anisotropy;data visualization;statistics;robustness;curve fitting;approximation theory	Visualization	47.1611051556512	-75.97380063197349	15950
cf3c84c7b8d556db9d630dc09b03554a8731ab38	micro nucleus detection in human lymphocytes using convolutional neural network	micro nucleus detection;image processing;human lymphocyte;detection rate;convolutional neural network;training algorithm;neural network	The application of the convolution neural network for detection of the micro nucleuses in the human lymphocyte images acquired by the image flow cytometer is considered in this paper. The existing method of detection, called IMAQ Match Pattern, is described and its limitations concerning zoom factors are analyzed. The training algorithm of the convolution neural network and the detection procedure were described. The performance of both detection methods, convolution neural network and IMAQ Match Pattern, were researched. Our results show that the convolution neural network overcomes the IMAQ Match Pattern in terms of improvement of detection rate and decreasing the numbers of false alarms.	convolutional neural network	Ihor Paliy;Francesco Lamonaca;Volodymyr Turchenko;Domenico Grimaldi;Anatoly Sachenko	2010		10.1007/978-3-642-15819-3_68	computer vision;image processing;computer science;artificial intelligence;machine learning;time delay neural network;convolutional neural network;artificial neural network	NLP	32.836759545832315	-74.70887685043195	15952
d041562d1d325caf9085b80da92103d5004baf33	discovering relational-based association rules with multiple minimum supports on microarray datasets	ncku 成功大學 成大 圖書館 機構典藏;dissertations and theses journal referred papers conference papers nsc reserach report patent nckur ir ncku institutional repostiory 博碩士論文 期刊論文 國科會研究報告 專利 成大機構典藏	MOTIVATION Association rule analysis methods are important techniques applied to gene expression data for finding expression relationships between genes. However, previous methods implicitly assume that all genes have similar importance, or they ignore the individual importance of each gene. The relation intensity between any two items has never been taken into consideration. Therefore, we proposed a technique named REMMAR (RElational-based Multiple Minimum supports Association Rules) algorithm to tackle this problem. This method adjusts the minimum relation support (MRS) for each gene pair depending on the regulatory relation intensity to discover more important association rules with stronger biological meaning.   RESULTS In the actual case study of this research, REMMAR utilized the shortest distance between any two genes in the Saccharomyces cerevisiae gene regulatory network (GRN) as the relation intensity to discover the association rules from two S.cerevisiae gene expression datasets. Under experimental evaluation, REMMAR can generate more rules with stronger relation intensity, and filter out rules without biological meaning in the protein-protein interaction network (PPIN). Furthermore, the proposed method has a higher precision (100%) than the precision of reference Apriori method (87.5%) for the discovered rules use a literature survey. Therefore, the proposed REMMAR algorithm can discover stronger association rules in biological relationships dissimilated by traditional methods to assist biologists in complicated genetic exploration.	apriori algorithm;association rule learning;exhibits as topic;gene expression;gene regulatory network;interaction network;microarray;minimal recursion semantics;name;national supercomputer centre in sweden;ninety nine;rule (guideline);short;protein protein interaction	Yu-Cheng Liu;Chun-Pei Cheng;Vincent S. Tseng	2011	Bioinformatics	10.1093/bioinformatics/btr526	biology;computer science;bioinformatics;data science;data mining;mathematics;statistics	Comp.	2.440888053132023	-56.03344525413915	15998
aef90636aa70e5897040862b9b63002aa8ef4072	miniaturization of plastic lens by injection molding for disposable endoscopie optical coherence tomography system	disposable endoscopic oct system injection molding disposable endoscopic optical coherence tomography system minimally invasive robotic surgery high resolution deep tissue imaging cross contamination prevention miniaturized plastic lens endoscopic oct system human skin imaging microspheres agarose gel;injection molding;optical coherence tomography plastic lens injection molding;plastics;optical imaging;robots;lenses;lenses plastics injection molding biomedical optical imaging optical imaging surgery robots;surgery;biomedical optical imaging;surgery biomedical optical imaging cellular biophysics endoscopes lenses medical image processing medical robotics optical tomography skin	Minimally invasive robotic surgery required disposable endoscopie optical coherence tomography (OCT) for high-resolution deep tissue imaging and cross-contamination prevention. Here, we developed a miniaturized plastic lens by injection molding for developing a disposable endoscopie optical coherence tomography system for the first time. Injection molding provided low-cost and high-production rate to produce a disposable plastic lens and enabled the miniaturization of plastic lens. Transmittance and curvature of the plastic lens was measured. The plastic lens was integrated with an endoscopie OCT system, and the images of human-skin and microspheres embedded in agarose gel were captured. We are currently trying to integrate the plastic lens with a disposable endoscopie OCT system.	embedded system;image resolution;robot;sol-gel;tomography	Sang Min Park;Yeoreum Yoon;Wonkyoung Kim;Moonwoo La;Ki Hean Kim;Dong Sung Kim	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057420	robot;plastic;optical imaging;lens	Robotics	42.37937069854531	-86.82964575885508	16027
905ba94c2d733efc1fe6e723acf9d044da6b1468	assistive technology design and preliminary testing of a robot platform based on movement intention using low-cost brain computer interface		The process through which children learn about the world and develop perceptual, cognitive and motor skills relies heavily on object exploration in their physical world. New types of assistive technology that enable children with impairments to interact with their environment have emerged in recent years, and they could be beneficial for children's cognitive and perceptual skills development. Many studies have reported on brain computer interface (BCI) research. However, a conventional electroencephalography (EEG) system is generally bulky and expensive. It also requires special equipment and technical expertise to operate successfully. In this study, a compact low-cost EEG system was used to detect signals related to movement intention and control a mobile robot control. EEG signals of three non-disabled adults were acquired by the BCI system and the movement intention was classified during physical movement and motor imagery. The average classification accuracies achieved during testing were 56.4% for the motor imagery and 72.7% for the physical movement. The results show moderate classification accuracy for the motor imagery; however, the classification accuracy for the physical movement was high for all the subjects. Even though further improvement of the system is still needed, the experimental results demonstrated the feasibility of a BCI-based robotic system that is affordable and accessible for many people including children with disabilities.	assistive technology;brain–computer interface;cognition;electroencephalography;entity–relationship model;futures studies;linear classifier;mobile robot;openbci;population;real-time locating system;robot control;sampling (signal processing)	Isao Sakamaki;Camilo Ernesto Perafan del Campo;Sandra A. Wiebe;Mahdi Tavakoli;Kim Adams	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122954	simulation;visualization;machine learning;motor skill;mobile robot;motor imagery;perception;artificial intelligence;perceptual learning;computer science;design technology;cognition	Robotics	11.635691647602979	-92.69133458437801	16032
80ad2ecf4a878d088a1046cac731711589760b45	application of a membrane protein structure prediction web service gpcrm to a gastric inhibitory polypeptide receptor model		A novel versatile tool named GPCRM has been developed. It targets structure prediction of a distinct protein family of G protein-coupled receptors (GPCRs). In principle, GPCRM builds a GPCR model using a MODELLER-based homology modeling procedure. In addition, that commonly used procedure was improved by using comparison of sequence profiles, multiple template structures and the extensive loop refinement in Rosetta. We applied our method to predict a three dimensional structure of a gastric inhibitory polypeptide receptor (GIPR) from the secretin-like class B of human GPCRs. The GIPR model was also tested in an ensemble docking study in which we investigated plausible interactions of four potential antagonists with that receptor. Out of those four ligands we suggested ChEMBL_1933363 as the most potent antagonist of GIPR based on the Glide docking results.	protein structure prediction;web service	Ewelina Rutkowska;Przemyslaw Miszta;Krzysztof Mlynarczyk;Jakub Jakowiecki;Pawel Pasznik;Sławomir Filipek;Dorota Latek	2017		10.1007/978-3-319-56154-7_15	endocrinology;biochemistry;bioinformatics	ML	9.535075777228128	-59.39104780533277	16039
14fc02a35d773636e1753324a7f01471404ea166	variational frameworks for dt-mri estimation, regularization and visualization	diffusion tensor imaging visualization tensile stress magnetic resonance imaging pipelines symmetric matrices neurons magnetic noise biomedical imaging motion measurement;diffusion tensor magnetic resonance imaging;white matter;image resolution;mrl images;processing pipeline;magnetic resonance image;data visualisation;visualization;fiber bundle visualization;diffusion tensor estimation;variational framework;regularization process dt mri diffusion tensor magnetic resonance imaging diffusion tensor estimation fiber bundle visualization variational framework processing pipeline mrl images human brain;dt mri;human brain;diffusion tensor;biomedical mri;regularization process;tensors;image resolution biomedical mri data visualisation tensors	We address three crucial issues encountered in DT-MRI (Diffusion Tensor Magnetic Resonance Imaging) : diffusion tensor Estimation, Regularization and fiber bundle Visualization. We first review related algorithms existing in the literature and propose then alternative variational formalisms that lead to new and improved schemes, thanks to the preservation of important tensor constraints (positivity, symmetry). We illustrate how our complete DT-MRI processing pipeline can be successfully used to construct and draw fiber bundles in the white matter of the brain, from a set of noisy raw MRI images.	algorithm;computation;numerical analysis;resonance;variational principle	David Tschumperlé;Rachid Deriche	2003		10.1109/ICCV.2003.1238323	diffusion mri;computer vision;visualization;tensor;image resolution;computer science;magnetic resonance imaging;mathematics;geometry;data visualization	Vision	50.82218604430821	-79.66707687137166	16057
4b3ad08cc3a62c6975bd482e00eb481be7c311b7	on evaluating methods for recovering image curve fragments	databases;image segmentation;edge detection;differential geometry;semantics;motion segmentation;image edge detection;image color analysis;image reconstruction;image reconstruction differential geometry edge detection;edge linkers evaluating methods image curve fragment recovery unorganized edge elements contour grouping algorithms contour segments human ground truth data collection contour fragments curve fragment formation deformation exploration geodesic paths;humans;humans image edge detection image segmentation semantics databases image color analysis motion segmentation	Curve fragments, as opposed to unorganized edge elements, are of interest and use in a large number of applications such as multiview reconstructions, tracking, motion-based segmentation, and object recognition. A large number of contour grouping algorithms have been developed, but progress in this area has been hampered by the fact that current evaluation methodologies are mainly edge-based, thus ignoring how edges are grouped into contour segments. We show that edge-based evaluation schemes work poorly for the comparison of curve fragment maps, motivating two novel developments: (i) the collection of new human ground truth data whose primary representation is contour fragments and where the goal of collection is not distinguished objects but curves evident in image data, and (ii) a methodology for comparing two sets of curve fragments which takes into account the instabilities inherent in the formation of curve fragments. The approach compares two curve fragment sets by exploring deformation of one onto another while traversing discontinuous transitions. The geodesic paths in this space represent the best matching between the two sets of contour fragment. This approach is used to compare the results of edge linkers on the new contour fragment human ground truth.	algorithm;edge enhancement;ground truth;multiview video coding;outline of object recognition	Yuliang Guo;Benjamin B. Kimia	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6238927	iterative reconstruction;computer vision;edge detection;topology;computer science;mathematics;geometry;semantics;image segmentation	Vision	52.53227563514682	-53.589189598846716	16062
d66600eb2af92e92dff0e342151da5b4fddcf497	a segmentation method based on dynamic programming for breast mass in mri images	edge detection;dynamic program;magnetic resonance image;region of interest;cumulant	The tumor segmentation in Breast MRI image is difficult due to the complicated galactophore structure. The work in this paper attempts to accurately segment the abnormal breast mass in MRI(Magnetic resonance imaging) Images. The ROI (Region of Interest) is segmented using a novel DP (Dynamic Programming) based optimal edge detection technique. DP is an optimal approach in multistage decision-making. The method presented in this paper processes the object image to get the minimum cumulative cost matrix combining with LUM nonlinear enhancement filter, Gaussian preprocessor, nonmaximum suppression and double-threshold filtering, and then trace the whole optimal edge. The experimental results show that this method is robust and efficient on image edge detection and can segment the breast tumor area more accurately.	dynamic programming	Jihong Liu;Weina Ma;Soo-Young Lee	2008		10.1007/978-3-540-77413-6_39	computer vision;mathematical optimization;computer science;machine learning	Vision	39.42662041466107	-75.93884972512186	16067
48bb92f4d339ef93264530282e73d5bb8f92ca55	critical operations in low-level human vision		This article examines the mechanisms of the initial stages of human vision and describes the critical functions that these processes have to achieve. The basis for the report is a series of computational studies that have sought to place what is known about human vision in the contexts of real tasks. Most considerations of low-level vision have focused on filtering and pointwise nonlinearities. Extending beyond what is already known, the present studies have identified the need for specific processes, over and above what is already known, to deal with the problems of (1) Contrast equalization: Differences in local illumination due to shadows and highlights cause variations in mean luminance and also in local contrast-the amplitude of luminance variations. This has severe effects for the filtering stage. (2) Segmentation: The image itself is a continuous and uniform data structure. It needs to be split into small primitive features from which object representations can be built. (3) Grouping: The primitive features are almost bound to be too primitive to be useful, singly, for any real visual tasks. Related primitive features that belong together need to be grouped together in a relatively tight fashion. The nature of these difficulties is examined, and potential approaches are considered and illustrated. A major feature of the work described concerns the formal properties of the data structures that will be required to hold the different types of information. Three distinct structures are considered: images, image descriptions, and visual descriptions. The latter two of these are novel.	computation;data structure;high- and low-level;list of common shading algorithms;xslt/muenchian grouping	R. J. Watt	1996	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(199622)7:2%3C65::AID-IMA1%3E3.0.CO;2-Q	computer science	Vision	51.7465462257779	-60.9343327857579	16085
8b085c3215389c4107850eac223c3e3ad253901d	automatic recognition of activities of daily living utilizing insole-based and wrist-worn wearable sensors		Automatic recognition of activities of daily living (ADL) is an important component in understanding of energy balance, quality of life, and other areas of health and well-being. In our previous work, we had proposed an insole-based activity monitor—SmartStep, designed to be socially acceptable and comfortable. The goals of the current study were: first, validation of SmartStep in recognition of a broad set of ADL; second, comparison of the SmartStep to a wrist sensor and testing these in combination; third, evaluation of SmartStep's accuracy in measuring wear noncompliance and a novel activity class (driving); fourth, performing the validation in free living against a well-studied criterion measure (ActivPAL, PAL Technologies); and fifth, quantitative evaluation of the perceived comfort of SmartStep. The activity classification models were developed from a laboratory study consisting of 13 different activities under controlled conditions. Leave-one-out cross validation showed 89% accuracy for the combined SmartStep and wrist sensor, 81% for the SmartStep alone, and 69% for the wrist sensor alone. When household activities were grouped together as one class, SmartStep performed equally well compared to the combination of SmartStep and wrist-worn sensor (90% versus 94%), whereas the accuracy of the wrist sensor increased marginally (73% from 69%). SmartStep achieved 92% accuracy in recognition of nonwear and 82% in recognition of driving. Participants then were studied for a day under free-living conditions. The overall agreement with ActivPAL was 82.5% (compared to 97% for the laboratory study). The SmartStep scored the best on the perceived comfort reported at the end of the study. These results suggest that insole-based activity sensors may present a compelling alternative or companion to commonly used wrist devices.	activity tracker;computer monitor;cross infection;cross-validation (statistics);pal;score;sixty nine;wearable computer;wearable technology;sensor (device)	Nagaraj Hegde;Matthew Bries;Tracy A. Swibas;Edward S. Sazonov	2018	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2017.2734803	wearable electronic device;activities of daily living;accelerometer;physical therapy;wearable computer;computer science;wrist	HCI	8.759806787948978	-85.50062612936381	16094
b0cc8c0f2d5b5ce506d336e1a7c81831b7ae40c8	recognition of printed text under realistic conditions	classification;reconnaissance caractere;pattern recognition;ocr;reconnaissance forme;reconocimiento patron;character recognition;clasificacion;reconocimiento caracter	Abstract   Past research in OCR has focused on the shape analysis of binarized images, quite often assuming good quality document and isolated characters. Such assumptions are challenged by the conditions met in practice: binarization is difficult for low contrast documents, characters often touch each other, not only on the sides but also between lines, etc. After a brief review of past work we will describe current efforts to deal with OCR as a signal processing problem where the causes of noise and distortions as well the idealized images (definitions of typefaces) are modeled and subjected to a quantitative analysis. The key idea of the analysis is that while printed text images may be binary in an ideal state, the images seen by the sensors are gray scale because of convolution distortion and other causes. Therefore binarization should be carried out at the same time as feature extraction.	printing	Theodosios Pavlidis	1993	Pattern Recognition Letters	10.1016/0167-8655(93)90097-W	computer vision;speech recognition;biological classification;computer science;artificial intelligence;machine learning;pattern recognition	Vision	34.61156358902925	-67.24594890193009	16116
f1ebc2abfb5a5ff3df5f60ac3a4341b086cae018	a new two-stage scheme for the recognition of persian handwritten characters	image sampling;handwriting recognition;support vector machines;chain code;support vector machines persian handwritten isolated character recognition feature extraction undersampled bitmaps technique chain code direction frequencies classifiers;persian character recognition;undersampled bitmaps;undersampled bitmaps technique;image classification;chain code direction frequencies;persian handwritten isolated character recognition;accuracy;shape;handwritten character recognition persian character recognition svm undersampled bitmaps chain code;feature extraction;pixel;natural language processing feature extraction handwritten character recognition image classification image sampling;classifiers;svm;computer science;support vector machine;character recognition;character recognition feature extraction shape support vector machines accuracy handwriting recognition pixel;natural language processing;handwritten character recognition	In this paper, a two-stage scheme for the recognition of Persian handwritten isolated characters is proposed. In the first stage, similar shaped characters are categorized into groups and as a result, 8 groups are obtained from 32 Persian basic characters. In the second stage, the groups containing more than one similar shape characters are considered further for the final recognition. Feature extraction is based on under sampled bitmaps technique and modified chain-code direction frequencies. For the first stage features, we compute 49-dimension features based on under sampled bitmaps from 49 non-overlapping 7×7 window-maps. 196-dimension chain-code direction frequencies from 49 overlapping 9×9 window-maps are computed and used as features for the second stage of the proposed scheme. Classifiers are one-against-other support vector machines (SVM). We evaluated our scheme on a standard dataset of Persian handwritten characters. Using 36682 samples for training, we tested our scheme on other 15338 samples and obtained 98.10% and 96.68% correct recognition rates when considered 8-class and 32-class problems, respectively.	bitmap;categorization;chain code;feature extraction;map;shape context;support vector machine	Alireza Alaei;P. Nagabhushan;Umapada Pal	2010	2010 12th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2010.27	support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;handwriting recognition	Vision	33.6495286338832	-65.55248526066121	16148
eb59220b17960c1e20f53816f898ced4806d1d6f	detection of tumor in brain mri using fuzzy feature selection and support vector machine	image segmentation;mr images;svm	This paper proposes a technique to categorize a brain MRI as normal, in the absence of a brain tumor or as abnormal in the presence of one. Proposed method is divided into two steps. First, a set of feature is generated for accurately differentiating between a normal and abnormal MR scan images. Then, these features are reduced using fuzzy c-means (FCM) algorithm. Further, a Support Vector Machine (SVM) is used to classify the scan images into two groups, namely, tumor-free and tumor affected. The proposed method aims to produce higher specificity and sensitivity than the previous methods.	algorithm;categorization;feature selection;fuzzy clustering;fuzzy cognitive map;sensitivity and specificity;support vector machine	Amiya Halder;Oyendrila Dobe	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732331	support vector machine;computer vision;computer science;machine learning;pattern recognition;image segmentation	Robotics	35.02994974210313	-75.92545878784979	16155
aee0026fe0887ad77a8523da627eddb7a85866ea	a novel cancelable iris recognition system based on feature learning techniques		A novel cancelable iris recognition system is proposed in this paper. Based on the performance of various feature learning techniques such as (i) Bag-of-Words, (ii) Sparse Representation Coding and (iii) Locality-constrained Linear Coding we choose the second one followed by Spatial Pyramid Mapping technique for feature computation from iris pattern. To build the proposed system the existing BioHashing technique is modified using two different tokens: one is user specific and the other user independent. To test the performance of the proposed system we have tried it on six benchmark iris databases namely: MMU1, UPOL, CASIA-Interval-v3, IITD, UBIRIS.v1 and CASIA-syn. The experimental results are demonstrated for each database and are compared with that of the state-of-the-art methods with respect to these databases. The results show the robustness and effectiveness of the proposed approach.	feature learning;iris recognition	Saiyed Umer;Bibhas Chandra Dhara;Bhabatosh Chanda	2017	Inf. Sci.	10.1016/j.ins.2017.04.026	computer vision;speech recognition;pattern recognition	AI	33.40339008355409	-59.49130789410268	16170
78461da4518c0c5164c60317e9eac4b6ebba8aa6	distinct neural representations of placebo and nocebo effects		"""Expectations shape the way we experience the world. In this study, we used fMRI to investigate how positive and negative expectation can change pain experiences in the same cohort of subjects. We first manipulated subjects' treatment expectation of the effectiveness of three inert creams, with one cream labeled """"Lidocaine"""" (positive expectancy), one labeled """"Capsaicin"""" (negative expectancy) and one labeled """"Neutral"""" by surreptitiously decreasing, increasing, or not changing respectively, the intensity of the noxious stimuli administered following cream application. We then used fMRI to investigate the signal changes associated with administration of identical pain stimuli before and after the treatment and control creams. Twenty-four healthy adults completed the study. Results showed that expectancy significantly modulated subjective pain ratings. After controlling for changes in the neutral condition, the subjective pain rating changes evoked by positive and negative expectancies were significantly associated. fMRI results showed that the expectation of an increase in pain induced significant fMRI signal changes in the insula, orbitofrontal cortex, and periaqueductal gray, whereas the expectation of pain relief evoked significant fMRI signal changes in the striatum. No brain regions were identified as common to both """"Capsaicin"""" and """"Lidocaine"""" conditioning. There was also no significant association between the brain response to identical noxious stimuli in the pain matrix evoked by positive and negative expectancies. Our findings suggest that positive and negative expectancies engage different brain networks to modulate our pain experiences, but, overall, these distinct patterns of neural activation result in a correlated placebo and nocebo behavioral response."""	capsaicin;central gray substance of midbrain;conditioning (psychology);dairy cream;experience;insula of reil;lidocaine;modulation;neostriatum;pain relief;tracer;fmri	Sonya Freeman;Rongjun Yu;Natalia Egorova;Xiaoyan Chen;Irving Kirsch;Brian Claggett;Ted J. Kaptchuk;Randy L. Gollub;Jian Kong	2015	NeuroImage	10.1016/j.neuroimage.2015.03.015	psychology;developmental psychology;anesthesia;social psychology	HCI	18.006459313372492	-78.69416713648862	16180
02eca7eae233580e8a1a7c8f0abb0bad26496018	analysis of the pelvic-to-external urethral sphincter reflex in intact and acute spinalized rats	external urethral sphincter	The lower urinary tract has two main functions: storage and periodic elimination of urine. These functions are regulated by a complex neural control system located in the brain and spinal cord which coordinates the activity of the two components of the lower urinary tract: (1) the reservoir (urinary bladder) and (2) the outlet (bladder neck, urethra and urethral sphincter). Normally these structures exhibit reciprocal activity. During urine storage the reservoir is quiescent and intravesical pressure remains low whereas activity in the outlet gradually increases during bladder filling to maintain continence. The relationship between the bladder and external urethral sphincter (EUS) is controlled by reflex pathways in the lumbosacral spinal cord that are activated by primary afferent input from the bladder or the urethra. Bladder afferent axons pass through the pelvic nerves and urethral afferent axons pass through the pelvic and pudendal nerves. This study was conducted to examine the reflexes that mediate bladder and sphincter coordination. The physiological properties of the pelvic nerve afferent to EUS reflex (pelvic-to-EUS reflex) were recorded in intact rats. Reflex responses consisted of a large early component and a small late component when the bladder was distended. However, the late component was not present when the bladder was empty. The stimulus threshold is 0.8 volts at 1 Hz with 0.05 ms pulse duration, and the latency of early component was 19.61 ± 0.16 ms during different frequencies of stimulation between 0.5~5 Hz. The small late component gone after acute spinal cord transection indicated the late responses of the pelvic-to-EUS reflex were mediated by supraspinal reflex pathway. Furthermore, the time-frequency Analysis by Matlab was utilized to verify the early and late components of the reflex. The frequencies of early component were between 100~600 Hz at consistent latency in both animal models. Only late component was induced by stimulus in intact rats, and the frequencies were variable from 100 Hz to 1000 Hz at different latencies. The firing amplitude was markedly smaller in spinalized rats than intact animals. The early and late components of the pelvic-to-EUS reflex were recognized by time-frequency analysis at timing, frequencies and amplitude.	access time;control system;digital single-lens reflex camera;frequency analysis;gene regulatory network;matlab;pulse duration;quiesce;sacral nerve stimulation;time–frequency analysis;tract (literature)	Hui-Yi Chang;Chi-Wei Peng;Jia-Jin Chen;Chen-Li Cheng;William C. de Groat	2004			urology;urethral sphincter;external sphincter muscle of female urethra	ML	18.67012814441565	-82.16588042308227	16194
448c410aec12082c2920d17440c8dfeaaf9e494b	learning about animals and their social behaviors for smart livestock monitoring		Things are increasingly getting connected. Emerging with the Internet of Things, new applications are requiring more intelligence on these things, for them to be able to learn about their environment or other connected objects. One such domain of application is for livestock monitoring, in which farmers need to learn about animals, such as percentage of time they spend feeding, the occurrence of diseases, or the percentage of fat on their milk. Furthermore, it is also important to learn about group patterns, such as flocking behaviors, and individual deviations to group dynamics. This paper addresses this problem, by collection and processing each animal location and selecting appropriate metrics on the data, so that behaviors can be learned afterwards using machine learning techniques running on the cloud.		João Ambrósio;Artur M. Arsénio;Orlando Remedios	2015		10.1007/978-3-319-47075-7_53	computer security;wireless sensor network;computer science;cloud computing;social behavior;livestock;internet of things	AI	4.75803256953758	-86.3966055974266	16214
456f79f6400c49c9e5656395834c7b1b02d9f272	a causal extraction scheme in top-down pyramids for large images segmentation	high resolution;image segmentation;top down;low resolution;tiled data structure;image analysis;irregular pyramid;data structure;topological model;combinatorial map	Applicative fields based on the analysis of large images must deal with two important problems. First, the size in memory of such images usually forbids a global image analysis hereby inducing numerous problems for the design of a global image partition. Second, due to the high resolution of such images, global features only appear at low resolutions and a single resolution analysis may loose important information. The tiled top-down pyramidal model has been designed to solve this two major challenges. This model provides a hierarchical encoding of the image at single or multiple resolutions using a top-down construction scheme. Moreover, the use of tiles bounds the amount of memory required by the model while allowing global image analysis. The main limitation of this model is the splitting step used to build one additional partition from the above level. Indeed, this step requires to temporary refine the split region up to the pixel level which entails high memory requirements and processing time. In this paper, we propose a new splitting step within the tiled top-down pyramidal framework which overcomes the previously mentioned limitations.	algorithm;ambiguous name resolution;applicative programming language;causal filter;computation;high memory;image analysis;image resolution;parallel computing;pixel;pyramid (geometry);refinement (computing);requirement;time complexity;top-down and bottom-up design	Romain Goffe;Guillaume Damiand;Luc Brun	2010		10.1007/978-3-642-14980-1_25	computer vision;theoretical computer science;mathematics;computer graphics (images)	Vision	47.59977925489497	-67.3214783127528	16224
