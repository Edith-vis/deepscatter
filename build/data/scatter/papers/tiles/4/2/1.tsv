id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
1531c4a26e02ccb431583cba58dc174805dcacba	incorporating the effects of hike in energy prices into energy consumption forecasting: a fuzzy expert system		This paper proposes an adaptive fuzzy expert system to concurrently estimate and forecast both long-term electricity and natural gas (NG) consumptions with hike in prices. Using a novel procedure, the impact of price hike is incorporated into energy demand modeling. Furthermore, adaptive network-based FIS (ANFIS) is used to model NG consumption in power generation (NGPG). To cope with random uncertainty in small historical data sets, Monte Carlo simulation is used to generate training data for ANFIS. The proposed ANFIS uses electricity consumption data to improve the estimation of total NG consumption. The unique contribution of this paper is three fold. First, it proposes a novel expert system for electricity consumption and NG consumption in end-use sector with hike in prices. Second, it uses ANFIS-Monte Carlo approach for NGPG. Third, electricity consumption is used in ANFIS for improvement of NGPG consumption estimation. A real case study is presented that illustrates the applicability and usefulness of the proposed model where it is applied for joint forecasting of annual electricity and NG consumption with hike in prices.	adaptive neuro fuzzy inference system;elasticity (cloud computing);expert system;first-order predicate;fuzzy concept;fuzzy rule;inference engine;matlab;monte carlo method;price point;rule-based system;serial ata;simulation	Vahid Majazi Dalfard;M. Nazari Asli;Salman Nazari Shirkouhi;S. M. Sajadi;Seyed Mohammad Asadzadeh	2012	Neural Computing and Applications	10.1007/s00521-012-1282-x	simulation	AI	8.194874174764953	-18.355106136151786	4276
9f9d670c4a653ac71ac5bbb0a49a358954dd81f1	scalability and efficiency of genetic algorithms for geometrical applications	theoretical model;algorithm performance;probleme np complet;population size;optimization method;cartographie;algoritmo genetico;metodo optimizacion;cartografia;resultado algoritmo;performance algorithme;methode optimisation;algorithme genetique;cartography;algorithme evolutionniste;genetic algorithm;problema np completo;algoritmo evolucionista;evolutionary algorithm;np complete problem	We study the scalability and efficiency of a GA that we developed earlier to solve the practical cartographic problem of labeling a map with point features. We argue that the special characteristics of our GA make that it fits in well with theoretical models predicting the optimal population size (the Gambler's Ruin model) and the number of generations until convergence. We then verify these predictions experimentally. It turns out that our algorithm indeed performs according to the theory, leading to a scale-up for the total amount of computational effort that is linear in the problem size.	genetic algorithm;scalability	Steven van Dijk;Dirk Thierens;Mark de Berg	2000		10.1007/3-540-45356-3_67	mathematical optimization;population size;genetic algorithm;np-complete;computer science;artificial intelligence;machine learning;evolutionary algorithm;calculus;mathematics;algorithm	AI	27.250854737968652	2.113447145961553	4283
23c6cfb87fca1afe926ef3888817a708d6c6c1ca	a discrete constraint-based method for pipeline build-up aware services sales forecasting		Services organizations maintain a pipeline of sales opportunities with different maturity level (belonging to progressive sales stages), lifespan (time to close) and contract values at any time point. As time goes, some opportunities close (contract signed, or lost) and new opportunities are added to the pipeline. Accurate forecasting of contract signing by the end of a time period (e.g., quarterly) is highly desirable to make appropriate sales activity management with respect to the projected revenue. While the problem of sales forecasting has been investigated in general, two specific aspects of sales engagement for services organizations, which entail additional complexity, have not been thoroughly investigated: (i) capturing the growth trend of current pipeline, and (ii) incorporating current pipeline build-up in updating the prediction model. We formulate these two issues as a dynamic curve-fitting problem in which we build a sales forecasting model by balancing the effect of current pipeline data and the model trained based on historical data. There are two challenges in doing so, (i) how to mathematically define such a balance and (ii) how to dynamically update the balance as more new data become available. To address these two issues, we propose a novel discrete-constraint method (DCM). It achieves the balance via fixing the value of certain model parameters and applying a leave-one-out algorithm to determine an optimal free parameter number. By conducting experiments on real business data, we demonstrate the superiority of DCM in sales pipeline forecasting.		Peifeng Yin;Aly Megahed;Hamid R. Motahari Nezhad;Taiga Nakamura	2016		10.1007/978-3-319-46295-0_57	real-time computing;data mining	Robotics	5.789110831440838	-14.729840298932755	4289
fe19096fe744a7ee4220239532941a3640f6e72a	self-segmentation of sequences	neural networks reinforcement learning self segmentation hierarchical structures task learning;dynamic programming;hierarchical structure;neural networks;learning;neural nets;reinforcement learning;learning artificial intelligence neural nets;state estimation;hierarchical structures;learning equations national electric code dynamic programming costs state estimation temperature;self segmentation;task learning;national electric code;temperature;learning artificial intelligence	|The paper presents an approach for hierarchical reinforcement learning that does not rely on a pri-ori hierarchical structures. Thus the approach deals with a more diicult problem compared with existing work. It involves learning to segment sequences to create hierarchical structures, based on reinforcement received during task execution , with diierent levels of control communicating with each other through sharing reinforcement estimates obtained by each others. The algorithm segments sequences to reduce non-Markovian temporal dependencies, to facilitate the learning of the overall task. Initial experiments demonstrated the basic promise of the approach.	algorithm;experiment;reinforcement learning	Ron Sun;Chad Sessions	1999		10.1109/IJCNN.1999.833413	national electrical code;temperature;computer science;artificial intelligence;machine learning;dynamic programming;pattern recognition;artificial neural network	AI	18.95333837277738	-23.23203038183008	4292
a9cfe1d610442261831b313d2b5eca987e841b19	a study on green manufacturing in a car battery manufacturing plant	environmental operations management;car battery manufacturing;cleaner production;pollution prevention;green manufacturing;environmentally integrated manufacturing	This study presents an environmental manufacturing system analysis for companies looking for the benefits of environmental management in achieving high productivity levels. When the relationship between environmental costs and manufacturing decisions is examined, it can be seen that the productivity of the company can be increased by adopting a methodology of an environmentally integrated manufacturing system analysis. This study presents such a methodology and the roadmap for generating environmentally friendly and economically favorable alternative waste management solutions is elaborated. The methodology combines data collection, operational analysis of the manufacturing processes, identification of wastes, and evaluation of waste reduction alternatives. The presented methodology is examined in a car battery manufacturing plant, which generates hazardous wastes composed of lead. It is aimed to decrease the wastes derived from the production so that the efficiency in raw materials usage is increased and the need for recycling the hazardous wastes is decreased. A Study on Green Manufacturing in a Car Battery Manufacturing Plant	environmental resource management;operations research;system analysis	Sedef Ergün;Sibel Uludag-Demirer;Suat Kasap	2013	IJAL	10.4018/ijal.2013100103	cleaner production;operations management;ecology;advanced manufacturing	Robotics	7.788285781663828	-6.3474565468533815	4298
66f7ae4940680a36039d53add7980b1abdd2a611	dendroclim2002: a c++ program for statistical calibration of climate signals in tree-ring chronologies ☆	computadora;contraste;computers;software;correlacion;analisis componente principal;modele numerique;paleoclima;bootstrap;dendrochronologie;dendrocronologia;logiciel;ordinateur;principal component regression;tree growth;paleoclimate;dendroclimatology;computer programs;algorithme;trees;confidence interval;dendrochronology;response function;statistical analysis;principal components analysis;tree rings;intervalle confiance;analyse statistique;analyse composante principale;arbre;algorithms;croissance;etalonnage;correlation;numerical models;growth;programa computador;calibration;tree ring;paleoclimat;programme ordinateur;temporal change;algoritmo	Tree-ring chronologies are often calibrated against instrumental climate records using correlation and response functions. DENDROCLIM2002 uses bootstrapped confidence intervals to estimate the significance of both correlation and response function coefficients. Input and output file selection, as well as analytical options, are chosen from a userfriendly GUI. Final results are saved in ASCII format, and are plotted on screen using color-coded symbols. DENDROCLIM2002 is an extension of existing task-specific software, which is mostly MS-DOS based, and of available user-supplied code for statistical packages, such as SAS. In addition, DENDROCLIM2002 incorporates the ability to test for temporal changes of dendroclimatic relationships by means of evolutionary and moving intervals. This simple approach allows for a complete, dynamical representation of statistical relationships between climate and tree growth. An example using published dendroclimatic data is used to illustrate the analytical and graphical capabilities of the software. r 2004 Elsevier Ltd. All rights reserved.	c++;calibration (statistics);coefficient;computer program;dos;dynamical system;embedded system;frequency response;graphical user interface;list of statistical packages;ms-dos;proxy server;sas	Franco Biondi;Kishor Waikul	2004	Computers & Geosciences	10.1016/j.cageo.2003.11.004	econometrics;geology;paleoclimatology;mathematics;dendrochronology;algorithm;statistics	AI	35.24603336679406	-22.45544409249469	4303
cd78ce6ba40ad2274e88dccf60a528aa30b54966	solving large single allocation p-hub problems with two or three hubs	mixed integer linear program;location problem;nodes;location;allocation;linear programming;linear program;branch and bound	In this paper we present an efficient approach for solving single allocation  p -hub problems with two or three hubs. Two different variants of the problem are considered: the uncapacitated single allocation  p -hub median problem and the  p -hub allocation problem. We solve these problems using new mixed integer linear programming formulations that require fewer variables than those formerly used in the literature. The problems that we solve here are the largest single allocation problems ever solved. The numerical results presented here will demonstrate the superior performance of our mixed integer linear programs over traditional approaches for large problems. Finally we present the first mixed integer linear program for solving single allocation hub location problems that requires only O( n  2 ) variables and O( n  2 ) constraints that is valid for any number of hubs.	usb hub	Jamie Ebery	2001	European Journal of Operational Research	10.1016/S0377-2217(99)00370-7	mathematical optimization;combinatorics;discrete mathematics;covering problems;computer science;linear programming;branch and price;mathematics;node;location;branch and bound	HPC	16.734320399380305	3.288896556361264	4306
3ed751b8cea3dcd7ea05df4de110a54c2b097ff4	economic design of the mean prognostic distance for canary-equipped electronic systems	economic analysis;detection panne;electronic component;failure detection;simulacion numerica;distance measurement;medicion distancia;simulation numerique;defaillance;composant electronique;analyse economique;failures;deteccion falla;fallo;mesure de distance;analisis economico;numerical simulation;componente electronico	Canaries are a type of early warning device that can be embedded or mounted on electronic systems to issue a pre-failure warning to the host system. A canary is designed to fail faster than the host system. The distance from the time of canary failure to host system failure is called the prognostic distance of the canary. This distance is clearly random depending on the design and operating characteristics of each particular canary and host system, so in practice we often refer to the mean prognostic distance as the measure of prognostic distance. The mean prognostic distance is often determined during the design stage of the canary and the system without taking into account the cost associated with system failure. Clearly, if the consequence of system failure is severe, then the mean prognostic distance should be longer. This paper introduces an economic design for the mean prognostic distance in a canary-equipped system from the perspective of an economic analysis. We establish the necessary conditions for such an economic-based mean prognostic distance to exist and demonstrate it through numerical examples.		Wenbin Wang;Shuxin Luo;Michael G. Pecht	2012	Microelectronics Reliability	10.1016/j.microrel.2011.12.010	computer simulation;econometrics;simulation;engineering;electrical engineering;electronic component	EDA	26.737029732457295	-16.61839012210984	4307
ac8ab8cb0b3c0373293ee74a65d7579176734406	a new gravitational search optimization algorithm to solve single and multiobjective optimization problems	mutation method;gravitational search algorithm;numerical methods;multi objective mo	This paper proposes a sufficient improved optimization algorithm based on gravitational search algorithm GSA to handle complex multi-objective MO optimization problems. The main idea behind the GSA is based on the Newton's law which has shown great success in recent years. Nevertheless, the existence of some deficiencies such as dependency of the algorithm on its adjusting parameters, stagnation and the probability of trapping in local optima has reduced the searching ability of the GSA. In order to overcome the above deficiencies, this paper suggests a novel mutation technique which is implemented based on four types of solutions. The first type belongs to the non-dominated solutions. The second type includes those solutions which are dominated by only one solution in the search space. Similarly, the third and the forth types belong to those solutions which are dominated by only three and four solutions respectively. After that, the initial GSA population is constructed by choosing from these four types of solutions. This procedure will let the next movement of the algorithm to reach more optimal solutions through the improvisation stage. The satisfying performance of the proposed algorithm is examined on several well-known benchmarks including both the single-objective and multi-objective optimization problems.	algorithm;mathematical optimization;multi-objective optimization	Sajad Tabatabaei	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-130791	mathematical optimization;numerical analysis;computer science;artificial intelligence;mathematics;algorithm	AI	27.501642580002464	-4.051592631722629	4317
b3ee38b4fb7201cdfddba69bcc567ab607b1f1b4	combnet-iii: a support vector machine based large scale classifier with probabilistic framework	desciframiento;traitement signal;agregacion;methode diviser pour regner;evaluation performance;tecnologia electronica telecomunicaciones;large scale classification problems;performance evaluation;decodage;learning;decoding;support vector machines;complexite calcul;estudio comparativo;multilayer perceptrons;evaluacion prestacion;speech processing;modele markov variable cachee;maquina vector soporte;tratamiento palabra;metodo dividir para vencer;traitement parole;posterior probability;probabilistic approach;aggregation;aprendizaje;perceptron multicouche;etude comparative;accuracy;machine vecteur support;large scale;codificacion;complejidad computacion;apprentissage;precision;red multinivel;hidden markov models;reconocimiento voz;probabilistic framework;computational complexity;enfoque probabilista;approche probabiliste;feature extraction;signal processing;probabilite a posteriori;divide and conquer method;signal classification;coding;comparative study;probabilidad a posteriori;pattern recognition;agregation;classification signal;speech recognition;reconnaissance forme;reconnaissance parole;multilayer network;support vector machine;extraction caracteristique;reseau multicouche;classification automatique;reseau neuronal;tecnologias;reconocimiento patron;escala grande;grupo a;automatic classification;procesamiento senal;clasificacion automatica;divide and conquer;red neuronal;handwritten character recognition;reconnaissance caractere manuscrit;codage;neural network;echelle grande	Several research fields have to deal with very large classification problems, e.g. handwritten character recognition and speech recognition. Many works have proposed methods to address problems with large number of samples, but few works have been done concerning problems with large numbers of classes. CombNET-II was one of the first methods proposed for such a kind of task. It consists of a sequential clustering VQ based gating network (stem network) and several Multilayer Perceptron (MLP) based expert classifiers (branch networks). With the objectives of increasing the classification accuracy and providing a more flexible model, this paper proposes a new model based on the CombNET-II structure, the CombNET-III. The new model, intended for, but not limited to, problems with large number of classes, replaces the branch networks MLP with multiclass Support Vector Machines (SVM). It also introduces a new probabilistic framework that outputs posterior class probabilities, enabling the model to be applied in different scenarios (e.g. together with Hidden Markov Models). These changes permit the use of a larger number of smaller clusters, which reduce the complexity of the final classifiers. Moreover, the use of binary SVM with probabilistic outputs and a probabilistic decoding scheme permit the use of a pairwise output encoding on the branch networks, which reduces the computational complexity of the training stage. The experimental results show that the proposed model outperforms both the previous model CombNET-II and a single multiclass SVM, while presenting considerably smaller complexity than the latter. It is also confirmed that CombNET-III classification accuracy scales better with the increasing number of clusters, in comparison with CombNET-II. key words: large scale classification problems, support vector machines, probabilistic framework, divide-and-conquer	cluster analysis;computational complexity theory;expert system;handwriting recognition;hidden markov model;markov chain;memory-level parallelism;multilayer perceptron;optical character recognition;speech recognition;support vector machine;vector quantization	Mauricio Kugler;Susumu Kuroyanagi;Anto Satriyo Nugroho;Akira Iwata	2006	IEICE Transactions	10.1093/ietisy/e89-d.9.2533	support vector machine;speech recognition;computer science;artificial intelligence;machine learning;signal processing;speech processing;accuracy and precision;algorithm;statistics	AI	11.009880304705424	-33.42777293693579	4331
720453140809e58d33c5f56fe12a8419384972e4	human action recognition using similarity degree between postures and spectral learning		In recent years, there has been renewed interest in developing methods for skeleton-based human action recognition. In this study, the challenging problem of the similarity degree of skeleton-based human postures is addressed. Human posture is described by screw motions between 3D rigid bodies, which can be seen as a relation matrix of 3D rigid bodies (RMRB3D). A linear subspace, a point of a Grassmannian manifold, is spanned by the orthonormal basis of matrix RMRB3D. A powerful way to compute the similarity degree between postures is researched to solve the geodesic distance between points on the Grassmannian manifold. Then representative postures are extracted through spectral clustering over representative postures. An action will be represented by a symbol sequence generated with a global linear eigenfunction constructed by spectral embedding. Finally, dynamic time warping and hidden Markov model (HMM) are used to classify these action sequences. The experimental evaluations of the proposed method on several challenging 3D action datasets show that the proposed approaches achieve promising results compared with other skeleton-based human action recognition algorithms.		Wenwen Ding;Kai Liu;Hao Chen;Fengqin Tang	2018	IET Computer Vision	10.1049/iet-cvi.2017.0031	geodesic;linear subspace;grassmannian;computer vision;spectral clustering;logical matrix;hidden markov model;artificial intelligence;pattern recognition;dynamic time warping;mathematics;orthonormal basis	Vision	36.11146092465655	-49.75336170393787	4333
5ad34d493af3144696856e1ccb05c0e48cb4e8f3	the application of multiwavelet transform to image coding	transformation ondelette;two dimensional digital filters image coding trees mathematics transform coding wavelet transforms;hierarchical system;set partitioning in hierarchical trees;image coding;image coding multiresolution analysis two dimensional displays vectors image resolution signal resolution filter bank partitioning algorithms compaction;algorithm performance;filter bank;image processing;image resolution;data compression;banc filtre;structure arborescente;multiwavelet coefficients;simulacion numerica;systeme hierarchise;procesamiento imagen;two dimensional displays;transform coding;trees mathematics;hierarchical trees;traitement image;transform domain;experimental result;two dimensional digital filters;wavelet transforms;sistema jerarquizado;codificacion;vectors;compaction;estructura arborescente;resultado algoritmo;banco filtro;tree structure;multiwavelet coefficients multiwavelet transform image coding multiwavelet filter bank two dimensional multiwavelet decomposition hierarchical trees transform domain set partitioning in hierarchical trees algorithm;simulation numerique;coding;performance algorithme;resultado experimental;signal resolution;two dimensional multiwavelet decomposition;set partitioning in hierarchical trees algorithm;rapport signal bruit;relacion senal ruido;compresion dato;transformacion ondita;signal to noise ratio;resultat experimental;multiresolution analysis;multiwavelet filter bank;wavelet transformation;compression donnee;codage;partitioning algorithms;multiwavelet transform;numerical simulation	This work presents a new image coding scheme based on multiwavelet filter banks. First, two dimensional (2-D) multiwavelet decomposition is performed on the original image. Then, several hierarchical trees are constructed in the transform domain, and an extension of set partitioning in hierarchical trees algorithm is proposed to quantize multiwavelet coefficients. Our simulation shows that this scheme is effective and promising.		Gang Lin;Zemin Liu	2000	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.821740	data compression;multiresolution analysis;compaction;computer vision;mathematical optimization;transform coding;image resolution;image processing;computer science;theoretical computer science;filter bank;mathematics;hierarchical control system;tree structure;coding;signal-to-noise ratio;set partitioning in hierarchical trees;algorithm;statistics;wavelet transform	Vision	46.33507139264285	-13.12870121769476	4347
2b2f59d6f37e6ed57982df27268fc913593125ac	no-reference algorithms for video quality assessment based on artifact evaluation in mpeg-2 and h.264 encoding standards	standards;data compression;measurement standards transform coding image coding conferences encoding streaming media;video coding;video coding data compression image sequences standards;blurring video quality assesment no reference h 264 tiling;temporal complexity no reference algorithms video quality assessment artifact evaluation h 264 encoding standards video sequences image decoding mpeg 2 compression standard spatial complexity;image sequences	In this paper, we propose two algorithms to assess quality in video sequences when no reference is available. The algorithms are the result of measuring artifacts such as blurring or tiling to the decoded image; firstly tested in MPEG-2 compression standard, and subsequently, analyzing the passage from this standard to its evolution H.264, taking into account its advanced techniques to mask the degradation, for example, the deblocking filtering or the variation in macroblocks size. The result of each metric affects in a different way depending on the spatial and temporal complexity of the video sequence. Several tests have been developed in a collection of representative sequences to demonstrate the efficiency of the algorithms.	algorithm;deblocking filter;elegant degradation;gaussian blur;h.264/mpeg-4 avc;mpeg-2;macroblock;reference implementation;representative sequences;sensor;tiling window manager	J. P. Lopez;David Jiménez;Ana Cerezo;José Manuel Menéndez	2013	2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)		video compression picture types;data compression;scalable video coding;computer vision;computer science;video quality;deblocking filter;theoretical computer science;video tracking;coding tree unit;block-matching algorithm;multimedia;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.262/mpeg-2 part 2;h.261;statistics;multiview video coding	Arch	44.85077581683876	-18.95125260651319	4361
b84f164dbccb16da75a61323adaca730f528edde	approximate least trimmed sum of squares fitting and applications in image analysis	minimisation;second order cone programming;concave programming;least squares approximations;semidefinite programming;image processing;regression analysis computational complexity concave programming convex programming image processing iterative methods least squares approximations minimisation;convex programming;iterative procedure approximate least trimmed sum of squares fitting image analysis least trimmed sum of squares regression lts regression estimation criterion robust statistical method model fitting np hard concave minimization problem convex minimization probalem concave maximization problem lts approximate complementary problem second order cone program;journal article;robust model fitting;iterative methods;computational complexity;least trimmed sum of squares lts regression;regression analysis;semidefinite programming least trimmed sum of squares lts regression outlier removal robust model fitting second order cone programming;robustness minimization least squares approximation principal component analysis face recognition optimization face;outlier removal	The least trimmed sum of squares (LTS) regression estimation criterion is a robust statistical method for model fitting in the presence of outliers. Compared with the classical least squares estimator, which uses the entire data set for regression and is consequently sensitive to outliers, LTS identifies the outliers and fits to the remaining data points for improved accuracy. Exactly solving an LTS problem is NP-hard, but as we show here, LTS can be formulated as a concave minimization problem. Since it is usually tractable to globally solve a convex minimization or concave maximization problem in polynomial time, inspired by , we instead solve LTS' approximate complementary problem, which is convex minimization. We show that this complementary problem can be efficiently solved as a second order cone program. We thus propose an iterative procedure to approximately solve the original LTS problem. Our extensive experiments demonstrate that the proposed method is robust, efficient and scalable in dealing with problems where data are contaminated with outliers. We show several applications of our method in image analysis.	application domain;approximation algorithm;cobham's thesis;computer vision;convex optimization;data point;entropy maximization;experiment;fits;heuristic;image analysis;inspiration function;iterative method;least squares;long-term survivors;np-hardness;polynomial;robustness (computer science);scalability;second-order cone programming;statistical technique;synthetic intelligence;time complexity	Fumin Shen;Chunhua Shen;Anton van den Hengel;Zhenmin Tang	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2237914	minimisation;econometrics;mathematical optimization;second-order cone programming;image processing;least trimmed squares;computer science;mathematics;iterative method;computational complexity theory;regression analysis;statistics;semidefinite programming	Vision	27.27267761099494	-35.222269525164904	4393
3f8b9878e150959725b6647ea59fd23d1671e35e	memory based differential evolution algorithms for dynamic constrained optimization problems	differential evolution;vectors constraint handling dynamic programming evolutionary computation;maintenance engineering;heuristic algorithms sociology statistics optimization genetic algorithms linear programming maintenance engineering;change frequency memory based differential evolution algorithm dynamic constrained optimization problems hybrid memory scheme short term memory long term memory best relaxed feasible individual best feasible individual iatm method change detection mechanism trial vector memory update operations;heuristic algorithms;statistics;linear programming;constraint handling;genetic algorithms;optimization;sociology;dynamic optimization;differential evolution constraint handling dynamic optimization	A memory based differential evolution algorithm is adapted in this paper to solve Dynamic Constrained Optimization Problems. The approach is based on a mechanism to utilize the useful past information based on the problem special characteristics. A hybrid memory scheme combined short-term and long term memory is adopted based on this approach and reuses the best feasible individual and the best relaxed feasible individual found before changes. Moreover, an IATM method balancing the feasibility, a change detection mechanism based on trial vector, and the memory update operations are adapted to handling the changes on constraint or objective. Finally, the approach was tested on the recently proposed benchmark problems with 1000 change frequency. The results show that the proposed approach provides a very competitive performance with nine other state-of-the-art techniques.	algorithm;benchmark (computing);constrained optimization;differential evolution	Chenggang Cui;Feng Tian;Ning Yang;Junfeng Chen	2015	2015 11th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2015.16	differential evolution;maintenance engineering;mathematical optimization;genetic algorithm;computer science;linear programming;machine learning;algorithm	AI	25.25361888875906	-2.9608652936782534	4434
f178411172d64f2c9529f55dce0e41f232603a1b	adaboost-svm for electrical theft detection and grnn for stealing time periods identification		Electric power companies lose $96 billion every year because of non-technical losses. Electrical theft is the main part of non-technical losses. With the popularization of smart meters in the world, it is possible to detect suspect customers by data mining technology. In this paper, we propose a scheme that can not only find the anomalous users but also determine specific time periods of electric theft. In reality, abnormal customers account for a small part of all electric power users. Firstly, in terms of an imbalanced dataset, we propose an ensemble approach combining Adaptive Boosting algorithm (AdaBoost) and Support Vector Machine (SVM). The proposed scheme uses SVM as a weak classifier and changes the weight of the training data according to each training results until reach the threshold. Then the strong classifier is built after many iterations of the loop. We also conduct comparison experiments using four conventional machine learning approaches and two ensemble learning algorithms. The obtained results indicate the proposed method has better performance in the imbalanced dataset. Secondly, for abnormal consumers, we use General Regression Neural Network (GRNN) to estimate their electrical consumption and compare with the actual consumption to find the stealing electricity intervals.		Rongli Wu;Liming Wang;Tianyu Hu	2018	IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2018.8591459		ML	8.581752429577202	-16.837171886421725	4442
13d0cf1965b534453f163bbe2d50abe2865ac582	a hybrid feature selection approach based on the bayesian network classifier and rough sets	bayesian network;bayesian network classifier;naive bayesian classifier;texture features;feature reduction;clustering method;naive bayesian network classifier;feature subset selection;rough sets;feature selection;rough set	The paper proposes a hybrid feature selection approach based on Rough sets and Bayesian network classifiers. In the approach, the classification result of a Bayesian network is used as the criterion for the optimal feature subset selection. The Bayesian network classifier used in the paper is a kind of naive Bayesian classifier. It is employed to implement classification by learning the samples consisting of a set of texture features. In order to simplify feature reduction using Rough Sets, a discrete method based on C-means clustering method is also presented. The proposed approach is applied to extract residential areas from panchromatic SPOT5 images. Experiment results show that the proposed method not only improves classification quality but also reduces computational cost.	algorithmic efficiency;bayesian network;cluster analysis;computation;computer vision;feature selection;loss function;rough set;statistical classification	Li Pan;Hong Zheng;Li Li	2008		10.1007/978-3-540-79721-0_94	rough set;variable-order bayesian network;computer science;machine learning;pattern recognition;data mining;feature selection	AI	11.15660511117145	-42.10921593459257	4463
1aec8613b69408813665693414221363044a8bb6	approximate infinite-dimensional region covariance descriptors for image classification	least squares approximations;kernel;manifolds;riemannian geometry region covariance descriptor reproducing kernel hilbert space;random processes approximation theory covariance matrices feature extraction image classification;geometry;computer vision;spd manifolds approximate infinite dimensional region covariance descriptor image classification feature mappings random fourier feature nystrom method infinite dimensional rcovd riemannian structure symmetric positive definite spd matrices explicit mappings finite dimensional approximation;kernel manifolds least squares approximations geometry computer vision conferences;conferences	We introduce methods to estimate infinite-dimensional Region Covariance Descriptors (RCovDs) by exploiting two feature mappings, namely random Fourier features and the Nyström method. In general, infinite-dimensional RCovDs offer better discriminatory power over their low-dimensional counterparts. However, the underlying Riemannian structure, i.e., the manifold of Symmetric Positive Definite (SPD) matrices, is out of reach to great extent for infinite-dimensional RCovDs. To overcome this difficulty, we propose to approximate the infinite-dimensional RCovDs by making use of the aforementioned explicit mappings. We will empirically show that the proposed finite-dimensional approximations of infinite-dimensional RCovDs consistently outperform the low-dimensional RCovDs for image classification task, while enjoying the Riemannian structure of the SPD manifolds. Moreover, our methods achieve the state-of-the-art performance on three different image classification tasks.	approximation algorithm;computer vision;nyström method	Masoud Faraki;Mehrtash Tafazzoli Harandi;Fatih Murat Porikli	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178193	mathematical optimization;combinatorics;discrete mathematics;kernel;manifold;mathematics	Vision	25.11997554934536	-39.504820198022166	4467
39d039ff5b611a38eae3463e2d5585e84ff167ca	ghic: a hierarchical pattern-based clustering algorithm for grouping web transactions	extraction information;cluster algorithm;pattern clustering;analyse amas;tratamiento transaccion;fouille web;data centric;customer services;red www;centre donnee;analisis datos;information extraction;classification non supervisee;clustering algorithms data mining itemsets association rules web mining parametric statistics;pattern classification transaction processing consumer behaviour data mining pattern clustering internet customer services;reseau web;customer loyalty;centrado dato;association rules;indexing terms;data mining;classification;objective function;data analysis;hierarchical classification;cluster analysis;regle association;internet;regla asociacion;association rule;mixture model;fouille donnee;clustering;customer behaviour customer transactions grouping customer loyalty cluster analysis mixture models data mining ghic user centric web usage data hierarchical pattern based clustering algorithm web transactions grouping association rules web mining;clasificacion no supervisada;pattern classification;classification hierarchique;consumer behaviour;unsupervised classification;web mining;world wide web;analyse donnee;analisis cluster;index terms data mining;modele amas;cluster model;transaction processing;web mining index terms data mining clustering classification association rules;clasificacion jerarquizada;busca dato;extraccion informacion;traitement transaction	Grouping customer transactions into segments may help understand customers better. The marketing literature has concentrated on identifying important segmentation variables (e.g., customer loyalty) and on using cluster analysis and mixture models for segmentation. The data mining literature has provided various clustering algorithms for segmentation without focusing specifically on clustering customer transactions. Building on the notion that observable customer transactions are generated by latent behavioral traits, in this paper, we investigate using a pattern-based clustering approach to grouping customer transactions. We define an objective function that we maximize in order to achieve a good clustering of customer transactions and present an algorithm, GHIC, that groups customer transactions such that itemsets generated from each cluster, while similar to each other, are different from ones generated from others. We present experimental results from user-centric Web usage data that demonstrates that GHIC generates a highly effective clustering of transactions.	algorithm;cluster analysis;data mining;experiment;heuristic (computer science);loss function;mixture model;observable;optimization problem;periodic boundary conditions;usage data	Yinghui Yang;Balaji Padmanabhan	2005	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2005.145	web mining;data stream clustering;association rule learning;computer science;machine learning;data mining;database;cluster analysis;world wide web;information extraction	DB	-4.445247847147599	-33.62087742254228	4488
427057ed2e0ce0ffb4f521e13c2e04cea4270d21	a zaslavskii firefly approach applied to loney's solenoid benchmark	solenoids optimization benchmark testing educational institutions algorithm design and analysis particle swarm optimization electromagnetics;swarm intelligence optimisation solenoids;loney s solenoid swarm intelligence firefly algorithm electromagnetic optimization chaotic sequences;zaslavskii map zaslavskii firefly approach loney solenoid benchmark nature inspired algorithms swarm intelligence field global optimization problems metaheuristic algorithms stochastic paradigm flashing characteristics control parameters randomization parameter setting chaotic sequences	Nature-inspired algorithms of the swarm intelligence field perform powerfully and efficiently in solving global optimization problems. Inspired by nature, these metaheuristic algorithms have obtained promising performance over continuous domains of optimization problems. Recently, a new swarm intelligence approach called firefly algorithm (FA) has emerged. The FA is a stochastic paradigm based on the idealized behavior of the flashing characteristics of fireflies. However, to achieve good performance with FA, the tuning of control parameters is essential as its performance is sensitive to the choice of the randomization parameter (α) setting. This paper introduces a FA approach combined with chaotic sequences generated by Zaslavskii map (FACZ) to tune the randomization parameter. Simulations of Loney's solenoid benchmark problem examine the effectiveness of the conventional FA and the proposed FACZ algorithms. Simulation results and comparisons with the FACZ demonstrated that the performance of the FA is promising in the Loney's solenoid case.	benchmark (computing);firefly (cache coherence protocol);firefly algorithm;firmware;global optimization;mathematical optimization;metaheuristic;programming paradigm;simulation;swarm intelligence;zaslavskii map	Leandro dos Santos Coelho;Emerson Hochsteiner de Vasconcelos Segundo;Viviana Cocco Mariani;Marcia de Fatima Morais;Roberto Zanetti Freire	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974584	mathematical optimization;multi-swarm optimization;simulation;artificial intelligence;firefly algorithm	Robotics	27.240337590427444	-5.701192162809706	4502
bed73acb2d14c574fc1f08116cbee30ea8e403bf	reliability models for a nonrepairable system with heterogeneous components having a phase-type time-to-failure distribution	standby redundancy;imperfect switching;phase type distribution;redundancy allocation problem;heterogeneous components	This research paper presents practical stochastic models for designing and analyzing the time-dependent reliability of nonrepairable systems. The models are formulated for nonrepairable systems with heterogeneous components having phase-type time-to-failure distributions by a structured continuous time Markov chain (CTMC). The versatility of the phase-type distributions enhances the flexibility and practicality of the systems. By virtue of these benefits, studies in reliability engineering can be more advanced than the previous studies. This study attempts to solve a redundancy allocation problem (RAP) by using these new models. The implications of mixing components, redundancy levels, and redundancy strategies are simultaneously considered to maximize the reliability of a system. An imperfect switching case in a standby redundant system is also considered. Furthermore, the experimental results for a well-known RAP benchmark problem are presented to demonstrate the approximating error of the previous reliability function for a standby redundant system and the usefulness of the current research.		Heungseob Kim;Pansoo Kim	2017	Rel. Eng. & Sys. Safety	10.1016/j.ress.2016.10.019	dual modular redundancy;reliability engineering;real-time computing;engineering;mathematics;phase-type distribution;statistics	DB	8.25768063402371	-1.0692506151128998	4517
4dfd96f456f18bf678158d158f33929c63b438b8	scarce face recognition via two-layer collaborative representation		The recent significant progress in face recognition is mainly achieved using learning-based (LE) techniques via an exhaustive training involving a huge number of face samples. However, in many applications, the number of face images available for training may be very limited. This makes LE techniques impractical for learning discriminative features and models. Thus, limited number of face samples (i.e. scarce data) degrades the recognition performance of most existing methods. To overcome this problem, the authors propose a novel approach based on two-layer collaborative representation to exploit the abundance of samples in some classes to enrich the scarce data in other classes. The first-layer collaborative representation uses the abundance of samples to construct representations for the scarce data. Then, a new face sample is recognised by computing residuals with the second-layer collaborative representation. Extensive experiments on four benchmark face databases demonstrate the effectiveness of their proposed approach which compares favourably against state-of-the-art methods.		Zhaoqiang Xia;Xianlin Peng;Xiaoyi Feng;Abdenour Hadid	2018	IET Biometrics	10.1049/iet-bmt.2017.0193	computer vision;discriminative model;pattern recognition;facial recognition system;computer science;artificial intelligence;exploit	Vision	26.10071542224239	-47.749588810662594	4521
782ac903469d32a1888fb9ee79653c7ca9d55ba3	including modal improvisation and music-inspired components to improve harmony search		In this paper we present new components to be included in harmony search algorithms. These components are inspired from music improvisation. The Modal improvisation uses musical modes rather than chord progressions as a har- monic framework. We also include the notion of tone scales that allows the algo- rithm to visit different parts of the search space. We evaluate our approach solving instances of the Multidimensional Knapsack Problem instances. We compare our results with those obtained by the former harmony search algorithm, and with the well-known state-of-the-art results.		Nicolás Rojas;María Cristina Riff	2013		10.1007/978-3-642-45111-9_9	artificial intelligence;mathematics;algorithm	Robotics	24.644398474195885	-2.6085524626188783	4536
d3e770279099c70fb17d83a475357efa4cc3e445	on the benefits of industrial network: a new approach with market survey and fuzzy statistical analysis	management cost;market research;fuzzy expect value;industrial economics;fuzzy statistical analysis;industries;testing;fuzzy set theory;industrial network;technology management;fuzzy questionnaire;statistical testing market research fuzzy set theory management science industrial economics industries;expected value;statistical analysis;market survey;environmental economics;statistics;humans;statistical testing;econometrics;statistical analysis costs testing power generation economics environmental economics econometrics humans industrial economics statistics technology management;market research method;power generation economics;management science;fuzzy questionnaire industrial network market survey fuzzy statistical analysis management cost market research method fuzzy expect value	Making the most of the industrial network to reduce management costs and increase competitiveness has gained a lot of attention among enterprises lately. In traditional market research methods, the approach always puts its emphasis on the data of a single value without considering the complexity of human thoughts. Thus, it is the purpose of this paper to first define fuzzy mode, fuzzy expect value and fuzzy /spl chi//sup 2/test. Then, a new market survey is proposed to develop a more efficient survey analysis to examine the characteristics of fuzzy questionnaire and sample material. A comparison between fuzzy /spl chi//sup 2/ test and traditional /spl chi//sup 2/ test was carried on at the end.	develop;fuzzy concept;fuzzy logic;information and computer science;information science;level of measurement;numerical analysis	Shu-Meei Ho;Berlin Wu	2005	Fourth Annual ACIS International Conference on Computer and Information Science (ICIS'05)	10.1109/ICIS.2005.95	actuarial science;economics;operations management;management science	Robotics	-3.5466743380937076	-14.023467960944293	4539
0f64749e039ac0182761fb2449521814bff32801	local reference with early termination in h.264 motion estimation	motion analysis;quantization;multiple reference frames;history;data compression;decoding;motion estimation encoding algorithm design and analysis video compression costs decoding statistical analysis history motion analysis quantization;video compression h 264 motion estimation multiple reference frames variable block sizes compression efficiency encoder complexity local reference with early termination algorithm;h 264 motion estimation;video compression;reference frame;video quality;motion estimation;encoder complexity;variable block size;video coding;statistical analysis;computational complexity;local reference with early termination algorithm;video coding computational complexity data compression motion estimation;variable block sizes;compression efficiency;encoding;algorithm design and analysis	Multiple reference frames and variable block sizes improve compression efficiency of H.264, however, they also increase the encoder complexity and motion estimation time. This paper proposes a new algorithm, called local reference with early termination (LRET) to reduce the H.264 motion estimation time without adding to the encoder complexity. The LERT algorithm rearranges the search order of the reference frames based on the selection probability of the reference frames in the current frame. The experimental results show that the LERT achieves up to 59% reduction in motion estimation time with comparable video quality and negligible increase in bit-rate, as compared to the best algorithm in H.264 reference software.	algorithm;encoder;h.264/mpeg-4 avc;motion estimation;reference frame (video)	Xiao Su;Sweta Singh;Yan Bai	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284667	data compression;reference frame;computer vision;real-time computing;computer science;theoretical computer science;motion estimation;motion compensation;algorithm;statistics	Vision	47.085078559760774	-18.629602722092468	4543
50a0930cb8cc353e15a5cb4d2f41b365675b5ebf	robust facial landmark detection and face tracking in thermal infrared images using active appearance models		Long wave infrared (LWIR) imaging is an imaging modality currently gaining increasing attention. Facial images acquired with LWIR sensors can be used for illumination invariant person recognition and the contactless extraction of vital signs such as respiratory rate. In order to work properly, these applications require a precise detection of faces and regions of interest such as eyes or nose. Most current facial landmark detectors in the LWIR spectrum localize single salient facial regions by thresholding. These approaches are not robust against out-of-plane rotation and occlusion. To address this problem, we therefore introduce a LWIR face tracking method based on an active appearance model (AAM). The model is trained with a manually annotated database of thermal face images. Additionally, we evaluate the effect of different methods for AAM generation and image preprocessing on the fitting performance. The method is evaluated on a set of still images and a video sequence. Results show that AAMs are a robust method for the detection and tracking of facial landmarks in the LWIR spectrum.	active appearance model;algorithm;contactless smart card;facial motion capture;hidden surface determination;modality (human–computer interaction);preprocessor;region of interest;sensor;simplified instructional computer;thresholding (image processing)	Marcin Kopaczka;Kemal Acar;Dorit Merhof	2016		10.5220/0005716801500158	computer vision	Vision	46.38403744045275	-43.85983277601708	4549
4cdadb83306b6fc2666aa2897231018306cade2d	random projection rbf nets for multidimensional density estimation	wielowymiarowa gestośc prawdopodobienstwa;dimension reduction;novelty detection;rzutowanie losowe;density estimation;detekcja nowości;radial basis function;normal random projection;radialne funkcje bazowe;estymacja;redukcja wymiaru;random projection;multivariate density estimation;radial basis functions	Principal component analysis (PCA) is a powerful fault detection and isolation method. However, the classical PCA, which is based on the estimation of the sample mean and covariance matrix of the data, is very sensitive to outliers in the training data ...	radial basis function;random projection	Ewa Skubalska-Rafajlowicz	2008	Applied Mathematics and Computer Science	10.2478/v10006-008-0040-9	mathematical optimization;radial basis function;computer science;machine learning;pattern recognition;mathematics;statistics	Theory	30.418657512480276	-28.144402075838155	4553
04c12fbdd04cf67a965e628d40b9098c8a7114b3	bayesian classifier combination		Bayesian model averaging linearly mixes the probabilistic predictions of multiple models, each weighted by its posterior probability. This is the coherent Bayesian way of combining multiple models only under certain restrictive assumptions, which we outline. We explore a general framework for Bayesian model combination (which differs from model averaging) in the context of classification. This framework explicitly models the relationship between each model’s output and the unknown true label. The framework does not require that the models be probabilistic (they can even be human assessors), that they share prior information or receive the same training data, or that they be independent in their errors. Finally, the Bayesian combiner does not need to believe any of the models is in fact correct. We test several variants of this classifier combination procedure starting from a classic statistical model proposed by Dawid and Skene (1979) and using MCMC to add more complex but important features to the model. Comparisons on several data sets to simpler methods like majority voting show that the Bayesian methods not only perform well but result in interpretable diagnostics on the data points and the models.	bayesian network;coherence (physics);data point;ensemble learning;markov chain monte carlo;naive bayes classifier;power dividers and directional couplers;statistical model	Hyun-Chul Kim;Zoubin Ghahramani	2012			bayes classifier;naive bayes classifier;quadratic classifier	ML	16.650358047085916	-38.24789463054907	4556
d7d8c6491666d5ea04ff341c74430fe149cec838	memristive nanowires exhibit small-world connectivity	memristors;nanowires;network science;neuromorphics;small-world	Small-world networks provide an excellent balance of efficiency and robustness that is not available with other network topologies. These characteristics are exhibited in the Memristive Nanowire Neural Network (MN3), a novel neuromorphic hardware architecture. This architecture is composed of an electrode array connected by stochastically deposited core-shell nanowires. We simulate the stochastic behavior of the nanowires by making various assumptions on their paths. First, we assume that the nanowires follow straight paths. Next, we assume that they follow arc paths with varying radii. Last, we assume that they follow paths generated by pink noise. For each of the three methods, we present a method to find whether a nanowire passes over an electrode, allowing us to represent the architecture as a bipartite graph. We find that the small-worldness coefficient increases logarithmically and is consistently greater than one, which is indicative of a small-world network.	anatomy, regional;artificial neural network;cmos;class;clustering coefficient;degree distribution;molecular wire;nanowires;nearest-neighbor interpolation;network topology;neural network simulation;neuromorphic engineering;neuron;neurons;pink noise;short;shortest path problem;synapse;synapses;centimeter;density;electrode;sq. cm;statistical cluster	Jose Antonio Luceño-Sánchez;Natalia Florea;Juan C. Nino	2018	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2018.07.002	mathematics;architecture;mathematical optimization;distributed computing;bipartite graph;network topology;neuromorphic engineering;hardware architecture;pink noise;electrode array;nanowire	ML	40.01713344579076	-0.5266978718577691	4571
dd1459e9dbc906c5c1927ed9ee74357f0772d016	an evolutionary mining model in incremental data mining	data mining association rules space technology iterative algorithms evolution biology computer science educational institutions information science data engineering knowledge engineering;evolutionary model;evolutionary computation;temporal dynamics;generic algorithm;cross operator;data mining;mutation operator;evolutionary computation data mining;heuristic algorithms;multiply evolutionary model;classification algorithms;evolutionary strategy;classification tree analysis;evolutionary model data mining evolutionary strategy;incremental data mining;algorithm design and analysis;data models;dynamic evolutionary mining;dynamic evolutionary mining incremental data mining multiply evolutionary model cross operator mutation operator	Incremental data mining is very important to solve the temporal dynamic property of knowledge, improve the performance of mining processes and efficiency of mining results. Incremental data occurs with the passage of time. Evolutionary methods can be adopted to solve such increment. A multiply evolutionary model is built to describe incremental data evolutionary mining processes. Copy operator, cross operator and mutation operator are designed. A general algorithm for dynamic evolutionary mining is also presented. The experiments showed that the evolutionary incremental data mining method could solve the scalable problem of data mining better, and have high accuracy and good time performance.	data mining	Jiancong Fan;Yongquan Liang;Jiuhong Ruan	2009		10.1109/ICNC.2009.488	evolutionary programming;computer science;data science;machine learning;data mining;data stream mining	ML	-4.058093575778487	-36.88545951719208	4579
f3eb49fd94f92e83ec64b30cc67c0090e30d30fb	the izi project: easy prototyping of interesting pattern mining algorithms	data integrity;relational database;classical solution;data mining;software engineering;pattern mining;artificial intelligent;frequent itemset mining;point of view;query rewriting;open source	In the last decade, many data mining tools have been developed. They address most of the classical data mining problems such as classification, clustering or pattern mining. However, providing classical solutions for classical problems is not always sufficient. This is especially true for pattern mining problems known to be “representable as set”, an important class of problems which have many applications such as in data mining, in databases, in artificial intelligence, or in software engineering. A common idea is to say that solutions devised so far for classical pattern mining problems, such as frequent itemset mining, should be useful to answer these tasks. Unfortunately, it seems rather optimistic to envision the application of most of publicly available tools even for closely related problems. In this context, the main contribution of this paper is to propose a modular and efficient tool in which users can easily adapt and control several pattern mining algorithms. From a theoretical point of view, this work takes advantage of the common theoretical background of pattern mining problems isomorphic to boolean lattices. This tool, a C++ library called iZi, has been devised and applied to several problems such as itemset mining, constraint mining in relational databases, and query rewriting in data integration systems. According to our first results, the programs obtained using the library have very interesting performance characteristics regarding simplicity of their development. The library is open source and freely available on the Web.	algorithm;artificial intelligence;association rule learning;c++;cluster analysis;data mining;open-source software;relational database;rewriting;software engineering;statistical classification;text mining;world wide web	Frédéric Flouvat;Fabien De Marchi;Jean-Marc Petit	2009		10.1007/978-3-642-14640-4_1	concept mining;text mining;relational database;computer science;data science;machine learning;data integrity;data mining;database;data stream mining;molecule mining	ML	-2.3721538553190706	-44.11205004950525	4582
e10819bf5307302b2a4df462b40ddb98acf19730	bc-bsp: a bsp-based parallel iterative processing system for big data on cloud architecture	disk cache;graph processing;会议论文;bsp;big data;map reduce;mapreduce;big datum	Many applications in real life can produce and collect large amount of data and many of them can be modeled by Graph. The number of vertexes of a graph could be several hundreds of millions to billions and the number of edges could be ten or more times of the number of its vertexes. A BSP-based system for large-scale data especially graph data parallel and iterative processing is discussed in this paper. The system has the ability to flexible configuration and the extendibility for functions and strategies such as adjusting the parameters according to the volume of data and supporting multiple aggregation functions at the same time, to process large-scale data, to tolerate faults, to balance load, and to run clustering or classification algorithms on metric datasets. Lots of experiments are done to evaluate the extendibility of the system implemented in the paper, and the comparison between BC-BSP-based applications and MapReduce-based ones are made. The experimental results show that BSP-based applications have higher efficiency than that of MapReduce-based applications when the volume of data can be put in the memory during the course of processing; on the contrary the latter are better than the former, and the performance of BC-BSP platform outperforms Hama and Giraph.	big data	Yubin Bao;Zhigang Wang;Yu Gu;Ge Yu;Fangling Leng;Hongxu Zhang;Bairen Chen;Chao Deng;Leitao Guo	2013		10.1007/978-3-642-40270-8_3	binary space partitioning;parallel computing;big data;computer science;theoretical computer science;data mining;database;programming language	ML	-3.420980089432196	-39.52185037462938	4585
97f6eccd7db3aae50ab7e1e2b93917b287dcc2cb	variant versus invariant time to total forgetting: the learn-forget curve model revisited	tecnologia industrial tecnologia mecanica;theoretical model;computacion informatica;learning;learning curve;forgetting;ciencias basicas y experimentales;variant time to total forgetting;production breaks;training program;tecnologias;grupo a;product development	Understanding and quantifying the learning–forgetting process helps predict the performance of an individual (or a group of individuals), estimate labor costs, bid on new and repeated orders, estimate costs of strikes, schedule production, develop training programs, set time standards, and improve work methods [IIE Trans. 29 (1997) 759]. Although there is agreement that the form of the learning curve is as presented by [J. Aeronaut. Sci. 3 (1936) 122], scientists and practitioners have not yet developed a full understanding of the behavior and factors affecting the forgetting process. The paucity of research on forgetting curves has been attributed to the practical difficulties involved in obtaining data concerning the level of forgetting as a function of time [IIE Transactions 21 (1989) 376]. The learn–forget curve model (LFCM) was shown to have many advantages over other theoretical models that capture the learning–forgetting relationship. However, the deficiency of the LFCM is in the assumption that the time for total forgetting is invariant of the experience gained prior to interruption. This paper attempts to correct this deficiency by incorporating the findings of [Int. J. Ind. Ergon. 10 (1992) 217] into the LFCM. Numerical examples are used to illustrate the behavior of the modified LFCM (MLFCM) and compare results to those of the LFCM.		Mohamad Y. Jaber;Hemant V. Kher	2004	Computers & Industrial Engineering	10.1016/j.cie.2004.05.006	simulation;computer science;engineering;artificial intelligence;operations management;machine learning;learning curve;management;forgetting;new product development;statistics	Robotics	1.5468014227650915	-14.223762236329781	4606
06f948556ffe7251660b8b1952fab2e86f9a6c84	fuzzy interpolative reasoning for sparse fuzzy rule-based systems based on the ranking values of fuzzy sets	fuzzy rule based system;fuzzy set;fuzzy rules;fuzzy interpolative reasoning;ranking proportional interpolative coefficients;sparse fuzzy rule based systems;ranking values	Fuzzy interpolative reasoning is an important research topic of sparse fuzzy rule-based systems. In recent years, some methods have been presented for dealing with fuzzy interpolative reasoning. However, the involving fuzzy sets appearing in the antecedents of fuzzy rules of the existing fuzzy interpolative reasoning methods must be normal and non-overlapping. Moreover, the reasoning conclusions of the existing fuzzy interpolative reasoning methods sometimes become abnormal fuzzy sets. In this paper, in order to overcome the drawbacks of the existing fuzzy interpolative reasoning methods, we present a new fuzzy interpolative reasoning method for sparse fuzzy rule-based systems based on the ranking values of fuzzy sets. The proposed fuzzy interpolative reasoning method can handle the situation of non-normal and overlapping fuzzy sets appearing in the antecedents of fuzzy rules. It can overcome the drawbacks of the existing fuzzy interpolative reasoning methods in sparse fuzzy rule-based systems.	fuzzy rule;fuzzy set;rule-based system;sparse matrix	Li-Wei Lee;Shyi-Ming Chen	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.07.027	fuzzy logic;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-0.6508449074666419	-26.141900858698474	4616
14f04607898e3f45424dc61d46c9571e104eab2a	discovering options from example trajectories	learning process;reinforcement learning	We present a novel technique for automated problem decomposition to address the problem of scalability in reinforcement learning. Our technique makes use of a set of near-optimal trajectories to discover options and incorporates them into the learning process, dramatically reducing the time it takes to solve the underlying problem. We run a series of experiments in two different domains and show that our method offers up to 30 fold speedup over the baseline.	baseline (configuration management);experiment;reinforcement learning;scalability;software quality;speedup	Peng Zang;Peng Zhou;David Minnen;Charles Lee Isbell	2009		10.1145/1553374.1553529	simulation;computer science;artificial intelligence;machine learning;reinforcement learning	ML	21.17687904377327	-20.02010403110176	4619
769678772ceaff5eceb34607a27d89d136e14251	incremental learning and concept drift in inthelex	concept drift;incremental learning	"""Real-world tasks often involve a continuous flow of new information that affects the learned theory, a situation that classical batch (one-step) learning systems are hardly suitable to handle. On the contrary, incremental (also called """"on-line"""") techniques are able to deal with such a situation by exploiting refinement operators. In many cases deep knowledge about the world is not available: Either incomplete information is available at the time of initial theory generation, or the nature of the concepts evolves dynamically. The latter situation is the most difficult to handle since time evolution needs to be considered. This work presents a new approach to learning in presence of concept drift, and in particular a special version of the incremental system INTHELEX purposely designed to implement such a technique. Its behavior in this context has been checked and analyzed by running it on two different datasets."""	concept drift	Floriana Esposito;Stefano Ferilli;Nicola Fanizzi;Teresa Maria Altomare Basile;Nicola Di Mauro	2004	Intell. Data Anal.		simulation;computer science;artificial intelligence;concept drift;machine learning;data mining	AI	-0.02862599412019616	-35.5131775071174	4621
38ea949acb14b7d6bbb04a289ef369e60d235e83	endogenous buyer-seller choice and divisible money in search equilibrium	policy analysis;political economy;money supply;value function	In the Lagos-Wright model [R. Lagos, R. Wright, A unified framework for monetary theory and policy analysis, J. Polit. Economy 113 (2005) 463–484], the quasi-linear preferences assumption is not necessary to generate simple distributions of money holdings if individuals choose endogenously to go to the search market as buyers or as sellers. The non-convex buyer–seller choice provides an incentive for gambling in lotteries, and, as a result, the value function has a linear interval. As long as this interval is the relevant one for evaluating their future utilities, individuals behave as if their preferences were quasi-linear. In the stationary equilibrium, individuals remain inside this linear interval if the money supply does not decline. © 2007 Elsevier Inc. All rights reserved. JEL classification: E40		Miquel Faig	2008	J. Economic Theory	10.1016/j.jet.2007.09.007	economics;policy analysis;endogenous money;finance;macroeconomics;microeconomics;bellman equation;mathematical economics;market economy	ECom	-4.038620977318571	-2.803256701170328	4623
23f2f9b5b0659d099caaccbb9da43d63ee11b6ab	optimizing the path of seedling low-density transplanting by using greedy genetic algorithm		Abstract Automated transplanters perform repetitive low-density transplanting and replugging of seedlings in greenhouses to resolve the labor shortage problem and consistently produce seedlings. The work efficiency of transplanters can be improved by optimizing the transplanting paths of end effectors. In this study, a greedy genetic algorithm (GGA) was developed for path optimization. GGA combines the characteristics of a greedy algorithm (GRA) and a genetic algorithm (GA). The performances of GGA, GRA, GA, and the common sequence method (CSM) in the path planning for seedling low-density transplanting were compared in terms of their optimization effects and computation time. Average transplanting paths were analyzed for sparse (32 and 50 holes) and dense (72 and 128 holes) seedling trays with 5%–20% randomly located vacant holes. Compared with the average optimization ratio of CSM, those of GA, GGA, and GRA were 10%, 8.7%, and 5.1%, respectively, for sparse trays, whereas 13.9%, 13.4%, and 11.8%, respectively, for dense trays. The standard deviations of GGA and GA overlapped in different vacant holes for the dense trays. The performance ranking of the suitable methods with short average paths was in the order of GA, GGA, and GRA. The superiority of GA over GGA gradually decreased with the increasing number of vacant and tray holes. The computation of path planning must satisfy the real-time operating requirement of transplanters. GA, GGA, and GRA consumed 9.61, 2.82, and 0.02 s, respectively, for the path planning for the dense trays. Compared with GA and GRA, GGA performed effectively in the path planning of seedling low-density transplanting due to its comprehensive performance derived from its path optimization ratio and low computation time cost. This combined optimization algorithm could have similar agricultural applications.	genetic algorithm;greedy algorithm;optimizing compiler	Junhua Tong;Chuanyu Wu;Huanyu Jiang;Yaxin Yu;Xiuqin Rao	2017	Computers and Electronics in Agriculture	10.1016/j.compag.2017.09.017	engineering;genetic algorithm;labor shortage;transplanting;mathematical optimization;seedling;greedy algorithm	Robotics	30.431267539553357	-2.6490134544358925	4626
585553d85f7a59630b2448d0c2ac48e086101c1f	extensions of logical analysis of data for growth hormone deficiency diagnoses	final height;classification;regression;logical analysis of data;general practitioner;growth hormone deficiency	We propose two extensions of the Logical Analysis of Data (LAD) methodology, designed in the context of diagnosing growth hormone deficiencies. On the one hand, combinatorial regression extends the standard methodology from classification problems to regression problems; it permits to predict the final height of children with particular growth troubles. On the other hand, function-based patterns extend the standard notion of pattern, leading to both accurate and simple models; it allows to produce an efficient diagnosis, straightforwardly usable by a general practitioner, that settles most of the doubtful cases of growth hormone deficiencies among short children. In both cases, we show the interest of the LAD extensions for each application, and we also point out the more general use that can be achieved through the two proposed approaches.	angular defect;least absolute deviations	Pierre Lemaire	2011	Annals OR	10.1007/s10479-011-0901-8	regression;biological classification;computer science;artificial intelligence;operations management;mathematics;algorithm;statistics	ML	10.957935097806109	-37.272719924136794	4633
d9d8425d337f0d57fd73ee31efaebc25ecadcf29	distributed-based hierarchical clustering system for large-scale semiconductor wafers		In this paper, we propose a Distributed-based Hierarchical Clustering System for Large-Scale Semiconductor Wafers (DHCSSW). By applying the bigdata framework to existing hierarchical clustering algorithm, the proposed system makes it feasible to cluster large-scale wafers with up to 320,000 wafers. To verify the performance of our approach, we used simulated wafer maps. The experimental results show that our system outperformed the existing hierarchical clustering in processing large-scale wafers, suggesting that currently used hierarchical clustering is insufficient in analyzing large-scale wafer maps. In addition, some failure patterns, which the existing approach is not able to detect, can be found with the DHCSSW. We anticipate that the DHCSSW will contribute in identifying the failure patterns in large-scale semiconductor wafers.		Seungchul Lee;Daeyoung Kim	2018	2018 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2018.8607492		SE	13.34463813174764	-15.911846261491842	4636
11a7ae32822c160e864a7fba1a4a1e1fe91c28f5	multidimensional epistasis and the advantage of sex	evolutionary computation;evolutionary computation genetics;multidimensional systems genetic mutations evolutionary computation space exploration computer science systems biology computational biology evolution biology biological system modeling surface treatment;genetics;linear functionals;epistatic model space multidimensional epistasis population genetic model epistatic system genotype fitness mutation unidimensional epistasis epistatic landscape evolutionary computation kondrashov model modification;evolutionary computing;population genetics	"""Kondrashov and Kondrashov (2001) point out that, although common in population genetic models, epistatic systems where the fitness of a genotype is a nonlinear function of the number of mutations it carries, which they term """"unidimensional epistasis"""", are a limited subset of possible epistatic landscapes. They claim that more general """"multidimensional epistasis"""" usually confers a disadvantage for sex. However, the evolutionary computation (EC) literature contains models that lie within the space of multidimensional epistasis yet demonstrate an advantage to sex. Here we provide modifications of the Kondrashov's model that connect with EC results and help to explore the space of epistatic models between the two disciplines."""	crossover (genetic algorithm);evolutionary computation;fitness function;ingo wegener;locus;nonlinear system;population;simulation;spontaneous order;time complexity;xfig	Richard A. Watson;John Wakeley	2005	2005 IEEE Congress on Evolutionary Computation	10.1109/CEC.2005.1555045	computer science;bioinformatics;population genetics;evolutionary computation	Vision	26.895334158110074	-9.422135480592594	4653
15eca81e74f652e454d00a13fc5d1503719301bc	optimization of the lvq network architectures with a modular approach for arrhythmia classification		In this paper, the optimization of LVQ neural networks with modular approach is presented for classification of arrhythmias, using particle swarm optimization. This work focuses only in the optimization of the number of modules and the number of cluster centers. Other parameters, such as the learning rate or number of epochs are static values and are not optimized. Here, the MIT-BIH arrhythmia database with 15 classes was used. Results show that using 5 modules architecture could be a good approach for classification of arrhythmias.	learning vector quantization	Jonathan Amezcua;Patricia Melin	2015		10.1007/978-3-319-26211-6_23	artificial neural network;architecture;learning vector quantization;machine learning;network architecture;modular design;computer science;artificial intelligence;particle swarm optimization	ML	13.763567584195343	-25.41681514118249	4665
d8267119e6e69c528bc7bee010acfb41c9efd861	using interpolation to improve path planning: the field d* algorithm	mobile robot;path planning;cost estimation;state transition	We present an interpolation-based planning and replanning algorithm for generating low-cost paths through uniform and nonuniform resolution grids. Most grid-based path planners use discrete state transitions that artificially constrain an agent’s motion to a small set of possible headings e.g., 0, /4, /2, etc. . As a result, even “optimal” gridbased planners produce unnatural, suboptimal paths. Our approach uses linear interpolation during planning to calculate accurate path cost estimates for arbitrary positions within each grid cell and produce paths with a range of continuous headings. Consequently, it is particularly well suited to planning low-cost trajectories for mobile robots. In this paper, we introduce a version of the algorithm for uniform resolution grids and a version for nonuniform resolution grids. Together, these approaches address two of the most significant shortcomings of grid-based path planning: the quality of the paths produced and the memory and computational requirements of planning over grids. We demonstrate our approaches on a number of example planning problems, compare them to related algorithms, and present several implementations on real robotic systems. © 2006 Wiley Periodicals, Inc.	algorithm;computation;john d. wiley;linear interpolation;mobile robot;motion planning;requirement	Dave Ferguson;Anthony Stentz	2006	J. Field Robotics	10.1002/rob.20109	mobile robot;mathematical optimization;simulation;any-angle path planning;computer science;artificial intelligence;theoretical computer science;motion planning;cost estimate	Robotics	53.614898060718716	-23.74963149788928	4667
fa6e6fea71c381e8f2c25d738a1c0bc9e1571762	an empirical study of hyperparameter importance across datasets		With the advent of automated machine learning, automated hyperparameter optimization methods are by now routinely used. However, this progress is not yet matched by equal progress on automatic analyses that yield information beyond performance-optimizing hyperparameter settings. Various post-hoc analysis techniques exist to analyze hyperparameter importance, but to the best of our knowledge, so far these have only been applied at a very small scale. To fill this gap, we conduct a large scale experiment to discover general trends across 100 datasets. The results in case studies with random forests and Adaboost show that the same hyperparameters typically remain most important across datasets. Overall, these results, obtained fully automatically, provide a quantitative basis to focus efforts in both manual algorithm design and in automated hyperparameter optimization.	adaboost;algorithm design;hoc (programming language);machine learning;mathematical optimization;random forest	Jan N. van Rijn;Frank Hutter	2017			empirical research;econometrics;hyperparameter;computer science	ML	15.089769720388826	-43.631339079439094	4676
5afb756c7d4c88d6f72da89cf5d345a4ce3cd8cc	emotion recognition and eye gaze estimation system: erege		In this paper, we proposed EREGE system, EREGE system considers as a face analysis package including face detection, eye detection, eye tracking, emotion recognition, and gaze estimation. EREGE system consists of two parts; facial emotion recognition that recognizes seven emotions such as neutral, happiness, sadness, anger, disgust, fear, and surprise. In the emotion recognition part, we have implemented an Active Shape Model (ASM) tracker which tracks 116 facial landmarks via webcam input. The tracked landmark points are used to extract face expression features. A support Vector machine (SVM) based classifier is implemented which gives rise to robust our system by recognizing seven emotions. The second part of EREGE system is the eye gaze estimation that starts by creating the head model followed by presenting both Active Shape Model (ASM) and Pose from Orthography and Scaling with Iterations (POSIT) algorithms for head tracking and position estimation.	emotion recognition	Suzan Anwar;Mariofanna G. Milanova;Shereen Abdulla;Zvetomira Svetleff	2018		10.1007/978-3-319-92279-9_49	face detection;human–computer interaction;gaze;active shape model;ransac;support vector machine;computer vision;sadness;eye tracking;facial expression;computer science;artificial intelligence	HCI	36.765350787104154	-48.52410693720794	4716
96e1c3b48a2a5600bcc9edd7818eea48a0f6a417	inverse covariance estimation from data with missing values using the concave-convex procedure	covariance matrices optimization convergence training tuning linear programming approximation algorithms;biology dataset inverse covariance estimation concave convex procedure sparse precision matrix estimation maximum likelihood estimation schur complements em procedure m cccp algorithm synthetic dataset;maximum likelihood estimation covariance matrices data handling	We study the problem of estimating sparse precision matrices from data with missing values. We show that the corresponding maximum likelihood problem is a Difference of Convex (DC) program by proving some new concavity results on the Schur complements. We propose a new algorithm to solve this problem based on the ConCave-Convex Procedure (CCCP), and we show that the standard EM procedure is a weaker CCCP for this problem. Numerical experiments show that our new algorithm, called m-CCCP, converges much faster than EM on both synthetic and biology datasets.	algorithm;approximation;concave function;convex optimization;experiment;logarithmically concave function;missing data;numerical method;sparse matrix;synthetic intelligence	Jerome Thai;Timothy Hunter;Anayo K. Akametalu;Claire J. Tomlin;Alexandre M. Bayen	2014	53rd IEEE Conference on Decision and Control	10.1109/CDC.2014.7040287	estimation of covariance matrices;econometrics;mathematical optimization;mathematics;statistics	ML	31.983521341046	-31.218272508339336	4724
00e763f6e4a8de80aa453c1740780034967578ea	market and locational equilibrium for two competitors	location problem;equilibrio nash;probleme localisation;markets;nash equilibrium;benefit;mercado;maximization;reseau;noncooperative two stage game;concurrence economique;red;profit;minimizacion costo;concurrencia economica;games group decisions;equilibre nash;estrategia empresa;minimisation cout;cost minimization;marche;competition economy;facilities equipment planning competitive location;problema localizacion;ganancia;firm strategy;strategie entreprise;maximizacion;network;maximisation;networks graphs location	We consider a two-stage location and allocation game involving two competing firms. The firms first select the location of their facility on a network. Then the firms optimally select the quantities each wishes to supply to the markets, which are located at the vertices of the network. The criterion for optimality for each firm is maximizing its profit, which is the total revenue minus the production and transportation costs. Under reasonable assumptions regarding the revenue, the production cost and the transportation cost functions, we show that there is a Nash equilibrium for the quantities offered at the markets by each firm. Furthermore, if the quantities supplied at the equilibrium by each firm at each market are positive, then there is also a Nash locational equilibrium, i.e., no firm finds it advantageous to change its location.		Martine Labbé;S. Louis Hakimi	1991	Operations Research	10.1287/opre.39.5.749	profit;economics;economy;market economy;welfare economics;nash equilibrium	Robotics	0.6302649287869337	-4.7831282881037405	4726
0154329d58963de8108bc538ad49047aa64044d0	pose-invariant face recognition using real and virtual views	object recognition;sface recognition;ai;computer vision;electrical engineering and computer science;face recognition;thesis;facial feature detection;artificial intelligence;mit;virtualsviews	"""The problem of automatic face recognition is to visually identify a person in an input image. This task is performed by matching the input face against the faces of known people in a database of faces. Most existing work in face recognition has limited the scope of the problem, however, by dealing primarily with frontal views, neutral expressions, and fixed lighting conditions. To help generalize existing face recognition systems, we are looking at the problem of recognizing faces under a range of viewpoints. In particular, we consider two cases of this problem: (i) many example views are available of each person, and (ii) only one view is available per person, perhaps a driver's license or passport photograph. Ideally, we would like to address these two cases using a simple view-based approach, where a, person is represented in the database by using a number of views on the viewing sphere. While the view-based approach is consistent with case (i), for case (ii) we need to augment the single real view of each person with synthetic views from other viewpoints, views we call """"virtual views"""". Virtual views are generated using prior knowledge of face rotation, knowledge that is """"learned"""" from images of prototype faces. This prior knowledge is used to effectively rotate in depth the single real view available of each person. In this thesis, I present the view-based face recognizer, techniques for synthesizing virtual views, and experimental results using real and virtual views in the recognizer. Thesis Supervisor: Dr. Tomaso Poggio Title: Uncas and Helen Whitaker Professor, Dept. of Brain and Cognitive Sciences"""	facial recognition system;finite-state machine;prototype;regular expression;synthetic intelligence;tomaso poggio	David J. Beymer	1995			computer vision;face detection;computer science;artificial intelligence;face recognition grand challenge;communication	Vision	37.187168688175795	-42.9039810876165	4729
561dd7bbc62c5e4931dabfe4c09a33a9c0c241b3	exploiting symmetries in pomdps for point-based algorithms	minimal model;partially observed markov decision process;conference;graph automorphism;state space;markov decision process	We extend the model minimization technique for partially observable Markov decision processes (POMDPs) to handle symmetries in the joint space of states, actions, and observations. The POMDP symmetry we define in this paper cannot be handled by the model minimization techniques previously published in the literature. We formulate the problem of finding the symmetries as a graph automorphism (GA) problem, and although not yet known to be tractable, we experimentally show that the sparseness of the graph representing the POMDP allows us to quickly find symmetries. We show how the symmetries in POMDPs can be exploited for speeding up point-based algorithms. We experimentally demonstrate the effectiveness of our approach. Partially observable Markov decision processes (POMDPs) are a natural model for stochastic sequential decision problems under observation uncertainty. However, due to intractability results in solving POMDPs, common algorithms are hindered by poor scalability on the size of the problem. One approach to address this issue is to exploit the redundancy present in the model: by aggregating equivalent states of the model, we can derive a minimized model which can be solved by traditional algorithms with a reduced computational complexity. Such model minimization method has been explored in depth for Markov decision processes (MDPs) and POMDPs. Dean & Givan (1997) define the notion of state equivalence in MDPs and provide a model minimization algorithm that computes the partition of the state space so that the blocks of the partition can be regarded as abstract states. The result is a reduced MDP, while preserving the equivalence to the original MDP, which can be solved by traditional algorithms. Since the complexity of the algorithm depends on the size of the state space, we achieve savings in the computational cost compared to directly solving the original MDP. Pineau, Gordon, & Thrun (2003) extends the technique to POMDPs. These model minimization methods concern with grouping behaviorally equivalent states only while leaving the original actions and observations unchanged. Hence, further reduction is possible by recoding actions and observations. Copyright c © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Ravindran & Barto (2001) define this type of regularity as the symmetry in the model, and extend the model minimization method to cover the symmetries in MDPs. Later work by Wolfe (2006) extends the method to POMDPs. In this paper, we study the symmetry in POMDPs that has not been previously explored in the literature. While previous approaches have been primarily focused on aggregating states in order to reduce the size of the model, we are interested in the POMDP symmetry that is not related to reducing the size of the POMDP, nonetheless can be exploited to speed up conventional POMDP algorithms. Homomorphisms of POMDPs A POMDP M is defined as a 7-tuple 〈S,A,Z, T,O,R, b0〉: S is a set of states; A is a set of actions; Z is a set of observations; T is a transition function where T (s, a, s) denotes the probability P (s|s, a) of changing to state s from state s by executing action a; O is an observation function where O(s, a, z) denotes the probability P (z|s, a) of making observation z when executing action a and arriving in state s; R is a reward function where R(s, a) denotes the immediate reward of executing action a in state s; b0 is the initial belief where b0(s) is the probability at we start at state s. Model minimization methods for POMDPs search for a homomorphism φ that maps M to another equivalent POMDP M ′ = 〈S, A, Z , T , O, R〉 with the minimal model size. Formally, homomorphism φ is defined as 〈f, g, h〉 where f : S → S is the function that maps the states, g : A → A maps the actions, and h : Z → Z ′ maps the observations. Note that M ′ is a reduced model of M if any of the mappings is many-to-one. Depending on the definition of homomorphism φ, we obtain different definitions of the minimal model. Pineau, Gordon, & Thrun (2003) concern themselves with homomorphism φ defined only on the state spaces; the action and observation spaces remain the same as the original POMDP. Hence φ takes a form of 〈f, 1, 1〉 where 1 denotes the identity mapping. f should satisfy the following constraints in order to hold equivalence between M and M : T (f(s), a, f(s)) = ∑ s′′∈f−1(s′) T (s, a, s ) R(f(s), a) = R(s, a) O(f(s), a, z) = O(s, a, z) Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (2008)	algorithm;algorithmic efficiency;andrew barto;artificial intelligence;cobham's thesis;computation;computational complexity theory;dfa minimization;decision problem;experiment;graph automorphism;map;markov chain;neural coding;one-to-many (data model);partially observable markov decision process;reinforcement learning;scalability;software release life cycle;state space;turing completeness	Kee-Eung Kim	2008			markov decision process;mathematical optimization;partially observable markov decision process;computer science;state space;artificial intelligence;machine learning;graph automorphism	AI	21.64427056123079	-15.456312838422114	4730
b70cb13e393af80f1476f00a2ee54bc426488433	influential dmus and outlier detection in data envelopment analysis with an application to health care	influential dmu;hospital efficiency;outlier detection;data envelopment analysis dea;bootstrapping	This paper explains some drawbacks on previous approaches for detecting influential observations in deterministic nonparametric data envelopment analysis models as developed by Yang et al. (Annals of Operations Research 173:89–103, 2010). For example efficiency scores and relative entropies obtained in this model are unimportant to outlier detection and the empirical distribution of all estimated relative entropies is not a MonteCarlo approximation. In this paper we developed a new method to detect whether a specific DMU is truly influential and a statistical test has been applied to determine the significance level. An application for measuring efficiency of hospitals is used to show the superiority of this method that leads to significant advancements in outlier detection.	anomaly detection;approximation;data envelopment analysis;kullback–leibler divergence;monte carlo;operations research;sensor;yang	Ali Reza Bahari;Ali Emrouznejad	2014	Annals OR	10.1007/s10479-014-1604-8	econometrics;anomaly detection;computer science;data mining;bootstrapping;statistics	ML	28.235778492177964	-23.723333866931586	4742
36c5ef361386b018aa205596e2e64a0eb3401488	feature selection using data envelopment analysis	redundancy;data envelopment analysis;feature selection;relevance;super efficiency	Feature selection has been attracting increasing attention in recent years for its advantages in improving the predictive efficiency and reducing the cost of feature acquisition. In this paper, we regard feature selection as an efficiency evaluation process with multiple evaluation indices, and propose a novel feature selection framework based on Data Envelopment Analysis (DEA). The most significant advantages of this framework are that it can make a trade-off among several feature properties or evaluation criteria and evaluate features from a perspective of “efficient frontier” without parameter setting. We then propose a simple feature selection method based on the framework to effectively search “efficient” features with high class-relevance and low conditional independence. Super-efficiency DEA is employed in our method to fully rank features according to their efficiency scores. Experimental results for twelve well-known datasets indicate that proposed method is effective and outperforms several representative feature selection methods in most cases. The results also show the feasibility of proposed DEA-based feature selection framework.	data envelopment analysis;feature selection	Yishi Zhang;Anrong Yang;Chan Xiong;Teng Wang;Zigang Zhang	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.03.022	relevance;computer science;machine learning;pattern recognition;data mining;data envelopment analysis;redundancy;feature selection	NLP	11.9736030015436	-44.196099910746625	4756
de51dafe74457eeb553bb2de99ad12f874c0b5c7	learning to distribute vocabulary indexing for scalable visual search	database indexing;parallel computing;indexing structures;keywords bag of words;file servers;visualization servers vocabulary indexing feature extraction computer architecture mobile communication;visual vocabulary;memory constraints;vocabulary;near duplicates;computational power;indexing scheme;single server;partial server crash scalable visual search bag of words based near duplicate visual search paradigm inverted indexing indexing structures memory constraint image indexing machine learning memory light search paradigm search latency vocabulary boosting optimal distribution function distributed vocabulary indexing scheme real world location search system landmark images single server search distributed search search latency optimal distribution function server query state of the art alternatives excellent robustness;journal article;search system;inverted indexing;visual search;optimal distribution functions;multiple servers;distributed search;search problems;learning artificial intelligence;v distributed search;vocabulary database indexing file servers image retrieval learning artificial intelligence search problems;visual vocabulary distributed search inverted indexing parallel computing visual search;scale up;image retrieval	In recent years, there is an ever-increasing research focus on Bag-of-Words based near duplicate visual search paradigm with inverted indexing. One fundamental yet unexploited challenge is how to maintain the large indexing structures within a single server subject to its memory constraint, which is extremely hard to scale up to millions or even billions of images. In this paper, we propose to parallelize the near duplicate visual search architecture to index millions of images over multiple servers, including the distribution of both visual vocabulary and the corresponding indexing structure. We optimize the distribution of vocabulary indexing from a machine learning perspective, which provides a “memory light” search paradigm that leverages the computational power across multiple servers to reduce the search latency. Especially, our solution addresses two essential issues: “What to distribute” and “How to distribute”. “What to distribute” is addressed by a “lossy” vocabulary Boosting, which discards both frequent and indiscriminating words prior to distribution. “How to distribute” is addressed by learning an optimal distribution function, which maximizes the uniformity of assigning the words of a given query to multiple servers. We validate the distributed vocabulary indexing scheme in a real world location search system over 10 million landmark images. Comparing to the state-of-the-art alternatives of single-server search [5], [6], [16] and distributed search [23], our scheme has yielded a significant gain of about 200% speedup at comparable precision by distributing only 5% words. We also report excellent robustness even when partial servers crash.	bag-of-words model in computer vision;boosting (machine learning);circuit complexity;distributed web crawling;image retrieval;inverted index;lossy compression;machine learning;programming paradigm;scalability;server (computing);simulation;speedup;vocabulary;web search engine	Rongrong Ji;Ling-yu Duan;Jie Chen;Lexing Xie;Hongxun Yao;Wen Gao	2013	IEEE Transactions on Multimedia	10.1109/TMM.2012.2225035	database index;file server;visual search;image retrieval;computer science;database;world wide web;information retrieval	ML	17.940317904285013	-46.50414656715473	4778
36b2e82d4e48ddc76ecbd566404836df98795226	extended random jumps for classification of hyperspectral data		Extended random walker (ERW) has shown a good performance in hyperspectral data classification. In this paper some adjustment on spatial relation is applied in ERW algorithm. Segments obtained from Watershed segmentation are used forming the new spatial space to improve the classification accuracy.	ambient occlusion;amiga walker;ground truth;random walker algorithm;watershed (image processing)	Emad Mouselli;N. Gökhan Kasapoglu	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404316	pattern recognition;data classification;hyperspectral imaging;robustness (computer science);image segmentation;artificial intelligence;computer science;spatial relation;computer vision;random walker algorithm;statistical classification;watershed	Robotics	32.00017044328985	-44.29083058708115	4795
b19f5b05f08bc0c53bc50a2ee9a2b9ed348d7957	a copula-based trend-renewal process model for analysis of repairable systems with multitype failures		Reliability analysis of multicomponent repairable systems with dependent component failures is challenging for two reasons. First, the failure mechanism of one component may depend on other components when considering component failure dependence. Second, imperfect repair actions can have accumulated effects on the repaired components and these accumulated effects are difficult to measure. In this paper, we propose a parametric statistical model to capture the failure dependence information with general component repair actions. We apply the maximum likelihood method to estimate the model parameters by utilizing the historical failure data. Statistical hypothesis tests are developed to determine the dependence structure of the component failures based on the proposed reliability model. The proposed methodology is demonstrated by a simulation study and case studies of a car body assembly process and a forklift vehicle system.	failure cause;failure mode and effects analysis;parametric model;simulation;statistical model	Qingyu Yang;Yili Hong;Nailong Zhang;Jie Li	2017	IEEE Transactions on Reliability	10.1109/TR.2017.2693155	statistical hypothesis testing;maximum likelihood;reliability engineering;mathematics;copula (linguistics);statistics;maintenance engineering;parametric statistics;statistical model;renewal theory	SE	29.70336443801507	-18.982650235691843	4797
d3a556c96ed6f170823006c1b3a043f139f9bd7d	automatic dimensionality selection from the scree plot via the use of profile likelihood	singular value decomposition svd;fonction vraisemblance;data compression;analisis datos;resampling methods;dimension reduction;singular value decomposition;profile likelihood;manifold learning;funcion verosimilitud;reduction dimension;62f07;data analysis;principal component analysis pca;principal component analysis;isomap;latent semantic indexing;statistical computation;calculo estadistico;vraisemblance profil;analyse donnee;calcul statistique;denoising;resampling method;likelihood function	Most dimension reduction techniques produce ordered coordinates so that only the first few coordinates need be considered in subsequent analyses. The choice of how many coordinates to use is often made with a visual heuristic, i.e., by making a scree plot and looking for a “big gap” or an “elbow.” In this article, we present a simple and automatic procedure to accomplish this goal by maximizing a simple profile likelihood function. We give a wide variety of both simulated and real examples. © 2005 Elsevier B.V. All rights reserved.	analysis of algorithms;dimensionality reduction;document-term matrix;emoticon;experiment;factor analysis;heuristic;resampling (statistics)	Mu Zhu;Ali Ghodsi	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.09.010	data compression;econometrics;latent semantic indexing;machine learning;pattern recognition;noise reduction;mathematics;nonlinear dimensionality reduction;likelihood function;data analysis;singular value decomposition;statistics;dimensionality reduction;principal component analysis	AI	34.366060388852425	-25.218178148076362	4821
40dcd9ae485894e7c86e43e666ed50597167aab8	fuzzy covering based rough sets revisited		In this paper we review four fuzzy extensions of the socalled tight pair of covering based rough set approximation operators. Furthermore, we propose two new extensions of the tight pair: for the first model, we apply the technique of representation by levels to define the approximation operators, while the second model is an intuitive extension of the crisp operators. For the six models, we study which theoretical properties they satisfy. Moreover, we discuss interrelationships between the models.	approximation;f. lynn mcnulty;feature selection;granada;logical connective;rough set	Lynn D'eer;Chris Cornelis;Daniel Sánchez	2015			combinatorics;discrete mathematics;rough set;mathematics	AI	-1.5113484773514951	-23.586824277155937	4871
d4c37a59100668cd6d9ea5a6b03738972c45be5c	reconstructing the pre-doubling genome	physical mapping of chromosomes;saccharomyces cerevisiae;nucleotide sequence;consecutive ones property;exact algorithm;polynomial time;present day;branch and cut;computational biology;gene function;genome duplication	Genome duplication is an important source of new gene functions and novel physiological pathways. In the course of evolution, the nucleotide sequences of duplicated genes tend to diverge through mutation, so that one copy loses function (and disappears from view) or develops a new function, encoding a distinct but similar product. Originally a duplicated genome contains two identical copies of each chromosome, but through reciprocal translocation, parallel linkage patterns between the two copies are disrupted. Eventually, all that can be detected are several chromosome segments of greater or lesser length (blocks), each of which appears twice in the genome, containing many paralogous genes in parallel orders. We present an exact algorithm for reconstructing the ancestral pm-doubling genome in polynomial time, minimizing in key cases the number of translocations required to derive the observed order and orientation of blocks along the present-day chromosomes. We apply this to the genome duplication which has been described for Saccharomyces cerevisiae. 1 Genome duplication Perhaps the most spectacular cause of gene duplication is tetraploidization of the genome. Normally a lethal accident of meiosis or other reproductive step, if this doubling of the genome can be resolved in the organism and eventually fixed as a normalized diploid state in a population, it represents a simultaneous duplication of the entire genetic complement. It transcends other mechanisms for gene duplication in that not only is one copy of each gene free to evolve its own function, but it can evolve in concert with any ‘DBpartement d’Informatique et de recherche op&ationnelle, Universitd de Montreal, CP 6128 Succursale Centre-ville, Montreal, Quebec H3C 357. E-mail: mabroukQiro.umontreal.ca. ’ Centre de recherches mathematiques, Universite de Montreal, CP 6128 Succursale Centre-ville, Mont&al, Quebec H3C 357. E-mail: bryantQcrm.umontreal.ca. *Centre de recherches mathematiques, Universite de Montreal, CP 6128 Succursale Centre-ville, Montreal, Quebec H3C 357. E-mail: sankoff@ere.umontreal.ca. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies arc not made or distributed for profit or commercial adbanragc wd that copies bear this notice and the full citation on the first pgc. ‘fo COPY otherwise. to republish, to post on sewers or to redistribute to lists. requires prior specilic permission an&or a fee. RECOMB ‘99 Lyon France Copyright ACM 1999 l-581 13-069-4/99/04...$5.00 subset of the thousands of other extra gene copies (cf [4] for accounts of gene family coevolution). Whole new physiological pathways may emerge, involving novel functions for many of these genes. Genome duplication is thus a likely source of rapid and far-reaching evolutionary progress. Its rarity does not detract from its importance. Evidence for its effects has shown up across the eukaryote spectrum. More than two hundred million years ago the vertebrate genome underwent two duplications [2, 7,121. Although numerous chromosome rearrangements such as inversions and reciprocal translocations have subsequently occurred, the number of rearrangements has been sufficiently modest that hundreds of conserved paralogous segments can be detected in the human genome since the ancient duplications; similar observations hold for the murine genome [lo, 111 and for less intensively mapped vertebrate genomes. More recent genome duplications are known to have occurred in some vertebrate lines, such as the frogs [19], the salmoniform fish [12] and zebra&h [14]. Comparison of chromatin-eliminating Ascadae with other nematodes suggest that somatic cells of these worms have discarded a good proportion of the genes present in germ cells, possible because these are redundant duplicates arising through genomic doubling some 200 million years ago bl. Genome duplication is particularly prevalent in plants. Comparison of the well-studied rice [I], oats (wild and domestic), corn [l, 51 and wheat [9] genomes indicate several occurrences in the cereal lineage. Soybeans [17], rapeseed [15], and other cultivars have genome duplications in their ancestry. Paterson et al. have presented convincing evidence that one or more genome duplications also occurred much earlier in plant evolution [13]. Recently, following the complete sequencing of all Saccharomyces cerewiaiae chromosomes, the prevalence of gene duplication has led to the conclusion that this yeast genome is also the product of an ancient doubling [18]. Subsequent to genome duplication, duplicated genes tend to diverge through mutation, so that one copy loses function (becomes a pseudogene) or develops a new function, encoding a distinct but similar product. Orig.inally a duplicated genome contains two identical copies of each chromosome, but through inversion or other intrachromosomal movement, the gene orders in each pair of chromosomes change independently, and through reciprocal translocation, parallel linkage patterns between the two copies are disrupted. Eventually, all that can be detected are several chromosome segments of greater or lesser length (blocks), each of which appears twice in the genome, containing many paralogous genes in parallel	emergentism;exact algorithm;gene family;homology (biology);lineage (evolution);linkage (software);mopy fish;mike lesser;period-doubling bifurcation;research in computational molecular biology;sequence homology;time complexity;whole earth 'lectronic link;whole genome sequencing	Nadia El-Mabrouk;David Bryant;David Sankoff	1999		10.1145/299432.299475	time complexity;biology;nucleic acid sequence;computer science;bioinformatics;mathematics;genetics;branch and cut	Comp.	0.3711965567631187	-51.92338742064807	4879
1410ebdb84c8806f75c2bde1f4aaa7ed388d3da5	bit-rate control using piecewise approximated rate-distortion characteristics	debit binaire variable;rate distortion;interpolation;transformation cosinus;deformation ratio;image processing;interframe encoding;decoding;interactive video;mpeg video;telecommunication control;procesamiento imagen;interactive video video coding rate distortion theory telecommunication control decoding computational complexity delays interpolation discrete cosine transforms transform coding;transform coding;indexing terms;qualite image;traitement image;rate distortion psnr decoding distortion measurement quantization rate distortion theory broadcasting delay computer applications benchmark testing;rate distortion theory;rate control;video coding;porcentaje deformacion;senal video;signal video;linearisation morceau;computational complexity;discrete cosine transforms;dct bit rate control piecewise approximated rate distortion digital video international standards mpeg 1 mpeg 2 h 263 rate control decoding playback quality encoder design channel rate buffer size input data quantization assignment decisions complexity distortion characteristics rate characteristics encoding delay interactive video peak signal to noise rate psnr mpeg test model 5 video sources encoding rates encoding time;senal numerica;image quality;transformacion coseno;variable bit rate;taux deformation;linearizacion trozo;signal numerique;video signal;calidad imagen;mpeg;rapport signal bruit;relacion senal ruido;digital signal;cosine transform;signal to noise ratio;rate distortion optimization;codage interimage;codificacion entre imagen;piecewise linearization;delays;internal standard	Digital video’s increased popularity has been driven to a large extent by a flurry of recently proposed international standards (MPEG-1, MPEG-2, H.263, etc.). In most standards, the rate control scheme, which plays an important role in improving and stabilizing the decoding and playback quality, is not defined, and thus different strategies can be implemented in each encoder design. Several rate–distortion (R–D)-based techniques have been proposed to aim at the best possible quality for a given channel rate and buffer size. These approaches are complex because they require the R–D characteristics of the input data to be measured before making quantization assignment decisions. In this paper, we show how the complexity of computing the R–D data can be reduced without significantly reducing the performance of the optimization procedure. We propose two methods which provide successive reductions in complexity by: 1) using models to interpolate the rate and distortion characteristics, and 2) using past frames instead of current ones to determine the models. Our first method is applicable to situations (e.g., broadcast video) where a long encoding delay is possible, while our second approach is more useful for computation-constrained interactive video applications. The first method can also be used to benchmark other approaches. Both methods can achieve over 1 dB peak signal-to-noise rate (PSNR) gain over simple methods like the MPEG Test Model 5 (TM5) rate control, with even greater gains during scene change transitions. In addition, both methods make fewa priori assumptions and provide robustness in their performance over a range of video sources and encoding rates. In terms of complexity, our first algorithm roughly doubles the encoding time as compared to simpler techniques (such as TM5). However, complexity is greatly reduced as compared to methods which exactly measure the R–D data. Our second algorithm has a complexity marginally higher than TM5 and a PSNR performance slightly lower than that of the first approach.	approximation algorithm;benchmark (computing);computation;digital video;distortion;encoder;interpolation;mpeg-1;mpeg-2;mathematical optimization;moving picture experts group;peak signal-to-noise ratio;rate–distortion theory	Liang-Jin Lin;Antonio Ortega	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.709411	image quality;transform coding;index term;rate–distortion theory;telecommunications;image processing;interpolation;digital signal;computer science;theoretical computer science;discrete cosine transform;internal standard;mathematics;variable bitrate;rate–distortion optimization;computational complexity theory;signal-to-noise ratio;algorithm;statistics	EDA	47.059870547224826	-16.530624769313757	4880
990230454299c1d89a5c7efe26e63d54efb12175	detection of land-cover transitions by combining multidate classifiers	remote sensing image;radial basis function neural networks;change detection;weighted averaging;multiple classifier systems;multilayer perceptron;remote sensing data;radial basis function neural network;expectation maximization algorithm;k nn technique;detection of land cover transitions;k nearest neighbour;multilayer perceptron neural networks;remote sensing images;majority voting;multiple classifier system;land cover;decision rule;neural network;multitemporal classification	This paper addresses the problem of detecting land-cover transitions by analysing multitemporal remote-sensing images. In order to develop an effective system for the detection of land-cover transitions, an ensemble of non-parametric multitemporal classifiers is defined and integrated in the context of a multiple classifier system (MCS). Each multitemporal classifier is developed in the framework of the compound classification (CC) decision rule. To develop as uncorrelated as possible classification procedures, the estimates of statistical parameters of classifiers are carried out according to different approaches (i.e., multilayer perceptron neural networks, radial basis functions neural networks, and k-nearest neighbour technique). The outputs provided by different classifiers are combined according to three standard strategies extended to the compound classification case (i.e., Majority voting, Bayesian average, and Bayesian weighted average). Experiments, carried out on a multitemporal remote-sensing data set, confirm the effectiveness of the proposed system. 2004 Elsevier B.V. All rights reserved.	artificial neural network;ensemble learning;ensembles of classifiers;k-nearest neighbors algorithm;learning classifier system;linear classifier;memory-level parallelism;multi categories security;multilayer perceptron;radial (radio);radial basis function;robustness (computer science);sensor;spatial variability;statistical classification	Lorenzo Bruzzone;Roberto Cossu;Gianni Vernazza	2004	Pattern Recognition Letters	10.1016/j.patrec.2004.06.002	random subspace method;majority rule;expectation–maximization algorithm;computer science;machine learning;pattern recognition;data mining;decision rule;multilayer perceptron;change detection;artificial neural network	AI	31.759403792866607	-44.08753391862912	4883
c51da765a82404269393589982ff88b869b63456	in situ tensorview: in situ visualization of convolutional neural networks		Convolutional Neural Networks(CNNs) are complex systems trained to recognize images, texts and more. However, once trained, they are regarded as black-boxes that are not easy to analyze and understand. Visualizing the dynamics within such deep artificial neural networks can provide a better understanding of how they are learning and making predictions. In the field of scientific simulations, visualization tools like Paraview have long been utilized to provide insights. We present in situ TensorView to visualize the training and functioning of CNNs as if they are systems of scientific simulations. In situ TensorView is a loosely coupled in situ visualization open framework that provides multiple viewers with the ability to visualize and understand their networks. It leverages the capability of co-processing from Paraview to provide real-time visualization during training and predicting phases, and avoids heavy I/O overhead. Tensorview is easily coupled with Tensorflow, as it only requires the insertion of a few lines of code into a TensorFlow framework. In this work, we showcase visualizing LeNet-5 and VGG16 using in situ TensorView. With the insight provided by Tensorview, users can adjust network architectures, or compress pre-trained networks guided by visualization results.	artificial neural network;code;complex systems;convolutional neural network;dynamical system;experiment;input/output;loose coupling;neural network software;overhead (computing);real-time clock;scalability;simulation;tensorflow	Xinyu Chen;Qiang Guan;Li-Ta Lo;Simon Su;James P. Ahrens;Trilce Estrada	2018	CoRR		complex system;visualization;machine learning;artificial intelligence;convolutional neural network;data mining;network architecture;artificial neural network;source lines of code;computer science;in situ	Visualization	18.092553713549552	-24.861237603250817	4884
4aa91934c3deea4d6613f8f41c7064b62ec939f0	structuring search space for accelerating large set character recognition	traitement signal;targe character set;tecnologia electronica telecomunicaciones;algoritmo busqueda;japonais;search space;algorithme recherche;search algorithm;juego caracter;feature space;similitude;character set;reconnaissance caractere;feature extraction;signal processing;similarity;jeu caractere;number of clusters;pattern recognition;reconnaissance forme;similitud;extraction caracteristique;candidate selection;tecnologias;reconocimiento patron;grupo a;procesamiento senal;large character set;character recognition;japones;reconocimiento caracter;pivot;japanese	This paper proposes a “structuring search space” (SSS) method aimed to accelerate recognition of large character sets. We divide the feature space of character categories into smaller clusters and derive the centroid of each cluster as a pivot. Given an input pattern, it is compared with all the pivots and only a limited number of clusters whose pivots have higher similarity (or smaller distance) to the input pattern are searched in, thus accelerating the recognition speed. This is based on the assumption that the search space is a distance space. We also consider two ways of candidate selection and finally combine them. The method has been applied to a practical off-line Japanese character recognizer with the result that the coarse classification time is reduced to 56% and the whole recognition time is reduced to 52% while keeping its recognition rate as the original.	character encoding;feature vector;finite-state machine;online and offline	Yiping Yang;Bilan Zhu;Masaki Nakagawa	2005	IEICE Transactions	10.1093/ietisy/e88-d.8.1799	japanese;speech recognition;similarity;feature extraction;computer science;artificial intelligence;similitude;machine learning;signal processing;mathematics;algorithm;search algorithm	Vision	15.825450044246974	-51.62530409086647	4890
01f06f307eab9e9218930e3f6a6a24378215fefc	a novel scalable video coding scheme using super resolution techniques	video sequence;image motion analysis;high resolution;scalable video coding scheme;scalable video coding;image resolution;decoding;high resolution frames;h 264 avc;low resolution;bit rate;subjective visual quality scalable video coding scheme super resolution techniques h 264 avc spatial temporal scalability high resolution frames video sequence skipped frames reconstruction decoding low bit rate transmission encoding complexity reduction peak signal to noise ratio;visual quality;computer vision;encoding complexity reduction;video coding;optical imaging;super resolution techniques;low bit rate transmission;computational complexity;image reconstruction image resolution image motion analysis computer vision optical imaging bit rate static var compensators;image reconstruction;peak signal to noise ratio;subjective visual quality;super resolution;static var compensators;video coding computational complexity decoding image reconstruction image resolution image sequences;inproceedings;spatial temporal scalability;skipped frames reconstruction;image sequences	A novel scheme of scalable video coding (SVC) using super-resolution techniques is proposed in this paper. Utilizing the spatial/temporal scalability of H.264/AVC, we encode half of input high-resolution(HR) frames and their low-resolution (LR) counterparts in a video sequence and employ super-resolution (SR) method to reconstruct skipped frames during decoding. The payload saved from skipped frames can be used to improve the quality of encoded frames. This scheme provides a choice of SVC to improve the quality of HR frames while maintaining low bit rate transmission and reducing encoding complexity. Experiments show that our scheme performs well in both peak signal to noise ratio (PSNR) and subjective visual quality at low bit rate.	algorithm;algorithmic efficiency;data compression;encode;emoticon;h.264/mpeg-4 avc;half rate;image fusion;jumbo frame;lr parser;peak signal-to-noise ratio;scalability;scalable video coding;super-resolution imaging	Minmin Shen;Ping Xue;Ci Wang	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665074	reference frame;computer vision;image resolution;computer science;theoretical computer science;computer graphics (images)	Vision	45.3568022566619	-18.009039697688156	4892
1ffdefd7839a57b222bd06d6e33449992aa4c44f	game analysis on concentration ratio of iron and steel industry	trigger strategy concentration ratio terms of binding punishment discount factor;steel industry game theory;incomplete information dynamic games;pillar industry;game theory;trigger strategy;discout factor;game analysis;dynamic game;iron industry;which;publishing;metals industry games field flow fractionation iron publishing steel;iron;meso three levels;infinitely repeated dynamic games;national economy;national economy which;punishment;macro three levels;concentration ratio;mico three levels;incomplete information;discount factor;metals industry;steel industry;games;steel;mico three levels concentration ratio iron industry steel industry pillar industry national economy which discout factor trigger strategy binding punishment complete information static games infinitely repeated dynamic games incomplete information dynamic games game theory macro three levels meso three levels;terms of binding;binding;field flow fractionation;complete information static games	Iron and steel industry is an important pillar industry in national economy, which plays a very important strategic role in the development of China's economy. That the concentration ratio of iron and steel industry of China is low has seriously constrainted the development of the industry. Based on an overview of the status quo of the concentration ratio of iron and steel industry, this paper reaches conclusions that terms of binding punishment discout factor and trigger strategy are main factors that affect the concentration ratio of iron and steel industry by adopting Complete Information Static Games???Infinitely Repeated Dynamic Games Incomplete Information Dynamic Games of three methods of game theory, and make recommendations from macro???meso and mico three levels.			2010		10.1109/ICEE.2010.1024	field flow fractionation;game theory;simulation;economics;marketing;operations management;publishing;management;iron;commerce;trigger strategy	ML	-0.21825505356772754	-8.866374589969473	4904
bba400b7747dfdb7ed343a432cfee3242874e0db	on an ethical use of neural networks: a case study on a north indian raga		"""The paper gives an artificial neural network (ANN) approach to time series modeling, the data being instance versus notes (characterized by pitch) depicting the structure of a North Indian raga, namely, Bageshree. Respecting the sentiments of the artists' community, the paper argues why it is more ethical to model a structure than try and""""manufacture""""an artist by training the neural network to copy performances of artists. Indian Classical Music centers on the ragas, where emotion and devotion are both important and neither can be substituted by such""""calculated artistry""""which the ANN generated copies are ultimately up to."""	neural networks;performance;time series	Ripunjai Kumar Shukla;Soubhik Chakraborty	2012	CoRR		computer science;artificial intelligence;machine learning	ML	7.493233289669996	-18.090104984940993	4905
68345b8fc27ae9b81500ef72f400e9bfa13040a1	characterization of the hokuyo ubg-04lx-f01 2d laser rangefinder	calibration model hokuyo ubg 04lx f01 2d laser rangefinder characterisation mobile robot distance measurement measurement error incidence angle brightness;measurement error;object detection brightness calibration laser ranging measurement errors mobile robots;mobile robot;laser rangefinder;mobile robots;laser ranging;brightness;measurement by laser beam brightness indexes color measurement uncertainty frequency measurement measurement errors;calibration;measurement errors;object detection	This paper presents a characterization of the Hokuyo UBG-04LX-F01 laser rangefinder (LRF). The Hokuyo LRFs are suitable for a small mobile robot due to their small size and light weight. In particular, the scan frequency of Hokuyo UBG-04LX-F01 is higher than those of the previous LRFs in its price range. However, there is no research on characterizing this LRF for the convenience of practical use. Therefore, this paper characterizes the Hokuyo UBG-04LX-F01 and analyzes the effect of the target properties, such as incidence angle, color, brightness, and material, by measuring the distance to a target. The experimental results show that the measurement error is strongly influenced by the incidence angle and the brightness of the target surface. From the experimental results, a calibration model is also proposed to measure the accurate distance to a target.	color;incidence matrix;mean squared error;mobile robot	Chan-Soo Park;Doik Kim;Bum-Jae You;Sang-Rok Oh	2010	19th International Symposium in Robot and Human Interactive Communication	10.1109/ROMAN.2010.5598672	mobile robot;computer vision;computer science;artificial intelligence;observational error	Robotics	53.36538328615682	-35.32266217373353	4906
cba51b276817b5d715ece73ba68844590e79ff06	an enhanced clusterer aggregation using nebulous pool	degree of agreement;nebulous pool;spatial data;spatial data mining;error rate;missing values;cluster validity;clustering ensembles	Cluster Ensembles is a framework for combining multiple partitioning obtained from separate clustering runs into a final consensus clustering. In this paper, we have analyzed that using a layered approach in combining the clusterer outputs can help in reducing the intensive computing and also provide scope for reuse the knowledge gained for further merging. We have discussed our proposed layered cluster merging technique for spatial datasets and used it in our three-phase nebulous pool aggregator in this paper. At the first level, B heterogeneous ensembles are run against the same spatial data set D of size n data points to generate clustering results. A voting matrix of size n X B is first generated from which agreement count is derived. Based of the agreement count and the degree of agreement, partial cluster aggregation is obtained. The aggregated clusters so obtained are scanned for data points with tie problem. Such uncertainly classified data points are then removed from the aggregated clusters and stored in a nebulous pool, which will be merged into our final clusters using maximum overshadow technique. We have also merged vertical sliced clusterings in our homogenous ensemble aggregation, and found better substitute for the missing value in any attribute for a given data point. Cluster validation metrics like cluster accuracy, inter and intra cluster density and error rates have been measured to confirm that our aggregation results are more robust and accurate.	cluster analysis;computer cluster;consensus clustering;data point;defense in depth (computing);missing data;pool (computer science);social network aggregation	R. J. Anandhi;S. Natarajan;Sunitha Abburu	2010		10.1145/1858378.1858439	computer science;machine learning;data mining;statistics	ML	1.1417752739209435	-42.297279414550886	4911
b679f4059c462b20b093fe700a0564ddcda0a686	particle swarm optimisation with stochastic ranking for constrained numerical and engineering benchmark problems	stochastic ranking;sr;pso;constrained optimisation;particle swarm optimisation	Most of the real world science and engineering optimisation problems are non-linear and constrained. This paper presents a hybrid algorithm by integrating particle swarm optimisation with stochastic ranking for solving standard constrained numerical and engineering benchmark problems. Stochastic ranking technique that uses bubble sort mechanism for ranking the solutions and maintains a balance between the objective and the penalty function. The faster convergence of particle swarm optimisation and the ranking technique are the major motivations for hybridising these two concepts and to propose the stochastic ranking particle swarm optimisation (SRPSO) technique. In this paper, SRPSO is used to optimise 15 continuous constrained single objective benchmark functions and five well-studied engineering design problems. The performance of the proposed algorithm is evaluated based on the statistical parameters such mean, median, best, worst values and standard deviations. The SRPSO algorithm is compared with six recent algorithms for function optimisation. The simulation results indicate that the SRPSO algorithm performs much better while solving all the five standard engineering design problems where as it gives a competitive result for constrained numerical benchmark functions.	algorithmic efficiency;benchmark (computing);bubble sort;cops (software);computation;engineering design process;hybrid algorithm;mathematical optimization;nonlinear system;numerical analysis;particle swarm optimization;penalty method;rate of convergence;simulation	Layak Ali;Samrat L. Sabat;Siba K. Udgata	2012	IJBIC	10.1504/IJBIC.2012.047238	mathematical optimization;artificial intelligence;machine learning;mathematics;special relativity	AI	24.198199729030005	-3.2371844806748853	4914
4ee91445c250de6e3cc7946d3a4a0dd1d8484070	y-aoi: y-means based attribute oriented induction identifying root cause for idss	intervalo tiempo;analyse amas;securite;time interval;classification;induccion;cluster analysis;induction;safety;analisis cluster;seguridad;clasificacion;intervalle temps	The attribute oriented induction (AOI) is a kind of aggregation method. By generalizing the attributes of the alert, it creates several clusters that includes a set of alerts having similar or the same cause. However, if the attributes are excessively abstracted, the administrator does not identify the root cause of the attack. In addition, deciding time interval of clustering and deciding min_size are one of the most critical problems. In this paper, we describe about the over-generalization problem because of the unbalanced generalization hierarchy and discuss the solution of the problem. We also discuss problem to decide time interval and meaningful min_size, and propose reasonable method to solve these problems.		Jungtae Kim;Gunhee Lee;Jung-Taek Seo;Eung Ki Park;Choonsik Park;Dong-Kyoo Kim	2005		10.1007/11540007_25	biological classification;computer science;artificial intelligence;machine learning;data mining;mathematics;cluster analysis;algorithm	NLP	-0.6577949487318271	-31.054968946409904	4922
0c8910fce41b399ea2729200b8b4cf4ee664f664	tracking a planar patch by additive image registration	alignement;pistage;matrice jacobi;image matching;rastreo;registro imagen;jacobi matrix;recalage image;matriz jacobi;image registration;alineamiento;sum of squares;alignment;appariement image;tracking	We present a procedure for tracking a planar patch based on a precomputed Jacobian of the target region to be tracked and the sum of squared differences between the image of the patch in the current position and a previously stored image of if. The procedure presented improves previous tracking algorithms for planar patches in that we use a minimal parameterisation for the motion model. In the paper, after a brief presentation of the incremental alignment paradigm for tracking, we present the motion model, the procedure to estimate the image Jacobian and, finally, an experiment in which we compare the gain in accuracy of the new tracker compared to previous approaches to solve the same problem.	additive model;algorithm;image registration;jacobian matrix and determinant;precomputation;programming paradigm	José Miguel Buenaposada;Enrique Muñoz;Luis Baumela	2003		10.1007/978-3-540-39798-4_9	jacobian matrix and determinant;computer vision;mathematical optimization;computer science;image registration;mathematics;geometry;tracking;explained sum of squares	Vision	51.37672014096593	-50.00322470125325	4932
5dfbdce7bd874f0cb1fa05940d4ab2836e3d4d73	reinforcement learning of a continuous motor sequence with hidden states	unsupervised learning;actor critic method;continuous time;perceptual aliasing problem;learning algorithm;motion control;reinforcement learning;state space;pendulum swing up;self organization;recurrent neural network;continuous time recurrent neural network	Reinforcement learning is the scheme for unsupervised learning in which robots are expected to acquire behavior skills through self-explorations based on reward signals. There are some difficulties, however, in applying conventional reinforcement learning algorithms to motion control tasks of a robot because most algorithms are concerned with discrete state space and based on the assumption of complete observability of the state. Real-world environments often have partial observablility; therefore, robots have to estimate the unobservable hidden states. This paper proposes a method to solve these two problems by combining the reinforcement learning algorithm and a learning algorithm for a continuous time recurrent neural network (CTRNN). The CTRNN can learn spatiotemporal structures in a continuous time and space domain, and can preserve the contextual flow by a self-organizing appropriate internal memory structure. This enables the robot to deal with the hidden state problem. We carried out an experiment on the pendulum swing-up task without rotational speed information. As a result, this task is accomplished in several hundred trials using the proposed algorithm. In addition, it is shown that the information about the rotational speed of the pendulum, which is considered as a hidden state, is estimated and encoded on the activation of a context neuron.	algorithm;artificial neural network;computer data storage;machine learning;neuron;organizing (structure);recurrent neural network;regular expression;reinforcement learning;robot;self-organization;state space;unsupervised learning	Hiroaki Arie;Tetsuya Ogata;Jun Tani;Shigeki Sugano	2007	Advanced Robotics	10.1163/156855307781389365	temporal difference learning;semi-supervised learning;unsupervised learning;motion control;robot learning;error-driven learning;self-organization;simulation;wake-sleep algorithm;computer science;state space;artificial intelligence;recurrent neural network;machine learning;control theory;learning classifier system;competitive learning;reinforcement learning;artificial neural network;generalization error	AI	19.28574070236763	-22.794005252120844	4943
cd09fbde838db199f5822a80bb2434a3f8efa22b	rotation and scale insensitive image watermarking	watermarking;detection probability;image coding;probability;fourier transform;data compression;watermarking fourier transforms copyright protection interpolation discrete fourier transforms communication system control underwater tracking noise robustness image resolution detectors;copyright;data encapsulation;copyright protection;fourier transforms;image coding watermarking copyright fourier transforms probability data compression data encapsulation;roc curve;rotation scaling and translation;image watermarking;jpeg coding rotation scale insensitive image watermarking electronic watermark copyright protection still image rotation scaling translation distortion rst watermark embedding 1d invariant domain polar fourier transform roc curve false positives detection probability geometrical attack;false positive	Using electronic watermarks as copyright protection for still images requires robustness against attacks. In this paper we propose a watermarking scheme that is robust to rotation, scaling and translation (RST) distortions. The watermark is embedded in a 1D invariant domain that is a projection of the polar Fourier transform. Roc curves depicting false positives versus detection probability of the watermark are provided, under various geometrical attacks and JPEG coding.	algorithm;digital watermarking;distortion;embedded system;image scaling;jpeg;receiver operating characteristic;reverse semantic traceability;watermark (data file)	Maxime Ossonce;Claude Delpha;Pierre Duhamel	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421638	fourier transform;computer vision;theoretical computer science;mathematics;computer security;statistics	Robotics	41.80432695716988	-10.645132711775306	4969
50dee7b274cdba3ccbaaf105bd0fd737988263d9	confidence transformation for combining classifiers	combination of classifiers;scale function;sample size;classifier combination;activation function;handwritten digit recognition;confidence transformation;logistic regression;combining classifier;gaussian modeling;pattern classification;evidence combination;parameter estimation	This paper investigates a number of confidence transformation methods for measurement-level combination of classifiers. Each confidence transformation method is the combination of a scaling function and an activation function. The activation functions correspond to different types of confidences: likelihood (exponential), log-likelihood, sigmoid, and the evidence combination of sigmoid measures. The sigmoid and evidence measures serve as approximates to class probabilities. The scaling functions are derived by Gaussian density modeling, logistic regression with variable inputs, etc. We test the confidence transformation methods in handwritten digit recognition by combining variable sets of classifiers: neural classifiers only, distance classifiers only, strong classifiers, and mixed strong/weak classifiers. The results show that confidence transformation is efficient to improve the combination performance in all the settings. The normalization of class probabilities to unity of sum is shown to be detrimental to the combination performance. Comparing the scaling functions, the Gaussian method and the logistic regression perform well in most cases. Regarding the confidence types, the sigmoid and evidence measures perform well in most cases, and the evidence measure generally outperforms the sigmoid measure. We also show that the confidence transformation methods are highly robust to the validation sample size in parameter estimation.	activation function;estimation theory;gaussian elimination;image scaling;logistic regression;sigmoid function;time complexity;wavelet	Cheng-Lin Liu;Hongwei Hao;Hiroshi Sako	2003	Pattern Analysis and Applications	10.1007/s10044-003-0199-5	random subspace method;sample size determination;computer science;machine learning;pattern recognition;mathematics;logistic regression;estimation theory;activation function;statistics	ML	21.949176900860934	-29.191330758515658	4977
caae7dffc9d747d8b925c2dad8eaf6cc930b5d04	optimized particles for 3d tracking	particle filter;3d tracking;importance density function	3D visual tracking is useful for many applications. In this paper, we propose two different ways for different system configurations to optimize particle filter for enhancing 3D tracking performances. On the one hand, a new data fusion method is proposed to obtain the optimal importance density function for active vision systems. With this method, the importance density function in particle filter can be modified to represent posterior states by particle crowds in a better way. Thus, it makes the tracking system more robust to noise and outliers. On the other hand, we develop a method for reconfigurable vision systems to maximize the effective sampling size in particle filter, which consequentially helps to solve the degeneracy problem and minimize the tracking error. Simulation and experimental results verified the effectiveness of the proposed method.		Huiying Chen;Youfu Li	2011	I. J. Humanoid Robotics	10.1142/S021984361100268X	computer vision;simulation;particle filter;auxiliary particle filter;computer science	Robotics	45.65880780357959	-47.31348417018119	4997
245abbb5e5d991d9e29219c198c8517f3558d380	solutions of fuzzy correspondence inequations with sup-conjunctor composition		This paper investigates the solutions of fuzzy correspondence inequations with sup-conjunctor composition, i.e., it discusses those solutions for the input and output fuzzy sets which are unknown while a fuzzy correspondence is fixed. First, it proves that the solutions of the fuzzy correspondence inequations can be analyzed and formulated by solving the corresponding cut set problems. It then shows that the space of solutions of the fuzzy correspondence inequation is a complete distributive lattice and the same holds for the space of solutions of the fuzzy correspondence equation. Moreover, it describes the solution set of the fuzzy correspondence inequation partly.	cut (graph theory);fuzzy set;input/output	Kai Zuo;Xueping Wang;Xiaohong Zhang	2017	2017 3rd International Conference on Computational Intelligence & Communication Technology (CICT)	10.1109/IFSA-SCIS.2017.8023313	inequation;mathematical optimization;fuzzy mathematics;discrete mathematics;fuzzy set operations;defuzzification;fuzzy set;fuzzy subalgebra;mathematics;fuzzy number;fuzzy classification	Robotics	0.1451538113487968	-22.629947853027865	5005
27a04a66f0535644b1841255bc8caf2b509fc62e	a multi-stack based phylogenetic tree building method	empirical study;generic model;maximum likelihood;tree estimation;distance estimation;multi stack;phylogenetic tree;constrained least square;priority queue;neighbor joining;tree joining operator;phylogenetics	Here we introduce a new Multi-Stack (MS) based phylogenetic tree building method. The Multi-Stack approach organizes the candidate subtrees (i.e. those having same number of leaves) into limited priority queues, always selecting the K-best subtrees, according to their distance estimation error. Using the K-best subtrees our method iteratively applies a novel subtree joining strategy to generate candidate higher level subtrees from the existing low-level ones. This new MS method uses the Constrained Least Squares Criteria (CLSC) which guarantees the non-negativity of the edge weights. The method was evaluated on real-life datasets as well as on artificial data. Our empirical study consists of three very different biological domains, and the artificial tests were carried out by applying a proper model population generator which evolves the sequences according to the predetermined branching pattern of a randomly generated model tree. The MS method was compared with the Unweighted Pair Group Method (UPGMA), Neighbor-Joining (NJ), Maximum Likelihood (ML) and FitchMargoliash (FM) methods in terms of Branch Score Distance (BSD) and Distance Estimation Error (DEE). The results show clearly that the MS method can achieve improvements in building phylogenetic trees.	algorithm;bsd;benchmark (computing);brute-force search;fm broadcasting;high- and low-level;iterative method;least squares;lineage (evolution);negativity (quantum mechanics);neighbor joining;phylogenetic tree;phylogenetics;priority queue;procedural generation;real life;resultant;tree (data structure);upgma	Róbert Busa-Fekete;András Kocsor;Csaba Bagyinka	2007		10.1007/978-3-540-72031-7_5	biology;mathematical optimization;combinatorics;discrete mathematics;phylogenetic tree;tree rotation;computer science;machine learning;neighbor joining;mathematics;maximum likelihood;search tree;empirical research;priority queue;statistics;phylogenetics	Comp.	31.537194415992154	-1.2904880621126364	5008
a6c465ca2b60e88e3795973c7e681edd5601b321	simsearcher: a local similarity search engine for biological sequence databases	biology computing;sequences;frequent pattern;query processing;search engines;data mining;search engines databases pattern matching genetic mutations biology computer science data engineering data mining information retrieval computational efficiency;query processing search engines data mining pattern matching biology computing scientific information systems sequences;pattern matching;proceedings paper;computational efficiency;ibm delphi system simsearcher local similarity search engine biological sequence databases data mining query sequence pattern matching database sequence;similarity search;scientific information systems	An efficient local similarity search engine is developed by exploiting some techniques of data mining. All frequent patterns in the database are retrieved and recorded in a one-time preprocessing process. Then a query sequence is checked to see whether any pattern from the preprocessing stage is matched to the query. Two regions coming from the query and a database sequence that both match a pattern form a possible seed for local similarity. Finally, we extend and score each such seed region pair to see whether there really exists local similarity with a score high enough for reporting. For computational efficiency, a novel clustering approach is proposed and integrated into the proposed system, which is based on the local similarity search engine - the DELPHI system proposed by IBM. Extensive experiments are demonstrated to show the performance of our system.	cluster analysis;data mining;database;experiment;preprocessor;similarity search;web search engine	Tian-Haw Tsai;Suh-Yin Lee	2003	Fifth International Symposium on Multimedia Software Engineering, 2003. Proceedings.	10.1109/MMSE.2003.1254456	search-oriented architecture;sargable;query optimization;query expansion;database search engine;ranking;computer science;pattern matching;data mining;sequence;database;programming language;web search query;world wide web;information retrieval;search engine	DB	-3.4176038081483395	-51.722890531427524	5023
3c477ab15e988d96f06f747c650f426231e07577	unbiased estimates for linear regression via volume sampling		Given a full rank matrix X with more columns than rows, consider the task of estimating the pseudo inverse X based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times X+>X+. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of X. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from X. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.	algorithm;column (database);ibm notes;kerrison predictor;linear least squares (mathematics);nyquist–shannon sampling theorem;sampling (signal processing);total loss	Michal Derezinski;Manfred K. Warmuth	2017			u-statistic;best linear unbiased prediction;bayesian multivariate linear regression;statistics	ML	25.777760398263496	-35.06879366176289	5030
7d3918160217d666bf8a4006e60433fc8ff11d98	rate distortion optimized curve determination for curved wavelet image coding	image sampling;high pass;rate distortion optimization curved wavelet transform image coding;rate distortion;optimisation;image coding;codecs;image quality curved wavelet image coding rate distortion optimized curve determination curved wavelet transform wavelet transform representation r d optimization algorithm variable block size sampling strategy;natural images;variable block size;curved wavelet transform;wavelet transforms;distortion;wavelet transform;image quality;rate distortion image coding continuous wavelet transforms wavelet transforms wavelet coefficients image segmentation computational efficiency codecs filtering discrete cosine transforms;optimization;curvelet transforms;sampling strategy;optimal algorithm;rate distortion optimization;encoding;wavelet transforms curvelet transforms distortion image coding image sampling optimisation;continuous wavelet transforms	The curved wavelet transform (CWT) was developed to enhance compactness of the wavelet transform (WT) representation. Curve determination is critical for the CWT because a well-defined curve set can increase the performance gain in terms of the rate-distortion (R-D). Conventionally, the image to be encoded is divided into blocks and the curve orientation in each block is independently determined through the minimization of its high-pass CWT energy. In this paper, we propose an R-D optimization algorithm for the curve determination, in which variable block size and the impact of neighboring blocks are taken into account. To reduce the computational cost, an alternative sampling strategy is exploited. Experiment results with natural images show that the proposed algorithm can provide better image quality, measured objectively or subjectively, compared to the conventional CWT coding algorithm. Importantly, the proposed approach overcomes the hurdles of computational cost and optimization at the global level opens the door for further performance enhancements of applications with the CWT.	algorithm;algorithmic efficiency;block size (cryptography);complex wavelet transform;computation;distortion;image quality;mathematical optimization;sampling (signal processing)	Dong Zheng;Liang Zhang;Demin Wang;André Vincent	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414479	computer vision;mathematical optimization;second-generation wavelet transform;computer science;pattern recognition;mathematics;wavelet transform	Robotics	44.66211218186071	-14.259062277073994	5047
ed552938f42fcd9f18aaffde7e192a8a55c73943	random subspace method for improving performance of credit cardholder classification	baggingcredit card riskmultiple criteria linear programming mclp � pcarandom subspace method rsm	  The main task of credit card risk management for commercial banks and credit card retailers is analyzing credit cardholders’  behavior and predicting bankruptcy. In this paper, we investigate dimension reduction techniques for improving performance  of the high-dimensional credit cardholder data classification. We choose MCLP as base classifier. We theoretically analyze  the characteristics of PCA, filter, wrapper, and RSM. Then, our experimental research focuses on RSM. Due to the specialities  of dimension reduction and ensemble of RSM, we experimentally compare its performance with that of PCA, single classifier  and Bagging on the same training set and test set. The results show that RSM can highly improve classification performance  of the credit cardholder data set. From the idea of ensemble, RSM demonstrates its superiority over Bagging. From the angle  of dimension reduction, RSM shows its predominance over single classifier, and the same advantage as PCA. Finally, we explain  our results from the aspects of MCLP algorithm, RSM and data set.    	random subspace method	Meihong Zhu;Aihua Li	2011		10.1007/978-3-642-18387-4_29	machine learning;pattern recognition;data mining	ML	12.416652403867817	-42.36799064544883	5073
654ab97271b2ca9707f9481390b43b4cea3bf264	traffic patterns and flow characteristics in an ant trail model	modelizacion;ecoulement trafic;invertebrata;swarm intelligence;intelligence en essaim;arthropoda;insecto social;traffic flow;modelisation;aculeata;insecta;automate cellulaire;hymenoptera;insecte social;social insect;formicoidea;modeling;inteligencia de enjambre;flujo trafico;cellular automaton;automata celular	We have constructed a minimal cellular automaton model for simulating traffic on preexisting ant trails. Uni- as well as bidirectional trails are investigated and characteristic patterns in the distribution of workers on the trail are identified. Some of these patterns have already been observed empirically. They give rise to unusual flow characteristics which are also discussed. The question of possible functions of the observed traffic patterns and the resulting flow characteristics will be treated for simplified biological scenarios.		Alexander John;Andreas Schadschneider;Debashish Chowdhury;Katsuhiro Nishinari	2006		10.1007/11839088_27	cellular automaton;simulation;systems modeling;swarm intelligence;computer science;artificial intelligence;traffic flow;algorithm	SE	22.415679823363945	-12.089726008880639	5078
42094ef44e66677b99fd812e99ba676ce7b35ab5	analysing driving patterns of electric taxi based on the location of charging station in urban area	driving pattern from charging behavior;electric taxi;charging station;electric vehicle	As electric vehicles are more adopted extensively, the charging station location and distribution problem is becoming more important. Efficient charging station planning can solve major problems, such as driving-range anxiety and the stagnation of new electric vehicle consumers. For efficient charging station planning, the influence of charging station location should be identified. This paper analyzes the electric taxi's driving patterns with constraint of insufficient charging stations. Comparing the driving patterns between general taxi and electric taxi, specific driving patterns can be revealed due to the location of charging station. Daejeon city in South Korea is selected for the experiment study area to examine the difference of driving patterns. The driving patterns caused by charging behavior are analyzed by real taxi trajectory data. In this study, the measures of driving patterns are analyzed by the sum of trip length in each cell and origin-destination direction pattern with the real data of taxis. In the initial planning phase of installing charging stations, the results of this paper can be efficiently applied to a relatively extensive area to encourage the usage of electric vehicles.		Yongjun Ahn;Chulmin Jun;Hwasoo Yeo	2016	2016 IEEE International Smart Cities Conference (ISC2)	10.1109/ISC2.2016.7580836	simulation;engineering;automotive engineering;transport engineering	Robotics	9.198285239796311	-9.111743843710997	5087
ae87ebbb20bb83d3678cd2f1f341d2e361b4df23	survey of search and optimization of p2p networks	technology development;search algorithm;p2p;resource sharing;p2p networks;optimal algorithm	Being highly dynamic and casualness, P2P nodes can organize a special network by themselves, therefore the resource search in a P2P network is a complex problem. A reasonable and efficient resource search algorithm is not only the key to resource sharing for users, but also a critical stage to P2P technology development. Firstly, this paper introduces some basic resource search algorithms on P2P network architecture. Secondly, the paper discusses the advantages and disadvantages of these methods. Above all, the paper focuses on the combination of the P2P resource search with some intelligent optimization algorithms, which have succeeded in many fields. The combination shows a good prospect, although being still immature now.	mathematical optimization;network architecture;peer-to-peer;search algorithm	Cai Kang	2011	Peer-to-Peer Networking and Applications	10.1007/s12083-010-0082-2	beam search;shared resource;computer science;peer-to-peer;data mining;database;management science;search algorithm	HPC	19.838399307049354	-2.6522141626823217	5102
abbf7fdf190769cdf93d413d3718ff918df340d0	vlsi fuzzy chip and inference accelerator board systems	very large scale integration fuzzy systems computer architecture fuzzy logic inference mechanisms decision making application software read write memory random access memory robots;computerised control;rule based;inference mechanisms;vlsi computerised control fuzzy logic inference mechanisms knowledge based systems microprocessor chips real time systems satellite computers;chip;fuzzy logic;sun 3 workstation vlsi fuzzy chip inference accelerator board systems vlsi fuzzy logic inference processor vmebus board systems fuzzy logic inference mechanism rule based control real time applications cmos ram memory single chip board;software development;vlsi;satellite computers;real time application;knowledge based systems;microprocessor chips;real time systems	Fuzzy logic based control uses a rule-based expert system paradigm in the area of real-time process control. The VLSI implementation of a fuzzy logic inference mechanism allows the use of rule-based control and decision making in demanding real-time applications. The second generation of full custom CMOS VLSI has been designed. The chip consists of 688,000 transistors of which 476,000 are used for RAM memory. A Fuzzy chip has been successfully fabricated and tested. This paper presents VLSI architecture in detail. We have built VME-bus single board systems based on the chip for Oak Ridge National Laboratory (ORNL) and f o r NASA Ames Research Center. The board is now installed an a robot at ORNL. Researchers at ORNL uses this board for experiment in autonomous robot navigation. The Fuzzy Logic system board places the Fuzzy chip into a VMEbus environment t o provide application process control through a VMEbus host.	autonomous robot;cmos;expert system;full custom;fuzzy logic;hardware acceleration;logic programming;motherboard;programming paradigm;random-access memory;real-time clock;robotic mapping;transistor;vmebus;very-large-scale integration	Hiroyuki Watanabe;James R. Symon;Wayne D. Dettloff;Kathy E. Yount	1991		10.1109/ISMVL.1991.130716	fuzzy logic;rule-based system;chip;embedded system;fuzzy electronics;computer architecture;electronic engineering;real-time computing;computer science;artificial intelligence;software development;knowledge-based systems;very-large-scale integration;fuzzy control system	Robotics	40.99914325206189	-24.8400097479037	5139
590abcaa5d85770fb916e74487ac087efb5edf63	embedded image compression using dct based subband decomposition and slcca data organization	image coding discrete cosine transforms computational complexity psnr degradation discrete wavelet transforms compaction entropy coding computational modeling bit rate;image coding;data compression;entropy coding;transform coding;wavelet transform embedded image compression dct slcca fast discrete cosine transform subband decomposition slcca data organization significance linked connected component analysis entropy coding computational complexity psnr peak signal to noise ratio image quality degradation image coding image transmission;discrete cosine transform;wavelet transform;image compression;computational complexity;discrete cosine transforms;computational complexity image coding discrete cosine transforms transform coding data compression entropy codes;entropy codes;peak signal to noise ratio;image quality;space frequency;high performance;hardware implementation	Adstract-Wavelet transform provides harmonic space frequency Localization and great energy compaction, hut with generally high computational complexity. In the paper, an 8x8 fast Discrete Cosine Transform (DCT) approach is adopted to perform subband decomposition, followed by SLCCA data organization and entropy coding. Simulation results showed that the embedded DCT-SLCCA image compression reduced the computational complexity to only a quarter of the wavelet based snbhand decomposition while the average Peak Signal-to-Noise Ratio (PSNR) degraded at the same bit rate by only 0.6 dB. The snbjeftive image quality degradation was almost unnoticeable. Moreover, due to 8x8 fast DCT hardware implementation being commercially available, the proposed DCT-SLCCA has the potential for high performance high speed image coding and transmission.	computational complexity theory;data compaction;discrete cosine transform;elegant degradation;embedded system;entropy encoding;image compression;image quality;peak signal-to-noise ratio;simulation;wavelet transform	Junqiang Lan;Xinhua Zhuang	2002		10.1109/MMSP.2002.1203253	data compression;image quality;computer vision;transform coding;peak signal-to-noise ratio;image compression;computer science;entropy encoding;theoretical computer science;discrete cosine transform;mathematics;computational complexity theory;algorithm;wavelet transform	EDA	44.15716911948155	-14.26366936618788	5144
81d015d81a5c9e9499be3568149415e111426775	systematic bias in ocr experiments	sample size;optical character recognition;large scale	In this paper, we present the results from large-scale OCR experiments simulating several diierent groups of researchers performing the same test, but using slightly diierent equipment and procedures. We analyze the variance inherent in the sampled accuracy for each experiment, and the biases that exist in the experiments. We rst show that minor systematic (i.e., equipment and procedural) diierences between experiments can result in signiicant biases in the computed accuracy. Thus, factors that introduce measurable variance and/or bias into OCR accuracy rates could be identiied to insure that performance evaluations are statistically sig-niicant. Then we show that a relatively small number of pages is suucient to obtain a precise estimate of OCR accuracy, thereby eliminating sampling error as a source of deviation from one experiment to another.	experiment;procedural programming;sampling (signal processing);signature block;simulation	Daniel P. Lopresti;Andrew Tomkins;Jiangying Zhou	1995		10.1117/12.205822	sample size determination;computer science;data mining;optical character recognition	NLP	30.867002108574752	-17.751527411322364	5148
7d75d8c359c68c48ee498c9a1db048137746bd5d	impact of backorder on supply chain performance - an experimental study		Increase in order variance from downstream stage to upstream stage in a supply chain is called bullwhip effect. Bullwhip effect is one of the performance measures of a supply chain. In the present study the impact of backorder in supply chain performance is studied under small lead time of one period by conducting experiments under lost sales and backorder settings using supply chain role play software package. The performance of supply chain under both settings is compared using Wilcoxon statistical test. The results show that the variance of orders is high under backorder than the lost sales setting. The study concludes that backorder is also one of the causes of bullwhip effect.		V. Madhusudanan Pillai;Chinna Pamulety Talari	2013		10.3182/20130619-3-RU-3018.00477	economics;marketing;bullwhip effect;operations management;commerce	DB	1.4760065112622291	-9.706788489774617	5152
0d0149578bbc896dbb96173f34d8de79ef00735f	evolving neural networks using ant colony optimization with pheromone trail limits	ant colony optimisation;evolutionary computation;neural nets;conference;training artificial neural networks optimization cancer diabetes heart testing;neural nets ant colony optimisation backpropagation evolutionary computation;backpropagation;stagnation behaviour avoidance evolving neural networks ant colony optimization pheromone trail limits hybrid training evolutionary algorithms aco algorithms back propagation technique bp technique;institutional repository research archive oaister	The back-propagation (BP) technique is a widely used technique to train artificial neural networks (ANNs). However, BP often gets trapped in a local optimum. Hence, hybrid training was introduced, e.g., a global optimization algorithm with BP, to address this drawback. The key idea of hybrid training is to use global optimization algorithms to provide BP with good initial connection weights. In hybrid training, evolutionary algorithms are widely used, whereas ant colony optimization (ACO) algorithms are rarely used, as the global optimization algorithms. And so far, only the basic ACO algorithm has been used to evolve the connection weights of ANNs. In this paper, we hybridize one of the best performing variations of ACO with BP. The difference of the improved ACO variation from the basic ACO algorithm lies in that pheromone trail limits are imposed to avoid stagnation behaviour. The experimental results show that the proposed training method outperforms other peer training methods.	ant colony optimization algorithms;artificial neural network;backpropagation;benchmark (computing);evolutionary algorithm;global optimization;gradient descent;levenberg–marquardt algorithm;local optimum;local search (optimization);mathematical optimization;software propagation;teaching method	Michalis Mavrovouniotis;Shengxiang Yang	2013	2013 13th UK Workshop on Computational Intelligence (UKCI)	10.1109/UKCI.2013.6651282	engineering;artificial intelligence;machine learning;operations research;metaheuristic	AI	28.313478405627198	-5.468716534478268	5156
07a66b74bccf97f0abe43dd8442e9617488001f2	exploiting the ensemble paradigm for stable feature selection: a case study on high-dimensional genomic data	selection stability;ensemble paradigm;high dimensional genomic data;feature selection;data perturbation	We discuss the rationale of ensemble feature selection.We empirically evaluate the effectiveness of a data perturbation ensemble approach.Our study involves both univariate and multivariate selection algorithms.A special emphasis is given to the stability level of the selected feature subsets.Useful insight is gained from the analysis of high-dimensional genomic datasets. Ensemble classification is a well-established approach that involves fusing the decisions of multiple predictive models. A similar ensemble logic has been recently applied to challenging feature selection tasks aimed at identifying the most informative variables (or features) for a given domain of interest. In this work, we discuss the rationale of ensemble feature selection and evaluate the effects and the implications of a specific ensemble approach, namely the data perturbation strategy. Basically, it consists in combining multiple selectors that exploit the same core algorithm but are trained on different perturbed versions of the original data. The real potential of this approach, still object of debate in the feature selection literature, is here investigated in conjunction with different kinds of core selection algorithms (both univariate and multivariate). In particular, we evaluate the extent to which the ensemble implementation improves the overall performance of the selection process, in terms of predictive accuracy and stability (i.e., robustness with respect to changes in the training data). Furthermore, we measure the impact of the ensemble approach on the final selection outcome, i.e. on the composition of the selected feature subsets. The results obtained on ten public genomic benchmarks provide useful insight on both the benefits and the limitations of such ensemble approach, paving the way to the exploration of new and wider ensemble schemes.	feature selection;programming paradigm	Barbara Pes;Nicoletta Dessì;Marta Angioni	2017	Information Fusion	10.1016/j.inffus.2016.10.001	computer science;machine learning;pattern recognition;data mining;ensemble learning;feature selection;statistics	ML	6.094475776912064	-43.86644541389745	5157
e6103dba56b7e0f60aa5816a20adf593d4b0850a	multiple attributes group decision making under fuzzy environment	fuzzy number;alternating projections;fuzzy set theory;distributed decision making;decision making aggregates environmental management mathematics information science frequency;group decision making;fuzzy set theory distributed decision making;group consensus opinions group decision making positive fuzzy numbers expectation difference fuzzy individual opinions autarchy decision making fuzzy vector projection multiple attributes decision making	Under the circumstance that opinions of members or group decision making are assumed as positive fuzzy numbers, the authors propose a new approach to aggregate the experts' opinions based on a measure near degree and an absolute value of expectation difference of fuzzy numbers. When individual fuzzy opinions are aggregated into group consensus, the group decision making turns to an autarchy decision making. Two vectors which are made up of expectations ponderance of two fuzzy vectors form an angle as the angle of the fuzzy vectors, a conception of fuzzy vector projection is brought forward in this paper. The sizes of project coefficient, which the fuzzy attribute vector of each alternative projects on the fuzzy weighted vector, are a criterion to judge each alternative superior or inferior.		Ji-bin Lan;Jia-Zhong Liu	2003		10.1109/ICSMC.2003.1245773	fuzzy logic;group decision-making;fuzzy cognitive map;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;linear partial information;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;weighted sum model	AI	-3.140105002453248	-19.974600811588942	5163
201463b5a59fcf1691e9d2119bfeaeac2e8de37e	ordering heuristics for reliability evaluation of multistate networks	two terminal networks minimal cut vectors minimal path vectors multistate reliability ordering heuristics;reliability graph theory network theory graphs;reliability;reliability indexes periodic structures scholarships mechanical engineering production transportation;network reliability reliability evaluation ordering heuristics multistate two terminal networks d mp minimal path vectors o1 heuristics o2 heuristics o3 heuristics o4 heuristics recursive sum of disjoint product method rsdp method;mechanical engineering;indexes;periodic structures;scholarships;transportation;production	This paper develops ordering heuristics to improve the efficiency of reliability evaluation for multistate two-terminal networks given all minimal path vectors ( d-MPs for short). In the existing methods, all d-MPs are treated equally. However, we find that the importance of each d-MP is different, and different orderings affect the efficiency of reliability evaluation. Based on the above observations, we introduce the length definitions for d-MPs in a multistate two-terminal network, and develop four ordering heuristics, called O1, O2, O3, and O4, to improve the efficiency of the Recursive Sum of Disjoint Products (RSDP) method for evaluating network reliability. The results show that the proposed ordering heuristics can significantly improve the reliability evaluation efficiency, and O1 performs the best among the four methods. In addition, an ordering heuristic is developed for the reliability evaluation of multistate two-terminal networks given all minimal cut vectors ( d-MCs).	heuristic (computer science);max-flow min-cut theorem;recursion (computer science)	Guanghan Bai;Ming Jian Zuo;Zhigang Tian	2015	IEEE Transactions on Reliability	10.1109/TR.2015.2430491	reliability engineering;database index;transport;mathematical optimization;computer science;reliability;mathematics;algorithm;statistics	DB	7.95632934497306	0.04170630906272756	5164
0713abc40f3b7ab8aff3b04893090ad2ea65a68f	a new approach for training lagrangian twin support vector machine via unconstrained convex minimization	generalized hessian approach;smooth approximation formulation;twin support vector machine	In this paper, a novel unconstrained convex minimization problem formulation for the Lagrangian dual of the recently introduced twin support vector machine (TWSVM) in simpler form is proposed for constructing binary classifiers. Since the objective functions of the modified minimization problems contain non-smooth ‘plus’ function, we solve them by Newton iterative method either by considering their generalized Hessian matrices or replacing the ‘plus’ function by a smooth approximation function. Numerical experiments were performed on a number of interesting real-world benchmark data sets. Computational results clearly illustrates the effectiveness and the applicability of the proposed approach as comparable or better generalization performance with faster learning speed is obtained in comparison with SVM, least squares TWSVM (LS-TWSVM) and TWSVM.	approximation;benchmark (computing);computation;convex optimization;experiment;hessian;iterative method;lagrange multiplier;least squares;matlab;newton;numerical analysis;semiconductor industry;support vector machine;whole earth 'lectronic link	S. Balasundaram;Deepak Gupta;Subhash Chandra Prasad	2016	Applied Intelligence	10.1007/s10489-016-0809-8	mathematical optimization;machine learning	ML	22.05240167534578	-38.3650050482753	5170
dc9fc529e1c0d8adc7bb67436cdd99d30b307fb6	mining rules of multi-level diagnostic procedure from databases	base donnee;aplicacion medical;hierarchized structure;diagnostico;database;base dato;structure hierarchisee;intelligence artificielle;data mining;fouille donnee;concept hierarchy;artificial intelligence;differential diagnosis;decision process;medical application;inteligencia artificial;diagnosis;busca dato;estructura jerarquizada;medical diagnosis;application medicale;diagnostic	One of the most important features of expert reasoning is that each reasoning rule may be composed of several diagnostic steps, usually hierarchical differential diagnosis. For example, medical diagnosis include hierarchical diagnostic steps In this paper, the characteristics of experts' rules are closely examined from the viewpoint of hiearchical decision steps and a new approach to extract plausible rules is introduced, which consists of the following three procedures. First, the characterization of decision attributes (given classes) is extracted from databases and the concept hierarchy for given classes is calculated. Second, based on the hierarchy, rules for each hierarchical level are induced from data. Then, for each given class, rules for all the hierarchical levels are integrated into one rule. The proposed method was evaluated on medical databases, the experimental results of which show that induced rules correctly represent experts' decision processes.	database	Shusaku Tsumoto	2003		10.1007/978-3-540-39804-2_41	computer science;artificial intelligence;machine learning;medical diagnosis;data mining;database	DB	-0.3264809983821954	-30.743796927304942	5180
894d5ecf9841514c6a640cb8e797812ce8158d87	a new approach against color attacks of watermarked images	watermarking;interpolation;stirmark color attacks watermarked images interpolation watermarking algorithm lsb;approximation algorithms;copyright;computer hacking;lsb;watermarked images;image color analysis;image quality;attacks;stirmark watermarking interpolation attacks image quality copyright authenticity;signal processing algorithms;watermarking interpolation;interpolation watermarking algorithm;watermarking image color analysis interpolation signal processing algorithms conferences approximation algorithms computer hacking;color attacks;conferences;authenticity;stirmark	We present a new approach for modeling color attacks of RGB-color watermarked images. We have used a based interpolation watermarking algorithm and supposed that the attacks are simulated by a scaling of the colors followed by a translation. We give bounds for the extracted watermark depending on the original image, the watermark and the attack. Different attacks like LSB, embedding another watermark, Stirmark have been simulated and the quality of the extracted watermark is discussed in each case.	algorithm;color;digital watermarking;image scaling;interpolation;least significant bit;watermark (data file)	Abdelhamid Benhocine;Lamri Laouamer;Laurent Nana;Anca Pascu	2008	2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2008.134	image quality;least significant bit;computer vision;digital watermarking;interpolation;computer science;theoretical computer science;mathematics;internet privacy;watermark;approximation algorithm;statistics	Robotics	40.77694137927289	-11.360872154530218	5189
8bacb1347921d7885d51a988f615f75af5414458	ant colony system with extremal dynamics for point matching and pose estimation	optimisation;image matching image registration optimisation combinatorial mathematics computer vision search problems;image matching;search strategy;image registration ant colony optimization robustness computer vision optimization methods feature extraction laboratories computer science layout neural networks;computer vision;ant colony system;affine transformation;image registration;extremal optimization;global optimization;search problems;extremal dynamics point based image registration method point matching outlying data noisy data combinatorial optimization task global optimization method pose estimation ant colony system population based search strategy affine transformations local search extremal optimization;combinatorial optimization;combinatorial mathematics;local search;pose estimation	For a point-based image registration method, point matching is a hard and a computationally intensive task to handle especially when issues of noisy and outlying data have to be considered. In this paper we cast the problem as a combinatorial optimization task and we describe a global optimization method to achieve robust point matching and pose estimation for image registration purpose. The basic idea is to use Ant Colony System (ACS) as a population based search strategy to evolve promising starting solutions i.e affine transformations. An appropriate local search inspired from extremal optimization is developed and embedded within the search strategy to refine the solutions found. Experimental results are very promising and show the ability of the method to cope with outliers and to achieve robust pose estimation.	3d pose estimation;ant colony;combinatorial optimization;embedded system;extremal optimization;global optimization;heuristic;image noise;image registration;local search (optimization);mathematical optimization	Souham Meshoul;Mohamed Batouche	2002		10.1109/ICPR.2002.1048148	extremal optimization;computer vision;mathematical optimization;pose;3d pose estimation;combinatorial optimization;computer science;image registration;local search;machine learning;affine transformation;mathematics;global optimization	Vision	49.75538293811638	-50.82515624047937	5191
24e0fc7c4e6ea548924f315cd6708e70a34035dc	video object detection speedup using staggered sampling	image sampling;video object;video object detection methods video object detection speedup staggered sampling standard sampling strategy filter based object detection object tracking video streams sampling density video frames filter based object detector tracking algorithm combination face detector opencv library video sequences qcif collection full dense sampling image locations;video streaming;software libraries;video signal processing;object detection sampling methods detectors image sampling streaming media face detection testing libraries video sequences robustness;face recognition;object tracking;sampling methods;sampling strategy;object detection;video streaming face recognition image sampling image sequences object detection sampling methods software libraries video signal processing;image sequences	This paper presents an enhancement of the standard sampling strategy for filter-based object detection and tracking in video streams. The proposed method, called staggered sampling, seeks to maximize the sampling density across video frames, thus reducing the number of patches sampled while retaining proportionally high recall rates. The method can be tailored to virtually any constraint on resources and may be used in conjunction with any filter-based object detector / tracking algorithm combination. We test our method using a modified version of the face detector in the OpenCV library and a simple tracking algorithm. The resulting detector was applied to some video sequences from the QCIF collection. Our results show that staggered sampling can achieve around 90% of the recall of full (dense) sampling while only evaluating the detector on around 10% of the image locations. At the same time the precision of the detector increases. The staggered sampling approach therefore addresses the problem of acquiring new objects in an object tracking framework by enabling a low-cost background scan of the video stream to run continuously. The simplicity and robustness of this approach make it an excellent enhancement to existing video object detection methods.	algorithm;ct scan;display resolution;object detection;opencv;overhead (computing);pin grid array;sampling (signal processing);sensor;speedup;streaming media	Darryl Greig	2009	2009 Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2009.5403129	facial recognition system;sampling;computer vision;real-time computing;object-class detection;computer science;viola–jones object detection framework;video tracking;video denoising;computer graphics (images)	Vision	40.44614087684049	-50.992352683975014	5197
4c206e4439aef8991860e981131da41716e208cd	higher-order and average reward myopic-affine dynamic models	modelo dinamico;dynamic programming;transformation affine;programacion dinamica;juego secuencial;myopic;recompense;dynamic model;aquaculture;sequential game;higher order;acuacultura;reward;aquiculture;mathematical programming;affine transformation;affine;modele dynamique;average reward;programmation dynamique;ejemplo;example;programmation mathematique;programacion matematica;transformacion afin;jeu sequentiel;exemple	The dynamics and rewards in a higher-order model depend directly on states and actions in earlier periods. Here, the expected values of the single-period reward and of the state in the next period are affine functions of the states in the current and earlier periods. It is shown that computations in higher-order infinite-horizon models with myopic solutions are not essentially harder than in the corresponding first-order models. These results are obtained for discounted and average-reward criteria, and similar conclusions are indicated for sequential games. The results are applied to an aquaculture harvesting model and an advertising model with delayed responses.		Matthew J. Sobel	1990	Math. Oper. Res.	10.1287/moor.15.2.299	mathematical optimization;simulation;artificial intelligence;affine transformation;mathematics;mathematical economics	Theory	3.870752528850121	-2.681146562270516	5199
7311d1bfc0fe98fcca8b9d717819763acde1018a	deep collaborative autoencoder for recommender systems: a unified framework for explicit and implicit feedback		In recent years, deep neural networks have yielded state-of-the-art performance on several tasks. Although some recent works have focused on combining deep learning with recommendation, we highlight three issues of existing works. First, most works perform deep content feature learning and resort to matrix factorization, which cannot effectively model the highly complex user-item interaction function. Second, due to the difficulty on training deep neural networks, existing models utilize a shallow architecture, and thus limit the expressiveness potential of deep learning. Third, neural network models are easy to overfit on the implicit setting, because negative interactions are not taken into account. To tackle these issues, we present a novel recommender framework called Deep Collaborative Autoencoder (DCAE) for both explicit feedback and implicit feedback, which can effectively capture the relationship between interactions via its non-linear expressiveness. To optimize the deep architecture of DCAE, we develop a three-stage pretraining mechanism that combines supervised and unsupervised feature learning. Moreover, we propose a popularity-based error reweighting module and a sparsity-aware data-augmentation strategy for DCAE to prevent overfitting on the implicit setting. Extensive experiments on three real-world datasets demonstrate that DCAE can significantly advance the state-of-the-art.	artificial neural network;autoencoder;cold start;deep learning;experiment;feature learning;feedback;interaction;nonlinear system;overfitting;recommender system;sparse matrix;user (computing);verification and validation	Qibing Li;Xiaolin Zheng	2017	CoRR		recommender system;overfitting;architecture;artificial neural network;machine learning;autoencoder;deep learning;mathematics;artificial intelligence;matrix decomposition;feature learning	AI	22.003283472084618	-45.76100520287211	5208
b409b2a40367c86baa73e04ffa009b872878f799	detection of image alternation and image id by two digital watermark methods with error correction.	digital watermark;error correction			Masato Nishio;Yutaka Ando;Nobuhiro Tsukamoto;Hironao Kawashima;Shinya Nakamura	2003			computer vision;error detection and correction;digital watermarking;computer science;theoretical computer science;computer security	Vision	42.1837281877169	-11.915741314425391	5217
243d08d5eb45fd2b5fda924487950d787b8e1dc5	elitism set based particle swarm optimization and its application			mathematical optimization;particle swarm optimization	Yanxia Sun;Zenghui Wang	2017	Int. J. Comput. Intell. Syst.	10.2991/ijcis.10.1.92	artificial intelligence;multi-swarm optimization;mathematical optimization;machine learning;mathematics;metaheuristic;particle swarm optimization;elitism	EDA	25.172430878574374	-4.849475933351587	5223
5e6035535d6d258a29598faf409b57a71ec28f21	an efficient centroid type-reduction strategy for general type-2 fuzzy logic system	karnik mendel algorithm;weighted averaging;general type 2 fuzzy logic system;α plane;computational complexity;type 2 fuzzy set;fuzzy logic system;fuzzy weighted average	In this paper, an efficient centroid type-reduction strategy for general type-2 fuzzy set is introduced. This strategy makes use of the result of @a-plane representation, and performs the centroid type-reduction on each @a-plane. Simulations show that it usually needs only several resolution of @a value such that the defuzzified value converges to a real value. Consequently, comparing with the exhaustive computation approach, this approach can tremendously decrease the computation complexity from exponential into linear.	fuzzy logic;reduction strategy (lambda calculus)	Feilong Liu	2008	Inf. Sci.	10.1016/j.ins.2007.11.014	fuzzy logic;mathematical optimization;discrete mathematics;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;computational complexity theory;fuzzy set operations;algorithm;fuzzy control system	AI	0.20725473927436444	-24.665821536147575	5227
203ea8ab1d9c48977be97e6caf3fdbcc84101354	video segmentation by tracking many figure-ground segments	unsupervised learning;tracking segments;image segmentation;video sequences unsupervised video segmentation approach multiple holistic figure ground segment tracking online nonlocal appearance models multioutput regularized least squares formulation composite statistical inference approach high order statistic estimates appearance model temporal consistency segtrack v2;proceedings;inference mechanisms;csi video segmentation cpmc tracking segments appearance model composite statistical inference;unsupervised learning higher order statistics image segmentation image sequences inference mechanisms;video segmentation;image segmentation motion segmentation training predictive models target tracking proposals;higher order statistics;composite statistical inference;cpmc;csi;appearance model;image sequences	We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figure-ground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing high-order statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, showing its efficiency and robustness to challenges in different video sequences.	algorithm;computation;experiment;holism;ibm notes;least squares;pixel;unsupervised learning	Fuxin Li;Taeyoung Kim;Ahmad Humayun;David Tsai;James M. Rehg	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.273	unsupervised learning;computer vision;active appearance model;computer science;machine learning;pattern recognition;image segmentation	Vision	47.14225463646843	-50.856168663242286	5230
443d9775ede31bf05d0b950b6bc131b65c020f87	experimental evaluation of three partition selection criteria for decision table decomposition	iterative algorithm;machine learning;concept hierarchy;experimental evaluation;selection criteria;decision table;exhaustive search;knowledge discovery	Decision table decomposition is a machine learning approach that decomposes a given decision table into an equivalent hierarchy of decision tables. The approach aims to discover decision tables that are overall less complex than the initial one, potentially easier to interpret, and introduce new and meaningful intermediate concepts. Since an exhaustive search for an optimal hierarchy of decision tables is prohibitively complex, the decomposition uses a suboptimal iterative algorithm that requires the so-called partition selection criterion to decide among possible candidates for decomposition. This article introduces two such criteria and experimentally compares their performance with a criterion originally used for the decomposition of Boolean functions. The experiments highlight the differences between the criteria, but also show that in all three cases the decomposition may discover meaningful intermediate concepts and relatively compact decision tables.	algorithm;brute-force search;decision table;experiment;iterative method;machine learning	Blaz Zupan;Marko Bohanec	1998	Informatica (Slovenia)		decision table;computer science;machine learning;pattern recognition;brute-force search;data mining;iterative method;knowledge extraction	Web+IR	-1.7058685398309816	-30.513554939397658	5232
17e5f5e479e7fef2a03dc0dfbd13a9c5db7048cf	synchronizing video cameras with non-overlapping fields of view	robot sensing systems;non overlapping fields of view video synchronization;robust estimator;video sequences;video cameras image sequences synchronisation;synchronisation;trajectory;video cameras;video synchronization;single line robust estimation;three dimensional displays;synchronization;field of view;cameras three dimensional displays video sequences trajectory robot sensing systems synchronization tracking;nonoverlapping fields;non overlapping fields of view;cameras;tracking;coordinate system;image sequences;synchronisation nonoverlapping fields video cameras video sequences single line robust estimation	This paper describes a method to estimate the temporal alignment between N unsynchronized video sequences captured by cameras with non-overlapping fields of view. The sequences are recorded by stationary video cameras, with fixed intrinsic and extrinsic parameters. The proposed approach reduces the problem of synchronizing N non-overlapping sequences to the robust estimation of a single line in RN+1. This line captures all temporal relations between the sequences and a moving sensor in the scene, whose locations in the world coordinate system may be estimated at a constant sampling rate. Experimental results with real-world sequences show that our method can accurately align the videos.	align (company);camera resectioning;line level;sampling (signal processing);stationary process	Darlan N. Brito;Flávio L. C. Pádua;Rodrigo L. Carceroni;Guilherme A. S. Pereira	2008	2008 XXI Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2008.28	computer vision;real-time computing;computer science;computer graphics (images)	Vision	49.057707942839	-45.47764442247751	5248
79f6e0cabc7587cef5d04bc2b3351e338350bccd	brkga algorithm for the capacitated arc routing problem	metaheuristics;vehicle routing;carp;brkga	We propose a new algorithm for the Capacitated Arc Routing Problem (CARP). Our motivation to deal with this problem is related to its application in several real world scenarios such as street sweeping, urban waste collection and electric meter reading just to mention a few. Based on BRKGA metaheuristic, our algorithm introduces a new random key encoding for CARP, mutation to random keys strings, a restart phase to avoid stagnation and local search . The algorithm was tested with several well-known instances from the literature. The results obtained were competitive in terms of objective function value and required computational time.	algorithm;arc routing;central processing unit;computation;crossover (genetic algorithm);feasible region;fitness function;heuristic (computer science);local search (optimization);loss function;mathematical optimization;metaheuristic;optimization problem;time complexity;verlet list	C. Martinez;Irene Loiseau;Mauricio G. C. Resende;S. Rodriguez	2011	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2011.11.026	mathematical optimization;simulation;computer science;vehicle routing problem;common address redundancy protocol;metaheuristic	ECom	20.282884364359447	1.8208213880863133	5250
72af1f1e806df4d4d021a35a70909253f2f37ea8	application of sstt control algorithm for underactuated: surface vessel, ppr manipulator and pneumatically actuated slider-inverted pendulum	pneumatic actuator;stabilization;trajectory tracking;sliding mode control;underactuated mechanical systems	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;francis;inverted pendulum;portland pattern repository;primary source;underactuation	Tihomir Zilic;Josip Kasac;Zeljko Situm;M. Essert	2014	Advanced Robotics	10.1080/01691864.2014.881265	control engineering;simulation;sliding mode control;pneumatic actuator;engineering;control theory	Robotics	50.88670471818809	-3.4591780567832586	5277
629deae201224acc4bf60e0ff4dddb63f7a25d1c	gray-box optimization using the parameter-less population pyramid	parameter less;global search;local search	Unlike black-box optimization problems, gray-box optimization problems have known, limited, non-linear relationships between variables. Though more restrictive, gray-box problems include many real-world applications in network security, computational biology, VLSI design, and statistical physics. Leveraging these restrictions, the Hamming-Ball Hill Climber (HBHC) can efficiently find high quality local optima. We show how 1) a simple memetic algorithm in conjunction with HBHC can find global optima for some gray-box problems and 2) a gray-box version of the Parameter-less Population Pyramid (P3), utilizing both the HBHC and the known information about variable relationships, outperforms all of the examined algorithms. While HBHC's inclusion into P3 adds a parameter, we show experimentally it can be fixed to 1 without adversely effecting search.  We provide experimental evidence on NKq-Landscapes and Ising Spin Glasses that Gray-Box P3 is effective at finding the global optima even for problems with thousands of variables. This capability is complemented by its efficiency, with running time and memory usage decreased by up to a linear factor from Black-Box P3. On NKq this results in a 375x speedup for problems with at least 1,000 variables.	ball project;black box;climber (beam);computational biology;display resolution;experiment;ising model;linear function;local optimum;mathematical optimization;memetic algorithm;network security;nonlinear system;speedup;time complexity;window function	Brian W. Goldman;William F. Punch	2015		10.1145/2739480.2754775	mathematical optimization;simulation;computer science;artificial intelligence;local search;machine learning;mathematics	ML	27.801449272782005	-0.5328287589543351	5284
89a43276c660d8a8b8298522e33e972bdb00822a	inference with seperately specified sets of probabilities in credal networks		We present new algorithms for inference in credal networksdirected acyclic graphs asso­ ciated with sets of probabilities. Credal networks are here interpreted as encoding strong indepen­ dence relations among variables. We first present a theory of credal networks based on separately specified sets of probabilities. We also show that inference with polytrees is NP-hard in this set­ ting. We then introduce new techniques that re­ duce the computational effort demanded by infer­ ence, particularly in polytrees, by exploring sep­ arability of credal sets.	algorithm;directed acyclic graph;np-hardness;theory	José Carlos Ferreira da Rocha;Fábio Gagliardi Cozman	2002				NLP	6.41209354277044	-11.532012855494282	5286
af696289e9df29091b9032ffc7e933c9f91abda0	demo paper: face detection using multiband camera system	face detection multiband camera system near infrared;cameras face detection image color analysis color face lighting video sequences;belief networks;image colour analysis belief networks cameras face recognition;face recognition;near infrared images face detection multiband camera system image quality illumination conditions image acquisition near infrared bands three color bands color image extracted bayer filter;image colour analysis;cameras	Various applications using a camera system have been developed and deployed commercially to improve our daily life. The performance of camera system are mainly dependent on image quality and illumination conditions. Multiband camera has been developed to provide a wealth of information for image acquisition. In this paper, we developed an application using a multiband camera, which is available in four bands consisting of a near infrared and three color bands. We presented a face detection using multiband camera system to utilize two different images i.e. color image extracted from Bayer filter and near infrared images. The experimental results showed the effectiveness of the proposed system.	bayer filter;color image;face detection;image quality;virtual camera system	Yousun Kang;Duk Shin	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618218	facial recognition system;computer vision;camera auto-calibration;camera resectioning;image resolution;computer science;three-ccd camera;computer graphics (images)	Robotics	44.22941067380438	-43.44786289381382	5287
ee195c1693b809d733c35571befceee9a232618d	a quay crane assignment approach for berth allocation problem in container terminal	bin packing problem;assignment problem;integer linear programming;bin packing;operational software package quay crane assignment berth allocation problem container terminal seaside operations 2d bin packing problem one dimension resource constraint integer linear programming model vessel time space layout;resource constraint;berth allocation problem;quay crane allocation;seaside operations;integer linear programming quay crane assignment quay crane allocation berth allocation problem;resource management;layout;production engineering computing;quay crane assignment;computational modeling;container terminal;scheduling bin packing cranes linear programming production engineering computing;vessel time space layout;one dimension resource constraint;scheduling;software package;linear programming;cranes;operational software package;planning;cranes containers integer linear programming software packages information technology information security informatics logistics rails time factors;tin;integer linear program;containers;2d bin packing problem;integer linear programming model	Berth allocation and quay crane assignment are two important problems in seaside operations. They are commonly integrated to support the plan. The first generates the berthing time, position and required quay crane. It can be taken as a 2D bin-packing problem with one-dimension resource constraint by quay cranes and variable length of operation time determined by quay crane numbers. The second plans the assignment of quay cranes to vessels whose layout is generated in the former problem. The interface is not clear in the literature. This paper builds an integer linear programming model of the quay crane assignment problem with the inputs: vessel time-space layout and quay crane requirements. The optimal assignment solution with minimal shifts of quay cranes is obtained with promising performance by commercial operational software package. It can be a part of the integral planning for berth allocation with quay crane assignment problem.	assignment problem;berth allocation problem;bin packing problem;integer programming;linear programming;operation time;programming model;requirement;seaside;set packing	Zhi-Hua Hu	2010	2010 Third International Symposium on Intelligent Information Technology and Security Informatics	10.1109/IITSI.2010.97	mathematical optimization;engineering;operations management;operations research	Arch	13.570007571072288	2.522320835038708	5302
8afe290603da8f4ba60c4a6957d1e88708de015a	the distribution of congestion on a class of stochastic kinematic wave models	pedestrian safety;poison control;injury prevention;stochastic traffic flow;safety literature;traffic safety;injury control;kinematics;home safety;injury research;kinematic wave model;safety abstracts;computer models;human factors;stochastic processes;traffic congestion;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;falls;waves;ergonomics;suicide prevention	This paper shows that a wide range of stochastic extensions of the kinematic wave model tend to the same parameter-free expression for the probability of congestion at a given time-space point. This is shown for white noise initial density with deterministic and stochastic fundamental diagram in the case of Riemann problems and the bottleneck problem. It is also found that the stochastic solution (i) preserves the structure of the deterministic solution, and (ii) tends to the deterministic solution with time at a given location.	diagram;network congestion;white noise	Jorge A. Laval;Bhargava R. Chilukuri	2014	Transportation Science	10.1287/trsc.2013.0462	waves;kinematics;simulation;engineering;suicide prevention;human factors and ergonomics;injury prevention;stochastic optimization;forensic engineering;computer security	Theory	10.162046556531763	-10.22993926269612	5330
85a28c8124165cee9343c4c560424f8b18ba2709	preliminary steps towards efficient classification in large medical datasets: structure optimization for deep learning networks through parallelized differential evolution		Deep Neural Networks are being more and more widely used to perform several tasks over highly-sized datasets, one of them being classification. Finding good configurations for Deep Neural Network structures is a very important problem in general, and particularly in the medical domain. Currently, either trial-and-error methodologies or sampling-based ones are considered. This paper describes some preliminary steps towards effectively facing this task. The first step consists in the use of Differential Evolution, a kind of an Evolutionary Algorithm. The second lies in using a parallelized version in order to reduce the turnaround time. The preliminary results obtained here show that this approach can be useful in easily obtaining structures that allow increases in the network accuracy with respect to those provided by humans.	big data;charles m. falco;data structure;de bruijn graph;deep learning;differential evolution;distributed computing;evolutionary algorithm;experiment;mathematical model;neural network software;parallel computing;reduction (complexity);sampling (signal processing);the times	Ivanoe De Falco;Giuseppe De Pietro;Antonio Della Cioppa;Giovanna Sannino;Umberto Scafuri;Ernesto Tarantino	2018		10.5220/0006730006330640	machine learning;computer science;differential evolution;deep learning;artificial intelligence	ML	26.01042613202653	-0.0243441805827465	5332
2937f3cfd5600a6e054736be8487e85ca49a79a8	deep compression of sum-product networks on tensor networks		Sum-product networks (SPNs) represent an emerging class of neural networks with clear probabilistic semantics and superior inference speed over graphical models. This work reveals a strikingly intimate connection between SPNs and tensor networks, thus leading to a highly efficient representation that we call tensor SPNs (tSPNs). For the first time, through mapping an SPN onto a tSPN and employing novel optimization techniques, we demonstrate remarkable parameter compression with negligible loss in accuracy.	artificial neural network;experiment;graphical model;mathematical optimization;network architecture;numerical analysis;probabilistic semantics;substitution-permutation network;tree structure;turing completeness	Ching Yun Ko;Cong Chen;Yuke Zhang;Kim Batselier;Ngai Wong	2018	CoRR			ML	21.548556625777604	-47.86362366256638	5342
5db4bb6b572dbd4946364b2e45d394e7b9de49a0	color space compatible coding framework for yuv422 video coding	evaluation performance;syntax;transcodage;performance evaluation;transcodificacion;video signal processing;color space;evaluacion prestacion;luminance;compresion senal;compatibilidad;visual quality enhancement yuv420 bitstream yuv422 video coding color space compatibility chrominance components luminance component;syntaxe;qualite image;compression signal;video coding;image enhancement;codage video;image colour analysis;image quality;signal compression;video coding code standards transcoding mpeg 4 standard space technology computers asia streaming media consumer electronics video equipment;compatibility;traitement signal video;compatibilite;calidad imagen;espace chromatique;espacio cromatico;sintaxis;transcoding;image enhancement video coding image colour analysis;luminancia	A new video coding framework for YUV422 video sources is proposed in this paper. It features color space compatibility to the more popular YUV420 syntax. Specifically, the chrominance components are separately into two parts and coded differently. The first part, together with the luminance component, conforms to the YUV420 layout and is coded the same way as a normal YUV420 video to produce a YUV420-compatible base bit stream. The second part, i.e., the remaining chrominance components, is coded to generate an enhancement chrominance bitstream for improving the chrominance quality. This is in sharp contrast to the YUV422 coding method of MPEG-2/4 standards where all the chrominance are coded together and in the same way. Consequently, the resulting YUV422 bitstream can be easily converted to a YUV420 bitstream by simple truncation instead of undergoing an expensive transcoding process. New coding modes are also introduced for more efficient coding of the enhancing chrominance components. Performance-wise, the new framework also outperforms existing methods thanks to the new coding modes introduced.	bitstream;color space;data compression;truncation	Lujun Yuan;Guobin Shen;Feng Wu;Shipeng Li;Wen Gao	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326512	scalable video coding;image quality;computer vision;transcoding;syntax;computer science;chrominance;coding tree unit;luminance;multimedia;color space;compatibility;h.262/mpeg-2 part 2;computer graphics (images)	Robotics	45.48584094923866	-14.817522318843913	5346
e161fe97f333677ffa7b9febc73468d31bb19ac3	bootstrap methods		• bootstrap is a general tool for assessing statistical accuracy. basic idea is to randomly draw datasets with replacement from the training data, each sample the same size as the original training set. • This is done B times (B = 100 say), producing B bootstrap datasets, as shown in the next slide. Then we refit the model to each of the bootstrap datasets, and examine the behavior of the fits over the B replications.		Michael R. Chernick;Wenceslao González-Manteiga;Rosa M. Crujeiras;Erniel B. Barrios	2011		10.1007/978-3-642-04898-2_150		Vision	14.832543400930803	-40.93692545806884	5355
0c1733a308b265ea310116cba78956bd6cd71b70	dynamic updating and downdating matrix svd and tensor hosvd for adaptive indexing and retrieval of motion trajectories	database indexing;motion trajectory analysis;video retrieval database indexing image motion analysis singular value decomposition tensors;image motion analysis;tensile stress;multilinear algebra video retrieval motion trajectory analysis singular value decomposition;singular value decomposition;matrix algebra adaptive indexing video retrieval motion trajectory query matching feature space dynamic tensor hosvd updating algorithm dynamic tensor hosvd downdating algorithm;video retrieval;partial information;matrix algebra;indexing and retrieval;data mining;feature space;dynamic tensor hosvd downdating algorithm;dynamic tensor hosvd updating algorithm;adaptive indexing;multiple objectives;trajectory;multilinear algebra;query matching;indexing;dynamics;matrix decomposition;heuristic algorithms;tensile stress indexing matrix decomposition databases robustness motion analysis singular value decomposition heuristic algorithms sampling methods algebra;motion trajectory;tensors	Motion information is regarded as one of the most important cues for developing semantics in video data. Yet it is extremely challenging to build indexing and browsing tools for video data, particularly when it involves interactive motions of multiple objects. The problem is further complicated when the video archives are dynamically updated, and/or queries contains partial information. An efficient solution would require that the feature space used to represent the data be dynamically updated or downdated to allow frequent additions/deletions and matching of queries. Assuming Tensor HOSVD as the feature space, in this paper, we propose two novel algorithms, namely, Dynamic Tensor HOSVD Updating Algorithm (DTSV D+) and Dynamic Tensor HOSVD Downdating Algorithm (DTSV D−), for dynamically updating and downdating existing tensor HOSVD, without recalculating it from the raw data. The proposed algorithms are robustly applied to both full and partial multiple motion trajectories events with varying number of objects, trajectory lengths, and sampling rates. Simulations on real-world multiple motion trajectories data demonstrate the robustness and accuracy of the proposed algorithms.	algorithm;archive;computer simulation;feature vector;sampling (signal processing);singular value decomposition	Xiang Ma;Dan Schonfeld;Ashfaq A. Khokhar	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959787	database index;computer vision;search engine indexing;dynamics;tensor;feature vector;multilinear algebra;computer science;trajectory;theoretical computer science;machine learning;mathematics;stress;matrix decomposition;singular value decomposition	DB	38.85786390760955	-50.88782064541953	5365
4a96bf4d9dbf0028275f51d1da7f20f4d8bdbd3c	a sparse grid based generative topographic mapping for the dimensionality reduction of high-dimensional data		Most high-dimensional data exhibit some correlation such that data points are not distributed uniformly in the data space but lie approximately on a lower-dimensional manifold. A major problem in many data-mining applications is the detection of such a manifold from given data, if present at all. The generative topographic mapping (GTM) finds a lower-dimensional parameterization for the data and thus allows for nonlinear dimensionality reduction. We will show how a discretization based on sparse grids can be employed for the mapping between latent space and data space. This leads to efficient computations and avoids the ‘curse of dimensionality’ of the embedding dimension. We will use our modified, sparse grid based GTM for problems from dimensionality reduction and data classification.	bottleneck (engineering);computation;computer;curse of dimensionality;data mining;data point;dataspaces;discretization;experiment;generative model;generative topographic map;manifold regularization;nonlinear dimensionality reduction;solver;sparse grid;sparse matrix;topography	Michael Griebel;Alexander Hullmann	2012		10.1007/978-3-319-09063-4_5	computer vision;generative topographic map;machine learning;pattern recognition	ML	30.197627779984504	-36.35506469908554	5378
f0f6a5a9f8a92210958b70a4d76e7f5f32a7d357	a survey of preference relations	linguistic preference relation;multiplicative preference relations;decision maker;fuzzy preference relation;multiplicative preference relation;intuitionistic preference relation	Preference Relations are a popular and powerful tool used by decision makers to provide their preference information in the process of decision making. Over the last decades, various types of preference relations have been developed, but they are scattered through the literature. The purpose of this paper is to present a comprehensive survey of preference relations. We also briefly discuss their properties and introduce some new preference relations. Finally, we put forward the directions of future research.		Zeshui Xu	2007	Int. J. General Systems	10.1080/03081070600913726	preference learning;decision-making;knowledge management	Logic	-3.314603865009292	-22.14829633312892	5381
b7c86d225e9d1a225e2fad59f96c10642fd4525a	reconstruction error in a motion capture system		Marker-based motion capture (MoCap) systems can be composed by several dozens of cameras with the purpose of reconstructing the trajectories of hundreds of targets.With a large amount of cameras it becomes interesting to determin e the optimal reconstruction strategy. For such aim it is of fundamental importance to understand the information provided by different camera measurements and how they are combined, i.e. how the reconstruction error changes by considering di fferent cameras. In this work, first, an approximation of the reconstruction error variance is derived. The results obtained in some simulations suggest that the proposed strategy allo ws to obtain a good approximation of the real error variance with significant reduction of the computational time.	approximation;computation;motion capture;simulation;time complexity	Andrea Masiero;Angelo Cenedese	2012	CoRR		computer vision;econometrics;simulation;computer science	Vision	51.85597005017213	-47.570464848055586	5382
a0278323b915256153a9690daf11d8cc806e5624	probabilistic appearance-based mapping and localization using visual features		An appearance-based approach for visual mapping and localization is proposed in this paper. On the one hand, a new image similarity measure between images based on number of matchings and their associated distances is introduced. On the other hand, to optimize running times, matchings between the current image and previous visited places are determined using an index based on a set of randomized KD-trees. Further, a discrete Bayes filter is used for predicting loop candidates, taking into account the previous relationships between visual locations. The approach has been validated using image sequences from several environments. Whereas most other approaches use omnidirectional cameras, a single-view configuration has been selected for our experiments.	embedded system;emoticon;epipolar geometry;experiment;internationalization and localization;key frame;matching (graph theory);omnidirectional camera;randomized algorithm;similarity measure	Emilio Garcia-Fidalgo;Alberto Ortiz	2013		10.1007/978-3-642-38628-2_33	computer vision;artificial intelligence;probabilistic logic;similarity measure;pattern recognition;recursive bayesian estimation;omnidirectional antenna;computer science	Vision	42.433349732187736	-50.64300372011532	5391
174de6f77264bd3d7f00da61f424375c80419e59	image superresolution using densely connected residual networks		Recently, convolutional neural networks (CNN) have achieved impressive breakthroughs in single image superresolution. In particular, an efficient nonlinear mapping by increasing the depth and width of the network can be learned between the low-resolution input image and the high-resolution target image. However, this will lead to a substantial increase in network parameters, requiring the massive amount of training data to prevent overfitting. Besides, most CNN-based methods ignore the full use of different levels of features and, therefore, achieve relatively low performance. In this letter, we propose a deep convolutional network named densely connected residual networks (DRNet). Our proposed DRNet can reach very deep and wide while requiring fewer parameters. The significant performance improvement of our model is mainly due to the integration of dense skip connection and residual learning. In this way, DRNet mitigates the problems of overfitting, vanishing gradient, and training instability during training very deep and wide networks. Moreover, it can improve the propagation and reuse of features by creating direct connections from the previous layers to the subsequent layers. We evaluate the proposed method using images from four benchmark datasets and set a new state of the art.	artificial neural network;autostereogram;benchmark (computing);convolutional neural network;gradient;image resolution;instability;nonlinear system;overfitting;software propagation;super-resolution imaging	Ran Wen;Kun Fu;Hao Sun;Xian Sun;Lei Wang	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2861989	iterative reconstruction;residual;artificial intelligence;overfitting;mathematics;convolutional neural network;pattern recognition;feature extraction;instability;superresolution;image resolution	Vision	23.05865828194699	-50.67298873441249	5415
2a4a22505b91b43682c8a926c49f3a988dc7806d	efficiency and robustness in a support platform for intelligent airport ground handling	intelligent transportation systems;real time information;aprons airports;heuristic methods;ground handling;equipment allocation;sequential heuristic;tegel airport;decision support systems;safety;robust assignment;fleet management;airport operations;airport ground traffic management	One of the effects of the development of air traffic during recent years has been the increase of congestion on major airports and their aprons, where ground handling operations take place. Efficiency and safety issues become important on the aprons. The “Integrated Airport Apron Safety Fleet Management—AAS” project aims at setting a decision support system for efficient and safe management of apron traffic, taking advantage of information gathered by “intelligent” vehicles equipped with on-board positioning and monitoring systems. In this project, we optimize the assignment of vehicles to apron operations. We present a mathematical formulation of the problem and sketch a fast sequential heuristic, which provides efficient assignments based on real-time data on position and status of vehicles and operations. We then discuss how the procedure can reach good trade-offs between efficiency and robustness against unpredicted, but frequent, delays occurring in real time at airports. The proposed approach has be...		Giovanni Andreatta;Lorenzo Capanna;Luigi De Giovanni;Michele Monaci;Luca Righi	2014	J. Intellig. Transport. Systems	10.1080/15472450.2013.802160	embedded system;real-time data;intelligent transportation system;simulation;decision support system;computer science;engineering;transport engineering	Robotics	12.376680751722782	-7.587836985350177	5417
2100ff60de20edbb217bd07c390f450a859a4972	em algorithm for one-shot device testing with competing risks under exponential distribution	one shot device;exponential distribution;inequality constrained least squares;masked data;ed01 data;em algorithm;competing risks	This paper is an extension of the work of Balakrishnan and Ling [1] introducing a competing risk model into a one-shot device testing analysis under an accelerated life-test setting. An Expectation Maximization (EM) algorithm is then developed for the estimation of the model parameters. An extensive Monte Carlo simulation study is carried out to assess the performance of the EM algorithm and then compare the obtained results with the initial estimates obtained by the Inequality Constrained Least Squares (ICLS) method of estimation. Finally, we apply the EM algorithm to a clinical data, ED01, to illustrate the method of inference developed here.	expectation–maximization algorithm;financial risk modeling;least squares;monte carlo method;simulation;software testing;time complexity	Narayanaswamy Balakrishnan;Hon Yiu So;Man Ho Ling	2015	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.12.014	exponential distribution;econometrics;mathematical optimization;expectation–maximization algorithm;mathematics;statistics	ML	30.560404773633202	-24.233337974922655	5419
1ffca9e4bb1d13bb99a338befa036d99bf4f5b52	multichannel linear prediction method compliant with the mpeg-4 als	tecnologia electronica telecomunicaciones;lossless compression;linear predictive;linear prediction coding;linear predictive coding;computational complexity;compression ratio;tecnologias;grupo a;multichannel signal;lossless compression and mpeg 4 als	A new linear prediction analysis method for multichannel signals was devised, with the goal of enhancing the compression performance of the MPEG-4 Audio Lossless Coding (ALS) compliant encoder and decoder. The multichannel coding tool for this standard carries out an adaptively weighted subtraction of the residual signals of the coding channel from those of the reference channel, both of which are produced by independent linear prediction. Our linear prediction method tries to directly minimize the amplitude of the predicted residual signal after subtraction of the signals of the coding channel, and the method has been implemented in the MPEG-4 ALS codec software. The results of a comprehensive evaluation show that this method reduces the size of a compressed file. The maximum improvement of the compression ratio is 14.6% which is achieved at the cost of a small increase in computational complexity at the encoder and without increase in decoding time. This is a practical method because the compressed bitstream remains compliant with the MPEG-4 ALS standard.	h.264/mpeg-4 avc	Yutaka Kamamoto;Noboru Harada;Takehiro Moriya	2008	IEICE Transactions	10.1093/ietfec/e91-a.3.756	data compression;data compression ratio;linear predictive coding;speech recognition;computer science;theoretical computer science;speech coding;compression ratio;lossless compression;adaptive coding;context-adaptive binary arithmetic coding;computational complexity theory;algorithm	Vision	46.65036760764874	-15.595524701093133	5425
bbfb0354ee746d791fc5cf710ca8f828066ca3e7	generalized extreme value distribution with time-dependence using the ar and ma models in state space form	time dependent;generalized extreme value distribution;bayesian approach;normal distribution;efficient algorithm;mixture sampler;extreme values;moving average;gumbel distribution;extreme value;markov chain monte carlo;markov chain monte carlo methods;state space;stock returns;state space representation;monte carlo;state space model;discussion paper;markov chain	A new state space approach is proposed to model the time-dependence in an extreme value process. The generalized extreme value distribution is extended to incorporate the time-dependence using a state space representation where the state variables either follow an autoregressive (AR) process or a moving average (MA) process with innovations arising from a Gumbel distribution. Using a Bayesian approach, an efficient algorithm is proposed to implement Markov chain Monte Carlo method where we exploit an accurate approximation of the Gumbel distribution by a ten-component mixture of normal distributions. The methodology is illustrated using extreme returns of daily stock data. The model is fitted to a monthly series of minimum returns and the empirical results support strong evidence of time-dependence among the observed minimum returns.	algorithm;approximation;autoregressive model;markov chain monte carlo;maxima and minima;monte carlo method;moving-average model;state space;state-space representation	Jouchi Nakajima;Tsuyoshi Kunihama;Yasuhiro Omori;Sylvia Frühwirth-Schnatter	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.04.017	gumbel distribution;econometrics;mathematical optimization;generalized extreme value distribution;state-space representation;extreme value theory;mathematics;statistics	ML	31.4333274169222	-20.962752529117687	5432
b845ffb6b978c4884cd1f92a217bc5dedea6573c	dynamical behaviors in complex-valued love model with or without time delays		In this paper, a novel version of nonlinear model, i.e. a complex-valued love model with two time delays between two individuals in a love affair, has been proposed. A notable feature in this model is that we separate the emotion of one individual into real and imaginary parts to represent the variation and complexity of psychophysiological emotion in romantic relationship instead of just real domain, and make our model much closer to reality. This is because love is a complicated cognitive and social phenomenon, full of complexity, diversity and unpredictability, which refers to the coexistence of different aspects of feelings, states and attitudes ranging from joy and trust to sadness and disgust. By analyzing associated characteristic equation of linearized equations for our model, it is found that the Hopf bifurcation occurs when the sum of time delays passes through a sequence of critical value. Stability of bifurcating cyclic love dynamics is also derived by applying the normal form theory and the c...	dynamical system	Wei Deng;Xiaofeng Liao;Tao Dong	2017	I. J. Bifurcation and Chaos	10.1142/S0218127417502005	hopf bifurcation;social psychology;the imaginary;mathematics;mathematical analysis;feeling;disgust;nonlinear system;sadness;ranging;critical value	Theory	53.13730745595658	-1.0112449914833872	5436
1b87951251b831286d7397ed08cf225df5678009	spatial cointegration and heteroscedasticity	autocorrelacion;c21 c40 c51 j60;lagrange multiplier test;simulation;autoregression;methode monte carlo;simulacion;estrategia;strategy;spatial autoregression;unobserved heteroscedasticity;autoregresion;spatial nonstationarity;spatial cointegration;monte carlo simulation;strategie;monte carlo analysis;spatial autocorrelation;autocorrelation;spatial	A two-step Lagrange Multiplier test strategy has recently been suggested as a tool to reveal spatial cointegration. The present paper generalises the test procedure by incorporating control for unobserved heteroscedasticity. Using Monte Carlo simulation, the behaviour of several relevant tests for spatial cointegration and/or heteroscedasticity is investigated. The two-step test for spatial cointegration appears to be robust towards heteroscedasticity. While several tests for heteroscedasticity prove to be inconclusive under certain circumstances, a Lagrange Multiplier test for heteroscedasticity based on spatially differenced variables is shown to serve well as an indication of heteroscedasticity irrespective of cointegration status.		Jørgen Lauridsen;Reinhold Kosfeld	2007	Journal of Geographical Systems	10.1007/s10109-007-0048-y	econometrics;mathematics;heteroscedasticity;statistics;monte carlo method	DB	33.23278318501005	-21.948609304678985	5442
7df0db2194c661a507b7d11c2f707dab1f705844	foveated figure-ground segmentation and its role in recognition	real time;datavetenskap datalogi;computer science	Figure-ground segmentation and recognition are two interrelated processes. In this paper we present a method for foveated segmentation and evaluate it in the context of a binocular real-time recognition system. Segmentation is solved as a binary labeling problem using priors derived from the results of a simplistic disparity method. Doing so we are able to cope with situations when the disparity range is very wide, situations that has rarely been considered, but appear frequently for narrow-field camera sets. Segmentation and recognition are then integrated into a system able to locate, attend to and recognise objects in typical cluttered indoor scenes. Finally, we try to answer two questions: is recognition really helped by segmentation and what is the benefit of multiple cues for recognition?	benchmark (computing);binocular disparity;binocular vision;contour line;experiment;feedback;ground truth;online and offline;real-time clock;robot;scale-invariant feature transform	Mårten Björkman;Jan-Olof Eklundh	2005		10.5244/C.19.84	computer vision;simulation;computer science;segmentation-based object categorization	Vision	42.17979462312107	-46.70056143722578	5444
b70beffade40446ed5156f6ee77bcb78b90dbe42	distortion optimization of model-based secure embedding schemes for data-hiding	institutional repositories;watermarking;distortion optimization;data hiding;fedora;vital;hungarian method;spread spectrum watermarking;vtls;security;ils;optimization model	This paper is the continuation of works about analysis of secure watermarking schemes in the case of WOA (Watermarked Only Attack) framework. In previous works, two new BPSK spread-spectrum watermarking modulations, Natural Watermarking (NW) and Circular Watermarking (CW), have been proposed and have been shown to be more secure than classical modulations. Because security is guaranted using specific distributions of watermarked contents, we propose to use optimal model-based embedding to ensure security while minimizing the overall distortion. Additionally, we propose a new secure watermarking scheme based on distribution of vector norms in the Gaussian case. We illustrate model-based embedding performance in the case of Gaussian signals and show that this approach not only allows to achieve excellent level of security in the WOA framework, but also allows to minimize distortion. Finally, a comparison of the robustness of the proposed embedding schemes is performed.	circular convolution;continuation;distortion;hungarian algorithm;mathematical optimization;netware;watermark (data file);web-oriented architecture	Benjamin Mathon;Patrick Bas;François Cayre;Fernando Pérez-González	2008		10.1007/978-3-540-88961-8_23	telecommunications;digital watermarking;computer science;information security;theoretical computer science;hungarian algorithm;mathematics;programming language;information hiding;computer security	Crypto	41.650416499144676	-9.293289117000278	5451
4b83cf2538a13428f546d7f7168a0830508946db	tabu search for the redundancy allocation problem of homogenous series-parallel multi-state systems	fiabilidad;reliability;execution time;system configuration;meta heuristics;redundancia;availability;disponibilidad;search space;series system;consumer demand;heuristic method;espace etat;metodo heuristico;organisation systeme;diagrama carga;moment generating function;satisfiability;algoritmo genetico;organizacion sistema;diagramme charge;redundancy;systeme serie;redundancy allocation;sistema serie;fiabilite;state space;rupture;defaillance;universal generating function;algorithme genetique;series parallel systems;temps execution;genetic algorithm;generating function;tabu search;methode heuristique;failures;load curve;cumulant;tiempo ejecucion;multi state systems;espacio estado;redundancy allocation problem;disponibilite;fallo;ruptura;series parallel;busqueda tabu;redondance;recherche tabou;problem solving	This paper develops an efficient tabu search (TS) heuristic to solve the redundancy allocation problem for multi-state series–parallel systems. The system has a range of performance levels from perfect functioning to complete failure. Identical redundant elements are included in order to achieve a desirable level of availability. The elements of the system are characterized by their cost, performance and availability. These elements are chosen from a list of products available in the market. System availability is defined as the ability to satisfy consumer demand, which is represented as a piecewise cumulative load curve. A universal generating function technique is applied to evaluate system availability. The proposed TS heuristic determines the minimal cost system configuration under availability constraints. An originality of our approach is that it proceeds by dividing the search space into a set of disjoint subsets, and then by applying TS to each subset. The design problem, solved in this study, has been previously analyzed using genetic algorithms (GAs). Numerical results for the test problems from previous research are reported, and larger test problems are randomly generated. Comparisons show that the proposed TS out-performs GA solutions, in terms of both the solution quality and the execution time.	series-parallel graph;tabu search	Mohamed Ouzineb;Mustapha Nourelfath;Michel Gendreau	2008	Rel. Eng. & Sys. Safety	10.1016/j.ress.2007.06.004	availability;mathematical optimization;generating function;series and parallel circuits;genetic algorithm;tabu search;state space;artificial intelligence;reliability;mathematics;redundancy;algorithm;moment-generating function;metaheuristic;statistics;cumulant;satisfiability	DB	8.89339123353837	-0.7385102898225754	5471
060df26bb3a447ffd63e9ca1ca0abdde01b2864b	a hybrid genetic - particle swarm optimization algorithm for the vehicle routing problem	vehicle routing problem;combinatorial optimization problem;genetics;memetic algorithm;particle swarm optimizer;particle swarm optimization;genetic algorithm;genetic algorithms;particle swarm optimization algorithm;local search	Usually in a genetic algorithm, individual solutions do not evolve during their lifetimes: they are created, evaluated, they may be selected as parents to new solutions and they are destroyed. However, research into memetic algorithms and genetic local search has shown that performance may be improved if solutions are allowed to evolve during their own lifetimes. We propose that this solution improvement phase can be assisted by knowledge stored within the parent solutions, effectively allowing parents to teach their offspring how to improve their fitness. In this paper, the evolution of each individual of the total population, which consists of the parents and the offspring, is realized with the use of a Particle Swarm Optimizer where each of them has to improve its physical movement following the basic principles of Particle Swarm Optimization until it will obtain the requirements to be selected as a parent. Thus, the knowledge of each of the parents, especially of a very fit parent, has the possibility to be transferred to its offspring and to the offspring of the whole population, and by this way the proposed algorithm has the possibility to explore more effectively the solution space. These ideas are applied in a classic combinatorial optimization problem, the vehicle routing problem, with very good results when applied to two classic benchmark sets of instances.	algorithm;particle swarm optimization;vehicle routing problem	Yannis Marinakis;Magdalene Marinaki	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.06.085	mathematical optimization;multi-swarm optimization;meta-optimization;genetic algorithm;computer science;derivative-free optimization;artificial intelligence;vehicle routing problem;machine learning;mathematics;particle swarm optimization;metaheuristic	EDA	26.634086574066774	-3.9930937669005777	5476
2c5f675c7a5650eda4fbf2a92e780c1922224bf9	investigating mcts modifications in general video game playing	games avatars artificial intelligence monte carlo methods animals sprites computer missiles;gvg ai framework monte carlo tree search board games mcts algorithm general video game playing general video game ai uct reverse penalty vanilla mcts controller;tree searching artificial intelligence computer games monte carlo methods	While Monte Carlo tree search (MCTS) methods have shown promise in a variety of different board games, more complex video games still present significant challenges. Recently, several modifications to the core MCTS algorithm have been proposed with the hope to increase its effectiveness on arcade-style video games. This paper investigates of how well these modifications perform in general video game playing using the general video game AI (GVG-AI) framework and introduces a new MCTS modification called UCT reverse penalty that penalizes the MCTS controller for exploring recently visited children. The results of our experiments show that a combination of two MCTS modifications can improve the performance of the vanilla MCTS controller, but the effectiveness of the modifications highly depends on the particular game being played.	ai challenge;algorithm;artificial intelligence (video games);experiment;general video game playing;monte carlo method;monte carlo tree search	Frederik Frydenberg;Kasper R. Andersen;Sebastian Risi;Julian Togelius	2015	2015 IEEE Conference on Computational Intelligence and Games (CIG)	10.1109/CIG.2015.7317937	simulation;computer science;artificial intelligence;machine learning;multimedia;monte carlo tree search	Vision	18.8952172441369	-18.040648545700467	5477
97a11654fd9052a55f1683024028d0ae278fcc27	a bi-level neural-based fuzzy classification approach for credit scoring problems	credit scoring;classification;artificial neural networks anns;multilayer perceptrons mlps;fuzzy logic and fuzzy models	The credit scoring is a risk evaluation task considered as a critical decision for financial institutions in order to avoid wrong decision that may result in huge amount of losses. Classification models are one of the most widely used groups of data mining approaches that greatly help decision makers and managers to reduce their credit risk of granting credits to customers instead of intuitive experience or portfolio management. Accuracy is one of the most important criteria in order to choose a credit-scoring model; and hence, the researches directed at improving upon the effectiveness of credit scoring models have never been stopped. In this article, a hybrid binary classification model, namely FMLP, is proposed for credit scoring, based on the basic concepts of fuzzy logic and artificial neural networks (ANNs). In the proposed model, instead of crisp weights and biases, used in traditional multilayer perceptrons (MLPs), fuzzy numbers are used in order to better model of the uncertainties and complexities in financial data sets. Empirical results of three well-known benchmark credit data sets indicate that hybrid proposed model outperforms its component and also other those classification models such as support vector machines (SVMs), K-nearest neighbor (KNN), quadratic discriminant analysis (QDA), and linear discriminant analysis (LDA). Therefore, it can be concluded that the proposed model can be an appropriate alternative tool for financial binary classification problems, especially in high uncertainty conditions. © 2013 Wiley Periodicals, Inc. Complexity 18: 46–57, 2013	black and burst;fuzzy classification	Mehdi Khashei;Mohammad Taghi Rezvan;Ali Zeinal Hamadani;Mehdi Bijari	2013	Complexity	10.1002/cplx.21458	biological classification;computer science;artificial intelligence;machine learning;data mining	AI	10.844090007587441	-38.36024971291993	5480
6a5b0edf3df426ab05404074db02bac656837bf0	path updating tree based fast path planner for unpredictable changing environments	trees mathematics approximation theory collision avoidance;crowded obstacle path updating tree fast path planner planning algorithm put crowding information approximate path collision possibility reachable possibility path length two level path planner local path generator computational resource distribution	For changing environments, although lots of planning algorithms have focused on how to get a valid and short path, seldom planning algorithms can be employed to extract a sustaining valid path. Especially for large-scale environments, getting a sustaining valid path is required to make intelligent decisions for robots. In this paper, a method of Path Updating Tree (PUT) is proposed to get such a sustainingly valid region path, which approximately estimates the environment using extended nodes in the projected Cspace. Each extended node is used to reflect local crowding information in this paper. An approximate path, which connected goal and start, is generated based on each region's collision possibility, reachable possibility and path length. With environment changing, nodes of PUT are updated and new region paths are generated by regrowing if necessary. Then a two-level path planner is introduced with PUT and local path generator to distribute the computational resource properly. The proposed method has shown to be efficient in large-scale scenarios with crowded obstacles.	anytime algorithm;approximation algorithm;computation;computational resource;crowding;estimation theory;fast path;high- and low-level;motion planning;planner;robot;uniform resource identifier	Chuangqi Wang;Bin Chen;Hong Liu	2012	2012 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2012.6491185	graphical path method;basis path testing;mathematical optimization;simulation;fast path;constrained shortest path first;any-angle path planning;computer science;machine learning;mathematics;shortest path problem;instruction path length	Robotics	53.24879985590678	-23.891300865241384	5482
0a51c01c7a3ff9c73c0e518e6eef51f30f71f4ba	learning micro-management skills in rts games by imitating experts	starcraft;learning by observation;behaviour learning;real time strategy;games;qualitative spatial relations	We investigate the problem of learning the control of small groups of units in combat situations in Real Time Strategy (RTS) games. AI systems may acquire such skills by observing and learning from expert players, or other AI systems performing those tasks. However, access to training data may be limited, and representations based on metric information – position, velocity, orientation etc. – may be brittle, difficult for learning mechanisms to work with, and generalise poorly to new situations. In this work we apply qualitative spatial relations to compress such continuous, metric state-spaces into symbolic states, and show that this makes the learning problem easier, and allows for more general models of behaviour. Models learnt from this representation are used to control situated agents, and imitate the observed behaviour of both synthetic (pre-programmed) agents, as well as the behaviour of human-controlled agents on a number of canonical micromanagement tasks. We show how a Monte-Carlo method can be used to decompress qualitative data back in to quantitative data for practical use in our control system. We present our work applied to the popular RTS game Starcraft.	control system;expectation propagation;game controller;monte carlo method;real-time clock;real-time locating system;situated;starcraft;synthetic intelligence;velocity (software development)	Jay Young;Nick Hawes	2014			games;simulation;computer science;artificial intelligence;machine learning	AI	49.57338840757953	-27.8285794862464	5491
e8cdfc11da79629a9318a8397360ac89f380c001	more efficient executions of monte carlo fusion codes by means of montera: the isdep use case	plasmas monte carlo methods dynamic scheduling trajectory mathematical model equations heuristic algorithms;integrator of stochastic differential equations for plasmas;plasma dynamics montera wms grid;grid environments;fusion device;distributed processing;stellarators monte carlo fusion codes montera isdep integrator of stochastic differential equations for plasmas plasma dynamics fusion device distributed computing platforms grid environments tokamaks;plasmas;stellarators;physics computing;grid;tokamak devices differential equations distributed processing grid computing monte carlo methods physics computing stellarators stochastic processes;trajectory;stochastic processes;montera;heuristic algorithms;monte carlo method;distributed computing platforms;wms;mathematical model;isdep;tokamak devices;differential equations;monte carlo fusion codes;plasma dynamics;monte carlo;tokamaks;grid computing;use case;monte carlo methods;heuristic algorithm;dynamic scheduling	ISDEP (Integrator of Stochastic Differential Equations for Plasmas) is a Monte Carlo code that solves the plasma dynamics in a fusion device and perfectly scales on distributed computing platforms. Montera is a recent framework developed for achieving Grid efficient executions of Monte Carlo applications, as ISDEP is. In this work, the improvement of performing the calculations of ISDEP with Montera, which rise up to 34.9%, is shown as well as an analysis on the implications it could have, which aim to show to the fusion research community the benefits of using Montera.	code;distributed computing;monte carlo method;monte carlo tree search;plasma active;radiology;scheduling (computing);web map service	Manuel Aurelio Rodriguez Pascual;Antonio J. Rubio-Montero;Rafael Mayo;Andrés Bustos;Francisco Castejón-Magaña;Ignacio Martín Llorente	2011	2011 19th International Euromicro Conference on Parallel, Distributed and Network-Based Processing	10.1109/PDP.2011.46	stochastic process;simulation;computer science;theoretical computer science;statistics;monte carlo method	HPC	43.07144631060772	1.0071371552004689	5493
23727d8dbbe3f4904f303f62b84d3717968e9f19	using anova to analyze modified gini index decision tree classification	decision tree	Decision tree classification is a commonly used method in data mining. It has been used for predicting medical diagnoses. Among data mining methods for classification, decision trees have several advantages such as they are simple to understand and interpret; they are able to handle both numerical and categorical attributes. However, it is well-known that when Gini index is used for classification, the method biases multivalued attributes. In addition to having difficulty when the number of classes is large, the method also tends to favor tests that result in equal-sized partitions and purity in all partitions. We modified the Gini-based decision tree method. To overcome the known problems, we normalize the Gini indexes by taking into account information about the splitting status of all attributes. Instead of using the Gini index for attribute selection as usual, we use ratios of Gini indexes and their splitting values in order to reduce the biases. In this paper, we utilize ANOVA, an analysis of variance method, to show that our modified Gini decision tree method is statistically different with other known decision tree methods at least for the tested benchmark medical data bases.	decision tree	Quoc-Nam Tran	2008			analysis of variance;normalization (statistics);categorical variable;decision tree;feature selection;mathematics;pattern recognition;artificial intelligence	ML	1.314273736078668	-29.940934900638673	5495
2408103d066869d44f7a17a983f76bd29240a78d	bayesian inference of log determinants	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	The log determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousands. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log determinant, while also quantifying the uncertainty due to budgetconstrained evidence.	approximation algorithm;cobham's thesis;computation;euler;experiment;gaussian process;linux;machine learning;markov chain;markov random field;matrix multiplication;maxima and minima;monolithic kernel;polynomial kernel;stochastic gradient descent;the matrix;theory	Jack K. Fitzsimons;Kurt Cutajar;Maurizio Filippone;Michael A. Osborne;Stephen J. Roberts	2017	CoRR		econometrics;computer science;machine learning;mathematics;statistics	ML	27.43625296549772	-29.577800541453428	5508
3b737b3917d80caf08fee5923c4d99c4bf5890b3	fuzzy clustering for knowledge discovery in oceanographic data	cluster algorithm;unsupervised clustering;fuzzy k means;information science;ocean temperature;scientific data;sea level pressure;spatio temporal data mining;indexing terms;data mining;clustering algorithms sea measurements data mining data analysis algorithm design and analysis spatiotemporal phenomena sea level ocean temperature us department of defense information science;fuzzy clustering;data analysis;spatio temporal data;us department of defense;spatiotemporal phenomena;clustering algorithms;cluster validity;algorithm design and analysis;sea level;sea measurements	can produce new insights into geospatial scientific data that is being collected from around the globe. This paper presents the use of unsupervised fuzzy clustering algorithms for analyzing spatio-temporal data. A new cluster validity measure for spatio-temporal data that facilitates the use of unsupervised clustering is proposed. A computationally efficient version of the clustering algorithm is defined and performance compared with the basic algorithm. These new algorithms are validated on global climate data (sea level pressure) and it is seen that these algorithms are able to discover interesting phenomena in the climate data and compare well with other existing techniques.	algorithm;algorithmic efficiency;cluster analysis;computation;data mining;fuzzy clustering;k-means clustering	Zhijian Liu;Bin Wu;Roy George	2006	2006 IEEE International Conference on Granular Computing	10.1109/GRC.2006.1635886	atmospheric pressure;algorithm design;sea surface temperature;index term;fuzzy clustering;information science;computer science;data science;machine learning;data mining;cluster analysis;data analysis;information retrieval;data;sea level	DB	0.4460211425165728	-37.18048389532986	5511
6e397c7b701df7bd0872a662c90621ee36790523	image processing algorithms for aal services		The image processing techniques are widely used in many fields, such as security and home automation. With these techniques algorithms are developed for its use in common vision systems (with regular video cameras connected to a computer), consuming too many resources.	atm adaptation layer;image processing	Iván Ovejero;Elena Romero;Zorana Bankovic;Pedro Malagón;Álvaro Araujo	2011		10.1007/978-3-642-21303-8_28	multimedia;digital image processing;image processing;home automation;computer science;mobile robot;computer vision;ambient intelligence;artificial intelligence	DB	45.141492104302	-36.06172705133679	5516
1e8ead5045c4b4de598c4eb570bfd9da14970129	a general two-step approach to learning-based hashing	image retrieval binary codes hashing;optimisation;binary codes;binary quadratic problems general two step approach learning based hashing hash function optimization process complex optimization problems hashing learning problem hash bit learning hash function learning;conference paper;binary codes optimization support vector machines kernel hamming distance training testing;hashing;cryptography;optimisation cryptography learning artificial intelligence;learning artificial intelligence;image retrieval	Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.	binary classification;code generation (compiler);cryptographic hash function;experiment;loss function;mathematical optimization;quadratic programming;unified framework	Guosheng Lin;Chunhua Shen;David Suter;Anton van den Hengel	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.317	feature hashing;hopscotch hashing;hash table;double hashing;hash function;linear hashing;perfect hash function;extendible hashing;dynamic perfect hashing;image retrieval;open addressing;computer science;cryptography;theoretical computer science;machine learning;universal hashing;pattern recognition;k-independent hashing;cuckoo hashing;locality preserving hashing;suha;2-choice hashing;locality-sensitive hashing	Vision	20.224302689291836	-46.70669996674817	5518
9100882d2bc7d8caccc65aaca4ee4eb1098f62f2	complexity regulation for real-time video encoding	encoding video compression rate distortion rate distortion theory discrete cosine transforms data compression runtime multimedia systems costs scalability;rate distortion;control algorithm;data compression;real time;video compression;rate distortion theory;rate distortion theory computational complexity video coding data compression real time systems;video coding;computational complexity;encoding control algorithm video compression rate distortion performance encoding complexity real time software based video encoding compressed video quality compression complexity dynamic encoding complexity control scheme input frame buffer;compressed video;real time systems	Past research in video compression has focused on improving the rate-distortion performance without active consideration of the encoding complexity. However, realtime software or processor based encoding has tight complexity constraints. In this paper we study real-time software-based video encoding as a problem of dynamic tradeoffs among rate, distortion, and complexity. We present a compression system where the compressed video quality is pre-set and fixed, but the compression complexity is constrained and regulated to ensure realtime performance, by changing the encoding configurations with varying compression efficiency. We propose a dynamic encoding complexity control scheme utilizing an input frame buffer. Our experimental results show that the proposed encoding control algorithm effectively avoids frame drops caused by complexity peaks for real-time encoding.	data compression;real-time transcription	Zhun Zhong;Yingwei Chen	2002		10.1109/ICIP.2002.1038130	video compression picture types;data compression;computer vision;data compression ratio;real-time computing;computer science;theoretical computer science;video tracking;move-to-front transform;lossless compression;block-matching algorithm;rate–distortion optimization;context-adaptive binary arithmetic coding;motion compensation;h.261;statistics;multiview video coding	Vision	47.31046913953305	-17.957678167322765	5520
96c90d62517615eb5658569c1096b73812d58c93	semi-fragile watermarking for video quality evaluation in broadcast scenario	digital video broadcasting;watermarking;side information digital tv video quality semi fragile watermarking;data compression;blind estimation;digital tv;video quality;digital television;watermarking multimedia communication video compression digital video broadcasting tv broadcasting testing digital tv companies transform coding discrete cosine transforms;video coding;digital video;side information;data compression watermarking video coding digital video broadcasting;mpeg compression semifragile watermarking video quality evaluation digital video content broadcasting blind estimation analog tv scenario distribution chain video quality	Nowadays the broadcasting of digital video contents is a common scenario. Evaluation of the quality of received information is an important issue for companies involved in this business. The goal is to obtain blind estimation of the quality - i.e. without information about the original video - like in the analog TV scenario. In this paper, we present a novel technique to perform this operation based on semi-fragile watermarking where the watermark is used as reference signal. The algorithm uses a watermarking technique in the transformed domain. A watermark is inserted in the original video. Then the video passes through different compression steps, it is transmitted and received. At the end of the distribution chain the video quality is estimate by using correlation between original watermark and received video. The algorithm is tested for typical digital television broadcast bit-rate. The results show that the proposed algorithm provides a good estimation for the video quality for MPEG compression in particular for digital TV bit-rates.	algorithm;analog television;digital video;digital watermarking;moving picture experts group;semiconductor industry	Stefano Bossi;Francesco Mapelli;Rosa Lancini	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529712	video compression picture types;composite video;computer vision;video;h.263;digital television;uncompressed video;computer science;video quality;video capture;video tracking;multimedia;video processing;smacker video;internet privacy;motion compensation;h.261;video denoising;s-video;multiview video coding	EDA	43.74727586184629	-19.988345502107396	5523
f33030d2dcc3111ecb0069dd626ecf1c7768d64a	feature selection based on the center of gravity of bswfms using newfm	euclidean distance;center of gravity;feature selection;fuzzy neural networks	Feature selection has commonly been used to remove irrelevant features and improve classification performance. Some of features are irrelevant to the learning process; therefore to remove these irrelevant features not only decreases training and testing times, but can also improve learning accuracy. This study proposes a novel supervised feature selection method based on the bounded sum of weighted fuzzy membership functions (BSWFM) and Euclidean distances between their centers of gravity for decreasing the computational load and improving accuracy by removing irrelevant features. This study compares the performance of a neural network with a weighted fuzzy membership function (NEWFM) without and with the proposed feature selection method. The superiority of the NEWFM with feature selection over NEWFM without feature selection was demonstrated using three experimental datasets from the UCI Machine Learning Repository: Statlog Heart, Parkinsons and Ionosphere. 13 features, 22 features, and 34 features were used as inputs for the NEWFM without feature selection and these resulted in performance accuracies of 85.6%, 86.2% and 91.2%, respectively, using Statlog Heart, Parkinsons and Ionosphere datasets. 10 minimum features, 4 minimum features and 25 minimum features were used as inputs for the NEWFM with feature selection and these resulted in performance accuracies of 87.4%, 88.2%, and 92.6%, respectively, using Statlog Heart, Parkinsons and Ionosphere datasets. The results show that NEWFM with feature selection performed better than NEWFM without feature selection. & 2015 Elsevier Ltd. All rights reserved.	artificial neural network;computation;feature selection;machine learning;relevance	Sang-Hong Lee	2015	Eng. Appl. of AI	10.1016/j.engappai.2015.08.003	computer science;machine learning;pattern recognition;data mining;euclidean distance;center of gravity;feature selection	AI	12.590778855199831	-39.82584210118761	5557
0e861aaba4fa142a5a896bfe1388d012d1fbb55c	detecting gross errors for steady state systems	data reconciliation;optimal method;error detection;steady state	Gross error detection is important to data reconciliation in process industry. In practice, gross errors cannot be identified exactly by any algorithm. The issue of unreasonable solutions of gross error detection algorithms is discussed. A novel mixed integer optimization method presented in a previous paper is used in this paper. A strategy is proposed to identify gross errors and its most possible alternatives for steady state systems by the method. Gross errors are identified without the need for measurements elimination. The computation results show the effectiveness of the proposed strategy.	algorithm;computation;error detection and correction;gaussian elimination;mathematical optimization;sensor;steady state	Congli Mei	2009		10.1007/978-3-642-02469-6_71	econometrics;error detection and correction;computer science;control theory;mathematics;steady state;statistics	EDA	28.162963292231307	-18.51896000916548	5570
ab6d3e26b8ab1daa1d59a06545a30d79a3a80d0f	a generic c++ library for multilevel quasi-monte carlo	multi object adaptive optics;real time processing;asynchronous execution;gpu hardware accelerators;multicore architecture;computational astronomy	We present a high-performance framework for single- and multilevel quasi-Monte Carlo methods applicable to a large class of problems in uncertainty quantification. A typical application of interest is the approximation of expectations of quantities depending on solutions to e.g. partial differential equations with uncertain inputs, often yielding integrals over high-dimensional spaces. The goal of the software presented in this paper is to allow recently developed quasi-Monte Carlo (QMC) methods with high, dimension-independent orders of convergence to be combined with a multilevel approach and applied to large problems in a generic way, easing distributed memory parallel implementation. The well-known multilevel Monte Carlo method is also supported as a particular case, and some standard choices of distributions are included. For so-called interlaced polynomial lattice rules, a recently developed QMC method, precomputed generating vectors are required; some such vectors are provided for common cases.  After a theoretical introduction, the implementation is briefly explained and a user guide is given to aid in applying the framework to a concrete problem. We give two examples: a simple model problem designed to illustrate the mathematical concepts as well as the advantages of the multilevel method, including a measured decrease in computational work of multiple orders of magnitude for engineering accuracy, and a larger problem that exploits high-performance computing to show excellent parallel scaling. Some concrete use cases and applications are mentioned, including uncertainty quantification of partial differential equation models, and the approximation of solutions of corresponding Bayesian inverse problems. This software framework easily admits modifications; custom features like schedulers and load balancers can be implemented without hassle, and the code documentation includes relevant details.	approximation;c++;computation;distributed memory;image scaling;interlaced video;monte carlo method;polynomial;precomputation;quantum monte carlo;quasi-monte carlo method;software documentation;software framework;supercomputer;uncertainty quantification;whole earth 'lectronic link	Robert N. Gantner	2016		10.1145/2929908.2929915	real-time computing;simulation;computer science;theoretical computer science	ML	42.085985220113706	0.29897530343912515	5576
b1022d2772d7b0fdff3e6a222539283312c87582	meta-raps with path relinking for the 0–1 multidimensional knapsack problem	optimisation;metaheuristics;knapsack problems;single machine scheduling;0 1 multidimensional knapsack problem;optimisation artificial intelligence knapsack problems;discrete optimization problems meta raps path relinking multidimensional knapsack problem intelligent algorithms artificial intelligence metaheuristics intelligent procedures;learning systems;path relinking;single machine scheduling learning systems optimization algorithm design and analysis educational institutions classification algorithms;0 1 multidimensional knapsack problem metaheuristics meta raps path relinking memory;classification algorithms;meta raps;artificial intelligence;optimization;algorithm design and analysis;memory	The rapid increase of dimensions and complexity of real life problems makes it more difficult to find optimal solutions by traditional optimization methods. This challenge requires intelligent and sophisticated algorithms to make the right decisions given a set of inputs and a variety of possible actions. In the problem solving arena, this definition is transformed into the term of artificial intelligence. Artificial intelligence emerges in metaheuristics via memory and learning in algorithms. Many successful metaheuristics employ “intelligent” procedures to obtain high quality solutions for discrete optimization problems. To demonstrate the contribution of memory and learning into metaheuristics, Path Relinking will be incorporated into Meta-RaPS (Metaheuristic for Randomized Priority Search) which is classified as a memoryless metaheuristic. The 0-1 multidimensional knapsack problem will be used to evaluate the proposed algorithm.	artificial intelligence;discrete optimization;display resolution;knapsack problem;mathematical optimization;metaheuristic;problem solving;randomized algorithm;real life	Arif Arin;Ghaith Rabadi	2012	2012 6th IEEE International Conference Intelligent Systems	10.1109/IS.2012.6335159	mathematical optimization;parallel metaheuristic;computer science;artificial intelligence;machine learning;hyper-heuristic	Robotics	22.86140274220793	-0.634514117725852	5590
bf856bd3811fb280ac2b0b7a9a52740d5c6c622a	coupled dictionaries for thermal to visible face recognition	stochastic gradient descent coupled dictionaries thermal face recognition visible face recognition thermal infrared face image thermal ir face image visible light face images sparse representation single domain independent latent space dictionary learning problem bi level optimization problem;optimisation face recognition gradient methods image representation infrared imaging;dictionaries face recognition training face support vector machines optimization probes	Thermal to visible face recognition is the problem of identifying a thermal infrared (IR) face image given a gallery of visible light face images. We attempt to solve this problem by learning coupled dictionaries to represent the two domains. The dictionaries provide a sparse representation which transforms the data into a single, domain-independent, latent space. We formulate the dictionary learning problem as a bi-level optimization problem and perform a stochastic gradient descent on the dictionaries to solve it. We present experimental results demonstrating the effectiveness of our approach.	black and burst;dictionary;facial recognition system;machine learning;mathematical optimization;optimization problem;sparse approximation;sparse matrix;stochastic gradient descent	Christopher Reale;Nasser M. Nasrabadi;Rama Chellappa	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025065	computer vision;computer science;machine learning;pattern recognition	Vision	25.448322548716188	-44.76385545691623	5592
9dcf8fcdfd1a065ff91e747c54928eb65fe40e8b	grid based variational approximations	variational approximation;bayes estimation;calculo de variaciones;analisis numerico;chaine markov;cadena markov;approximation laplace;metodo monte carlo;theorie approximation;65c05;laplace approximation;analisis datos;stochastic method;divergence;methode approchee;bayesian inference;62e17;65kxx;methode monte carlo;metodo aproximado;optimization method;approximate method;maillage;distribucion estadistica;65k10;metodo optimizacion;kullback liebler divergence;65c40;approximate bayesian inference;analyse numerique;estimation parametrique;approximation theory;data analysis;calcul variationnel;estimacion bayes;numerical analysis;posterior distribution;celdarada;distribution statistique;marginal distribution;markov chain monte carlo;mathematical programming;62f15;monte carlo method;statistical computation;calculo estadistico;49r50;methode optimisation;ley a posteriori;ley marginal;methode stochastique;grid pattern;analyse donnee;calcul statistique;bayesian inference variational approximation kullback liebler divergence markov chain monte carlo;60j10;variational method;programmation mathematique;loi a posteriori;programacion matematica;statistical distribution;variational calculus;loi marginale;divergencia;estimation bayes;markov chain;metodo estocastico	Variational methods for approximate Bayesian inference provide fast, flexible, deterministic alternatives to Monte Carlo methods. Unfortunately, unlike Monte Carlo methods, variational approximations cannot, in general, be made to be arbitrarily accurate. This paper develops grid-based variational approximations which endeavor to approximate marginal posterior densities in a spirit similar to the Integrated Nested Laplace Approximation (INLA) of Rue, Martino & Chopin (2009) but may be applied in situations where INLA cannot be used. The method can greatly increase the accuracy of a base variational approximation, although not in general to arbitrary accuracy. The methodology developed is at least reasonably accurate on all of the examples considered in the paper.	approximation algorithm;calculus of variations;eternal sonata;marginal model;monte carlo method;variational principle	John T. Ormerod	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.04.024	probability distribution;marginal distribution;econometrics;markov chain;markov chain monte carlo;numerical analysis;variational method;calculus;mathematics;laplace's method;posterior probability;data analysis;divergence;bayesian inference;statistics;calculus of variations;monte carlo method;approximation theory	ML	33.74963301730576	-23.331505075315317	5594
d8ea288c6d4e14df2895f422402e377c603b687d	rolling horizon non-myopic scheduling of multifunction radar for search and track	silicon;radar tracking;surveillance;schedules;radar tracking target tracking silicon schedules search problems surveillance;search problems;tracking sensor management radar surveillance search;target tracking;target tracking adaptive scheduling radar receivers radar tracking search radar;target searching multifunction radar rolling horizon nonmyopic scheduling adaptive scheduling approach multiple time step look ahead nonmyopic scheduling problem multifunction electronically scanned radar target tracking	New developments in sensor technologies have enabled more efficient and effective application of resources. Sensors that are agile and support multiple modes of operation require adaptive scheduling approaches in order to yield performance benefits compared to fixed off-line approaches. User objectives for different types of tasks must be carefully defined in order to ensure appropriate allotment of sensor resources. Myopic scheduling approaches concentrate on present sensing actions only, while non-myopic scheduling approaches concentrate on present and future actions. In this paper, two different algorithms are developed for the rolling horizon, or multiple time step look-ahead, non-myopic scheduling problem. An example of a multifunction electronically scanned radar that is tasked to (i) search for targets and (ii) update tracks on targets is used to show how the algorithms can be applied in practice.	adaptive filter;agile software development;algorithm;block cipher mode of operation;greedy algorithm;horizon effect;loss function;mathematical optimization;multi-function printer;newton;newton's method;online and offline;phased array;radar;scheduling (computing);sensor;simulation	Marion Byrne;Kruger White;Jason Williams	2016	2016 19th International Conference on Information Fusion (FUSION)		man-portable radar;electronic engineering;radar tracker;real-time computing;simulation;engineering;fire-control radar	Robotics	37.354685082878056	0.2549146899162831	5600
b6f30b14afd609b24a770e7f73744ce30a4d840b	fpga based real-time data processing daq system for the mercury imaging x-ray spectrometer	x ray spectroscopy calibration data acquisition data reduction field programmable gate arrays x ray detection x ray imaging;x ray imaging;real time data processing;fpga;x ray spectroscopy;mixs;depfet;depfet fpga daq real time data processing x ray mixs;data reduction;detectors data acquisition field programmable gate arrays real time systems application specific integrated circuits data analysis;field programmable gate arrays;optimal evaluation platform fpga based real time data processing daq system mercury imaging x ray spectrometer x ray detector system data processing algorithms serialised input data stream detector recalibration cycles dynamic adaptation autonomous system operation low memory bandwidth hardware resource utilisation data reduction mixs detector system bepicolombo esa corner stone mission x ray detection;x ray;x ray detection;daq;data acquisition;calibration	This paper presents a new DAQ system with real-time data processing capability for X-ray detector systems. The data processing algorithms are working directly on the serialised input data stream from the detector system and run in real-time on a FPGA processing system. The DAQ system includes new algorithms for dynamic adaptation of important processing parameters during a measurement. These dynamic algorithms enable better measurement precisions, autonomous system operation and avoid detector recalibration cycles. The developed and presented data processing algorithms are optimised for low memory bandwidth and hardware resource utilisation. High data reduction and live monitoring of important system parameters are achieved with the real-time processing. The concept of the new DAQ system was tested and evaluated with the Mercury Imaging X-ray Spectrometer (MIXS). MIXS is an instrument on board of the ESA corner stone mission BepiColombo which will be launched 2015 into an orbit around Mercury. The 64x64 detector matrix used in the MIXS detector system for X-ray detection delivers 6000 frames per second and provides an optimal evaluation platform for the new DAQ system.	algorithm;autonomous robot;autonomous system (internet);dynamic problem (algorithms);esa;field-programmable gate array;memory bandwidth;mercury;real-time clock;real-time computing;real-time data;real-time operating system;real-time transcription;semiconductor;sensor;sound quality;uncompressed video;x-ray detector	Florian Aschauer;Walter Stechele;Johannes Treis	2013	2013 Euromicro Conference on Digital System Design	10.1109/DSD.2013.63	embedded system;real-time computing;computer science;x-ray spectroscopy;data acquisition;field-programmable gate array	Embedded	44.098364470698236	-3.1253970124118995	5608
ed886bd6bfc270c64b1909e14d7bf871f48118a3	will a supplier benefit from sharing good information with a retailer?	returns policy;forecast updating;supply chain management	"""Information sharing has been known to be crucial in supply chain management. Prior empirical finding reveals that suppliers in practice tend to help their trading partners improve forecast accuracy. This paper examines this issue and explores the up-down (from an upstream supplier to a downstream retailer) strategic information sharing issues in a two-echelon supply chain. We first model a supply chain with forecast updating and returns policy. The forecast updating scheme adopts the Bayesian approach with unknown mean and unknown variance. We then proceed to analytically explore the effects of forecast updating on the supplier and the retailer. Our analysis has revealed that: 1. Demand information with low relevance can lead to a loss to the retailer. 2. In the absence of returns policy, the supplier has an incentive to provide """"bad information"""" which may be harmful to the retailer. 3. The supplier will provide """"good information"""" to the retailer only under the returns policy. 4. With up-down information sharing, win-win coordination can be achieved by using a proper returns policy. Many of these results can supplement and challenge the prior research findings that supplier has good incentive to help retailers in improving forecast. Demand information with low relevance can lead to a loss to the retailer.A supplier has incentive to provide """"bad information"""" to the retailer in the absence of returns policy.Reveal that a supplier will provide """"good information"""" to a retailer under the returns policy.Coordination can be achieved by using a proper returns policy.New insights which supplement prior research findings on information sharing incentive."""		Tsan-Ming Choi;Ying Wei	2013	Decision Support Systems	10.1016/j.dss.2013.05.011	supply chain management;marketing;commerce	ECom	-3.6149199581668188	-7.95025563078352	5610
acd912d91796391dc6f09eeef7f58c49e41031e7	decision making on sustainable forest harvest production using goal programming approach (case study: iranian hyrcanian forest)		This paper aims to determine the optimal stock level in Hyrcanian forest of Iran. In this study, a goal programming techniques used to estimate the optimum stock level of different tree species considering economics, environmental and social issues. We consider multiple objectives in the process of decision making to realize the balance of maximizing annual growth, net present value, carbon sequestration and labor. We use regression analysis to develop a forest growth model using allometric functions for the quantification of carbon budget. The expected mean price was estimated to determine the net present value of forest harvesting. We use Expert knowledge to weight the goals in order to generate the optimal stock level. Results show that the total optimum stock is 0.5%lower than based on questioners. The results indicate that goal programming is a suitable methodology in this case.		Soma Etemad;Soleiman Mohammadi Limaei;Leif Olsson;Rasoul Yousefpour	2018	2018 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2018.8607503		Robotics	9.08575865211035	-5.480637230364029	5624
73fcd0b073ac55ec170ff9f7b98488fab330ea1d	a new classifier combination scheme using clustering ensemble		Combination of multiple classifiers has been shown to increase classification accuracy in many application domains. Besides, the use of cluster analysis techniques in supervised classification tasks has shown that they can enhance the quality of the classification results. This is based on the fact that clusters can provide supplementary constraints that may improve the generalization capability of the classifiers. In this paper we introduce a new classifier combination scheme which is based on the Decision Templates Combiner. The proposed scheme uses the same concept of representing the classifiers decision as a vector in an intermediate feature space and builds more representatives decision templates by using clustering ensembles. An experimental evaluation was carried out on several synthetic and real datasets. The results show that the proposed scheme increases the classification accuracy over the Decision Templates Combiner, and other classical classifier combinations methods.		Miguel A. Duval;Joan Sosa-García;Alejandro Guerra-Gandón;Sandro Vega-Pons;José Ruiz-Shulcloper	2012		10.1007/978-3-642-33275-3_19	margin classifier;correlation clustering;pattern recognition;cluster analysis	AI	11.846606425248403	-42.71027724822996	5636
b30247bc0e9ea6bcb1b96e2819da99c00ffb0fbf	the game analysis of eco-efficiency driven restructuring of china's coal industry	investments;game theory;supervise game;game analysis;mixed strategy equilibrium;coal enterprises;mixed strategy equilibrium eco efficiency china coal industry structure supervise game model local government coal enterprises;policy implementation;local government;coal industry;coal industry structure;supervise game model;fuel processing industries;fuel processing industries local government production investments games;games;industry restructuring;government policies;coal;production;local government coal fuel processing industries game theory government policies;mixed strategy equilibrium coal industry industry restructuring eco efficiency supervise game;eco efficiency;china;industry structure;mixed strategy	Based on the analysis of affecting factors of the coal industry structure, the method for taking eco-efficiency as the standard evaluation criterion of coal enterprises is proposed. The eco-efficiency supervise game model is established to analyze local governments’ action and coal enterprises’ action in the process of policy implementation. In the end, some strategy is given based on the analysis for the game’s mixed strategy equilibrium.	optimizing compiler;yang	Peng Yi	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.1326	game theory;coal;economics;operations management;market economy;economic system	Robotics	3.619591150495634	-8.815829069890825	5641
c9a747957f8ef16a10a684533ca63986b7e13ddd	new results for provable dynamic robust pca		Robust PCA (RPCA) is the problem of separating a given data matrix into the sum of a sparse matrix and a low-rank matrix. Dynamic RPCA assumes that the true data vectors lie in a low-dimensional subspace that can change with time, albeit slowly. The goal is to track this changing subspace over time in the presence of sparse outliers. This work provides the first guarantee for dynamic RPCA that holds under (weakened) standard RPCA assumptions and a realistic model of slow subspace change. We analyze an existing method called ReProCS. Our result removes the strong assumptions needed by the two previous complete guarantees for ReProCS. Both these required an unrealistic model of subspace change and very specific assumptions on how the outlier support could change. Most importantly, our guarantees show that, because it exploits slow subspace change, ReProCS (and its offline counterpart) can provably tolerate much larger outlier fractions, are faster than most other provable methods, and have near-optimal storage complexity.	online and offline;provable security;sparse matrix	Praneeth Narayanamurthy;Namrata Vaswani	2017	CoRR			ML	26.557370725554065	-36.14408341024984	5660
cf80b1372f7ac1b528a8916e331031ea463951cf	noise statistics oblivious gard for robust regression with sparse outliers		Linear regression models contaminated by Gaussian noise (inlier) and possibly unbounded sparse outliers are common in many signal processing applications. Sparse recovery inspired robust regression (SRIRR) techniques are shown to deliver high-quality estimation performance in such regression models. Unfortunately, most SRIRR techniques assume a priori knowledge of noise statistics like inlier noise variance or outlier statistics like number of outliers. Both inlier and outlier noise statistics are rarely known a priori, and this limits the efficient operation of many SRIRR algorithms. This paper proposes a novel noise statistics oblivious algorithm called residual ratio thresholding GARD (RRT-GARD) for robust regression in the presence of sparse outliers. RRT-GARD is developed by modifying the recently proposed noise statistics dependent greedy algorithm for robust denoising (GARD). Both finite sample and asymptotic analytical results indicate that RRT-GARD performs nearly similar to GARD with a priori knowledge of noise statistics. Numerical simulations in real and synthetic data sets also point to the highly competitive performance of RRT-GARD.		Sreejith Kallummil;Sheetal Kalyani	2019	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2883025	robust regression;regression analysis;gaussian noise;statistics;noise reduction;outlier;linear regression;synthetic data;mathematics;greedy algorithm	ML	30.26414321256502	-29.443733224442223	5696
a0add5fd170c65a1806841aa3ffd6f0b4dffdd77	academic co-author networks based on the self-organizing feature map	h index academic coauthor networks self organizing feature map algorithms k means algorithms som algorithms pattern area number;self organising feature maps pattern clustering;clustering algorithms neurons vectors algorithm design and analysis organizing prototypes self organizing feature maps;weight influential researcher co author networsk som k means	This paper studies mathematician Paul Erdös, one of the most famous academic co-authors and his large co-author network. K-means and Self-Organizing feature Map (SOM) algorithms are applied to study the network. First, the SOM algorithm is introduced to recognize the pattern area number, which can identify the sub-segment automatically and export the central point of each cluster as well as the weights. Taking the results as the initial input of the K-means algorithm to make the further clustering, the accurate clustering results are gained. Then search the largest weight node of each cluster and set it as the most influential researcher. Finally we compared the h-index of the most influential researcher with the corresponding weight node of the cluster, the results confirm that the algorithm is better than SOM and K-means algorithms when they are separately used.	algorithm;cluster analysis;k-means clustering;organizing (structure);paul brainerd;self-organization;self-organizing map	Gege Zhang;Weixing Zhou;Yuanyuan Zhang;Xiaohui Hu;Yun Xue;Jianping Wang;Meihang Li	2014	2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2014.6980858	computer science;machine learning;pattern recognition;data mining	ML	2.7246941393652695	-42.90812669712188	5697
bb22d91eadaa99a88829fb01181e0f5a509cfe5b	profit maximization problem with coupons in social networks		Viral marketing has become one of the most effective marketing strategies. In the process of real commercialization, in order to let some seed individuals know the products, companies can provide free samples to them. However, for some companies, especially famous ones, they are more willing to offer coupons than give samples. In this paper, we consider the Profit Maximization problem with Coupons (PM-C) in our new diffusion model named the Independent Cascade Model with Coupons and Valuations (IC-CV). To solve this problem, we propose the PMCA algorithm which can return a ((frac{1}{3}-varepsilon ))-approximate solution with at least (1-2n^{-l}) probability, and runs in (O(log (np)cdot m n^3log n(llog n +nlog 2)/varepsilon ^3)) expected time. Further more, during the analysis we provide a method to estimate the non-monotone submodular function.	expectation–maximization algorithm;social network	Bin Liu;Xiao Li;Huijuan Wang;Qizhi Fang;Junyu Dong;Weili Wu	2018		10.1007/978-3-030-04618-7_5	combinatorics;submodular set function;approximation algorithm;viral marketing;computer science;social network;binary logarithm;profit maximization	ECom	-1.076202551371667	-0.27883138449393996	5698
ae753fd46a744725424690d22d0d00fb05e53350	describing clothing by semantic attributes	attribute classifier;automated system;clothing appearance;conditional random field;human body;complementary feature;semantic attribute;challenging clothing attribute dataset;appealing technique;nameable attribute	Describing clothing appearance with semantic attributes is an appealing technique for many important applications. In this paper, we propose a fully automated system that is capable of generating a list of nameable attributes for clothes on human body in unconstrained images. We extract low-level features in a pose-adaptive manner, and combine complementary features for learning attribute classifiers. Mutual dependencies between the attributes are then explored by a Conditional Random Field to further improve the predictions from independent classifiers. We validate the performance of our system on a challenging clothing attribute dataset, and introduce a novel application of dressing style analysis that utilizes the semantic attributes produced by our system.	conditional random field;feature extraction;ground truth;high- and low-level;kinect;mutual information;name;rule (guideline);sensor;silo (dataset)	Huizhong Chen;Andrew C. Gallagher;Bernd Girod	2012		10.1007/978-3-642-33712-3_44	machine learning;data mining	Vision	34.41660311718899	-47.75435258557305	5699
5f9ab4aec5e22a28f4deb53d73e4ca45047674f3	modelling and solving anti-aircraft mission planning for defensive missile battalions		The theater defense distribution is an important problem in the military that determines strategies against a sequence of offensive attacks in order to protect his targets. This study focuses on developing mathematical models for two defense problems that generate anti-aircraft mission plans for a group of missile battalions. While the Anti-aircraft Mission Planning problem maximizes the defender’s effectiveness using all his available battalions, the Inverse Anti-aircraft Mission Planning problem computes necessary weapon resources (battalions and their missiles) to obtain a given defensive effectiveness value. The proposed formulations are Mixed Integer Programs that describe not only the positions of missile battalions, but also engage battalions to fleets of attacking aircrafts. We additionally prove that these problems are NP-hard. A comprehensive set of experiments is then evaluated to show that these proposed programs can be applied to solve fast real-life instances to optimality.		Trang T. Nguyen;Trung Q. Bui;Bang Q. Nguyen;Su T. Le	2017		10.1007/978-3-319-71150-8_31	simulation;computer science;mathematical model;offensive;operations research;missile	Robotics	14.688376878733417	-0.15106522264352348	5702
ad58b92ebc45e71e40ce68d4375441267549b054	da-vlad: discriminative action vector of locally aggregated descriptors for action recognition		In this paper, we propose a novel encoding method for the representation of human action videos, that we call Discriminative Action Vector of Locally Aggregated Descriptors (DA-VLAD). DA-VLAD is motivated by the fact that there are many unnecessary and overlapping frames that cause non-discriminative codewords during the training process. DA-VLAD deals with this issue by extracting class-specific clusters and learning the discriminative power of these codewords in the form of informative weights. We use these discriminative action weights with standard VLAD encoding as a contribution of each codeword. DA-VLAD reduces the inter-class similarity efficiently by diminishing the effect of common codewords among multiple action classes during the encoding process. We present the effectiveness of DA-VLAD on two challenging action recognition datasets: UCF101 and HMDB51, improving the state-of-the-art with accuracies of 95.1 % and 80.1 % respectively.		Fiza Murtaza;Muhammad Haroon Yousaf;Sergio A. Velastin	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451255	discriminative model;code word;task analysis;feature extraction;pattern recognition;encoding (memory);artificial intelligence;computer science	Vision	32.045858854919686	-49.457922278677266	5709
b5b28068ecf64a276db667e0e5da31e42b9bf77d	integrating multiple uncalibrated views for human 3d pose estimation	3d pose estimation;multiple views;human subjects;belief propagation;consistency checking;pose estimation	We address the problem of how human pose in 3D can be estimated from video data. The use of multiple views has the potential of tackling self-occlusion of the human subject in any particular view, as well as of estimating the human pose more precisely. We propose a scheme of allowing multiple views to be put together naturally for determining human pose, allowing hypotheses of the body parts in each view to be pruned away efficiently through consistency check over all the views. The scheme relates the different views through a linear combination-like expression of all the image data, which captures the rigidity of the human subject in 3D. The scheme does not require thorough calibration of the cameras themselves nor the camera inter-geometry. A formulation is also introduced that expresses the multi-view scheme, as well as other constraints, in the pose estimation problem. A belief propagation approach is used to reach a final human pose under the formulation. Experimental results on in-house captured image data as well as publicly available benchmark datasets are shown to illustrate the performance of the system.	3d modeling;3d pose estimation	Zibin Wang;Ronald Chung	2010		10.1007/978-3-642-17277-9_29	computer vision;simulation;pose;3d pose estimation;computer science;data mining;articulated body pose estimation;mathematics;belief propagation	Vision	50.49429936505934	-47.5894958517698	5710
c6f765cabf03ca6f04bc9ad1af6ae65484b1ed7e	process control of minimax based on the process target and on process capability index, in fuzzy environment	spc;reliability;fuzzy set;probability;process capability analysis;statistical process control fuzzy set theory minimax techniques process capability analysis quality control reliability;statistical process control;product reliability minimax process target process capability index fuzzy environment statistics process control product quality;process capability index;fuzzy set theory;indexes;minimax techniques;process control of minimax spc process capability index fuzzy set;process control indexes production probability quality control flowcharts reliability;process control;production;quality control;flowcharts;process control of minimax	Statistics process control(SPC)is one of the crucial methods securing product quality and reliability in modern production, and process capability index is an important indicator mostly used in SPC. Process capability index is a degree in which, using the ratio of process specifications' upper limit and low limit to the actual change range (indicated by 6σ), to measure a process meeting requirement of the product capability indicators. SPC's main aim is to be able to keep comparatively small change round the given target for the process. Besides process capability index, SPC should concern process target that would not be precise. This paper shows the process control of minimax based on the process target and on process capability index, in fuzzy environment.	minimax	Kaibin Zhao	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019697	process performance index;process capability;process capability index;computer science;process control;fuzzy set;statistical process control;statistics	Robotics	14.358113272992018	-12.996140173632178	5716
be032b66a42a66bb3435d63266c4ee564c835bf7	nature-inspired chemical reaction optimisation algorithms	nature-inspired computing;biologically inspired algorithm;physics inspired algorithms;chemical reaction optimisation	Nature-inspired meta-heuristic algorithms have dominated the scientific literature in the areas of machine learning and cognitive computing paradigm in the last three decades. Chemical reaction optimisation (CRO) is a population-based meta-heuristic algorithm based on the principles of chemical reaction. A chemical reaction is seen as a process of transforming the reactants (or molecules) through a sequence of reactions into products. This process of transformation is implemented in the CRO algorithm to solve optimisation problems. This article starts with an overview of the chemical reactions and how it is applied to the optimisation problem. A review of CRO and its variants is presented in the paper. Guidelines from the literature on the effective choice of CRO parameters for solution of optimisation problems are summarised.	cognitive computing;computation (action);contract research organization;convergence (action);drug allergy;heuristic (computer science);inspiration function;iontophoresis;lam/mpi;lamivudine;license;machine learning;mathematical optimization;optimization problem;population parameter;premature convergence;programming paradigm;rna;scientific literature;tellurium;algorithm	Nazmul H. Siddique;Hojjat Adeli	2017		10.1007/s12559-017-9485-1	computer science;scientific literature;cognitive computing;chemical reaction;algorithm;artificial intelligence;population	ML	30.442534503474512	-8.574339606030348	5720
ff6360298dcb81af6efda52de9a7e2e161bd3a55	commands generation by face movements applied to the guidance of a wheelchair for handicapped people	fuzzy control;kalman filters;mobile robots;intelligent control;handicapped aids;fuzzy detector face movements handicapped people vision based commands generation system electric wheelchair severe disabilities 2d color face tracker;wheelchairs face detection skin magnetic heads tracking detectors robustness wheels velocity control electronic mail;user interfaces;fuzzy systems;intelligent control user interfaces handicapped aids mobile robots fuzzy control kalman filters fuzzy systems	This paper describes a vision-based commands generation system, by face movements, applied to the guidance of an electrical wheelchair for handicapped people with severe disabilities. Using a 2 0 color face tracker and a fuzzy detector the system computes face movements of the user and, depending on them, some commands are generated to drive the wheelchair. The system is non-intrusive and it allows visibility and freedom of head movements. It is able to learn the face movements of the user in an automatic initial setup, working even for people of different races. It is adaptive and, therefore, robust to light and background changes in inside environments. We report on some experimental results of this kind of guidance and some conclusions of its performance.		Luis Miguel Bergasa;Manuel Mazo;Alfredo Gardel Vicente;Rafael Barea;Luciano Boquete	2000		10.1109/ICPR.2000.903004	kalman filter;mobile robot;computer vision;simulation;computer science;user interface;fuzzy control system	Vision	46.08817304136218	-41.49495568060071	5736
b6b8d595140f441e2ac192a2cec40b42e1814070	artificial neural network modelling of the bioethanol-to-olefins process on a hzsm-5 catalyst treated with alkali	artificial neural networks;prediction model;bto process	In this work the kinetic modelling of the transformation of Bioethanol-To-Olefins (BTO) process over a HZSM-5 catalyst treated with alkali using Artificial Neural Networks (ANN) is presented. The main goal has been to obtain a BTO process neuronal model with the desired accuracy that allows the simplification and reduction of the computational cost with respect to a mechanistic knowledge model. To check the goodness of ANN base model structures, during the study a comparison with other alternative modelling techniques such as Support Vector Machines was performed. Following a parameters optimization procedure and testing different training methods, the optimal ANN structure results to be a Feed-Forward 3-5-1 network with the Bayesian Regularization training method. Using a set of experimental data obtained in a laboratory scale fixed bed reactor, we have obtained a similar fit to the knowledge model but with the advantage of being up to 43 times faster. These results are important for moving forward real time automatic control strategies in the biorefinery context.	algorithmic efficiency;artificial neural network;automatic control;bayesian network;computation;knowledge representation and reasoning;level of detail;mathematical optimization;matrix regularization;reactor (software);support vector machine;teaching method	Gorka Sorrosal;Eloy Irigoyen;Cruz E. Borges;Cristina Martín;Ana María Macarulla;Ainhoa Alonso-Vicario	2017	Appl. Soft Comput.	10.1016/j.asoc.2017.05.006	computer science;artificial intelligence;machine learning;predictive modelling;operations research;artificial neural network	ML	13.562245271098524	-20.434376752019425	5758
3fd3f85e8bb205447da386be30621617b0e01536	information forests	image classification;entropy	We describe Information Forests, an approach to classification that generalizes Random Forests by replacing the splitting criterion of non-leaf nodes from a discriminative one - based on the entropy of the label distribution - to a generative one - based on maximizing the information divergence between the class-conditional distributions in the resulting partitions. The basic idea consists of deferring classification until a measure of “classification confidence” is sufficiently high, and instead breaking down the data so as to maximize this measure. In an alternative interpretation, Information Forests attempt to partition the data into subsets that are “as informative as possible” for the purpose of the task, which is to classify the data. Classification confidence, or informative content of the subsets, is quantified by the Information Divergence. Our approach relates to active learning, semi-supervised learning, mixed generative/discriminative learning.	active learning (machine learning);information;random forest;semi-supervised learning;semiconductor industry;supervised learning;tree (data structure)	Zhao Yi;Stefano Soatto;Maneesh Dewan;Yiqiang Zhan	2012	2012 Information Theory and Applications Workshop	10.1109/ITA.2012.6181810	entropy;contextual image classification;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	ML	28.72157382445732	-43.65502550776364	5766
24827e41f70f728907038ada04ef8a01b86a9137	non-linear ir scene prediction for range video surveillance	range video surveillance;arbitrary motion;prediction method;linear estimation;time varying;least mean square error;least mean square;video surveillance;image motion analysis;infrared surveillance;layout video surveillance navigation infrared detectors gabor filters infrared surveillance prediction methods image sequences motion detection kalman filters;least mean squares methods;nonlinear ir scene prediction;nonlinear kalman filter;kalman filters;kalman filter;gabor filters;layout;infra red;navigation;gabor filter;prediction methods;range image;region of interest;ir range image sequence;autonomous navigation;video surveillance gabor filters image motion analysis kalman filters least mean squares methods;motion detection;heat change based range measurement;infrared detectors;least mean square error nonlinear ir scene prediction range video surveillance gabor filter bank ir range image sequence region of interest arbitrary motion nonlinear kalman filter heat change based range measurement autonomous navigation;gabor filter bank;image sequences	This paper describes a non-linear IR (infra-red) scene prediction method for range video surveillance and navigation. A Gabor-filter bank is selected as a primary detector for any changes in a given IR range image sequence. The detected ROI (region of interest) involving arbitrary motion is fed to a non-linear Kalman filter for predicting the next scene in time-varying 3D IR video. Potential applications of this research are mainly in indoor/outdoor heat-change based range measurement, synthetic IR scene generation, rescue missions, and autonomous navigation. Experimental results reported herein show that non-linear Kalman filtering-based scene prediction can perform more accurately than linear estimation of future frames in range and intensity driven sensing. The low least mean square error (LMSE), on the average of about 2% using a bank of 8 Gabor filters, also proves the reliability of the IR scene estimator (or predictor) developed in this work.	autonomous robot;closed-circuit television;filter bank;gabor filter;kalman filter;kerrison predictor;mean squared error;nonlinear system;range imaging;region of interest;synthetic intelligence	Mehmet Celenk;James Graham;Kai-Jen Cheng	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383445	kalman filter;computer vision;simulation;speech recognition;computer science;machine learning	Vision	44.03418768243059	-45.744078529749196	5791
fd0a3a57df96f0ef04c4b9465c4ad910004e7112	an improved differential evolution for parameter optimisation	parameter optimisation;differential evolution;simulation;bare bones de;adaptive crossover rate	In this paper, we propose an improved differential evolution DE to solve parameter optimisation problems. The new approach is called ICBBDE, which is an enhanced version of bare bones DE BBDE. The ICBBDE employs an adaptive strategy to dynamically adjust the crossover rate. Moreover, a Cauchy mutation is used to improve the exploration ability. Experiments are conducted on a set of benchmark functions and two real-world parameter optimisation problems. Simulation results demonstrate the efficiency and effectiveness of our approach.	differential evolution;mathematical optimization	Xiuqin Pan;Ruixiang Li	2015	IJWMC	10.1504/IJWMC.2015.070937	differential evolution;mathematical optimization;simulation;computer science;artificial intelligence;machine learning	Vision	26.91866249886859	-4.696753910004773	5798
1b69d361377c5102f6a263a4961f14af926fe748	an efficient iterated greedy algorithm for the makespan blocking flow shop scheduling problem	iterated greedy method;blocking;makespan;flow shop	We propose in this paper a Blocking Iterated Greedy algorithm (BIG) which makes an adjustment between two relevant destruction and construction stages to solve the blocking flow shop scheduling problem and minimize the maximum completion time (makespan). The greedy algorithm starts from an initial solution generated based on some well-known heuristic. Then, solutions are enhanced till some stopping condition and through the above mentioned stages. The effectiveness and efficiency of the proposed technique are deduced from all the experimental results obtained on both small randomly generated instances and on Taillard’s benchmark in comparison with state-of-the-art methods.	benchmark (computing);blocking (computing);dinic's algorithm;flow shop scheduling;greedy algorithm;heuristic (computer science);iterated function;local search (optimization);makespan;procedural generation;randomness;scheduling (computing)	Nouri Nouha;Talel Ladhari	2016	Polibits	10.17562/PB-53-10	job shop scheduling;greedy randomized adaptive search procedure;mathematical optimization;flow shop scheduling	AI	20.339490895468096	2.1662141835583038	5804
19bebdd4a7cc1158c2b0e728b624eaa571eb13cd	optimization and analisys of vaccination schedules using simulated annealing and agent based models		Vaccines represent nowadays one of the most efficient weapons against foreign pathogens. To be effective, vaccines need a proper administration strategy that requires multiple administrations in order to ensure the acquisition of immunological memory. Vaccination schedules are usually based on past experience, and economical, ethical and time constraints have limited the research for better combinations of timing and dosage. We present here a computational approach based on the use of stochastic optimization techniques and validated “in silico” models to study and optimize vaccination protocols. We use Simulated Annealing in conjunction with a validated agent based model to suggest some interventions that may improve the efficacy of a candidate vaccine composed by influenza-A virosome and natural citrus-derived adjuvants to prevent influenza-A infection.	agent-based model;mathematical optimization;simulated annealing;stochastic optimization	Marzio Pennisi;Juan A. Sanchez-Lantaron;Pedro A. Reche;Giulia Russo;Francesco Pappalardo	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217877	machine learning;agent-based model;computer science;artificial intelligence;management science;stochastic optimization;simulated annealing;schedule;virosome;vaccination	Robotics	1.8767456538221448	-47.005769573684724	5814
a77e766f13cf29d3bb88582bd9870adeb629418c	smith-waterman alignment of huge sequences with gpu in linear space	microprocessors;species peculiarity identification;parallel algorithm;gpu;high performance computing platform;parallel algorithms bioinformatics cellular biophysics coprocessors;coprocessors;smith waterman alignment;computer architecture;heuristic algorithms;myers miller algorithm;cross species chromosome alignments;linear space complexity;graphics processing unit microprocessors bioinformatics computer architecture instruction sets heuristic algorithms mathematical model;mathematical model;graphic processing unit;gtx 285 board smith waterman alignment gpu cross species chromosome alignments ancestral relationships species peculiarity identification bioinformatics high performance computing platform parallel algorithm myers miller algorithm linear space complexity;gtx 285 board;graphics processing unit;ancestral relationships;linear space;cellular biophysics;heuristic algorithm;instruction sets;bioinformatics;parallel algorithms	Cross-species chromosome alignments can reveal ancestral relationships and may be used to identify the peculiarities of the species. It is thus an important problem in Bioinformatics. So far, aligning huge sequences, such as whole chromosomes, with exact methods has been regarded as unfeasible, due to huge computing and memory requirements. However, high performance computing platforms such as GPUs are being able to change this scenario, making it possible to obtain the exact result for huge sequences in reasonable time. In this paper, we propose and evaluate a parallel algorithm that uses GPU to align huge sequences, executing the Smith-Waterman algorithm combined with Myers-Miller, with linear space complexity. In order to achieve that, we propose optimizations that are able to reduce significantly the amount of data processed and that enforce full parallelism most of the time. Using the GTX 285 Board, our algorithm was able to produce the optimal alignment between sequences composed of 33 Millions of Base Pairs (MBP) and 47 MBP in 18.5 hours.	align (company);bioinformatics;dspace;geforce 200 series;graphics processing unit;million book project;parallel algorithm;parallel computing;requirement;smith–waterman algorithm;supercomputer	Edans Flavius de Oliveira Sandes;Alba Cristina Magalhaes Alves de Melo	2011	2011 IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2011.114	parallel computing;computer science;bioinformatics;theoretical computer science;operating system;distributed computing;parallel algorithm;programming language;algorithm	HPC	-1.9733704033251085	-51.78231193112009	5829
1e9c65c4ac982e6b2347296d83d806e767ed33ba	comparison of or and ai methods in discrete manufacturing using fuzzy logic	production system;discrete manufacturing;operations research;classification;production process;artificial intelligent;fuzzy logic;artifical intelligent;manufacturing system;system management	Since 1950s the techniques of Operations Research (OR) and Optimization have been utilized to increase the efficiency of the production systems. With the widespread use of computers, it has even become easier to deal with industrial problems. However the complexity of the problems still reveals the difficulty in providing solutions. The use of artificial intelligence (AI) seems to attract the attention of the researcher to overcome to the difficulties. This has already been realized with several successful applications. In this study, the use of AI and OR techniques is compared using fuzzy logic. The progress of manufacturing systems, characteristics of production processes, system managements and system behavior are taken into account. The study is focussed on only discrete manufacturing.	discrete manufacturing;fuzzy logic	Cemalettin Kubat;Harun Taskin;Bayram Topal;Safiye Turgay	2004	J. Intelligent Manufacturing	10.1023/B:JIMS.0000034115.63358.7e	fuzzy logic;fuzzy electronics;systems management;biological classification;computer science;engineering;artificial intelligence;scheduling;computer-integrated manufacturing;production system;manufacturing engineering	AI	6.801762759239521	-23.857728103767396	5836
c487089d6db6c227c5ad10cbbb07b4adc2856cce	unsupervised learning of neural network ensembles for image classification	unsupervised learning;electronic mail;multisensor remote sensing images;neural networks;neural nets;neural network ensemble;unsupervised learning neural networks image classification training data pattern recognition design methodology electronic mail remote sensing voting image sensors;image classification;image sensors;training data;voting;remote sensing;pattern recognition;sensor fusion image classification unsupervised learning neural nets;sensor fusion;multisensor remote sensing images unsupervised learning neural network ensembles image classification pattern recognition error independent networks;high performance;neural network;error independent networks;design methodology;neural network ensembles	"""In the field of pattern recognition, the combination of an ensemble of neural networks has been proposed as an approach to the development of high performance image classification systems. However, previous work clearly showed that such image classification systems are effective only if the neural networks forming them make different errors. Therefore, the fundamental need for methods aimed to design ensembles of """"error-independent” networks is currently acknowledged. In this paper, an approach to the automatic design of effective neural network ensembles is proposed. Given an initial large set of neural networks, our approach is aimed to select the subset formed by the most error-independent nets. Reported results on the classification of multisensor remote-sensing images show that this approach allows one to design effective neural network ensembles."""	artificial neural network;computer vision;pattern recognition;unsupervised learning	Giorgio Giacinto;Fabio Roli;Giorgio Fumera	2000		10.1109/IJCNN.2000.861297	training set;contextual image classification;voting;design methods;computer science;machine learning;pattern recognition;image sensor;data mining;time delay neural network;sensor fusion;deep learning;artificial neural network	AI	32.11281164789611	-43.98462879618111	5842
57b33c1dcadd77083bea7acb25f64e1cef579443	comparison of discriminatory pricing and uniform pricing rules in electricity markets using an agent model with risk consideration	agent based simulation;agent modeling;pricing;standard deviation;market dynamics;electricity market;pricing power markets;power markets;weighted sums;expected returns;scheduled dispatch quantity discriminatory pricing rules uniform pricing rules electricity markets agent model agent based simulation market dynamics market price;profitability;pricing electricity supply industry reactive power risk management measurement standards power generation fluctuations performance evaluation costs learning	Agent-based simulation is widely applied in modeling generators' bid behavior and analyzing market dynamics in electricity markets. A generator agent's profit depends on market price and scheduled dispatch quantity, both of which are uncertain due to competition among generators and demand fluctuation. An agent model with risk consideration is proposed where risk with respect to a bid action is measured as standard deviation or VaR of the profit obtained after performing the action. Instead of solely expected return, the weighted sum of expected return and risk is thereby defined as a bid action's value based on which actions are evaluated and selected. The model is applied to market simulation with discriminatory pricing and uniform pricing rules. Our experiment demonstrated that uniform pricing leads to higher variance in market price than discriminatory pricing when demand is high and uncertain.	agent-based model;agent-based social simulation;dynamic dispatch;experiment;quantum fluctuation;risk management;universal conductance fluctuations;weight function	Guilan Zhi;Shigeyoshi Watanabe	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424791	financial economics;pricing;variable pricing;investment theory;arbitrage pricing theory;electricity market;consumption-based capital asset pricing model;rational pricing;pricing schedule;standard deviation;security market line;statistics;profitability index	AI	1.8149059144609605	-6.740802972295769	5846
e8d1427d34a5e80a4baa25ed30fd191c54bc7598	programming robots with associative memories	robot learning;44 instrumentation including nuclear and particle detectors;maps;robot programming self organizing feature maps orbital robotics space exploration supervised learning computer science mathematics laboratories impedance application software;nonexplicit model robot programming associative memories robot learning techniques lazy learning approach self organizing maps;lazy learning;self organising feature maps;robots;associative memory;self organising feature maps robot programming content addressable storage;self organized map;behavior;instrumentation including nuclear and particle detectors;content addressable storage;programming;robot programming	Today, there are several drawbacks that impede the necessary and much needed use of robot learning techniques in real applications. First, the time needed to achieve the synthesis of any behavior is prohibitive. Second, the robot behavior during the learning phase is – by definition – bad, it may even be dangerous. Third, except within the lazy learning approach, a new behavior implies a new learning phase. We propose in this paper to use self-organizing maps to encode the non explicit model of the robot-world interaction sampled by the lazy memory, and then generate a robot behavior by means of situations to be achieved, i.e., points on the self-organizing maps. Any behavior can instantaneously be synthesized by the definition of a goal situation. Its performance will be minimal (not evidently bad) and will improve by the mere repetition of the behavior.	encode;lazy evaluation;lazy learning;organizing (structure);robot learning;self-organization;self-organizing map	Claude F. Touzet	1999		10.1109/IJCNN.1999.832705	robot learning;computer vision;programming;computer science;artificial intelligence;machine learning;behavior	Robotics	16.886495036574516	-21.148175119699197	5847
5cc11a953f85d5913465cdc94a142b3f2f7753a4	reconstruction of human motion trajectories to support human gait analysis in free moving subjects		Understanding of human motion is based on analysis of complex motion patterns and requires systematic study of body mechanics. The technologies used to support analysis of human locomotion has advanced dramatically over past decades and were efficiently applied also in many clinical research studies to help clinicians recognize their patients’ motion related health problems. Computerized motion capture systems are used to asses human kinematics in quantitative way. Such systems usually track motion of small either passive or active markers attached to the patients’ bodies and offer highly accurate measurements. However, they can not by used out of the laboratory because of sophisticated equipment or they are to expensive to be widely used. Therefore, there are efforts to derive kinematics directly from video records where no special apparel is needed. In our work, the digital image processing techniques were used to develop algorithms for automatic detection of human motion trajectories. Based on these algorithms, the marker-free analysis system has been created and tested in analysis of human gait.	gait analysis	Jaroslav Majerník	2015		10.1007/978-3-319-16844-9_4	computer vision	Robotics	41.32016884816515	-39.22324780882421	5850
fd44f4cbad2e0cc6ed11cf4bf590b25d10de3ed5	on the pattern recognition of verhulst-logistic itô processes in market price data.	artificial intelligent;geometric brownian motion;pattern recognition;hough transform	We introduce a highly error resistant method of extracting Itô processes as applied to market data. This method is inspired by an AI method known as Hough transforms (HT). The HT method has been used in extracting geometric shape patterns from noisy and corrupted image data. We use this method to extract simultaneously logistic geometric Brownian motion trends from simulated price histories data. It turns out that this approach is an effective method of extracting market processes for both simulated and real-world market price data.	brownian motion;effective method;hough transform;pattern recognition;simulation	Silas Onyango	2007			computer vision;computer science;machine learning;pattern recognition	Vision	5.1627460485324725	-17.409632327539356	5851
680b00595dc45273a8abd5d9d025a23855f4e777	clear box evaluation of vision algorithms application to the design of a new color region growing segmentation for robotics	image segmentation;image processing;real time;mobile robots;testing;satisfiability;region segmentation;navigation;color segmentation;robot vision;evaluation methodology;machine vision;feature extraction;evaluation criteria;pixel;region growing;robot vision systems;algorithm design and analysis;algorithm design and analysis robot vision systems image segmentation mobile robots testing pixel machine vision navigation feature extraction image processing	We propose a Clear Box Evaluation Methodology. Its goal is to create new region segmentations from the principles of well known ones. We illustrated this by an example of creation of a new color segmentation for robotic vision applications. First, we present the evaluation criteria used for these specific applications. For this purpose, four well known algorithms are evaluated. The results of this evaluation are the outlines of the new created segmentation. Then, the new created method is detailed and evaluated. The quality of the obtained segmentation is widely sufficient for robotic applications and its implementation satisfy the real time constraint (i.e. the video rate).	algorithm;common criteria;image segmentation;region growing;robot	Aymeric de Cabrol;Patrick Bonnin;Maryline Chetto;Vincent Hugel;Pierre Blazevic	2005	Proceedings of the Eighth International Symposium on Signal Processing and Its Applications, 2005.	10.1109/ISSPA.2005.1581056	mobile robot;algorithm design;computer vision;navigation;simulation;machine vision;image processing;feature extraction;computer science;machine learning;region growing;software testing;image segmentation;scale-space segmentation;pixel;satisfiability;computer graphics (images)	Robotics	46.10211748953085	-44.130857996443396	5853
3043aa3e094ae24620290d882b3ccf8d3b82e649	improving robustness of monocular vt&r system with multiple hypothesis		Visual Teach and Repeat (VT&R) has proven to be an important ingredient for mobile robot navigation. For VT&R, visual localization on a known map is a challenging task, especially in the case of motion jitter, feature-poor scenes and occlusion. State-of-the-art feature-based localization or SLAM algorithms sometimes may fail to overcome these challenges, and, as a result, suffer from tracking loss. To solve the problem of tracking loss in monocular-SLAM-based VT&R, we propose a particle filter (PF) based algorithm, which can provide robust location estimation even under challenging conditions. Our experiments verify the ability of our proposed PF-VT&R method.	algorithm;emoticon;experiment;mobile robot;particle filter;robotic mapping;simultaneous localization and mapping	Xubin Lin;Weinan Chen;Li He;Yisheng Guan;Guanfeng Liu	2017	2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2017.8324443	control theory;computer vision;robustness (computer science);simultaneous localization and mapping;mobile robot navigation;monocular;particle filter;jitter;engineering;artificial intelligence	Robotics	53.11979303545201	-39.87496293814986	5860
e1d58d1551b8c98a742e9178fa196335dfc3a468	multi-relation octomap based heuristic icp for air/surface robots cooperation	robot vision feature extraction image registration image resolution iterative methods octrees;iterative closest point multirelation octomap based heuristic icp air surface robots cooperation outdoor large scale 3d point clouds view of point feature based algorithm octree based multiresolution heuristic icp marked features hybrid icp algorithms outdoor riverside environment point based registration algorithm;iterative closest point algorithm robots covariance matrices three dimensional displays octrees standards shape	In this paper, we focus on the problem of fast and accurate featureless registration of outdoor large scale 3D point-clouds which possess great differences in the aspects of both resolution and view of point. There are two main methods generally used to solve this problem: feature based algorithm and point based one. However, feature based method can only be used in very special environments with clear geometric structure, while traditional point based method can only obtain a relative coarse estimation and is sensitive to initial alignment. Thus, in this paper, a registration algorithm, called Octree Based Multiresolution Heuristic ICP, is proposed. Without relying on the good initial registration and marked features, hybrid-ICP combines different ICP algorithms, and improve the alignments using finer levels of representation. In our outdoor riverside environments experiments, our method outperform the classical point based registration algorithm with an accuracy of 7 times better than classical Generalized-ICP and a speedup 1.6 times.	algorithm;experiment;heuristic (computer science);octree;point set registration;robot;speedup	Peng Yin;Yuqing He;Feng Gu;Jianda Han	2015	2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2015.7418987	computer vision;mathematical optimization;simulation;iterative closest point	Robotics	50.156370205268125	-50.813414915726824	5880
4bc9a767d7e63c5b94614ebdc24a8775603b15c9	understanding visual information: from unsupervised discovery to minimal effort domain adaptation	inf 01 informatica	Visual data interpretation is a fascinating problem which has received an increasing attention in the last decades. Reasons for this growing trend can be found within multiple interconnected factors, such as the exponential growth of visual data (e.g. images and videos) availability, the consequent demand for an automatic way to interpret these data and the increase of computational power. In a supervised machine learning approach, a large effort within the research community has been devoted to the collection of training samples to be provided to the learning system, resulting in the generation of very large scale datasets. This has lead to remarkable performance advances in tasks such as scene recognition or object detection, however, at a considerable high cost in terms of human labeling effort. In light of the labeling cost issue, together with the dataset bias one, another significant research direction was headed towards developing methods for learning without or with a limited amount of training data, by leveraging instead on data properties like intrinsic redundancy, time constancy or commonalities shared among different domains. Our work is in line with this last type of approach. In particular, by covering different case scenarios from dynamic crowded scenes to facial expression analysis we propose a novel approach to overcome some of the state-ofthe-art limitations. Based on the renowned bag of words (BoW) approach, we propose a novel method which achieves higher performances in tasks such as learning typical patterns of behaviors and anomalies discovery from complex scenes, by considering the similarity among visual words in the learning phase. We also show that including sparsity constraints can help dealing with noise which is intrinsic to low level cues extracted from complex dynamic scenes. Facing the so called dataset bias issue, we propose a novel method for adapting a classifier to a new unseen target user without the need of acquiring additional labeled samples. We prove the effectiveness of this method in the context of facial expression analysis showing that our method achieves higher or comparable performance to the state of the art, at a drastically reduced time cost.	bag-of-words model;cyclic redundancy check;domain adaptation;image noise;machine learning;object detection;performance;sparse matrix;statistical classification;supervised learning;time complexity	Guangpu Zen	2015			computer science;artificial intelligence;machine learning;data mining	Vision	28.409850007959346	-50.752756763564264	5899
27fd778840e8c3c8ff7a92a6d196115fbf9398af	comparative analysis of evolutionary fuzzy models for premises valuation using keel	comparative analysis;real estate;real estate appraisal;statistical test;keel;genetic fuzzy systems;prediction accuracy;fuzzy model	The experiments aimed to compare evolutionary fuzzy algorithms to create models for the valuation of residential premises were conducted using KEEL. Out of 20 algorithms divided into 5 groups to final comparison five best were selected. All models were applied to actual data sets derived from the cadastral system and the registry of real estate transactions. A dozen of predictive accuracy measures were employed. Although statistical tests were not decisive, final evaluation of models could be done on the basis of the measures used.	algorithm;experiment;value (ethics)	Marek Krzystanek;Tadeusz Lasota;Bogdan Trawinski	2009		10.1007/978-3-642-04441-0_73	qualitative comparative analysis;statistical hypothesis testing;artificial intelligence;data mining;keel;real estate	ML	3.171665727589373	-17.30523907866756	5906
12a0d8db5b0da5cff52758d7f68bc1e327d0f0cb	engineering optimization using interior search algorithm	mirrors;engineering optimization interior search algorithm global optimization;mirror search global optimization algorithm interior search algorithm engineering optimization problems isa search operators composition optimization;gears;search problems optimisation;linear programming;optimization;algorithm design and analysis;pistons;concrete;optimization mirrors algorithm design and analysis gears pistons concrete linear programming	A new global optimization algorithm, the interior search algorithm (ISA), is introduced for solving engineering optimization problems. The ISA has been recently proposed and has two new search operators, composition optimization and mirror search. In the current study, the optimization process starts with composition optimization and linearly switches to mirror search. For validation against engineering optimization problems, ISA is applied to several benchmark engineering problems reported in the literature. The optimal solutions obtained by ISA are better than the best solutions obtained by the other methods representative of the state-of-the-art in optimization algorithms.	benchmark (computing);global optimization;mathematical optimization;network switch;search algorithm	Amir Hossein Gandomi;David A. Roke	2014	2014 IEEE Symposium on Swarm Intelligence	10.1109/SIS.2014.7011771	beam search;probabilistic-based design optimization;discrete optimization;optimization problem;mathematical optimization;multi-swarm optimization;engineering optimization;multidisciplinary design optimization;test functions for optimization;meta-optimization;combinatorial optimization;search-based software engineering;derivative-free optimization;local search;hill climbing;mathematics;continuous optimization;vector optimization;engineering drawing;algorithm;3-opt;metaheuristic;global optimization	AI	25.61956118397772	-1.479593169168636	5932
bbfd2f1998d242fce0760ab1c70c9d5cb54c1e63	biologically inspired obstacle avoidance - a technology independent paradigm	extremely high frequency;front end;sensors;very large scale integration;data fusion;inspection;process monitoring;obstacle avoidance;paradigm shift;gallium arsenide;millimeter wave;infrared;vision;autonomous robot	With regard to obstacle avoidance, a paradigm shift from technology centered solutions to technology independent solutions is taking place. This trend also gives rise to a shift from function specific solutions to multifunctional solutions. A number of existing approaches will be reviewed and a case study of a biologically inspired insect vision model will be used to illustrate the new paradigm. The insect vision model leads to the realization of a sensor that is low in complexity, high in compactness, multifunctional and technology independent. Technology independence means that any front end technology, resulting in either optical, infrared or mm wave detection, for example, can be used with the model. Each technology option can be used separately or together with simple data fusion. Multifunctionality implies that the same system can detect obstacles, perform tracking, estimate time-to-impact, estimate bearing etc. and is thus non-function specific. Progress with the latest VLSI realization of the insect vision sensor is reviewed and gallium arsenide is proposed as the future medium that will support a multifunctional & multitechnology fusion of optical, infrared, millimeter wave etc. approaches. Applications are far reaching and include autonomous robot guidance, automobile anti-collision warning, IVHS, driver alertness warning, aids for the blind, continuous process monitoring/web inspection and automated welding, for example.	autonomous robot;multi-function printer;obstacle avoidance;programming paradigm;robot welding;very-large-scale integration	Derek Abbott;Andre Yakovleff;Alireza Moini;X. Thong Nguyen;Andrew J. Blanksby;Richard Beare;Andrew Beaumont-Smith;Gyudong Kim;Abdesselam Bouzerdoum;Robert E. Bogner;Kamran Eshraghian	1995		10.1117/12.228964	computer vision;simulation;telecommunications;artificial intelligence;extremely high frequency;optics;physics	Robotics	51.80083239414912	-32.69085551165414	5965
0bc3b6d7e79c8eaf67878c9ffce284289605ed39	towards understanding the predictability of stock markets from the perspective of computational complexity	trading strategy;computer simulation;computational complexity;polynomial time	This paper initiates a study into the century-old issue of market predictability from the perspective of computational complexity. We develop a simple agent-based model for a stock market where the agents are traders equipped with simple trading strategies, and their trades together determine the stock prices. Computer simulations show that a basic case of this model is already capable of generating price graphs which are visually similar to the recent price movements of high tech stocks. In the general model, we prove that if there are a large number of traders but they employ a relatively small number of strategies, then there is a polynomial-time algorithm for predicting future price movements with high accuracy. On the other hand, if the number of strategies is large, market prediction becomes complete in two new computational complexity classes CPP and BCPP, where PNP[&Ogr;(log n)] e BCPP e CPP = PP. These computational completeness results open up a novel possibility that the price graph of a actual stock could be sufficiently deterministic for various prediction goals but appear random to all polynomial-time prediction algorithms.	agent-based model;algorithm;complexity class;computational complexity theory;computer simulation;consistent pricing process;pp (complexity);polynomial;time complexity;traders	James Aspnes;David F. Fischer;Michael J. Fischer;Ming-Yang Kao;Alok Kumar	2001			computer simulation;time complexity;mathematical optimization;combinatorics;verification;dominating set;performance;computer science;clique-width;artificial intelligence;trading strategy;machine learning;edge coloring;mathematics;mathematical economics;computational complexity theory;theory;algorithm;measurement	Theory	-3.6103359806425512	0.5503395926647116	5966
447f633e81838e0f7d7aef67805f0fa04b1a4106	an end-to-end model based on improved adaptive deep belief network and its application to bearing fault diagnosis		Effective machinery prognostics and health management play a crucial role in ensuring the safe and continuous operation of equipment, and satisfactory characteristics’ expression of machine health status plays a key role in the ability to diagnose faults with high accuracy. At present, most methods based on signal processing and the shallow learning model rely on artificial feature extraction to identify the machine fault type. In practical applications, however, meaningful health management requires correct recognition of not only the health type but also the fault degree, if any occurs. Such recognition is useful for determining the priority level of mechanical maintenance and minimizing economic losses. Deep learning techniques, such as deep belief network (DBN), have demonstrated great potential in exploring characteristic information from machine status signals. In this paper, an end-to-end fault diagnosis model based on an adaptive DBN optimized by the Nesterov moment (NM) is proposed to extract deep representative features from rotating machinery and recognize bearing fault types and degrees simultaneously. Frequency-domain signals are inputted into the model for feature learning, and NM is introduced to the training process of the DBN model. Individual adaptive learning rate algorithms are then applied to optimize parameter updating. The performance of the proposed method is validated using a self-made bearing fault test platform, and the model is shown to achieve satisfactory convergence and a testing accuracy higher than those obtained from standard DBN and support vector machine.		J. Xie;Guifu Du;Changqing Shen;Nan Chen;Liang Chen;Zhongkui Zhu	2018	IEEE Access	10.1109/ACCESS.2018.2877447	support vector machine;deep learning;machine learning;feature extraction;prognostics;signal processing;computer science;feature learning;adaptive learning;distributed computing;deep belief network;artificial intelligence	AI	36.723074675861014	-30.711237068421664	5975
f82d1bc14cc3dbdf54ad8506b4ceace219a9d457	a spintronic memristor bridge synapse circuit and the application in memrisitive cellular automata	memristor bridge synapse circuit;spintronic memristor;期刊论文;memristive cellular automata;synapse	Spintronic memristor is a new nonlinear circuit element which has property of memory and similar synapse, and the memristive effect can be realized by the spin-torque-induced magnetization switching or the magnetic domain wall motion. In this paper, a simple and compact memristor bridge synapse circuit which is able to perform signed synaptic weighting was proposed. The synaptic model have the good matching features in the system and is verified by matlab simulation experiment on the cellular automata, so this memristor synapse circuit is expected to be applied to neuromorphic system such as cellular neural network.	automata theory;cellular automaton;memristor;spintronics;synapse	Lidan Wang;Xiaodong Wang;Shukai Duan;Huifang Li	2015	Neurocomputing	10.1016/j.neucom.2015.04.061	embedded system;computer science;synapse;memistor	Logic	39.54748485462412	-0.5230868740914085	5976
e44d6811377c1b9af1c62c8c78988dd59d06a265	production and inter-facility transportation scheduling for a process industry	deterministic;production planning and scheduling;cost saving;inventory production;planning and scheduling;transportation scheduling;mixed integer program	We study in this paper the problem of coordinating the short-term production and inter facility transportation scheduling decisions between a plant that produces intermediate products and a finishing plant which processes the intermediate products into finished goods. Our goal is to develop a better understanding of the general relationships between production and transportation scheduling decisions, in particular, how changes in plant capacity and costs affect the coordination of scheduling decisions as well as the choice of transportation modes and carriers. We formulate the problem as a mixed integer programming model and use a solution procedure that exploits the underlying structure of our problem. Using real and simulated data from a process industry firm, our computational study, which compares the production and transportation schedules obtained from coordinated scheduling and sequential scheduling, shows that coordinated schedules yield significant cost savings resulting from the modest use of the expensive fast transport mode, coordinated product changeovers between plants and reduced intermediate product inventories. Discussions of managerial implications of coordinated scheduling are presented.	meta-object facility;scheduling (computing)	Renato de Matta;Tan Miller	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00358-8	fair-share scheduling;nurse scheduling problem;real-time computing;simulation;flow shop scheduling;dynamic priority scheduling;rate-monotonic scheduling;operations management;genetic algorithm scheduling;deadline-monotonic scheduling;scheduling;lottery scheduling;determinism	Robotics	9.555397914549435	-2.180543339819818	5980
513500584e3ece93c3fe8da42dc66ac84f5174f5	on the persistence of clustering solutions and true number of clusters in a dataset		Typically clustering algorithms provide clustering solutions with prespecified number of clusters. The lack of a priori knowledge on the true number of underlying clusters in the dataset makes it important to have a metric to compare the clustering solutions with different number of clusters. This article quantifies a notion of persistence of clustering solutions that enables comparing solutions with different number of clusters. The persistence relates to the range of dataresolution scales over which a clustering solution persists; it is quantified in terms of the maximum over two-norms of all the associated cluster-covariance matrices. Thus we associate a persistence value for each element in a set of clustering solutions with different number of clusters. We show that the datasets where natural clusters are a priori known, the clustering solutions that identify the natural clusters are most persistent in this way, this notion can be used to identify solutions with true number of clusters. Detailed experiments on a variety of standard and synthetic datasets demonstrate that the proposed persistence-based indicator outperforms the existing approaches, such as, gap-statistic method, X-means, Gmeans, PG-means, dip-means algorithms and informationtheoretic method, in accurately identifying the clustering solutions with true number of clusters. Interestingly, our method can be explained in terms of the phase-transition phenomenon in the deterministic annealing algorithm, where the number of distinct cluster centers changes (bifurcates) with respect to an annealing parameter. Introduction and Related Work Many high-impact application areas such as bio-informatics (Andreopoulos et al. 2009), (Di Nuovo and Catania 2008), exploratory data mining (Larose and Larose 2014), combinatorial drug discovery (Sharma, Salapaka, and Beck 2008), data and network aggregation (Yuan, Zhan, and Wang 2014), medical imaging (Huang et al. 2015) and many other information processing fields have fueled significant work on clustering algorithms. Most of these algorithms such as kmeans (Hartigan and Wong 1979), k-medoids (Park and Jun 2009), and expectation-maximization (Dempster, Laird, and Rubin 1977) require the number of clusters to be prespecified. Despite substantial work on clustering algorithms, there is relatively scant literature on determining the true number Copyright c © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. of clusters in a dataset. In this context, it should be noted that there is no single agreed-upon notion of natural clusters or true number of clusters; typically existing algorithms make assumptions on the datasets (e.g. generated from a mixture of Gaussian distributions) and validate their results on datasets that satisfy the assumptions. There are various measures developed to characterize the clustering solutions resulting from a clustering algorithm with different number (k) of clusters. One of the popular methods for determining the number of clusters is based on computing gap statistic (Tibshirani, Walther, and Hastie 2001). It compares the total intracluster variation for different values of k (number of clusters) with their expected values under null reference distribution of the data. The number of clusters k is ascribed to the case where the gap is largest. However, as remarked in (Feng and Hamerly 2007), this method works well for finding a small number of clusters, but has difficulty as the true k increases. Some of the recent methods that determine the number of clusters under some assumptions on datasets include X-means algorithm (Pelleg, Moore, and others 2000), where clustering using k-means is performed for a range of number k of clusters, and the value kt := k that yields the best Bayesian Information Criterion (BIC) (Kass and Wasserman 1995) score is chosen as an estimate for the true number of clusters. Other related algorithms use criteria such as Akaike information criteria (Akaike 2011) or minimum description length (Rissanen 1978) instead of BIC. The X-means algorithm works well for well-separated spherical clusters but tends to overfit in the case of non-spherical clusters (Feng and Hamerly 2007). The information-theoretic approach (Sugar and James 2003) where it estimates the number of true clusters kt by detecting a significant jump in the modified distortion D vs k plot; here D is the clustering distortion objective, k is the number of clusters, γ ≈ −d/2, and the data points are in R. Although the choice γ ≈ −d/2 works well for certain datasets, one can find examples where this choice fails (Sugar and James 2003). G-means (Hamerly and Elkan 2004) algorithm identifies the number of clusters in a dataset under the assumption that each cluster is a Gaussian distribution. It is a hierarchical algorithm that increases the number of clusters k until the hypothesis that each cluster comes from a single Gaussian distribution is ar X iv :1 81 1. 00 10 2v 2 [ cs .L G ] 1 6 N ov 2 01 8 Figure 1: Illustration of a mixture of nine Gaussian distributions arranged in groups of three superclusters. In (a1) the three superclusters are well separated from each other while in (b1) they are closer to each other. Observe that in (a2) for a large range in resolution scales (radii r) within the blue annulus, each supercluster appears as a single cluster, and only for a small range of resolution scale depicted by green annulus, each Gaussian distribution is identifiable separately. In other words for a large range of resolution the only the three superclusters are distinguishable from one another while for a smaller range of resolution each Gaussian distribution is identified separately. In (b1) since the three superclusters are closer to each other, the range of resolution scales within the blue annulus gets reduced, thereby indicating existence of three natural clusters in (a1) and nine natural clusters in (b1). validated; typically done using the Anderson-Darling statistic test (Stephens 1974) on each cluster after projecting it onto a one-dimensional space. PG-means algorithm (Feng and Hamerly 2007) is an improvement on the G-means algorithm, where the number of clusters in a Gaussian mixture model is obtained by applying the Kolmogorov-Smirnov (KS) test to the one-dimensional projection of the entire dataset; PG-means also works well when the true clusters are overlapping with each other. Dip-means (Kalogeratos and Likas 2012) is another method that assumes the dataset is generated from a mixture of unimodal distributions. Here, Hartigan’s dip statistic test (Hartigan and Hartigan 1985) is used to verify the unimodal nature of the admissible cluster. The authors also extend the dip-means algorithm to shape clustering problems by using kernel k-means (Dhillon, Guan, and Kulis 2004) for clustering. Other alternative approaches in the literature to estimate the true number of clusters are Bayesian k-means (Kurihara and Welling 2009) that uses Maximization-Expectation to learn a mixture model, a method based on repairing faults in Gaussian mixture models (Sand and Moore 2001) and various stability-based model validation methods (Lange et al. 2003), (Tibshirani and Walther 2005), (Levine and Domany 2001). The main drawbacks in most of the above existing methods stem from the underlying restrictive assumptions on the datasets; accordingly, the algorithms do not perform well when datasets do not meet the assumptions, which is often the case when considering standard non-synthetic datasets. These methods fail to accurately estimate the true number of clusters in most of the standard datasets as illustrated in the experiment section in this paper. In this article we develop a notion of persistence of clustering solutions that enables comparing solutions, which result from a clustering algorithm, with different number of clusters. Here we do not make any assumptions on the underlying data distribution. Since a clustering solution requires grouping a set of points in such a way that points in the same cluster are more similar to each other than to those in other clusters. We characterize persistence of a clustering solution as the range of resolution scales for which (a) points within each cluster seem indistinguishable, and (b) points in different clusters are distinguishable. For instance, Figure 1(a1) illustrates a dataset containing nine Gaussian clusters which are arranged in groups of three superclusters. If we choose the resolution scale of radius r, as shown in Figure 1(a2), then the points within each super-cluster is indistinguishable. Therefore, one will conclude that at this resolution level the dataset consists of only three clusters. Also note that in Figure 1(a1), the three super-clusters are persistent for a large range of resolution scales as indicated by the thickness of the blue annulus around each of them. On the other hand, the green annulus around each of the nine Gaussian clusters, is relatively thinner indicating that a clustering solution that identifies all the nine Gaussian clusters is relatively less persistent. In a later section we quantify this notion of persistence of a clustering solution with k distinct clusters. In particular, the persistence is characterized in terms of the maximum over two-norms of all the cluster covariance matrices at two successive values of k. We also show analytically how for a clustering solution that identifies natural clusters, this measure correctly estimates the true number of clusters through a simple illustrative example consisting of spherical clusters with uniform distributions. We also provide extensive experimental results on a variety of standard and synthetic datasets in a later section. The results demonstrate that our method outperforms over the existing algorithms described above. In particular, our method correctly estimates the true number of clusters on 13 of the 14 benchmark dataset	akaike information criterion;artificial intelligence;bayesian information criterion;benchmark (computing);bioinformatics;british informatics olympiad;cluster analysis;computer cluster;data mining;data point;distortion;earl levine;emoticon;expectation–maximization algorithm;experiment;information processing;information theory;k-means clustering;k-medoids;kolmogorov complexity;medical imaging;medoid;minimum description length;mixture model;overfitting;persistence (computer science);persistent data structure;sensor;simulated annealing;synthetic intelligence;thickness (graph theory)	Amber Srivastava;Mayank Baranwal;Srinivasa M. Salapaka	2018	CoRR			ML	1.8233344074929971	-45.15126669742321	5982
679628fa1c051f238b51ad91cede73cf361d497e	optimal path planning using an improved a* algorithm for homeland security applications		Path finding is an important sub task in mobile robotics and Homeland Security applications and has been subjected to extensive research. This paper analyses a variety of search algorithms used for path finding and planning. Using the Breve simulation environment, a general search algorithm and then the A* algorithm have been implemented. An improvement to the A* algorithm is introduced and presented. In this paper it has been proved through experimental results that the performance of the A* algorithm improves considerably after adding an additional heuristic. Dynamic path planning has also been implemented in this paper by allowing the vehicle to check for changes in the environment at every simulation time step and recalculate paths if there is a change in the environment. In the past ad-hoc sensor networks have been used in homeland security. In this paper ad-hoc sensor networks have been modeled using the patch class in the Breve simulation tool and path finding techniques have been used in this environment. Time and space complexity analysis of the various algorithms implemented in this paper have been presented.	a* search algorithm;analysis of algorithms;automated planning and scheduling;dspace;heuristic;hoc (programming language);mobile robot;motion planning;pathfinding;robotics;simulation;breve	Anand Veeraswamy;Bala P. Amavasai	2006				Robotics	53.34235894029006	-24.03061598934181	6005
4ee55ca511542d8c58c63b0512b50f49d53845e7	bat algorithm: literature review and applications	optimisation;metaheuristics;bat algorithm;nature inspired algorithm;algorithm;firefly algorithm;eagle strategy;cuckoo search	Bat algorithm (BA) is a bio-inspired algorithm developed by Yang in 2010 and BA has been found to be very efficient. As a result, the literature has expanded significantly in the last 3 years. This paper provides a timely review of the bat algorithm and its new variants. A wide range of diverse applications and case studies are also reviewed and summarized briefly here. Further research topics are also discussed.	bat algorithm;british informatics olympiad;business architecture;yang	Xin-She Yang	2013	IJBIC	10.1504/IJBIC.2013.055093	mathematical optimization;simulation;bat algorithm;computer science;artificial intelligence;firefly algorithm;operations research;cuckoo search;metaheuristic	PL	22.210791648590032	-5.183272256034225	6013
0372b7125819dc6595733cb16b3f3495ba42a149	markovian state and action abstractions for mdps via hierarchical mcts		State abstraction is an important technique for scaling MDP algorithms. As is well known, however, it introduces difficulties due to the non-Markovian nature of state-abstracted models. Whereas prior approaches rely upon ad hoc fixes for this issue, we propose instead to view the state-abstracted model as a POMDP and show that we can thereby take advantage of state abstraction without sacrificing the Markov property. We further exploit the hierarchical structure introduced by state abstraction by extending the theory of options to a POMDP setting. In this context we propose a hierarchical Monte Carlo tree search algorithm and show that it converges to a recursively optimal hierarchical policy. Both theoretical and empirical results suggest that abstracting an MDP into a POMDP yields a scalable solution approach.	feature vector;hoc (programming language);machine learning;markov chain;markov property;monte carlo method;monte carlo tree search;partially observable markov decision process;recursion;reinforcement learning;scalability;search algorithm;state space;tree traversal	Aijun Bai;Siddharth Srivastava;Stuart J. Russell	2016			mathematical optimization;computer science;theoretical computer science;machine learning	AI	21.82251580486412	-17.154895308298908	6029
79257cd2b6de33a80a63fd9c3835a34c01f061bb	application of bayesian neural networks to protein sequence classification	protein sequence;neural network	In this paper we present an application of neural networks to biomedical data mining. Speciically we propose a hybrid approach, combining similarity search and Bayesian neural networks, to classify protein sequences. We apply our techniques to recognizing the globin sequences obtained from the database maintained in the Protein Information Resources (PIR) at the National Biomedi-cal Research Foundation. Experimental results indicate an excellent performance of the proposed approach .	artificial neural network;bayesian network;data mining;peptide sequence;similarity search	Qicheng Ma;Jason Tsong-Li Wang	1999			electrical connection;probabilistic neural network;control theory;artificial neural network;lateral movement;time delay neural network;bayesian probability;computer science	ML	12.198338226603635	-28.232815209053623	6030
ae48cf90bbfd72560d5cc691134b48089f43db21	automatic specification of piecewise linear additive models: application to forecasting natural gas demand	generalized additive models;prediction;natural gas demand;short-term forecasting;piecewise linear models;nonlinear modeling	When facing any forecasting problem not only is accuracy on the predictions sought. Also, useful information about the underlying physics of the process and about the relevance of the forecasting variables is very much appreciated. In this paper, it is presented an automatic specification procedure for models that are based on additivity assumptions and piecewise linear regression. This procedure allows the analyst to gain insight about the problem by examining the automatically selected model, thus easily checking the validity of the forecast. Monte Carlo simulations have been run to ensure that the model selection procedure behaves correctly under weakly dependent data. Moreover, comparison over other well-known methodologies has been done to evaluate its accuracy performance, both in simulated data and in the context of short-term natural gas demand forecasting. Empirical results show that the accuracy of the proposed model is competitive against more complex methods such as neural networks.	additive model;piecewise linear continuation	Alberto Gascón;Eugenio Fco. Sánchez-Úbeda	2018	Statistics and Computing	10.1007/s11222-017-9726-x	mathematical optimization;statistics;model selection;econometrics;monte carlo method;artificial neural network;generalized additive model;segmented regression;piecewise linear function;additive model;computer science;demand forecasting	Vision	7.7475266882921385	-15.439380464383882	6038
8bb23758ff2b04372873d6f4e0b31d04f4a77761	sensor-based planning with the freespace assumption	robot sensing systems;topology;shortest path;performance guarantee;robot sensing systems motion planning computer science performance analysis algorithm design and analysis navigation path planning uncertainty topology;worst case optimal planning;sensors;uncertainty;path planning;freespace assumption;mobile robots;optimal control;navigation;basic veca;basic veca sensor based planning freespace assumption worst case optimal planning restricted graph topologies;sensor based planning;performance analysis;motion planning;optimal control mobile robots path planning sensors navigation;computer science;restricted graph topologies;algorithm design and analysis	A popular technique for getting to a goal location in unknown terrain is planning with the freespace assumption. The robot assumes that the terrain is clear unless it knows otherwise. It always plans a shortest path to the goal location and re-plans whenever it detects an obstacle that blocks its path or, more generally, when it detects that its current path is no longer optimal. It has been unknown whether this sensor-based planning approach is worst-case optimal, given the lack of initial knowledge about the terrain. We demonstrate that planning with the freespace assumption can make good performance guarantees on some restricted graph topologies (such as grids) but is not worstcase optimal in general. For situations in which its performance guarantee is insu cient, we also describe an algorithm, called Basic-VECA, that exhibits good average-case performance and provides performance guarantees that are optimal up to a constant (user-de ned) factor.	algorithm;best, worst and average case;robot;sensor;shortest path problem;subroutine;worst-case complexity	Sven Koenig;Yury V. Smirnov	1997		10.1109/ROBOT.1997.606883	control engineering;mathematical optimization;simulation;computer science;artificial intelligence;motion planning	Robotics	53.45010783963778	-23.52152767774939	6039
1eb5e2af31f93b7bb8de26436deb8f4ad972515b	enhancing the food locations in an artificial bee colony algorithm	artificial bee colony;optimisation;optimisation learning artificial intelligence number theory;convergence;gbest;optimization algorithm design and analysis benchmark testing mathematical model convergence equations artificial neural networks;population based swarm intelligence algorithm food locations artificial bee colony algorithm i abc algorithm population based nature inspired algorithms intermediate abc uniformly generated random numbers opposition based learning;number theory;artificial neural networks;mathematical model;optimization;learning artificial intelligence;gbest artificial bee colony opposition based learning;optimal algorithm;algorithm design and analysis;benchmark testing;artificial neural network;opposition based learning	Artificial Bee Colony or ABC is one of the newest additions to the class of population based Nature Inspired Algorithms (NIA). In the present study we suggest some modifications in the structure of basic ABC to further improve its performance. The corresponding algorithm proposed in the present study is named Intermediate ABC (I-ABC). In I-ABC, the potential food sources are generated by using the intermediate positions between the uniformly generated random numbers and random numbers generated by opposition based learning (OBL). The proposed I-ABC is further modified by guiding the bees towards the best food location. I-ABC is validated on a set of 15 benchmark problems with bound constraints. The numerical results indicate the competence of the proposed I-ABC algorithm.	artificial bee colony algorithm	Tarun Kumar Sharma;Millie Pant	2011		10.1109/SIS.2011.5952582	mathematical optimization;engineering;artificial intelligence;machine learning	AI	27.909855441311638	-5.723840969961449	6056
181644b61cf7dad553b6440b0f1bfcd25d8c510d	a probabilistic model for latent least squares regression	structural information;latent least squares regression;pattern recognition;least squares regression	By far, least squares regression (LSR) is the most widely used data modeling method in statistics and mathematics because of its effectiveness and completeness. It plays an important underlying role in many extensions, e.g., regularized LSR, weighted LSR, and  lasso . Since LSR is a discriminative model, it allows only sampling of the target variables conditioned on observations. In this paper, we present the latent LSR (LLSR), a generative model, which enables LSR to exploit the structural information hidden in the explanatory variables by imposing a sparsity-encouraging prior over the precision matrix of the latent variable. A maximum a posteriori (MAP) estimate is applied to obtain a point estimate of the model parameters. Both the toy example and real data tests suggest the effectiveness of LLSR.	least squares;statistical model	Sheng-Zheng Wang;Jie Yang	2015	Neurocomputing	10.1016/j.neucom.2014.09.014	generalized least squares;total least squares;econometrics;computer science;pattern recognition;mathematics;partial least squares regression;least squares;statistics	ML	28.13026965516977	-32.348579357129914	6082
b8845ad22b93ca45d24698449b9f9997df88a170	robust distance metric learning via simultaneous l1-norm minimization and maximization		Traditional distance metric learning with side information usually formulates the objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannotlinks. Because the covariance matrix computes the sum of the squared l2-norm distances, it is prone to both outlier samples and outlier features. To develop a robust distance metric learning method, we propose a new objective for distance metric learning using the l1-norm distances. The resulted objective is challenging to solve, because it simultaneously minimizes and maximizes (minmax) a number of non-smooth l1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general l1-norm minmax problem. We performed extensive empirical evaluations, where our new distance metric learning method outperforms related state-of-the-art methods in a variety of experimental settings.	data point;expectation–maximization algorithm;experiment;iterative method;minimax;signal-to-noise ratio;taxicab geometry	Hua Wang;Feiping Nie;Heng Huang	2014			mathematical optimization;distance matrix;machine learning;mathematics;rational quadratic covariance function;statistics	ML	27.661519923013007	-37.22046275675539	6084
82fb1f4604ffc62c5b136060a1c9cd316793086a	an axiomatic characterization of nonadditive information improvement		Abstract   A generalized nonadditive information improvement satisfying nonadditivity and containing the parameters α,β,γ has been axiomatically characterized by a general method. The particular cases of the new measure have been also studied.		D. S. Hooda	1980	Inf. Sci.	10.1016/0020-0255(80)90028-6	econometrics;mathematical optimization;mathematics	DB	0.4654383387351399	-20.85895534242345	6093
1a799d24fd63a502a89b90ca752ec155546853d4	inter- and intragenerational mutation shape adaptation	di- rected covariance matrix adaptation;intragenerational adaptation;directed mutation;intergenerational adaptation;evolutionary algo- rithm;covariance matrix adaptation;vector control;evolution strategy	Till now, only uncorrelated evolution strategies benefit from the strength of the directed mutation principle. It is thus a natural idea to enhance more powerful correlated evolution strategies with directed mutation, too. This work aims at porting this approach from the uncorrelated setting in classical evolution strategies to the correlated case as given in covariance matrix adaptation-evolution strategies. The main problem to be addressed here is the shape vector update. The shape vector controls the distribution’s skewness and can be updated intergenerationally as well as intragenerationally. Starting with an analogue to the intergenerational parameter update mechanics used in CMA-ES, we argue that an additional intragenerational update is of greater benefit. An appropriate heuristic will be presented and some experimental data of several test functions is provided.	algorithm;cma-es;distribution (mathematics);evolution strategy;heuristic;multi-channel memory architecture;overhead (computing);undo	Stefan Berlik;Madjid Fathi;Alexander Holland	2006			cma-es;mathematical optimization;machine learning;mathematics;artificial intelligence;evolution strategy;mutation;vector control	ML	29.113208604452264	-6.6091558748054755	6103
5b5f427932422b404d8fa79f02e94dc86c44806b	multiple query probabilistic roadmap planning using single query planning primitives	path planning;mobile robots;spectrum;rapidly exploring random tree;single query planners motion planning probabilistic roadmap method bidirectional rapidly exploring random trees multiple query planner rigid nonconvex robots;random processes;motion planning;random processes path planning mobile robots;probabilistic roadmap method;robots motion planning sampling methods space exploration switches interpolation computer science layout data structures orbital robotics	We propose a combination of techniques that solve multiple queries for motion planning problems with single query planners. Our implementation uses a probabilistic roadmap method (PRM) with bidirectional rapidly exploring random trees (BI-RRT) as the local planner. With small modifications to the standard algorithms, we obtain a multiple query planner which is significantly faster and more reliable than its component parts. Our method provides a smooth spectrum between the PRM and BI-RRT techniques and obtains the advantages of both. We observed that the performance differences are most notable in planning instances with several rigid nonconvex robots in a scene with narrow passages. Our work is in the spirit of non-uniform sampling and refinement techniques used in earlier work on PRM.	algorithm;motion planning;nonuniform sampling;probabilistic roadmap;refinement (computing);robot;sampling (signal processing);statistical relational learning	Kostas E. Bekris;Brian Y. Chen;Andrew M. Ladd;Erion Plaku;Lydia E. Kavraki	2003		10.1109/IROS.2003.1250704	stochastic process;computer vision;mathematical optimization;simulation;computer science;artificial intelligence;motion planning	Robotics	52.049250444015506	-24.256541074399667	6113
76188f978a4c41d174d9972d23dcb54356763821	low-rank bilinear pooling for fine-grained classification		Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of fine-grained classification tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a low-rank bilinear classifier. The resulting classifier can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned. To further compress the model, we propose a classifier co-decomposition that factorizes the collection of bilinear classifiers into a common factor and compact per-class terms. The co-decomposition idea can be deployed through two convolutional layers and trained in an end-to-end architecture. We suggest a simple yet effective initialization that avoids explicitly first training and factorizing the larger bilinear classifiers. Through extensive experiments, we show that our model achieves state-of-the-art performance on several public datasets for fine-grained classification trained with only category labels. Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model [8], and three orders smaller than the standard bilinear CNN model [19].	bilinear filtering;bilinear transform;computation;end-to-end principle;experiment;ibm notes;linear classifier;low-rank approximation;matrix multiplication;mobile device;statistical classification	Shu Kong;Charless C. Fowlkes	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.743	computer science;machine learning;pattern recognition;data mining;statistics	Vision	20.3782244724068	-45.88016601681106	6132
e1cacfc73458d07f0b74ebd55f24c7f6f1129b30	rate distortion efficiency of subband coding with crossband prediction	phase change materials;rate distortion;decoding;markov source;gaussian processes;rate distortion iir filters bandwidth bit rate image reconstruction encoding gaussian processes image coding filter bank frequency;speech processing;filters;asymptotically rate distortion optimal coder;speech coding;crossband prediction;bit rate;rate distortion theory;prediction theory;rate distortion filters gaussian processes decoding laboratories milling machines bit rate phase change materials speech processing speech coding;milling machines;infinite impulse response;asymptotically rate distortion optimal coder rate distortion efficiency subband coding crossband prediction;markov processes;rate distortion optimization;encoding;filter coefficients rate distortion efficiency subband coding crossband prediction gaussian sources asymptotically rate distortion optimum coding gauss markov sources crossband prediction filter infinite impulse response decay;subband coding;markov processes iir filters prediction theory rate distortion theory source coding gaussian processes;iir filters;rate distortion efficiency;source coding	Traditional subband coding, where each subband is encoded independently, has been shown by Fischer (1992) to be suboptimum for encoding Gaussian sources in the rate-distortion sense. We show here that if we use prediction across subbands when encoding Gaussian sources, the resulting coder is asymptotically rate-distortion-optimum at high rate. We also show for Gauss-Markov sources that although the crossband prediction filter in this case is of infinite impulse response, the filter can be implemented very efficiently to high accuracy due to the fast decay of the filter coefficients.	distortion;gene prediction;sub-band coding	Ping Wah Wong	1997	IEEE Trans. Information Theory	10.1109/18.567761	sub-band coding;mathematical optimization;speech recognition;rate–distortion theory;computer science;speech coding;gaussian process;speech processing;mathematics;markov process;rate–distortion optimization;infinite impulse response;encoding;statistics;source code	Theory	48.526348870459984	-12.344845051268514	6140
a490d9ed013baff20fe11f393f714ba452df6129	a twin-screw coded evolutionary algorithm for multi-level production scheduling	manufacturing systems;optimal solution;optimisation;scheduling combinatorial mathematics convergence evolutionary computation manufacturing systems optimisation production control;convergence;evolutionary computation;job shop scheduling;multilevel production scheduling;processor scheduling;twin screw coding;manufacturing system twin screw coded evolutionary algorithm convergence multilevel production scheduling combinatorial optimization problem;combinatorial optimization problem;satisfiability;biological cells;production control;twin screw coded evolutionary algorithm convergence;evolutionary computation job shop scheduling processor scheduling manufacturing systems production systems optimization methods production planning power engineering computing manufacturing automation computer science;scheduling;production;multi level production scheduling;optimization;multi level production scheduling evolutionary algorithm twin screw coding;production scheduling;evolutionary algorithm;manufacturing system;combinatorial mathematics;algorithm design and analysis	Multi-level production scheduling problem is a typical combinatorial optimization problem in manufacturing system, which is traditionally modeled as hierarchical sub-problems and optimized at each sub-problem level. An improved integrate evolutionary algorithm, which can cope with the whole multi-levels' scheduling information at one time and can find the satisfied optimal solution in acceptable computational cost. The new algorithm employs a twin-screw coding strategy, which enhances the efficiency and performance of the algorithm's convergence. Besides, a real three-level production scheduling problem case study is employed to evaluate the performance of the proposed algorithm. Experimental results show that our algorithm has outperformed the existing one.	algorithmic efficiency;combinatorial optimization;computation;evolutionary algorithm;heuristic (computer science);mathematical optimization;optimization problem;particle swarm optimization;scheduling (computing);tabu search	Ruifeng Shi;Yiming Zhou	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.463	fair-share scheduling;nurse scheduling problem;mathematical optimization;real-time computing;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operations management;genetic algorithm scheduling;two-level scheduling;fsa-red algorithm;least slack time scheduling;lottery scheduling;round-robin scheduling;population-based incremental learning	Robotics	19.894187881033854	-0.6055341196235023	6143
86e9556c5e11c7ced0701706b8ce1ee7ed6cb9af	privacy-concerned multiagent planning	finite state machines;action landmarks;multiagent planning;delete relaxation	Coordinated sequential decision making of a team of cooperative agents can be described by principles of multiagent planning. Provided that the mechanics of the environment the agents act in is described as a deterministic transitions system, an appropriate planning model is MA-Strips. Multiagent planning modeled as MA-Strips prescribes exactly what information has to be kept private and which information can be communicated in order to coordinate toward shared or individual goals. We propose a multiagent planning approach which combines compilation for a classical state-of-the-art planner together with a compact representation of local plans in the form of finite-state machines. Proving soundness and completeness of the approach, the planner efficiency is further boosted up using distributed delete-relaxation heuristics and using an approximative local plan analysis. We experimentally evaluate applicability of our approach in full privacy setting where only public information can be communicated. We analyze properties of standard multiagent benchmarks from the perspective of classification of private and public information. We show that our approach can be used with different privacy settings and that it outperforms state-of-the-art planners designed directly for particular privacy classification.	agent-based model;benchmark (computing);compiler;experiment;finite-state machine;heuristic (computer science);linear programming relaxation;privacy;strips	Jan Tozicka;Jan Jakubuv;Antonín Komenda;Michal Pechoucek	2015	Knowledge and Information Systems	10.1007/s10115-015-0887-7	computer science;knowledge management;artificial intelligence;machine learning;data mining;database;finite-state machine	AI	18.36784679424656	-14.912871898947873	6148
dff37284d5b1fc18cb05aac588f8294f772b7162	efficient tests for one sample correlated binary data with applications	exact test;ophthalmology;cluster analysis data processing;mathematical statistics;statistics;unconditional test;otolaryngology;otolaryngology study;probabilities;clustered data	Four testing procedures are considered for testing the response rate of one sample correlated binary data with a cluster size of one or two, which often occurs in otolaryngologic and ophthalmologic studies. Although an asymptotic approach is often used for statistical inference, it is criticized for unsatisfactory type I error control in small sample settings. An alternative to the asymptotic approach is an unconditional approach. The first unconditional approach is the one based on estimation, also known as parametric bootstrap (Lee and Young in Stat Probab Lett 71(2):143–153, 2005). The other two unconditional approaches considered in this article are an approach based on maximization (Basu in J Am Stat Assoc 72(358):355–366, 1977), and an approach based on estimation and maximization (Lloyd in Biometrics 64(3):716–723, 2008a). These two unconditional approaches guarantee the test size and are generally more reliable than the asymptotic approach. We compare these four approaches in conjunction with a test proposed by Lee and Dubin (Stat Med 13(12):1241–1252, 1994) and a likelihood ratio test derived in this article, in regards to type I error rate and power for sample sizes from small to medium. An example from an otolaryngologic study is provided to illustrate the various testing procedures. The unconditional approach based on estimation and maximization using the test in Lee and Dubin (Stat Med 13(12):1241–1252, 1994) is preferable due to the power advantageous. G. Shan (B) Department of Environmental and Occupational Health, Epidemiology and Biostatistics Program, School of Community Health Sciences, University of Nevada Las Vegas, Las Vegas, NV 89154, USA e-mail: guogen.shan@unlv.edu C. Ma Department of Biostatistics, University at Buffalo, 3435 Main Street, Buffalo, NY 14214, USA 123 G. Shan, C. Ma	binary data;biometrics;biostatistics;bootstrapping (statistics);buffalo network-attached storage series;debian-med;email;error detection and correction;expectation–maximization algorithm;nv network;sql/med	Guogen Shan;Changxing Ma	2014	Statistical Methods and Applications	10.1007/s10260-013-0251-6	econometrics;speech recognition;computer science;probability;mathematical statistics;otorhinolaryngology;mathematics;exact test;statistics	SE	28.665849995532806	-22.114206341083975	6153
44bb322b2ed27c8ffbf95c26a4af6babe50b064f	video quality and system resources: scheduling two opponents	resource scheduling;perceived quality;resource constraint;video streaming;real time;decoding time prediction;video quality;visual quality;video codec;error propagation;adaptation;video decoding;h 264;mpeg;prediction	In this article we present three key ideas which together form a flexible framework for maximizing user-perceived quality under given resources with modern video codecs (H.264). First, we present a method to predict resource usage for video decoding online. For this, we develop and discuss a video decoder model using key metadata from the video stream. Second, we explain a light-weight method for providing replacement content for a given region of a frame. We use this method for online adaptation. Third, we select a metric modeled after human image perception which we extend to quantify the consequences of available online adaptation decisions. Together, these three parts allow us, to the best of our knowledge for the first time, to maximize user-perceived quality in video playback under given resource constraints.	codec;h.264/mpeg-4 avc;scheduling (computing);streaming media;video decoder	Michael Roitzsch;Martin Pohlack	2008	J. Visual Communication and Image Representation	10.1016/j.jvcir.2008.09.007	scalable video coding;real-time computing;simulation;prediction;computer science;video quality;propagation of uncertainty;video tracking;mathematics;block-matching algorithm;multimedia;smacker video;rate–distortion optimization;motion compensation;statistics;multiview video coding;adaptation	Graphics	46.155032601606585	-21.365735058336128	6155
60e31c1c0962189aeb5acc7177aea97054a294d6	bio-inspired computing – theories and applications		Self-assembly is the process that the component form an ordered form or structure. Because of the biochemical characteristics of DNA molecules, they become a research emphasis in the field of selfassembly. DNA-based self-assembly technology has been widely used in the fields of nanometer machining, molecular circuit, polymer materials, and so on. DNA self-assembly is an effective mechanism that nanometer structure is built bottom-up. In order to overcome the problem that any kind of self-assembled model can only solve the single algorithm, in this paper, a new DNA self-assembly algorithmic model is designed to solve compound logic operators problem. Five types of DNA tiles are designed according to the characteristic of compound operation problem, namely Initial Tile, Process Tile, Operation Tile, End Tile and Boundary Tile. At last, the process of self-assembly are demonstrated by an instance.	algorithm;bio-inspired computing;bottom-up parsing;polymer;self-assembly;top-down and bottom-up design	Maoguo Gong;Linqiang Pan;Tao Song;Gexiang Zhang	2016		10.1007/978-981-10-3611-8		Theory	34.23797870180394	-7.698308541739471	6158
153ce5a3fd09b7e29a9ce1b02facff41b6a00e19	integrating fuzzy logic and gis analysis to assess sediment characterization within a confined harbour	marine sediment;accuracy assessment;fuzzy logic;sediment;gis;harbour;empirical model;area determination;human activity	Characterization of marine sediments in areas heavily impacted by human activities is a good example for situations where high complexity of physical and chemical processes can lead to an incomplete understanding of the configuration and distribution of pollutants material. These processes are often very complex for a direct prediction from a mathematical theory, making necessary that the process of identifying areas of contaminated sediment is mainly based on defined empirical models of concentrations distribution. In this paper an ontological fuzzy approach in GIS serves as a framework to define a specific scenario grounded on the abilities from an existing dataset. The final model is based on a large number of known concentrations (samples for characterization), which are considered sufficiently similar in terms of features. Therefore the model is working as guides (description of the model) for the identification of areas of same type.	fuzzy logic;geographic information system	Nicoletta Gazzea;Andrea Taramelli;Emiliana Valentini;Maria Elena Piccione	2009		10.1007/978-3-642-02454-2_3	fuzzy logic;geomatics;computer science;artificial intelligence;sediment;empirical modelling	Crypto	3.4615198661527526	-20.209860054186226	6164
c603d892239ee8ba0f97efa3e2d22dbfa350eda9	the convergence properties of a clipped hopfield network and its application in the design of keystream generator	convergence intelligent networks neurons encoding cryptography decoding difference equations limit cycles dynamic range very large scale integration;convergence cryptography hopfield neural nets;convergence;decoding;hopfield network;very large scale integration;cryptographically secure keystream generator design convergence properties clipped hopfield neural network synaptic weights binary vector storage stable points parallel updating mode state vectors attraction basins;binary vector storage;basin of attraction;hopfield neural nets;indexing terms;parallel updating mode;state vectors;difference equations;synaptic weights;limit cycles;cryptography;convergence properties;dynamic range;clipped hopfield neural network;attraction basins;intelligent networks;neurons;encoding;stable points;cryptographically secure keystream generator design	We first present a modified Hopfield network, the clipped Hopfield network, with synaptic weights assigned to three values {-1,0,+1}. We give the necessary conditions under which a set of 2n binary vectors can be stored as stable points of the network. We show that in the parallel updating mode, for most of the state vectors, the network will always converge to these 2n stable points. We further demonstrate that these 2n stable points can be divided into two groups, the alpha group and the beta group, each with n stable points. It is shown that the basins of attraction of the stable points in the alpha group are evenly distributed, and the basins of attraction of the stable points in the beta group are also evenly distributed. By ways of application, we show that this class of Hopfield network can be used to build a cryptographically secure keystream generator.		Chi-Kwong Chan;Lee-Ming Cheng	2001	IEEE transactions on neural networks	10.1109/72.914528	dynamic range;discrete mathematics;convergence;computer science;cryptography;theoretical computer science;machine learning;mathematics;hopfield network;algorithm;encoding	ML	39.11881135557321	-4.259607786627182	6167
3b90f1b4fcf61aac1ad20c6e39893b56e1f38b98	delay-dependent decentralised control for a class of uncertain similar interconnected systems with state delay and input delay	delay dependent;interconnected systems;similar structure;decentralised control	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	display lag;francis;input lag;primary source	Yuechao Ma;Shujie Jin;Nannan Gu	2015	Int. J. Systems Science	10.1080/00207721.2014.880196	control engineering;real-time computing;control theory;mathematics	Robotics	50.770100743369646	-3.363911326035732	6174
e874d4d173c1da20b3cbcd1a8c8e2a2162fccb3c	a biologically inspired automatic system for media quality assessment	semantics;biology;videos visualization semantics image color analysis sparse matrices feature extraction biology;sparsity actively viewing biologically inspired gaze shifting path;visualization;image color analysis;feature extraction;probability feature extraction gaze tracking learning artificial intelligence;sparse matrices;eye tracking data biologically inspired automatic system media quality assessment photo aesthetic quality evaluation artificial intelligence systems biologically inspired aesthetic descriptor visually salient regions low level visual features semantically salient regions high level visual features weakly supervised learning paradigm local image descriptors low dimensional semantic space sparsity constrained graphlet ranking algorithm probabilistic aesthetic measure learning actively viewing paths avp training photos human gaze shifting paths;videos	Photo aesthetic quality evaluation is a challenging task in artificial intelligence systems. In this paper, we propose a biologically inspired aesthetic descriptor that mimicks humans sequentially perceiving visually/semantically salientIn general, visually salient regions are perceived by low-level visual features, such as the high contrast between the foreground and the background objects; while semantically salient regions are perceived by high-level visual features such as human faces.regions in a photo. In particular, a weakly supervised learning paradigm is developed to project the local image descriptors into a low-dimensional semantic space. Then, each graphlet can be described by multiple types of visual features, both in low-level and in high-level. Since humans usually perceive only a few salient regions in a photo, a sparsity-constrained graphlet ranking algorithm is proposed that seamlessly integrates both the low-level and the high-level visual cues. Top-ranked graphlets are those visually/semantically prominent local aesthetic descriptors in a photo. They are sequentially linked into a path that simulates humans actively viewing process. Finally, we learn a probabilistic aesthetic measure based on such actively viewing paths (AVPs) from the training photos. Experimental results show that: 1) the AVPs are 87.65% consistent with real human gaze shifting paths, as verified by the eye-tracking data and 2) our aesthetic measure outperforms many of its competitors.	algorithm;artificial intelligence;attribute–value pair;computer;experiment;eye tracking;high- and low-level;programming paradigm;simulation;sparse matrix;statistical model;supervised learning;visual descriptor	Luming Zhang;Richang Hong;Liqiang Nie;Chaoqun Hong	2016	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2015.2418223	computer vision;visualization;sparse matrix;feature extraction;computer science;machine learning;semantics;multimedia	AI	33.83669695307713	-48.51444252920642	6175
8fc4f0643223f67cb491b740daeafcdf2be26683	a low complexity lossless compression scheme for wearable ecg sensors	medical signal processing body sensor networks data reduction electrocardiography;sensor phenomena and characterization redundancy packaging complexity theory monitoring biomedical monitoring;wearable ambulatory ecg monitoring applications wearable ecg sensors low complexity lossless ecg compression algorithm data reduction wireless ambulatory ecg sensors linear prediction technique redundancy removal joint coding packaging scheme residual prediction error compaction multiple linear predictor incoming data temporal signal characteristics improved dynamic coding packaging scheme frames estimation error fixed length format compression ratio mit bih ecg database	This paper presents a low complexity lossless ECG compression algorithm for data reduction in wireless ambulatory ECG sensors. The proposed algorithm uses a novel linear prediction technique for redundancy removal and a joint coding-packaging scheme for compaction of the residual prediction error. Multiple linear predictors are engaged simultaneously to track the incoming data and the best prediction estimate is adaptively chosen based on the temporal signal characteristics to minimize error. An improved dynamic coding-packaging scheme frames the resulting estimation error into fixed-length 16-bit format. The proposed technique achieves an average compression ratio of 2.38× on MIT/BIH ECG database. Low complexity and good compression performance makes the proposed technique suitable for wearable ambulatory ECG monitoring applications.	16-bit;algorithm;bounding interval hierarchy;data compaction;data compression ratio;lossless compression;sensor;wearable computer	Chacko John Deepu;Yong Lian	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2015.7251912	data compression;embedded system;electronic engineering;real-time computing;computer science	EDA	50.30657619738396	-17.278322274327145	6176
eef58a6fad28525108bb2a117b229926fe11bc83	evaluation of techniques for signature classification from accelerometer and gyroscope data	signature classification;gyroscope;time series;comparative study signature classification accelerometer gyroscope time series polynomial approximation;comparative study;accelerometer;accelerometers gyroscopes support vector machines;polynomial approximation	In this paper, we present an exhaustive comparison of techniques for classification of signature data extracted from gyroscope and accelerometer devices. Since there exists large pool of classifiers and features for this kind of data, in order to provide a guide in choosing a particular setup, we decided to explore performance of these methods in a comparative study, which is a missing factor of current works on the topic. Also, we propose a framework for the combination of evaluated techniques in order to achieve a higher precision of the final classifier. The evaluated factors are: transformation of the time-series data into a fixed-size vector, classification methods and the performance of generative techniques without fixed-size input.	gyroscope;time series	Lukas Tencer;Marta Reznáková;Mohamed Cheriet	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333925	speech recognition;gyroscope;comparative research;time series;data mining;accelerometer;statistics	Visualization	11.137960235650327	-40.918417903511404	6177
af86df8185e61c6922f4d19d8895ef186bb8aece	enhancements to the localized genetic algorithm for large scale capacitated vehicle routing problems		"""This paper is a continuation of two previous papers where the authors used Genetic Algorithm with automated problem decomposition strategy for small scale capacitated vehicle routing problems (CVRP) and vehicle routing problem with time windows (VRPTW). In this paper they have extended their scheme to large scale capacitated vehicle routing problems by introducing selective search version of the automated problem decomposition strategy, a faster genotype to phenotype translation scheme, and various search reduction techniques. The authors have shown that genetic algorithm used with automated problem decomposition strategy outperforms the GAs applied on the problem as a whole not only in terms of solution quality but also in terms of computational time on the large scale problems. DOI: 10.4018/jaec.2013010102 18 International Journal of Applied Evolutionary Computation, 4(1), 17-38, January-March 2013 Copyright © 2013, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. them. In this context, the VRP can be said to represent the intersection of two combinatorial problems i.e., finding an optimal problem decomposition (Set Partitioning) and route optimization (Ordering). Please note that the two terms ‘problem decomposition’ and ‘set partitioning’ may have dissimilar meaning in other combinatorial problems. Some approaches optimize VRP as a whole without explicitly separating these two built in sub-tasks, while others perform decomposition then order the elements of the sub-problems independent of each other. Evidence has shown that the latter strategy is more successful. Work using genetic algorithms has mostly treated this problem as a whole. Although some applications of GA have treated these two sub-tasks separately, those applications used GA only for set partitioning and not for ordering. To our knowledge only one application of GA has appeared in the literature where the GA is used for optimization of customer ordering of sub-problems. However, it is a multi-depot problem (Surekha & Sumathi, 2011), where each sub-problem surrounds a single depot constituting a complete Capacitated Vehicle Routing Problem (CVRP). Apart from our earlier work (Ursani et al., 2009, 2011) no report has appeared in the literature where the GA is used for optimization of customer order of sub-problems of either of the two most fundamental VRP problems, that is the CVRP and the Vehicle Routing Problem with Time Windows (VRPTW). In the following subsections literature regarding the two sub-tasks of VRP i.e., (problem decomposition and ordering) is discussed separately. The ordering section deals with the problem representation of customer order and subsequent genotype to phenotype translation that has remained a core issue of GA in the area of VRP. In the context of VRP the term ‘order optimization’ is preferred to ‘route optimization.’ Problem Decomposition Strategies Problem decomposition strategies have been in place for some time. They are also used for optimization of Vehicle Routing Problems. Some of those applications are discussed here. The first successful problem decomposition strategy applied to the VRP was a Tabu search (Rochat & Taillard, 1995). The problem was partitioned into sub-problems and then each sub-problem was optimized through the Tabu Search. The optimized sub-problems were joined together to form a global solution. This procedure was performed iteratively until good a quality solution was obtained. Different partition methods have been proposed for different types of datasets. The same method was extended by Gendreau, Hertz, and Laporte (1994) but ensured different partitions in each iteration. The GA has also been applied to the VRP with problem decomposition strategies but not for optimization of sub-problems. Instead it has been used for the formation of sub-problems only, with those sub-problems subsequently being optimized through different approaches. One such application was introduced by Thangiah, Nygard, and Paul (1991). In this paper the GA was used to partition the dataset into sectors that were optimized through other heuristics. A similar approach was extended for school bus routing by Thangia and Nygard (1992) and again for VRPTW by Thangiah (1995). The approach was improved by incorporation of Tabu Search and Simulated Annealing by Thangiah (1999). Another adaptation for VRP with a multi-depot problem is presented by Thangiah and Salhi (2001). All the above work used a cluster first, route second approach where a GA was applied for clustering, sectoring or problem decomposition but not directly for route optimization itself. A very different problem decomposition strategy was adopted by Ralphs et al. (2003) where a CVRP problem was decomposed into a convex combination of Travelling Saleman Problem (TSP) tours that were later optimized through a parallel branch, cut, and price algorithm. An application of Ant Colony Optimization (ACO) with a problem decomposition strategy was used by Reimann, Doerner, and Hartl (2004), and called D-Ants i.e., Decomposed Ants. In this work the ACO was initially applied to the whole problem then 20 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/enhancements-localized-geneticalgorithm-large/75823?camid=4v1 This title is available in InfoSci-Journals, InfoSci-Journal Disciplines Computer Science, Security, and Information Technology. Recommend this product to your librarian: www.igi-global.com/e-resources/libraryrecommendation/?id=2"""	ant colony optimization algorithms;cluster analysis;computer science;continuation;evolutionary computation;genetic algorithm;heuristic (computer science);iteration;librarian;mathematical optimization;microsoft windows;simulated annealing;software release life cycle;syntax-directed translation;tabu search;time complexity;vehicle routing problem;web page	Ziauddin Ursani;Daryl Essam;David Cornforth;Rob Stocker	2013	IJAEC	10.4018/jaec.2013010102	mathematical optimization;computer science;destination-sequenced distance vector routing;operations research	AI	18.555425014266334	1.303769705335511	6179
a4f89951c43206d622cbd6e8d32e2f54b0e475a4	minimal feedforward parity networks using threshold gates	parity network;reseau a boucle anticipation;feedforward;mcculloch pitts node;mc p node;boucle anticipation;reseau parite;threshold logic;ciclo anticipacion;logica umbral;feedforward network;logique seuil;reseau neuronal;noeud mcculloch pitts;red neuronal;neural network	This article presents preliminary research on the general problem of reducing the number of neurons needed in a neural network so that the network can perform a specific recognition task. We consider a single-hidden-layer feedforward network in which only McCulloch-Pitts units are employed in the hidden layer. We show that if only interconnections between adjacent layers are allowed, the minimum size of the hidden layer required to solve the n-bit parity problem is n when n 4.	artificial neural network;feed forward (control);feedforward neural network;walter pitts	Hon-Kwok Fung;Leong Kwan Li	2001	Neural Computation	10.1162/089976601300014556	computer science;artificial intelligence;machine learning;echo state network;feed forward;artificial neural network;algorithm	ML	11.371157706673928	-28.805771587206166	6187
56e5e454f5081c344ed5975e93ee58a2408f3a01	scale and rotation invariant approach to tracking human body part regions in videos	dynamic programming;graph theory;video signal processing;tracking regions;background clutter scale invariant approach rotation invariant approach human body part region tracking cluttered video human body plan object scale change object rotation change human body part graph dynamic programming method n best whole body configuration loopy structur video frame trellis construction trellis shortest path finding object appearance variation;会议论文;human pose;scale and rotation invariant tracking regions human pose;object tracking;video signal processing dynamic programming graph theory object tracking;videos shape dynamic programming torso proposals histograms clutter;scale and rotation invariant	We propose a novel scale and rotation invariant method to track a human subject's body part regions in cluttered videos. The proposed method optimizes the assembly of body part region proposals with the spatial and temporal constraints of a human body plan. This approach is invariant to the object scale and rotation changes. To enable scale and rotation invariance, the human body part graph of the proposed method has to be loopy, efficiently optimizing the body part region assembly is a great challenge. We propose a dynamic programming method to solve the problem. We devise a method that finds N-best whole body configurations from loopy structures in each video frame using dynamic programming. The N-best configurations are then used to construct trellises with which we track human body part regions by finding shortest paths on the trellises. Our experiments on a variety of videos show that the proposed method is efficient, accurate and robust against object appearance variations, scale and rotation changes and background clutter.	casio loopy;clutter;dynamic programming;experiment;human computer;human–computer interaction;shortest path problem	Yihang Bo;Hao Jiang	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2013.151	computer vision;simulation;computer science;graph theory;machine learning;dynamic programming;video tracking;mathematics	Vision	48.30004454080013	-49.68560738759114	6200
2d345a79347af2443ee913641b3f5c0eede87fda	logistic evaluation of an underground mine using simulation	discrete event simulation;logistics;mining;mining industry;production engineering computing;traffic;transportation;tunnels;anglogold ashanti;discrete-event simulation model;mine operation life;traffic rules;transportation capacity;underground gold mine;underground mine logistic evaluation;arena;simulation	This paper describes a logistic study about an underground gold mine, belonging to AngloGold Ashanti, where four different layout options could be applied to the tunnels, and also different transportation strategies. Each evaluated layout had its own configuration for shaft and truck fleet. The study was made individually for each year of the mine operation life, determining the necessary transportation capacity to achieve the planned production at that year. Due to the very restrictive traffic options in the tunnels, a framework was developed to represent the tunnels and traffic rules in a discrete-event simulation model. The results identified the scenario with the lowest necessary transportation capacity to achieve the planned production.	list of discrete event simulation software	Marcelo Moretti Fioroni;Letícia Cristina Alves dos Santos;Luiz Augusto Gago Franzese;Isac Reis de Santana;Gustavo Dezem Telles;Josiane Cordeiro Seixas;Bruno Penna;Gerson Mendes de Alkmim	2014	Proceedings of the Winter Simulation Conference 2014		simulation;region-based memory management;engineering;civil engineering	Mobile	11.49837954211579	1.0998933291674784	6201
e8b88f22721eaab095ace7e4fc70618664347ffd	using visual interpretation of small ensembles in microarray analysis	molecular biophysics arrays decision trees genetics learning artificial intelligence medical computing;decision trees visual interpretation ensemble classifiers microarray analysis gene expression;decision trees visualization data mining neural networks iterative algorithms gene expression data analysis decision making classification algorithms artificial neural networks;genetics;medical computing;gene expression;arrays;microarray analysis;molecular biophysics;ensemble classifiers;learning artificial intelligence;decision trees;visual interpretation	Many different classification models and techniques have been employed on gene expression data. These computational methods are in rapid and continuous evolution and there is no clear consensus on which methods are best to cope with the complex microarray data analysis. Currently ensembles of classifiers are regarded as one of the best classification techniques as they can achieve excellent classification accuracy in comparison to single classifiers methods. One of their main drawbacks is their incomprehensibility. This paper addresses the important issue of the tradeoff between accuracy and comprehensibility when building ensembles and proposes a novel visual technique for interactive interpretation of the knowledge from the small ensembles consisting of only a few decision trees. This way we can achieve better accuracy compared to single classifier, but still maintain a certain level of comprehensibility in small ensembles. The results show that our small ensembles outperform the single classifiers and still retain comprehensibility. Our study also points out that in order to take advantage of our proposed method we need more effective small ensemble building techniques	computation;decision tree;ensembles of classifiers;microarray	Gregor Stiglic;Matej Mertik;Vili Podgorelec;Peter Kokol	2006	19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)	10.1109/CBMS.2006.169	microarray analysis techniques;gene expression;computer science;bioinformatics;artificial intelligence;machine learning;decision tree;data mining;molecular biophysics	Vision	8.341898530793749	-47.70006228996642	6203
fbf97eb49b72c513d5ff38a1e5b81aed3fc9666e	bayesian structural equation models for cumulative theory building in information systems		Theories are sets of causal relationships between constructs and their proxy indicator variables. Theories are tested and their numerical parameters are estimated using statistical models of latent and observed variables. A considerable amount of theoretical development in Information Systems occurs by theory extension or adaptation. Moreover, researchers are encouraged to reuse existing measurement instruments when possible. As a consequence, there are many cases when a relationship between two variables (latent and/or observed) is re-estimated in a new study with a new sample or in a new context. To aid in cumulative theory building, a re-estimation of parameters should take into account our prior knowledge about their likely values. In this paper, we show how Bayesian statistical models can provide a statistically sound way of incorporating prior knowledge into parameter estimation, allowing researchers to keep a “running tally” of the best estimates of model parameters.	bayesian network;bayesian programming;best practice;structural equation modeling;theory	Joerg Evermann;Mary Tate	2012	CAIS		econometrics;discrete mathematics;variable-order bayesian network;mathematics;statistics	AI	27.968448657784784	-24.521802137978774	6209
62564ca10f008bf10ea1a446378587432c373b86	a trail-following robot which uses appearance and structural cues		We describe a wheeled robotic system which navigates along outdoor “trails” intended for hikers and bikers. Through a combination of appearance and structural cues derived from stereo omnidirectional color cameras and a tiltable laser range-finder, the system is able to detect and track rough paths despite widely varying tread material, border vegetation, and illumination conditions. The approaching trail region is efficiently segmented in a top-down fashion based on color, brightness, and/or height contrast with flanking areas, and a differential motion planner searches for maximally-safe paths within that region according to several criteria. When the trail tracker’s confidence drops the robot slows down to allow amore detailed search, and when it senses a dangerous situation due to excessive slope, dense trailside obstacles, or visual trail segmentation failure, it stops entirely to acquire and analyze a ladar-derived point cloud in order to reset the tracker. Our system’s ability to negotiate a variety of challenging trail types over long distances is demonstrated through a number of live runs through different terrain and in different weather conditions.	bittorrent tracker;computation;depth perception;global positioning system;kinect;motion planning;omnidirectional camera;point cloud;robot;top-down and bottom-up design;visual odometry	Christopher Rasmussen;Yan Lu;Mehmet Kemal Kocamaz	2012		10.1007/978-3-642-40686-7_18	computer vision	Robotics	49.96206914850762	-36.766914914980646	6222
a345d2359de9371a08c840b266d9b44bb7a8d641	off-policy evaluation for mdps with unknown structure		Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computationally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples.		Assaf Hallak;François Schnitzler;Timothy Arthur Mann;Shie Mannor	2015	CoRR		computer science;machine learning;data mining;algorithm;statistics	NLP	22.301430062909294	-19.283953854719655	6227
1fa5d8c090312a09f75b778270f2e10022e84cb6	robust relative pose estimation with integrated cheirality constraint	ransac robust relative pose estimation integrated cheirality constraint point correspondence reconstruction epipolar geometry estimation image feature correspondence random sample consensus;generators;geometry;computational geometry;mobile robots;computer vision;epipolar geometry;pose estimation cameras computational geometry image reconstruction;estimation;image reconstruction;robustness geometry cameras matrix decomposition layout voting computer science image reconstruction calibration least squares approximation;robots;cameras;pose estimation	The cheirality constraint, which requires that reconstructed point correspondences lie in front of the cameras, has not typically been integrated into traditional RANSAC-based pose estimators. We have developed a new RANSAC-based relative pose estimator which incorporates the cheirality constraint not only to preempt invalid epipolar geometry hypotheses, but also as a criterion in identifying inliers for image feature correspondences. Because the application of the cheirality constraint is tightly related to the estimation of epipolar geometry, integrating them inside RANSAC can prevent inliers being falsely identified as cheirality outliers. The result is a more consistent and stable estimation which leaves denser feature correspondences for subsequent processing. Experimental comparison between the usual RANSAC-based approach and the proposed approach is performed.	3d pose estimation;epipolar geometry;feature (computer vision);random sample consensus;robustness (computer science)	Wei Xu;Jane Mulligan	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761509	iterative reconstruction;robot;mobile robot;computer vision;mathematical optimization;estimation;ransac;pose;3d pose estimation;computational geometry;computer science;machine learning;articulated body pose estimation;mathematics;epipolar geometry	Robotics	52.832661184631064	-49.615900000918785	6236
b34638797dd6baf4ff08b8954e62b1a8ccce7e0e	inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks		While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the backpropagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.		Timo Hackel;Mikhail Usvyatsov;Silvano Galliani;Jan Dirk Wegner;Konrad Schindler	2018	CoRR		artificial intelligence;machine learning;implementation;computation;sparse matrix;convolution;exploit;computer science;inference;upper and lower bounds	ML	21.197107311110027	-47.13525510689181	6239
d13e2a6c0315b65ed238edd4fb6a27748499c827	ranking with decision tree	busqueda informacion;machine learning ranking;classification tree;decision tree;information retrieval;recommandation;metric;intelligence artificielle;arbol decision;hierarchical classification;machine learning;ranking;collaborative filtering;recherche information;classification hierarchique;recomendacion;artificial intelligence;recommendation;metrico;splitting rule;inteligencia artificial;perceptron;reseau neuronal;clasificacion jerarquizada;categorical data;arbre decision;red neuronal;metrique;donnee categorielle;dato categorico;neural network	Ranking problems have recently become an important research topic in the joint field of machine learning and information retrieval. This paper presented a new splitting rule that introduces a metric, i.e., an impurity measure, to construct decision trees for ranking tasks. We provided a theoretical basis and some intuitive explanations for the splitting rule. Our approach is also meaningful to collaborative filtering in the sense of dealing with categorical data and selecting relative features. Some experiments were made to illustrate our ranking approach, whose results showed that our algorithm outperforms both perceptron-based ranking and the classification tree algorithms in term of accuracy as well as speed.	algorithm;categorical variable;classification chart;collaborative filtering;decision tree learning;experiment;information retrieval;instability;machine learning;perceptron	Fen Xia;Wensheng Zhang;Fuxin Li;Yanwu Yang	2007	Knowledge and Information Systems	10.1007/s10115-007-0118-y	ranking;categorical variable;metric;decision tree learning;ranking;computer science;artificial intelligence;collaborative filtering;perceptron;machine learning;decision tree;pattern recognition;data mining;ranking svm;artificial neural network	ML	8.365578746317672	-33.09634134570052	6247
043cdf34e8547212be9f02f0d292f330db1e1d02	a simple technique for improving multi-class classification with neural networks		We present a novel method to perform multi-class pattern classification with neural networks and test it on a challenging 3D hand gesture recognition problem. Our method consists of a standard oneagainst-all (OAA) classification, followed by another network layer classifying the resulting class scores, possibly augmented by the original raw input vector. This allows the network to disambiguate hard-to-separate classes as the distribution of class scores carries considerable information as well, and is in fact often used for assessing the confidence of a decision. We show that by this approach we are able to significantly boost our results, overall as well as for particular difficult cases, on the hard 10-class gesture classification task.	artificial neural network;gesture recognition;multiclass classification;open agent architecture	Thomas Kopinski;Alexander Gepperth;Uwe Handmann	2015	CoRR		computer science;artificial intelligence;machine learning;data mining	ML	16.618508697625284	-45.17670881851456	6255
245aa95e677c93a52b103fcea42c432edff960cc	a 3d line alignment method for loop closure and mutual localisation in limited resourced mavs	robot sensing systems;three dimensional displays;feature extraction;cameras;robot kinematics;pose estimation	In this paper we present a new 3D line alignment technique that can be used in limited resourced MAVs for performing loop closures as well as mutual localisation between MAVs. We identify pairs of 3D line matches from RGB-D frames and find the optimal transformation which aligns these two line sets onto each other in order to find the corresponding relative pose. The alignment of the two 3D line sets are achieved by converting each line match into two matching point pairs and then subjecting them to a least squares minimization process. As we only maintain line features extracted from key frames, our method does not require large memory, high processing power nor a high communication bandwidth between robots. We validate our 3D line alignment technique by performing loop closures and mutual localisation on real-world datasets.	anomaly detection;benchmark (computing);closing (morphology);collaborative mapping;experiment;for loop;key frame;least squares;map;random sample consensus;robot	Ilankaikone Senthooran;Jan Carlo Barca;Hoam Chung	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838773	computer vision;simulation;pose;feature extraction;computer science;engineering;artificial intelligence;robot kinematics	Robotics	51.47446144664352	-46.36206934460384	6260
51bb64a7ac16130967bb1f9dab1b2d010172d898	an optimized artificial neural network approach for epileptiform activity recognition	artificial neural network;activity recognition		activity recognition;artificial neural network	Melvin Ayala;Malek Adjouadi	2003			machine learning;activity recognition;artificial neural network;time delay neural network;artificial intelligence;computer science	AI	12.42632474327206	-27.22471476602499	6264
3c762ccb6d6ebeb99c26bdd5a55498820204f04c	body-part tracking from partial-view depth data		This paper presents a high-accuracy body-part tracking algorithm, capable of achieving efficient human motion analysis from partial view depth-data, suitable for deployment in real-life applications. The algorithm uses a consumer-grade depth camera for data input and combines a discriminative body part estimator along with a generative tracker, utilizing a realistic human body model, in order to track individual body limbs in short camera-distance, partial-view scenarios. Additionally, a shape adaptation feature is also introduced in order to further morph the human model based on the observations. The implementation is tested in a lower-body limbs tracking scenario, achieving promising accuracy and performance on consumer-grade hardware. Moreover, a lower-body motion dataset is also provided, consisting of 16 real-world sequences using automatic ground-truth annotations from a commercial motion capture system.	algorithm;generative science;ground truth;kinesiology;motion capture;real life;software deployment	Manolis Vasileiadis;Dimitrios Giakoumis;Sotiris Malassiotis;Ioannis Kostavelis;Dimitrios Tzovaras	2017	2017 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2017.8280408	discriminative model;motion analysis;software deployment;artificial intelligence;estimator;computer science;computer vision;hidden markov model;solid modeling;motion capture	Vision	50.78997556124035	-44.96243998114864	6270
2de52846be5358a8aacebb5fe940dea011145a32	ontology specific data mining based on dynamic grammars	databases;sequence comparison;filtering;information science;bioinformatics databases data mining engines filtering robustness computer science educational institutions information science biology;biology;data mining;engines;formal grammar;robustness;computer science;biological data;biological database;bioinformatics	In this paper we introduce a new formal approach for mining biological data sets. The proposed grammar based approach provides a flexible and powerful tool for advanced sequence comparison and data mining. The approach benefits from the power of regular expressions in allowing the use of advanced queries in comparing sequences and searching for motifs or sequence attributes in biological databases. The formal grammar and the corresponding data mining engine is capable of extracting records from biological databases, filtering a subset of those records for mining, and then sorting those records based on similarity scheme designed by the user. This model is based on the objective (ontology) of the user and scoring is dynamic that is provided at runtime.	biological database;data mining;formal grammar;ontology (information science);regular expression;run time (program lifecycle phase);sorting	Daniel Quest;Hesham H. Ali	2004	Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.	10.1109/HICSS.2005.17	filter;biological database;biological data;information science;computer science;bioinformatics;data science;data mining;database;formal grammar;data stream mining;world wide web;robustness	DB	-3.536228350772531	-51.70381363084916	6272
24fef6fe0a6f25b63e6d458fcfb7f8b143a5bd22	template-based semi-automatic workflow construction for gene expression data analysis	databases;histograms;genomics;filtering;gene expression accuracy algorithm design and analysis histograms data analysis filtering databases;molecular biophysics bioinformatics data analysis genomics inference mechanisms;grammar like inference;semi automatic workflow template;xgene org semi automatic workflow template gene expression analysis data analysis grammar like inference;inference mechanisms;gene expression;accuracy;data analysis;gene expression analysis;xgene org;molecular biophysics;data transformation;gene expression data analysis;algorithm design;algorithm design and analysis;bioinformatics	We propose a technique for semi-automatic construction of gene expression data analysis workflows by grammar-like inference based on predefined workflow templates. The templates represent routinely used sequences of procedures such as normalization, data transformation, classifier learning, etc. Variations of such workflows (such as different instantiations to specific algorithms) may entail significant variance in the quality of the analysis results and our formalism enables to automatically explore such variations. Adhering to proven templates helps preserve the sanity of explored workflows and prevents the combinatorial explosion encountered by fully automatic workflow planners. Here we propose the basic principles of template-based workflow construction and demonstrate their working in the publicly available tool XGENE.ORG for multi-platform gene expression analysis.	algorithm;database normalization;semantics (computer science);semiconductor industry	Jiri Belohradsky;David A. Monge;Filip Zelezný;Matej Holec;Carlos García Garino	2011	2011 24th International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2011.5999046	algorithm design;genomics;gene expression;computer science;bioinformatics;data science;machine learning;data mining;database;statistics;molecular biophysics;workflow technology	Arch	4.592329592702593	-49.645670744646466	6285
0f94f6b021afa8f3fbe5a8aee375605ac5fcf851	a novel approach for object extraction from video sequences based on continuous background/foreground classification	continuous background foreground classification;pattern clustering;image motion analysis;self organizing network;complexity theory;video signal processing;pixel clustering algorithms complexity theory mathematical model computational modeling adaptation model equations;image classification;video sequences;robust initialization schema object extraction video sequences continuous background foreground classification computer vision gaussians mixtures self organizing network;gaussians mixtures;computer vision;robust initialization schema;video signal processing image classification image motion analysis image sequences object detection pattern clustering;computational modeling;adaptation model;pixel;object extraction;mathematical model;clustering algorithms;self organization;mixture of gaussians;theoretical foundation;object detection;image sequences	In many computer vision related applications it is necessary to distinguish between the background of an image and the objects that are contained in it. This is a difficult problem because of the constraints imposed by the available time and the computational cost of robust object extraction algorithms. This report describes a new method that benefits from state of the art background/foreground classification combined with the strong theoretical foundations of clustering. The pixels on the scene background are modeled as Mixtures of Gaussians and the output of the classification process are continuous values representing the likelihood that each pixel belongs to the foreground. The clustering is based on a Self Organizing Network (SON) which has a robust initialization schema and is able to find the number of objects in an image or grid. The algorithm's complexity is linear with respect to the number of pixels or cells.	algorithm;algorithmic efficiency;cluster analysis;computation;computer vision;mixture model;pixel	Thiago C. Bellardi;Jorge Rios-Martinez;Dizan Vasquez;Christian Laugier	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650101	computer vision;contextual image classification;self-organization;background subtraction;computer science;artificial intelligence;machine learning;pattern recognition;mixture model;mathematical model;cluster analysis;self-organizing network;computational model;pixel	Robotics	44.786600847078084	-50.48519795981513	6286
c04dc80ec1376b2dbbb1d60534bb4142ea38d039	analysis of a single-agent search	game tree;pathology	Game playing is one of the first areas of artificial intelligence that was studied by AI researchers. The developed algorithms and heuristics combined with an ever-increasing computer speed efficiently searched large game trees and thus effectively competed with the best human players. The key advantage comes from the generally accepted notion that a deeper search produces better results. However, while trying to provide a mathematical explanation, researchers have discovered that under certain conditions the opposite situation occurs, i.e., a deeper search results in worse decisions. In this paper we analyse single-agent search algorithms and the influence of three properties of the search trees on the quality of the search. The analysis was performed on one-player search models and in the maze-path-finding problem.	artificial intelligence;heuristic (computer science);search algorithm	Ales Tavcar	2012	Informatica (Slovenia)		game tree;artificial intelligence;computer science;machine learning;search algorithm;heuristics	AI	18.79510889736362	-12.597892555129798	6289
461e757313d381c0ad7e440aac923754020c9536	operation performance evaluation method for transitional mode of multiple mode production processes		Multiple mode production process is a kind of production process which has many stable operating points. The transitional mode is the transient process which links up two stable operating points. This transitional mode has complex dynamic characteristics. According to thecharacteristics of transitional mode itself,based on the difference principal component analysis method, this thesis puts forward an operation performance evaluation method for evaluating transitional mode of multiple mode production processes. This method can effectively evaluate the running state of transitional mode. It can help to improve the efficiency of the production process and ensure production safety. It is important to improve the economic benefits of production process.	performance evaluation	Xiaochen Zhu;Fuli Wang	2015	J. Comput. Meth. in Science and Engineering	10.3233/JCM-150554	real-time computing;engineering;operations management;control theory	DB	7.423225321717104	-1.3719934937643228	6292
bf2e47aeb53822365145fff316b87dc8b67166f8	deepdiary: lifelogging image captioning and summarization		Abstract Automatic image captioning has been studied extensively over the last few years, driven by breakthroughs in deep learning-based image-to-text translation models. However, most of this work has considered captioning web images from standard data sets like MS-COCO, and has considered single images in isolation. To what extent can automatic captioning models learn finer-grained contextual information specific to a given person’s day-to-day visual experiences? In this paper, we consider captioning image sequences collected from wearable, life-logging cameras. Automatically-generated captions could help people find and recall photos among their large-scale life-logging photo collections, or even to produce textual “diaries” that summarize their day. But unlike web images, photos from wearable cameras are often blurry and poorly composed, without an obvious single subject. Their content also tends to be highly dependent on the context and characteristics of the particular camera wearer. To address these challenges, we introduce a technique to jointly caption sequences of photos, which allows captions to take advantage of temporal constraints and evidence across time, and we introduce a technique to increase the diversity of generated captions, so that they can describe a photo from multiple perspectives (e.g., first-person versus third-person). To test these techniques, we collect a dataset of about 8000 realistic lifelogging images, a subset of which are annotated with nearly 5000 human-generated reference sentences. We evaluate the quality of image captions both quantitatively and qualitatively using Amazon Mechanical Turk, finding that while these algorithms are not perfect, they could be an important step towards helping to organize and summarize lifelogging photos.	lifelog	Chenyou Fan;Zehua Zhang;David J. Crandall	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.05.008	automatic summarization;mathematics;lifelog;computer vision;deep learning;wearable computer;artificial intelligence;closed captioning	Vision	33.197312987201585	-51.03508012562388	6293
f466d040761d877556c79791b306cde33f8d29e5	integrated adaptive design and planning of supply networks	optimal solution;adaptive design;optimal control theory;adaptive control;dynamic model;operations research;optimal control;supply chain design;feedback loop;execution environment;linear program;supply chain;supply chain management	  In recent years, the works on supply chain management (SCM) have been broadened from isolated static models of supply chain  design (SCD) and planning (SCP) to integrated SCD and SCP models. This paper develops a framework for integrated SCD and SCP  on the basis of adaptation principles and regarding SC execution dynamics. To achieve this integration, static models of SCD  are brought in correspondence to the dynamic models of SCP and control. The adaptive feedback loops between the planning and  execution models are established. It becomes possible if traditional operations research (OR) techniques are extended by optimal  control theory. We illustrate the general framework with the help of a modelling complex. We show explicitly how to distribute  static and dynamics variables and constraints of SCD and SCP by interconnecting static elements in SCD optimization linear  programming model with corresponding SCP dynamic elements in optimal control model. This makes it possible to consider conventionally  isolated SCD and SCP problems taking into account non-stationarity of supply chain execution along with the adaptive control  within a conceptually and mathematically integrated framework. In doing so, the developed framework contributes to the advancing  decision-making support for SCM on the basis of interrelating planning and execution levels instead of generating optimal  solutions that fail in a real perturbed execution environment.    	assistive technology	Dmitry A. Ivanov;Boris V. Sokolov;Joachim Käschel	2010		10.1007/978-3-642-12494-5_14	control engineering;real-time computing;operations management	Robotics	12.888975614822225	-6.9100105862248045	6296
c1910cc18cc2ca51d181425fed5c1b4acffafe91	bayesian models for dna sequencing	large dataset;biological system modeling;model based approach;statistical analysis;bayesian model selection;signal processing;dna sequence;bayesian model	It is becoming increasingly important to develop novel signal processing and statistical analysis techniques to extract information from biotechnology. This task is complicated by large datasets, intricate physical systems, and the sheer diversity of information that is available. In many systems, classical non-parametric signal processing techniques have been applied with some success. However, where sufficient information is available to construct accurate models, substantial gains can sometimes be derived from a model-based approach. The Bayesian paradigm provides an elegant and mathematically rigorous framework for the objective incorporation of information. In this paper, we develop a Bayesian model for DNA sequencing, with an emphasis on generally relevant Bayesian model selection issues.	bayes factor;bayesian network;model selection;programming paradigm;signal processing	Nicholas M. Haan;Simon J. Godsill	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745539	dna sequencing;variable-order bayesian network;computer science;bioinformatics;machine learning;signal processing;data mining;mathematics;bayesian inference;statistics	Visualization	5.642324534218165	-49.99889894022013	6320
821676c3c2de6a33765de1023ca8cf23a84cc401	a dynamically adjusted mixed emphasis method for building boosting ensembles	emphasis adaboost boosting convex combination dynamic parameter selection;algorithms artificial intelligence learning neural networks computer nonlinear dynamics principal component analysis;haute performance;modele agrege;associated edge parameter maximization;supervised learning;support vector machines;multilayer perceptrons;facteur forme;modelo agregado;real adaboost ensemble;metodo mixto;form factor;pattern classification learning artificial intelligence multilayer perceptrons;multilayer perceptron;factor forma;classification;diversity reception;methode mixte;quadratic factor;artificial neural networks;performance improvement;boosting;proximite;estimation erreur;proximidad;parameter selection;mixed method;error estimation;boosting artificial neural networks diversity reception jacobian matrices scholarships support vector machines support vector machine classification;dynamically adjusted mixed emphasis method;proximity;scholarships;estimacion error;adaboost;pattern classification;alto rendimiento;aggregate model;quadratic error;support vector machine classification;apprentissage supervise;learning artificial intelligence;dynamic parameter selection;reseau neuronal;dynamic adaptation;jacobian matrices;aprendizaje supervisado;emphasis;high performance;pattern classification dynamically adjusted mixed emphasis method real adaboost ensemble emphasis function quadratic error quadratic factor associated edge parameter maximization multilayer perceptron;emphasis function;clasificacion;red neuronal;convex combination;neural network	Progressively emphasizing samples that are difficult to classify correctly is the base for the recognized high performance of real Adaboost (RA) ensembles. The corresponding emphasis function can be written as a product of a factor that measures the quadratic error and a factor related to the proximity to the classification border; this fact opens the door to explore the potential advantages provided by using adjustable combined forms of these factors. In this paper, we introduce a principled procedure to select the combination parameter each time a new learner is added to the ensemble, just by maximizing the associated edge parameter, calling the resulting method the dynamically adapted weighted emphasis RA (DW-RA). A number of application examples illustrates the performance improvements obtained by DW-RA.	adaboost;algorithm;appendix;coefficient;convergence (action);dreamwidth;generalization (psychology);mandibular right second molar tooth;multinet;numerical analysis;population parameter;quantitative real-time polymerase chain reaction;recursion;rewrite (programming);rheumatoid arthritis;scarab of ra;time complexity;traffic collision avoidance system;weight;exponential	Vanessa Gómez-Verdejo;Jerónimo Arenas-García;Aníbal R. Figueiras-Vidal	2008	IEEE Transactions on Neural Networks	10.1109/TNN.2007.902723	adaboost;support vector machine;emphasis;convex combination;form factor;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;multilayer perceptron;distance;boosting;artificial neural network;statistics	Visualization	11.969595533656998	-32.46130838020452	6332
7d1d07304da8d2c174c70e34a8dac03ab0f58512	performance analysis of hybrid forecasting model in stock market forecasting		This paper presents performance analysis of hybrid model comprise of concordance and Genetic Programming (GP) to forecast financial market with some existing models. This scheme can be used for in depth analysis of stock market. Different measures of concordances such as Kendall’s Tau, Gini’s Mean Difference, Spearman’s Rho, and weak interpretation of concordance are used to search for the pattern in past that look similar to present. Genetic Programming is then used to match the past trend to present trend as close as possible. Then Genetic Program estimates what will happen next based on what had happened next. The concept is validated using financial time series data (S&P 500 and NASDAQ indices) as sample data sets. The forecasted result is then compared with standard ARIMA model and other model to analyse its performance.	autoregressive integrated moving average;concordance (publishing);genetic programming;heart rate variability;machine learning;mathematical optimization;profiling (computer programming);search algorithm;time series	Mahesh Khadka;K. M. George;Nohpill Park	2012	CoRR		financial economics;econometrics;actuarial science;economics	AI	6.711258746358818	-18.802052630173705	6349
d01d7532588771eea008791f7ca4187a39baa7f1	binary relevance for multi-label learning: an overview	machine learning;multi-label learning;binary relevance;label correlation;class-imbalance;relative labeling-importance	Multi-label learning deals with problems where each example is represented by a single instance while being associated with multiple class labels simultaneously. Binary relevance is arguably the most intuitive solution for learning from multi-label examples. It works by decomposing the multi-label learning task into a number of independent binary learning tasks (one per class label). In view of its potential weakness in ignoring correlations between labels, many correlation-enabling extensions to binary relevance have been proposed in the past decade. In this paper, we aim to review the state of the art of binary relevance from three perspectives. First, basic settings for multi-label learning and binary relevance solutions are briefly summarized. Second, representative strategies to provide binary relevancewith label correlation exploitation abilities are discussed. Third, some of our recent studies on binary relevance aimed at issues other than label correlation exploitation are introduced. As a conclusion, we provide suggestions on future research directions.	algorithm;machine learning;multi-label classification;performance evaluation;relevance;single-instance storage;sourceforge;stacking	Min-Ling Zhang;Yu-Kun Li;Xu-Ying Liu;Xin Geng	2017	Frontiers of Computer Science	10.1007/s11704-017-7031-7	machine learning;artificial intelligence;binary number;pattern recognition;computer science	AI	15.674821531912459	-43.052584612285926	6357
64f3a644e584479004bd2e62c5917f262b7cd3c9	differential evolution: difference vectors and movement in solution space	expanding knowledge in the information and computing sciences;differential evolution;evolutionary computation;probability density function;swinburne;difference vectors;data mining;distance measurement;chromium equations exponential distribution robust control robustness random number generation intelligent systems competitive intelligence laboratories communications technology;randomly selected point differential evolution difference vectors de rand 1 variant;chromium;randomly selected point;clustering algorithms;expanding knowledge;neural evolutionary and fuzzy computation;information and computing sciences;artificial intelligence and image processing;algorithm design and analysis;de rand 1 variant	In the commonly used DE/rand/1 variant of differential evolution the primary mechanism of generating new solutions is the perturbation of a randomly selected point by a difference vector. The newly selected point may, if good enough, then replace a solution from the current generation. As the magnitude of difference vectors diminishes as the population converges, the size of moves made also diminishes, an oft-touted and obvious benefit of the approach. Additionally, when the population splits into separate clusters difference vectors exist for both small and large moves. Given that a replaced solution is not the one perturbed to create the new, candidate solution, are the large difference vectors responsible for movement of population members between clusters? This paper examines the mechanisms of small and large moves, finding that small moves within one cluster result in solutions from another being replaced and so appearing to move a large distance. As clusters tighten this is the only mechanism for movement between them.	algorithm;differential evolution;feasible region;global optimization;local optimum;population;principle of good enough;randomness	James Montgomery	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983298	differential evolution;mathematical optimization;probability density function;chromium;computer science;artificial intelligence;machine learning;mathematics;statistics;evolutionary computation	Vision	28.02731778326679	-5.913857578673796	6379
0f9b97f8a0a1769ec82ce3a051d5997fcc0d1f2a	visual lane analysis and higher-order tasks: a concise review	driver assistance;free space;curb detection;road modelling;journal article;real view navigation;lane detection;tracking	Lane detection, lane tracking, or lane departure warning have been the earliest components of vision-based driver assistance systems. At first (in the 1990s), they have been designed and implemented for situations defined by good viewing conditions and clear lane markings on highways. Since then, accuracy for particular situations (also for challenging conditions), robustness for a wide range of scenarios, time efficiency, and integration into higher-order tasks define visual lane detection and tracking as a continuing research subject. The paper reviews past and current work in computer vision that aims at real-time lane or road understanding under a comprehensive analysis perspective, for moving on to higher-order tasks combined with various lane analysis components, and introduces related work along four independent axes as shown in Fig. 2. This concise review provides not only summarizing definitions and statements for understanding key ideas in related work, it also presents selected details of potentially applicable methods, and shows applications for illustrating progress. This review helps to plan future research which can benefit from given progress in visual lane analysis. It supports the understanding of newly emerging subjects which combine lane analysis with more complex road or traffic understanding issues. The review should help readers in selecting suitable methods for their own targeted scenario.	computer vision;real-time locating system	Bok-Suk Shin;Zezhong Xu;Reinhard Klette	2014	Machine Vision and Applications	10.1007/s00138-014-0611-8	computer vision;simulation;tracking;operations research	Visualization	41.570757398053395	-41.624287268375234	6389
2b72cbaf86295a048900bbb21ef8a8c85442e73d	an enhanced training algorithm for multilayer neural networks based on reference output of hidden layer	metodo cuadrado menor;least squares backpropagation;methode moindre carre;learning algorithm;mackey glass time series;serie temporelle mackey glass;least squares method;methode newton;quasi newton;simulacion numerica;algorithme apprentissage;time series;backpropagation;conjugate gradient method;algorithme;algorithm;retropropagation;conjugate gradient;lsb;hidden layer;metodo gradiente conjugado;simulation numerique;least square;serie temporelle;couche cachee;serie temporal;reseau neuronal a couche multiple;metodo newton;newton method;methode gradient conjugue;reseau neuronal;algoritmo aprendizaje;retropropagacion;multilayer neural network;red neuronal;training algorithm;neural network;numerical simulation;algoritmo	In this paper, the authors propose a new training algorithm which does not only rely upon the training samples, but also depends upon the output of the hidden layer. We adjust both the connecting weights and outputs of the hidden layer based on Least Square Backpropagation (LSB) algorithm. A set of ‘required’ outputs of the hidden layer is added to the input sets through a feedback path to accelerate the convergence speed. The numerical simulation results have demonstrated that the algorithm is better than conventional BP, Quasi-Newton BFGS (an alternative to the conjugate gradient methods for fast optimisation) and LSB algorithms in terms of convergence speed and training error. The proposed method does not suffer from the drawback of the LSB algorithm, for which the training error cannot be further reduced after three iterations.	artificial neural network;backpropagation;broyden–fletcher–goldfarb–shanno algorithm;conjugate gradient method;converge;epoch (reference date);error-tolerant design;fastest;gradient descent;iteration;layer (electronics);least significant bit;mathematical optimization;multilayer perceptron;newton;numerical analysis;numerical weather prediction;quasi-newton method;recurrent neural network;service layer;simulation;steady state	Yan Li;Ahmad B. Rad;W. Peng	1999	Neural Computing & Applications	10.1007/s005210050024	computer simulation;computer science;artificial intelligence;machine learning;calculus;mathematics;conjugate gradient method;least squares;artificial neural network;algorithm	ML	16.41989655841324	-28.49184278806048	6394
8ec5e6f13e029749282003776c13ad2168ed10f4	exploration for nonrenewable resources in a dynamic oligopoly: an arrovian result	natural resources;differential games;oligopoly	I investigate two versions of a differential Cournot oligopoly game with nonrenewable resource exploitation, in which each firm may either exploit its own private pool or exploit a common pool jointly with the rivals. Firms use a deterministic technology to invest in exploration activities. In both models, there emerges that (i) the individual exploration effort is higher when each firms has exclusive rights on a pool of its own, and (ii) depending on the assumptions on technology and demand, the aggregate exploration effort is either constant or increasing in the number of firms. JEL Codes: C73, L13, Q30	aggregate data	Luca Lambertini	2014	IGTR	10.1142/S0219198914400118	economics;microeconomics;market economy;commerce	Logic	-2.0663662786246144	-5.4293327104230915	6399
7fa4e972da46735971aad52413d17c4014c49e6e	how to train triplet networks with 100k identities?		Training triplet networks with large-scale data is challenging in face recognition. Due to the number of possible triplets explodes with the number of samples, previous studies adopt the online hard negative mining(OHNM) to handle it. However, as the number of identities becomes extremely large, the training will suffer from bad local minima because effective hard triplets are difficult to be found. To solve the problem, in this paper, we propose training triplet networks with subspace learning, which splits the space of all identities into subspaces consisting of only similar identities. Combined with the batch OHNM, hard triplets can be found much easier. Experiments on the large-scale MS-Celeb-1M challenge with 100 K identities demonstrate that the proposed method can largely improve the performance. In addition, to deal with heavy noise and large-scale retrieval, we also make some efforts on robust noise removing and efficient image retrieval, which are used jointly with the subspace learning to obtain the state-of-the-art performance on the MS-Celeb-1M competition (without external data in Challenge1).	experiment;facial recognition system;image retrieval;ms-dos;maxima and minima;triplet state	Chong Wang;Xue Zhang;Xipeng Lan	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.225	robustness (computer science);linear subspace;artificial intelligence;facial recognition system;pattern recognition;machine learning;image retrieval;maxima and minima;computer science;subspace topology	Vision	27.3250986003123	-47.562049060540744	6401
7a0d6d60091e8846233428559ac13a833d00157e	mutual information estimation in higher dimensions: a speed-up of a k -nearest neighbor based estimator	projection method;spatial index;entropy estimation;nearest neighbor;higher dimensions;mutual information;k nearest neighbor;nearest neighbor search;neighborhood search	In this paper we focus on the recently introduced nearest neighbor based entropy estimator from Kraskov et al. [11]. Its nearest neighbor search is performed by the so called box assisted algorithm [8]. Since KSG appears currently as the most promising entropy estimator, we focus on its performance in higher dimensions (important for the application to mutual information) from the point of view of its k-nearest neighbor search. We present an experimental comparison of three spatial indexing methods, box-assisted, k-D trie and projection method, which were applied to the nearest neighbor search in KSG algorithm for a problem of mutual information estimation of a variety of pdfs in higher dimension. From our experiments we conclude that the k-D trie method should be applied to fixed-mass and fixed-radius neighborhood searches in higher dimensions because it performs better than box-assisted search. We also find the k-D trie method to be simpler to code than the boxassisted search method.	algorithm;c-trie;mutual information;nearest neighbor search;projection method (fluid dynamics);trie	Martin Vejmelka;Katerina Hlavácková-Schindler	2007		10.1007/978-3-540-71618-1_88	nearest-neighbor chain algorithm;large margin nearest neighbor;r-tree;ball tree;nearest neighbor graph;best bin first;computer science;machine learning;pattern recognition;data mining;mathematics;cover tree;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm	ML	-4.358032357094772	-42.47831010250406	6426
b62464fabbabde59165c87ec18cbcac0ed61fba1	intelligent production planning for complex garment manufacturing	group technology;intelligent erp;production planning;genetic algorithms;apparel production	Apparel production is characterised by labour-intensive manual operations, frequent style changes, seasonal demand and shortening production lead times. With fierce competition worldwide, many manufacturers are switching their production from mass mode to lean mode to shorten their response time to changes. In a complex mixed mode production environment, it is very important to allocate job orders to suitable production lines so as to ensure the effective utilization of production resources and on-time completion of all job orders. In this paper, planning algorithms are proposed for automatic job allocations based on group technology and genetic algorithms. For genetic algorithms based intelligent planning algorithms, single-run and multiple-run genetic algorithms are suggested. Real production data are used to validate the proposed method. The proposed algorithms has been shown being able to substantially improve planning quality. These planning algorithms are currently used by apparel manufacturers in Hong Kong as part of their routine planning operations.		P. Y. Mok;T. Y. Cheung;Wai Keung Wong;Sunney Yung-Sun Leung;J. T. Fan	2013	J. Intelligent Manufacturing	10.1007/s10845-011-0548-y	simulation;genetic algorithm;computer science;engineering;artificial intelligence;industrial engineering;machine learning	Robotics	10.651344801284134	2.9306581818448674	6428
80223effabdc33d5e94f4535840ad48c46d46173	selection procedures for simulations with multiple constraints under independent and correlated sampling	common random numbers;fully sequential algorithms;constraints;multiple performance measures	We consider the problem of selecting the best feasible system with constraints on multiple secondary performance measures. We develop fully sequential indifference-zone procedures to solve this problem that guarantee a nominal probability of correct selection. In addition, we address two issues critical to the efficiency of these procedures: namely, the allocation of error between feasibility determination and selection of the best system, and the use of Common Random Numbers. We provide a recommended error allocation as a function of the number of constraints, supported by an experimental study and an approximate asymptotic analysis. The validity and efficiency of the new procedures with independent and CRN are demonstrated through both analytical and experimental results.	approximation algorithm;computer simulation;design of experiments;digital library;experiment;gibbs sampling;heuristic;memory data register	Christopher M. Healey;Sigrún Andradóttir;Seong-Hee Kim	2014	ACM Trans. Model. Comput. Simul.	10.1145/2567921	econometrics;mathematical optimization;mathematics;statistics	PL	29.11074904059803	-16.552408772651546	6434
15bc2cd0dd10d71528e196a0f16bb3b594bba9e4	why prices need algorithms	equilibrium computation;market equilibrium;complexity;market design;computational game theory	Understanding when equilibria are guaranteed to exist is a central theme in economic theory, seemingly unrelated to computation. In this note we survey our main result from [Roughgarden and Talgam-Cohen 2015], which shows that the existence of pricing equilibria is inextricably connected to the computational complexity of related optimization problems: demand oracles, revenue-maximization and welfare-maximization. We demonstrate how this relationship implies, under suitable complexity assumptions, a host of impossibility results. We also suggest a complexity-theoretic explanation for the lack of useful extensions of the Walrasian equilibrium concept: such extensions seem to require the invention of novel polynomial-time algorithms for welfare-maximization.	algorithm;computation;computational complexity theory;entropy maximization;mathematical optimization;nash equilibrium;polynomial;time complexity	Tim Roughgarden;Inbal Talgam-Cohen	2015	SIGecom Exchanges	10.1145/2904104.2904109	mathematical optimization;complexity;economics;computer science;microeconomics;mathematical economics;welfare economics;equilibrium selection;algorithm	ECom	-3.1305415052136274	-0.6809565834787539	6444
0430c2d3d0c607625d7d021b15658083f82ef1a5	ensemble representation learning: an analysis of fitness and survival for wrapper-based genetic programming methods		Recently we proposed a general, ensemble-based feature engineering wrapper (FEW) that was paired with a number of machine learning methods to solve regression problems. Here, we adapt FEW for supervised classification and perform a thorough analysis of fitness and survival methods within this framework. Our tests demonstrate that two fitness metrics, one introduced as an adaptation of the silhouette score, outperform the more commonly used Fisher criterion. We analyze survival methods and demonstrate that ϵ-lexicase survival works best across our test problems, followed by random survival which outperforms both tournament and deterministic crowding. We conduct a benchmark comparison to several classification methods using a large set of problems and show that FEW can improve the best classifier performance in several cases. We show that FEW generates consistent, meaningful features for a biomedical problem with different ML pairings.	benchmark (computing);crowding;feature engineering;fitness function;genetic programming;machine learning	William La Cava;Jason H. Moore	2017	CoRR	10.1145/3071178/3071215	machine learning;silhouette;computer science;artificial intelligence;crowding;tournament;genetic programming;pattern recognition;feature learning;feature engineering	ML	10.890808183097475	-42.675142419505576	6476
28ef5252abf3427cd86ca3725fab7966efd95bf3	optimal recommendation to users that react: online learning for a class of pomdps	history;uncertainty;hidden markov models;markov processes;numerical models;adaptation models;context modeling	We describe and study a model for an Automated Online Recommendation System (AORS) in which a user's preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multi-armed bandit problem are also presented.	algorithm;machine learning;markov chain;multi-armed bandit;numerical analysis;partially observable markov decision process;partially observable system;provable security;recommender system;regret (decision theory);reinforcement learning;sampling (signal processing);thompson sampling	Rahul Meshram;Aditya Gopalan;D. Manjunath	2016	2016 IEEE 55th Conference on Decision and Control (CDC)	10.1109/CDC.2016.7799381	simulation;uncertainty;partially observable markov decision process;computer science;artificial intelligence;machine learning;mathematics;context model;markov process;markov model;hidden markov model;statistics	ML	20.976461621343567	-17.940996396141237	6477
e310e04efdf2f269488b5924b00c5b455d021540	evaluating unconfined compressive strength of cohesive soils stabilized with geopolymer: a computational intelligence approach		Soil stabilization using geopolymers is a new technique for improvement of weak cohesive soils. Evaluating behavior of improved soils requires an initial estimation of strength parameters. In this study, extensive experimental results on geopolymer-stabilized soil specimens were collected and analyzed. A model was then developed using group method of data handling (GMDH) and employing particle-swarm optimization algorithm to estimate the unconfined compressive strength (UCS) of stabilized cohesive soils using geopolymers. Type of additives and their compositions as well as soil characteristics were taken as the influential parameters on the UCS of soil specimens. Subsequently, sensitivity analysis was carried out to verify the performance of the proposed UCS model. Finally, the developed GMDH-based model was compared with artificial neural network model to predict unconfined compressive strength of stabilized soils. The results clearly illustrate the reasonable accuracy of the developed computational Intelligence-based model for estimating the unconfined compressive strength of geopolymer-stabilized cohesive soils.		Hamed Javdanian;Saro Lee	2018	Engineering with Computers	10.1007/s00366-018-0592-8	geotechnical engineering;compressive strength;geopolymer;mathematical optimization;soil water;group method of data handling;mathematics;soil stabilization;computational intelligence	AI	11.820646351533476	-19.654287708409214	6489
82b229e6411362976b619ee31ed69d8ffe938b7e	process capability for a non-normal quality characteristics data	six sigma;statistical measures;process capability indices;nonnormal quality characteristics data;manufacturing industries parameter estimation measurement standards statistics six sigma quality control manufacturing processes moment methods process design information technology;process capability;process capability analysis;normal distribution;information technology;moment methods;clements method;manufacturing industries;decision maker;process capability index;manufacturing industry process capability index nonnormal quality characteristics data statistical measures decision making process improvement burr distribution pci calculation clements method nonnormal distribution;process design;statistical distributions;manufacturing processes;nonnormal distribution;burr distribution;process parameters;statistics;manufacturing industry;process improvement;parameter estimation;measurement standards;quality control;word processing;pci calculation;statistical distributions manufacturing industries process capability analysis	Process capability indices (PCI) are widely used in manufacturing industry today. These statistical measures (Cp, Cpk) provide a quantitative measure of the process performance for decision makers. They are based on normality assumptions and provide a better estimation of process parameters if the process data is normally distributed. Unfortunately, this assumption is often violated in practice. In most cases, the distribution of a process characteristic data is non-normal. Application of conventional methods for calculation of process capability indices based on normal assumption will therefore give erroneous results that could lead to wrong decisions. In non-normal data situations, estimation of accurate PCI is critical for process improvement purposes. This paper explores application of a novel method (Pei-Hsi Liu and Feng-Long Chen, 2006) based on Burr distribution for PCI calculations when the process data is not normally distributed and compares simulation results with the commonly used Clements' method. Finally, an example illustrating application of this method with real world data is presented	chow–liu tree;entity–relationship model;randomness;simulation	Shafiq Ahmad;Mali Abdollahian;Panlop Zeephongsekul	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.159	process capability;process capability index;manufacturing;information technology;statistics	DB	28.129912031723713	-18.825128902406192	6503
7ca62f41fb0bbaa6d0c9b59258a2c219c2036e14	a confidence interval procedure for expected shortfall risk measurement via two-level simulation	administracion financiera;experimental design;analyse risque;criblage;worst conditional expectation;methode empirique;finance;analisis estadistico;intervalo confianza;financial management;risk analysis;efficiency screening methods;efficiency;screening;two level simulation;metodo empirico;simulation;gestion risque;conditional value at risk;risk management;empirical method;plan experiencia;grupo de excelencia;mesure niveau;conditional expectation;probabilistic approach;tail conditional expectation;esperance conditionnelle;scenario;analisis riesgo;confidence interval;esperanza condicional;hierarchical classification;design of experiments;statistical analysis;conditional tail expectation;plan experience;argumento;ciencias basicas y experimentales;enfoque probabilista;approche probabiliste;expected shortfall;intervalle confiance;matematicas;script;analyse statistique;level measurement;portfolio management;depistage;descubrimiento;classification hierarchique;cernido;gestion cartera;medical screening;gestion riesgo;risk measure;screening methods;gestion portefeuille;gestion financiere;grupo a;portfolio risk management;empirical likelihood;clasificacion jerarquizada;portfolio;finanzas;medicion nivel	We develop and evaluate a two-level simulation procedure that produces a confidence interval for expected shortfall. The outer level of simulation generates financial scenarios, whereas the inner level estimates expected loss conditional on each scenario. Our procedure uses the statistical theory of empirical likelihood to construct a confidence interval. It also uses tools from the ranking-and-selection literature to make the simulation efficient.	expected shortfall;simulation	Hai Lan;Barry L. Nelson;Jeremy Staum	2010	Operations Research	10.1287/opre.1090.0792	econometrics;confidence interval;economics;risk management;expected shortfall;operations management;mathematics;design of experiments;statistics	Metrics	25.657466796179808	-19.453413685496834	6509
2504b772d400e6f71611538a3fea861be08448d9	using input-space bisection and backpropagation as a method to emulate complex numerical simulations for design decision making	databases;design engineering;neural nets;cad;information technology;training;geometry;testing;backpropagation;decision maker;numerical simulation decision making artificial neural networks research and development computational modeling backpropagation algorithms testing information technology laboratories design engineering;fixed point;physics;artificial neural networks;computational modeling;research and development;complex numerical simulations;backpropagation algorithm;design decision making;backpropagation algorithms;error bound;contaminant trace;numerical models;hydraulic flow;input space bisection;hydraulic flow input space bisection complex numerical simulations design decision making backpropagation algorithm artificial neural networks contaminant trace;neural nets backpropagation cad decision making design engineering;artificial neural network;numerical simulation	This paper advocates integrating two decades-old tools, namely, input-space bisection and backpropagation, in support of design decision making. Either technique independently could offer some improvements in efficiency and reduced computational costs; however, taken in concert their effects could be greatly amplified. Traditional design decisions have often relied too heavily upon expensive, time-consuming HPC runs. This paper demonstrates from three disparate cases that a carefully chosen input space serving as training instances for a backpropagation algorithm can often perform within acceptable error bounds for the test cases, while providing a much quicker response to decision makers. First, we introduce the problem domain, stressing the inadequacies and the inefficiencies of using too highly resolved parameter variations in batch HPC runs. We then provide a step-by-step overview of our surrogate model approach. We describe the similarities of the requirements for running both numerical solvers and artificial neural networks. Our method entails using an input-space bisection to reduce (in both the spatial and temporal domains) the output of the numerical solver, the results of which will serve as training cases for the backpropagation algorithm. The technique is demonstrated in three test cases, two of which are built to predict behavior at a fixed point in time, while the other, more ambitiously, tackles contaminant trace in hydraulic flow over multiple time-steps. Finally, we summarize some of the benefits this approach offers and outline directions for future research.	algorithm;algorithmic efficiency;artificial neural network;backpropagation;brute-force search;computation;correctness (computer science);fixed point (mathematics);interpolation;mathematical optimization;numerical analysis;problem domain;requirement;simulation;solver;surrogate model;test case	Mark A. Cowan;Cary D. Butler	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5178629	computer science;artificial intelligence;backpropagation;machine learning;artificial neural network;algorithm	ML	14.698943790952866	-20.663354385599348	6526
2601442d2c436c56ac05729a994fa749835f4858	dynamic texture recognition via orthogonal tensor dictionary learning	dictionaries tensile stress encoding learning systems scalability feature extraction computational modeling;tensors image coding image recognition image sequences image texture;dynamic texture recognition dt descriptor separability orthogonality structured tensor dictionary learning method local dt patterns sparse coding based approach stationary properties video sequences orthogonal tensor dictionary learning algorithm	Dynamic textures (DTs) are video sequences with stationary properties, which exhibit repetitive patterns over space and time. This paper aims at investigating the sparse coding based approach to characterizing local DT patterns for recognition. Owing to the high dimensionality of DT sequences, existing dictionary learning algorithms are not suitable for our purpose due to their high computational costs as well as poor scalability. To overcome these obstacles, we proposed a structured tensor dictionary learning method for sparse coding, which learns a dictionary structured with orthogonality and separability. The proposed method is very fast and more scalable to high-dimensional data than the existing ones. In addition, based on the proposed dictionary learning method, a DT descriptor is developed, which has better adaptivity, discriminability and scalability than the existing approaches. These advantages are demonstrated by the experiments on multiple datasets.	algorithm;algorithmic efficiency;atom;computation;dictionary;experiment;feature integration theory;layer (electronics);linear separability;machine learning;neural coding;numerical analysis;refinement (computing);scalability;sparse approximation;sparse matrix;stationary process	Yuhui Quan;Yan Huang;Hui Ji	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.17	k-svd;computer science;theoretical computer science;machine learning;pattern recognition	Vision	27.884520319458	-44.58067318095736	6533
1499145734c36e111df74d3bab5c4a84ab054c1b	two-way decodable variable-length data blocks for robust video transmission	reversible variable length codes;variable length code;quality improvement;video coding;uncertainty analysis;error propagation;video transmission;compression ratio;video;compressed video	In this paper a novel robust video transmission scheme is proposed that can not only decode the compressed video stream in two directions but also correct channel errors with limit burst length. Standard video coders usually utilize variable length coding (VLC) to obtain high compression ratio, which makes the compressed bit stream very vulnerable to channel errors. Even one bit error may lead to loss of synchronization with the following bits undecodable and result in error propagation. In order to limit error propagation due to VLC we put forward a new video coding scheme named as two-way decodable variable length data blocks (TDVLDB), which makes the compressed bit stream bi-directionally decodable through reversal and XOR operations. Great subjective and object quality improvements have been found in our simulation results compared with other resilient schemes, such as the reversible variable length codes (RVLC) scheme, the partial backward decodable bit stream (PBDBS) scheme and the early resynchronization (ER) scheme.© (2004) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Shaoshuai Gao;Can Zhang	2004		10.1117/12.525331	real-time computing;telecommunications;computer science;theoretical computer science	Vision	47.827441285010124	-16.20460530445422	6539
4ceb9f530549f3edb3369fd0bf7406d55354f9c4	scenenet: an annotated model generator for indoor scene understanding	statistical analysis image reconstruction rendering computer graphics simulated annealing solid modelling;rendered annotated sequences scenenet framework indoor scene understanding 3d scene annotation hierarchical simulated annealing optimisation statistics learning 3d object database opensurfaces archivetextures modelnet database 3d reconstruction benchmarking supervised training;three dimensional displays solid modeling training simulated annealing databases simultaneous localization and mapping layout	We introduce SceneNet, a framework for generating high-quality annotated 3D scenes to aid indoor scene understanding. SceneNet leverages manually-annotated datasets of real world scenes such as NYUv2 to learn statistics about object co-occurrences and their spatial relationships. Using a hierarchical simulated annealing optimisation, these statistics are exploited to generate a potentially unlimited number of new annotated scenes, by sampling objects from various existing databases of 3D objects such as ModelNet, and textures such as OpenSurfaces and ArchiveTextures. Depending on the task, SceneNet can be used directly in the form of annotated 3D models for supervised training and 3D reconstruction benchmarking, or in the form of rendered annotated sequences of RGB-D frames or videos.	3d modeling;3d reconstruction;graphics pipeline;hierarchical database model;mathematical model;mathematical optimization;sampling (signal processing);simulated annealing;synthetic intelligence	Ankur Handa;Viorica Patraucean;Simon Stent;Roberto Cipolla	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487797	computer vision;computer science;machine learning;data mining	Robotics	46.191557575386575	-50.6880823989986	6540
ba3828f90850194daf764c4fc28f39f33dc48719	pruning gp-based classifier ensembles by bayesian networks	classifier ensemble;gp-based classifier ensemble;single classifier system;large number;large datasets;proposed system;bayesian network;large memory requirement;decision tree classifier;classifier selection;original ensemble;classifier ensemble technique	Classifier ensemble techniques are effectively used to combine the responses provided by a set of classifiers. Classifier ensembles improve the performance of single classifier systems, even if a large number of classifiers is often required. This implies large memory requirements and slow speeds of classification, making their use critical in some applications. This problem can be reduced by selecting a fraction of the classifiers from the original ensemble. In this work, it is presented an ensemble-based framework that copes with large datasets, however selecting a small number of classifiers composing the ensemble. The framework is based on two modules: an ensemble-based Genetic Programming (GP) system, which produces a high performing ensemble of decision tree classifiers, and a Bayesian Network (BN) approach to perform classifier selection. The proposed system exploits the advantages provided by both techniques and allows to strongly reduce the number of classifiers in the ensemble. Experimental results compare the system with well-known techniques both in the field of GP and BN and show the effectiveness of the devised approach. In addition, a comparison with a pareto optimal strategy of pruning has been performed.	algorithm;bayesian network;decision tree;ensemble forecasting;genetic programming;learning classifier system;overhead (computing);pareto efficiency;requirement;run time (program lifecycle phase);the k2	Claudio De Stefano;Gianluigi Folino;Francesco Fontanella;Alessandra Scotto di Freca	2012		10.1007/978-3-642-32937-1_24	random subspace method;cascading classifiers;computer science;machine learning;pattern recognition;data mining;ensemble learning	ML	11.89689740123674	-41.18999143939107	6544
ee6a3f972f68b8d7211a79c96e78995ca7389117	predictive mitigation of short term voltage instability using a faster than real-time digital replica		Predictive mitigation of undesired events has long been seen as a supportive complement to corrective mitigation that could relax the stringent requirements on the corrective actions and increase reliability of the overall system. This article describes one such predictive measure, i.e. the use of faster than real-time simulation in detecting faults and predicting the dynamic behavior for the resilient operation of future smart grid systems. A predictive mitigation strategy is proposed for a fault induced dynamic voltage recovery (FIDVR) event. These events, although rare, are typically addressed with under voltage load shedding schemes (UVLS) which leave significant portion of load under-supplied. We show that, by using the digital faster than real-time replica, the minimal level of UVLS can be determined on-the fly as the event develops while ensuring only the minimal amount of load shed.		Arun Joseph;Milo&#x0161; Cvetkovi&#x0107;;Peter Palensky	2018	2018 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)	10.1109/ISGTEurope.2018.8571803	replica;smart grid;load shedding;voltage;real-time computing;instability;ac power;computer science	EDA	36.264820188902455	0.20621518406941056	6556
16971eabb2ce18ae7aeaf7bc31f08db2efc4ef81	generating real-time decision systems with the new miaamics		We sketch miAamics, an approach and a tool to rapidly evaluate large systems of rules. These large systems of rules can be used to express performance critical decision functions and allow for the miAamics approach to optimize the function and to generate its implementation fully automatically. In this way, we allow experts to define functions without having to be familiar with general purpose programming languages and also allow to optimize existing decision functions that can be expressed in form of these rules. The proposed approach first transforms the system of rules to Algebraic Decision Diagrams. From this data structure, we generate code in a variety of commonly used target programming languages. We present preliminary results from experiments with randomly generated rules and show that the proposed representation is significantly faster to evaluate and is also smaller in size than the original representation. We give an outlook on possible applications for the miAamics approach to real world tasks focusing on the field of machine learning. In particular, we aim to reduce ensembles of classifiers and to allow for a much faster evaluation of these classification methods.	real-time transcription	Frederik Gossen;Tiziana Margaria	2017	ECEASST	10.14279/tuj.eceasst.74.1056	algebraic number;ensembles of classifiers;theoretical computer science;sketch;computer science;data structure	Embedded	9.232620239836756	-43.77952362741892	6558
a8bb6843d69149b6c067d6016d42f5375a67be39	a background subtraction algorithm for a pan-tilt camera	cameras optical imaging computational modeling conferences computer vision adaptation models image registration;computer vision;statistical analysis cameras image sequences motion compensation object detection;computational modeling;optical imaging;image registration;marginal statistical models background subtraction algorithm pan tilt camera moving object detection motion compensation pixel misalignment pixel motion optical flow false positive foreground detection;adaptation models;cameras;conferences	This paper is concerned with the detection of moving objects using a pan-tilt camera and a background subtraction algorithm. Traditionally, motion compensation is performed on the current image to align its pixels with their background models in previous frames. Pixel misalignment however can occur during motion compensation. Although this problem can be alleviated by using pixel motion such as the optic flow, motion information itself can be inaccurate and, together with pixel misalignment, contributes to false positive foreground detection. In this paper, we exploit the fact that pixel misalignment and inaccurate optic flow tend not to occur simultaneously for a pixel. Consequently, we can substantially improve the performance of the background subtraction algorithm by evaluating the marginal statistical models of appearance and motion separately - rather than jointly - in classifying whether a pixel is foreground. We will use experiments to validate our approach and establish its superiority to other competing algorithms in the literature.	algorithm;align (company);background subtraction;experiment;focal (programming language);image registration;marginal model;mobile device;motion compensation;optical flow;pixel;sensor;statistical model	Ying Chen;Hong Zhang	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090472	computer vision;background subtraction;computer science;image registration;motion estimation;optical imaging;optics;motion field;computational model;physics;computer graphics (images)	Vision	47.180202804610396	-48.15198057691677	6564
80aa91afa0629181369f9312232a9b959220d37b	point cloud data filtering and downsampling using growing neural gas	three dimensional displays neurons solid modeling noise detectors feature extraction;object recognition;neural nets;image classification;mobile robots;image sensors;robot vision feature extraction image classification image registration image sensors mesh generation mobile robots neural nets object recognition;robot vision;feature extraction;image registration;3d scene registration point cloud data filtering point cloud data downsampling growing neural gas 3d sensors mobile robotic tasks scene classification object recognition noisy data keypoint detection feature extraction techniques 3d filtering gng 3d spaces delaunay triangulation voxel grid robotics applications;mesh generation	3D sensors provide valuable information for mobile robotic tasks like scene classification or object recognition, but these sensors often produce noisy data that makes impossible applying classical keypoint detection and feature extraction techniques. Therefore, noise removal and downsampling have become essential steps in 3D data processing. In this work, we propose the use of a 3D filtering and downsampling technique based on a Growing Neural Gas (GNG) network. GNG method is able to deal with outliers presents in the input data. These features allows to represent 3D spaces, obtaining an induced Delaunay Triangulation of the input space. Experiments show how GNG method yields better input space adaptation to noisy data than other filtering and downsampling methods like Voxel Grid. It is also demonstrated how the state-of-the-art keypoint detectors improve their performance using filtered data with GNG network. Descriptors extracted on improved keypoints perform better matching in robotics applications as 3D scene registration.	algorithm;computer-aided design;decimation (signal processing);delaunay triangulation;experiment;feature extraction;kinect;mobile robot;neural gas;outline of object recognition;point cloud;point set registration;robotic mapping;robotics;sensor;signal-to-noise ratio;voxel	Sergio Orts;Vicente Morell;José García Rodríguez;Miguel Cazorla	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706719	mobile robot;mesh generation;computer vision;contextual image classification;feature extraction;computer science;image registration;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;image sensor;artificial neural network	Robotics	53.10675053029736	-43.328535843121124	6569
4b76b63c25869d87228f3838b6e53dc78be8eed8	using artificial anomalies to detect unknown and known network intrusions	misuse detection;empirical study;analisis datos;securite telecommunication;apprentissage inductif;anomaly detection;intrusion detection;securite reseau;data analysis;artificial anomaly;aprendizaje por induccion;detection anomalie;anomalie artificielle;telecommunication security;inductive learning;intrusion detection systems;analyse donnee;security;systeme detection intrusion;building detection;intrusion detection system	Intrusion detection systems (IDSs) must be capable of detecting new and unknown attacks, or anomalies. We study the problem of building detection models for both pure anomaly detection and combined misuse and anomaly detection (i.e., detection of both known and unknown intrusions). We show the necessity of artificial anomalies by discussing the failure to use conventional inductive learning methods to detect anomalies. We propose an algorithm to generate artificial anomalies to coerce the inductive learner into discovering an accurate boundary between known classes (normal connections and known intrusions) and anomalies. Empirical studies show that our pure anomaly-detection model trained using normal and artificial anomalies is capable of detecting more than 77% of all unknown intrusion classes with more than 50% accuracy per intrusion class. The combined misuse and anomaly-detection models are as accurate as a pure misuse detection model in detecting known intrusions and are capable of detecting at least 50% of unknown intrusion classes with accuracy measurements between 75 and 100% per class.	algorithm;anomaly detection;concurrency (computer science);intrusion detection system;misuse detection;sensor	Wei Fan;Matthew Miller;Salvatore J. Stolfo;Wenke Lee;Philip K. Chan	2003	Knowledge and Information Systems	10.1007/s10115-003-0132-7	anomaly-based intrusion detection system;intrusion detection system;anomaly detection;computer science;artificial intelligence;machine learning;computer security	Security	6.63814286289212	-33.580715974388326	6570
9a0428a8a377bf4137023ca31deb5be1b272a49b	fast landmark defection system based on the multi-resolution image matching scheme for home robot navigation		"""We devised real-time navigation schemes that enable autonomous travel of a robot on the unoccupied premises of an ordinary house based only on visual information. To accomplish this, we made use of the limitations on the specific operating range of a robot in an empty house. The range can be limited to specific work locations and the paths that connect them. The schemes consist of one with a low degree of accuracy that is used for the travel paths and one with a high degree of accuracy that is used for work locations. The use of low-resolution images helped enable removal of the effects of small changes made daily in the scenes by human activities. The identification of high-resolution characteristic points through application of a coarse-to-fine method ensured the accuracy of self-localization, which is required in work locations. Although such image processing generally requires a large amount of computation, the introduction of hardware dedicated to image correlation provided practical speed. 1: Introduction Now that the average family is becoming smaller, there is an increasing demand for domestic robots designed for assistance in performing household chores. One of the technologies required for domestic robots that we are interested in is the autonomous travel of a robot in an ordinary household while the residents are absent. In this situation, the information that a robot can use for self-navigation is assumed to be only the image information that is received from its cameras. We established this limitation for two reasons. 1) A person, even in an unfamiliar place, can walk around relatively safely without bumping into anything by recognizing the presence of nearby objects and creating a visual image in his or her brain. In a situation where a person is provided with only traveling images through a video camera, he or she can guess the camera location and its travel fairly accurately if the locations shown by the images are known or similar to what is known. In other words, it is clear that there is a method for estimating location using only a memorized map and image information. 2) In terms of reduced cost, a robot should contain as few types and numbers of sensors as possible. Since image processing such as matching requires a large amount of computation, robot navigation that depends entirely on this method has, so far, been rarely examined. However, this barrier was lowered considerably by """"Tracking Vision,"""" specialized hardware for image correlation. We tested a method of matching using this hardware. The following paragraphs discuss the results of the test. 2: Problems We encountered several problems in attempting to have a robot conduct human-like self-navigation based only on visual information. As yet, no scheme is established to automate the creation of a map of the environment based only on visual information in the same way that a human being does so subconsciously. Therefore, a detailed map consisting of coordinate information that is linked with visual information must be input to a robot. This approach requires a large amount of labor and does not lead to a practical solution. However, it is expected that the tasks that an unattended robot performs at home do not vary greatly from day to day as far as the travel paths, work locations, and work items are concerned. In other words, we can think of the following schemes that do not require detailed map information but only simple instructions and specifications of locations. Having a robot memorize only the positions where tasks are performed, including the """"home location"""" and the major """"transit points"""" for traveling (hereafter called """"work locations""""), and the travel paths connecting these locations as a set of surrounding images seen from the cameras of the robot at these locations and on these paths This method has the following technical problems: (a) During autonomous travel, errors must be visually detected so the robot can reach the specified location with sufficient accuracy. (b) The robot must not be confused by the following two visual characteristics of the human living environment. Furthermore, the use of visual cues designed only to help the robot avoid such confusion does not conform to the premise of using the robot in an ordinary household. (b-1) Details of the surroundings in an ordinary household are temporally unstable due to daily activities of the residents. (b-2) In a living space for humans, such as an ordinary household, from one place, a robot might see more than one image that is similar to characteristic points, i.e., possibly effective cues for the robot. (c) Since image matching generally requires a large amount of computation, a preferable implementation is one that has as little as possible. 3: Proposed Schemes In the proposed schemes, a robot travels basically by measuring the distance to landmarks in images and identifying and correcting its own location. However, we effectively utilize the requirement that different levels of accuracy are employed for self-localization when the robot is on a travel path and when it arrives at a work location. This is because the constant comparison of large numbers of all recorded images and the current surroundings is impractical due to the limitations imposed by the required amounts of computation and storage. Therefore, we propose the following schemes for travel. (1) Navigation along a travel path While traveling along a travel path from one work location to another, a robot records its surroundings as a set of sequenced low-resolution panoramic images. The low-resolution approach avoids problems (b-I) and (b-2), mentioned above, and alleviates problem (c). Problem (a) is not considered here because its solution is only necessary at a """"work location."""" The navigation procedure used for autonomous travel is ----...Address: KEN-52.4II. Kamikodanaka, Nakahra-ku, Kawasaki-city. Kanagi < iwa-ken. 21 1-8588 Japan. E-mail: sirnisi@stnrs.~ab.fujitsu.co j p sawasnki@flab.f~i tsu.~o~~ as follows: From a database, a robot retrieves a set of low-resolution panoramic images that constitute the path it is to follow. During travel, the robot repeatedly performs the following calculations and continuously corrects the direction of travel. The robot reduces a front image captured by the camera to create a small, low-resolution rectangular image. The robot correlates the reduced image with the low-resolution panoramic images from the database and determines which of the latter has the highest correlation. The robot recognizes the vicinity of the location where the chosen low-resolution panoramic image was captured as the current position of the camera (and thus, of the robot). In addition, the robot finds the location in the low-resolution panoramic image that best matches the currently captured image and converts the deviation between the location and the center of the low-resolution panoramic image to an angle. The deviation should be close to the deviation between the direction in which the robot is supposed to be looking and in which the camera is actually looking. The end of a travel path is the next """"work location."""" When approaching the work location, the robot makes a more accurate """"work location amval decision."""" The procedure for this is to clip a rectangular area from the center of the image in front of the robot and convert it to the same resolution as the low-resolution panoramic images. This is then reduced or enlarged through several levels to compare it with the past low-resolution panoramic images captured at the """"work location."""" If the reduced front image has a greater match, the robot decides that it has gone past the """"work location."""" If the enlarged front image has a greater match, it has not yet arrived at the """"work location."""" If the front image that is neither reduced nor enlarged has the greatest match, it is approximately at the """"work location."""" Based on these results, the robot fine-tunes its location consisting of low-resolution panoEstimating self-location and direction and correcting the travel direction by comparing the current surroundings with the low-resolution panoramic images"""	autonomous robot;computation;control theory;database;domestic robot;emoticon;image processing;image registration;image resolution;ku band;map;matching (graph theory);real-time web;reduced cost;robotic mapping;sensor;temporal logic;webcam	Atsushi Shiraishi;Naoyuki Sawasaki	2002			artificial intelligence;mathematics;computer vision;reduced cost;computation;image processing;landmark;robot;mobile robot navigation;template matching;video camera	Robotics	48.653424049473536	-37.8141277200946	6582
b4f3fb8bd7cb0704887fab02899905f775d15e2c	rate distortion optimized inter-view frame level bit allocation method for mv-hevc	analytical models;rate distortion;video coding convex programming rate distortion theory;mv hevc;bit rate;rate distortion optimization bit allocation inter view dependency mv hevc;video coding;distortion;bit rate rate distortion video coding analytical models rate distortion theory distortion convex functions;lagrangian multiplier method rate distortion optimized inter view frame level bit allocation method mv hevc multiview video coding nonreference view pictures nrv pictures convex optimization;期刊论文;cities and towns;bit allocation;rate distortion optimization;encoding;inter view dependency	In multi-view video coding, since inter-view prediction has been adopted as an important coding tool which could improve coding efficiency greatly, inter-view dependency is inevitable, i.e., the distortion of the reference view (RV) picture could be propagated to the non-reference view (NRV) pictures . Therefore, in order to achieve higher coding efficiency , the inter-view dependency must be taken into account for inter-view bit allocation. In this paper, the inter-view dependency is analyzed in detail, and a rate-distortion (RD) model for NRVs is derived by taking the distortion of RV into account. Based on the derived RD model, the inter-view bit allocation is represented as a mathematical problem with an analytic form, and is solved by a convex optimization (Lagrangian Multiplier) method. Experimental results demonstrate that the RD performance and the inter-view quality consistency of the proposed method is better than existing methods, while the complexity of the proposed method is comparable with the existing methods.	algorithmic efficiency;augmented lagrangian method;convex optimization;data compression;distortion;high efficiency video coding;lagrange multiplier;mv-algebra;mathematical optimization;optimization problem;rate–distortion theory;ruby document format;software propagation	Hui Yuan;Sam Kwong;Xu Wang;Wei Gao;Yun Zhang	2015	IEEE Transactions on Multimedia	10.1109/TMM.2015.2477682	mathematical optimization;discrete mathematics;distortion;telecommunications;computer science;theoretical computer science;mathematics;rate–distortion optimization;encoding	AI	46.585653151874126	-17.67619816891385	6584
cd2523dc9a2585d15ac0c4d311b6f7196db4c887	a model-free voting approach to cue integration		Vision systems, such as “seeing” robots, should be able to operate robustly in generic environments. In this thesis, we investigate certain aspects of how these demands of robustness of a systems approach to vision could be met. Firstly, we suggest that robustness can be improved by fusing the variety of information offered by the environment, and, therefore, we investigate the effectiveness of using the coincidence of multiple cues. Secondly, we are concerned about the use of coarse algorithms. Even though the environment provides much information, it is neither necessary nor possible to extract all information available. Therefore, we will show that coarse algorithms will suffice for certain problems. To investigate the effectiveness of using the coincidence of multiple cues, we perform a series of experiments on detecting planar surfaces in binocular images. These experiments are based on two schemes of a somewhat different character. The first one is a hypothesis-and-test scheme that incorporates the cues in a certain order and hence, by design, imposes a ranking of them. The general idea is to use arbitrary cues exploiting local image data to get an idea about whether the model (a planar surface) is seen in the image and at which location it is found. If one or more cues strongly indicate a certain instance of a model, then this observation serves as a hypothesis to be tested by other cues to support or reject this hypothesis. In comparison to the cues used for hypothesis generation, those used for hypothesis testing should be more reliable and can also have a higher computational complexity since they are only employed when needed. The general idea of the second scheme is to first use a simple, and quick cue exploiting local image data to get an idea of where in the image the model (a planar surface) could be found. After this initial localization step, all cues	algorithm;binocular vision;computational complexity theory;experiment;robot;sensor	Carsten G. Bräutigam	1998			psychology;computer vision;simulation;social psychology	Vision	43.038087971078184	-39.735275114987544	6586
7d097fb3fc1977d4f05122860560e5483e91b398	planning with action prioritization and new benchmarks for classical planning	benchmarks;planning;action priority	We introduce a new class of planning problems in which there is a separate set of actions with higher priority than regular actions. We present new planning domains to show that problems of practical interest may easily fit in this framework. We argue that though this framework is quite succinctly encoded in classical planning itself, existing planners are disappointingly inept at solving them. To demonstrate this, we have built a wrapper tool for planners which uses ad-hoc techniques to give far better results. Therefore, we also propose our encoded domains as new challenges for general-purpose planners.		Kamalesh Ghosh;Pallab Dasgupta;S. Ramesh	2012		10.1007/978-3-642-35101-3_66	simulation;computer science;operations management;management science	Vision	19.035903048648947	-10.50955261720858	6603
feea624a4a24147023cf54f1574a3d301cbda345	output-input ratio analysis and dea frontier	efficiency;efficient frontier;mathematical programming;data envelopment analysis dea;ratio analysis;decision making unit;frontier;data envelope analysis	Data envelopment analysis (DEA) is a mathematical programming technique for identifying efficient frontiers for peer decision making units (DMUs). The ability of identifying frontier DMUs prior to the DEA calculation is of extreme importance to an effective and efficient DEA computation. In this paper, we present mathematical properties which characterize the inherent relationships between DEA frontier DMUs and output–input ratios. It is shown that top-ranked performance by ratio analysis is a DEA frontier point. This in turn allows identification of membership of frontier DMUs without solving a DEA program. Such finding is useful in streamlining the solution of DEA.		Yao Chen;Agha Iqbal Ali	2002	European Journal of Operational Research	10.1016/S0377-2217(01)00318-6	efficient frontier;econometrics;economics;operations management;data envelopment analysis;efficiency;operations research	Vision	0.35198032938288504	-13.740753244927866	6604
5d40ad4b56f23c0cb68c72453efe0b379068d457	towards analyzing multimodality of continuous multiobjective landscapes		This paper formally defines multimodality in multiobjective optimization (MO). We introduce a test-bed in which multimodal MO problems with known properties can be constructed as well as numerical characteristics of the resulting landscape. Gradientand local search based strategies are compared on exemplary problems together with specific performance indicators in the multimodal MO setting. By this means the foundation for Exploratory Landscape Analysis in MO is provided.	local search (optimization);mathematical optimization;multi-objective optimization;multimodal interaction;numerical analysis;testbed	Pascal Kerschke;Hao Wang;Mike Preuss;Christian Grimme;André H. Deutz;Heike Trautmann;Michael T. M. Emmerich	2016		10.1007/978-3-319-45823-6_90	computer science;artificial intelligence;machine learning;mathematical optimization;multimodality;local search (optimization);multi-objective optimization	SE	23.87464657541817	-5.353183595835675	6607
f23ad0528d4d2139fade4f8a51d3aedeb9c85666	local relative transformation with application to isometric embedding	evaluation performance;high dimensionality;performance evaluation;learning;noisy data;analisis forma;evaluacion prestacion;isometrie;isomap algorithm;recherche voisinage;outlier;manifold learning;algorithme isomap;aprendizaje;observacion aberrante;large scale;apprentissage;cognitive law;isometria;local relative transformation;observation aberrante;pattern analysis;isometric embedding;neighborhood search;isometry;neighborhood graph;analyse forme;busqueda de cercania;relative transformation	Current isometric embedding approaches are topologically unstable when confronted with noisy data, as where the neighborhood is critically distorted. Based on the cognitive law, a relative transformation (RT), which improves the distinction between data points and diminishes the impact of noise on isometric embedding approaches, is proposed. As the constructed space from large scale data by RT is high-dimensional, local relative transformation (LRT) is further proposed. Subsequently, a new isometric embedding approach is developed by using LRT to construct a better neighborhood graph with fewer short-circuit edges, while the embedding is still performed in the original space. This approach has significantly increased performance and reduced running time. The proposed approach was validated by experiments on challenging benchmark data sets. 2008 Elsevier B.V. All rights reserved.	benchmark (computing);control theory;data point;distance (graph theory);euclidean distance;experiment;isometric projection;long-running transaction;nonlinear dimensionality reduction;signal-to-noise ratio;time complexity	Guihua Wen;Lijun Jiang;Jun Wen	2009	Pattern Recognition Letters	10.1016/j.patrec.2008.09.005	combinatorics;outlier;topology;isometry;computer science;machine learning;mathematics;geometry;nonlinear dimensionality reduction;statistics	AI	29.251885383862753	-38.995197009425326	6642
90bb92df9740dfa4de179ac25e46ff8b3a285910	transportation policies for single and multi-objective transportation problem using fuzzy logic	logical relation;calculo de variaciones;computer aided analysis;analisis numerico;transportation models;sistema de transporte;problema transporte;transportation problem;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;stochastic method;numerical method;probleme transport;fuzzy relation;logique floue;65kxx;multi objective optimization;logica difusa;optimization method;modelo matematico;algoritmo genetico;65k10;03b52;metodo optimizacion;analyse numerique;65c20;fuzzy logic;49xx;calcul variationnel;transport policy;numerical analysis;metodo numerico;mathematical programming;methode optimisation;mathematical model;algorithme genetique;methode stochastique;systeme transport;analisis asistido;genetic algorithm;transport costs;applied mathematics;programmation mathematique;transportation system;programacion matematica;variational calculus;methode numerique;real coded genetic algorithm;object model;metodo estocastico	In this paper, single and multi-objective transportation models are formulated with fuzzy relations under the fuzzy logic. In the single-objective model, objective is to minimize the transportation cost. In this case, the amount of quantities transported from an origin to a destination depends on the corresponding transportation cost and this relation is verbally expressed in an imprecise sense i.e., by the words 'low', 'medium', 'high'. For the multi-objective model, objectives are minimization of (i) total transportation cost and (ii) total time for transportation required for the system. Here, also the transported quantity from a source to a destination is determined on the basis of minimum total transportation cost as well as minimum transportation time. These relations are imprecise and stated by verbal words such as 'very high', 'high', 'medium', 'low' and 'very low'. Both single objective and multi-objective problems using Real coded Genetic Algorithms (GA and MOGA) are developed and used to solve the single level and bi-level logical relations respectively. The models are illustrated with numerical data and optimum results are presented.	fuzzy logic;transportation theory (mathematics)	Anupam Ojha;Shyamal Kumar Mondal;Manoranjan Maiti	2011	Mathematical and Computer Modelling	10.1016/j.mcm.2010.12.029	mathematical optimization;fuzzy transportation;numerical analysis;artificial intelligence;mathematics;operations research;algorithm	Vision	14.432948181880374	-2.7559123235901675	6650
99c0f1194f99dd3c6586f39c945b6cc8cbb43d67	power, prices, and incomes in voting systems	voting game;linear program;voting power;simple game	The power of voters in a voting game (simple game) is viewed as the amount they would be paid by a lobbyist buying their votes. Equilibrium prices for the voters are shown to exist whenever there is no veto player, and the expected incomes of the voters are compared with other measures of their voting power.		H. Peyton Young	1978	Math. Program.	10.1007/BF01588961	bullet voting;mathematical optimization;single-member district;linear programming;approval voting;mathematics;cardinal voting systems;preferential block voting;weighted voting;anti-plurality voting;condorcet method;disapproval voting	ECom	-2.1444614351323117	-3.402778041165128	6657
3bf5f03e1b758d4c6e9f231ec97cd60023b051d0	web usage mining for improving students performance in learning management systems	educational data mining;web system;genetic program;grammar guided genetic programming;multiple instance learning;multi objective evolutionary algorithm;learning management system;feature extraction;classification rules;web usage mining;web based system;missing values;student performance	An innovative technique based on multi-objective grammar guided genetic programming (MOG3P-MI) is proposed to detect the most relevant activities that a student needs to pass a course based on features extracted from logged data in an education web-based system. A more flexible representation of the available information based on multiple instance learning is used to prevent the appearance of a great number of missing values. Experimental results with the most relevant proposals in multiple instance learning in recent years demonstrate that MOG3P-MI successfully improves accuracy by finding a balance between specificity and sensitivity values. Moreover, simple and clear classification rules which are markedly useful to identify the number, type and time of activities that a student should do within the web system to pass a course are provided by our proposal.	algorithm;final exam;genetic programming;management system;missing data;multiple instance learning;sensitivity and specificity;web application;web mining	Amelia Zafra;Sebastián Ventura	2010		10.1007/978-3-642-13033-5_45	web mining;web modeling;feature extraction;computer science;artificial intelligence;data science;machine learning;data mining	ML	8.41477375903317	-43.25863441573449	6659
47e7c679bbc1f66d53052b54deef300751763e51	playout policy adaptation for games		Monte Carlo Tree Search (MCTS) is the state of the art algorithm for General Game Playing (GGP). We propose to learn a playout policy online so as to improve MCTS for GGP. We test the resulting algorithm named Playout Policy Adaptation (PPA) on Atarigo, Breakthrough, Misere Breakthrough, Domineering, Misere Domineering, Go, Knightthrough, Misere Knightthrough, Nogo and Misere Nogo. For most of these games, PPA is better than UCT with a uniform random playout policy, with the notable exceptions of Go and Nogo.	algorithm;experiment;general game playing;monte carlo method;monte carlo tree search;nested raid levels;playout;quickdraw 3d;redundancy (engineering)	Tristan Cazenave	2015		10.1007/978-3-319-27992-3_3	multimedia	AI	20.606856495154336	-17.080322214105028	6663
45f4c1c63ee8bb97d355d1434982292877e7c289	gaze estimation in a gaze tracking system	gaze tracking;pupil center cornea reflection pccr;individual calibration;head compensation;gaze tracking gaze estimation pupil center cornea reflection pccr head compensation individual calibration;gaze estimation	This article presents a gaze estimation method (GEMHSSO) based on the pupil center cornea reflection (PCCR) technique. Existing PCCR systems suffer from several problems, including the restriction of users’ head movement, and the requirement of individual calibration. This paper presents a head position compensation model using a single camera and a single light source, which realizes the analytic compensation of head motion effects on pupil-glint vectors. We then present a transformation model for individual differences to simplify the calibration process to a one-point calibration. On this basis, we establish a novel gaze estimation method that reduces the minimum hardware requirements for accurate estimation to a single camera (not calibrated) and a single light source. Without the need for a complex system, our method has the ability to estimate gaze during natural head movement, and to substantially simplify user calibration. Each step of the proposed method in this paper makes real-time implementation possible, which provides an effective solution for eye-gaze tracking in human-computer interaction systems.	complex system;eye tracking;feature detection (web development);human–computer interaction;real-time clock;requirement;tracking system	Chuang Zhang;Jian-Nan Chi;Zhaohui Zhang;Xiaoliang Gao;Tao Hu;Zhiliang Wang	2011	Science China Information Sciences	10.1007/s11432-011-4243-6	computer vision	Robotics	48.593972964669184	-44.225377288872906	6671
6514cdf414e64af1125637860350175f23dd5ae1	parallel nearest neighbour algorithms for text categorization	classification algorithm;personal computer;machine learning;message passing;nearest neighbour;text categorization	In this paper we describe the parallelization of two nearest neighbour classification algorithms. Nearest neighbour methods are well-known machine learning techniques. They have been successfully applied to Text Categorization task. Based on standard parallel techniques we propose two versions of each algorithm on message passing architectures. We also include experimental results on a cluster of personal computers using a large text collection. Our algorithms attempt to balance the load among the processors, they are portable, and obtain very good speedups and scalability.	benchmark (computing);categorization;central processing unit;computation;distributed memory;document classification;experiment;filter (signal processing);k-nearest neighbors algorithm;knowledge organization;machine learning;master/slave (technology);message passing interface;parallel algorithm;parallel computing;personal computer;pipeline (computing);routing;scalability;speedup	Reynaldo Gil-García;José M. Badía;Aurora Pons-Porrata	2007		10.1007/978-3-540-74466-5_36	message passing;computer science;theoretical computer science;machine learning;pattern recognition;programming language	NLP	12.218286634217314	-35.962830619973786	6672
c272c5a2e53cc81defa25e479da1074c0262b80b	predictive models for the control of basic oxygen steelmaking process				M. M. Zaman	1972				Robotics	11.318643146328919	-25.299125651728833	6685
6c73304b930cbc5f030b8d4d68982d6b1dc6c2d0	coevolving predator and prey robots: do arms races arise in artificial evolution?	arms race;evolutionary robotics;incremental learning;competitive evolution;incremental learning competitive evolution;artificial evolution	Coevolution (i.e., the evolution of two or more competing populations with coupled fitness) has several features that may potentially enhance the power of adaptation of artificial evolution. In particular, as discussed by Dawkins and Krebs [3], competing populations may reciprocally drive one another to increasing levels of complexity by producing an evolutionary arms race. In this article we will investigate the role of coevolution in the context of evolutionary robotics. In particular, we will try to understand in what conditions coevolution can lead to arms races. Moreover, we will show that in some cases artificial coevolution has a higher adaptive power than simple evolution. Finally, by analyzing the dynamics of coevolved populations, we will show that in some circumstances well-adapted individuals would be better advised to adopt simple but easily modifiable strategies suited for the current competitor strategies rather than incorporate complex and general strategies that may be effective against a wide range of opposing counter-strategies.	acclimatization;alveolar rhabdomyosarcoma;biological evolution;coat of arms;evolutionary algorithm;evolutionary robotics;population;prey;race condition;robot (device)	Stefano Nolfi;Dario Floreano	1998	Artificial Life	10.1162/106454698568620	biology;computer science;artificial intelligence;evolutionary algorithm;evolutionary robotics;ecology;evolutionary biology	AI	25.182438867329992	-12.801049651876694	6689
9df923bc15eccead00fd7fc1d6a0cc9ddef953e1	a near-lossless image compression algorithm suitable for hardware design in wireless endoscopy system	signal image and speech processing;overhead line;medical imagery;arquitectura circuito;endoscopia;image coding;image processing;data compression;format image;telecommunication sans fil;debit information;information transmission;implementation;endoscopy;lossless image compression;procesamiento imagen;ligne aerienne;circuit architecture;lossless compression;tecnologia mos complementario;region interes;buffer system;qualite image;indice informacion;traitement image;time varying system;power supply;sistema amortiguador;algorithme;algorithm;codage image;compression image;quantum information technology spintronics;image compression;alimentation electrique;telecomunicacion sin hilo;systeme parametre variable;image quality;architecture circuit;imagineria medica;imagerie medicale;information rate;hardware design;formato imagen;calidad imagen;endoscopie;compression sans perte;compresion dato;sistema parametro variable;transmision informacion;linea aerea;region interet;transmission information;implementacion;technologie mos complementaire;systeme tampon;alimentacion electrica;compresion sin perdida;image format;compression donnee;complementary mos technology;interest region;algoritmo;compresion imagen;wireless telecommunication	In order to decrease the communication bandwidth and save the transmitting power in the wireless endoscopy capsule, this paper presents a new near-lossless image compression algorithm based on the Bayer format image suitable for hardware design. This algorithm can provide low average compression rate (2.12 bits/pixel) with high image quality (larger than 53.11 dB) for endoscopic images. Especially, it has low complexity hardware overhead (only two line buffers) and supports real-time compressing. In addition, the algorithm can provide lossless compression for the region of interest (ROI) and high-quality compression for other regions. The ROI can be selected arbitrarily by varying ROI parameters. In addition, the VLSI architecture of this compression algorithm is also given out. Its hardware design has been implemented in 0.18 μm CMOS process.	algorithm;application-specific integrated circuit;cmos;data compression;decibel;digital image;image compression;image quality;image sensor;lossless compression;overhead (computing);peak signal-to-noise ratio;pixel;real-time clock;region of interest;tract (literature);transmitter;very-large-scale integration	Xiang Xie;Guolin Li;Zhihua Wang	2007	EURASIP J. Adv. Sig. Proc.	10.1155/2007/82160	data compression;data compression ratio;telecommunications;image processing;image compression;computer science;lossless compression;algorithm	EDA	46.932719376789606	-14.071869020727764	6693
2df2e4f9b6c9df5f455c3181b31dfdce34d52b14	template-based isometric deformable 3d reconstruction with sampling-based focal length self-calibration	self calibrating 3d reconstruction algorithm;sampling methods image reconstruction;isometric deformable 3d reconstruction;surface reconstruction;camera self calibration;surface deforming isometric ally;affine image template 3d shape isometric deformable 3d reconstruction sampling based focal length self calibration surface deforming isometric ally general variational framework self calibrating 3d reconstruction algorithm camera self calibration;template 3d shape;shape;general variational framework;three dimensional displays;image reconstruction;mathematical model;sampling methods;jacobian matrices;sampling based focal length self calibration;cameras;equations three dimensional displays cameras surface reconstruction mathematical model jacobian matrices shape;affine image	It has been shown that a surface deforming isometric ally can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However, they all assume that the camera model is calibrated, which drastically limits their applicability. We propose (i) a general variational framework that applies to (calibrated and uncalibrated) general camera models and (ii) self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case, our algorithm returns the normal field and camera's scale factor. In the latter case, our algorithm returns the normal field, depth and camera's focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods. Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods (which use the true focal length) on perspective images, and correctly find the normal field on affine images for which the existing methods fail.	3d projection;3d reconstruction;algorithm;autostereogram;c++;calculus of variations;computation;degeneracy (graph theory);focal (programming language);graphics processing unit;isometric projection;matlab;merge sort;nonlinear system;normal (geometry);personal computer;refinement (computing);stat (system call);well-posed problem;whole earth 'lectronic link	Adrien Bartoli;Toby Collins	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.199	iterative reconstruction;sampling;computer vision;camera auto-calibration;surface reconstruction;shape;mathematical model;mathematics;geometry;statistics	Vision	52.81119034201354	-48.82596900988242	6696
f141ef3d583ea7c0d7c3fe6b2aba1d4f1577a127	soft authentication using an infrared ceiling sensor network	reseau capteur;a domicilio;pistage;a domicile;sensor attached ceiling;infrared thermography;biometrie;authentication;biometrics;customization;biometria;rastreo;personnalisation;probabilistic approach;sensor network;human tracking;authentification;captador medida;thermographie ir;measurement sensor;red sensores;autenticacion;capteur mesure;enfoque probabilista;approche probabiliste;soft authentication;detecteur ir;personalizacion;sensor array;at home;infrared sensor;termografia ir;infrared;infrared detector;detector rayos infrarrojos;tracking	Person identification is needed to provide various personalized services at home or at the office. We propose a system for tracking persons identified at the entrance to a room in order to realize “soft authentication.” Our system can be constructed at low cost and works anytime and anywhere in a room. Through experiments, we confirmed that the system could track up to 5 persons with a high probability of correct identification, though precise identification is difficult.	anytime algorithm;authentication;experiment;high-level programming language;microwave;personalization;sensor;television set	Taisuke Hosokawa;Mineichi Kudo;Hidetoshi Nonaka;Jun Toyama	2008	Pattern Analysis and Applications	10.1007/s10044-008-0119-9	telecommunications;computer science;authentication;computer security	HCI	42.157016844953695	-38.32935365388911	6708
aa1204e0468afae1478af3441523dc8a91116d2a	self-advised incremental one-class support vector machines: an application in structural health monitoring		Incremental One-Class Support Vector Machine (OCSVM) methods provide critical advantages in practical applications, as they are able to capture variations of the positive samples over time. This paper proposes a novel self-advised incremental OCSVM algorithm, which decides whether an incremental step is required to update its model or not. As opposed to existing method, this novel online algorithm does not rely on any fixed threshold, but it uses the slack variables in the OCSVM as proxies for data in order to determine which new data points should be included in the training set and trigger an update of the model’s coefficients. This new online OCSVM algorithm was extensively evaluated using real data from Structural Health Monitoring (SHM) case studies. These results showed that this new online method provided significant improvements in classification error rates, was able to assimilate the changes in the positive data distribution over the time, and maintained a high damage detection accuracy in these SHM cases.	support vector machine	Ali Anaissi;Nguyen Lu Dang Khoa;Thierry Rakotoarivelo;Mehri Makki Alamdari;Yang Wang	2017		10.1007/978-3-319-70087-8_51	incremental learning;anomaly detection;online algorithm;machine learning;support vector machine;data point;pattern recognition;structural health monitoring;artificial intelligence;computer science;training set;slack variable	Robotics	14.640374370821963	-37.743546632030316	6709
f2738446622acfd424b1635ac2e884785c0ac525	tumor classification using non-negative matrix factorization	gene expression data;non negative matrix factorization;support vector machine	With the advent of DNA microarrys, it is now possible to use the microarry data for tumor classification. Yet previous works have not use the nonnegative information of gene expression data for classification. In this paper, we propose a new method for tumor classification using gene expression data. In this method, we first extract new features of the gene expression data by virtue of non-negative matrix factorization (NMF) and its extension, i.e. sparse NMF (SNMF) then apply support vector machines (SVM) to classify the tumor samples using the extracted features. To better fit for classification aim, a new SNMF algorithm is also proposed.	non-negative matrix factorization	Ping Zhang;Qing Yan;Bo Li;Chang-Gang Wen	2008		10.1007/978-3-540-85930-7_32	support vector machine;computer science;machine learning;non-negative matrix factorization	Vision	9.278401198293064	-48.10875715103465	6712
4125d94d6c0cfce5cd3beff9ed5da0f7bee053d3	comparing hard and overlapping clusterings	overlapping;probabilistic;fuzzy;clustering evaluation;article	Similarity measures for comparing clusterings is an important component, e.g., of evaluating clustering algorithms, for consensus clustering, and for clustering stability assessment. These measures have been studied for over 40 years in the domain of exclusive hard clusterings (exhaustive and mutually exclusive object sets). In the past years, the literature has proposed measures to handle more general clusterings (e.g., fuzzy/probabilistic clusterings). This paper provides an overview of these new measures and discusses their drawbacks. We ultimately develop a corrected-for-chance measure (13AGRI) capable of comparing exclusive hard, fuzzy/probabilistic, non-exclusive hard, and possibilistic clusterings. We prove that 13AGRI and the adjusted Rand index (ARI, by Hubert and Arabie) are equivalent in the exclusive hard domain. The reported experiments show that only 13AGRI could provide both a fine-grained evaluation across clusterings with different numbers of clusters and a constant evaluation between random clusterings, showing all the four desirable properties considered here. We identified a high correlation between 13AGRI applied to fuzzy clusterings and ARI applied to hard exclusive clusterings over 14 real data sets from the UCI repository, which corroborates the validity of 13AGRI fuzzy clustering evaluation. 13AGRI also showed good results as a clustering stability statistic for solutions produced by the expectation maximization algorithm for Gaussian mixture. Implementation and supplementary figures can be found at http://sn.im/25a9h8u.	cluster analysis;consensus clustering;expectation–maximization algorithm;experiment;fuzzy clustering;rand index	Danilo Horta;Ricardo J. G. B. Campello	2015	Journal of Machine Learning Research		fuzzy logic;rand index;computer science;machine learning;pattern recognition;data mining;mathematics;probabilistic logic	ML	1.2376969456292757	-40.04499365053912	6730
6dd545bc5f5a1650be918165aeeefe9b026dad2a	relieff-mi: an extension of relieff to multiple instance learning	multiple instance learning;relief algorithm;feature selection	In machine learning the so-called curse of dimensionality, pertinent to many classification algorithms, denotes the drastic increase in computational complexity and classification error with data having a great number of dimensions. In this context, feature selection techniques try to reduce dimensionality finding a new more compact representation of instances selecting the most informative features and removing redundant, irrelevant, and/or noisy features. In this paper, we propose a filter-based feature selection method for working in the multiple-instance learning scenario called ReliefF-MI; it is based on the principles of the well-known ReliefF algorithm. Different extensions are designed and implemented and their performance checked in multiple instance learning. ReliefF-MI is applied as a pre-processing step that is completely independent from the multi-instance classifier learning process and therefore is more efficient and generic than wrapper approaches proposed in this area. Experimental results on five benchmark realworld data sets and seventeen classification algorithms confirm the utility and efficiency of this method, both statistically and from the point of view of execution time.	algorithm;benchmark (computing);centrality;computation;computational complexity theory;curse of dimensionality;dimensionality reduction;feature selection;hausdorff dimension;information;machine learning;multiple instance learning;preprocessor;relevance;run time (program lifecycle phase);supervised learning;time complexity;whole earth 'lectronic link	Amelia Zafra;Mykola Pechenizkiy;Sebastián Ventura	2012	Neurocomputing	10.1016/j.neucom.2011.03.052	semi-supervised learning;instance-based learning;computer science;machine learning;pattern recognition;data mining;stability;feature selection;k-nearest neighbors algorithm;dimensionality reduction;generalization error	ML	11.000381002839832	-43.24189996720674	6751
45b99b6f0961628c23ba22395dace8ff86961918	confidence levels based pythagorean fuzzy aggregation operators and its application to decision-making process		Pythagorean fuzzy set, an extension of the intuitionistic fuzzy set which relax the condition of sum of their membership function to square sum of its membership functions is less than one. Under these environment and by incorporating the idea of the confidence levels of each Pythagorean fuzzy number, the present study investigated a new averaging and geometric operators namely confidence Pythagorean fuzzy weighted and ordered weighted operators along with their some desired properties. Based on its, a multi criteria decision-making method has been proposed and illustrated with an example for showing the validity and effectiveness of it. A computed results are compared with the aid of existing results.	fuzzy set	Harish Garg	2017	Computational & Mathematical Organization Theory	10.1007/s10588-017-9242-8	mathematical optimization;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;fuzzy set operations	AI	-2.5164813833820276	-20.772711567636936	6775
1d526fe3bbd26e3beb02ba2e289e5bf6074bf8bb	gaussian process topic models		We introduce Gaussian Process Topic Models (GPTMs), a new family of topic models which can leverage a kernel among documents while extracting correlated topics. GPTMs can be considered a systematic generalization of the Correlated Topic Models (CTMs) using ideas from Gaussian Process (GP) based embedding. Since GPTMs work with both a topic covariance matrix and a document kernel matrix, learning GPTMs involves a novel component—solving a suitable Sylvester equation capturing both topic and document dependencies. The efficacy of GPTMs is demonstrated with experiments evaluating the quality of both topic modeling and embedding.	apriori algorithm;experiment;gaussian process;ibm notes;kernel (operating system);linear separability;semi-supervised learning;semiconductor industry;topic model	Amrudin Agovic;Arindam Banerjee	2010			theoretical computer science;machine learning;pattern recognition;mathematics	ML	23.878166261237027	-43.77068585822366	6790
f90c6e349461e8f52d3774ba1cd1f8ace3a3770e	actively avoiding nonsense in generative models		A generative model may generate utter nonsense when it is fit to maximize the likelihood of observed data. This happens due to “model error,” i.e., when the true data generating distribution does not fit within the class of generative models being learned. To address this, we propose a model of active distribution learning using a binary invalidity oracle that identifies some examples as clearly invalid, together with random positive examples sampled from the true distribution. The goal is to maximize the likelihood of the positive examples subject to the constraint of (almost) never generating examples labeled invalid by the oracle. Guarantees are agnostic compared to a class of probability distributions. We first show that proper learning may require exponentially many queries to the invalidity oracle. We then give an improper distribution learning algorithm that uses only polynomially many queries.	algorithm;generative model;oracle database	Steve Hanneke;Adam Tauman Kalai;Gautam Kamath;Christos Tzamos	2018			artificial intelligence;generative grammar;probability distribution;nonsense;machine learning;oracle;generative model;mathematics;binary number	ML	19.074462351367796	-33.74522793233319	6809
3b2b9f269300d5a4247cb6036c467bc66e7c8217	setup planning and operation sequencing using neural network and genetic algorithm	unsupervised learning;setup planning neural network;traveling salesman problem;neural networks genetic algorithms process planning traveling salesman problems machining hopfield neural networks technology planning cost function unsupervised learning fixtures;process planning cad cam computer aided production planning genetic algorithms neural nets;computer aided design;neural nets;operation sequencing;process planning setup planning;objective function;setup planning;capp;operation sequencing unsupervised learning genetic algorithm capp setup planning;capp system;cad cam;computer aided production planning;computer aided process planning system;genetic algorithm;genetic algorithms;process planning;manufacturing system;neural network;manufacturing system process planning setup planning operation sequencing setup planning neural network genetic algorithm traveling salesman problem computer aided process planning system capp system computer aided design	In process planning setup planning and operation sequencing are the major issues. In setup planning feature having same approach direction & tool commonality are grouped into setup. After making different setups operation sequencing in each setup is done for setup planning neural network is efficient. Operation sequencing problem is converted into the traveling salesman problem in which objective function is to reduce total cost. To solve these issues in an efficient manner Genetic Algorithm technique is more suitable because it is a viable means for searching the solution space of operation sequence providing a computational time on the order of a few seconds. The present work generates the results for the prismatic parts. The presented algorithm for setup planning and operation sequencing is efficient enough for its use as a module within the development of a CAPP system.	artificial neural network;automated planning and scheduling;computation;feasible region;genetic algorithm;loss function;optimization problem;time complexity;travelling salesman problem	Ravinder Singh Joshi;Navdeep Kumar;Anju Sharma	2008	Fifth International Conference on Information Technology: New Generations (itng 2008)	10.1109/ITNG.2008.94	simulation;genetic algorithm;computer science;machine learning;artificial neural network	Robotics	18.646793973077635	-0.09290962190557278	6816
62feed1b1814487c03317f9137e7132f92007e4d	online robust action recognition based on a hierarchical model	robust online action recognition;bottom up approach;hierarchical model	Action recognition solely based on video data has known to be very sensitive to background activity, and also lacks the ability to discriminate complex 3D motion. With the development of commercial depth cameras, skeleton-based action recognition is becoming more and more popular. However, the skeleton-based approach is still very challenging because of the large variation in human actions and temporal dynamics. In this paper, we propose a hierarchical model for action recognition. To handle confusing motions, a motion-based grouping method is proposed, which can efficiently assign each video a group label, and then for each group, a pre-trained classifier is used for frame-labeling. Unlike previous methods, we adopt a bottom-up approach that first performs action recognition for each frame. The final action label is obtained by fusing the classification to its frames, with the effect of each frame being adaptively adjusted based on its local properties. To achieve online real-time performance and suppressing noise, bag-of-words is used to represent the classification features. The proposed method is evaluated using two challenge datasets captured by a Kinect. Experiments show that our method can robustly recognize actions in real-time.	bag-of-words model in computer vision;bottom-up parsing;depth map;experiment;hierarchical database model;kinect;real-time clock;real-time transcription;top-down and bottom-up design;topological skeleton	Xinbo Jiang;Fan Zhong;Qunsheng Peng;Xueying Qin	2014	The Visual Computer	10.1007/s00371-014-0923-8	computer vision;computer science;machine learning;hierarchical database model	Vision	34.37554485585166	-49.01352991545763	6820
2fadae9360fbeb0e8be702006c1917c8360e5926	affine transform resilient image fingerprinting	watermarking;radon transforms;image fingerprint extraction;image processing;radon transform;fourier transform;image fingerprinting;multimedia systems;fingerprint recognition cryptography robustness fourier transforms autocorrelation digital multimedia broadcasting watermarking multimedia systems modems data security;affine transform resilience;digital multimedia broadcasting;fingerprint recognition;cryptography;feature extraction;affine transformation;feature extraction affine transform resilience image fingerprinting multimedia fingerprinting autocorrelation radon transform log mapping fourier transform security database search efficiency robust hash function image fingerprint extraction;fourier transforms;feature extraction radon transforms image processing cryptography fourier transforms;robustness;log mapping;modems;database search efficiency;robust hash function;database search;security;autocorrelation;data security;multimedia fingerprinting	Affine transformations are a well-known robustness issue in many multimedia fingerprinting systems. Since it is quite easy with modem computers to apply affine transformations to audio, image and video content, there is an obvious necessity for affine transformation resilient fingerprinting. In this paper we present a new method for affine transformation resilient fingerprints that is based upon the auto-correlation of the Radon transform, the log mapping and the Fourier transform. Besides robustness, we also address other issues such as security, database search efficiency and independence with perceptually different inputs. Experimental results show that the proposed fingerprints are highly robust to affine transformations.	autocorrelation;computer security;digital video;fingerprint (computing);modem	Jin S. Seo;Jaap Haitsma;Ton Kalker;Chang Dong Yoo	2003		10.1109/ICASSP.2003.1199107	fourier transform;computer vision;speech recognition;image processing;computer science;theoretical computer science;harris affine region detector;mathematics;affine shape adaptation	Security	37.418052136723105	-11.948970097602043	6822
cd55c974b6734acec91d5f02924e30b825a4bded	efficient reinforcement learning with multiple reward functions for randomized controlled trial analysis	randomized controlled trial;reinforcement learning	We introduce new, efficient algorithms for value iteration with multiple reward functions and continuous state. We also give an algorithm for finding the set of all nondominated actions in the continuous state setting. This novel extension is appropriate for environments with continuous or finely discretized states where generalization is required, as is the case for data analysis of randomized controlled trials.	approximation algorithm;convex set;discretization;iteration;linear function;markov decision process;randomized algorithm;reinforcement learning	Daniel J. Lizotte;Michael H. Bowling;Susan A. Murphy	2010			computer science;n of 1 trial;machine learning;reinforcement learning;randomized controlled trial	ML	22.539552761453148	-18.18145072339132	6832
cdcadab8fad1671607e81d9f6bcec82ea2c882d2	learning how students learn with bayes nets	modelizacion;bayesian network;student model;computer assisted teaching;backpropagation neural network;computational techniques;acceso directo;informacion mutual;ensenanza asistida por computador;probabilistic approach;user assistance;modelisation;reseau bayes;information mutuelle;assistance utilisateur;red bayes;knowledge structure;enfoque probabilista;approche probabiliste;backpropagation algorithm;acces direct;asistencia usuario;bayes network;direct access;learning problems;algorithme retropropagation;mutual information;student learning;educacion;reseau neuronal;modeling;red neuronal;enseignement assiste ordinateur;neural network;algoritmo retropropagacion	This extended abstract summarizes an exploration of how computational techniques may help educational experts identify finegrained student models. In particular, we look for methods that help us learn how students learn composite concepts. We employ Bayesian networks for the representation of student models, and cast the problem as an instance of learning the hidden substructures of Bayesian networks. The problem is challenging because we do not have direct access to students’ competence in concepts, though we can observe students’ responses to test items that have only indirect and probabilistic relationships with the competence levels. We apply mutual information and backpropagation neural networks for this learning problem, and experimental results indicate that computational techniques can be helpful in guessing the hidden knowledge structures under some circumstances.	artificial neural network;backpropagation;bayesian network;mutual information;random access	Chao-Lin Liu	2006		10.1007/11774303_96	simulation;computer science;artificial intelligence;machine learning;bayesian network;artificial neural network	ML	7.854135532110403	-31.210537465942448	6845
1ea29f99126804df626cbfc349c5e658031f5ee5	autonomous trajectory design system for mapping of unknown sea-floors using a team of auvs		This research develops a new on-line trajectory planning algorithm for a team of Autonomous Underwater Vehicles (AUVs). The goal of the AUVs is to cooperatively explore and map the ocean seafloor. As the morphology of the seabed is unknown and complex, standard non-convex algorithms perform insufficiently. To tackle this, a new simulation-based approach is proposed and numerically evaluated. This approach adapts the Parametrized Cognitive-based Adaptive Optimization (PCAO) algorithm. The algorithm transforms the exploration problem to a parametrized decision-making mechanism whose real-time implementation is feasible. Upon that transformation, this scheme calculates off-line a set of decision making mechanism’s parameters that approximate the - non-practically feasible - optimal solution. The advantages of the algorithm are significant computational simplicity, scalability, and the fact that it can straightforwardly embed any type of physical constraints and system limitations. In order to train the PCAO controller, two morphologically different seafloors are used. During this training, the algorithm outperforms an unrealistic optimal-one-step-ahead search algorithm. To demonstrate the universality of the controller, the most effective controller is used to map three new morphologically different seafloors. During the latter mapping experiment, the PCAO algorithm outperforms several gradient-descent-like approaches.		Georgios Salavasidis;Athanasios Ch. Kapoutsis;Savvas A. Chatzichristofis;Panagiotis Michailidis;E. B. Kosmatopoulos	2018	2018 European Control Conference (ECC)	10.23919/ECC.2018.8550174	mathematical optimization;control theory;approximation algorithm;adaptive optimization;simultaneous localization and mapping;parametrization;exploration problem;scalability;search algorithm;computer science	Robotics	50.87918109760768	-24.862668966259672	6854
859af6e67aec769c58ec1ea6a971108a60df0b9d	structured perceptron with inexact search	new separability condition;inexact search;exact inference;framework subsumes;structured perceptron;existing theory;beam search;general framework;structured prediction;new update method	Structured learning with inexact inference is a fundamental problem. We propose variants of the structured perceptron algorithm under a general “violation-fixing” framework that guarantees convergence. This framework subsumes previous remedies including “early update” as special cases, and also explains why standard perceptron may fail with inexact search. We also propose new update methods within this framework which learn better models with dramatically reduced training times on state-of-the-art part-of-speech tagging and incremental parsing systems.	algorithm;parsing;part-of-speech tagging;perceptron;structured prediction;vergence	Liang Huang;Suphan Fayong;Yang Guo	2012			mathematical optimization;computer science;theoretical computer science;machine learning	NLP	24.01531065025427	-32.87688499319667	6861
b63c4eadf46398a1d72cf1142fcfa516ccb00ee5	nonuniform coverage and cartograms	informatica movil;mobile sensor networks;desplazamiento activo animal;red sin hilo;metodo adaptativo;reseau capteur;time varying;multiagent system;euclidean theory;informatique mobile;sintesis control;reseau sans fil;autonomous system;echantillonnage;wireless network;mobile sensor network;metric;methode adaptative;control centralizado;cartographie;65j10;time varying system;sistema autonomo;sampling;deplacement actif animal;cartografia;red sensores;couverture;comportement alimentaire;synthese commande;systeme parametre variable;adaptive method;systeme autonome;theorie euclidienne;optimal coverage;sensor array;adaptive sampling;commande centralisee;35g10;cartography;metrico;centralized control;coverage;sistema parametro variable;cartogram;mobile agent;muestreo;sistema multiagente;mobile computing;conducta alimenticia;control synthesis;metrique;feeding behavior;systeme multiagent;teoria euclidiana;animal active movement;49k99;cobertura	In this paper, we investigate nonuniform coverage of a planar region by a network of autonomous, mobile agents. We derive centralized nonuniform coverage control laws from uniform coverage algorithms using cartograms, transformations that map nonuniform metrics to a near Euclidean metric. We also investigate time-varying coverage metrics and the design of control algorithms to cover regions with slowly varying, nonuniform metrics. Our results are applicable to the design of mobile sensor networks, notably when the coverage metric varies as data is collected such as in the case of an information metric. The results apply also to the study of animal groups foraging for food that is nonuniformly distributed and possibly changing.		Francois Lekien;Naomi Ehrich Leonard	2009	SIAM J. Control and Optimization	10.1137/070681120	sampling;metric;autonomous system;wireless network;mobile agent;mathematics;mobile computing;sensor array;cartogram	Theory	51.929873612372766	-7.349404263314047	6868
d34e1e78e77b0a96b891674c02f347937029ca9c	an assembly sequence planning approach with a multi-state particle swarm optimization	tk electrical engineering electronics nuclear engineering;ts manufactures	Assembly sequence planning (ASP) becomes one of the major challenges in the product design and manufacturing. A good assembly sequence leads in reducing the cost and time of the manufacturing process. However, assembly sequence planning is known as a classical hard combinatorial optimization problem. Assembly sequence planning with more product components becomes more difficult to be solved. In this paper, an approach based on a new variant of Particle Swarm Optimization Algorithm (PSO) called the multi-state of Particle Swarm Optimization (MSPSO) is used to solve the assembly sequence planning problem. As in of Particle Swarm Optimization Algorithm, MSPSO incorporates the swarming behaviour of animals and human social behaviour, the best previous experience of each individual member of swarm, the best previous experience of all other members of swarm, and a rule which makes each assembly component of each individual solution of each individual member is occurred once based on precedence constraints and the best feasible sequence of assembly is then can be determined. To verify the feasibility and performance of the proposed approach, a case study has been performed and comparison has been conducted against other three approaches based on Simulated Annealing (SA), Genetic Algorithm (GA), and Binary Particle Swarm Optimization (BPSO). The experimental results show that the proposed approach has achieved significant improvement.	particle swarm optimization;program optimization	Ismail Ibrahim;Zuwairie Ibrahim;Hamzah Ahmad;Zulkifli Md Yusof	2016		10.1007/978-3-319-42007-3_71	mathematical optimization;multi-swarm optimization;simulation;computer science;particle swarm optimization;metaheuristic	Robotics	21.172265090519364	-1.3314176968236964	6869
a9be314ebe24b2891a2ca5520ceaddd14034954d	programmable synaptic weights for an avlsi network of spiking neurons	mismatch calibration;local synaptic weights;on chip digital analog converter circuits;neural networks;neural nets;digital analog converter;very large scale integration;chip;spiking neurons;mismatch calibration programmable synaptic weights avlsi network spiking neurons spiking neuronal network local synaptic weights on chip digital analog converter circuits;programmable circuits;spiking neuronal network;vlsi;neurons circuits biological neural networks timing digital analog conversion decoding delay effects testing protocols system on a chip;avlsi network;vlsi digital analogue conversion neural nets programmable circuits;digital analogue conversion;digital analog conversion;neuronal network;programmable synaptic weights	We describe a spiking neuronal network which allows local synaptic weights to be assigned to individual synapses. In previous implementations of neuronal networks, the biases that control the parameters of a particular synapse are global to all synapses of the same type regardless of the target neuron. In this new implementation, the parameters for a synapse are set by on-chip digital-analog-converter (DAC) circuits, and the DACs are updated before the selected synapses are activated. Results from the fabricated chip show that the local weights are programmable and the DACs settle in the order of microseconds. These on-chip DACs allow the user to program a selected synaptic weight for connections between neurons and they can also be used for mismatch calibration	digital-to-analog converter;neuron;synapse;synaptic weight	Yingxue Wang;Shih-Chii Liu	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693637	electronic engineering;computer science;theoretical computer science;machine learning;very-large-scale integration;artificial neural network	Theory	38.99864701756726	-1.8147833602225252	6878
1de62bb648cf86fe0c1165a370aa8539ba28187e	mode-seeking on hypergraphs for robust geometric model fitting	robust geometric model fitting computer vision real image synthetic data data distribution weight aware sampling technique similarity measure model instance detection data points hyperedge model hypotheses multistructure data msh mode seeking on hypergraph;object detection geometry graph theory image sampling;data models computational modeling robustness mathematical model computer vision weight measurement kernel	"""In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH), to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a """"weight-aware sampling"""" technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images."""	algorithm;curve fitting;data point;geometric modeling;pdf/a;sampling (signal processing);scalability;similarity measure;synthetic data;unbalanced circuit;vertex (geometry)	Hanzi Wang;Guobao Xiao;Yan Yan;David Suter	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.332	computer vision;theoretical computer science;machine learning;pattern recognition;mathematics;statistics	Vision	46.64612007507663	-50.87071810733699	6881
c26ab5f34c884233955e8fead264e83ac1625940	an improved smote imbalanced data classification method based on support degree	computers;support degree;smote;bagging;training;testing;data mining;classification;boundary sample;classification algorithms;algorithm design and analysis;imbalanced data sets	Imbalanced data-set Classification has become a hotspot problem in Data Mining. The essential assumption of the traditional classification algorithms is that the distribution of the classes is balanced, therefore the algorithms used in Imbalanced data-set Classification cannot achieve an ideal effect. In view of imbalance date-set classification, we propose an over sampling method based on support degree in order to guide people to select minority class samples and generate new minority class samples. In the light of support degree, it is now possible to identify minority class boundary samples, then produce a number of new samples between the boundary samples and their neighbors, finally add the synthetic samples to the original data-set to participate in training and testing. Experimental results show that the method has an obvious advantage in dealing with imbalanced data-set.	algorithm;application domain;data mining;data pre-processing;java hotspot virtual machine;operation time;oversampling;preprocessor;sampling (signal processing);synthetic intelligence	Kewen Li;Wenrong Zhang;Qinghua Lu;Xianghua Fang	2014	2014 International Conference on Identification, Information and Knowledge in the Internet of Things	10.1109/IIKI.2014.14	statistical classification;algorithm design;bootstrap aggregating;biological classification;computer science;machine learning;pattern recognition;data mining;software testing	Robotics	13.650288288075538	-41.220078601682495	6885
6bdf57a61c145b7c410acae028f95a6632ef7042	inductive inference of chess player strategy	evaluation function;inductive inference	We investigate the problem of inferring, from records of chess games, some aspects of the strategy used to play the games. Initially, game records are generated from self-play by two simple chess programs, one of which does a one-ply search while the other does a four-ply quies-cent search. In each case, we are able to infer, from just the game records, good estimates of the weights used in the evaluation function. The approach is then applied to grandmaster games. Our one-ply and quiescent four-ply programs are now drastic simpliications of the true strategy used. Nonetheless, using inferred weights for these hypothetical models, we are still able to achieve some success (as measured by compression rates for the games) in predicting moves made by the players.	chess engine;evaluation function;inductive reasoning;manhunters	Anthony R. Jansen;David L. Dowe;Graham E. Farr	2000		10.1007/3-540-44533-1_10	combinatorial game theory;transposition table;simulation;computer science;artificial intelligence;inductive reasoning;machine learning;evaluation function;sequential game	ML	19.42088650880993	-12.486859394157841	6896
08a4577773f331f4a1c199ab24709fd8a06bac0e	a simple additive re-weighting strategy for improving margins	tangent distance;vector quantizer	We present a sample re-weighting scheme inspired by recent results in margin theory. The basic idea is to add to the training set replicas of samples which are not classified with a sufficient margin. We prove the convergence of the input distribution obtained in this way. As study case, we consider an instance of the scheme involving a 1-NN classifier implementing a Vector Quantization algorithm that accommodates tangent distance models. The tangent distance models created in this way have shown a significant improvement in generalization power with respect to the standard tangent models. Moreover, the obtained models were able to outperform state of the art algorithms, such as SVM.	additive model;algorithm;boosting (machine learning);committee machine;experiment;high-speed serial interface;iteration;lazy initialization;linear model;nonlinear system;statistical learning theory;test set;utility functions on indivisible goods;vector quantization;weatherstar	Fabio Aiolli;Alessandro Sperduti	2001			local tangent space alignment;mathematical optimization;discrete mathematics;computer science;machine learning;mathematics;statistics	ML	21.059199090279467	-34.95755828636945	6932
e6b45d5a86092bbfdcd6c3c54cda3d6c3ac6b227	pairwise relational networks for face recognition		Existing face recognition using deep neural networks is difficult to know what kind of features are used to discriminate the identities of face images clearly. To investigate the effective features for face recognition, we propose a novel face recognition method, called a pairwise relational network (PRN), that obtains local appearance patches around landmark points on the feature map, and captures the pairwise relation between a pair of local appearance patches. The PRN is trained to capture unique and discriminative pairwise relations among different identities. Because the existence and meaning of pairwise relations should be identity dependent, we add a face identity state feature, which obtains from the long short-term memory (LSTM) units network with the sequential local appearance patches on the feature maps, to the PRN. To further improve accuracy of face recognition, we combined the global appearance representation with the pairwise relational feature. Experimental results on the LFW show that the PRN using only pairwise relations achieved 99.65% accuracy and the PRN using both pairwise relations and face identity state feature achieved 99.76% accuracy. On the YTF, both the PRN using only pairwise relations and the PRN using pairwise relations and the face identity state feature achieved the state-of-the-art (95.7% and 96.3%). The PRN also achieved comparable results to the state-of-the-art for both face verification and face identification tasks on the IJB-A, and the state-of-the-art on the IJB-B.	artificial neural network;connectionism;deep learning;embedded system;facial recognition system;long short-term memory;map	Bong-Nam Kang;Yonghyun Kim;Daijin Kim	2018		10.1007/978-3-030-01216-8_39	discriminative model;pattern recognition;machine learning;computer science;artificial intelligence;artificial neural network;facial recognition system;pairwise comparison	Vision	26.384896820591486	-49.824101203693324	6935
e4075c532cc82ac32786eab1460766e1f7dd9e54	a framework for evolving fuzzy classifier systems using genetic programming	genetic program;fuzzy rules;expressive power;fuzzy rule base;evolutionary algorithm;fuzzy classifier	A fuzzy classifier system framework is proposed which employs a tree-based representation for fuzzy rule (classifier) antecedents and genetic programming for fuzzy rule discovery. Such a rule representation is employed because of the expressive power and generality it endows to individual rules. The framework proposes accuracy-based fitness for individual fuzzy classifiers and employs evolutionary competition between simultaneously matched classifiers. The evolutionary algorithm (GP) is therefore searching for compact fuzzy rule bases which are simultaneously general, accurate and co-adapted. Additional extensions to the proposed framework are suggested.	association rule learning;computer data storage;evolutionary algorithm;expressive power (computer science);fuzzy rule;fuzzy set;genetic programming;learning classifier system;q-learning	Brian Carse;Anthony G. Pipe	2001			fuzzy logic;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;evolutionary algorithm;pattern recognition;data mining;fuzzy associative matrix;fuzzy set operations;expressive power;fuzzy control system	AI	4.257692185299284	-29.85073764413073	6937
6f5bc83f92a9ca406f852c6ed32640dbd1f562fd	investigation on the evolutionary algorithms with their applications in mimo detecting systems	particle swarm optimization;genetic algorithm;detecting;mimo	SUMMARY#R##N##R##N#In this paper, with the purpose of integrating the advantages of both the genetic algorithm and the particle swarm optimization, a new genetic particle swarm optimization (GPSO) algorithm is proposed. Furthermore, these three evolutionary algorithms are successfully applied to address the MIMO detection problem. Simulation results reveal that the GPSO-based detection algorithm takes much less population size and iteration number when compared with the particle swarm optimization-based detection method and the genetic algorithm-based detection method. Besides, when compared with the optimal maximum likelihood detection method, the GPSO-based detection algorithm can strike a much better balance between the BER performance and the computational complexity. Copyright © 2012 John Wiley & Sons, Ltd.	evolutionary algorithm;mimo;sensor	Yongqiang Hei;Xiaohui Li;Wentao Li	2013	Int. J. Communication Systems	10.1002/dac.2317	mathematical optimization;multi-swarm optimization;meta-optimization;genetic algorithm;computer science;theoretical computer science;machine learning;particle swarm optimization;metaheuristic;statistics;mimo	Networks	29.92353994367676	-4.161513932934841	6950
735be418308a2d8f2d48b28fcda48d5ff87ea6e0	multimodal target detection via integrated glrt	detectors;rf signals;radio frequency;three dimensional displays;cameras;data models	The focus of this paper is the multimodal detection and localization of a ground moving target based on radio frequency (RF) and infrared (IR) data. The target radiates a low probability of intercept (LPI) RF signal received by multiple passive RF sensors at scene and is imaged by using a stationary IR camera concurrently. To obtain the multimodal detector proposed in this paper, first, the generalized likelihood ratio test (GLRT) is employed to derive the RF and IR detectors individually. Then, the RF and IR detectors are integrated optimally to get the integrated GLRT. In order to avoid the computational complexity associated with the integrated GLRT, a suboptimal multimodal detector is implemented by applying the random basis functions (RBF) approach to the IR image sequence to reduce down the search space for the RF detector. It is shown that the suboptimal multimodal detector has better localization performance compared to the RF detector when one of the RF sensors is partially occluded.	computational complexity theory;linear partial information;multimodal interaction;pinhole camera model;rf modulator;rf probe;radial basis function;radio frequency;sensor;simulation;stationary process;visual intercept	Steven Kay;Fuat Cogun	2015	MILCOM 2015 - 2015 IEEE Military Communications Conference	10.1109/MILCOM.2015.7357442	computer vision;electronic engineering;telecommunications;engineering	Vision	47.493122130999055	-34.778138370759244	6964
b73a8b5f93bab91561c337c9c67d304312ce68f9	fast decision of cu partitioning based on sao parameter, motion and pu/tu split information for hevc	video coding computational complexity image sequences image texture;cu partitioning complex texture motion natures video sequences computational complexity structural complexity hevc encoders side information sample adaptive offset parameter values sao parameter values mv sizes pu sizes fast split decision bit rate transform unit prediction unit coding unit hierarchical block coding structures high coding efficiency improvements motion information pu tu split information high efficiency video coding sao parameter;image texture;video coding;computational complexity;encoding video coding indexes image edge detection complexity theory joints standards;image sequences	High Efficiency Video Coding (HEVC) has recently been standardized with a significant improvement of coding efficiency compared to its preceding video coding standards. To achieve high coding efficiency improvements, HEVC adopts deeper hierarchical block coding structures of coding unit (CU), prediction unit (PU) and transform unit (TU), to better adapt the complex texture and motion natures of various video sequences. However, this causes a dramatically increased computational and structural complexity of HEVC encoders and decoders. In this paper, we propose a fast decision method of CU partitioning, which significantly reduces total encoding time with negligible RD-performance loss for HEVC. For this, the proposed method effectively utilizes the available side information such as sample adaptive offset (SAO) parameter values, PU sizes, MV sizes and coded block flag (cbf) data, so that the required computational complexity for fast CU split decision is minimized. Our experiment results show that the proposed method reduces the total encoding time to average 43.2% only with average 1.58% bit-rate increase for ten test sequences of HEVC.	algorithmic efficiency;codec;computational complexity theory;data compression;decibel;encoder;high efficiency video coding;ibm systems network architecture;mv-algebra;peak signal-to-noise ratio;ruby document format;sword art online: progressive;system on a chip;tip (unix utility);video coding format	Sangsoo Ahn;Munchurl Kim;Seongmo Park	2013	2013 Picture Coding Symposium (PCS)	10.1109/PCS.2013.6737696	image texture;computer vision;real-time computing;simulation;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;computational complexity theory;h.261	AI	46.59409377976379	-19.203955383157016	6965
832e6c4396e7b74396a1766daef8facadedf502f	enhanced migrating birds optimization algorithm for the permutation flow shop problem with sequence dependent setup times		Abstract This paper presents an enhanced migrating bird optimization (MBO) algorithm and a new heuristic for solving a scheduling problem. The proposed approaches are applied to a permutation flowshop with sequence dependent setup times and the objective of minimizing the makespan. In order to augment the MBOs intensification capacity, an original problem specific heuristic is introduced. An adapted neighborhood, a tabu list, a restart mechanism and an original process for selecting a new leader also improved the MBO’s behavior. Using benchmarks from the literature, the resulting enhanced MBO (EMBO) gives state-of-the-art results when compared with other algorithms reference. A statistical analysis of the numerical experiments confirms the relative efficiency and effectiveness of both EMBO and the new heuristic.	algorithm;flow shop scheduling;mathematical optimization	Aymen Sioud;Caroline Gagné	2018	European Journal of Operational Research	10.1016/j.ejor.2017.06.027	operations management;efficiency;permutation;mathematical optimization;scheduling (computing);job shop scheduling;sequence-dependent setup;algorithm;heuristic;computer science;flow shop scheduling	Theory	22.39069055277249	-0.5729359830455956	6968
9e551ebdf99a73e62b3f4977882c62ea45b1f21a	object tracking based on mean shift algorithm and kernelized correlation filter algorithm		In order to solve the problems of motion blur and fast motion, a new robust object tracking algorithm using the Kernelized Correlation Filters (KCF) and the Mean Shift (MS) algorithm, called KCFMS is presented in this paper. The object tracking process can be described as: First, we give the initial position and size of the object and use the Mean Shift algorithm to obtain the position of the object. Second, the Kernelized Correlation Filtering algorithm is used to obtain the position of the object in the same frame. Third, we use the cross update strategy to update the object models. In order to improve the tracking speed as much as possible, our object tracking algorithm works only over one layer. This hybrid algorithm has a good tracking effect on the target fast motion and motion blur. We present extensive experimental results on a number of challenging sequences in terms of efficiency, accuracy and robustness.		Huazheng Zhou;Xiaohu Ma;Lina Bian	2017		10.1007/978-3-319-70090-8_6	robustness (computer science);machine learning;motion blur;video tracking;hybrid algorithm;filter (signal processing);pattern recognition;artificial intelligence;mean-shift;computer science;computer vision;algorithm;correlation	Vision	42.92420462597833	-48.71079162971408	6980
cb1f84323c6146dbf0c92f66662d843c5c5469b9	movement segmentation and recognition for imitation learning	abt schaal	In human movement learning, it is most common to teach constituent elements of complex movements in isolation, before chaining them into complex movements. Segmentation and recognition of observed movement could thus proceed out of this existing knowledge, which is directly compatible with movement generation. In this paper, we address exactly this scenario. We assume that a library of movement primitives has already been taught, and we wish to identify elements of the library in a complex motor act, where the individual elements have been smoothed together, and, occasionally, there might be a movement segment that is not in our library yet. We employ a flexible machine learning representation of movement primitives based on learnable nonlinear attractor system. For the purpose of movement segmentation and recognition, it is possible to reformulate this representation as a controlled linear dynamical system. An Expectation-Maximization algorithm can be developed to estimate the open parameters of a movement primitive from the library, using as input an observed trajectory piece. If no matching primitive from the library can be found, a new primitive is created. This process allows a straightforward sequential segmentation of observed movement into known and new primitives, which are suitable for robot imitation learning. We illustrate our approach with synthetic examples and data collected from human movement. Appearing in Proceedings of the 15 International Conference on Artificial Intelligence and Statistics (AISTATS) 2012, La Palma, Canary Islands. Volume XX of JMLR: W&CP XX. Copyright 2012 by the authors.	artificial intelligence;dynamical system;expectation–maximization algorithm;feature learning;journal of machine learning research;linear algebra;nonlinear system;smoothing;synthetic intelligence	Franziska Meier;Evangelos Theodorou;Stefan Schaal	2012			computer vision;computer science;artificial intelligence;communication	Robotics	33.43122529909016	-38.98787458369387	7015
55cc90968e5e6ed413dd607af2a850ac2f54e378	active subclustering		Although there are many excellent clustering algorithms, effective clustering remains very challenging for large datasets that contain many classes. Image clustering presents further problems because automatically computed image distances are often noisy. We address these challenges in two ways. First, we propose a new algorithm to cluster a subset of the images only (we call this subclustering), which will produce a few examples from each class. Subclustering will produce smaller but purer clusters. Then we make use of human input in an active subclustering algorithm to further improve results. We run experiments on a face image dataset and a leaf image dataset and show that our proposed algorithms perform better than baseline methods.	algorithm;baseline (configuration management);cluster analysis;experiment	Arijit Biswas;David W. Jacobs	2014	Computer Vision and Image Understanding	10.1016/j.cviu.2014.03.008		Vision	27.988211181363624	-46.425938213732465	7031
a6d73b74c97e09c045608a467c14a0c835c50ddd	inverted pendulum control system based on ga optimization	fuzzy sets decision making uncertainty set theory information technology fuzzy set theory educational institutions quality management technology management mathematical analysis;uncertain systems decision making fuzzy set theory mathematical analysis supply chain management;uncertain systems;fuzzy set;multiple criteria decision making;set theory;mathematical analysis;vague sets theory;fuzzy set theory;supplier selection problem multiple criteria decision making decision making decision process fuzzy sets theory vague sets theory uncertain information mathematical analysis;uncertain information;fuzzy sets theory;decision process;supplier selection problem;supply chain management;supplier selection	A kind of new GA is put forward by this paper, that calculate way satisfies the control demand of the inverted pendulum. Adopt the gene to carry out the evolution operation for the unit. Cross with the average Euclid distance of the gene control gene with the variation. Adopt the phase margin is the function of penalty that one adaptive degree calculate. Guaranteed the calculate way don't sink into the part to refrain from rash action. Considered the index sign of time domain and robustness of the system to unify at the same time. The good result is through certificated. Applied in the actual engineering by the inverted pendulum PD control parameter that based on the new SAGA acquires excellent results.	control system;euclid;inverted pendulum;phase margin;program optimization;robustness (computer science);software release life cycle	Fu Wei;Giang Liangzhong;Yang Jin;Bian Qingqing	2007	Workshop on Intelligent Information Technology Application (IITA 2007)	10.1109/IITA.2007.85	discrete mathematics;supply chain management;type-2 fuzzy sets and systems;computer science;artificial intelligence;machine learning;data mining;mathematics;fuzzy set	Robotics	-1.8899008391387446	-19.326137896901358	7048
01ccbbe5f1b1064f08b8087a7c18bc20ba00b6c9	accelerating the optimal trade-off circular harmonic function filter design on multicore systems	parallel computing;circular harmonic function filter;correlation filter;gpu;hpc;performance optimization;muticore cpu	Optimal correlation filters are widely used in signal processing and pattern recognition applications. Correlation filters are a set of synthesized spatial filters that produce controlled response with sharp peaks. While providing excellent discrimination capabilities correlation filters offer shift, rotation and scale invariance for 2D images. Correlation filters are optimized to enhance the recognition of consistent parts while suppressing the varying patterns. Synthesizing the correlation filters for pattern recognition applications involves several complex mathematical operations and requires high computation resources especially for high resolution images and videos. In this paper, we show that near real time performance can be achieved for the design of the OTCHF filter with help of optimization and parallelization on multicore GPUs and CPUs.	central processing unit;computation;digital filter;filter design;graphics processing unit;image resolution;mathematical optimization;multi-core processor;parallel computing;pattern recognition;real-time computing;signal processing	Anubhav Jain;Amit Kalele	2016		10.1145/2851553.2851579	adaptive filter;electronic engineering;supercomputer;real-time computing;computer science;theoretical computer science;filter design	Vision	41.38614096295089	-32.47947628482537	7052
1975ace94431321319a3c1d4c12031fd41b88b70	neural networks in an artificial life perspective	mobile robot;autonomous system;robotics;algoritmo genetico;sistema autonomo;systeme autonome;algorithme genetique;robotica;genetic algorithm;robotique;reseau neuronal;red neuronal;artificial life;artificial neural network;neural network	"""In the last few years several researchers within the Artificial Life and Mobile Robotics community used Artificial Neural Networks. Expli citl y viewing Neural Networks in an Artificial Life perspective has a number of consequences that make research on what we will call Artificial Life Neural Networks (ALNNs) rather different from traditional connectionist research. The aim of the paper is to make the differences between ALNNs and """"classical"""" neural networks expli cit."""	artificial life;connectionism;mobile robot;neural networks;robotics	Stefano Nolfi;Domenico Parisi	1997		10.1007/BFb0020241	nervous system network models;mobile robot;genetic algorithm;computer science;autonomous system;artificial intelligence;machine learning;artificial intelligence system;operations research;artificial neural network;artificial life	AI	15.473105333830889	-20.249691792195033	7058
4f100fb20531559920941a15cd25bcac56b70b48	pattern matching image compression: algorithmic and empirical results	lempel ziv;approximate pattern matching;lossy lempel ziv scheme;hamming distance image compression 1d pattern matching lempel ziv scheme square root distortion fast fourier transform shannon entropy;image processing;data compression;pattern matching image coding data compression transform coding hamming distance image recognition entropy fast fourier transforms video compression modems;fast fourier transform;efficient implementation;image compression;pattern matching;generalized shannon entropy;algorithms on words;fast fourier transforms;approximate matching;entropy;entropy image processing data compression pattern matching fast fourier transforms;hamming and square root distortion	ÐWe propose a nontransform image compression scheme based on approximate one-dimensional pattern matching that we name Pattern Matching Image Compression (PMIC). The main idea behind it is a lossy extension of the Lempel-Ziv data compression scheme in which one searches for the longest prefix of an uncompressed image that approximately occurs in the already processed image (e.g., in the sense of the Hamming distance or, alternatively, of the square error distortion). This main algorithm is enhanced with several new features such as searching for reverse approximate matching, recognizing substrings in images that are additively shifted versions of each other, introducing a variable and adaptive maximum distortion level D, and so forth. These enhancements are crucial to the overall quality of our scheme and their efficient implementation leads to algorithmic issues of interest in their own right. Both algorithmic and experimental results are presented. Our scheme turns out to be competitive with JPEG and wavelet compression for good quality graphical images. We also review related theoretical results. Index TermsÐLossy Lempel-Ziv scheme, approximate pattern matching, image compression, generalized Shannon entropy, Hamming and square root distortion, algorithms on words, Fast Fourier Transform.	approximation algorithm;data compression;distortion;entropy (information theory);fast fourier transform;hamming distance;image compression;jpeg;lempel–ziv–stac;lempel–ziv–welch;lossy compression;pattern matching;power management integrated circuit;regular expression;shannon (unit);substring;wavelet transform;window function	Mikhail J. Atallah;Yann Génin;Wojciech Szpankowski	1999	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.777372	data compression;fast fourier transform;discrete mathematics;image processing;image compression;computer science;theoretical computer science;pattern recognition;mathematics	ML	46.16017658137356	-11.215974366161957	7064
868eebb9cce9bdfeb805cd391974f9f9c5f22e58	an improved multi-gene genetic programming approach for the evolution of generalized model in modelling of rapid prototyping process	fdm modeling;fdm;rapid prototyping modeling;over fitting;wear strength prediction	The Rapid prototyping RP processes are widely used for the fabrication of complex shaped functional prototypes from the 3-D design. Among the various RP processes, fused deposition modeling FDM is widely known among researchers. The working mechanism behind the FDM process is governed by multiple input and output variables, which makes this process complex and its implementation costly. Therefore, the highly generalized mathematical models are an alternative for the practical realization of the process. Artificial intelligence methods such as multi-gene genetic programming MGGP, artificial neural network and support vector regression can be used. Among these methods, MGGP evolves explicit models and its coefficients automatically. Since MGGP uses a multiple sets of genes for the formulation of model and is population based, it suffers from the problem of over-fitting. Over-fitting is caused due to inappropriate procedure of formation of MGGP model and the difficulty in model selection. To counter over-fitting, the present paper proposes an improved MGGP I-MGGP approach by embedding the statistical and classification algorithms in the paradigm of MGGP. The proposed I-MGGP approach is tested on the wear strength data obtained from the FDM process and results show that the I-MGGP has performed better than the standard MGGP approach. Thus, the I-MGGP model can be deployed by experts for understanding the physical aspects as well as optimizing the performance of the process.	genetic programming;rapid prototyping	Akhil Garg;Kang Tai	2014		10.1007/978-3-319-07455-9_23	simulation;computer science;artificial intelligence;machine learning;overfitting	AI	14.43135676718176	-22.18583237038793	7075
5df4e8af48fbf2adab71dccac10c6cae8a3e5c32	representation of finite state automata in recurrent radial basis function networks	automata;backpropagation through time;high-order neural networks;inductive inference;learning from hints;radial basis functions;recurrent radial basis functions;recurrent networks	In this paper, we propose some techniques for injecting finite state automata into Recurrent Radial Basis Function networks (R2BF). When providing proper hints and constraining the weight space properly, we show that these networks behave as automata. A technique is suggested for forcing the learning process to develop automata representations that is based on adding a proper penalty function to the ordinary cost. Successful experimental results are shown for inductive inference of regular grammars.	automata theory;automaton;finite-state machine;inductive reasoning;penalty method;radial (radio);radial basis function;regular grammar	Paolo Frasconi;Marco Gori;Marco Maggini;Giovanni Soda	1996	Machine Learning	10.1007/BF00116897	radial basis function;continuous spatial automaton;quantum finite automata;computer science;artificial intelligence;machine learning;automata theory;ω-automaton;mathematics;finite-state machine;artificial neural network;algorithm	ML	17.685889747601703	-26.89226757170545	7086
37b4ab0d475d8208050d17519c8f9399d812f710	interval linear optimization problems with fuzzy inequality constraints	linear programming problem interval linear optimization problems fuzzy inequality constraints imprecise parameters fuzzy intervals;cost function;fuzzy sets;vectors;linear programming;linear programming fuzzy set theory;programming;linear programming fuzzy sets programming vectors educational institutions cost function	In many real-life situations, we come across problems with imprecise input values. Imprecisions are dealt with by various ways. One of them is interval based approach in which we model imprecise quantities by intervals, and suppose that the quantities may vary independently and simultaneously within their intervals. In most optimization problems, they are formulated using imprecise parameters. Such parameters can be considered as fuzzy intervals, and the optimization tasks with interval cost function are obtained. When realistic problems are formulated, a set of intervals may appear as coefficients in the objective function or the constraints of a linear programming problem. In this paper, we introduce a new method for solving linear optimization problems with interval parameters in the objective function and the inequality constraints, and we show the efficiency of the proposed method by presenting a numerical example.	coefficient;interval arithmetic;linear programming;mathematical optimization;numerical analysis;optimization problem;real life;social inequality	Ibraheem Alolyan	2014	2014 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making (MCDM)	10.1109/MCDM.2014.7007197	stochastic programming;programming;mathematical optimization;discrete mathematics;linear-fractional programming;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;linear programming;fuzzy number;machine learning;mathematics;fuzzy set;management;fuzzy set operations	Logic	-0.5981458674982498	-18.064209776772675	7088
128af79ff87575a291ba2b2f5ec6ee2695d2eec0	a new fuzzy connectivity class application to structural recognition in images	image segmentation;fuzzy set theory	Fuzzy sets theory constitutes a poweful tool, that can lead to more robustness in problems such as image segmentation and recognition. This robustness results to some extent from the partial recovery of the continuity that is lost during digitization. Here we deal with fuzzy connectivity notions. We show that usual fuzzy connectivity definitions have some drawbacks, and we propose a new definition, based on the notion of hyperconnection, that exhibits better properties, in particular in terms of continuity. We illustrate the potential use of this definition in a recognition procedure based on connected filters. A max-tree representation is also used, in order to deal efficiently with the proposed connectivity.	applicative programming language;image segmentation;scott continuity	Olivier Nempont;Jamal Atif;Enrico Angelini;Isabelle Bloch	2008		10.1007/978-3-540-79126-3_40	computer vision;discrete mathematics;defuzzification;fuzzy classification;computer science;fuzzy number;machine learning;data mining;mathematics;fuzzy set;image segmentation;fuzzy set operations	Vision	-0.03333168919441341	-24.387870848831216	7114
ed9903c6b8bbe119a8bf12bc081250b5a2cc7748	evaluation of the financial derivative risk from the probability of default angle	american mortgage crisis;financial crisis;probability of default;financial derivative risk;financial supervision;investments;probability;technological innovation;empirical analysis;financial management;risk analysis;financial supervision financial crisis financial derivative financial risk;default angle;risk analysis financial management probability;global liquidity;data mining;financial derivatives;american mortgage crisis financial derivative risk probability default angle american financial crisis global liquidity geometric progression;geometric progression;loans and mortgages;financial derivative;financial risk;correlation;security;american financial crisis;economic indicators;loans and mortgages investments computer security logic costs risk analysis computational modeling computer simulation asset management nonhomogeneous media	The over development and excessive use of financial derivatives is one of the most important reasons in the evolvement of this American financial crisis. The theory on the formation and accumulation of financial risks and their transformation into the financial crisis shows that the development and use of large number of derivatives increase the global liquidity rapidly, and lead to the excess of global liquidity. And while the sectional and visible risk was transferred and avoided, the total risks were accumulated speedily in geometric progression, and ultimately triggered American mortgage crisis. The result of our empirical analysis confirmed the above theoretical argument.	color gradient;offset binary;tree accumulation	Degong Ma;Yancun Chan;Yang Wei	2009	2009 Third UKSim European Symposium on Computer Modeling and Simulation	10.1109/EMS.2009.94	financial economics;liquidity crisis;liquidity risk;actuarial science;financial risk;finance;business	Logic	0.21183229588749833	-11.46585142549463	7117
27a81ed7a773a9c08429eb7209eb22c8d162a9dd	theoretical foundations of deep learning via sparse representations: a multilayer sparse model and its connection to convolutional neural networks		Modeling data is the way we-scientists-believe that information should be explained and handled. Indeed, models play a central role in practically every task in signal and image processing and machine learning. Sparse representation theory (we shall refer to it as Sparseland) puts forward an emerging, highly effective, and universal model. Its core idea is the description of data as a linear combination of few atoms taken from a dictionary of such fundamental elements.	artificial neural network;convolutional neural network;deep learning;dictionary;image processing;machine learning;sparse approximation	Vardan Papyan;Yaniv Romano;Jeremias Sulam;Michael Elad	2018	IEEE Signal Processing Magazine	10.1109/MSP.2018.2820224	task analysis;image processing;computer science;convolutional neural network;theoretical computer science;deep learning;sparse matrix;linear combination;sparse approximation;data modeling;artificial intelligence	ML	20.746291679044898	-46.394970988325554	7123
7e4381cbbfbfb37de4519ac4933561dc4d47e552	dynamic characteristic of a multiple chaotic neural network and its application	engineering;aerospace;chaotic dynamics;automotive;industry sectorsit software electronics;journal;annealing strategy;combinatorial optimization;telecommunications;neural network	Based on chaotic neural network, a multiple chaotic neural network algorithm combining two different chaotic dynamics sources in each neuron is proposed. With the effect of self-feedback connection and non-linear delay connection weight, the new algorithm can contain more powerful chaotic dynamics to search the solution domain globally in the beginning searching period. By analyzing the dynamic characteristic and the influence of cooling schedule in simulated annealing, a flexible parameter tuning strategy being able to promote chaotic dynamics convergence quickly is introduced into our algorithm. We show the effectiveness of the new algorithm in two difficult combinatorial optimization problems, i.e., a traveling salesman problem and a maximum clique problem.	algorithm;artificial neural network;chaos theory;clique (graph theory);clique problem;combinatorial optimization;computer cooling;in the beginning... was the command line;mathematical optimization;maxima and minima;neuron;nonlinear system;simulated annealing;simulation;travelling salesman problem	Gang Yang;Junyan Yi	2013	Soft Comput.	10.1007/s00500-012-0948-8	mathematical optimization;simulation;combinatorial optimization;computer science;artificial intelligence;machine learning;aerospace;artificial neural network	ML	30.739883483884512	3.2993147762818484	7129
8d3f642aaae9f6e06b70f1399067e94db4cb8c02	determining the position of a robot using a single calibration object	azimuth;mobile robot;mobile robots;three dimensional;calibration robot kinematics mobile robots cameras robot vision systems wheels lenses azimuth humans dead reckoning;lenses;humans;dead reckoning;robot vision systems;calibration;cameras;robot kinematics;wheels	A procedure for determining the position uniquely, of a mobile robot in a threedimensional space is presented. The method consists of viewing a single sphere with horizontal and vertical calibration great circles and computing distance and elevation and azimuth angles with respect to the sphere. The method is simple and provides good results as long as the sphere is projected onto a large portion of the image plane. Results using simulated and actual data are presented. *On leave from the Computer Science Department, University of Wyoming, Laramie, Wyoming 82071. +This work was supported in part by the Air Force Office of Scientific Research under Contract F49620-83-K-0013.	computer science;image plane;mobile robot	M. J. Magee;Jake K. Aggarwal	1984		10.1109/ROBOT.1984.1087164	mobile robot;computer vision;cartesian coordinate robot;simulation;geodesy;computer science;artificial intelligence;mobile robot navigation;robot kinematics;robot calibration	Robotics	52.54761833934951	-40.88417899767308	7133
116527b155d1c7679b99f64a6e88a32ea264a3bc	accurate face localization in videos using effective information propagation	particle filtering numerical methods face recognition object detection image colour analysis;videos face detection detectors particle tracking humans skin filtering target tracking robustness surveillance;face tracking;skin color;face recognition;particle filter;image colour analysis;backward propagation scheme face localization information propagation human face tracking face detection person specific skin color model particle filtering;face detection;object detection;particle filtering numerical methods	In this paper, we present a novel approach to accurate detection and tracking of human faces in videos. The idea is to propagate the information of a group of seed faces detected off-line with high certainties to recover the faces undetected or detected with only low confidence. Specifically, our approach first estimates from the color of the seed faces a person-specific skin color model, and based on which a particle filtering is performed for sequential face tracking. Then, a backward propagation scheme is devised to optimize the overall localization results in terms of smoothness. Experimental results demonstrate the efficacy of our approach.	face detection;online and offline;particle filter;software propagation	Ji Tao;Yap-Peng Tan	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530506	facial recognition system;computer vision;face detection;facial motion capture;speech recognition;object-class detection;particle filter;computer science	Vision	43.96350995957518	-48.17843820439035	7138
0888cd8cfc4fdf22286626cc313d5a720ad030ea	compact modeling technique for outdoor navigation	robot sensing systems;navigation mobile robots laser modes trajectory transmission line measurements robot sensing systems global positioning system terrain mapping indoor environments;navigation map;path planning;real time;3d scanner laser;computational geometry;robot navigation;voronoi diagram technique;mobile robots;robot mapping;compact modeling technique;navigation;computational modeling;adaptation model;robots;goliat;3 d scanner laser;outdoor robot navigation;3 d scanner laser navigation map outdoor robot mapping traversability;numerical models;traversable region model;outdoor;article;laser modes;goliat compact modeling technique outdoor robot navigation 3d scanner laser traversable region model voronoi diagram technique;path planning computational geometry mobile robots;traversability;compact model;voronoi diagram	In this paper, a new methodology to build compact local maps in real time for outdoor robot navigation is presented. The environment information is obtained from a 3-D scanner laser. The navigation model, which is called traversable region model, is based on a Voronoi diagram technique, but adapted to large outdoor environments. The model obtained with this methodology allows a definition of safe trajectories that depend on the robot's capabilities and the terrain properties, and it will represent, in a topogeometric way, the environment as local and global maps. The application presented is validated in real outdoor environments with the robot called GOLIAT.	3d scanner;map;robot;robotic mapping;voronoi diagram	Cristina Castejón;Dolores Blanco;Luis Moreno	2008	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2007.904786	robot;mobile robot;computer vision;navigation;simulation;voronoi diagram;computational geometry;computer science;artificial intelligence;motion planning;computational model;mobile robot navigation	Robotics	52.73889445551574	-36.02196314262861	7147
0162e84bbe995ec06e8e59dd9023c67d8f0e8880	learning to hash with partial tags: exploring correlation between tags and hashing bits for large scale image retrieval		Similarity search is an important technique in many large scale vision applications. Hashing approach becomes popular for similarity search due to its computational and memory efficiency. Recently, it has been shown that the hashing quality could be improved by combining supervised information, e.g. semantic tags/labels, into hashing function learning. However, tag information is not fully exploited in existing unsupervised and supervised hashing methods especially when only partial tags are available. This paper proposes a novel semi-supervised tag hashing (SSTH) approach that fully incorporates tag information into learning effective hashing function by exploring the correlation between tags and hashing bits. The hashing function is learned in a unified learning framework by simultaneously ensuring the tag consistency and preserving the similarities between image examples. An iterative coordinate descent algorithm is designed as the optimization procedure. Furthermore, we improve the effectiveness of hashing function through orthogonal transformation by minimizing the quantization error. Extensive experiments on two large scale image datasets demonstrate the superior performance of the proposed approach over several state-of-the-art hashing methods.	algorithm;code;coordinate descent;experiment;generalization error;gradient descent;hash function;image retrieval;iterative method;mathematical optimization;quantization (signal processing);semiconductor industry;similarity search;tag cloud;unsupervised learning	Qifan Wang;Luo Si;Dan Zhang	2014		10.1007/978-3-319-10578-9_25	feature hashing;hopscotch hashing;hash table;double hashing;hash function;linear hashing;extendible hashing;dynamic perfect hashing;open addressing;computer science;universal hashing;pattern recognition;data mining;k-independent hashing;cuckoo hashing;locality preserving hashing;watermark;2-choice hashing;information retrieval;locality-sensitive hashing	AI	23.826183654085316	-45.006192387265564	7161
b8fbd3f03d8e8874645884f60682f8d3cff79ad8	fusion of stereo and optical flow data using occupancy grids	stereo data;theoretical model;3d point cloud;mobile robot;real time;occupancy grid;optical flow data;cycab mobile robot;mobile robots;stereo camera;cycab mobile robot stereo data optical flow data real time obstacle detection 3d point cloud stereo camera sensor modality occupancy grid framework;incomplete data;robot vision;stereo image processing;image motion analysis optical sensors cameras optical noise stereo vision robot vision systems optical computing mobile robots image sequences computational efficiency;occupancy grid framework;optical flow;real time obstacle detection;collision avoidance;sensor modality;stereo image processing collision avoidance image sequences mobile robots robot vision;image sequences;time integration	In this paper, we propose a real-time method to detect obstacles using theoretical models of the ground plane, first in a 3D point cloud given by a stereo camera, and then in an optical flow field given by one of the stereo pair's camera. The idea of our method is to combine two partial occupancy grids from both sensor modalities with an occupancy grid framework. The two methods do not have the same range, precision and resolution. For example, the stereo method is precise for close objects but cannot see further than 7 m (with our lenses), while the optical flow method can see considerably further but has lower accuracy. Experiments that have been carried on the CyCab mobile robot and on a tractor demonstrate that we can combine the advantages of both algorithms to build local occupancy grids from incomplete data (optical flow from a monocular camera cannot give depth information without time integration)	algorithm;image resolution;mobile robot;numerical methods for ordinary differential equations;optical flow;point cloud;real-time clock;sensor;simultaneous localization and mapping;stereo camera;vii	Christophe Braillon;Kane Usher;Cédric Pradalier;James L. Crowley;Christian Laugier	2006	2006 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2006.1707392	computer vision;simulation;geography;computer graphics (images)	Robotics	52.35813018779175	-45.2623180093299	7162
49a8f38b7440e4b129432969062d31f9526253fc	evaluation of learning systems: an artificial data-based approach	generic model;learning system;machine learning;evaluation criteria	Experimentation has an important role in determining the capacities and restrictions of machine learning (ML) systems. In this paper we present the definition of some sensitivity and evaluation criteria which can be used to perform an evaluation of learning systems. Moreover, in order to overcome some of the limitations of real data sets, we introduce the specification of a parametrable generator of artificial learning sets which allows us to make easily complete experiments to discover some empirical rules of behavior for ML algorithms. Finally, we give some results obtained with different algorithms, showing that artificial data bases approach is an interesting direction to explore.		Hakim Lounis;Gilles Bisson	1991		10.1007/BFb0017038	natural language processing;unsupervised learning;robot learning;instance-based learning;algorithmic learning theory;artificial intelligence;machine learning;learning classifier system;computational learning theory;active learning;hyper-heuristic	AI	9.78983221984631	-38.49005541307163	7176
299b65d5d3914dad9aae2f936165dcebcf78db88	weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation	learning artificial intelligence expectation maximisation algorithm feedforward neural nets image segmentation;pascal voc 2012 image segmentation benchmark weakly supervised learning semisupervised learning deep convolutional neural network semantic image segmentation model training dcnn pixel level annotation weakly annotated training data strongly labeled image weakly labeled image expectation maximization method em method;image segmentation training semantics benchmark testing training data convolutional codes google	Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.	artificial neural network;benchmark (computing);bitbucket;convolutional neural network;expectation–maximization algorithm;image segmentation;pixel;semi-supervised learning;semiconductor industry;supervised learning	George Papandreou;Liang-Chieh Chen;Kevin P. Murphy;Alan L. Yuille	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.203	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	23.682063195463524	-47.689871887120574	7191
e954bd3d9dc080792e7d084929ed5add67e623b7	an intermodal transport network planning algorithm using dynamic programming—a case study: from busan to rotterdam in intermodal freight routing	dynamic programming;weighted constrained shortest path problem;intermodal freight routing problem;label setting algorithm	This paper presents a dynamic programming algorithm to draw optimal intermodal freight routing with regard to international logistics of container cargo for export and import. This study looks into the characteristics of intermodal transport using multi-modes, and presents a Weighted Constrained Shortest Path Problem (WCSPP) model. This study draws Pareto optimal solutions that can simultaneously meet two objective functions by applying the Label Setting algorithm, a type of Dynamic Programming algorithms, after setting the feasible area. To improve the algorithm performance, pruning rules have also been presented. The algorithm is applied to real transport paths from Busan to Rotterdam, as well as to large-scale cases. This study quantitatively measures the savings in both transport cost and time by comparing single transport modes with intermodal transport paths. Last, this study applies a mathematical model and MADM model to the multiple Pareto optimal solutions to estimate the solutions.	algorithm;alpha–beta pruning;automated planning and scheduling;dynamic programming;experiment;logistics;mathematical model;pareto efficiency;routing;shortest path problem;transportation theory (mathematics)	Jae Hyung Cho;Hyun Soo Kim;Hyung Rim Choi	2010	Applied Intelligence	10.1007/s10489-010-0223-6	simulation;computer science;dynamic programming	ML	16.159362763261036	1.400705250901779	7196
a28f4b43466504bd1ce336ac62a992ea5703b31b	road detection and vehicle tracking by vision for adaptive cruise control	monocular vision;road detection;statistical model;three dimensional;process design;adaptive cruise control;three dimensional reconstruction;vehicle tracking;reconstruction algorithm	This article deals first with a process designed to detect the circulation lane of a vehicle by onboard monocular vision. This detection process is based on a recursive updating of a statistical model of the lane obtained by a training phase. Once the lane has been located, a reconstruction algorithm computes the vehicle location on its lane and the three-dimensional shape of the road. Thereafter, the authors seek to detect and track vehicles situated in front of their vehicle and equipped with specific visual markers in order to achieve an accurate determination of their location and speed. By combining these various data, the most dangerous obstacle can be identified. Each of these three processes is described in detail, and significant examples are provided. KEY WORDS—road detection, three-dimensional reconstruction, vehicle tracking	algorithm;recursion;situated;statistical model;vehicle tracking system	Romuald Aufrère;François Marmoiton;Roland Chapuis;François Collange;Jean-Pierre Dérutin	2001	I. J. Robotics Res.	10.1177/02783640122067390	control engineering;statistical model;process design;three-dimensional space;computer vision;simulation;computer science;monocular vision;artificial intelligence;mathematics;statistics	Robotics	51.841406252681665	-36.87176477244531	7197
7328a892beb59fb89a1769def541815cfdd13e6a	determination of process variables in melt-based manufacturing processes	process observation;gas metal arc welding;websearch;manufacturing;ikz053100;laser cutting;self optimisation;rwth publications	Industrial manufacturing requires continuous production at reliable quality to be competitive. Many manufacturing processes are run close to their technological limits to increase productivity what leads to a significant threat of malfunction in the case of limited control over setting parameters or deviating boundary conditions.This paper discusses the difficulties of determining process variables during manufacturing for two melt-based manufacturing processes, laser cutting and gas metal arc welding. Both manufacturing processes show a highly dynamic and complex behaviour which today prevents a physical description of all interactions of the process variables. On the practical side, even the dominant process variables cannot be measured as they are not directly accessible.The approach that is presented here suggests a combined solution with both modelling and measuring tools that connect through surrogate criteria. It involves a simplified modelling of the manufacturing process that describes the proces...		Ulrich Thombansen;Marion Purrio;Guido Buchholz;Torsten Hermanns;Thomas Molitor;Konrad Willms;Wolfgang Schulz;Uwe Reisgen	2016	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2016.1145799	engineering;operations management;manufacturing;engineering drawing;gas metal arc welding;manufacturing engineering;mechanical engineering	Robotics	10.745156358547014	3.340565946607975	7198
f6c1777114f9a46e1539ca742db803aeea1af52d	a two-phase method of detecting abnormalities in aircraft flight data and ranking their impact on individual flights	support vector machines svms aircraft landing guidance artificial intelligence fault diagnosis;aircraft landing guidance;support vector machines;aircraft training air safety support vector machines fault diagnosis artificial intelligence;risk management;time series aerospace computing pattern classification support vector machines;time series;classification;computing;aerospace computing;pattern classification;artificial intelligence;aircraft parameter two phase method abnormality detection aircraft flight data individual flight impact two phase novelty detection approach aircraft flight descent phase normal time series data uk air accident investigation branch uk civil aviation authority united kingdom support vector machine svm k means one class classifier f score tool feature selection tool event based approach;data quality;detection and identification systems;methodology;fault diagnosis;descent	A two-phase novelty detection approach to locating abnormalities in the descent phase of aircraft flight data is presented. It has the ability to model normal time series data by analyzing snapshots at chosen heights in the descent, weight individual abnormalities, and quantitatively assess the overall level of abnormality of a flight during the descent to a given runway. The method models normal approaches to a given runway (as determined by the airline's standard operating procedures) and detects and ranks deviations from that model. The approach expands on a recommendation by the UK Air Accident Investigation Branch to the UK Civil Aviation Authority. The first phase quantifies abnormalities at certain heights in a flight. The second phase ranks all flights to identify the most abnormal, each phase using a one-class classifier. For both the first and second phases, i.e., the support vector machine (SVM), the mixture of Gaussians and the K-means one-class classifiers are compared. The method is tested using a data set containing manually labeled abnormal flights. The results show that the SVM provides the best detection rates and that the approach identifies unseen abnormalities with a high rate of accuracy. The feature selection tool F-score is used to identify differences between the abnormal and normal data sets. It identifies the heights where the discrimination between the two sets is largest and the aircraft parameters most responsible for these variations. The method presented adds much value to the existing event-based approach.	f1 score;feature selection;k-means clustering;lévy flight;mixture model;novelty detection;sensor;standard operating procedure;support vector machine;time series;two-phase commit protocol;two-phase locking	Edward Smart;David J. Brown;John Denman	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2012.2188391	computing;simulation;data quality;risk management;computer science;engineering;artificial intelligence;machine learning;time series;methodology;data mining;statistics	ML	5.361087196797981	-36.14065570236425	7222
c2a65f6006e7fd9641100b4660f99b1e8eff1c32	value-at-risk forecasts based on decomposed return series: the short run matters		We apply wavelet decomposition to decompose financial return series into a time frequency domain and assess the relevant frequencies for adequate daily Value-at-Risk (VaR) forecasts. Our results indicate that the frequencies that describe the short-run information of the underlying time series comprise the necessary information for daily VaR forecasts.	value at risk	Theo Berger	2015		10.1007/978-3-319-42902-1_68	actuarial science	NLP	4.724065117777696	-14.374445652170742	7231
474d1ae8324c57cb318dfd6a3cb96833cf24ff39	entropic component analysis and its application in geological data	model selection;discriminant function analysis;oil ﬁeld brines;bayesian approach;oil field brines;reference model;qualitative analysis;relative entropy;variable selection;sample classiﬁcation;data analysis;complex system;component analysis;sample classification;likelihood function;maximum entropy;logistic likelihood function	We present an entropic component analysis for identifying key parameters or variables and the joint effects of various parameters that characterize complex systems. This approach identifies key parameters through solving the variable selection problem. It consists of two steps. First, a Bayesian approach is utilized to convert the variable selection problem into the model selection problem. Second, the model selection is achieved uniquely by evaluating the information difference of models by relative entropies of these models and a reference model. We study a geological sample classification problem, where a brine sample from Texas and Oklahoma oil field is considered, to illustrate and examine the proposed approach. The results are consistent with qualitative analysis of the lithology and quantitative discriminant function analysis. Furthermore, the proposed approach reveals the joint effects of the parameters, while it is unclear from the discriminant function analysis. The proposed approach could be thus promising to various geological data analysis. & 2011 Elsevier Ltd. All rights reserved.	complex system;complex systems;discriminant;feature selection;kullback–leibler divergence;model selection;reference model;selection (genetic algorithm);selection algorithm	Chih-Yuan Tseng;Chien-Chih Chen	2011	Computers & Geosciences	10.1016/j.cageo.2010.11.016	econometrics;complex systems;reference model;bayesian probability;qualitative research;principle of maximum entropy;pattern recognition;discriminant function analysis;mathematics;likelihood function;kullback–leibler divergence;data analysis;feature selection;model selection;statistics	AI	27.11637864041047	-24.494238290179705	7232
5123cd18e334945912c6d10037c4cc9f1d53df61	coordinated bidding of ancillary services for vehicle-to-grid using fuzzy optimization	tendering autoregressive moving average processes electric vehicles fuzzy set theory power grids power markets;vehicle to grid v2g electric vehicles evs electricity market fuzzy set theory regulation service smart grid;optimization uncertainty electricity supply industry batteries linear programming predictive models power grids;electric vehicles texas electricity markets electric reliability council deployment signals ancillary service prices fuzzy set theory arima models autoregressive integrated moving average models electricity market parameters fuzzy optimization coordinated bidding optimal bidding aggregated capacity aggregators v2g vehicle to grid power grid ev	Electric vehicles (EVs) can be effectively integrated with the power grid through vehicle-to-grid (V2G). V2G has been proven to reduce the EV owner cost, support the power grid, and generate revenues for the EV owner. Due to regulatory and physical considerations, aggregators are necessary for EVs to participate in electricity markets. The aggregator combines the capacities of many EVs and bids their aggregated capacity into electricity markets. In this paper, an optimal bidding of ancillary services coordinated across different markets, namely regulation and spinning reserves, is proposed. This coordinated bidding considers electricity market uncertainties using fuzzy optimization. The electricity market parameters are forecasted using autoregressive integrated moving average (ARIMA) models. The fuzzy set theory is used to model the uncertainties in the forecasted data of the electricity market, such as ancillary service prices and their deployment signals. Simulations are performed on a hypothetical group of 10000 EVs in the electric reliability council of Texas electricity markets. The results show the benefit of the proposed fuzzy algorithm compared with previously proposed deterministic algorithms that do not consider market uncertainties.	algorithm;autoregressive integrated moving average;autoregressive model;computer simulation;extended validation certificate;fuzzy set;load profile;mathematical optimization;news aggregator;set theory;software deployment	Muhammad Ansari;Ali T. Al-Awami;Eric Sortomme;M. A. Abidoeric	2015	IEEE Transactions on Smart Grid	10.1109/TSG.2014.2341625	financial economics;electricity market;marketing;operations management	Robotics	3.5509225456485303	3.3387678255179316	7235
817819a88c5c038741898fb84ff7baf15b881768	colour image encryption based on logistic mapping and double random-phase encoding		In this paper, we propose a novel method to encrypt a color image by use of Logistic mapping and double random phase encoding. Firstly, use Logistic mapping to diffuse the color image, then the RGB components of the result are scrambled by replacement matrixes generated by Logistic mapping. Secondly, by utilizing double random phase encoding to encrypt the three scrambled images into one encrypted image. Experiment results reveal the fact that the proposed method not only can achieve good encryption result, but also that the key space is large enough to resist against common attack.	color image;encryption;key space (cryptography);logistic map	Huiqing Huang;Shouzhi Yang	2017	IET Image Processing	10.1049/iet-ipr.2016.0552	computer vision;theoretical computer science;pattern recognition	EDA	38.69925032272491	-8.88191993538526	7247
a3c3032732f209db66967e73688ee719c179eabb	surveillance system of power transmission line via object recognition and 3d vision computation	object recognition;video surveillance;3d vision;surveillance systems	Surveillance systems have been widely applied on power transmission system for security precaution of the major risk factor that is the construction activity in the vicinity of power transmission line. However, currently used automatic object detection in surveillance systems suffers from high error rate and has at least two limitations: first, the type of the object can’t be recognized; second, the dangerous strength of the object cannot be identified. In this paper, we propose a video surveillance method for the security precaution of power transmission line via the techniques of object recognition and 3D spatial location detection so that the motion objects are recognized and the position and size of the object are determined to identify the dangerous strength. Experimental results show that the developed system based on our proposed method is feasible and practical.	closed-circuit television;computation;nvidia 3d vision;object detection;outline of object recognition;transmission line	Yuanxin Zhang;Xuanqin Mou	2014		10.1117/12.2041412	computer vision;simulation;cognitive neuroscience of visual object recognition;video tracking;3d single-object recognition;computer security	Mobile	40.98293754382438	-44.83196202528929	7254
322c5c61859c9c858daf717873d70c2ac39658d2	context-based support vector machines for interconnected image annotation	kernel function;context free;contextual information;positive definite;image annotation;fixed point;social network;variational approach;kernel method;context dependent;support vector machine	We introduce in this paper a novel image annotation approach based on support vector machines (SVMs) and a new class of kernels referred to as context-dependent. The method goes beyond the naive use of the intrinsic low level features (such as color, texture, shape, etc.) and context-free kernels, in order to design a kernel function applicable to interconnected databases such as social networks. The main contribution of our method includes (i) a variational approach which helps designing this function using both intrinsic features and the underlying contextual information resulting from different links and (ii) the proof of convergence of our kernel to a positive definite fixed-point, usable for SVM training and other kernel methods. When plugged in SVMs, our context-dependent kernel consistently improves the performance of image annotation, compared to context-free kernels, on hundreds of thousands	ambiguous name resolution;automatic image annotation;cdk;context-free language;context-sensitive language;database;kernel (operating system);kernel method;lossless compression;mathematical optimization;ontology (information science);performance;social network;support vector machine;texture mapping;variational principle	Hichem Sahbi;Xi Li	2010		10.1007/978-3-642-19315-6_17	kernel;support vector machine;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;context-dependent memory;pattern recognition;data mining;graph kernel;mathematics;fixed point;positive-definite matrix;tree kernel;automatic image annotation;polynomial kernel;social network	ML	23.39709470284936	-44.43492097177124	7265
9099cdd86b7c956e0cc398e1faa623aad5d0f4fe	secure image encryption without size limitation using arnold transform and random strategies	image encryption;image hashing;random division;arnold transform;image scrambling	Encryption is an efficient way to protect the contents of digital media. Arnold transform is a significant technique of image encryption, but has weaknesses in security and applications to images of any size. To solve these problems, we propose an image encryption scheme using Arnold transform and random strategies. It is achieved by dividing the image into random overlapping square blocks, generating random iterative numbers and random encryption order, and scrambling pixels of each block using Arnold transform. Experimental results show that the proposed encryption scheme is robust and secure. It has no size limitation, indicating the application to any size images.	arnold;digital media;encryption;iteration;pixel	Zhenjun Tang;Xianquan Zhang	2011	Journal of Multimedia	10.4304/jmm.6.2.202-206	multiple encryption;disk encryption theory;40-bit encryption;plaintext-aware encryption;theoretical computer science;deterministic encryption;probabilistic encryption	Crypto	38.90494184577295	-9.395541752224615	7273
f45beec2d68ad93dd07801501f2c045d4e983745	bca: bi-symmetric component analysis for temporal symmetry in human actions	eigenvalues and eigenfunctions;measurement;action detection bi symmetric;bi symmetric;action recognition bca bisymmetric component analysis temporal symmetry action detection;video sequences;video signal processing image motion analysis object detection object recognition principal component analysis;symmetric matrices;action detection;visualization;feature extraction;symmetric matrices video sequences proposals feature extraction eigenvalues and eigenfunctions visualization measurement;proposals	In the past, many research efforts are invested into discriminative action recognition task but the general temporal structure of human actions is overlooked. In this paper, we focus on a specific yet common structure of human actions: temporal symmetry. The key contribution is that we model the temporal symmetry property of human action and separate this signal out of original action sequences without specifying which action category. Based on this modeling, a novel and effective method is proposed to detect the temporal symmetric part of any given human action sequence. Experimental results on two popular human action datasets verify that the temporal symmetry benefits both action detection and action recognition.	effective method	Chenyang Zhang;Yingli Tian	2016	2016 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2016.7552918	computer vision;visualization;feature extraction;computer science;theoretical computer science;machine learning;mathematics;measurement;symmetric matrix	Vision	37.530324620112324	-47.86520479793119	7274
1662fa9cd011d7c06cd4ade627ef6191332b235f	low bit rate software-only wavelet video coding	data transmission;image coding;real time;refinement bit video coding embedded wavelet software only spiht algorithm wavelet transforms template based filtering progressive reconstruction complexity;wavelet transforms;video coding;wavelet transform;image reconstruction;indexation;bit rate video coding filtering image reconstruction image coding codecs streaming media application software wavelet transforms indexing;high efficiency;image reconstruction video coding wavelet transforms	This paper extends the concept of embedded wavelet still image coding to video coding. The starting point of our work is the SPIHT image codec introduced by Said and Pearlman in [4]. We choose this algorithm because of its simplicity and its high efficiency. Our goal is to streamline its performance in order to explore its potential in real time software-only video coding applications. At low rates, the computational bottleneck of the SPIHT algorithm is in the forward and inverse wavelet transforms. To circumvent this problem, we propose a template-based filtering approach which replaces all multiplies by indexing operations. Unlike other multiplierless filtering techniques, the distortion penalty paid for the reduction in complexity is negligible. Furthermore, the proposed technique can efficiently accommodate progressive reconstruction. Specifically, we show that bitwise progressive reconstruction, i.e., updating the output after receiving each new refinement bit, can be achieved with a complexity proportional to the number of nonzero bits transmitted. This approach is specially interesting for data transmission over low bandwidth channels.	adobe streamline;algorithm;bandwidth management;bitwise operation;codec;data compression;distortion;embedded system;refinement (computing);set partitioning in hierarchical trees;wavelet transform	J. Paris;J. Alamanac;R. Mateu;X. Masvidal;Xavier Ginesta	1997		10.1109/MMSP.1997.602631	wavelet;computer vision;harmonic vector excitation coding;computer science;theoretical computer science;coding tree unit;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;wavelet transform;computer graphics (images)	Vision	45.11544860178608	-16.574956666207434	7283
3a167a51a50abcd390e96f91016a8b5109d939a2	indirect estimation of proportions in natural resource surveys	auxiliary information;qualitative variable;regression estimator	Abstract: Regression type estimators of a population proportion under a general sampling design and using auxiliary information are obtained. Confidence intervals based on various methods, involving auxiliary information, are also derived. An application of the proposed methods is illustrated by estimating the proportion of lakes at risk of acidification, based on data from the U.S. Environmental Protection Agency. Theoretical properties suggest that the proposed methods can outperform alternative methods, and the results derived from a Monte Carlo simulation study support this view.		María del Mar Rueda;Juan Francisco Muñoz;Antonio Arcos;Encarnación Álvarez	2011	Mathematics and Computers in Simulation	10.1016/j.matcom.2010.12.028	econometrics;data mining;mathematics;statistics;dummy variable	ECom	28.355845806826974	-22.395241762516008	7285
ef7ce942af4ac05daec5acd1821ff5f46c2598d9	q value-based dynamic programming with sarsa learning for real time route guidance in large scale road networks	dynamic programming;sound 4u;real time route guidance;digital tv;sarsa learning;large scale microscopic simulator;traffic engineering computing dynamic programming learning artificial intelligence;traffic congestion;distributed dynamic traffic management model;mathematical model;traffic engineering computing;real time traffic information;approximate optimal traveling time;learning artificial intelligence;programming;large scale road networks;q value based dynamic programming;programming equations mathematical model digital tv;traffic congestion q value based dynamic programming sarsa learning real time route guidance large scale road networks distributed dynamic traffic management model real time traffic information approximate optimal traveling time large scale microscopic simulator sound 4u	In this paper, a distributed dynamic traffic management model has been proposed to guide the vehicles, in order to minimize the computation time, make full use of real time traffic information and consequently improve the efficiency of the traffic system. For making the model work, we proposed a new dynamic route determination method, in which Q value-based Dynamic Programming and Sarsa Learning are combined to calculate the approximate optimal traveling time from each section to the destinations in the road networks. The proposed traffic management model is applied to the large scale microscopic simulator SOUND/4U based on the real world road network of Kurosaki, Kitakyushu in Japan. The simulation results show that the proposed method could reduce the traffic congestion and improve the efficiency of the traffic system effectively compared with the conventional method in the real world road network.	approximation algorithm;computation;dynamic programming;emoticon;network congestion;real-time computing;simulation;state-action-reward-state-action;time complexity	Shanqing Yu;Jing Zhou;Bing Li;Shingo Mabu;Kotaro Hirasawa	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252507	programming;real-time computing;simulation;floating car data;computer science;dynamic programming;mathematical model	Robotics	11.503102506365058	-8.505143013513363	7295
b48590976d34842c9039ba59424e1682cdde3e9a	perceptron-like large margin classifiers	perceptrons;large margin classifiers	We consider perceptron-like algorithms with margin in which the standard classification condition is modified to require a specific value of the margin in the augmented space. The new algorithms are shown to converge in a finite number of steps and used to approximately locate the optimal weight vector in the augmented space following a procedure analogous to Bolzano’s bisection method. We demonstrate that as the data are embedded in the augmented space at a larger distance from the origin the maximum margin in that space approaches the maximum geometric one in the original space. Thus, our algorithmic procedure could be regarded as an approximate maximal margin classifier. An important property of our method is that the computational cost for its implementation scales only linearly with the number of training patterns.	algorithmic efficiency;approximation algorithm;bisection method;computation;converge;embedded system;margin classifier;maximal set;perceptron;stepwise regression	Petroula Tsampouka	2007			mathematical optimization;margin;machine learning;pattern recognition;mathematics	ML	21.46803960017812	-37.04550112864032	7306
5fefbba7c1cbd39496b8631b27a48dc9b9825fc5	information-theoretic feature selection with discrete k-median clustering		We propose a novel computational framework that integrates information-theoretic feature selection with discrete \(k\)-median clustering (DKM). DKM is a domain-independent clustering algorithm which requires a pairwise distance matrix between samples that can be defined arbitrarily as input. In the proposed DKM clustering, the center of each cluster is represented by a set of samples, which induce a separate set of clusters for each feature dimension. We evaluate the relevance of each feature by the normalized mutual information (NMI) scores between the base clusters using all features and the induced clusters for that feature dimension. We propose a spectral cluster analysis (SCA) method to determine the number of clusters using the average of the relevance NMI scores. We introduce filter- and wrapper-based feature selection algorithms that produce a ranked list of features using the relevance NMI scores. We create an information gain curve and calculate the normalized area under this curve to quantify information gain and identify the contributing features. We study the properties of our information-theoretic framework for clustering, SCA and feature selection on simulated data. We demonstrate that SCA can accurately identify the number of clusters in simulated data and public benchmark datasets. We also compare the clustering and feature selection performance of our framework to other domain-dependent and domain-independent algorithms on public benchmark datasets and a real-life neural time series dataset. We show that DKM runs comparably fast with better performance.	cluster analysis;feature selection;theory	Onur Seref;Ya-Ju Fan;Elan Borenstein;W. Art Chaovalitwongse	2018	Annals OR	10.1007/s10479-014-1589-3	data mining;feature selection;mutual information;cluster analysis;single-linkage clustering;k-medians clustering;feature dimension;correlation clustering;artificial intelligence;computer science;pattern recognition;distance matrix	ML	0.5778246633880851	-42.61075892767146	7336
c3318b9540a8a73cb618dff1e8baf7bdf027d2af	uncertainty quantification for radio interferometric imaging: i. proximal mcmc methods		Uncertainty quantification is a critical missing component in radio interferometric imaging that will only become increasingly important as the big-data era of radio interferometry emerges. Since radio interferometric imaging requires solving a high-dimensional, ill-posed inverse problem, uncertainty quantification is difficult but also critical to the accurate scientific interpretation of radio observations. Statistical sampling approaches to perform Bayesian inference, like Markov Chain Monte Carlo (MCMC) sampling, can in principle recover the full posterior distribution of the image, from which uncertainties can then be quantified. However, traditional high-dimensional sampling methods are generally limited to smooth (e.g. Gaussian) priors and cannot be used with sparsity-promoting priors. Sparse priors, motivated by the theory of compressive sensing, have been shown to be highly effective for radio interferometric imaging. In this article proximal MCMC methods are developed for radio interferometric imaging, leveraging proximal calculus to support non-differential priors, such as sparse priors, in a Bayesian framework. Furthermore, three strategies to quantify uncertainties using the recovered posterior distribution are developed: (i) local (pixel-wise) credible intervals to provide error bars for each individual pixel; (ii) highest posterior density credible regions; and (iii) hypothesis testing of image structure. These forms of uncertainty quantification provide rich information for analysing radio interferometric observations in a statistically robust manner.	big data;compressed sensing;markov chain monte carlo;monte carlo method;pixel;radio-frequency identification;sampling (signal processing);sparse matrix;uncertainty quantification;well-posed problem	Xiaohao Cai;Marcelo Pereyra;Jason D. McEwen	2017	CoRR	10.1093/mnras/sty2004	statistical hypothesis testing;compressed sensing;uncertainty quantification;statistics;sampling (statistics);prior probability;physics;markov chain monte carlo;posterior probability;bayesian inference	ML	37.62954945621256	-24.86730391767228	7341
5da75a5b5772ee573d37e82595915965b20a8c80	modification of evolutionary multiobjective optimization algorithms for multiobjective design of fuzzy rule-based classification systems	design optimization algorithm design and analysis fuzzy systems evolutionary computation design engineering optimization methods humans pattern classification neural networks;optimisation;3d objective space;probability;fuzzy rule based classification systems;fuzzy rules;search problems computational complexity fuzzy logic fuzzy set theory multivalued logic optimisation pattern classification probability;training patterns;search ability;evolutionary multiobjective optimization;antecedent condition;fuzzy set theory;three dimensional;fuzzy logic;fuzzy rule base;complexity measure;computer experiment;computational complexity;classification system;pattern classification;search problems;emo algorithms;multivalued logic;crossover operations;selection probability evolutionary multiobjective optimization algorithm multiobjective design fuzzy rule based classification systems training patterns complexity measure antecedent condition search ability emo algorithms 3d objective space crossover operations;evolutionary multiobjective optimization algorithm;multiobjective design;selection probability	We examine three methods for improving the ability of evolutionary multiobjective optimization (EMO) algorithms to find a variety of fuzzy rule-based classification systems with different tradeoffs with respect to their accuracy and complexity. The accuracy of each fuzzy rule-based classification system is measured by the number of correctly classified training patterns while its complexity is measured by the number of fuzzy rules and the total number of antecedent conditions. One method for improving the search ability of EMO algorithms is to remove overlapping rule sets in the three-dimensional objective space. Another method is to choose similar rule sets as parents for crossover operations. The other method is to bias the selection probability of parents toward rule sets with high accuracy. The effectiveness of each method is examined through computational experiments on benchmark data sets	approximation algorithm;benchmark (computing);computation;evolutionary algorithm;experiment;fuzzy rule;logic programming;mathematical optimization;multi-objective optimization;rule 90;selection algorithm	Kaname Narukawa;Yusuke Nojima;Hisao Ishibuchi	2005	The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.	10.1109/FUZZY.2005.1452498	fuzzy logic;three-dimensional space;mathematical optimization;computer experiment;computer science;artificial intelligence;machine learning;probability;mathematics;fuzzy set;computational complexity theory;statistics	DB	4.24042697344489	-29.86674780109899	7361
c1d42e307cb16505cbcc7aba3f81fb8d1c765ec2	firefly algorithm for demand estimation of water resources		Firefly algorithm (FA) is an efficient swarm intelligence optimization technique, which has been used to solve many engineering optimization problems. In this paper, we present a new FA (called NFA) variant for demand estimation of water resources in Nanchang city of China. The performance of the standard FA highly depends on its control parameters. To tackle this issue, a dynamic step factor strategy is proposed. In NFA, the step factor is not fixed and it is dynamically updated during the search process. Three models in different forms (linear, exponential and hybrid) are developed based on the structure of social and economic conditions. Water demand in Nanchang city from 2003 to 2015 is considered as a case study. The data from 2003 to 2012 is used for finding the optimal weights, and the rest data (2013–2015) is for testing the models. Simulation results show that three FA variants can achieve promising performance. Our proposed NFA outperforms the standard FA and memetic FA (MFA), and the prediction accuracy is up to 97.91%.	firefly algorithm;mathematical optimization;memetic algorithm;memetics;nondeterministic finite automaton;simulation;swarm intelligence;time complexity	Hui Wang;Zhihua Cui;Wenjun Wang;Xinyu Zhou;Jia Zhao;Li Lv;Hui Sun	2017		10.1007/978-3-319-70093-9_2	swarm intelligence;artificial intelligence;firefly algorithm;engineering optimization;machine learning;computer science	AI	22.734162375786607	-2.4861172942854686	7367
c31ed4e2dc6e2c25a36a3ef5fe12bdcf58be268e	co-trained generative and discriminative trackers with cascade particle filter	discriminative tracker;cascade particle filter;generative tracker;co training;visual tracking	Visual tracking is a challenging problem, as the appearance of an object may change due to viewpoint variations, illumination changes, and occlusion. It may also leave the field of view (FOV), then reappears. In order to track and reacquire an unknown object with limited labeling data, we propose to learn these changes online and incrementally build a model that encodes all appearance variations while tracking. To address this semi-supervised learning problem, we propose a co-training framework with cascade particle filter to label incoming data continuously and online update hybrid generative and discriminative models. Each of the layers in the cascade contains one or more either generative or discriminative appearance models. The cascade manner of organizing the particle filter enables the efficient evaluation of multiple appearance models with different computational costs; thus improves the speed of the tracker. The proposed online framework provides temporally local tracking that adapts to appearance changes. Moreover, it provides an object-specific detection ability that allows to reacquire an object after total occlusion. Extensive experiments demonstrate that under challenging situations, our method has strong reacquisition ability and robustness to distracters in clutter background. We also provide quantitative comparisons to other state of the art trackers.	discriminative model;particle filter	Thang N. Dinh;Qian Yu;Gérard G. Medioni	2014	Computer Vision and Image Understanding	10.1016/j.cviu.2013.11.003	computer vision;eye tracking;machine learning;pattern recognition	Vision	33.73186802039523	-47.94267790383469	7371
7a88b30fcc825a27e55a68d696d8c90aa834a38b	mapreduce intrusion detection system based on a particle swarm optimization clustering algorithm	pattern clustering;computer network security;pattern clustering computer network security parallel algorithms particle swarm optimisation;intrusion detection clustering algorithms vectors testing particle swarm optimization mathematical model data models;particle swarm optimisation;false alarm rates mapreduce intrusion detection system particle swarm optimization clustering algorithm computer networks anomaly fragments discover sensitivity problem initial cluster centroids premature convergence commodity hardware large data sets;parallel algorithms	The increasing volume of data in large networks to be analyzed imposes new challenges to an intrusion detection system. Since data in computer networks is growing rapidly, the analysis of these large amounts of data to discover anomaly fragments has to be done within a reasonable amount of time. Some of the past and current intrusion detection systems are based on a clustering approach. However, in order to cope with the increasing amount of data, new parallel methods need to be developed in order to make the algorithms scalable. In this paper, we propose an intrusion detection system based on a parallel particle swarm optimization clustering algorithm using the MapReduce methodology. The use of particle swarm optimization for the clustering task is a very efficient way since particle swarm optimization avoids the sensitivity problem of initial cluster centroids as well as premature convergence. The proposed intrusion detection system processes large data sets on commodity hardware. The experimental results on a real intrusion data set demonstrate that the proposed intrusion detection system scales very well with increasing data set sizes. Moreover, it achieves close to the linear speedup by improving the intrusion detection and false alarm rates.	algorithm;anomaly detection;cluster analysis;commodity computing;experiment;intrusion detection system;mapreduce;mathematical optimization;monte carlo method;network traffic control;parallel programming model;particle swarm optimization;premature convergence;sampling (signal processing);scalability;speedup;test set	Ibrahim Aljarah;Simone A. Ludwig	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557670	multi-swarm optimization;computer science;theoretical computer science;network security;machine learning;data mining;parallel algorithm;cluster analysis	ML	4.787274140602732	-37.959993281190755	7386
6143a057065b5fbdf7e82538dcfc57e4cebf79c6	analysis of window-observation recurrence data	forecast;modelizacion;poisson process;parametric model;dato observacion;forecasting nonhomogeneous poisson process nonparametric methods repairable systems;point estimation;analisis estadistico;censure;nonparametric estimation;bepress selected works;recidivante;estimation non parametrique;database;grupo de excelencia;base dato;repairable system data;statistical method;non homogeneous poisson process;repairable system;windows;sistema reactivo;recurrence;modelisation;non parametric estimation;confidence interval;statistical analysis;censura;recidivant;recurrent;ciencias basicas y experimentales;fenetre;recurrencia;analytical method;matematicas;analyse statistique;recurrent events;base de donnees;reactive system;asymptotic properties;systeme reactif;simulation study;ventana;censorship;proceso poisson;confidence interval procedure;donnee observation;data quality;estimacion no parametrica;sistema reparable;recurrence data;cumulant;mean cumulative function;modeling;asymptotic normal;forecast mean cumulative function nonhomogeneous poisson process nonparametric estimation repairable system data;observation data;systeme reparable;nonhomogeneous poisson process;processus poisson;right censoring	Many systems experience recurrent events. Recurrence data are collected to analyze quantities of interest, such as the mean cumulative number of events. Methods of analysis are available for recurrence data with left and/or right censoring. Due to practical constraints, however, recurrence data are sometimes recorded only in windows, with gaps in between. Nelson (2003, page 75) gives one example, and Chapter 2 describes two other applications that window-observation recurrence data arise. With the need for analytical methods with windowobservation recurrence data, our research achieves the following three objectives: • Extend the existing statistical methods, both nonparametric and parametric, to analyze window-observation recurrence data, and our focus is to estimate the mean cumulative function (MCF). • Study and compare CI procedures for the MCF estimators with window-observation recurrence data, and make recommendations when the amount of observed information in the data is small. • Establish the asymptotic (i.e., large-sample) properties for the MCF estimators with window-observation recurrence data. Our research shows that the existing statistical methods, both nonparametric and parametric, can be extended to estimate the MCF with window-observation recurrence data. Chapter 2 provides the details on four MCF estimators, including the NP estimator, the nonhomogeneous Poisson process (NHPP) estimator, the local hybrid estimator, and the NHPP hybrid estimator. The NP estimator and the NHPP estimator for analysis with window-observation	censoring (statistics);microsoft windows;numerical analysis;numerical method;parametric model	Jianying Zuo;William Q. Meeker;Huaiqing Wu	2008	Technometrics	10.1198/004017008000000091	econometrics;parametric model;systems modeling;confidence interval;data quality;poisson process;reactive system;censorship;point estimation;data mining;mathematics;inhomogeneous poisson process;censoring;statistics;cumulant	ML	33.677221623752985	-21.619888041580715	7388
233bb2e24ab57de23f056ceaee5ef183e3ac4c2b	cosmo: a correlation sensitive mutation operator for multi-objective optimization	fitness landscape;operators;evolutionary multi objective optimization;monotone function;multi objective optimization;monotonic functions;evolutionary;circuits;multi objective optimization problem;circuit optimization;evolutionary algo rithm	This contribution is the first to discover exploitable structural features within circuit optimization problems (COP) and discuss how it is indicative of a general structure and possibly a 'measure of hardness' in real-world multi-objective optimization problems. We then present a methodology to exploit this structure in a multi-objective evolutionary algorithm by designing a novel Correlation Sensitive Mutation Operator, COSMO. COSMO is, at the least, universally applicable in the domain of circuits and we discuss how it can be easily extended to other domains. We discuss the rationale behind COSMO and interpret it in context of dimensional locality. We compare COSMO's performance with the traditional operators used for multi-objective optimization. For two instances of circuits, we show that COSMO gives significantly faster and better optimization than conventional operators. The paper also takes the first steps in thinking and interpreting how operators for MO-EAs should be designed.	design rationale;evolutionary algorithm;locality of reference;mathematical optimization;multi-objective optimization	Varun Aggarwal;Una-May O'Reilly	2007		10.1145/1276958.1277111	mathematical optimization;monotonic function;artificial intelligence;machine learning;mathematics	AI	23.222329423346633	-6.386780319345137	7404
0f98b2e6af11a5b0aaeaae40bddeea929ea3142c	a new framework of designing sequential ranking-and-selection procedures		Many classical sequential procedures model the partial sum difference process between two competing alternatives as a Brownian motion process. In this paper, the marginal probability of eliminating the best alternative is considered while modeling the partial sum difference process. We adaptively allocate the total amount of the probability of incorrect selection α to every time point where the comparisons between alternatives are conducted and set the continuation regions to ensure the marginal probability of eliminating the best alternative does not exceed the probability assigned to each time point t. We show by examples that under our framework, the procedure can be easily developed for both indifference-zone (IZ) and IZ free formulations.	brownian motion;continuation;continuation-passing style;decision boundary;marginal model;norton zone;sampling (signal processing)	Ying Zhong;L. Jeff Hong	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8247955	simulation;mathematical optimization;continuation;computer science;marginal distribution;brownian motion;time point;ranking;random variable;upper and lower bounds	Vision	25.720245565591796	-16.86997540236453	7413
691010486100506d9885289a768073aab74d8583	evolutionary approach to the game of checkers	evaluation function;genetics	A new method of genetic evolution of linear and nonlinear evaluation functions in the game of checkers is presented. Several practical issues concerning application of genetic algorithms for this task are pointed out and discussed. Experimental results confirm that proposed approach leads to efficient evaluation functions comparable to the ones used in some of commercial applications.	evaluation function;genetic algorithm;nonlinear system	Magdalena Kusiak;Karol Waledzik;Jacek Mandziuk	2007		10.1007/978-3-540-71618-1_48	simulation;computer science;artificial intelligence;evaluation function;algorithm	AI	26.607327985450866	-7.165368843111154	7415
85587bab4e5c4101b1bf70c4b715a1090b483b66	on inclusion measures of intuitionistic and interval-valued intuitionistic fuzzy values and their applications to group decision making		Ranking intuitionistic fuzzy values (IFVs) and interval-valued intuitionistic fuzzy values (IVIFVs) is an important and necessary work in intuitionistic fuzzy group decision making. Since the set of all IFVs is a poset and inclusion measure indicates the degree to which a given element of a poset is contained in another one. This paper studies hybrid monotonic (HM) inclusion measures of IFVs and IVIFVs respectively and discuss their applications to group decision making. Firstly, HM inclusion measure is defined on the posets of all IFVs and IVIFVs respectively. Then HM inclusion measures are studied by constructive approach. Furthermore, the HM inclusion measures are employed to make intuitionistic and interval-valued intuitionistic fuzzy group decisions. Lastly, practical examples are provided to illustrate the developed approaches respectively.	degree (graph theory);fuzzy set;intuitionistic logic	Hong-Ying Zhang;Shu Yun Yang;Zhi Wei Yue	2016	Int. J. Machine Learning & Cybernetics	10.1007/s13042-015-0410-1	discrete mathematics;mathematics;algorithm	AI	-2.437723442289129	-21.18240546277643	7428
cf29c3ed4a8d59a6305d5463d722da24502ef80f	fast inter-mode selection for h.264 encoders based on coded block patterns and interblock correlation		Provision of variable block sizes and multiple reference frames in motion estimation (ME) significantly improves the compression efficiency of H.264. The involved computation for finding the best reference picture and ME mode in the rate-distortion minimization sense, however, is usually intimidating. In this paper, a new H.264 fast inter-mode decision algorithm is presented to resolve the performance-complexity dilemma. The proposed method employs the coded block patterns, correlated modes and rate-distortion costs to avoid exhaustive mode evaluation. Specific values of coded block patterns, which reflect the accuracy of 16×16 ME, are used to identify whether large partitioning blocks are suitable. Inter-frame correlation in terms of modes and rate-distortion costs, is further incorporated for fast mode decision. Accuracy of the proposed expediting criteria is verified through comprehensive evaluations. Compared with state-of-the-art fast mode-decision algorithms, the proposed method achieves consistently better time savings. In addition to the reduction in computational complexity, the proposed method is elegant without any ad-hoc thresholds.	algorithm;computation;computational complexity theory;distortion;encoder;h.264/mpeg-4 avc;hoc (programming language);macroblock;motion estimation;precision and recall;reference frame (video);speedup	Dong-Wei Lin;Jin-Long Hsieh;Shih-Hsuan Yang;Chia-Chi Yang	2011	J. Inf. Sci. Eng.			AI	47.0403798975469	-18.972924065763394	7437
fb4dc483a183a4b80136573cff1d101a45691bdb	alternative statistical methods for bone atlas modelling	statistical methods;statistical method;nonlinear modelling;high dimensional data;linear model;bone;modeling	Traditional bone atlas modelling is carried out using linear methods such as PCA. Such linear models use a mean shape and principal modes to represent the atlas. A new shape, which is a high dimensional data vector, is then described using this mean and a weighted combination of the principal modes. The use of alternate methods for modelling statistical atlases have not been explored very much. Recently, there has been a lot of new work in the areas of multilinear modelling and nonlinear modelling. They present new ways of modelling high dimensional data. In this work, we compare and contrast several linear, multilinear and nonlinear methods for bone atlas modelling.	data point;linear model;nonlinear modelling;nonlinear system;principal component analysis;statistical model	Sharmishtaa Seshamani;Gouthami Chintalapani;Russell H. Taylor	2011		10.1117/12.878686	systems modeling;computer science;linear model;data mining;clustering high-dimensional data	Vision	29.969252851701413	-28.677822219658943	7441
de5c03878a7a4e32fac18d864ae0e857ea457e6d	wavelets and fractal image compression based on their self-similarity of the space-frequency plane of images	transformation ondelette;selfsimilarity;image coding;image processing;data compression;motion compensation;solution similitude;procesamiento imagen;similarity solution;traitement image;motion compensated;compensation mouvement;codage image;compression image;wavelet transform;solucion semejanza;senal video;signal video;image compression;autosimilitud;fractal;video signal;autosimilitude;compresion dato;transformacion ondita;space frequency;computer simulation;wavelet transformation;compression donnee;compresion imagen;fractal image compression	This paper presents a fusion scheme for Wavelets and Fractal image compression based on the self-similarity of the space-frequency plane of sub-band encoded images. Various kinds of Wavelet transform are examined for the characteristics of their self-similarity and evaluated for the adoption of Fractal modeling. The aim of this paper is to reduce the information of the two sets of blocks involved in the Fractal image compression by using the self-similarity of images. And also, the new video encoder using the fusion method of Wavelets and Fractal adopts the similar manner as the motion compensation technique of MPEG encoder. Experimental results show almost the same PSNR and bits rate as conventional Fractal image encoder depending on the sampled images by computer simulations.	fractal compression;image compression;self-similarity;wavelet	Yoshito Ueno	2001		10.1007/3-540-45333-4_13	data compression;computer simulation;computer vision;fractal;image processing;image compression;computer science;theoretical computer science;mathematics;fractal transform;fractal compression;motion compensation;algorithm;wavelet transform;computer graphics (images)	Vision	46.2477273037713	-14.334717532226922	7451
769e7992e0281bb69f1b8931dad6c22942930380	bankruptcy prediction in banks by fuzzy rule based classifier	pattern classification bank data processing fuzzy set theory multilayer perceptrons optimisation;optimisation;threshold accepting;type i error;multilayer perceptrons;us banks bankruptcy data set;rule based;combinatorial optimization problem;neural networks logistics feedforward neural networks predictive models regulators banking electronic mail profitability feeds forward contracts;multilayer perceptron;bank data processing;fuzzy set theory;multiobjective combinatorial optimization problem;optimization problem;fuzzy rule base;pattern classification;multi layer perceptron;bankruptcy prediction;fuzzy if then rule based classifier;multilayer perceptron bankruptcy prediction fuzzy rule based classifier fuzzy if then rule based classifier multiobjective combinatorial optimization problem us banks bankruptcy data set;fuzzy rule based classifier	In this paper, a fuzzy 'if-then' rule based classifier is employed to predict bankruptcy in banks. This classification problem is formulated as a multi objective combinatorial optimization problem, where the rule base size is minimized and classification rate is maximized. Modified threshold accepting is applied to solve this optimization problem. The efficacy of the classifier is tested on the well-known US banks bankruptcy data set. It performed very well and in the case of 2 partitions outperformed the multi layer perceptron by yielding higher average classification rate and lower average Type-I error.	combinatorial optimization;filter bank;fuzzy rule;mathematical optimization;optimization problem;perceptron;rule-based system;statistical classification;whole earth 'lectronic link	P. Ravi Kumar;Vadlamani Ravi	2006	2006 1st International Conference on Digital Information Management	10.1109/ICDIM.2007.369357	rule-based system;computer science;machine learning;pattern recognition;data mining;multilayer perceptron	Robotics	9.720414351767284	-40.936042753538885	7476
23aacb87d45ee7deabc07d80604b417b202be60e	constrained distance based clustering for time-series: a comparative and experimental study		Constrained clustering is becoming an increasingly popular approach in data mining. It offers a balance between the complexity of producing a formal definition of thematic classes—required by supervised methods—and unsupervised approaches, which ignore expert knowledge and intuition. Nevertheless, the application of constrained clustering to time-series analysis is relatively unknown. This is partly due to the unsuitability of the Euclidean distance metric, which is typically used in data mining, to time-series data. This article addresses this divide by presenting an exhaustive review of constrained clustering algorithms and by modifying publicly available implementations to use a more appropriate distance measure—dynamic time warping. It presents a comparative study, in which their performance is evaluated when applied to time-series. It is found that k-means based algorithms become computationally expensive and unstable under these modifications. Spectral approaches are easily applied and offer state-of-the-art performance, whereas declarative approaches are also easily applied and guarantee constraint satisfaction. An analysis of the results raises several influencing factors to an algorithm’s performance when constraints are introduced.	algorithm;analysis of algorithms;cluster analysis;constrained clustering;constraint satisfaction;control theory;data mining;dynamic time warping;euclidean distance;experiment;k-means clustering;stellar classification;supervised learning;time series;unsupervised learning	Thomas Andrew Lampert;Thi-Bich-Hanh Dao;Baptiste Lafabregue;Nicolas Serrette;Germain Forestier;Bruno Crémilleux;Christel Vrain;Pierre Gançarski	2018	Data Mining and Knowledge Discovery	10.1007/s10618-018-0573-y	artificial intelligence;machine learning;implementation;computer science;cluster analysis;euclidean distance;constrained clustering;dynamic time warping;constraint satisfaction;intuition	ML	7.394760972785507	-42.56747546137355	7483
2a6fb2e7f39b4ca038d6db055950fb796cb5623a	hysteresis effects under cir interest rates	modelizacion;tesoreria;movimiento caja;stochastic interest rate;tresorerie;finance;taux interet;discount rate;hysteresis;systeme aide decision;economic sciences;inversion;proceso irreversible;prise de decision;sunk cost;finance real options interest rate uncertainty perpetuities investment hysteresis;sistema ayuda decision;investment;interest rate;actualizacion;modelisation;actualisation;ciencias economicas;systeme incertain;decision support system;real options;interest rate models;option reelle;perpetuities;processus irreversible;real option;investissement;cox model;modele cox;coste;treasury assets;discounting;sciences economiques;cash flow;tasa interes;histeresis;toma decision;sistema incierto;modeling;investment hysteresis;uncertain system;interest rate uncertainty;modelo cox;opcion real;finanzas;irreversible process;cout	Most decision making research in real options focuses on revenue uncertainty assuming discount rates remain constant. However, for many decisions revenue or cost streams are relatively static and investment is driven by interest rate uncertainty, for example the decision to invest in durable machinery and equipment. Using interest rate models from Cox et al. (1985b), we generalize the work of Ingersoll and Ross (1992) in two ways. Firstly, we include real options on perpetuities (in addition to zero coupon cash flows). Secondly, we incorporate abandonment or disinvestment as well as investment options, and thus model interest rate hysteresis (parallel to revenue uncertainty in Dixit (1989a)). Under stochastic interest rates, economic hysteresis is found to be significant, even for small sunk costs. 2011 Elsevier B.V. All rights reserved. 1 See Pindyck (1991) and Dixit (1992) for an overview of the literature. Dixit and Pindyck (1994) provide an excellent revision of the various approaches and applications. A complementary survey may be found in Caballero (1999). 2 Moreover, there may even exist cases where additional costs of closing a project	closing (morphology);committed information rate;hysteresis	José Carlos Dias;Mark B. Shackleton	2011	European Journal of Operational Research	10.1016/j.ejor.2010.12.021	financial economics;actuarial science;decision support system;economics;hysteresis;rendleman–bartter model;economy	AI	2.9731051368723445	-5.584486252723084	7488
5fabdc00fb3391b43232d9b372f5e6cae4eb6400	algorithms for graph-constrained coalition formation in the real world	networks;collective energy purchasing;graphs;coalition formation	Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this article, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network. We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (Coalition Formation for Sparse Synergies (CFSS)), which is particularly efficient when applied to a general class of characteristic functions called m + a functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a nonredundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is four orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2,700 agents).	anytime algorithm;approximation algorithm;benchmark (computing);best, worst and average case;dhrystone;edge contraction;social network;sparse;synergy;telecommunications network	Filippo Bistaffa;Alessandro Farinelli;Jesús Cerquides;Juan A. Rodríguez-Aguilar;Sarvapali D. Ramchurn	2017	ACM TIST	10.1145/3040967	machine learning;characteristic function (probability theory);telecommunications network;artificial intelligence;partition (number theory);edge contraction;special case;computer science;algorithm;mathematical optimization;social network;graph	AI	14.382392820642071	-9.016292549854798	7494
d4d63d77d1ebb666e50c4cdd65e89fdf0466a7fa	complex interbank network estimation: sparsity-clustering threshold				Nils Bundi;Khaldoun Khashanah	2018		10.1007/978-3-030-05414-4_39		Vision	2.965173289233695	-8.579381221168282	7501
112647b226bb165b09ab49f9817404c06c75024e	modeling and analysis of the effects of qos and reliability on pricing, profitability, and risk management in multiperiod grid-computing networks	system reliability;risk management;decision support systems;market network equilibrium;quality of service;pricing mechanism;grid computing	In this paper we develop a network equilibrium model for optimal pricing and resource allocation in Computational Grid Network. We consider a general network economy model with Grid Resource Providers, Grid Resource Brokers and Grid Users. The proposed framework allows for the modeling and theoretical analysis of Computational Grid Markets that considers a non-cooperative behavior of decision-makers in the same tier of the grid computing network (such as, for example, Grid Resource Providers) as well as cooperative behavior between tiers (between Resource Providers and Grid Brokers). We introduce risk management into the decision making process by analyzing the decision-markeru0027s reliability and quality of service (QoS) requirement. We analyze resource allocation patterns as well as equilibrium price based on demand, supply, and cost structure of the grid computing market network. We specifically answer the following questions with several numerical examples: How do system reliability levels affect the QoS levels of the service providers and brokers under competition? How do system reliability levels affect the profits of resource providers and brokers in a competitive market? How do system reliability levels influence the pricing of the services in a competitive environment? How do usersu0027 service request types, QoS requirements, and timing concerns affect usersu0027 behaviors, costs and risks in equilibrium? How does the market mechanism allocate resources to satisfy the demands of users? We find that for users who request same services certain timing flexibility can not only reduce the costs but also lower the risks. The results indicated that the value of QoS can be efficiently priced based on the heterogeneous service demands.	computation;entropy maximization;explicit modeling;foremost;grid computing;interaction;mathematical optimization;quality of service;risk management	José Miguel Cruz;Zugang Liu	2012	Decision Support Systems	10.1016/j.dss.2011.10.012	mobile qos;quality of service;decision support system;risk management;computer science;marketing;management;grid computing;commerce	Vision	-0.9471919581286424	-4.7635503655266	7503
04368199febeee87ba3a84bf303d64e4554759b1	a study on the global convergence time complexity of estimation of distribution algorithms	modelizacion;algoritmo busqueda;time complexity;algorithme recherche;search algorithm;search method;global convergence;probabilistic approach;artificial intelligent;upper bound;modelisation;probabilistic model;complexite temps;estimation of distribution algorithm;enfoque probabilista;approche probabiliste;complejidad tiempo;borne superieure;modeling;cota superior;evolutionary computing	The Estimation of Distribution Algorithm is a new class of population based search methods in that a probabilistic model of individuals are estimated based on the high quality individuals and used to generate the new individuals. In this paper we compute 1) some upper bounds on the number of iterations required for global convergence of EDA 2) the exact number of iterations needed for EDA to converge to global optima.	computation;converge;display resolution;estimation of distribution algorithm;global optimization;iteration;local convergence;statistical model;time complexity	Reza Rastegar;Mohammad Reza Meybodi	2005		10.1007/11548669_46	time complexity;statistical model;mathematical optimization;systems modeling;estimation of distribution algorithm;computer science;artificial intelligence;mathematics;upper and lower bounds;algorithm;evolutionary computation;search algorithm	EDA	27.386081146758055	2.2220694820627873	7507
a8c1761ec7c6a5619398929000f51fbbdf4e8751	an adaptive algorithm for constrained optimization problems	optimisation sous contrainte;constrained optimization;non linear programming;algorithm performance;algoritmo adaptativo;programacion no lineal;programmation non lineaire;optimizacion con restriccion;adaptive algorithm;algorithme adaptatif;funcion penalidad;resultado algoritmo;performance algorithme;evolution strategy;algorithme evolutionniste;algoritmo evolucionista;evolutionary algorithm;fonction penalite;constrained optimization problem;penalty function	A b s t r a c t . Adaptivity has become a key issue in Evolutionary Algorithms, since early works in Evolution Strategies. The idea of letting the algorithm adjust its own parameters for free is indeed appealing. This paper proposes to use adaptive mechanisms at the population level for constrained optimization problems in three important steps of the evolutionary algorithm: First, an adaptive penalty function takes care of the penalty coefficients according to the proportion of feasible individuals in the current population; Second, a Seduction/Selection strategy is used to mate feasible individuals with infeasible ones and thus explore the region around the boundary of the feasible domain; Last, selection is tuned to favor a given number of feasible individuals. A detailed discussion of the behavior of the algorithm on two small constrained problems enlights adaptivity at work. Finally, experimental results on eleven test cases from the literature demonstrate the power of this approach.	adaptive algorithm;care-of address;coefficient;constrained optimization;evolution strategy;evolutionary algorithm;mathematical optimization;penalty method;test case	Sana Ben Hamida;Marc Schoenauer	2000		10.1007/3-540-45356-3_52	mathematical optimization;constrained optimization;computer science;artificial intelligence;evolutionary algorithm;penalty method;mathematics;evolution strategy;algorithm	ML	27.644710280471244	-0.09955408785777073	7509
0c8cd095e42d995b38905320565e17d6afadf13f	discriminant kernel and regularization parameter learning via semidefinite programming	perforation;linear discriminate analysis;feature space;computational complexity;approximation scheme;learning problems;convex set;semidefinite program	Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.	approximation;benchmark (computing);computational complexity theory;convex set;feature vector;kernel (operating system);kernel method;linear discriminant analysis;matrix regularization;semidefinite programming	Jieping Ye;Jianhui Chen;Shuiwang Ji	2007		10.1145/1273496.1273634	principal component regression;kernel method;mathematical optimization;combinatorics;kernel embedding of distributions;feature vector;radial basis function kernel;kernel principal component analysis;computer science;machine learning;graph kernel;mathematics;convex set;tree kernel;computational complexity theory;variable kernel density estimation;polynomial kernel;kernel smoother	ML	23.759655904240987	-40.23226004408272	7510
8395172c623be4c7dd405875731eadc2ef1dd4c6	finding optimal demand paging algorithms	time dependent;formal model;dynamic program;optimal policy;policy iteration;stochastic model	A cost is defined for demand paging algorithms with respect to a formal stochastic model of program behavior. This cost is shown to exist under rather general assumptions, and a computational procedure is given which makes it possible to determine the optimal cost and optimal policy for moderate size programs, when the formal model is known and not time dependent. In this latter case it is shown that these computational procedures may be extended to larger programs to obtain arbitrarily close approximations to their optimal policies. In previous models either unwarranted information is assumed beyond the formal model, or the complete stochastic nature of the model is not taken into account.	algorithm;approximation;computation;formal language;mathematical model;paging	Giorgio P. Ingargiola;James F. Korsh	1974	J. ACM	10.1145/321796.321801	mathematical optimization;computer science;stochastic modelling;management science	Theory	3.454836781679619	-1.500819107332178	7514
cf7114a083dda01f56a5af5ffe42deed591823c2	numerical comparison of some penalty-based constraint handling techniques in genetic algorithms	constrained optimization;constraint handling;genetic algorithm;genetic algorithms;global optimization;penalty functions;penalty function	We study five penalty function-based constraint handling techniques to be used with genetic algorithms in global optimization. Three of them, the method of superiority of feasible points, the method of parameter free penalties and the method of adaptive penalties have already been considered in the literature. In addition, we introduce two new modifications of these methods. We compare all the five methods numerically in 33 test problems and report and analyze the results obtained in terms of accuracy, efficiency and reliability. The method of adaptive penalties turned out to be most efficient while the method of parameter free penalties was the most reliable.	genetic algorithm;global optimization;mathematical optimization;numerical analysis;penalty method	Kaisa Miettinen;Marko M. Mäkelä;Jari Toivanen	2003	J. Global Optimization	10.1023/A:1026065325419	mathematical optimization;constrained optimization;genetic algorithm;computer science;machine learning;penalty method;mathematics;algorithm;global optimization	SE	28.840304648558547	-1.1641064836317838	7520
6590a9578fea237f30af514b46c82e3911b3887f	continuity of fuzzified functions using the generalized extension principle		To fuzzify the crisp functions, the extension principle has been widely used for performing this fuzzification. The purpose of this paper is to investigate the continuity of fuzzified function using the more generalized extension principle. The Hausdorff metric will be invoked to study the continuity of fuzzified function. We also apply the principle of continuity of fuzzified function to the fuzzy topological vector space.	fuzzy set;hausdorff dimension;scott continuity	Hsien-Chung Wu	2017	Symmetry	10.3390/sym9120299	mathematical analysis;normed vector space;fuzzy logic;mathematics;fuzzy set;topological vector space;hausdorff distance	AI	-0.36397604136620815	-22.47188846382407	7527
acef19aee7ade3f55b1e0f7386947553e8d69113	joint gaussian based measures for multiple-instance learning	density measurement;clustering algorithms;computer science;extraterrestrial measurements;meteorology;benchmark testing	As an actively investigated topic in machine learning, Multiple-Instance Learning (MIL) has many proposed solutions, including both supervised and unsupervised methods. Most of these solutions are restricted to the original assumption that comes with the notion of MIL: the label of a multipleinstance object is directly determined by the labels of its instances. However, this assumption faces adverse circumstances when there is no clear relation between the over-all label and the labels of instances. Most previous approaches avoid this problem in practice by taking each multiple-instance object as a whole instead of starting with learning in instance spaces, but they either lose information or are time consuming. In this paper, we introduce two joint Gaussian based measures for MIL, Joint Gaussian Similarity (JGS) and Joint Gaussian Distance (JGD), which require no prior knowledge of relations between the labels of multiple-instance objects and their instances. JGS is a measure of similarity while JGD is a metric of which the properties are necessary for many techniques like clustering and embedding. JGS and JGD take all the information into account and many traditional machine learning methods can be introduced to MIL. Extensive experimental evaluations on various real-world data demonstrate the effectiveness of both measures, and better performances than state-of-the-art MIL algorithms on benchmark tasks.	algorithm;benchmark (computing);cluster analysis;instance (computer science);machine learning;multiple-instance learning;performance	Linfei Zhou;Claudia Plant;Christian Böhm	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.75	benchmark;computer science;artificial intelligence;machine learning;data mining;database;cluster analysis;statistics	DB	21.10981752054599	-42.871044807626696	7530
f8dd1adaf0ecab0ac6cb6250e48d150ebdb10d1f	network games with and without synchroneity	network game;nash equilibrium;network security problem;new network game;new game;asynchronous game;strategic game;infinite game;simultaneous game;finite game	Let G = (V , E) be a graph with no isolated vertices. Mavronicolas et al. (2005) presented a network game:-attackers: aim to damage by attacking a vertex.-defender: aim to protect the network by select an edge. (i.e., attacker and defenders have conflicting objectives). Subsequently, MedSalem et al. (2007) genelized the model so that they have many defenders instead of a single defender. Then we introduced a new network game with the roles of players interchanged (2011). In particular, we proposed a new network game where:-attackers: aim to damage the network by attacking an edge-defenders: aim to protect the network by choosing a vertex. We obtained a graph-theoretic characterization of pure Nash equilibrium. In this paper, we study mixed Nash equilibria for stochastic strategies in this new game. We then generalize our network game to an asynchronous game, where two players repeatedly execute simultaneous games.	graph (discrete mathematics);graph theory;nash equilibrium;vertex (graph theory);video game developer	Ahmad Termimi Ab Ghani;Kazuyuki Tanaka	2011		10.1007/978-3-642-25280-8_9	non-cooperative game;combinatorial game theory;game theory;example of a game without a value;simulation;best response;game tree;extensive-form game;simultaneous game;computer science;information set;repeated game;distributed computing;stochastic game;screening game;chicken;normal-form game;sequential game;computer security;symmetric game;centipede game	AI	-4.165754382463677	1.9646216680329835	7537
07724f6f025568827ce5cc81ce1551f73c07b4fc	general game playing with stochastic csp	stochastic constraint satisfaction problem;game description language;general game playing;stochastic games	The challenge of General Game Playing (GGP) is to devise game playing programs that take as input the rules of any strategic game, described in the Game Description Language (GDL), and that effectively play without human intervention. The aim of this paper is to address the GGP challenge by casting GDL games (potentially with chance events) into the Stochastic Constraint Satisfaction Problem (SCSP). The stochastic constraint network of a game is decomposed into a sequence of µ SCSPs (also know as one-stage SCSP), each associated with a game round. Winning strategies are searched by coupling the MAC (Maintaining Arc Consistency) algorithm, used to solve each µ SCSP in turn, together with the UCB (Upper Confidence Bound) policy for approximating the values of those strategies obtained by the last µ SCSP in the sequence. Extensive experiments conducted on various GDL games with different deliberation times per round, demonstrate that the MAC-UCB algorithm significantly outperforms the state-of-the-art UCT (Upper Confidence bounds for Trees) algorithm.	algorithm;constraint programming;constraint satisfaction problem;experiment;game description language;general game playing;local consistency;multi-armed bandit;yamaha ymf292	Frédéric Koriche;Sylvain Lagrue;Éric Piette;Sébastien Tabary	2015	Constraints	10.1007/s10601-015-9199-5	bondareva–shapley theorem;non-cooperative game;combinatorial game theory;mathematical optimization;example of a game without a value;simulation;game tree;simultaneous game;computer science;artificial intelligence;machine learning;repeated game;mathematics;stochastic game;screening game;normal-form game;simulations and games in economics education;algorithmic game theory;sequential game;symmetric game;algorithm	AI	20.22864843276269	-16.91926268290891	7544
ee822b8ffa5913ecadea01b4fd56c7daa5ffd9a2	a silicon nanodisk array structure realizing synaptic response of spiking neuron models with noise	neurons arrays nanostructures noise logic gates voltage control electrodes;neural chips;single electron circuit simulation silicon nanodisk array structure spiking neuron models neuron operation post synaptic potentials psp generation electron hopping stochastic electron movement neural processing fluctuation controllability	In the implementation of spiking neuron models, which can achieve realistic neuron operation, generation of post-synaptic potentials (PSPs) is an essential function. We have already proposed a new nanodisk array structure for generating PSPs using delay in electron hopping among nanodisks. Generated PSPs have fluctuation caused by stochastic electron movement. Noise or fluctuation is effectively used in neural processing. In this paper, we review our proposed structure and show fluctuation controllability based on single-electron circuit simulation.	action potential;british informatics olympiad;electron;electronic circuit simulation;frequency-hopping spread spectrum;gnu nano;neuron;one-electron universe;quantum fluctuation;self-assembly;software bug;spiking neural network;synaptic package manager;universal conductance fluctuations	Takashi Morie;Haichao Liang;Yilai Sun;Takashi Tohara;Makoto Igarashi;Seiji Samukawa	2014	2014 19th Asia and South Pacific Design Automation Conference (ASP-DAC)	10.1109/ASPDAC.2014.6742887	electronic engineering;electrical engineering	EDA	40.013125005100974	-0.7396997665990832	7553
a1a9a434939faf81b7e0109e728f667ab21838fd	adaptive image representation with segmented orthogonal matching pursuit	orthogonal matching pursuit;time varying;audio signal processing;image segmentation;image matching;natural images;image representation image segmentation matching pursuit algorithms dictionaries signal resolution pursuit algorithms fourier transforms signal processing image processing robustness;adaptive signal processing;icip ieee image processing signal processing;somp adaptive image representation segmented orthogonal matching pursuit adaptive signal expansion natural images audio signals one dimensional signals two dimensional signals sparseness;image representation;signal representation;multidimensional signal processing;multidimensional signal processing image representation image segmentation image matching adaptive signal processing adaptive signal processing signal representation audio signal processing multidimensional signal processing	Recent projects ▸ The Founder and Director of MCI mobile Value Added Services Lab (VASL) ▸ Major Consultant for the National Fuel Smart Card Project ▸ The National Free/Open Source (FOSS) Project ▸ National Network Development Model into Next Generation Networks ▸ The National Intranet and IDC Topology & Standards Project ▸ Scalable & Error Resilience Video Coding in VoD and IPTV Applications ▸ Estimating Depth of Anesthesia Using EEG Signals During Operation ▸ Object Recognition and Tracking in Video Sequences ▸ QoS in Multimedia Wireless Networks (Ad Hoc & Sensor Networks) ▸ Wireless Network Security ▸ Mobile Software & Value Added Services	data center;electroencephalography;iptv;intranet;matching pursuit;network security;next-generation network;outline of object recognition;quality of service;smart card	Hamid R. Rabiee;Rangasami L. Kashyap;S. Rasoul Safavian	1998		10.1109/ICIP.1998.723354	multidimensional signal processing;adaptive filter;computer vision;pyramid;image processing;audio signal processing;computer science;digital signal processing;machine learning;digital image processing;pattern recognition;mathematics;multi-scale approaches;image segmentation;matching pursuit;signal	Mobile	42.02713561696447	-28.333339565312485	7570
18b503b56af22097bf34c75bb5f70f605a23a4d4	genetic and bacterial memetic programming approaches in hierarchical-interpolative fuzzy system construction	interpolation;learning artificial intelligence fuzzy systems genetic algorithms interpolation;complexity theory;memetics;supervised machine learning;machine learning;supervised machine learning hierarchical interpolative fuzzy systems memetic programming;hierarchical interpolative machine learning systems bacterial memetic programming approach genetic programming approach hierarchical interpolative fuzzy system construction supervised machine learning system black box systems input output pairs stochastic evolutionary optimization methods deterministic local search steps pure evolutionary algorithms;hierarchical interpolative fuzzy systems;genetic algorithms;optimization;memetic programming;microorganisms optimization memetics programming complexity theory genetic algorithms machine learning;learning artificial intelligence;programming;microorganisms;fuzzy systems	As a straightforward continuation of our previous work in this paper new memetic (combined evolutionary and gradient based) methods are proposed for constructing hierarchical-interpolative fuzzy rule bases in the frame of a supervised machine learning system modeling black box systems defined by input-output pairs. In this work the resulting hierarchical rule bases are constructed by using structure building Genetic and Bacterial Memetic Programming Algorithms, thus stochastic evolutionary optimization methods containing deterministic local search steps. Applying hierarchical-interpolative fuzzy rule bases has proved an efficient way of reducing the complexity of knowledge bases, whereas memetic techniques often ensure a relatively fast convergence in the learning process. The literature has highlighted the advantages of memetic methods against pure evolutionary algorithms, thus the combination of hierarchical-interpolative fuzzy rule bases with memetic techniques may form promising hierarchical-interpolative machine learning systems.	black box;continuation;evolutionary algorithm;evolutionary computation;evolutionary programming;experiment;fuzzy control system;fuzzy rule;genetic algorithm;gradient;knowledge base;local search (optimization);machine learning;mathematical optimization;memetics;simulation;supervised learning;systems modeling	Krisztián Balázs;László T. Kóczy	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6251218	programming;memetics;mathematical optimization;genetic algorithm;interpolation;computer science;artificial intelligence;machine learning;microorganism;fuzzy control system;memetic algorithm	AI	26.19362623159149	-10.167047477883154	7579
d35b12b16fafce8d9c5bdc7b0b89d280ef572975	traffic modeling and real-time control for metro lines. part ii - the effect of passenger demand on the traffic phases		We present traffic flow and control models for the train dynamics in metro lines. In a first model we introduce the effect of passenger demand on the train dwell times at platforms. We recall that, if this effect is not well controlled, then the traffic is unstable. Then we propose a second traffic control model which deals with this instability, by modifying the control of the train dwell times at platforms. We show that the dynamic system admits a unique stable asymptotic regime, and calculate by numerical simulations the asymptotic average train time-headway as a function of the number of moving trains. By that, we obtain the traffic phases of the train dynamics, giving the effect of the passenger travel demand on the frequency of the metro line, under the proposed control model. Finally, we draw some conclusions, and give a direction for the upcoming research.	control theory;dynamic programming;dynamical system;instability;numerical analysis;real-time transcription;simulation	Nadir Farhi;Cyril Nguyen Van Phu;Habib Haj Salem;Jean-Patrick Lebacque	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7963541	control engineering;computer science;dynamic programming;train;traffic flow;stochastic process;optimal control;instability;real-time control system	Embedded	10.011607990020412	-10.611350039220627	7584
5366d955dcfc1d11b3ebd1a76cdf5509a592f5fb	a survey on multimedia communicating technology based on spatial audio coding	multimedia communication audio coding bit rate auditory system computer science usa councils computer networks testing transform coding application software;audio communication multimedia processing binaural cues spatial audio coding mpeg surround;multimedia communication audio coding;application software;audio communication;auditory system;testing;multimedia processing;usa councils;transform coding;bit rate;computer networks;emerging technology;multimedia communicating technology;audio coding;visualization;binaural cues;multimedia communication;transforms;mpeg surround;binaural cue coding;computer science;cumulant;mpeg surround multimedia communicating technology spatial audio coding binaural cue coding;multimedia services;spatial audio;spatial audio coding	Spatial Audio Coding (SAC) is an emerging technology with a distinguishing feature of delivering good even excellent audio quality at monotonic or stereo bitrate of conventional perceptual transform coders. By a systematic exploitation of spatial hearing, Binaural Cue Coding illustrates the power and potentials of SAC in the future for intelligent multimedia services. MPEG Surround, receiving cumulative efforts from industry and academy, strives to build a SAC system with great versatility and high quality. The initial test results of MPEG Surround show its performance advantage over conventional state-of-the-art coders in a wide range of coding configurations.	academy;binaural beats;display resolution;mpeg surround;moving picture experts group;sound card;surround sound	Naixue N. Xiong;Shuixian Chen;Jing He;Yanxiang He;Athanasios V. Vasilakos;Jong Hyuk Park;Yan Yang	2010	2010 24th IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2010.149	application software;transform coding;speech recognition;visualization;computer science;multimedia;software testing;emerging technologies;statistics;cumulant	Visualization	46.23309030677024	-7.931976790722619	7628
98a1a597cbfb825d3b846f87d29ceb838d9e6048	weighted convolutional neural network ensemble		We introduce a new method to combine the output probabilities of convolutional neural networks which we call Weighted Convolutional Neural Network Ensemble. Each network has an associated weight that makes networks with better performance have a greater influence at the time to classify in relation to networks that performed worse. This new approach produces better results than the common method that combines the networks doing just the average of the output probabilities to make the predictions. We show the validity of our proposal by improving the classification rate on a common image classification benchmark.	artificial neural network;benchmark (computing);computer vision;convolutional neural network;epoch (reference date);experiment;map;overfitting;randomness extractor	Xavier Frazão;Luís A. Alexandre	2014		10.1007/978-3-319-12568-8_82	machine learning;time delay neural network;deep learning;convolutional neural network	ML	15.922253026483626	-44.63555421629433	7631
a09e739bb82a4b7f837eca983ddc40e6e9eb6a98	trading tests of long-term market forecast by text mining	bond markets;banking;text mining;trading test;economic forecasting;japanese government bond;economic indicators data mining feature extraction numerical models biological system modeling indexes;biological system modeling;support vector regression;regression analysis text mining bond market out of sample forecast;jgb market;text analysis;data mining;feature vector;indexes;text analysis banking data mining economic forecasting feature extraction marketing regression analysis;out of sample forecast;marketing;feature extraction;bond market;bank of japan;regression analysis;numerical models;support vector regression trading test long term market forecasting text mining japanese government bond feature extraction bank of japan jgb market regression analysis;long term market forecasting;economic indicators	We propose a new approach for analyzing the Japanese government bond (JGB) market using text-mining technology. First, we extracted the feature vectors of the monthly reports from the Bank of Japan (BOJ). Then, the trends in the JGB market were estimated by a regression analysis using the feature vectors. As a result of comparison with support vector regression and other methods, the proposal method could forecast in higher accuracy about both the level and direction of long-term market trends. Moreover, our method showed high returns with annual rate averages as a result of the implementation test.	external variable;feature vector;level of measurement;numerical analysis;principal component analysis;simulation;support vector machine;text corpus;text mining;traders	Kiyoshi Izumi;Takashi Goto;Tohgoroh Matsui	2010	2010 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2010.60	text mining;bond market;computer science;machine learning;data mining	Robotics	5.939661899364159	-18.716132091055314	7644
7b158758d529175566af4d390686fcb7ff275db1	a recursive scenario decomposition algorithm for combinatorial multistage stochastic optimisation problems			algorithm;mathematical optimization;multistage amplifier;recursion (computer science);stochastic gradient descent;stochastic optimization	David Hemmi;Guido Tack;Mark Wallace	2018			mathematical optimization;machine learning;recursion;artificial intelligence;computer science	AI	33.669110233687526	3.538090442862246	7646
b41a68fbdc62982c5623fb0f89ea24da66d7dd46	boosting distributed constraint satisfaction	heavy tail phenomenon;heavy tail;search;keyword search;distributed search;distributed constraint satisfaction;divide and conquer	Competition and cooperation can boost the performance of a combinatorial search process. Both can be implemented with a portfolio of algorithms which run in parallel, give hints to each other and compete for being the first to finish and deliver the solution. In this paper we present a new generic framework for the application of algorithms for distributed constraint satisfaction that makes use of both cooperation and competition. This framework improves the performance of two different standard algorithms by one order of magnitude. Furthermore, it can reduce the risk of poor performance by up to three orders of magnitude diminishing the heavy-tailed behaviour of complete distributed search. Moreover it greatly reduces the classical idleness flaw usually observed in distributed tree-based searches. We expect our new methods to be similarly beneficial for any tree-based distributed search and describe ways on how to incorporate them. Remarkably, our ideas while applied to a parallel SAT setting were able to beat divide-and-conquers approaches, and win the gold medal of the parallel track of the 2008 SAT-Race.	boosting (machine learning);constraint satisfaction	Youssef Hamadi;Georg Ringwelski	2011	J. Heuristics	10.1007/s10732-010-9134-2	distributed algorithm;mathematical optimization;divide and conquer algorithms;simulation;heavy-tailed distribution;computer science;artificial intelligence;theoretical computer science;mathematics	AI	19.9899114686054	-12.689996665182639	7650
4e02e09ebdc1b490cfc75cc257b94dc4c8f25e29	robust shrinkage m-estimators of large covariance matrices	random matrix theory robust estimation covariance matrices;robust estimation;optimal shrinkage parameter robust shrinkage m estimators covariance matrices robust high dimensional covariance estimators linear shrinkage maronna s classical m estimators random matrix theory shrinkage maronna type data driven method;covariance matrices;covariance matrices robustness signal processing conferences sociology estimation;signal processing covariance matrices estimation theory;random matrix theory	Robust high dimensional covariance estimators are considered, comprising regularized (linear shrinkage) modifications of Maronna's classical M-estimators. Such estimators aim to provide robustness to outliers, while simultaneously giving well-defined solutions under high dimensional scenarios where the number of samples does not exceed the number of variables. By applying tools from random matrix theory, we characterize the asymptotic performance of such estimators when the number of samples and variables grow large together. In particular, our results show that, when outliers are absent, many estimators of the shrinkage-Maronna type share the same asymptotic performance, and for such estimators we present a data-driven method for choosing the asymptotically optimal shrinkage parameter. Although our results assume an outlier-free scenario, simulations suggest that certain estimators perform substantially better than others when subjected to outlier samples.	asymptotically optimal algorithm;robustness (computer science);simulation	Nicolas Auguin;David Morales-Jiménez;Matthew R. McKay;Romain Couillet	2016	2016 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2016.7551720	covariance mapping;estimation of covariance matrices;econometrics;mathematical optimization;law of total covariance;covariance;mathematics;statistics;covariance function	ML	31.05131300077432	-27.391583752882635	7661
ec366ac893c3fcb0712033f7cdadfcb11987b1e5	a swarm-trained k-nearest prototypes adaptive classifier with automatic feature selection for interval data	interval data;swarm optimization;symbolic data analysis;feature selection;prototype learning;weighted distance	Some complex data types are capable of modeling data variability and imprecision. These data types are studied in the symbolic data analysis field. One such data type is interval data, which represents ranges of values and is more versatile than classic point data for many domains. This paper proposes a new prototype-based classifier for interval data, trained by a swarm optimization method. Our work has two main contributions: a swarm method which is capable of performing both automatic selection of features and pruning of unused prototypes and a generalized weighted squared Euclidean distance for interval data. By discarding unnecessary features and prototypes, the proposed algorithm deals with typical limitations of prototype-based methods, such as the problem of prototype initialization. The proposed distance is useful for learning classes in interval datasets with different shapes, sizes and structures. When compared to other prototype-based methods, the proposed method achieves lower error rates in both synthetic and real interval datasets.	adaptive grammar;algorithm;class;euclidean distance;feature selection;hl7 data type;heart rate variability;mathematical optimization;prototype;swarm;symbolic data analysis;synthetic intelligence	Telmo de Menezes e Silva Filho;Renata M. C. R. de Souza;Ricardo B. C. Prudêncio	2016	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2016.04.006	computer science;machine learning;pattern recognition;data mining;symbolic data analysis;feature selection;statistics	DB	8.897719469021162	-42.19721006179185	7668
bc3a7226b6c4174850a8277377c65e6e1da7575b	continuous 3d environment sensing for autonomous robot navigation and mapping		Presented here is a novel approach for continuously sensing dynamic indoor environments in 3D. Based on this procedure virtual 2D maps are introduced that allow for computationally efficient navigation algorithms. Additionally, a methodology is proposed to interpret the gathered information in a way applicable for prevailing 2D and 3D mapping algorithms.	algorithm;algorithmic efficiency;autonomous robot;map;robotic mapping	Dirk Holz;Christopher Lörken	2007			computer vision;robot control;robotic mapping;mobile robot navigation;mobile robot;artificial intelligence;avm navigator;autonomous robot;computer science	Robotics	53.42122870210809	-33.15476019322641	7676
fcee586a35d666fceb2c2bf8b7ce9a5379a26fa2	a subspace-based multinomial logistic regression for hyperspectral image classification	training;vectors geophysical image processing hyperspectral imaging image classification mixing regression analysis;hyperspectral imaging vectors training accuracy logistics;accuracy;logistics;vectors;subspace multinomial logistic regression mlr hyperspectral imaging pixelwise classification;hyperspectral imaging;rosis system subspace based multinomial logistic regression method pixelwise hyperspectral image classification feature vector formation spectral vector energy class indexed subspace linear mixing processing hyperspectral measurement process nasa airborne visible infrared imaging spectrometer aviris reflective optics system imaging spectrographic system	In this letter, we propose a multinomial-logistic-regression method for pixelwise hyperspectral classification. The feature vectors are formed by the energy of the spectral vectors projected on class-indexed subspaces. In this way, we model not only the linear mixing process that is often present in the hyperspectral measurement process but also the nonlinearities that are separable in the feature space defined by the aforementioned feature vectors. Our experimental results have been conducted using both simulated and real hyperspectral data sets, which are collected using NASA's Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) and the Reflective Optics System Imaging Spectrographic (ROSIS) system. These results indicate that the proposed method provides competitive results in comparison with other state-of-the-art approaches.	algorithm;binary prefix;computer vision;feature vector;learning to rank;multinomial logistic regression;nonlinear system;system image	Mahdi Khodadadzadeh;Jun Li;Antonio J. Plaza;Jos&#x00E9; M. Bioucas-Dias	2014	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2014.2320258	full spectral imaging;logistics;computer vision;hyperspectral imaging;pattern recognition;mathematics;accuracy and precision;remote sensing	Vision	30.17185498236327	-44.021105877709815	7691
a2c59dc49523e0e18e80f8143cb5c64e81a64eca	a co-evolutionary approach for optimal bidding strategy of multiple electricity suppliers	generators;evolutionary computation;iso;iterative methods;games;optimization;electricity supply industry	Determining the optimal bidding strategies in a competitive electricity market has become an important research topic over the last few decades. In this paper, a supply function equilibrium game model is considered and formulated as a bilevel optimization problem, where the upper level is used to maximize the individual profit of each supplier and the lower one to minimize the overall operating cost. To solve this problem, a co-evolutionary approach is designed in which each supplier uses its own sub-population of a genetic algorithm to maximize its profit through a bidding strategy based on each of its generators' cost coefficients, while either a self-adaptive differential evolution or sequential quadratic programming is used to optimally allocate the generation of each supplier by minimizing the operation cost. To validate the results obtained from the proposed method, an iterative method is also used to solve the two well-known benchmarks in the literature. The results are compared with those from a state-of-art method in the literature which reveals that the co-evolutionary approach has some merits in terms of quality and reliability.	benchmark (computing);bilevel optimization;coefficient;differential evolution;genetic algorithm;iterative and incremental development;iterative method;mathematical optimization;optimal design;optimization problem;sequential quadratic programming	Forhad Zaman;Saber M. Elsayed;Tapabrata Ray;Ruhul A. Sarker	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7744234	games;mathematical optimization;iso image;computer science;iterative method;evolutionary computation	AI	18.37222397050237	-3.6042408072926504	7692
932b00a9b51854184046ca595c1bdf6b449938d4	clustering high dimensional data using subspace and projected clustering algorithms	cluster algorithm;high dimensionality;data mining;data clustering;qa75 electronic computers computer science;high dimensional data;statistical pattern recognition;subspace clustering;clustered data	Problem statement: Clustering has a number of techniques that have been developed in statistics, pattern recognition, data mining, and other fields. Subspace clustering enumerates clusters of objects in all subspaces of a dataset. It tends to produce many over lapping clusters. Approach: Subspace clustering and projected clustering are research areas for clustering in high dimensional spaces. In this research we experiment three clustering oriented algorithms, PROCLUS, P3C and STATPC. Results: In general, PROCLUS performs better in terms of time of calculation and produced the least number of un-clustered data while STATPC outperforms PROCLUS and P3C in the accuracy of both cluster points and relevant attributes found. Conclusions/Recommendations: In this study, we analyze in detail the properties of different data clustering method.	algorithm;cluster analysis;clustering high-dimensional data;computer cluster;data mining;pattern recognition;recommender system	Rahmat Widia Sembiring;Jasni Mohamad Zain;Abdullah Embong	2010	CoRR	10.5121/ijcsit.2010.2414	correlation clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	0.8343674494430106	-41.73232906787297	7699
4d33db0dd6cfb5395dcb898b0adeda3bcea1794c	beyond knowledge tracing: modeling skill topologies with bayesian networks	constrained optimization;parameter learning;knowledge tracing;prediction;bayesian networks	Modeling and predicting student knowledge is a fundamental task of an intelligent tutoring system. A popular approach for student modeling is Bayesian Knowledge Tracing (BKT). BKT models, however, lack the ability to describe the hierarchy and relationships between the different skills of a learning domain. In this work, we therefore aim at increasing the representational power of the student model by employing dynamic Bayesian networks that are able to represent such skill topologies. To ensure model interpretability, we constrain the parameter space. We evaluate the performance of our models on five large-scale data sets of different learning domains such as mathematics, spelling learning and physics, and demonstrate that our approach outperforms BKT in prediction accuracy on unseen data across all learning domains.	dynamic bayesian network;influence diagram;network topology	Tanja Käser;Severin Klingler;Alexander G. Schwing;Markus H. Gross	2014		10.1007/978-3-319-07221-0_23	constrained optimization;prediction;computer science;data science;machine learning;pattern recognition;bayesian network;statistics	AI	16.525542662955832	-34.25931418118863	7724
9afc0d6237460c002cc637291cd6ed6d952aa5d9	uncertainty and the acquisition of capacity: a competitive analysis	experimental design;capacite production;game theory;incertidumbre;uncertainty;dynamic game;plan experiencia;teoria juego;theorie jeu;concurrence economique;concurrencia economica;estrategia empresa;plan experience;competition economy;acquisition;competitive analysis;jeu dynamique;capacidad produccion;juego dinamico;incertitude;firm strategy;strategie entreprise;adquisicion;production capacity	Abstract   Motivated by experience with the telecommunications industry, analysis of a dynamic game approach to price competition is undertaken. In the underlying model, two firms competing for demand determine the amount of new capacity to be acquired which has the effect of reducing the unit cost of production and thereby the price charged for output. Using a factorial experimental design approach, policy oriented results are obtained which explore the effects of several factors in the exogenous environment (level of exogenous demand, purchase cost of capacity, and interest rate) on a firm's optimal behavior and competitive position. In particular, the experimental designs facilitate analysis of the impact of synergistic forces in the external environment. Lastly, the effect of different levels and forms of uncertainty associated with exogenous demand are explored.	competitive analysis (online algorithm)	Cheryl Gaimon;Johnny C. Ho	1994	Computers & OR	10.1016/0305-0548(94)90038-8	competitive analysis;game theory;uncertainty;productive capacity;sequential game;design of experiments;statistics	NLP	3.855503053061536	-5.539220056351223	7727
56708f2791be26a242782d1680b21e0e5b9c08cb	electric load signature analysis for home energy monitoring system	multi feature multi algorithm decision method;home energy monitoring system;non conventional features;load signature	This paper focuses on identifying which appliance is currently operating by analyzing electrical load signature for home energy monitoring system. The identification framework is comprised of three steps. Firstly, specific appliance features, or signatures, were chosen, which are DC (Duty Cycle), SO (Slope of On-state), VO (Variance of On-state), and ZC (Zero Crossing) by reviewing observations of appliances from 13 houses for 3 days. Five appliances of electrical rice cooker, kimchi-refrigerator, PC, refrigerator, and TV were chosen for the identification with high penetration rate and total operation-time in Korea. Secondly, K-NN and Naive Bayesian classifiers, which are commonly used in many applications, are employed to estimate from which appliance the signatures are obtained. Lastly, one of candidates is selected as final identification result by majority voting. The proposed identification frame showed identification success rate of 94.23%.	antivirus software;bayesian network;duty cycle;electrical load;electronic signature;k-nearest neighbors algorithm;kimchi;naive bayes classifier;personal computer;zero crossing	Lu-Lulu;Sung Wook Park;Bo-Hyeun Wang	2012	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2012.12.3.193	embedded system;real-time computing;simulation;engineering	AI	10.01404997607029	-16.094934667115286	7729
dcaf34ef61a5d426009374e3dcc4f9ed2b22ca99	proprties of biclustering algorithms and a novel biclustering technique based on relative density		Biclustering is found to be useful in areas like data mining and bioinformatics. The term biclustering involves searching subsets of observations and features forming coherent structure. This can be interpreted in different ways like spatial closeness, relation between features for selected observations etc. This paper discusses different properties, objectives and approaches of biclustering algorithms. We also present an algorithm which detects feature relation based biclusters using density based techniques. Here we use relative density of regions to identify biclusters embedded in the data. Properties of this algorithm are discussed and demonstrated using artificial datasets. Proposed method is seen to give better results on these datasets using paired right tailed t test. Usefulness of proposed method is also demonstrated using real life datasets.	algorithm;biclustering;bioinformatics;centrality;coherence (physics);data mining;embedded system;real life	Namita Jain;Susmita Ghosh;Ashish Ghosh;C. A. Murthy	2018	CoRR			ML	1.0826264131761862	-42.627398888917654	7738
2cb743bddc572345fa79724475004ceb684cee66	optimal pipe inspection paths considering inspection tool limitations		Abstract The inspection of deteriorating water distribution pipes is an important process for utilities. It helps them gain a better understanding of the condition of their buried conveyance systems and aids better decision making for risk-based asset management. In-pipe continuous inspection tools provide high resolution and accurate data, but they have seen relatively limited use due to cost and operational constraints. To facilitate-cost efficient deployment of these technologies and maximal information gain, a process that finds high risk pipes to inspect while accounting for the limitations of the tools at hand is needed. This paper shows how to incorporate these considerations within an optimization formulation, and examines the use of Evolutionary Programming, Simulated Annealing, and Greedy Search heuristics to identify inspection paths. Case studies performed on both synthetic and real world networks demonstrate that Evolutionary Programs are the most effective. While only three factors are used to characterize tool limitations, the method presented in this paper can be extended to include technology-specific complexities in real world applications.		T Y Chen;Seth D. Guikema;Craig Michael Daly	2019	Rel. Eng. & Sys. Safety	10.1016/j.ress.2018.09.019	software deployment;evolutionary programming;reliability engineering;engineering;simulated annealing;asset management;heuristics;greedy algorithm	SE	11.89684551542787	-4.8805666458776376	7742
0b58b902f50bc0859eae5b524e28ed68f5ef7039	an agent based approach to patient scheduling using experience based learning	agent based;experience based learning;multi agent systems;integer programming;patient scheduling;experience base;agent based approaches	This paper describes an agent based approach to patient scheduling using experience based learning and an integer programming model. The evaluation on different learning techniques shows that the experience based learning (EBL) provides a better solution. The time required to process a particular job is reduced as the experience processed by it increases. The processing time can be calculated with the help of EBL. The main objective of this patient scheduling system is to reduce the waiting time of patient in hospitals and to complete their treatment in minimum required time. The proposed framework is implemented in JADE. In this approach the patients are represented as patient agent (PA) and resources as resource agent (RA). This mathematical model provides an optimal solution. The comparisons of the proposed framework with other scheduling rules shows that an agent based approach to patient scheduling using EBL gives better results. learning technique would be a good choice (Kanaga, Valarmathi, & Murali, in press). Here we consider each patient and resources as agents and they interact with each other (Janiak & Rudek, 2005). The agent based approach considers the patients as Patient Agents (PA) and resources as Resource Agents (RA). The PA requests for the resource. A special agent named Common Agent (CA) is also introduced in this framework. CA refers to a general physician who decides on what tasks the patient has to undergo. The proposed framework is trying to reduce the DOI: 10.4018/jats.2010100101 2 International Journal of Agent Technologies and Systems, 2(4), 1-9, October-December 2010 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. patients waiting time and tardiness. We can further reduce this by incorporating experience based learning. The learning models in scheduling are based on the learning curve introduced by Wright (Janiak, Janiak, Rudek, & Wielgus, 2009). In scheduling problems with a new experience-based learning model, job processing times are described by “S”-shaped functions that are dependent on the experience of the processor. In patient scheduling, decisions are made according to the learning model (Becker, Navarro, Krempels & Panchenko, 2003).	agent-based model;donald becker;explanation-based learning;integer programming;jade;mathematical model;programming model;quasicircle;scheduling (computing);software agent;traffic collision avoidance system	E. Grace Mary Kanaga;M. L. Valarmathi;Juliet A. Murali	2010	IJATS	10.4018/jats.2010100101	fair-share scheduling;real-time computing;simulation;integer programming;dynamic priority scheduling;computer science;artificial intelligence;multi-agent system	AI	13.060631270454197	-1.401659425330019	7747
684970db2d785753498c8367a2516d348b79d166	detection for mobile robot navigation based on multisensor fusion	vision system;theory of evidence;ccd camera;sensors;mobile robot;mobile robots;laser range finders;robotics;autonomous mobile robot;laser range finder;mobile robot navigation;sensor fusion;ccd cameras;vision;cameras	ABSTRACTThe work presented in this paper is a part of the research project on multisensor integrated vision system andsensor fusion algorithm for the navigation of an autonomous mobile robot equipped with a laser range finder (LRF)and a color visual CCD camera. To combine the complementary information from LRF and color camera, a fusionalgorithm is designed based on the Dempster-Shafer's theory ofevidence (DSTE). As we known, DSTE can only beused to the independent evidences, but our fusion algorithm can deal with dependent ones, which in line with thedependent character of mobile robotics sensor information. Finally, the system and the algorithm have been tested in real environments, and their effectiveness has been proved.Keywords: mobile robot, environment modeling, obstacle detection, multisensor fusion, generalized Dempster-Shafer's theory of evidence. 1. FTRODUCTION Mobile robots often operate in inherently uncertain environment which makes it very difficult for the singlesensory system to supply complete information about the environment, so in recent years people have seen anincreasing interest in the development of multisensor systems. It was already proved in theory and practice thatmultiseneor integrated systems can resolve single sensor ambiguity, discover and interpret the environment more	mobile robot;robotic mapping	Jingyu Yang;Yong-Ge Wu	1995		10.1117/12.228974	mobile robot;computer vision;simulation;robotics;mobile robot navigation;remote sensing	Robotics	52.58561504388601	-34.672692764206154	7757
a8cedc9a77616d682c4af69d42a5ad5a17064868	head posture detection using skin and hair information	human computer interaction;image segmentation;skin;human presence detection novel head posture detection algorithm hair information skin information human computer interactions pattern training based image segmentation algorithm gaze direction estimation method;head image segmentation hair skin humans human computer interaction training;object detection;skin human computer interaction image segmentation object detection	This paper addresses a novel head posture detection algorithm to recognize human-computer interactions. A pattern training based image segmentation algorithm is used to detect the skin and hair of students. A simple and efficient human presence detection and gaze direction estimation method is then proposed based on the segmentation results. Finally, the proposed algorithm is tested on ten different students for seven different head postures each. The experimental results show that 92% of head posture images are accurately identified.	algorithm;experiment;feature detection (computer vision);feature detection (web development);human–computer interaction;image segmentation;microphone;poor posture;sensor;skin (computing)	Yücel Ugurlu	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		computer vision;speech recognition;computer science;skin;image segmentation	Robotics	46.3493538174049	-43.84449942535645	7758
bbf5e1dea35e6cc7b8674f92a7ef57a469c9c2fc	an optimization model for container transportation network with aco approach	containers transportation evolutionary computation;transportation networks;containerisation;optimisation;probability;ant colony optimization;transportation computational complexity containerisation containers optimisation probability search problems;reference model;search method;computational complexity;transportation;probability accumulation searching method container transportation network system ant colony optimization model international container handling mega containerships seaports intelligent algorithm nonlinear np complete problem;search problems;optimization model;np complete problem;containers	The rapid growth of international container handling in recent years has led to an increased utilization of mega-containerships and the rebuild of container network. In this paper, we focus on the optimization of total cost for regional containers transportation network system of seaports, and propose a new reliable intelligent algorithm, named Ant Colony Optimization (ACO). ACO is to solve the nonlinear NP-complete problems, which is greatly inspired from real ants and their food seeking behavior. The paper constructs an optimization model for certain containers transportation network system and simulates the whole process. The results show the improved ACO Algorithm is of a credible and excellent probability accumulation searching method to reduce the integrated cost of container transportation network obviously. The optimization model that we constructed provides a reference model for containers transportation network layout.	algorithm;ant colony optimization algorithms;mathematical optimization;np-completeness;nonlinear system;reference model;tree accumulation	Boxin Fu;Xiangqun Song;Zijian Guo;Peng Zhang	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4425098	transport;ant colony optimization algorithms;reference model;np-complete;computer science;probability;computational complexity theory;operations research;algorithm	DB	25.98923101985112	-0.7990707779092238	7759
cd50f0c26553df3ce8b1de5e8049dc873b5bb701	pruning training samples using a supervised clustering algorithm	cluster algorithm;patent classification;gaussian zerocrossing function;min max modular network;large scale;nearest neighbor;pattern classification;supervised clustering;support vector machine;training sample pruning;practice pattern	As practical pattern classification tasks are often very-large scale and serious imbalance such as patent classification, using traditional pattern classification techniques in a plain way to deal with these tasks has shown inefficient and ineffective. In this paper, a supervised clustering algorithm based on min-max modular network with Gaussian-zero-crossing function is adopted to prune training samples in order to reduce training time and improve generalization accuracy. The effectiveness of the proposed training sample pruning method is verified on a group of real patent classification tasks by using support vector machines and nearest neighbor algorithm.	cluster analysis;k-nearest neighbors algorithm;maxima and minima;preprocessor;statistical classification;support vector machine;zero crossing	Minzhang Huang;Hai Zhao;Bao-Liang Lu	2010		10.1007/978-3-642-13318-3_32	support vector machine;computer science;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm	Vision	12.943372893111489	-40.93992116680889	7768
28667739ea897eb8d29e3ebc55e79a91a504db5d	probabilistic neural programmed networks for scene generation		In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain varied visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations. We propose PNPNet, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images.	addresses (publication format);autoencoder;encoder device component;entity;generative modelling language;glossary of computer graphics;interaction;mathematical model;physical object;pixel;spatial variability;speech coding;transformers;variational principle	Zhongliang Deng;Jiacheng Chen;Yifang Fu	2018			machine learning;computer science;artificial intelligence;rendering (computer graphics);generative grammar;probabilistic logic;semantics;dynamic network analysis	ML	24.05184216034697	-49.13781365367577	7777
ddfa227b46f816b57fbc5ca8c907715cbf7988c9	selecting orchestrator paths in manufacturing lines: alternatives to scheduling	machine failure;scheduling maintenance engineering manufacturing processes production control production equipment;online equipment change;formal model;schedules job shop scheduling layout service oriented architecture mathematical model distance measurement transportation;job shop scheduling;maintenance engineering;layout;manufacturing lines;service oriented architecture manufacturing lines scheduling service encapsulation machine failure online equipment change formal model modeling methodology;distance measurement;manufacturing processes;production control;scheduling;transportation;production equipment;schedules;mathematical model;modeling methodology;service oriented architecture;service encapsulation	Service encapsulation of processes in manufacturing should facilitate a natural and rapid response to machine failure or replacement and to changes in required product types and quantities. Such events are traditionally addressed by offline modifications in the schedule of the entire line. New techniques, alternative to scheduling, are needed to address unknown input job mix and online equipment changes. Formal models can help in developing this kind of methods. This paper discusses this idea in the light of a specific modeling methodology.	encapsulation (networking);online and offline;scheduling (computing)	Corina Popescu;José L. Martínez Lastra	2008	2008 IEEE International Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2008.4638378	maintenance engineering;layout;job shop scheduling;transport;real-time computing;schedule;computer science;systems engineering;engineering;operating system;service-oriented architecture;mathematical model;scheduling	Robotics	10.366005169332764	4.067696632413464	7794
3ff3057452675d7c3b86b777416739f240353e6c	music clustering with constraints		This paper studies the problem of building clusters of music tracks in a collection of popular music in the presence of constraints. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). We present an approach based on the generalized constraint clustering algorithm by incorporating the constraints for grouping music by “similar” artists. The approach is evaluated on a data set consisting of 53 albums covering 41 popular artists. The “correctness” of the clusters generated is tested using artist similarity provided by All Music Guide.	algorithm;cluster analysis;constraint (mathematics);correctness (computer science);multi-user	Wei Peng;Tao Li;Mitsunori Ogihara	2007			artificial intelligence;machine learning;cluster analysis;popular music;computer science;correctness;brown clustering	Web+IR	-0.7127280407008584	-39.68626172592106	7819
2d2ef13a7f5cf37ac50313a408d81993442a4d5d	custom jpeg quantization for improved iris recognition accuracy	iris recognition;hamming distance	Custom JPEG quantization matrices are proposed to be used in the context of compression within iris recognition. Superior matching results in terms of average Hamming distance and improved ROC is found as compared to the use of the default quantization table especially for low FAR. This leads to improved user convenience in case high security is required.	algorithm;hamming distance;interference (communication);iris recognition;jpeg;mathematical optimization;peak signal-to-noise ratio;quantization (image processing);quantization (signal processing)	Gerald Stefan Kostmajer;Herbert Stögner;Andreas Uhl	2009		10.1007/978-3-642-01244-0_7	computer vision;hamming distance;speech recognition;computer science;pattern recognition;iris recognition;quantization	Vision	41.568854849038814	-12.961371463191947	7823
0c0935f28c06064946e4999c8ccff7893b89ac20	optimization of anfis using mine blast algorithm for predicting strength of malaysian small medium enterprises	sme;anfis;rule optimization;mine blast algorithm mba;sme anfis optimization mine blast algorithm malaysian small medium enterprises adaptive neuro fuzzy inference system metaheuristic algorithm fuzzy rule base grid partitioning anfis rule base;neuro fuzzy;sme anfis neuro fuzzy fuzzy system mine blast algorithm mba rule optimization;small to medium enterprises inference mechanisms optimisation;fuzzy system;optimization training computer architecture predictive models adaptation models firing complexity theory	Adaptive Neuro-Fuzzy Inference System (ANFIS) has been popular among other fuzzy inference systems. It has been widely applied in the field of business and economics. Many have trained ANFIS parameters using metaheuristic algorithms but very few have tried optimizing its fuzzy rule-base. The auto-generated rules, using grid partitioning, comprise of both the potential and weak rules. This increases the complexity of ANFIS architecture as well as the cost of computation. Therefore, pruning less or non-contributing rules would serve as optimizing ANFIS rule-base. However, reducing complexity and increasing accuracy of ANFIS network needs effective training and optimization mechanism. This paper proposes an efficient technique for optimizing ANFIS rule-base without compromising on accuracy. The proposed technique uses a newly developed optimization algorithm called Mine Blast Algorithm (MBA) for the first time for ANFIS learning. The ANFIS optimized by MBA is employed to model strength prediction for Malaysian small medium enterprises (SMEs). The results prove that MBA optimized ANFIS rule-base and trained its parameters more efficiently than Genetic Algorithm (GA) and Particle Swarm Optimization (PSO).	adaptive neuro fuzzy inference system;computation;fuzzy logic;fuzzy rule;genetic algorithm;inference engine;mathematical optimization;metaheuristic;neuro-fuzzy;optimization mechanism;particle swarm optimization;rule-based system;space partitioning	Kashif Hussain;Mohd. Najib Mohd. Salleh;Abdul Mutalib Leman	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7381926	simulation;adaptive neuro fuzzy inference system;computer science;artificial intelligence;neuro-fuzzy;machine learning;fuzzy control system	Robotics	6.864341988929735	-26.15747754817627	7838
1c1deead1a331ce6ae33c4e36490dda0f32e21e7	analysis of the bullwhip effect based on different information sharing models	inventory management;supply chains customer services inventory management;customer services;demand bullwhip effect information sharing order inventory;supply chains;information sharing;bullwhip effect;inventory information bullwhip effect information sharing models inventory cost customer service level reduction demand variability amplification information distortion order information demand information;supply chains safety companies fluctuations games	The bullwhip effect brings higher inventory cost and customer service level reduction, which is an amplification of the demand variability because of information distortion in the transmission process. In this paper, for reducing the effect, we analyze comparatively how three information sharing models affect the bullwhip effect, such as order information, demand information, inventory information. And, we also analyze related parameters and present some measures to reduce bullwhip effect.	distortion;spatial variability	Yao Xiao;Gang Zhao;Chunhua Yin	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023211	bullwhip effect;supply chain	HPC	1.0750726036947997	-6.622544561479178	7841
53b787c284085c774c564a0e7c88f79927ebd143	sparse iterative closest point	i 3 5 computer graphics computational geometry and object modeling geometric algorithms;i;5 computer graphics computational geometry and object modelinggeometric algorithms;languages;and systems;3	Rigid registration of two geometric data sets is essential in many applications, including robot navigation, surface reconstruction, and shape matching. Most commonly, variants of the Iterative Closest Point (ICP) algorithm are employed for this task. These methods alternate between closest point computations to establish correspondences between two data sets, and solving for the optimal transformation that brings these correspondences into alignment. A major difficulty for this approach is the sensitivity to outliers and missing data often observed in 3D scans. Most practical implementations of the ICP algorithm address this issue with a number of heuristics to prune or reweight correspondences. However, these heuristics can be unreliable and difficult to tune, which often requires substantial manual assistance. We propose a new formulation of the ICP algorithm that avoids these difficulties by formulating the registration optimization using sparsity inducing norms. Our new algorithm retains the simple structure of the ICP algorithm, while achieving superior registration results when dealing with outliers and incomplete data. The complete source code of our implementation is provided at http://lgg.epfl.ch/sparseicp.	assistive technology;computation;genetic algorithm;heuristic (computer science);iterative closest point;iterative method;mathematical optimization;missing data;robotic mapping;sparse matrix	Sofien Bouaziz;Andrea Tagliasacchi;Mark Pauly	2013	Comput. Graph. Forum	10.1111/cgf.12178	computer vision;mathematical optimization;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;geometry;language;algorithm;iterative closest point	Vision	50.603862169559065	-50.874477364809266	7843
51b0a165fa9d67995a631740a449bb5e16e4d814	a martingale framework for concept change detection in time-varying data streams	change detection;data stream;time varying data	"""In a data streaming setting, data points are observed one by one. The concepts to be learned from the data points may change infinitely often as the data is streaming. In this paper, we extend the idea of testing exchangeability online (Vovk et al., 2003) to a martingale framework to detect concept changes in time-varying data streams. Two martingale tests are developed to detect concept changes using: (i) martingale values, a direct consequence of the Doob's Maximal Inequality, and (ii) the martingale difference, justified using the Hoeffding-Azuma Inequality. Under some assumptions, the second test theoretically has a lower probability than the first test of rejecting the null hypothesis, """"no concept change in the data stream"""", when it is in fact correct. Experiments show that both martingale tests are effective in detecting concept changes in time-varying data streams simulated using two synthetic data sets and three benchmark data sets."""	benchmark (computing);characterization test;data point;experiment;maximal set;numerical analysis;sensor;social inequality;streaming media;synthetic data;with high probability	Shen-Shyang Ho	2005		10.1145/1102351.1102392	econometrics;doob's martingale inequality;martingale;data mining;mathematics;martingale difference sequence;change detection;statistics	ML	31.021192639379503	-13.703653314994899	7853
44475dbe8905773b299dfa56c65e2a39be88b77a	tool flank wear prediction in cnc turning of 7075 al alloy sic composite	prediction error;cnc turning;neuro fuzzy inference system;tungsten carbide;co active neuro fuzzy inference system;flank wear;product quality;artificial neural network	Flank wear occurs on the relief face of the tool and the life of a tool used in a machining process depends upon the amount of flank wear; so predicting of flank wear is an important requirement for higher productivity and product quality. In the present work, the effects of feed, depth of cut and cutting speed on flank wear of tungsten carbide and polycrystalline diamond (PCD) inserts in CNC turning of 7075 AL alloy with 10 wt% SiC composite are studied; also artificial neural network (ANN) and co-active neuro fuzzy inference system (CANFIS) are used to predict the flank wear of tungsten carbide and PCD inserts. The feed, depth of cut and cutting speed are selected as the input variables and artificial neural network and co-active neuro fuzzy inference system model are designed with two output variables. The comparison between the results of the presented models shows that the artificial neural network with the average relative prediction error of 1.03% for flank wear values of tungsten carbide inserts and 1.7% for flank wear values of PCD inserts is more accurate and can be utilized effectively for the prediction of flank wear in CNC turning of 7075 AL alloy SiC composite. It is also found that the tungsten carbide insert flank wear can be predicted with less error than PCD flank wear insert using ANN. With Regard to the effect of the cutting parameters on the flank wear, it is found that the increase of the feed, depth of cut and cutting speed increases the flank wear. Also the feed and depth of cut are the most effective parameters on the flank wear and the cutting speed has lesser effect.	wear leveling	Saeed Zare Chavoshi	2011	Production Engineering	10.1007/s11740-010-0282-x	computer science;engineering;mean squared prediction error;forensic engineering;engineering drawing;metallurgy;artificial neural network	NLP	13.332536976668985	-19.07064860787151	7855
1cfa0113fe9a659a6ec8f5ed40a710ea2997b348	qualitative and quantitative car tracking from a range image sequence	autonomous driving tasks quantitative car tracking qualitative car tracking range image sequence motion estimates motion models constant velocity constant acceleration turning extended kalman filters extended interacting multiple model multiple motion representations;kalman filters;tracking kalman filters image sequences;range image;image sequences motion estimation motion detection motion analysis real time systems vehicles target tracking acceleration robustness path planning;tracking;image sequences	In this paper, we present a car tracking system which provides quantitative and qualitative motion estimates of the tracked car simultaneously from a moving observer. First, we construct three motion models (constant velocity, constant acceleration, and turning) to describe the qualitative motion of a moving car. Then the models are incorporated into the Extended Kalman Filters to perform quantitative tracking. Finally, we develop an Extended Interacting Multiple Model (EIMM) algorithm to manage the switching between models and to output both qualitative and quantitative motion estimates of the tracked car. Accurate motion modeling and e cient model management result in a high performance tracking system. The experimental results on simulated and real data demonstrate that our tracking system is reliable and robust, and runs in real-time. The multiple motion representations make the system useful in various autonomous driving tasks.	algorithm;autonomous car;extended kalman filter;radar tracker;range imaging;real-time computing;real-time transcription;tracking system;velocity (software development)	Liang Zhao;Charles E. Thorpe	1998		10.1109/CVPR.1998.698651	kalman filter;computer vision;simulation;tracking system;quarter-pixel motion;computer science;motion estimation;control theory;mathematics;tracking;motion field	Robotics	53.601763177796045	-39.06858736010775	7858
10b368285e35191150e9726c6168136c2f0efeca	discovering several robot behaviors through speciation	behavior based robotics;topological space;distance measure;control system;evolutionary robotics;levenshtein distance;neural network;evolutionary computing	This contribution studies speciation from the standpoint of evolutionary robotics (ER). A common approach to ER is to design a robot’s control system using neuro-evolution during training. An extension to this methodology is presented here, where speciation is incorporated to the evolution process in order to obtain a varied set of solutions for a robotics problem using a single algorithmic run. Although speciation is common in evolutionary computation, it has been less explored in behavior-based robotics. When employed, speciation usually relies on a distance measure that allows different individuals to be compared. The distance measure is normally computed in objective or phenotypic space. However, the speciation process presented here is intended to produce several distinct robot behaviors; hence, speciation is sought in behavioral space. Thence, individual neurocontrollers are described using behavior signatures, which represent the traversed path of the robot within the training environment and are encoded using a character string. With this representation, behavior signatures are compared using the normalized Levenshtein distance metric (N-GLD). Results indicate that speciation in behavioral space does indeed allow the ER system to obtain several navigation strategies for a common experimental setup. This is illustrated by comparing the best individual from each species with those obtained using the Neuro-Evolution of Augmenting Topologies (NEAT) method which speciates neural networks in topological space.	antivirus software;artificial neural network;behavior-based robotics;control system;edit distance;electronic signature;erdős–rényi model;evolutionary computation;evolutionary robotics;levenshtein distance;neat chipset;neuroevolution of augmenting topologies;robot;similarity measure;string (computer science);type signature	Leonardo Trujillo;Gustavo Olague;Evelyne Lutton;Francisco Fernández de Vega	2008		10.1007/978-3-540-78761-7_17	simulation;artificial intelligence;machine learning;mathematics	Robotics	25.15169010240034	-12.847485372767306	7862
2c43d89f8bd6482624727004a3f541076c5eb060	matrix completion with e-algorithm		We show in this paper how the convergence of an algorithm for matrix completion can be significantly improved by applying Wynn’s e-algorithm. Straightforward generalization of the scalar e-algorithm to matrices fails. However, accelerating the convergence of only the missing matrix elements turns out to be very successful.		Walter Gander;Qiquan Shi	2018	Numerical Algorithms	10.1007/s11075-018-0579-y		ML	24.57593154059721	-34.302351496194134	7888
90cea5755fee727328824685fbcecbdae523a894	active aode learning based on a novel sampling strategy and its application	averaged one dependence estimator;active learning;farthest first sampling;bayesian;remote sensing;uncertainty sampling;classification accuracy;aode	In this paper, we first present active Averaged One-Dependence Estimator AODE learning classification model, which can improve the performance of AODE by selecting and asking experts to label the samples only with maximum information. Several common sampling strategies for active learning are discussed. Unfortunately, these methods can get the outlier, which will lead to scale up the classification-reduced error and high complexity. Motivated by those analyses, we propose a new active learning strategy, which is based on the uncertainty sampling and classification accuracy loss sampling strategy. Experimental results on three UCI standard data sets and a real remote sensing data set show that the AODE classification model and our novel active learning strategy can get better classification accuracy with fewer labelled samples than that of the state-of-the-art approaches for active learning.	averaged one-dependence estimators;sampling (signal processing)	Jia Wu;Zhihua Cai;Xiaolin Chen;Shuang Ao	2013	IJCAT	10.1504/IJCAT.2013.055325	bayesian probability;computer science;machine learning;pattern recognition;data mining;active learning;statistics	ML	15.827331419668946	-39.97236504102431	7925
9fd30084ab51d6a973718f8365ac01091c975853	objective fitness correlation	objective fitness;ofc;evaluation method;coevolution;objective fitness correlation;subjective fitness;solution concept	This paper introduces the Objective Fitness Correlation, a new tool to analyze the evaluation accuracy of coevolutionary algorithms. Accurate evaluation is an essential ingredient in creating adequate coevolutionary dynamics. Based on the notion of a solution concept, a new definition for objective fitness in coevolution is provided. The correlation between the objective fitness and the subjective fitness used in a coevolutionary algorithm yields the Objective Fitness Correlation. The OFC measure is applied to three coevolutionary evaluation methods. It is found that the Objective Fitness Correlation varies substantially over time. Moreover, a high OFC is found to correspond to periods where the algorithm is able to increase the objective quality of individuals. This is evidence of the utility of OFC as a measure to evaluate and compare coevolutionary evaluation mechanisms. The Objective Fitness Correlation (OFC) provides a precise analytical tool to measure the accuracy of evaluation in coevolutionary algorithms.	algorithm;fitness function	Edwin D. de Jong	2007		10.1145/1276958.1277055	simulation;coevolution;artificial intelligence;machine learning;fitness approximation;fitness function;solution concept	HCI	19.626039174291883	-6.708628379487784	7933
a998c52704d038cf525125d6463eecf4547a734a	simulation of evolable hardware to solve low level image processing tasks	tratamiento paralelo;analisis imagen;traitement signal;genetic program;image processing;traitement parallele;edge detection;procesamiento imagen;evolvable hardware;traitement image;genetics;deteccion contorno;detection contour;parallel image processing;signal processing;algorithme evolutionniste;image analysis;algoritmo evolucionista;evolutionary algorithm;procesamiento senal;analyse image;parallel processing	The long term goal of the work described in this paper is the development of a bio-inspired system, employing evolvable hardware, that adapts according to the needs of the environment in which it is deployed. The application described here is the design of a novel and highly parallel image processing tool to detect edges within a wide range of conventional grey-scale images. We discuss the simulation of such a system based on a genetic programming paradigm, using a simple binary logic tree to implement the genetic string coding. The results acquired from the simulation are compared with those obtained from the application of a conventional Sobel edge detector, and although rudimentary, show the great potential of such bio-inspired systems.	british informatics olympiad;edge detection;evolutionary algorithm;evolvable hardware;genetic programming;grayscale;image processing;programming paradigm;simulation;sobel operator	Gordon Hollingworth;Andrew M. Tyrrell;Steve Smith	1999		10.1007/10704703_4	parallel processing;computer vision;image analysis;edge detection;image processing;computer science;artificial intelligence;evolutionary algorithm;signal processing;algorithm	Robotics	42.01866902874925	-30.176509964554032	7935
9f284b9cd344970c0197e5931b2bdfa7b0ef657d	verification of effect on next purchase when many vice category products are brought		Abstract   The purpose of this article is to verify the effect of previous purchases on later purchases, using shopping path data. We focus on the effect of vice category products bought before. In existing research, the effect of the prior purchase of virtue category products (which are relative necessities) on later purchases, is explained as licensing effect. Based on earlier studies, the purchase of vice category products is also expected to impact later purchase behavior. For example, prior vice category products purchased may impel customers to purchase virtue category products. But the verification result clearly indicated that prior purchase of vice category products did not impel customers to buy virtue category products, rather it indirectly led to a tendency to refrain from further purchases of products.		Ken Ishibashi;Kei Miyazaki;Katsutoshi Yada	2015		10.1016/j.procs.2015.08.288	purchase order	ECom	-2.1593622515058337	-9.57084453860189	7954
c7bdf14c7ca90678169a86781e00c8d617b27184	optimal placement of distributed generation units in distribution systems via an enhanced multi-objective particle swarm optimization algorithm	shan cheng min you chen rong jongwai fang zong wang 粒子群优化算法 多目标优化模型 分布式发电 最佳位置 配电系统 pso算法 单元 功率损耗 optimal placement of distributed generation units in distribution systems via an enhanced multi objective particle swarm optimization algorithm	This paper deals with the optimal placement of distributed generation (DG) units in distribution systems via an enhanced multi-objective particle swarm optimization (EMOPSO) algorithm. To pursue a better simulation of the reality and provide the designer with diverse alternative options, a multi-objective optimization model with technical and operational constraints is constructed to minimize the total power loss and the voltage fluctuation of the power system simultaneously. To enhance the convergence of MOPSO, special techniques including a dynamic inertia weight and acceleration coefficients have been integrated as well as a mutation operator. Besides, to promote the diversity of Pareto-optimal solutions, an improved non-dominated crowding distance sorting technique has been introduced and applied to the selection of particles for the next iteration. After verifying its effectiveness and competitiveness with a set of well-known benchmark functions, the EMOPSO algorithm is employed to achieve the optimal placement of DG units in the IEEE 33-bus system. Simulation results indicate that the EMOPSO algorithm enables the identification of a set of Pareto-optimal solutions with good tradeoff between power loss and voltage stability. Compared with other representative methods, the present results reveal the advantages of optimizing capacities and locations of DG units simultaneously, and exemplify the validity of the EMOPSO algorithm applied for optimally placing DG units.	aggregate function;benchmark (computing);coefficient;converge;crowding;discontinuous galerkin method;exemplification;genetic algorithm;high- and low-level;iteration;linear programming;mathematical optimization;multi-objective optimization;nonlinear system;numerical analysis;pareto efficiency;particle swarm optimization;quantum fluctuation;simulation;sorting;stationary process;verification and validation	Shan Cheng;Min-You Chen;Rong-jong Wai;Fang-zong Wang	2014	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1300250	mathematical optimization;computer science;artificial intelligence	EDA	17.412747174697657	-3.8441372210923284	7958
b9b61d71271d77161682785040966ced688e1f15	model detection and estimation for single-index varying coefficient model	combined penalization;model detection;oracle property;primary;primary62g05;single index varying coefficient model;secondary62g20;secondary;期刊论文;mave	"""Single-index varying coefficient model (SIVCM) is a powerful tool for modeling nonlinearity in multivariate estimation, and has been widely used in the literature due to the fact that it can overcome the well-known phenomenon of """"curse-of-dimensionality"""". In this paper, we consider the problem of model detection and estimation for SIVCM. Based on the proposed combined penalization procedure, we can identify the true model structure consistently, and obtain a new semiparametric model-partially linear single-index varying coefficient model (PLSIVCM). Under the appropriate conditions, we demonstrate that the proposed penalized estimators of parametric and nonparametric components of PLSIVCM are consistent, but their asymptotic distributions are not available. Hence, we extend the minimum average variance estimation method to PLSIVCM, and establish the asymptotic normality for the refined estimators of index parameters, constant coefficients and varying coefficient functions, respectively. The finite sample performances of the proposed methods are illustrated by a Monte Carlo simulation study and the real data analysis."""	coefficient;single-index model	Sanying Feng;Liugen Xue	2015	J. Multivariate Analysis	10.1016/j.jmva.2015.03.008	econometrics;mathematical optimization;primary ideal;mathematics;statistics	AI	29.45603386688735	-23.86938229239584	7964
bc1974aaa38c3954d58315bc42d64502a1e22c50	asymptotic features of parallel agent-based immunological system				Aleksander Byrski;Robert Schaefer;Maciej Smolka	2011		10.7148/2011-0518-0524	mathematics	NLP	17.60566832792013	-9.254368011719311	7966
ed5ee76212077492bba207716d8b5c9be1a49741	comparative analysis of three techniques for predictions in time series having repetitive patterns		Modelling nonlinear patterns is possible through using regression (curve fitting) methods. However, they can be modelled by linear regression (LR) methods, too. This kind of modelling is usually used to depict and study trends and it is not used for prediction purposes. Our goal is to study the applicability and accuracy of piecewise linear regression in predicting a target variable in different time spans (where a pattern is being repeated). Using moving average, we identified the split points and then tested our approach on a real world case study. The dataset of the amount of recycling material in Blue Carts in Calgary (including more than 31,000 records) was taken as a case study for evaluating the performance of the proposed approach. Root mean square error (RMSE) and Spearman rho were used to evaluate and prove the applicability of this prediction approach and evaluate its performance. A comparison between the performances of Support Vector Machine (SVM), Neural Networks (NN), and the proposed LR-based prediction approach is also presented. The results show that the proposed approach works very well for such prediction purposes. It outperforms SVM and is a powerful competitor for NN.	artificial neural network;curve fitting;lr parser;mean squared error;neural network software;nonlinear system;performance;piecewise linear continuation;support vector machine;time series	Arash Niknafs;Bo Sun;Michael M. Richter;Günther Ruhe	2011			data mining;computer science	ML	8.046239882253875	-20.01304477980727	7982
b396d5614f8864a21b7c2bfd1e7abab5d2e01a39	motif counting beyond five nodes		Counting graphlets is a well-studied problem in graph mining and social network analysis. Recently, several papers explored very simple and natural algorithms based on Monte Carlo sampling of Markov Chains (MC), and reported encouraging results. We show, perhaps surprisingly, that such algorithms are outperformed by color coding (CC) [2], a sophisticated algorithmic technique that we extend to the case of graphlet sampling and for which we prove strong statistical guarantees. Our computational experiments on graphs with millions of nodes show CC to be more accurate than MC; furthermore, we formally show that the mixing time of the MC approach is too high in general, even when the input graph has high conductance. All this comes at a price however. While MC is very efficient in terms of space, CC’s memory requirements become demanding when the size of the input graph and that of the graphlets grow. And yet, our experiments show that CC can push the limits of the state-of-the-art, both in terms of the size of the input graph and of that of the graphlets.	algorithm;conductance (graph);endeavour (supercomputer);experiment;markov chain;monte carlo method;motif;requirement;sampling (signal processing);social network analysis;structure mining	Marco Bressan;Flavio Chierichetti;Ravi Kumar;Stefano Leucci;Alessandro Panconesi	2018	TKDD	10.1145/3186586	machine learning;conductance;social network analysis;artificial intelligence;sampling (statistics);monte carlo method;computer science;graph;color-coding;markov chain	ML	25.526418706963028	-28.617692599413157	7989
63ef64193895416b71be72da4a9d1886dffa1b3a	improved single shot object detector using enhanced features and predicting heads		Object detection attracts much attention for its great value in theories and applications. The one-stage single shot object detectors outperform the two-stage methods in running speed with a comparable performance. In this paper, we propose three novel strategies, to further improve the performances of single shot detector without sacrificing their runtime efficiency. Firstly, we design the multi-scale context aggregation module to embeds the context information into the learned features. Secondly, we design the multi-path predicting head, which decouples the network layers and can easily learn the effective receptive fields of different aspect ratios, to detect objects of various aspect ratios better. Thirdly, we adopt a top-down feature map pyramid to detect objects using features of different semantic powers and resolutions. Sufficient ablation experiments are conducted to prove the efficiency of the proposed methods. We design a one stage single detector named as ISSD, using the three strategies. Experimental results on PASCAL VOC 2007 and 2012 shows ISSD achieves the new state-of-the-art on accuracy with the comparable running speed.	baseline (configuration management);benchmark (computing);experiment;object detection;performance;sensor;solid-state drive;theory;top-down and bottom-up design	Xu Zhao;Chaoyang Zhao;Yousong Zhu;Ming Tang;Jinqiao Wang	2018	2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2018.8499089	feature extraction;aspect ratio (image);computer vision;object detection;detector;receptive field;computer science;artificial intelligence	Vision	29.927424606424342	-51.427245321155	7994
841d23d3916c9481b0d67c48d8a595df181a38d8	optimal artificial neural network topology for foreign exchange forecasting	foreign exchange market;feed forward;trade volume;artificial neural networks;financial time series;foreign exchange rate;profitability;neural network model;optimal topology;foreign exchange;back propagation;artificial neural network	Foreign exchange market is one of the highest investments markets the average daily trade volume is 1.8 trillion USD. Foreign exchange rate forecasting has been always one of the most challenging subject and area of researches. Trader around the world is relying on the technical indicators which just following the price and has emerged a lag results. When the currency market has a random move (when the market is not trending) most of the indicators gets confused because of the fact that classical linear methods are unable to react with the non linearity in the data and hence with the market behavior.  This research reports empirical results that tend to confirm the applicability of a neural network model to the prediction of the foreign exchange rates market. Artificial neural networks have proven to be efficient and profitable in forecasting financial time series in particular, feed forwarded back propagation. It is important to use an optimal ANN topology that emerged great results in short term prediction and the daily predication results showed that ANN model learns well and most likely to generalize well. Weekly predication results demonstrate good results in the low prediction while failed to have a good results on the high and the close prediction while the monthly prediction did not give a satisfactory results due to a very few data samples.	artificial neural network;backpropagation;foreign exchange service (telecommunications);network model;network topology;software propagation;time series;trader media east	Ahmed Emam	2008		10.1145/1593105.1593121	financial economics;economics;operations management;economy	ML	6.0444019117802785	-17.32310221569256	7996
8d6e76ac8de190bf3b6c40a91effe95a6da0559c	multisensor fusion for simultaneous localization and map building	charge coupled image sensors;2d laser rangefinder;robot sensing systems;ccd camera;mobile robots sensor phenomena and characterization redundancy charge coupled image sensors simultaneous localization and mapping sensor fusion laser fusion robot vision systems cameras robot sensing systems;sensor phenomena and characterization;map building;mobile robot;laser rangefinder;correlated sensor observations;simultaneous localization;mobile robots;ccd image sensors mobile robots sensor fusion redundancy laser ranging;laser ranging;ccd image sensors;multisensor fusion;laser fusion;redundancy;simultaneous localization and mapping;ccd camera multisensor fusion simultaneous localization map building correlated sensor observations related sensor observations partial redundancy mobile robot 2d laser rangefinder charge coupled device camera;partial redundancy;charged couple device;sensor fusion;simultaneous localization and map building;robot vision systems;cameras;charge coupled device camera;related sensor observations	"""In this paper, we have studied strategies for controlling formations of mobile robots using methods from nonlinear control theory and graph theory. We have focused on decomposing the problem of controlling a formation of nonholonomic mobile robots into: 1) controlling a single lead robot and 2) controlling other follower robots in the team. We used the terms l 0 and l 0 l control to reflect whether the control laws are based on tracking the position and orientation of the robot relative to a leader, or the position relative to two leaders, respectively. We also defined the concept of a transition matrix, which governs the addition and deletion of edges in the control graph and hence the change in the communication protocol. Based on this, we presented an exhaustive list of all possible transitions that can occur within the robots in the formation and the corresponding transition matrix column. There are several important issues that need to be addressed in future research in this area, including: 1) how to choose a control graph and the desired shape based on the constraints in the environment; 2) how to plan changes in (g; r; H) depending on sensor constraints; 3) how to allow formations to be split into sub-formations, leading to multiple lead robots; and 4d) though the transition matrix gives us the information needed to change formations, it is not clear if there is an optimal way for carrying out these changes, rather than the sequential algorithm presented here. Some of these topics are the focus of our present research. A motion planner for nonholonomic mobile robots, """" IEEE Trans. [6] O. Khatib, """" Real-time obstacle avoidance for manipulators and mobile robots, """" Int. Abstract—This paper describes how multisensor fusion increases both reliability and precision of the environmental observations used for the simultaneous localization and map-building problem for mobile robots. Mul-tisensor fusion is performed at the level of landmarks, which represent sets of related and possibly correlated sensor observations. The work emphasizes the idea of partial redundancy due to the different nature of the information provided by different sensors. Experimentation with a mobile robot equipped with a multisensor system composed of a 2-D laser rangefinder and a charge coupled device camera is reported."""	charge-coupled device;communications protocol;computation;computational complexity theory;control theory;correspondence problem;graph theory;image fusion;landmark point;mobile robot;nonlinear system;obstacle avoidance;real-time clock;real-time transcription;robotic mapping;sensor;sequential algorithm;simultaneous localization and mapping;stochastic matrix	José A. Castellanos;José Neira;Juan D. Tardós	2001	IEEE Trans. Robotics and Automation	10.1109/70.976024	mobile robot;computer vision;electronic engineering;computer science;engineering;artificial intelligence;charge-coupled device;remote sensing	Robotics	52.416802026409364	-33.75092808089492	7999
4500a5bd2f92e11b1ffb7222f44949f017fbc7bb	video retrieval based on tracked features quantization	compact discreet value;histograms;quantization;motion pictures;prototypes;space time features;feature tracking;video retrieval;space time;quantization video retrieval content based retrieval descriptor;quantisation signal;video indexing;indexes;copy detection benchmark muscle vcd 2007;indexing;histograms feature extraction indexes motion pictures prototypes benchmark testing;feature extraction;indexation;video indexing purpose;descriptor;copy detection benchmark muscle vcd 2007 video retrieval features quantization image retrieval method compact discreet value video indexing purpose space time features;content based retrieval;image retrieval method;features quantization;benchmark testing;video retrieval feature extraction indexing quantisation signal;image retrieval	In this paper, we present an image retrieval method based on feature tracking. Feature tracks are summarized into a compact discreet value and used for video indexing purpose. As opposed to existing space-time features, we do not make any assumption on the motion visible on the indexed videos. As a result, given an example query, our system is able to retrieve related videos from a large database. We evaluated our system with the copy detection benchmark MUSCLE-VCD-2007. We also ran retrieval experiment on hours of TV broadcast.	benchmark (computing);database;image retrieval;indexed grammar;motion estimation	Hiroaki Kubo;Julien Pilet;Hideo Saito;Shin'ichi Satoh	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.794	database index;benchmark;computer vision;search engine indexing;quantization;feature extraction;image retrieval;computer science;space time;histogram;prototype;multimedia;information retrieval	Vision	38.51796171456176	-51.71505882491986	8001
fb1e2c41f9edf9e42c1dffb193fd81d3c6591b44	arima model estimated by particle swarm optimization algorithm for consumer price index forecasting	estimation method;consumer price index;arima model;power optimization;scientific communication;particle swarm optimization algorithm;moment estimation	This paper presents an ARIMA model which uses particle swarm optimization algorithm (PSO) for model estimation. Because the traditional estimation method is complex and may obtain very bad results, PSO which can be implemented with ease and has a powerful optimizing performance is employed to optimize the coefficients of ARIMA. In recent years, inflation and deflation plague the world moreover the consumer price index (CPI) which is a measure of the average price of consumer goods and services purchased by households is usually observed as an important indicator of the level of inflation, so the forecast of CPI has been focused on by both scientific community and relevant authorities. Furthermore, taking the forecast of CPI as a case, we illustrate the improvement of accuracy and efficiency of the new method and the result shows it is predominant in forecasting.	algorithm;angular defect;autoregressive integrated moving average;capacitor plague;coefficient;holism;mathematical optimization;particle swarm optimization	Hongjie Wang;Weigang Zhao	2009		10.1007/978-3-642-05253-8_6	econometrics;mathematical optimization;autoregressive integrated moving average;consumer price index;power optimization	Robotics	5.775575122458334	-16.930779395763736	8002
146b6313101def81eb2bc559456917806bdb7379	new geometric semantic operators in genetic programming: perpendicular crossover and random segment mutation		Various geometric search operators have been developed to explore the behaviours of individuals in genetic programming (GP) for the sake of making the evolutionary process more effective. This work proposes two geometric search operators to fulfil the semantic requirements under the theoretical framework of geometric semantic GP for symbolic regression. The two operators approximate the target semantics gradually but effectively. The results show that the new geometric operators can not only lead to a notable benefit to the learning performance, but also improve the generalisation ability of GP. In addition, they also bring a significant improvement to Random Desired Operator, which is a state-of-the-art geometric semantic operator.	approximation algorithm;genetic programming;requirement;symbolic regression	Qi Chen;Mengjie Zhang;Bing Xue	2017		10.1145/3067695.3076008	symbolic regression;artificial intelligence;operator (computer programming);machine learning;mathematical optimization;perpendicular;theoretical computer science;genetic programming;crossover;semantics;generalization;mathematics	AI	23.917049058513467	-7.637734245550622	8007
86273fa4325c8441e8ff21b1c81645021d8112b8	fuzzy qualitative simulation with multivariate constraints	fuzzy set theory fuzzy reasoning;nonfuzzy situations fuzzy multivariate relations fuzzy qualitative simulation fuzzy qualitative reasoning framework fuzzy multivariate function constraints fmf constraints differential planes morven model fuzzy multivariate monotonicity relations fmm relations mm add constraints alpha cut qualitative sign strictness fuzzy situations;qa75 electronic computers computer science;conference item;equations jacobian matrices vectors mathematical model educational institutions approximation methods numerical models	"""In this research we focus on dealing with fuzzy multivariate relations and how we could perform fuzzy qualitative simulation with models containing such relations. To achieve this, we extended Morven, a fuzzy qualitative reasoning framework, and proposed novel types of constraints for the framework. We first introduced fuzzy multivariate function (FMF) constraints, and presented their corresponding constraints in higher differential planes of a Morven model. We then implemented the fuzzy multivariate monotonicity (FMM) relations by FMF constraints and MM_add constraints, another kind of constraints we proposed for Morven. In addition, we employed alpha-cut to determine the """"strictness"""" of qualitative signs in the MM_add constraints. Finally, proof-of-concept experiments were performed to validate the proposed constraints, and both fuzzy and non-fuzzy situations were considered in these experiments."""	apply;benchmark (computing);c date and time functions;directx plugin;experiment;fast multipole method;fuzzy set;qml;schedule (computer science);simulation;sum rule in quantum mechanics	Wei Pang;George Macleod Coghill	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891702	fuzzy logic;mathematical optimization;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy mathematics;fuzzy classification;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;statistics	Vision	-2.0311275487079845	-20.982701436104165	8009
68666089c00e08f008bf089d3e7a21e48262b40d	case-base maintenance in an associative memory organized by a self-organization map	case base reasoning;soft computing;associative memory;self organization;self organized map	  Case-Based Reasoning (CBR) systems solve new problems using others which have been previously resolved in a case memory, where  each case represents a solved situation. Therefore, the case memory size and its organization influences on the computational  time needed to solve new situations. For this reason, we organize the memory using a Self-Organization Map for defining patterns  to allow system to do a selective retrieval using only the cases of the most suitable pattern. This works presents a case-based  maintenance to incrementally introduce knowledge in SOM without retraining it because this process is very expensive in terms  of computational time. The strategy is semi-supervised because we use the feedback provided by the expert and, at the same  time, the self-organization of cases when clusters are readjusted. Results show a successful case-based maintenance.    		Albert Fornells;Elisabet Golobardes	2007		10.1007/978-3-540-74972-1_41	artificial intelligence;theoretical computer science;machine learning;memory map	Robotics	5.189249501700709	-31.088296906359492	8014
6b0cb114ddbb4238879187556d6d7dc2bf9aa61e	nonparametric discriminant analysis in relevance feedback for content-based image retrieval	small sample size problem;discriminant analysis;proposedfull-space nda;full-space method;visual databases;single gaussian distribution;ill-posed problem;corel database;nonparametric discriminant analysis;null-space method;regularization method;nonparametric statistics;content-based image retrieval;kernel bda;image retrieval;relevance feedback;kernel parameter tuning;content-based image;parameter tuning problem;content-based retrieval;biased discriminant analysis;positive feedback;gaussian distribution	Relevance feedback (RF) has been widely used to improve the performance of content-based image retrieval (CBIR). How to select a subset of features from a large-scale feature pool and to construct a suitable dissimilarity measure are key steps in RF. Biased discriminant analysis (BDA) has been proposed to select features during relevance feedback iterations. However, BDA assumes all positive feedbacks form a single Gaussian distribution, which may not be the case for CBIR. Although kernel BDA can overcome the drawback to some extent, the kernel parameter tuning makes the online learning unfeasible. To avoid the parameter tuning problem and the single Gaussian distribution assumption in BDA, we construct a new nonparametric discriminant analysis (NDA). To address the small sample size problem in NDA, we introduce the regularization method and the null-space method. Because the regularization method may meet the ill-posed problem and the null-space method will lose some discriminant information, we proposed here a full-space method. The proposed full-space NDA is demonstrated to outperform BDA based RF significantly based on a large number of experiments in Corel database with 17,800 images.	broadcast driver architecture;content-based image retrieval;experiment;iteration;kernel (linear algebra);kernel (operating system);linear discriminant analysis;matrix regularization;online machine learning;radio frequency;relevance feedback;well-posed problem	Dacheng Tao;Xiaoou Tang	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334431	normal distribution;nonparametric statistics;positive feedback;image retrieval;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	Vision	24.85336930020552	-40.849503545866035	8017
848bf543b90efc3a715da5896b13a056a6de8892	convergence of maxgeneralized mean-mingeneralized mean powers of intuitionistic fuzzy matrices		Intuitionistic fuzzy relations on finite universes can be represent by intuitionistic fuzzy matrices and the limiting behavior of the power matrices depends on the algebraic operation employed on the matrices. In this paper, the power of intuitionistic fuzzy matrices with maxgeneralized mean-mingeneralized mean operation have been studied. Here it is shown that the power of intuitionistic fuzzy matrices with the said operations are always convergent. The convergence of powers for an intuitionistic fuzzy matrix with convex combination of max-min and maxarithmetic meanminarithmetic mean are also dicussed here.	fuzzy associative matrix;fuzzy set;graph theory;instrumental convergence;interaction-free measurement;intuitionistic logic;linear algebra;maxima and minima	Rajkumar Pradhan;Madhumangal Pal	2014	CoRR		combinatorics;mathematical analysis;discrete mathematics;fuzzy number;mathematics;fuzzy set operations	AI	-0.45046709359644654	-22.262379813825785	8034
41a5807960bf166e89079eb44bf0f1f65fcb1781	a stochastic model for investments in different technologies for electricity production in the long period	secs s 06 metodi mat dell economia e scienze attuariali e finanziarie;mat 09 ricerca operativa;mixed integer stochastic model;fuel prices uncertainty;capacity production planning	We present a single stage stochastic mixed integer linear model for determining the optimal mix of different technologies for electricity generation, ranging from coal, nuclear and combined cycle gas turbine to hydroelectric, wind and photovoltaic, taking into account the existing plants, the cost of investment in new plants, maintenance costs, purchase and sale of CO2 emission trading certificates and green certificates, in order to satisfy regulatory requirements. The power producer is assumed to be a price-taker. Stochasticity of future fuel prices, which affect the generation variable costs, is included in the model by means of a set of scenarios. The main contribution of the paper, beyond considering stochasticity in the future fuel prices, is the introduction of CVaR risk measure in the objective function in order to limit the possibility of low profits in bad scenarios with a fixed confidence level.	cvar;gene regulatory network;linear model;loss function;multistage amplifier;optimization problem;requirement;risk measure;stochastic process	Maria Teresa Vespucci;Marida Bertocchi;Mario Innorta;Stefano Zigrino	2014	CEJOR	10.1007/s10100-013-0317-4	economics;operations management;economy;operations research;commerce	NLP	9.04982563870158	-3.9573678214381016	8036
230eca39cff068b8af00b20f7f668ee1cd7a6b90	connecting the top-down to the bottom-up: pricing cdo under a conditional survival (cs) model	conditional survival;concrete model;cs model;cdo tranche pricing;single name;cdo tranches;itraxx tranches;single name cds spread;new dynamic model;excellent calibration;automatic calibration;pricing cdo;pricing;cross section;bottom up;subprime mortgage crisis;simulation;top down;mean squared error;collateralized debt obligation	In this paper, we use exact simulation to price CDO under a new dynamic model, the Conditional Survival (CS) model, which provided excellent calibration to both iTraxx tranches and underlying single name CDS spreads on March 14, 2008, the day before the collapse of Bear Sterns, when the market was highly volatile. The distinct features of the CS model include: (1) it is able to generate clustering of defaults occurring dynamically in time and strong cross-sectional correlation, i.e., the simultaneous defaults of many names, both of which have been evidenced in the current subprime mortgage crisis; (2) it incorporates idiosyncratic default risk of single names but does not specify concrete models for them; (3) it provides automatic calibration to underlying single name CDS; (4) it allows fast CDO tranche pricing and calculation of sensitivity of CDO tranches to underlying single name CDS.	bottom-up proteomics;cluster analysis;cross-sectional data;mathematical model;simulation;top-down and bottom-up design	Xian Hua Peng;Steven Kou	2008	2008 Winter Simulation Conference		actuarial science;top-down and bottom-up design	AI	1.7104685073956534	-11.377217604028216	8064
35d570680920326ff227da6b315db07406b72554	bayesian models based on test statistics for multiple hypothesis testing problems	model assessment;estadistica test;perforation;statistique test;hypothese;bioinformatique;differential gene expression;test;bayesian method;ensayo;gene expression;assessment tool;essai;modelo;hipotesis;multiple hypothesis testing;graphical model;modele;bioinformatica;hypothesis;multiple;models;test statistic;bayesian model;bioinformatics	MOTIVATION We propose a Bayesian method for the problem of multiple hypothesis testing that is routinely encountered in bioinformatics research, such as the differential gene expression analysis. Our algorithm is based on modeling the distributions of test statistics under both null and alternative hypotheses. We substantially reduce the complexity of the process of defining posterior model probabilities by modeling the test statistics directly instead of modeling the full data. Computationally, we apply a Bayesian FDR approach to control the number of rejections of null hypotheses. To check if our model assumptions for the test statistics are valid for various bioinformatics experiments, we also propose a simple graphical model-assessment tool.   RESULTS Using extensive simulations, we demonstrate the performance of our models and the utility of the model-assessment tool. In the end, we apply the proposed methodology to an siRNA screening and a gene expression experiment.	algorithm;bayesian network;bioinformatics;experiment;genes, vif;graphical model;null (sql);null value;probability;simulation;tissue-specific gene expression	Yuan Ji;Yiling Lu;Gordon B. Mills	2008	Bioinformatics	10.1093/bioinformatics/btn049	econometrics;gene expression;hypothesis;test statistic;bayesian probability;data mining;mathematics;software testing;graphical model;bayesian statistics;bayesian inference;multiple comparisons problem;multiple;statistics	ML	5.087205309089614	-52.07924597802922	8074
804a62a9b4be26994ecf569ec53267bf05cfafc0	self-adaptive two-phase support vector clustering for multi-relational data mining	distributed system;base relacional dato;metodo adaptativo;relational data;analyse amas;adaptability;cluster computing;adaptabilite;systeme reparti;analisis datos;racimo calculadura;kernel function;methode adaptative;relational database;data mining;classification;adaptabilidad;data analysis;grappe calculateur;sistema repartido;cluster analysis;fouille donnee;decouverte connaissance;machine exemple support;adaptive method;funcion nucleo;fonction noyau;support vector clustering;base donnee relationnelle;descubrimiento conocimiento;analyse donnee;analisis cluster;maquina ejemplo soporte;vector support machine;multi relational data mining;busca dato;clasificacion;knowledge discovery	This paper proposes a novel Self-Adaptive Two-Phase Support Vector Clustering algorithm (STPSVC) to cluster multi-relational data. The algorithm produces an appreciate description of cluster contours and then extracts cluster centers information by iteratively performing classification procedure. An adaptive Kernel function is designed to find a desired width parameter for diverse dispersions. Experimental results indicate that the designed Kernel can capture multi-relational features well and STPSVC is of fine performance.	algorithm;cluster analysis;relational data mining;two-phase commit protocol;two-phase locking	Ping Ling;Yan Wang;Chunguang Zhou	2006		10.1007/11731139_27	k-medians clustering;relational database;computer science;artificial intelligence;data mining;mathematics;knowledge extraction;cluster analysis;algorithm	ML	-2.970467035751615	-32.636879071542495	8075
26d3efb45b48eeb55ee11b323adb79c4b90d67c8	the usefulness of recombination	nearest neighbor;genetic algorithm;point of view	In this paper, we examine the usefulness of recombination from two points of view. First, the problem of crossover disruption is investigated. This is done by comparing two Genetic Algorithms with diierent crossover operators (one-point and uniform) to each other on NK-landscapes with diierent values of K relative to N, and with different epistatic interactions (random and nearest neighbor). Second, the usefulness of recombination in relation to the location of local optima in the tness landscape is investigated. There appears to be a clear relation between the type of tness landscape and the type of recombination that is most useful on this landscape. Furthermore, there also is a clear relation between the location of local optima in the tness landscape and the usefulness of recombination.	crossover (genetic algorithm);denial-of-service attack;genetic algorithm;interaction;local optimum	Wim Hordijk;Bernard Manderick	1995		10.1007/3-540-59496-5_352	nearest-neighbor chain algorithm;genetic algorithm;computer science;k-nearest neighbors algorithm	AI	24.85716024769425	-7.609633270761095	8091
70e69a85fdf369f4d75574b68825f9d1b5009dff	predicting changes in market segments based on customer behavior		In modern marketing, knowing the development of different market segments is crucial. However, simply measuring the occurred changes is not sufficient when planning future marketing campaigns. Predictive models are needed to show trends and to forecast abrupt changes such as the elimination of segments, the splitting of a segment, or the like. For predicting changes, continuously collected data are needed. Behavioral data are suitable for spotting trends in customer segments as they can easily be recorded. For detecting changes in a market structure, fuzzy-clustering is used since gradual changes in cluster memberships can implicate future abrupt changes. In this paper, we introduce different measurements for the analysis of gradual changes that comprise the currentness of data and can be used in order to predict abrupt changes.		Anneke Minke;Klaus Ambrosi	2012		10.1007/978-3-319-01595-8_22	marketing;business;commerce	ECom	4.512184597876332	-14.932551865294409	8092
e9ab1e7b1d2f3fc7f78584fd142025fa4d233a84	the fast multipole algorithm	computational complexity digital simulation physics computing n body problems;periodic boundary condition;physics computing;molecular dynamics method;electrostatic solver fast multipole algorithm mutual interactions gravitational forces simulation science computational complexity very large astrophysical simulations barnes hut scheme biophysical simulation world ewald summation method multipole codes error behavior periodic boundary conditions fma derived codes;algorithm theory;computational complexity;n body problems;digital simulation;lipidomics computational modeling dna proteins biological cells biomembranes biological system modeling atomic layer deposition electrostatics computational complexity;fast multipole algorithm	Accurate computation of the mutual interactions of N particles through electrostatic or gravitational forces has impeded progress in many areas of simulation science. The fast multipole algorithm (FMA) provides an efficient scheme for reducing computational complexity. Researchers are studying very large astrophysical simulations with hybrids of the FMA and the earlier Barnes-Hut scheme (J.E. Barnes and P. Hut, 1986). In the biophysical-simulation world, the Ewald summation method is an additional competitor. Since the development of the FMA, scientists have created various fast versions of the nearly 80-year-old Ewald method that are faster than multipole codes in some cases, although their error behavior is harder to quantify. The Ewald codes also handle periodic boundary conditions automatically; FMA-derived codes can be extended to this case with extra effort. None the less, FMA and its offspring remain important, and the newest formulations promise to again challenge Ewald codes for the title of fastest electrostatic solver.	algorithm;fast fourier transform;fast multipole method	John Board;Klaus Schulten	2000	Computing in Science and Engineering	10.1109/5992.814662	computational science;parallel computing;simulation;fast multipole method;computer science;theoretical computer science;ewald summation;mathematics;periodic boundary conditions;computational physics;computational complexity theory;physics;algorithm;quantum mechanics	DB	43.062589035203416	1.600289311118595	8115
fb0e79fc2d9fbecb6bd95873727eb8cc6da6f1b7	an enhanced extreme learning machine based on ridge regression for regression	ridge regression;least square method;extreme learning machine;single hidden layer feedforward neural networks	The extreme learning machine (ELM) is a novel single hidden layer feedforward neural network, which has the superiority in many aspects, especially in the training speed; however, there are still some shortages that restrict the further development of ELM, such as the perturbation and multicollinearity in the linear model. To the adverse effects caused by the perturbation or the multicollinearity, this paper proposes an enhanced ELM based on ridge regression (RR-ELM) for regression, which replaces the least square method to calculate output weights. With an additional adjustment of ridge regression, all the characteristics become even better. Simulative results show that the RR-ELM, compared with ELM, has better stability and generalization performance.	artificial bee colony algorithm;artificial neural network;benchmark (computing);elm;feedforward neural network;linear model;rapid refresh;round-robin scheduling	Guoqiang Li;Peifeng Niu	2011	Neural Computing and Applications	10.1007/s00521-011-0771-7	computer science;artificial intelligence;machine learning;pattern recognition;tikhonov regularization;least squares	AI	15.689618756947908	-29.793520089205554	8137
5e053cd164b02433c4efc0fc675f6273a8a1c46a	scalable bayesian learning of recurrent neural networks for language modeling		Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs, using back-propagation through time, often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.	algorithm;artificial neural network;backpropagation;computation;experiment;gradient noise;language model;markov chain monte carlo;mathematical optimization;monte carlo method;neural network software;overfitting;overhead (computing);random neural network;recurrent neural network;scalability;software propagation;stochastic optimization;suicidegirls	Zhe Gan;Chunyuan Li;Changyou Chen;Yunchen Pu;Qinliang Su;Lawrence Carin	2017		10.18653/v1/P17-1030	computer science;artificial intelligence;machine learning;data mining;statistics	NLP	24.656727586104086	-30.59212377142121	8149
a762e815bbfa0c1ffec2da23e3163348161d8198	symmetric complex-valued hopfield neural networks	digital simulation hopfield neural nets image denoising image resolution;biological patents;biomedical journals;text mining;complex valued neural networks hopfield neural networks noise tolerance rotational invariance;europe pubmed central;hebbian theory;training;citation search;symmetric complex valued hopfield neural networks multilevel data gray scale images hermitian connection weights asynchronous update rotational invariance noise tolerance symmetric chnn schnn computer simulations;citation networks;learning systems;research articles;abstracts;open access;life sciences;neurons training computer simulation hebbian theory learning systems biological neural networks;clinical guidelines;neurons;full text;rotational invariance complex valued neural networks hopfield neural networks noise tolerance;computer simulation;rest apis;biological neural networks;orcids;europe pmc;biomedical research;bioinformatics;literature search	Complex-valued neural networks, which are extensions of ordinary neural networks, have been studied as interesting models by many researchers. Especially, complex-valued Hopfield neural networks (CHNNs) have been used to process multilevel data, such as gray-scale images. CHNNs with Hermitian connection weights always converge using asynchronous update. The noise tolerance of CHNNs deteriorates extremely as the resolution increases. Noise tolerance is one of the most controversial problems for CHNNs. It is known that rotational invariance reduces noise tolerance. In this brief, we propose symmetric CHNNs (SCHNNs), which have symmetric connection weights. We define their energy function and prove that the SCHNNs always converge. In addition, we show that the SCHNNs improve noise tolerance through computer simulations and explain this improvement from the standpoint of rotational invariance.	algorithm;artificial neural network;computer simulation;converge;gradient descent;grayscale;hebbian theory;hopfield network;image resolution;immune tolerance;learning rule;machine learning;mathematical optimization;neural networks;weight	Masaki Kobayashi	2017	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2016.2518672	computer simulation;text mining;hebbian theory;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;statistics	ML	16.753839016509133	-26.78588535320672	8155
a2888faa9c42fb27706dfed4f050c3e0ddf29ef4	solving the xor problem and the detection of symmetry using a single complex-valued neuron	generalizacion;orthogonality;probleme ou exclusif;decision boundary;xor problem;egalisation;canal evanouissement;equalization;sistema complejo;detection of symmetry;generalisation;systeme complexe;complex system;igualacion;fading equalization problem;detection symetrie;reseau neuronal;fading channels;complex valued neuron;generalization;red neuronal;orthogonalite;neural network;ortogonalidad	This letter presents some results on the computational power of complex-valued neurons. The main results may be summarized as follows. The XOR problem and the detection of symmetry problem which cannot be solved with a single real-valued neuron (i.e. a two-layered real-valued neural network), can be solved with a single complex-valued neuron (i.e. a two-layered complex-valued neural network) with the orthogonal decision boundaries, which reveals the potent computational power of complex-valued neurons. Furthermore, the fading equalization problem can be successfully solved with a single complex-valued neuron with the highest generalization ability.		Tohru Nitta	2003	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(03)00168-0	winner-take-all;generalization;complex systems;computer science;artificial intelligence;machine learning;mathematics;artificial neural network	ML	16.725140653859526	-26.60959630584739	8156
a4e36dfa00f7307fc5d43f964cec42c5eb38d9b9	from an image sequence to a recognized polyhedral object	sensory processing;three dimensional;visual motion;image sequence	We describe the combination of several novel algorithms into a system which obtains visual motion from a sequence of images and uses it to recover the 3D geometry and 3D motion of polyhedral objects relative to the sensor. The system goes on to use the recovered geometry to recognize the object from a database, a stage which also resolves the depth/speed scaling ambiguity, resulting in absolute depth and motion recovery. We demonstrate its performance of imagery from a well carpentered CSG model and on real imagery from a simple wooden model.	algorithm;constructive solid geometry;database;image scaling;polyhedron	David W. Murray;David A. Castelow;Bernard F. Buxton	1988	Image Vision Comput.	10.1016/0262-8856(88)90006-6	three-dimensional space;computer vision;mathematics;computer graphics (images)	Vision	53.26833228613979	-51.304817518338645	8157
09729b82d47fa7e4b0b13b2955c212ca8c1b67b6	multi class machine learning algorithms for intrusion detection - a performance study		Advancement of the network technology has increased our dependency on the Internet. Hence the security of the network plays a very important role. The network intrusions can be identified using Intrusion Detection System (IDS). Machine learning algorithms are used to predict the network behavior as intrusion or normal. This paper discusses the prediction analysis of different supervised machine learning algorithms namely Logistic Regression, Gaussian Naive Bayes, Support Vector Machine and Random Forest on NSL-KDD dataset. These machine learning classification techniques are used to predict the four different types of attacks namely Denial of Service attack, Remote to Local (R2L), Probe and User to Root(U2R) attacks using multi-class classification technique.	algorithm;intrusion detection system;machine learning	Manjula C. Belavagi;Balachandra	2017		10.1007/978-981-10-6898-0_14	naive bayes classifier;support vector machine;network security;random forest;computational learning theory;algorithm;intrusion detection system;statistical classification;machine learning;pattern recognition;denial-of-service attack;computer science;artificial intelligence	ML	7.042456644653758	-37.27536436134564	8161
ad09c6bc52e69b2023ea4d2b7cd269a6b1fd68a0	impact of the transmission grid on the operational system flexibility	stochastic modeling;system flexibility;renewable energy sources;california iso energy market transmission grid operational system flexibility flexible generation renewable energy sources generation res generation day ahead market hour ahead market real time market mixed integer programming security constrained unit commitment function security constrained economic dispatch function wholesale energy markets;security renewable energy sources europe reliability power system reliability economics;transmission grid locational marginal pricing renewable energy sources security constrained economic dispatch security constrained unit commitment stochastic modeling system flexibility;security constrained unit commitment;renewable energy sources integer programming load dispatching power grids power markets;security constrained economic dispatch;locational marginal pricing;transmission grid	In this paper we consider the effects of the transmission grid on the operational system flexibility and evaluate the trade-offs between various options including committing flexible generation, curtailing Renewable Energy Sources (RES) generation, or allowing flexibility violations in the form of ramping up/down requirements in the presence of high penetration of renewable generation. All ISO/TSO markets in the different time scales are considered in this study, namely the Day Ahead Market (DAM), Hour Ahead Market (HAM) and Real-Time Market (RTM). Our proposed methodology is based on the co-optimization of energy and ancillary services. We deployed the Mixed Integer Programming (MIP) Security Constrained Unit Commitment (SCUC) and the Security Constrained Economic Dispatch (SCED) functions applied to all wholesale energy markets in sequence as they are executed in practice today. This paper describes the proposed methodology and presents numerical results resulting from detailed energy simulations applied to the California ISO energy market. The deployment of a detailed transmission model, the implementation of a new Flexible Ramping AS, and the accurate transition between DAM to HAM to RTM markets are the key original contributions of this paper.	integer programming;linear programming;mathematical optimization;numerical analysis;operational system;real-time transcription;requirement;simulation;software deployment;terminator 2: judgment day	Alex Papalexopoulos;Chuck Hansen;Rod Frowd;Aidan Tuohy;Eamonn Lannoye	2016	2016 Power Systems Computation Conference (PSCC)	10.1109/PSCC.2016.7541027	economics;operations management;microeconomics;commerce	Embedded	2.694660483472752	3.3629167190634317	8170
cdde76dbd0ed9d40d8b4413c7eb0f8d3f2b3bc4c	implicit variational inference with kernel density ratio fitting		Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. Work has been done to use implicit distributions, i.e., distributions without tractable likelihoods as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and can hardly scale to high-dimensional latent variable models. In this paper, we present an implicit variational inference approach with kernel density ratio fitting that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.	artificial neural network;bayesian network;calculus of variations;cobham's thesis;curve fitting;latent variable;variational principle	Jiaxin Shi;Shengyang Sun;Jun Zhu	2017	CoRR		kernel;econometrics;mathematical optimization;variable kernel density estimation;statistics	ML	27.048578409696937	-31.021614230160537	8178
a0b631cb891caf19b9817be515b315506235d37b	large-scale subspace clustering using sketching and validation		The nowadays massive amounts of generated and communicated data present major challenges in their processing. While capable of successfully classifying nonlinearly separable objects in various settings, subspace clustering (SC) methods incur prohibitively high computational complexity when processing large-scale data. Inspired by the random sampling and consensus (RANSAC) approach to robust regression, the present paper introduces a randomized scheme for SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale data. At the heart of SkeVa-SC lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments. Sparsity in data representations is also exploited to reduce the computational burden of SC, while achieving high clustering accuracy. Performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of SkeVa-SC and its competitive performance relative to state-of-theart scalable SC approaches.	cluster analysis;clustering high-dimensional data;computational complexity theory;consensus (computer science);dhrystone;exploit (computer security);nonlinear system;numerical analysis;random sample consensus;randomized algorithm;sampling (signal processing);scalability;smoothing;sparse matrix	Panagiotis A. Traganitis;Konstantinos Slavakis;Georgios B. Giannakis	2015	CoRR		computer science;theoretical computer science;machine learning;data mining;statistics	ML	26.02612341441287	-36.00606345751207	8182
bf13223e55a62259ce6c158ab6c5e087fa44808d	fuzzy ant clustering by centroid positioning	pattern clustering;optimisation;fuzzy c mean;swarm intelligence;ant colony optimization;iterative algorithms;clustering algorithms partitioning algorithms ant colony optimization iterative algorithms stochastic processes equations computer science data engineering fuzzy logic particle swarm optimization;data engineering;feature space;fuzzy set theory;fuzzy logic;data clustering;optimisation fuzzy set theory pattern clustering;hard c means;stochastic processes;particle swarm optimization;ant colony optimization principles;clustering algorithms;hard c means fuzzy ant clustering centroid positioning swarm intelligence based algorithm data clustering ant colony optimization principles fuzzy c means criterion;swarm intelligence based algorithm;computer science;fuzzy ant clustering;fuzzy c means criterion;partitioning algorithms;centroid positioning	We present a swarm intelligence based algorithm for data clustering. The algorithm uses ant colony optimization principles to find good partitions of the data. In the first stage of the algorithm ants move the cluster centers in feature space. The cluster centers found by the ants are evaluated using a reformulated fuzzy c-means criterion. In the second stage the best cluster centers found are used as the initial cluster centers for the fuzzy c-means (FCM) algorithm. Results on 8 datasets show that the partitions found by FCM using the ant initialization are better optimized than those from randomly initialized FCM. Hard c-means was also used in the second stage and the partitions from the algorithm are better optimized than those from randomly initialized hard c-means.	algorithm;ant colony optimization algorithms;cluster analysis;feature vector;fuzzy clustering;fuzzy cognitive map;mathematical optimization;randomness;swarm intelligence	Parag M. Kanade;Lawrence O. Hall	2004	2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)	10.1109/FUZZY.2004.1375751	mathematical optimization;swarm intelligence;computer science;artificial intelligence;machine learning;mathematics;cluster analysis	Visualization	3.5822766021705914	-40.85132172836522	8192
3bdcec2c3b6de0193c3a9d8956b336a73c4b5c83	bio-inspired optic flow sensors based on fpga: application to micro-air-vehicles	field programmable gate array;uav;awhh;of;fov;ic;integrated circuit;intellectual property;biorobotics;emd;fpga;mav;µc;fpaa;fpga implementation;ip;asf;vhdl;lut;optic flow sensor;present day;functional model;vlsi;elementary motion detector;optical flow;visual field;visual system;micro air vehicle;spatial resolution	Tomorrow’s Micro-Air-Vehicles (MAVs) could be used as scouts in many civil and military missions without any risk to human life. MAVs have to be equipped with sensors of several kinds for stabilization and guidance purposes. Many recent findings have shown, for example, that complex tasks such as 3-D navigation can be performed by insects using optic flow (OF) sensors although insects’ eyes have a rather poor spatial resolution. At our Laboratory, we have been performing electrophysiological, micro-optical, neuroanatomical and behavioral studies for several decades on the housefly’s visual system, with a view to understanding the neural principles underlying OF detection and establishing how OF sensors might contribute to performing basic navigational tasks. Based on these studies, we developed a functional model for an Elementary Motion Detector (EMD), which we first transcribed into electronic terms in 1986 and subsequently used onboard several terrestrial and aerial robots. Here we present a Field Programmable Gate Array (FPGA) implementation of an EMD array, which was designed for estimating the OF in various parts of the visual field of a MAV. FPGA technology is particularly suitable for applications of this kind, where a single Integrated Circuit (IC) can receive inputs from several photoreceptors of similar (or different) shapes and sizes located in various parts of the visual field. In addition, the remarkable characteristics of present-day FPGA applications (their high clock frequency, large number of system gates, embedded RAM blocks and Intellectual Property (IP) functions, small size, light weight, low cost, etc.) make for the flexible design of a multi-EMD visual system and its installation onboard MAVs with extremely low permissible avionic payloads. 2007 Elsevier B.V. All rights reserved.	adobe air;array data structure;autonomous system (internet);british informatics olympiad;charge-coupled device;clock rate;digital electronics;ecosystem management decision support;embedded system;focal (programming language);field-programmable gate array;function model;integrated circuit;motion detector;obstacle avoidance;optical flow;random-access memory;robot;sampling (signal processing);sensor;television antenna;terrestrial television;top-down and bottom-up design	F. Aubépart;Nicolas H. Franceschini	2007	Microprocessors and Microsystems	10.1016/j.micpro.2007.02.004	embedded system;computer vision;biorobotics;parallel computing;simulation;telecommunications;computer science;electrical engineering;operating system;field-programmable gate array	Robotics	45.99490642464297	-34.027360044759085	8198
de6f0cd336f1a6c54400c1e77d844645b5dc98ce	entropy for intuitionistic fuzzy sets based on distance and intuitionistic index	intuitionistic fuzzy set;entropy;intuitionistic index;distance	Many researchers have put forward lots of entropy and distance measures for intuitionistic fuzzy sets (IFSs). This paper firstly reviews some basic concepts for IFSs and several widely used distance measures between IFSs. And then a series of distance measures are presented on the basis of above widely used distance measures. Based on such distance measures and intuitionistic index, a set of entropies for IFSs are proposed and proved to meet the axiomatic requirements given by Szmidt and Kacprzyk in 2001. Besides, two numerical examples are demonstrated to verify the efficiency of the proposed entropies for IFSs. Finally, the paper concludes with suggestions for future research.	fuzzy set;intuitionistic logic;numerical analysis;requirement	Huimin Zhang	2013	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488513500086	entropy;mathematical analysis;discrete mathematics;mathematics;distance	DB	-2.336928562985662	-22.150125041820257	8201
2c2520af4a172f48a80b15896645fff16ae8c5c3	decreasing weighted sorted  ${\ell_1}$ regularization	pattern clustering;regression analysis;ℓ∞ norms;dwsl1;moreau proximity operator;oscar;argument vector;decreasing weighted sorted ℓ1 regularization;dual norm;octagonal shrinkage and clustering algorithm for regression;proximal splitting algorithms;sorted ${ell_1}$ norm;structured sparsity	We consider a new family of regularizers, termed weighted sorted ℓ1 norms (WSL1), which generalizes the recently introduced octagonal shrinkage and clustering algorithm for regression (OSCAR) and also contains the ℓ1 and ℓ∞ norms as particular instances. We focus on a special case of the WSL1, the decreasing WSL1 (DWSL1), where the elements of the argument vector are sorted in non-increasing order and the weights are also non-increasing. In this letter, after showing that the DWSL1 is indeed a norm, we derive two key tools for its use as a regularizer: the dual norm and the Moreau proximity operator.	algorithm;cluster analysis;dual norm;matrix regularization;taxicab geometry	Xiangrong Zeng;Mário A. T. Figueiredo	2014	IEEE Signal Processing Letters	10.1109/LSP.2014.2331977	mathematical optimization;discrete mathematics;calculus;mathematics	ML	3.8742996634627005	-39.73172341754327	8235
87dfc07b994cfc4b7280602801bdb8a95e2d908f	moreau-yosida regularization for grouped tree structure learning	tree structure	We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-defined tree structure is based on a group-Lasso penalty, where one group is defined for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efficient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.	algorithm;ar (unix);bioinformatics;computer vision;elastic net regularization;fenchel–moreau theorem;lasso;loss function;manifold regularization;mathematical optimization;matrix regularization;optimization problem;sparse matrix;tree (data structure);tree structure	Jun Liu;Jieping Ye	2010			regularization perspectives on support vector machines;mathematical optimization;combinatorics;vantage-point tree;computer science;order statistic tree;machine learning;incremental decision tree;interval tree;mathematics;fractal tree index;tree structure;tree;tree traversal	AI	25.85078681862745	-39.45795075318257	8240
e19e590d93558d6c5d0ca55f770492d86e4c5116	minimization of akaike's information criterion in linear regression analysis via mixed integer nonlinear program		Akaike’s information criterion (AIC) is a measure of the quality of a statistical model for a given set of data. We can determine the best statistical model for a particular data set by the minimization of the AIC. Since we need to evaluate exponentially many candidates of the model by the minimization of the AIC, the minimization is unreasonable. Instead, stepwise methods, which are local search algorithms, are commonly used to find a better statistical model though it may not be the best. We propose a branch and bound search algorithm for a mixed integer nonlinear programming formulation of the AIC minimization by Miyashiro and Takano (2015). More concretely, we propose methods to find lower and upper bounds, and branching rules for this minimization. We then combine them with SCIP, which is a mathematical optimization software and a branch-and-bound framework. We show that the proposed method can provide the best statistical model based on AIC for small-sized or medium-sized benchmark data sets in UCI Machine Learning Repository. Furthermore, we show that this method finds good quality solutions for large-sized benchmark data sets.	akaike information criterion;bayesian information criterion;benchmark (computing);branch and bound;computation;emoticon;experiment;linear equation;linear programming relaxation;list of optimization software;local search (optimization);loss function;machine learning;mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;optimization problem;parallel computing;scip;search algorithm;statistical model;stepwise regression	Keiji Kimura;Hayato Waki	2018	Optimization Methods and Software	10.1080/10556788.2017.1333611	econometrics;mathematical optimization;akaike information criterion;mathematics;statistics	ML	23.332483011429527	-35.52388701314728	8253
11869aa16014898b1a4d82d396587c7976b396da	some large deviations results for latin hypercube sampling	monte carlo methods;probability;sampling methods;latin hypercube sampling;markovian dependence;monte carlo sampling;large deviation;probability;random variable;variance reduction	Large deviations theory is a well-studied area which has shown to have numerous applications. The typical results, however, assume that the underlying random variables are either i.i.d. or exhibit some form of Markovian dependence. Our interest in this paper is to study the validity of large deviations results in the context of estimators built with Latin Hypercube sampling, a well-known sampling technique for variance reduction. We show that a large deviation principle holds for Latin Hypercube sampling for functions in one dimension and for separable multi-dimensional functions. Moreover, the upper bound of the probability of a large deviation in these cases is no higher under Latin Hypercube sampling than it is under Monte Carlo sampling. We extend the latter property to functions that preserve negative dependence (such as functions that are monotone in each argument). Numerical experiments illustrate the theoretical results presented in the paper.	experiment;monte carlo method;sampling (signal processing);variance reduction;monotone	Shane S. Drew;Tito Homem-de-Mello	2005	Proceedings of the Winter Simulation Conference, 2005.		sampling;econometrics;mathematical optimization;latin hypercube sampling;stochastic optimization;mathematics;statistics;monte carlo method	HPC	28.328099229089393	-26.895815144618265	8256
7d9d096de799506e3829aef513e298d007e35c7f	weighted optimum bit allocations to orthogonal transforms for picture coding	institutional repositories;transformation cosinus;transforms encoding picture processing;fedora;image processing;quantization noise;procesamiento imagen;picture processing;analog tv weighted optimum bit allocations orthogonal transforms picture coding frequency weighted snr ccir;signal to quantization noise ratio;quantization transform coding signal to noise ratio bit rate fourier transforms optimization methods filtering discrete cosine transforms frequency tv;traitement image;transformation orthogonale;vital;transformacion ortogonal;codificacion;quality criterion;critere qualite;transformacion coseno;coding;transforms;codec;orthogonal transformation;bit allocation;cosine transform;vtls;encoding;ils;bruit quantification;codage;ruido cuantificacion;criterio calidad	The classical optimization criterion used is the signal-to-(quantization) noise ratio (SNR). A general quantization methodology is developed which is based on the optimization of a frequency-weighted SNR: first, a quality criterion based on the weighted SNR is derived from the CCIR (Comite Consultatif International de Radiodiffusion) curves for analog TV: then, a methodology for optimizing the quantization is derived in a general form. >		Benoit M. Macq	1992	IEEE Journal on Selected Areas in Communications	10.1109/49.138992	codec;speech recognition;quantization;telecommunications;image processing;computer science;discrete cosine transform;mathematics;signal-to-quantization-noise ratio;coding;encoding;statistics;orthogonal transformation	Vision	46.925407556638255	-12.45787332917375	8264
8f3a12025ff390c1460a2e560d3b6d49b190625b	permutation test for incomplete paired data with application to cdna microarray data	microarray data;permutation statistic;colorectal cancer;cdna microarray;conditional variance;incomplete paired data;differentially expressed gene;sampling theory;permutation test	A paired data set is common in microarray experiments, where the data are often incompletely observed for some pairs due to various technical reasons. In microarray paired data sets, it is of main interest to detect differentially expressed genes, which are usually identified by testing the equality of means of expressions within a pair. While much attention has been paid to testing mean equality with incomplete paired data in previous literature, the existing methods commonly assume the normality of data or rely on the large sample theory. In this paper, we propose a new test based on permutations, which is free from the normality assumption and large sample theory. We consider permutation statistics with linear mixtures of paired and unpaired samples as test statistics, and propose a procedure to find the optimal mixture that minimizes the conditional variances of the test statistics, given the observations. Simulations are conducted for numerical power comparisons between the proposed permutation tests and other existing methods. We apply the proposed method to find differentially expressed genes for a colorectal cancer study.	dna microarray;resampling (statistics)	Donghyeon Yu;Johan Lim;Feng Liang;Kyunga Kim;Byung Soo Kim;Woncheol Jang	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.08.012	microarray analysis techniques;econometrics;resampling;bioinformatics;colorectal cancer;conditional variance;mathematics;statistics	ML	28.91873433828639	-24.358331931216124	8265
5e9f8894ca82cafc424e7cb1185764026eab16b5	the development of rating methods for preference ranking of water level prediction models	water level;prediction model			Carl Steidley;Alexey L. Sadovski;Kelly Torres Knott;Rafic Bachnak	2005			predictive modelling;computer science;data mining;ranking;machine learning;water level;artificial intelligence	NLP	5.311305779496629	-23.01384541577879	8268
00dd7f713ff39a13a8beb82164543cf8ffe62de0	markerless outdoor localisation based on sift descriptors for mobile applications	mobile device;image matching;real time;building recognition;sift;scale invariant feature transform;feature extraction;augmented reality;content based image retrieval;mobile application;pose estimation	This study proposes augmented reality from mobile devices based on SIFT (Scale Invariant Feature Transform) features for markerless outdoor augmented reality application. The proposed application is navigation help in a city. These SIFT features are projected on a digital model of the building façades of the square to obtain 3D co-ordinates for each feature point. The algorithms implemented calculate the camera pose for frame of a video from 3D-2D point correspondences between features extracted in the current video frame and points in the reference dataset. The algorithms were successfully tested on video films of city squares. Although they do not operate in real-time, they are capable of a correct pose estimation and projection of artificial data into the scene. In case of a loss of track, the algorithms recover automatically. The study shows the potential of SIFT features for purely image based markerless outdoor augmented reality applications. This study takes place in the MoSAIC project.	algorithm;augmented reality;mobile device;ncsa mosaic;real-time clock;real-time computing;scale-invariant feature transform	Frank Lorenz Wendt;Stéphane Bres;Bruno Tellez;Robert Laurini	2008		10.1007/978-3-540-69905-7_50	computer vision;augmented reality;computer science;scale-invariant feature transform;multimedia;computer graphics (images)	HCI	48.8931700510909	-44.14588484509688	8276
fffdb7c06352ac2b79f64a5e83d965897f649e16	event-based encoding from digital magnetic compass and ultrasonic distance sensor for navigation in mobile systems	robot sensing systems;microcontrollers;neuromorphics;acoustics;robot sensing systems encoding acoustics compass microcontrollers neuromorphics data visualization;data visualization;encoding;compass	Event-based encoding reduces the amount of generated data while keeping relevant information in the measured magnitude. While this encoding is mostly associated with spiking neuromorphic systems, it can be used in a broad spectrum of tasks. The extension of event-based data representation to other sensors would provide advantages related to bandwidth reduction, lower computing requirements, increased processing speed and data processing. This work describes two event-based encoding procedures (magnitude-event and rate-event) for two sensors widely used in industry, especially for navigation in mobile systems: digital magnetic compass and ultrasonic distance sensor. Encoded data meet Address Event Representation (AER) format for further transmission, processing and visualization. Two encoding procedures and their associated AER conversion from sensor data are described, using an AVR microprocontroller to perform the task in real-time. Results are transmitted to a computer via USART, displayed and recorded using Matlab or jAER visualization tool. A comparison with classic transmitting and processing of sensor data is done to evaluate the convenience of the proposed encoding. Results show that data transmission bandwith can be reduced up to 95% under certain conditions, envisaging that pure or mixed regular/event-based data sensors are desirable for high speed transmission and low computer processing while keeping relevant information, which is highly desirable for mobile systems.	atmel avr;data (computing);matlab;neuromorphic engineering;real-time clock;requirement;sensor;transmitter	Juan Barrios-Aviles;Taras Iakymchuk;Alfredo Rosado Muñoz;José V. Francés;Manuel Bataller-Mompeán;Juan Guerrero-Martínez	2016	2016 IEEE 14th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2016.7819239	embedded system;computer vision;real-time computing;computer science	Visualization	16.70482712998099	-11.447137460558087	8290
3ad6d3394cf0988dab75db1024d61ea47c00ec1f	testing for nonlinearity in mean and volatility for heteroskedastic models	model specification;garch model;credible interval;markov chain monte carlo method;bayesian;simulation experiment;markov chain monte carlo methods;double threshold garch models;asymmetric volatility model;time series model	This paper proposes a simple test for threshold nonlinearity in either the mean or volatility equation, or both, of a heteroskedastic time series. Our proposal adopts existing Bayesian Markov chain Monte Carlo methods to fit a general double threshold GARCH model, which may have an explosive regime, then forms posterior credible intervals on model parameters to detect and specify threshold nonlinearity in the mean and/or volatility equations. Simulation experiments demonstrate that the method works favorably in identifying model specifications varying in complexity from the conventional GARCH up to the full double-threshold nonlinear GARCH model and is robust to over-specification in model orders. In an application to nine international financial market indices, clear evidence supporting the hypothesis of threshold nonlinearity in mean and volatility is discovered.	experiment;knowledge spillover;markov chain monte carlo;monte carlo method;nonlinear system;persistence (computer science);simulation;time series;volatility	Cathy W. S. Chen;Richard Gerlach;Amanda P. J. Tai	2008	Mathematics and Computers in Simulation	10.1016/j.matcom.2008.01.044	autoregressive conditional heteroskedasticity;econometrics;markov chain monte carlo;bayesian probability;time series;mathematics;credible interval;specification;statistics	ML	32.30466151280903	-20.366966171733107	8293
b30bc9664010f305645f42ef0cd58866adcd687e	drowsiness detection based on brightness and numeral features of eye image	video sequence;traffic accident;preventing traffic drowsiness detection eye state analysis skin color;eyelids distance;eyelids distance drowsiness detection eye image numeral feature traffic accidents preventing eye state analysis face detection eye detection drowsy decision brightness video sequence;drowsiness detection;traffic accidents preventing;eye detection;preventing traffic;eye state analysis;road safety brightness face recognition;brightness;training data;skin color;face recognition;drowsy decision;image color analysis;driver circuits;face;eye image numeral feature;face detection;road safety;iris;eyelids;brightness algorithm design and analysis face detection image sequence analysis road accidents training data cameras image analysis video sequences eyelids	Drowsiness detection is vital in preventing traffic accidents. Eye state analysis – detecting whether the eye is open or closed − is critical step for drowsiness detection. In this paper, we propose a new algorithm for eye state analysis, which we incorporate into a four step system for drowsiness detection: face detection, eye detection, eye state analysis, and drowsy decision. This new system requires no training data at any step or special cameras. Our novel eye state analysis algorithm detects open, semi-open, and closed eye during two steps which is based on brightness and numeral features of the eye image. We analyze our eye state analysis algorithm using ten video sequences and show superior results compared to the common technique based on distance between eyelids.	algorithm;face detection;semiconductor industry;sensor	Pooneh R. Tabrizi;Reza Aghaeizadeh Zoroofi	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.186	facial recognition system;face;computer vision;training set;face detection;simulation;speech recognition;computer science;brightness	Robotics	40.5171230022014	-45.41865932149714	8302
8798d7660654d944d2f111633f623104ef32d5ad	modified sparse linear-discriminant analysis via nonconvex penalties		"""This paper considers the linear-discriminant analysis (LDA) problem in the undersampled situation, in which the number of features is very large and the number of observations is limited. Sparsity is often incorporated in the solution of LDA to make a well interpretation of the results. However, most of the existing sparse LDA algorithms pursue sparsity by means of the <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{1}$ </tex-math></inline-formula>-norm. In this paper, we give elaborate analysis for nonconvex penalties, including the <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{0}$ </tex-math></inline-formula>-based and the sorted <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{1}$ </tex-math></inline-formula>-based LDA methods. The latter one can be regarded as a bridge between the <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{0}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{1}$ </tex-math></inline-formula> penalties. These nonconvex penalty-based LDA algorithms are evaluated on the gene expression array and face database, showing high classification accuracy on real-world problems."""	algorithm;anterior descending branch of left coronary artery;linear discriminant analysis;sparse matrix	Jia Cai;Xiaolin Huang	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2785324	artificial intelligence;algorithm;sparse matrix;pattern recognition;algorithm design;feature extraction;gene expression array;linear discriminant analysis;computer science	ML	26.63786973755888	-38.38465468281504	8303
b833f0bb3ae7cacfea941cb71ccf8128346b29f2	exploiting a coevolutionary approach to concurrently select training instances and learn rule bases of mamdani fuzzy systems	high dimensional dataset;fuzzy rule based system;pareto optimisation;complexity theory;regression analysis fuzzy systems genetic algorithms knowledge based systems learning artificial intelligence pareto optimisation;ucl;pareto front;rule based;training;multiobjective evolutionary learning;discovery;theses;conference proceedings;complexity theory biological cells training approximation methods accuracy indexes fuzzy sets;fuzzy sets;mamdani fuzzy systems;accuracy;indexes;soga;training set;digital web resources;biological cells;ucl discovery;open access;indexation;genetic algorithm;ucl library;genetic algorithms;regression analysis;approximation methods;learning rule mamdani fuzzy systems multiobjective evolutionary learning fuzzy rule based training set regression analysis co evolutionary approach single objective genetic algorithm soga pareto fronts moel;single objective genetic algorithm;book chapters;open access repository;learning artificial intelligence;evolutionary learning;pareto fronts;learning rule;co evolutionary approach;fuzzy systems;knowledge based systems;fuzzy system;moel;ucl research;fuzzy rule based	When applied to high dimensional datasets, multi-objective evolutionary learning (MOEL) of fuzzy rule-based systems suffers from high computational costs, mainly due to the fitness evaluation. To use a reduced training set (TS) in place of the overall TS could considerably lessen the required effort. How this reduction should be performed, especially in the context of regression, is still an open issue. In this paper, we propose to adopt a co-evolutionary approach. In the execution of the MOEL, periodically, a single-objective genetic algorithm (SOGA) evolves a population of reduced TSs. The SOGA aims to maximize a purposely-defined index which measures how much a reduced TS is representative of the overall TS in the context of the MOEL. We tested our approach on a real world high dimensional dataset. We show that the Pareto fronts generated by applying the MOEL with the overall and the reduced TSs are comparable, although the use of the reduced TS allows saving on average the 75% of the execution time.	fuzzy control system;fuzzy rule;genetic algorithm;iterative and incremental development;pareto efficiency;rule 184;rule-based system;run time (program lifecycle phase);test set	Michela Antonelli;Pietro Ducange;Francesco Marcelloni	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584292	genetic algorithm;computer science;artificial intelligence;machine learning;data mining;fuzzy control system	AI	10.749959629319394	-40.568793501918066	8306
890ac2e43b2aa7bd2223ac7059cd5e82c85f2cea	evolving hardware using a new evolutionary algorithm based on evolution of a species	optimisation methods;evolvable hardware;optimal control;bio inspired computation;mathematical modelling;evolutionary algorithms;evolutionary algorithm;robot systems;species evolution	This work presents the analysis of species evolution properties which are considered to design a new evolutionary algorithm for evolvable hardware. These properties reduce the risk of malfunctions in a physical system when it is evolving. A mathematical model, that characterises the natural evolution phenomena of a species, is proposed. A new evolutionary algorithm based on this model is proposed as well. This algorithm is designed to evolve hardware, e.g., to obtain the optimal control parameters of a real control system, while it is executing a repetitive task. The convergence of the proposed algorithm was proven by means of new theorems. Simulations and experimentation studies were carried out in order to validate the theoretical aspects and to evaluate the performance of the evolutionary algorithm.	evolutionary algorithm	Celso De La Cruz;Teodiano Freire Bastos Filho;Hector D. Patiño;Ricardo O. Carelli	2009	IJBIC	10.1504/IJBIC.2009.023812	evolutionary programming;optimal control;interactive evolutionary computation;human-based evolutionary computation;cultural algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;mathematical model;evolution strategy	Theory	26.49032065209395	-8.41122383940213	8331
f9853144061106bcbe804e0c9e5545b486bc5c27	performance limits of single-agent and multi-agent sub-gradient stochastic learning	gradient noise;gradient noise sotochastic sub gradient method affine lipschitz exponential rate diffusion strategy svm lasso;support vector machines convergence stochastic processes cost function context indexes;sotochastic sub gradient method;affine lipschitz;svm;support vector machines gradient methods learning artificial intelligence stochastic processes;lasso;diffusion strategy;exponential weighting procedure single agent subgradient stochastic learning strategy multiagent subgradient stochastic learning strategy support vector machine sparsity inducing learning solution;exponential rate	This work examines the performance of stochastic sub-gradient learning strategies, for both cases of stand-alone and networked agents, under weaker conditions than usually considered in the literature. It is shown that these conditions are automatically satisfied by several important cases of interest, including support-vector machines and sparsity-inducing learning solutions. The analysis establishes that sub-gradient strategies can attain exponential convergence rates, as opposed to sub-linear rates, and that they can approach the optimal solution within O(p), for sufficiently small step-sizes, p. A realizable exponential-weighting procedure is proposed to smooth the intermediate iterates and to guarantee these desirable performance properties.	gradient;multi-agent system;sparse matrix;support vector machine;time complexity	Bicheng Ying;Ali H. Sayed	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472610	gradient noise;support vector machine;mathematical optimization;computer science;online machine learning;machine learning;lasso;pattern recognition;mathematics;statistics	Vision	21.454276324446788	-18.999427122175405	8341
26d00529a8a5907d6ebda1d01b89513a9b20be21	mosaicking cluttered ground planes based on stereo vision	stereo vision;iterative closest point;distance transform;3d reconstruction	Recent stereo cameras provide reliable 3D reconstructions. These are useful for selecting ground-plane points, register them and building mosaics of cluttered ground planes. In this paper we propose a 2D Iterated Closest Point (ICP) registration method, based on the distance transform, combined with a fine-tuning-registration step using directly the image data. Experiments with real data show that ICP is robust to 3D reconstruction differences due to motion and the fine tuning step minimizes the effect of the uncertainty in the 3D reconstructions.	3d reconstruction from multiple images;distance transform;image stitching;iterated function;stereo camera;stereo cameras;stereopsis	José António Gaspar;Miguel Realpe;Boris Xavier Vintimilla;José Santos-Victor	2007		10.1007/978-3-540-72849-8_3	3d reconstruction;stereo cameras;computer vision;computer science;stereopsis;distance transform;iterative closest point;computer graphics (images)	Vision	52.224087593448445	-46.275836499738084	8348
390d4fb96c4ed396acd9dcf7f1d92eb08624d2f1	semi-supervised generation with cluster-aware generative models		Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of−79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.	cluster analysis;data point;generative model;mnist database;network address translation;performance;real life;semi-supervised learning;semiconductor industry;supervised learning;unsupervised learning	Lars Maaløe;Marco Fraccaro;Ole Winther	2017	CoRR		machine learning;mathematics;generative grammar;permutation;artificial intelligence;unsupervised learning;data point;generative model;cluster analysis;mnist database;data set	ML	21.672209900792172	-45.23653479224382	8353
c92268807e013a47b6bd27262a1531b0daf9bf39	capacity sizing in the presence of a common shared resource: dimensioning an inbound call center	call center design;queueing;service system;numerical analysis;staff dimensioning;resource sharing;call center	This paper studies a capacity sizing problem for service systems where capacity is determined by multiple types of resources that are required simultaneously in order to provide service. In addition to the simultaneous use of resources, the systems are characterized by the presence of a common resource that is shared across multiple types of customers. The paper focusses on inbound call centers as an important example of such systems. The capacity sizing problem in this context is one where the optimal number of servers that need to be allocated to different call types is determined. Optimality is defined as the number of servers that maximize revenues net of staffing costs. For the case where customers do not wait, it is shown that a greedy allocation procedure yields the optimal server allocation. Heuristics are proposed for the case with waiting customers that can exhibit impatience. The numerical analysis illustrates that for systems experiencing heavy loads and serving a diverse set of customers, the proposed heuristics outperform current methods that ignore the role of a shared resource in these types of dimensioning problems.	inbound marketing	O. Zeynep Aksin;Patrick T. Harker	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00274-6	shared resource;real-time computing;simulation;numerical analysis;computer science;marketing;operations management;mathematics;queueing theory;service system	Theory	4.2086878973705755	-0.3927349184628003	8373
93ba68cac1c7f6ea23422c0eba18dc14930f137c	combining a probabilistic sampling technique and simple heuristics to solve the dynamic path planning problem	multi stage;sampling methods path planning orbital robotics robots costs computer science artificial intelligence motion planning navigation computational efficiency;probability;high dimensionality;path planning;probabilistic sampling artificial intelligence motion planning rrt multi stage local search greedy heuristics;probabilistic algorithm;rrt;mobile robots;trees mathematics;greedy heuristic;greedy heuristics;artificial intelligent;dynamic environment;trees mathematics mobile robots path planning probability;sampling technique;rapidly exploring random tree;rapidly exploring random trees probabilistic sampling technique simple heuristics dynamic path planning problem single shot path planning problems;motion planning;artificial intelligence;sampling methods;local search;probabilistic sampling	Probabilistic sampling methods have become very popular to solve single-shot path planning problems. Rapidly-exploring Random Trees (RRTs) in particular have been shown to be very efficient in solving high dimensional problems. Even though several RRT variants have been proposed to tackle the dynamic replanning problem, these methods only perform well in environments with infrequent changes. This paper addresses the dynamic path planning problem by combining simple techniques in a multi-stage probabilistic algorithm. This algorithm uses RRTs as an initial solution, informed local search to fix unfeasible paths and a simple greedy optimizer. The algorithm is capable of recognizing when the local search is stuck, and subsequently restart the RRT. We show that this combination of simple techniques provides better responses to a highly dynamic environment than the dynamic RRT variants.	expectation propagation;greedy algorithm;heuristic (computer science);kinodynamic planning;local search (optimization);mathematical optimization;motion planning;online and offline;random graph;randomized algorithm;randomness;sampling (signal processing);way to go	Nicolas Arturo Barriga;Mauricio Solar;Mauricio Araya-López	2009	2009 International Conference of the Chilean Computer Science Society	10.1109/SCCC.2009.11	sampling;mathematical optimization;computer science;artificial intelligence;machine learning;motion planning	Robotics	23.470686013436882	-15.017543477476181	8388
438ab98a4a2026b5966c48804cf0c985235803c7	feature construction and feature selection in presence of attribute interactions	learning difficulties;empirical study;feature construction;prediction accuracy;genetic algorithm;genetic algorithms;feature selection;data reduction	When used for data reduction, feature selection may successfully identify and discard irrelevant attributes, and yet fail to improve learning accuracy because regularities in the concept are still opaque to the learner. In that case, it is necessary to highlight regularities by constructing new characteristics that abstract the relations among attributes. This paper highlights the importance of feature construction when attribute interaction is the main source of learning difficulty and the underlying target concept is hard to discover by a learner using only primitive attributes. An empirical study centered on predictive accuracy shows that feature construction significantly outperforms feature selection because, even when done perfectly, detection of interacting attributes does not sufficiently facilitates discovering the target concept.	feature selection;interaction	Leila Shila Shafti;Eduardo Fazolino Perez	2009		10.1007/978-3-642-02319-4_71	genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection;feature	Logic	6.759957844114222	-43.122674591562756	8389
4c64752918ee9a4804308c94b8249588ade18f04	implementation and application of maximum likelihood reliability estimation from subsystem and full system tests	reliability metric;performance evaluation;maximum likelihood estimation;test and evaluation;series and non series systems	This paper provides an overview and examples of a novel and practical method for estimating the reliability of a complex system, with confidence regions, based on a combination of full system and subsystem (and/or component or other) tests. It is assumed that the system is composed of multiple processes, where the subsystems may be arranged in series, parallel, combination series/parallel, or other mode. Maximum likelihood estimation (MLE) is used to estimate the overall system reliability based on the fusion of system and subsystem test data. The method is illustrated on two real-world systems: an aircraft-missile system and a highly reliable low-pressure coolant injection system in a commercial nuclear-power reactor. The examples are used to demonstrate the following properties of the method. One, increasing the number of full system tests improves the confidence in the full system reliability estimate. Two, increasing the number of tests of one subsystem stabilizes the subsystem reliability estimate. Three, the likelihood function and optimization constraints can readily be modified to handle systems consisting of repeated components in a mixed series/parallel configuration. Four, the asymptotic normal assumption for computing confidence intervals is not always appropriate, especially for highly reliable systems. Five, performing a mixture of full system and subsystem tests is important when the model that relates the subsystem reliability to the full system reliability is uncertain.	complex system;mathematical optimization;parallel computing;reactor (software);series and parallel circuits;system testing;test data;world-system	Coire J. Maranzano;James C. Spall	2010		10.1145/2377576.2377604	reliability engineering;mathematical optimization;computer science;statistics	HPC	27.886510228732355	-16.80531430357622	8405
c7c8d150ece08b12e3abdb6224000c07a6ce7d47	demeshnet: blind face inpainting for deep meshface verification		MeshFace photos have been widely used in many Chinese business organizations to protect ID face photos from being misused. The occlusions incurred by random meshes severely degenerate the performance of face verification systems, which raises the MeshFace verification problem between MeshFace and daily photos. Previous methods cast this problem as a typical low-level vision problem, i.e., blind inpainting. They recover perceptually pleasing clear ID photos from MeshFaces by enforcing pixel level similarity between the recovered ID images and the ground-truth clear ID images and then perform face verification on them. Essentially, face verification is conducted on a compact feature space rather than the image pixel space. Therefore, this paper argues that pixel level similarity and feature level similarity jointly offer the key to improve the verification performance. Based on this insight, we offer a novel feature oriented blind face inpainting framework. Specifically, we implement this by establishing a novel DeMeshNet, which consists of three parts. The first part addresses blind inpainting of the MeshFaces by implicitly exploiting extra supervision from the occlusion position to enforce pixel level similarity. The second part explicitly enforces a feature level similarity in the compact feature space, which can explore informative supervision from the feature space to produce better inpainting results for verification. The last part copes with face alignment within the net via a customized spatial transformer module when extracting deep facial features. All three parts are implemented within an end-to-end network that facilitates efficient optimization. Extensive experiments on two MeshFace data sets demonstrate the effectiveness of the proposed DeMeshNet as well as the insight of this paper.	end-to-end principle;experiment;feature extraction;feature vector;high- and low-level;information;inpaint;inpainting;mathematical optimization;pixel;transformer	Shu Zhang;Ran He;Tieniu Tan	2018	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2017.2763119	pattern recognition;inpainting;pixel;artificial intelligence;computer vision;facial recognition system;computer science;polygon mesh;deep learning;feature extraction;feature vector;data set	Vision	25.934761363247265	-49.33238983249735	8418
a2af99e0402ebf15655b0341ef910e6d2ff8af95	trapezoidal intuitionistic fuzzy bonferroni means and its application in multi-attribute decision making	multi attribute decision making bonferroni mean trapezoidal intuitionistic fuzzy number ranking of trapezoidal intuitionistic fuzzy number;operations research;operations research decision making fuzzy logic fuzzy set theory number theory;fuzzy set theory;fuzzy logic;number theory;gold decision making indexes aggregates equations electronic mail;trapezoidal intuitionistic fuzzy bonferroni means weighted trifbm application madm technique multiattribute decision making technique trifn ranking method modeling capability trapezoidal intuitionistic fuzzy numbers trifbm operator intuitionistic trapezoidal fuzzy environment bonferroni mean aggregation operator multiattribute decision making process	The aim of this study is to investigate Bonferroni mean (BM) aggregation operator under intuitionistic trapezoidal fuzzy environment. BM operator is more applicable in decision making process due to the fact that it has the capability to capture the interrelationship between input arguments. In view of this, a trapezoidal intuitionistic fuzzy BM (TrIFBM) operator is developed here, which is represented as trapezoidal intuitionistic fuzzy numbers (TrIFNs). The desirable properties of TrIFBM are also studied. Further, TrIFBM has been generalized by replacing its inner and outer mean with other type aggregation operators. To explore its modeling capability some examples have been given. A new ranking method of TrIFNs has been proposed. Finally, weighted TrIFBM is developed and a multi-attribute decision making (MADM) technique is proposed. A practical example is presented to illustrate the application of weighted TrIFBM in MADM.	intuitionistic logic	Bapi Dutta;Debashree Guha	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622367	fuzzy logic;number theory;discrete mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;data mining;mathematics;fuzzy set;fuzzy set operations;algorithm	Robotics	-2.4307360785538807	-21.236705163583995	8427
1bd90f92fd3e21e0a2cab09ff891a8ed90b23187	accuracy of a high-level, loss-tolerant video fingerprint for surveillance authentication	public key cryptography;video surveillance;lossy video authentication high level loss tolerant video fingerprint length video surveillance authentication multiple public safety agencies video feeds cryptographic means transcoded transmissions wireless transmissions udp public key cryptography;video surveillance fingerprint identification public key cryptography;fingerprint recognition authentication robustness streaming media time series analysis cameras multimedia communication;fingerprint identification	There are cases where surveillance video must be transmitted in the clear, which is without encryption, such as when multiple public safety agencies use the same video feeds. To ensure trust of the received video, it must be authenticated. While lossless video is straightforward to authenticate by cryptographic means, lossy video as may result from UDP, wireless, or transcoded transmissions, is more difficult to authenticate. We describe a method that combines a concise and efficiently computed video fingerprint with public key cryptography to enable lossy video authentication. Experimental results show how video fingerprint length relates to successful authentication rate and latency of its calculation.	authentication;closed-circuit television;digital video fingerprinting;encryption;feedback;fingerprint (computing);high- and low-level;lossless compression;lossy compression;plaintext;public-key cryptography	Jennifer Ren;Lawrence O'Gorman	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		fingerprint;computer science;video tracking;internet privacy;public-key cryptography;computer security;computer network	Mobile	38.43048355509716	-13.169142603713137	8440
b0002800631118099b962a21c905faca3727ddad	orientation selective subband decomposition and stack-run coding	image coding filter bank testing image reconstruction;image coding;filter bank;data compression;band pass filters;reconstructed images orientation selective subband decomposition stack run coding separable filter banks nonseparable filter bank image compression separable octave band decomposition high frequency subband orientation selective filter bank image data coding subjective testing;runlength codes image coding image reconstruction data compression band pass filters filtering theory;image compression;image reconstruction;runlength codes;orientation selectivity;high frequency;filtering theory	We compare using traditional separable filter banks with a simple non-separable filter bank with orientation selectivity, for image compression. The image is decomposed into 19 subbands (6-level) using a separable octave-band decomposition. The high-frequency subband at each decomposition level is split into two bands with an orientation selective filter bank. The image data is then coded using stack-run coding. Results based on subjective testing showed an improvement in the oriented data of the reconstructed images.		Magatho A. Mello;Roberto H. Bamberger;Thomas R. Fischer	1997		10.1109/ICIP.1997.638842	data compression;iterative reconstruction;computer vision;speech recognition;image compression;computer science;high frequency;filter bank;mathematics;band-pass filter;separable filter;composite image filter;algorithm	Theory	44.28986558556957	-15.38896308266222	8445
c3e2564d41cc1d7e5cb9c8901265024e0b696aa0	visual understanding with rgb-d sensors: an introduction to the special issue		In recent years, we have witnessed the rapid growth of research in visual understanding with RGB-D sensors since the release of Micrsoft’s Kinect sensor in November 2010. For a long time, researchers have been challenged by many visual understanding problems such as detecting and identifying objects or human activities in real-world situations. Traditional segmentation and tracking algorithms are not always reliable when the environment is cluttered or the illumination changes suddenly. However, the effective combination of depth and RGB data can alleviate the negative effects of environmental changes, thus improving the accuracy of object identification and tracking. The freely available online Kinect SDKs and pose trackers for environment modeling further encourage novel solutions to traditional visual understanding problems. However, Kinect sensors face a number of specific challenges, such as missing or corrupted depth pixels and inaccurate calibration between depth and RGB cameras. This special issue is specifically dedicated to new algorithms and applications based on the Kinect sensors. The goals of this special issue are twofold: (1) provide a survey on the progress of visual understanding with RGB-D sensors in the past years, and (2) introduce novel research and discuss new applications with the RGB-D sensors. We believe it serves as a convincing forum for researchers and practitioners to disseminate their latest research on visual understanding with RGB-D sensors.	algorithm;kinect;pixel;sensor;software development kit	Richang Hong;Shuicheng Yan;Zhengyou Zhang	2015	ACM TIST	10.1145/2732265	machine learning;artificial intelligence;rgb color model;computer science	AI	39.606029092250836	-42.71477883378469	8447
77c056b2099809783b215763739d08c51e77f7dd	a feature associative processor for image recognition based on a-d merged architecture	image recognition	An Image Feature Associative Processor (IFAP), which extracts local and grobal features of input image data based on bio-inspired parallel architecture, is proposed. It consists of an image sensor, a cellular automaton and pattern matching processors based on PWM analogdigital merged circuits. IFAP extracts image features at a standard video frame rate with low power dissipation.	british informatics olympiad;cellular automaton;central processing unit;computer vision;image sensor;parallel computing;pattern matching	Atsushi Iwata;Makoto Nagata;Hiroyuki Nakamoto;Noriaki Takeda;Mitsuru Homma;Hiroto Higashi;Takashi Morie	1999		10.1007/978-0-387-35498-9_8	computer vision;feature detection;feature extraction;computer science;pattern recognition;feature	EDA	44.79305653111909	-34.81490098577315	8450
d5d9726ac5eda8fef05eeb00ea1bb7eb7a588718	distribution-dependent sample complexity of large margin learning	supervised learning;linear classifiers;distribution dependence;sample complexity	We obtain a tight distribution-specific characterization o f the sample complexity of large-margin classification withL2 regularization: We introduce the margin-adapted dimension , which is a simple function of the second order statistics of the data distr ibution, and show distribution-specific upperand lower bounds on the sample complexity, both governed by the m argin-adapted dimension of the data distribution. The upper bounds are universa l, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent fea ures. We conclude that this new quantity tightly characterizes the true sample complexity of largemargin classification. To prove the lower bound, we develop several new tools of independent interest . These include new connections between shattering and hardness of learning, new properties o f shattering with linear classifiers, and a new lower bound on the smallest eigenvalue of a random Gram ma trix generated by sub-Gaussian variables. Our results can be used to quantitatively compar e large margin learning to other learning rules, and to improve the effectiveness of methods that use s ample complexity bounds, such as active learning.	linear classifier;sample complexity;trix (operating system)	Sivan Sabato;Nathan Srebro;Naftali Tishby	2013	Journal of Machine Learning Research		combinatorics;sample exclusion dimension;computer science;machine learning;mathematics;supervised learning;statistics	ML	20.657860345457	-33.044333367595925	8457
5f3bdb2769daeda0d6ce8260a609b06b52c137a1	belief propagation for detecting moving objects from a moving platform.	moving object;belief propagation		belief propagation;sensor;software propagation	Chung-Ching Lin;Marilyn Wolf	2010			computer vision;belief propagation;computer science;artificial intelligence	Vision	51.11771537570926	-42.85187043693452	8482
96f49862b7e0c0ae298e9b197ded72046f2f9c60	continuously heterogeneous hyper-objects in cryo-em and 3-d movies of many temporal dimensions		Single particle cryo-electron microscopy (EM) is an increasingly popular method for determining the 3-D structure of macromolecules from noisy 2-D images of single macromolecules whose orientations and positions are random and unknown. One of the great opportunities in cryo-EM is to recover the structure of macromolecules in heterogeneous samples, where multiple types or multiple conformations are mixed together. Indeed, in recent years, many tools have been introduced for the analysis of multiple discrete classes of molecules mixed together in a cryo-EM experiment. However, many interesting structures have a continuum of conformations which do not fit discrete models nicely; the analysis of such continuously heterogeneous models has remained a more elusive goal. In this manuscript we propose to represent heterogeneous molecules and similar structures as higher dimensional objects. We generalize the basic operations used in many existing reconstruction algorithms, making our approach generic in the sense that, in principle, existing algorithms can be adapted to reconstruct those higher dimensional objects. As proof of concept, we present a prototype of a new algorithm which we use to solve simulated reconstruction problems.	3d film;agent-based model;algorithm;electron tomography;prototype;reconstruction filter;triune continuum paradigm	Roy R. Lederman;Amit Singer	2017	CoRR		artificial intelligence;machine learning;mathematics;algorithm	ML	33.33998080581807	-37.570705659827226	8488
c0c807b59e6497fe07de537d9eb11fdbd442ecf6	addressing big data time series: mining trillions of time series subsequences under dynamic time warping	lower bounds;time series;similarity search	Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.	anomaly detection;big data;cluster analysis;data mining;dynamic time warping;euclidean distance;experiment;image scaling;image warping;real-time clock;search algorithm;sequence motif;similarity search;subroutine;time series	Thanawin Rakthanmanon;Bilson J. L. Campana;Abdullah Mueen;Gustavo E. A. P. A. Batista;M. Brandon Westover;Qiang Zhu;Jesin Zakaria;Eamonn J. Keogh	2013	TKDD	10.1145/2500489	computer science;data science;machine learning;time series;data mining;statistics	ML	-2.5780806740414617	-37.344936993585684	8503
580274a791594300da506e14b03b676c79ff819b	identification of irregular motion in automotive navigation systems using novelty detection	automobiles;testing;navigation;robustness;transmission line matrix methods;algorithm design and analysis	Automated display testing for visual unpleasant and erroneous navigation sequences is an important step to preserve a high quality standard for premium vehicle manufacturers. This paper presents a novel error detection algorithm for navigation sequences based on novelty detection on motion parameters obtained from real world navigation sequences. Motion parameters are accumulated through key point matching using BRISK and subsequent homography calculation. With these parameters one is able to describe the motion between two successive frames. Combinations of translational and rotational components allow the novelty detection algorithm to predict outliers. These outliers either show positioning errors or abnormal motion behaviour which are both unacceptable for high quality. Experimental results demonstrate that this algorithm works significantly better than state of the art, where one has to know errors before analysing the data set in order to determine thresholds for particular errors. The gains in precision and recall are 49.67% and 6.06% respectively, the accuracy is 1.37% higher compared to optimized threshold results.	algorithm;automotive navigation system;cartography;display resolution;error detection and correction;homography (computer vision);image resolution;novelty detection;precision and recall;requirement;standard map;thresholding (image processing)	Martin Pollot;Dominic Springer;Ralph Schleifer;Dieter Niederkorn;André Kaup	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7850000	computer vision;simulation;engineering;machine learning	Vision	45.321778664122135	-44.4316210383453	8514
0891d7080f80c7d120e52649dd9b3e4c58deb565	a new fuzzy twin support vector machine for pattern classification		Fuzzy SVM is often used to solve the problem that patterns belonging to one class often play more significant roles in classification. In order to improve the efficiency and performance of fuzzy SVM, this paper proposes a new fuzzy twin support vector machine (NFTSVM) for binary classification, in which fuzzy neural networks and twin support vector machine (TWSVM) are incorporated. By design, the influence of the samples with high uncertainty can be mitigated by employing fuzzy membership to weigh the margin of each training sample, which improves the generalization ability. In addition, we show that the existing TWSVM and twin bounded support vector machines (TBSVM) are special cases of the proposed NFTSVM when the parameters of NFTSVM are appropriately selected. Moreover, the successive overrelaxation (SOR) technique is adopted to solve the quadratic programming problems (QPPs) in the proposed NFTSVM algorithm to speed up the training procedure. Experimental results obtained on several artificial and real-world datasets validate the feasibility and effectiveness of the proposed method.	support vector machine	Sugen Chen;Xiaojun Wu	2018	Int. J. Machine Learning & Cybernetics	10.1007/s13042-017-0664-x	fuzzy classification;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set operations;structured support vector machine	ML	13.554660684839375	-40.12432617475813	8526
c3139e0e1cee764b3b149700ddf7f58ededb41db	improving graph-based image classification by using emerging patterns as attributes	graph based classification;approximate graph mining;emerging patterns;approximate graph matching	In recent years, frequent approximate subgraph (FAS) mining has been used for image classification. However, using FASs leads to a high dimensional representation. In order to solve this problem, in this paper, we propose using emerging patterns for reducing the dimensionality of the image representation in this approach. Using our proposal, a dimensionality reduction over 50% of the original patterns is achieved, additionally, better classification results are obtained. HighlightsWe combine FASs together with emerging patterns for image classification.To the best of our knowledge, this is the first work that proposes such combination.A dimensionality reduction of over 50% of the original patterns is achieved.Improvements on classification results are achieved.	computer vision	Niusvel Acosta-Mendoza;Andrés Gago Alonso;Jesús Ariel Carrasco-Ochoa;José Francisco Martínez Trinidad;José Eladio Medina-Pagola	2016	Eng. Appl. of AI	10.1016/j.engappai.2016.01.030	machine learning;pattern recognition;data mining	AI	12.901360446992783	-47.74072366665916	8535
c1859bafbaec2c189b46a3f195ea1caf7a10f1f2	missing categorical data imputation approach based on similarity	heart;information systems;data mining;rough set data mining missing data imputation similarity;uci benchmark data sets missing categorical data imputation approach missing data data mining missing categorical data imputation based on similarity mibos similarity model object similarity matrix;accuracy;similarity;single photon emission computed tomography;rough set;missing data imputation;algorithm design and analysis;accuracy information systems data mining single photon emission computed tomography heart algorithm design and analysis data models;data models	Imputation for missing data is an important task of data mining, which may influence the data mining result. In this paper, Missing Categorical Data Imputation Based on Similarity (MIBOS) is proposed to solve this problem. The algorithm defines a similarity model between objects with incomplete data, constructing the similarity matrix of objects and further gets the nearest undifferentiated object sets of each object to impute the missing data iteratively. In the imputing process, the imputed value will be directly applied to the same iteration and the following iterations. Experiments with three UCI benchmark data sets show the improvement of the proposed algorithm from perspectives of complete rate, accuracy and time efficiency.	benchmark (computing);categorical variable;data mining;experiment;geo-imputation;iteration;k-nearest neighbors algorithm;missing data;rough set;similarity measure	Sen Wu;Xiaodong Feng;Yushan Han;Qiang Wang	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378177	data modeling;algorithm design;rough set;similarity;missing data;computer science;machine learning;pattern recognition;data mining;mathematics;accuracy and precision;imputation;information system;heart;statistics	DB	1.5668156866226672	-39.22875029598702	8538
6abb12e65a69729397b51b77e7d6c3eb0782cf6c	optimal production control of a failure-prone machine	time dependent;unreliable machine;exponential distribution;inventory model;optimal control;production control;variational analysis;threshold control;infinite horizon	We consider a problem of optimal production control of a single unreliable machine. The objective is to minimize a discounted convex inventory/backlog cost over an infinite horizon. Using the variational analysis methodology, we develop the necessary conditions of optimality in terms of the co-state dynamics. We show that an inventorythreshold control policy is optimal when the work and repair times are exponentially distributed, and demonstrate how to find the value of the threshold in this case. We consider also a class of distributions concentrated on finite intervals and prove properties of the optimal trajectories, as well as properties of an optimal inventory threshold that is time dependent in this case.	karush–kuhn–tucker conditions;numerical analysis;optimal control;time complexity;variational analysis	Eugene Khmelnitsky;Ernst Presman;Suresh P. Sethi	2011	Annals OR	10.1007/s10479-009-0668-3	exponential distribution;mathematical optimization;optimal control;variational analysis;operations management;mathematics;mathematical economics	ML	5.857483014896003	-1.2429407380312194	8544
d2a02957f38acdf05e98d1545480776375f77b1a	extrinsic calibration of 3d range finder and camera without auxiliary object or human intervention		Fusion of heterogeneous extroceptive sensors is the most effient and effective way to representing the environment precisely, as it overcomes various defects of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems should be available before precisely fusing the multisensor information. Researchers have proposed several approaches to estimating the extrinsic parameters. These approaches require either auxiliary objects, like chessboards, or extra help from human to select correspondences. In this paper, we proposed a novel extrinsic calibration approach for the extrinsic calibration of range and image sensors. As far as we know, it is the first automatic approach with no requirement of auxiliary objects or any human interventions. First, we estimate the initial extrinsic parameters from the individual motion of the range finder and the camera. Then we extract lines in the image and point-cloud pairs, to refine the line feature associations by the initial extrinsic parameters. At the end, we discussed the degenerate case which may lead to the algorithm failure and validate our approach by simulation. The results indicate high-precision extrinsic calibration results against the ground-truth.	algorithm;camera resectioning;download;image sensor;plug and play;point cloud;simulation	Qinghai Liao;Ming Liu;Lei Tai;Haoyang Ye	2017	CoRR		computer vision;camera auto-calibration;optics;remote sensing	Robotics	53.269678231237734	-41.90928757785442	8546
3c68e1d55bf46c75ce813e26f721a0e094afe54d	speeding up moving-target search	mt adaptive a;video games;search method;video game;d lite;heuristic search;target;incremental heuristic search;moving target search;planning with the freespace assumption;hunter;unknown terrain	In this paper, we study moving-target search, where an agent (= hunter) has to catch a moving target (= prey). The agent does not necessarily know the terrain initially but can observe it within a certain sensor range around itself. It uses the strategy to always move on a shortest presumed unblocked path toward the target, which is a reasonable strategy for computer-controlled characters in video games. We study how the agent can find such paths faster by exploiting the fact that it performs A* searches repeatedly. To this end, we extend Adaptive A*, an incremental heuristic search method, to moving-target search and demonstrate experimentally that the resulting MT-Adaptive A* is faster than isolated A* searches and, in many situations, also D* Lite, a state-of-the-art incremental heuristic search method. In particular, it is faster than D* Lite by about one order of magnitude for moving-target search in known and initially unknown mazes if both search methods use the same informed heuristics.	a* search algorithm;adobe flash lite;experiment;heuristic (computer science);incremental heuristic search;prey	Sven Koenig;Maxim Likhachev;Xiaoxun Sun	2007		10.1145/1329125.1329353	interpolation search;beam search;bidirectional search;simulation;heuristic;computer science;artificial intelligence;machine learning;jump search;incremental heuristic search;iterative deepening depth-first search;best-first search	AI	19.806727405477588	-11.652788242118513	8558
4d1cb7e1d30f8b4c303c5297cd9b3fef2470a556	a plaintext-related image encryption algorithm based on chaos	image encryption;piecewise linear chaotic map;plaintext-related scrambling;identical encryption and decryption algorithm;security analysis	A symmetric key image cryptosystem based on the piecewise linear map is presented in this paper. In this cryptosystem, the encryption process and the decryption process are exactly same. They both include the same operations of plaintext-related scrambling once, diffusion twice and matrix rotating of 180 degrees four times. The length of secret key in the system is 64d where d is a positive integer. The proposed system can fight against the chosen/known plaintext attacks due to the using of plaintext-related scrambling. The simulate results and comparison analysis show that the proposed system has many merits such as high encryption/decryption speed, large key space, strong key sensitivity, strong plaintext sensitivity, strong cipher-text sensitivity, good statistical properties of cipher images, and large cipher-text information entropy. So the proposed system can be applied to actual communications.	cipher;cryptosystem;encryption;entropy (information theory);key (cryptography);key space (cryptography);known-plaintext attack;piecewise linear continuation;plaintext;simulation;strong key;symmetric-key algorithm	Yong Zhang;Yingjun Tang	2017	Multimedia Tools and Applications	10.1007/s11042-017-4577-1	40-bit encryption;computer science;ciphertext;theoretical computer science;null cipher;deterministic encryption;plaintext-aware encryption;key clustering;plaintext;probabilistic encryption	Crypto	38.57087625468022	-8.741008969270544	8571
8527523983fe46dc73214ab04b8cf57466eecee4	a self-learning algorithm for predicting bus arrival time based on historical data model	unsupervised learning;neural nets;historical data model bus arrival time prediction gps bp neural network;backpropagation;traffic information systems;global positioning system;bus arrival time information prediction arrival time prediction average speed prediction bp neural network road sections gps senor bus locations bus speeds self learning prediction algorithm transit user satisfaction ridership historical data model;prediction algorithms training vehicles classification algorithms biological neural networks roads;unsupervised learning backpropagation global positioning system neural nets road vehicles traffic information systems;road vehicles	The provision of timely and accurate bus arrive time information is very important. It helps to attract additional ridership and increase the satisfaction of transit users. In this paper, a self-learning prediction algorithm is proposed based on historical data model. Locations and speeds of the bus are periodically obtained from GPS senor installed on the bus and stored in database. Historical travel time in all road sections is collected. These historical data are trained using BP neural network to predict the average speed and arrival time of the road sections. Experimental results indicate that the proposed algorithm achieves outstanding prediction accuracy compared with general solutions based on historical travel time.	algorithm;artificial neural network;data model;global positioning system;real-time transcription;time of arrival	Jian Pan;Xiuting Dai;Xiaoqi Xu;Yanjun Li	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664555	unsupervised learning;simulation;global positioning system;computer science;backpropagation;machine learning	Robotics	8.686458884124363	-15.305896419575475	8576
84d872925c4cc0c95fbd7ace6c849fd554670f64	"""comment on """"on convex vector optimization problems with possibilistic weights"""" fuzzy sets and systems 51 (1992) 289-294"""	optimal solution;fuzzy set;vector optimization problems;α possibly optimal;possibilistic variables;vector optimization	Abstract   In the article above, Hussein characterized the convex vector optimization problem and specified the concept of an α-possibly optimal solution for the parametric approach with possibilistic data. However, there is some confusion and error in the article.	fuzzy sets and systems;mathematical optimization;vector optimization	Shijing Zhu;Ting Chen	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00159-5	mathematical optimization;discrete mathematics;computer science;artificial intelligence;machine learning;mathematics;fuzzy set;vector optimization	ML	-0.5573977444876163	-18.229980826856046	8582
318db1a13521c4fc56979e2e77f942a15bdec732	from an individual to a population: an analysis of the first hitting time of population-based evolutionary algorithms	time average;population;text;dk atira pure researchoutput researchoutputtypes contributiontojournal article;evolutionary computation;probability;exponential time;algorithm complexity;temps polynomial;combinatorial optimization problems;time complexity;helium;complejidad algoritmo;algorithm design and analysis evolutionary computation polynomials software engineering computer science;population size;combinatorial optimization problem;combinatorial mathematics computational complexity evolutionary computation probability;first hitting time;promedio temporal;analyse temporelle;indexing terms;algoritmo genetico;analisis temporal;software engineering;polynomials;time analysis;optimisation combinatoire;combinatorial problem;temps calcul;probleme combinatoire;complexite temps;problema combinatorio;complexite algorithme;computational complexity;poblacion;polynomial time;algorithme genetique;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;computer science;evolutionary algorithm;tiempo computacion;computation time;complejidad tiempo;combinatorial optimization;probability first hitting time population based evolutionary algorithms time complexity average computation time combinatorial optimization problems exponential time polynomial time;combinatorial mathematics;algorithm design and analysis;average computation time;population based evolutionary algorithms;moyenne temporelle;optimizacion combinatoria;tiempo polinomial	Almost all analyses of time complexity of evolutionary algorithms (EAs) have been conducted for(1 + 1) EAs only. Theoretical results on the average computation time of population-based EAs are few. However, the vast majority of applications of EAs use a population size that is greater than one. The use of population has been regarded as one of the key features of EAs. It is important to understand in depth what the real utility of population is in terms of the time complexity of EAs, when EAs are applied to combinatorial optimization problems. This paper compares(1 + 1) EAs and( + ) EAs theoretically by deriving their first hitting time on the same problems. It is shown that a population can have a drastic impact on an EA’s average computation time, changing an exponential time to a polynomial time (in the input size) in some cases. It is also shown that the first hitting probability can be improved by introducing a population. However, the results presented in this paper do not imply that population-based EAs will always be better than(1 + 1) EAs for all possible problems.	combinatorial optimization;computation;evolutionary algorithm;information;mathematical optimization;polynomial;population;time complexity	Jun He;Xin Yao	2002	IEEE Trans. Evolutionary Computation	10.1109/TEVC.2002.800886	time complexity;mathematical optimization;combinatorics;combinatorial optimization;computer science;evolutionary algorithm;mathematics;algorithm;evolutionary computation	Theory	27.967991135405736	2.6523576335110457	8609
82027159db1cc2cc72171c12f5e667225ae61746	hiding data reversibly in an image via increasing differences between two neighboring pixels	protection information;tecnologia electronica telecomunicaciones;steganographie;information hiding;signal distortion;useful information;informacion util;distorsion signal;qualite image;steganography;esteganografia;proteccion informacion;information protection;image quality;calidad imagen;tecnologias;grupo a;information utile;reversible data hiding;distorsion senal	This paper proposes a simple, efficient method that, based on increasing the differences between two neighboring pixels, losslessly embeds a message into a host image. The point at which the number of pixel differences in the image is at a maximum is selected to embed the message. The selected difference is increased by 1 or left unchanged if the embedded bit is “1” or “0”, respectively. On the other hand, differences larger than the selected difference are increased by 1. Increasing a difference is done by adding 1 to or subtracting 1 from the pixel if its value is larger or smaller than its preceding pixel, respectively. Experimental results show that the proposed method can achieve a high payload capacity while the image distortion of the stego-image remains minimal.	pixel	Ching-Chiuan Lin;Nien-Lin Hsueh	2007	IEICE Transactions	10.1093/ietisy/e90-d.12.2053	image quality;sum of absolute differences;telecommunications;computer science;mathematics;steganography;programming language;information hiding;computer security;information protection policy;statistics	HCI	44.05746041605024	-11.598917175427617	8616
8d5e423de46f9e444b23d8407e94e15b19f84873	experience with a successful system for forecasting and inventory control	inventory control	The wealth of management-science literature dealing with optimal solutions to the inventory-control problem would be more impressive if there were not a dearth of evidence of successful applications of these tools. As a step into this gap, this paper describes experiences with an evolutionary approach to a system for the centralized control of inventories of finished products in several branch warehouses; it embodies forecasting and inventory ordering, and is characterized by a man-computer partnership. Both manual overrides of the computer-generated order parameters and exception reporting a Low significant cost savings with no degradation of service. A very simple inventory-control doctrine allowed the operating personnel to gain confidence in the system—and management science in general. Based on this confidence, a simple simulation model was developed to permit evaluating alternative simple algorithms.	inventory control	Jack L. Bishop	1974	Operations Research	10.1287/opre.22.6.1224	inventory control;simulation;economics;marketing;operations management;mathematics;management;operations research	Robotics	6.301193146814285	-4.441862176781077	8631
3158a150badf494bd4433102cb55a2d486398dfa	fast implementations of fuzzy arithmetic operations using fast fourier transform (fft)	fuzzy number;fast fourier transform;fuzzy logic;fast algorithm;membership function;fuzzy system	Abstract   In engineering applications of fuzzy logic, the main goal is not to simulate the way the experts really think, but to come up with a good engineering solution that would (ideally) be better than the expert's control. In such applications, it makes perfect sense to restrict ourselves to simplified approximate expressions for membership functions. If we need to perform arithmetic operations with the resulting fuzzy numbers, then we can use simple and fast algorithms that are known for operations with simple membership functions.  In other applications, especially the ones that are related to humanities, simulating experts is one of the main goals. In such applications, we must use membership functions that capture every nuance of the expert's opinion; these functions are therefore complicated, and fuzzy arithmetic operations with the corresponding fuzzy numbers become a computational problem.  In this paper, we design a new algorithm for performing such operations. This algorithm uses Fast Fourier Transform (FFT) to reduce computation time from O( n  2 ) to O( n log( n )) (where  n  is the number of points  x  at which we know the membership functions  μ ( x )). To compute FFT even faster, we propose to use special hardware.  The results of this paper were announced in the work of Kosheleva et al. [Proc. 1996 IEEE Int. Conf. on Fuzzy Systems, Vol. 3, pp. 1958–1964].	fast fourier transform	Olga Kosheleva;Sergio D. Cabrera;Glenn A. Gibson;Misha Koshelev	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00147-4	fuzzy logic;fast fourier transform;mathematical optimization;membership function;defuzzification;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;theoretical computer science;machine learning;mathematics;fuzzy set;prime-factor fft algorithm;fuzzy set operations;algorithm;fuzzy control system	Theory	-4.4145233788828975	-23.292167559537106	8637
b6c16f38c90bcf6bc0bcd316c7dbdc4902c1f9ef	comparison of several approaches for the segmentation of texture images	correlacion;energy;metodo adaptativo;learning rate;inertie;entropia;image segmentation;fuzzy k means;fault tolerant;analisis textura;supervised learning;fonction repartition;energia;algoritmo borroso;real time;k means;logique floue;logica difusa;fuzzy kohonen sofm;methode adaptative;texture segmentation;homogeneidad;fuzzy art2;fuzzy logic;optimization problem;funcion distribucion;distribution function;texture analysis;energie;inercia;self organized feature map;fonction appartenance;fuzzy algorithm;entropie;adaptive method;membership function;algorithme flou;self organization;k means algorithm;entropy;homogeneite;funcion pertenencia;correlation;analyse texture;adaptive resonance theory;inertia;homogeneity	Abstract   In this paper, several approaches including K-Means, Fuzzy K-Means (FKM), Fuzzy Adaptive Resonance Theory (ART2) and Fuzzy Kohonen Self-Organizing Feature Mapping (SOFM) are adapted to segment the texture image. In our tests, five features, energy, entropy, correlation, homogeneity, and inertia, are used in texture analysis. The K-Means algorithm has the following disadvantages: (i) slow real-time ability, (ii) unstability. The FKM algorithm has improved the performance of the unstability by means of the introduction of fuzzy distribution functions. The Fuzzy ART2 has advantages, such as unsupervised training, low computation, and great degree of fault tolerance (stability/plasticity). Fuzzy operator and mapping functions are added into the network to improve the generality. The Fuzzy SOFM integrates the FKM algorithm into fuzzy membership value as learning rate and updating strategies of the Kohonen network. This yields automatic adjustment of both the learning rate distribution and update neighborhood, and has an optimization problem related to FKM. Therefore, the Fuzzy SOFM is independent of the sequence of feed of input patterns whereas final weight vectors by the Kohonen method depend on the sequence. The Fuzzy SOFM is “self-organizing” since the “size” of the update neighborhood and learning rate are automatically adjusted during learning. Clustering errors are reduced by Fuzzy SOFM as well as better convergence. The numerical results show that Fuzzy ART2 and Fuzzy SOFM are better than the K-Means algorithms. The images segmented by the algorithms are given to prove their performances.		Zhiling Wang;Andrea Guerriero;Marco De Sario	1996	Pattern Recognition Letters	10.1016/0167-8655(96)00006-2	inertia;entropy;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;supervised learning;fuzzy set operations;algorithm;k-means clustering	Vision	11.716303635917539	-31.284731239678063	8648
8acf6d462a39f65bddac2920f1c358c4a7ec8afe	spike-based image processing: can we reproduce biological vision in hardware?	conventional computational hardware;memristor-based hardware device;biological vision;specific hardware;sophisticated visual processing;processing strategy;different lane;image processing;intelligent image processing system;spike-based image processing;software image processing system;spike-based processing strategy	conventional computational hardware;memristor-based hardware device;biological vision;specific hardware;sophisticated visual processing;processing strategy;different lane;image processing;intelligent image processing system;spike-based image processing;software image processing system;spike-based processing strategy	image processing	Simon J. Thorpe	2012		10.1007/978-3-642-33863-2_53	computer vision;simulation;computer science;artificial intelligence;machine learning	Vision	45.88520639978104	-32.688926017949385	8649
123e2ea504b5bc6bf1b669de8f5d4aef9c3c8cf8	linucb applied to monte carlo tree search	linucb;mcts;contextual bandit;multi armed bandit problem	UCT is a standard method of Monte Carlo tree search (MCTS) algorithms, which have been applied to various domains and have achieved remarkable success. This study proposes a family of LinUCT algorithms that incorporate LinUCB into MCTS algorithms. LinUCB is a recently developed method that generalizes past episodes by ridge regression with feature vectors and rewards. LinUCB outperforms UCB1 in contextual multi-armed bandit problems. We introduce a straightforward application of LinUCB, LinUCT PLAIN by substituting UCB1 with LinUCB in UCT. We show that it does not work well owing to the minimax structure of game trees. To better handle such tree structures, we present LinUCT RAVE and LinUCT FP by further incorporating two existing techniques, rapid action value estimation (RAVE) and feature propagation, which recursively propagates the feature vector of a node to that of its parent. Experiments were conducted with a synthetic model, which is an extension of the standard incremental random tree model in which each node has a feature vector that represents the characteristics of the corresponding position, and Finnsson's shock step game which is used to empirically analyze the performance of UCT with respect to the distribution of suboptimal moves. The experiments results indicate that LinUCT RAVE and LinUCT FP outperform UCT, especially when the branching factor is relatively large.	monte carlo tree search	Yusaku Mandai;Tomoyuki Kaneko	2016	Theor. Comput. Sci.	10.1016/j.tcs.2016.06.035	combinatorics;simulation;multi-armed bandit;computer science;artificial intelligence;machine learning;mathematics;algorithm;statistics	AI	21.871532571985167	-17.785706718366182	8655
7e8daccda919bbbcbb247642865d96eacd73a9a7	a new compression scheme for color-quantized images	entropia;image coding;image processing;color index;data compression;decoding;image databases;sorting;lossy compression;metodo arborescente;procesamiento imagen;image indexing;traitement image;experimental result;algorithme;algorithm;codage image;compression image;image compression;arbol binario;discrete cosine transforms;pixel;indexation;entropie;arbre binaire;resultado experimental;tree structured method;image coding sorting decoding image databases pixel laboratories computer science pulse modulation discrete cosine transforms data compression;entropy;methode arborescente;computer science;resultat experimental;imagen color;color quantization;image couleur;pulse modulation;color image;algoritmo;compresion imagen;binary tree	An efficient compression scheme for color-quantized images based on progressive coding of color information has been developed. Instead of sorting color indexes into a linear list structure, a binary-tree structure of color indexes is proposed. With this structure, the new algorithm can progressively recover an image from two colors to all of the colors contained in the original image, i.e., a lossless recovery is achieved. Experimental results showed that it can efficiently compress images in both lossy and lossless cases. Typically for color-quantizedLena image with 256 colors, the algorithm achieved 0.5 bpp below state-of-the-art lossless compression methods while preserving the efficient lossy compression. Such a compression scheme is very attractive to many applications that require the ability of fast browsing or progressive transmission, and if necessary, to exactly recover the original image.	8-bit color;algorithm;lossless compression;lossy compression;quantization (signal processing);sorting;tree structure	Xin Chen;Sam Kwong;Ju-fu Feng	2002	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2002.804896	data compression;lossy compression;computer vision;entropy;color quantization;block truncation coding;image processing;image compression;computer science;theoretical computer science;jpeg;mathematics;lossless compression;algorithm;computer graphics (images)	Vision	45.112616745674806	-12.986216945520162	8661
ff8eeaf74f65167d07cbca36717c5577458fa45c	heuristic for constrained t-shape cutting patterns of rectangular pieces	cutting stock;t shape patterns;two dimensional cutting	T-shape patterns are often used in dividing stock plates into rectangular pieces, because they make good balance between plate cost and cutting complexity. A dividing cut separates the plate into two segments, each of which contains parallel strips, and the strip orientations of the two segments are perpendicular to each other. This paper presents a heuristic algorithm for constrained T-shape patterns, where the optimization objective is to maximize the pattern value, and the frequency of each piece type does not exceed the demand. The algorithm considers many dividing-cut positions, determines the pattern value associated to each position using a layout-generation procedure, and selects the one with the maximum pattern value as the solution. Pseudo upper bounds are used to skip some non-promising positions. The computational results show that the algorithm is fast and able to get solutions better than those of the optimal two-staged patterns in terms of material utilization. Highlights? We present a heuristic for the constrained two-dimensional cutting problem of rectangular pieces. ? T-shape patterns are used to simplify the cutting process. ? The approach can solve large-scale instances quickly. ? The computational results indicate that T-shape patterns can yield much better material utilization than both two-staged and homogenous T-shape patterns.	heuristic	Yaodong Cui;Baixiong Huang	2012	Computers & OR	10.1016/j.cor.2012.03.001	mathematical optimization	HCI	19.060331490239463	3.258744972190005	8663
e5d5a0a433f2c62a69ad49546c7b0558f728d9af	intelligent 3d obstacles recognition technique based on support vector machines for autonomous underwater vehicles	auv;svm;opengl;obstacle recognition;bp algorithm	This paper describes a classical algorithm carrying out dynamic 3D obstacle recognition for autonomous underwater vehicles (AUVs), Support Vector Machines (SVMs). SVM is an efficient algorithm that was developed for recognizing 3D object in recent years. A recognition system is designed using Support Vector Machines for applying the capabilities on appearance-based 3D obstacle recognition. All of the test data are taken from OpenGL Simulation. The OpenGL which draws dynamic obstacles environment is used to carry out the experiment for the situation of three-dimension. In order to verify the performance of proposed SVMs, it compares with Back-Propagation algorithm through OpenGL simulation in view of the obstacle recognition accuracy and the time efficiency.	algorithm;autonomous robot;backpropagation;opengl;simulation;software propagation;support vector machine;test data	Zhen-Shu Mi;Yong-Gi Kim	2009	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2009.9.3.213	computer vision;simulation;engineering;artificial intelligence	Robotics	52.692252912317514	-30.00010328449588	8665
54aace620d40a5cd24a3c8abcde6a3a520808f2d	hybrid learning based on multiple self-organizing maps and genetic algorithm	self organising feature maps genetic algorithms learning artificial intelligence pattern classification;self organising feature maps;hybrid learning;usa councils neural networks joints;pattern classification;genetic algorithm;genetic algorithms;self organized map;learning artificial intelligence;uci machine learning repository hybrid learning multiple selforganizing map based classification method genetic algorithm unsupervised learning mechanism input data space data clusters supervised som label matching global som lattice space davies bouldin index mean square error objective functions	Multiple Self-Organizing Maps (MSOMs) based classification methods are able to combine the advantages of both unsupervised and supervised learning mechanisms. Specifically, unsupervised SOM can search for similar properties from input data space and generate data clusters within each class, while supervised SOM can be trained from the data via label matching in the global SOM lattice space. In this work, we propose a novel classification method that integrates MSOMs with Genetic Algorithm (GA) to avoid the influence of local minima. Davies-Bouldin Index (DBI) and Mean Square Error (MSE) are adopted as the objective functions for searching the optimal solution space. Experimental results demonstrate the effectiveness and robustness of our proposed approach based on several benchmark data sets from UCI Machine Learning Repository.	benchmark (computing);dataspaces;davies–bouldin index;feasible region;genetic algorithm;machine learning;maxima and minima;mean squared error;organizing (structure);perl dbi;self-organizing map;software release life cycle;supervised learning;unsupervised learning	Qiao Cai;Haibo He;Hong Man	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033517	semi-supervised learning;unsupervised learning;genetic algorithm;self-organizing map;wake-sleep algorithm;computer science;artificial intelligence;machine learning;pattern recognition;competitive learning;generalization error	AI	10.766478077666294	-39.31211019479306	8670
231fb35fdbf6f51761bcde8bd87d679c3e1af3ce	appearance learning for 3d tracking of robotic surgical tools	fusion;surgical robotics;tool tracking;learning features	In this paper, we present an appearance learning approach which is used to detect and track surgical robotic tools in laparoscopic sequences. By training a robust visual feature descriptor on low-level landmark features, we build a framework for fusing robot kinematics and 3D visual observations to track surgical tools over long periods of time across various types of environment. We demonstrate 3D tracking on multiple types of tool (with different overall appearances) as well as multiple tools simultaneously. We present experimental results using the da Vinci® surgical robot using a combination of both ex-vivo and in-vivo environments.	algorithm;certificate authority;high- and low-level;multi language virtual machine;robot;statistical classification;tooltip;video-in video-out;visual descriptor	Austin Reiter;Peter K. Allen;Tao Zhao	2014	I. J. Robotics Res.	10.1177/0278364913507796	computer vision;simulation;fusion;engineering;machine learning	Robotics	36.55994662245199	-46.99341934804166	8672
9d3edc86787e6115ce3f35a4b0cc3da9bd9fbb9b	class-specific reconstruction transfer learning via sparse low-rank constraint		Subspace learning and reconstruction have been widely explored in recent transfer learning work and generally a specially designed projection and reconstruction transfer matrix are wanted. However, existing subspace reconstruction based algorithms neglect the class prior such that the learned transfer function is biased, especially when data scarcity of some class is encountered. Different from those previous methods, in this paper, we propose a novel reconstruction-based transfer learning method called Class-specific Reconstruction Transfer Learning (CRTL), which optimizes a well-designed transfer loss function without class bias. Using a class-specific reconstruction matrix to align the source domain with the target domain which provides help for classification with class prior modeling. Furthermore, to keep the intrinsic relationship between data and labels after feature augmentation, a projected Hilbert-Schmidt Independence Criterion (pHSIC), that measures the dependency between two sets, is first proposed by mapping the data from original space to RKHS in transfer learning. In addition, combining low-rank and sparse constraints on the class-specific reconstruction coefficient matrix, the global and local data structures can be effectively preserved. Extensive experiments demonstrate that the proposed method outperforms conventional representation-based domain adaptation methods.	algorithm;align (company);coefficient;data structure;domain adaptation;experiment;loss function;schmidt decomposition;sparse matrix;transfer function;transfer matrix	Shanshan Wang;Wangmeng Zuo	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.116	artificial intelligence;transfer function;transfer of learning;transfer matrix;pattern recognition;sparse approximation;computer science;subspace topology;data structure;matrix (mathematics);machine learning;reproducing kernel hilbert space	Vision	24.287539777632634	-42.93737407407208	8674
12e57eec913d9a5122374535b0ddda7c2e5518a2	fractal initialization for high-quality mapping with self-organizing maps	chevauchement;selfsimilarity;algoritmo paralelo;neurone;calcul neuronal;neural computation;self organizing maps;parallel algorithm;neural networks;qualite;vector space;62m45;inicializacion;overlap;hilbert curves;imbricacion;algorithme parallele;random vector;neurona;28a80;quality;autosimilitud;application auto organisante;fractal;autosimilitude;self organized map;vector aleatorio;som;espace vectoriel;reseau neuronal;vecteur aleatoire;espacio vectorial;red neuronal;computacion neuronal;initialization;neuron;initialisation;calidad;neural network;hilbert curve	Initialization of self-organizing maps is typically based on random vectors within the given input space. The implicit problem with random initialization is the overlap (entanglement) of connections between neurons. In this paper, we present a new method of initialization based on a set of self-similar curves known as Hilbert curves. Hilbert curves can be scaled in network size for the number of neurons based on a simple recursive (fractal) technique, implicit in the properties of Hilbert curves. We have shown that when using Hilbert curve vector (HCV) initialization in both classical SOM algorithm and in a parallel-growing algorithm (ParaSOM), the neural network reaches better coverage and faster organization.	algorithm;apollonian network;artificial neural network;cluster analysis;flow network;fractal;hilbert curve;hilbert space;neuron;organizing (structure);quantum entanglement;recursion;self-organization;self-organizing map;self-similarity;video post-processing	Iren Valova;Derek Beaton;Alexandre Buer;Dan Maclean	2010	Neural Computing and Applications	10.1007/s00521-010-0413-5	initialization;mathematical optimization;multivariate random variable;self-organizing map;fractal;hilbert r-tree;vector space;computer science;artificial intelligence;machine learning;mathematics;parallel algorithm;artificial neural network;models of neural computation	ML	12.061984673518124	-31.041558728914858	8675
4d3f050801bd76ef10855ce115c31b301a83b405	theory of the backpropagation neural network	backpropagation neural network	The author presents a survey of the basic theory of the backpropagation neural network architecture covering architectural design, performance measurement, function approximation capability, and learning. The survey includes previously known material, as well as some new results, namely, a formulation of the backpropagation neural network architecture to make it a valid neural network (past formulations violated the locality of processing restriction) and a proof that the backpropagation mean-squared-error function exists and is differentiable. Also included is a theorem showing that any L/sub 2/ function from (0, 1)/sup n/ to R/sup m/ can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. The author presents a speculative neurophysiological model illustrating how the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of the cerebral cortex.<<ETX>>	approximation;artificial neural network;backpropagation;locality of reference;mean squared error;multitier architecture;network architecture;speculative execution	Robert Hecht-Nielsen	1988	International 1989 Joint Conference on Neural Networks	10.1016/0893-6080(88)90469-8	madaline;feedforward neural network;probabilistic neural network;types of artificial neural networks;computer science;recurrent neural network;physical neural network;time delay neural network;multilayer perceptron	ML	18.212330339043756	-28.644012510698857	8688
aee878e96d7220d2f91f875c4ecce164cbb9fc26	analysis of interval-valued decision formal contexts		This study investigates a novel approach to knowledge reduction in interval-valued decision formal contexts. Applying rule acquisition, we formulate a new framework of knowledge reduction for interval-valued decision formal contexts and the presented framework can be applicable to any interval-valued decision formal contexts. Based on this reduction method, more compact decision rules that can imply all of those derived from the initial interval-valued decision formal contexts in the form of implication rules are obtained. Furthermore, a corresponding reduction method is developed by constructing a discernibility matrix and its associated Boolean function.		Hong Wang;Pin-Zhi Cui	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151635	decision analysis	Robotics	-3.143850477483342	-25.793384853021116	8692
4ac409fc5cd95c75a7aeb5512030e729d0df8ba4	light-synchronized acoustic toa measurement system for mobile smart nodes	microphones;time of arrival estimation cmos image sensors mobile radio synchronisation;light emitting diodes cameras acoustics cmos image sensors synchronization acoustic measurements microphones;acoustics;light emitting diodes;airborne sound wave light synchronized acoustic toa measurement system mobile smart nodes time synchronization technique modulated led light video camera cmos image sensor line byline basis time information time of arrival measurements;cmos image sensors;synchronization;time synchronization sonic localization toa measurement led light cmos image sensor;acoustic measurements;cameras	We describe a novel time-synchronization technique for mobile smart nodes. We capture modulated LED light with a video camera, which is usually built into a smart node. The CMOS image sensor of a video camera does not take a snapshot at a certain time. Instead, the sensor captures data on a line-byline basis and the sensor output consists of lines taken at slightly different times. Therefore, we can extract time information from the image. This can be used for time synchronization for time-of-arrival (ToA) measurements. In this paper, we describe the fundamentals and an experiment using light-synchronized acoustic ToA measurements with mobile smart nodes. The acknowledged precision of the obtained time information is equivalent to 5.8 mm with an airborne sound wave.	acoustic cryptanalysis;airborne ranger;cmos;image sensor;modulation;refinement (computing);snapshot (computer storage);system of measurement	Takayuki Akiyama;Masanori Sugimoto;Hiromichi Hashizume	2014	2014 International Conference on Indoor Positioning and Indoor Navigation (IPIN)	10.1109/IPIN.2014.7275557	embedded system;electronic engineering;telecommunications;physics	Mobile	50.066925640653345	3.5184018627762557	8700
b507ef2354a39265b52392dd306889a07ac500cc	cosine similarity measure between hybrid intuitionistic fuzzy sets and its application in medical diagnosis		In this paper, a cosine similarity measure between hybrid intuitionistic fuzzy sets is proposed. The aim of the paper is to investigate the cosine similarity measure with hybrid intuitionistic fuzzy information and apply it to medical diagnosis. Firstly, we construct the cosine similarity measure between hybrid intuitionistic fuzzy sets, and the relevant properties are also discussed. In order to obtain a reasonable evaluation in group decision, the weight of experts under different attributes is determined by the projection of individual decision information on the ideal decision information, where the ideal decision information is the average values of each expert's evaluation. Furthermore, we propose a decision method for medical diagnosis based on the cosine similarity measure between hybrid intuitionistic fuzzy sets, and the patient can be diagnosed with the disease according to the values of proposed cosine similarity measure. Finally, an example is given to illustrate feasibility and effectiveness of the proposed cosine similarity measure, which is also compared with the existing similarity measures.	abbreviations;conflict (psychology);cosine similarity;discrete cosine transform;fuzzy logic;fuzzy set;intuitionistic logic;numerical analysis;patients;pattern recognition;server message block;similarity measure	Donghai Liu;Xiaohong Chen;Dan Peng	2018		10.1155/2018/3146873	medical diagnosis;machine learning;fuzzy set;artificial intelligence;fuzzy logic;decision model;cosine similarity;computer science	AI	-3.1967681226801874	-21.325547868728947	8703
96fa4022210c05e96f233d5208315d7f9cd00b68	a low complexity on-chip ecg data compression methodology targeting remote health-care applications	electrocardiography system on chip discrete wavelet transforms complexity theory data compression databases memory management;voltage 1 62 v cross correlation r 2 statistics mit bih ptb db application specific integrated circuit low complexity remote health care data compression on chip ecg frequency 1 mhz;signal reconstruction application specific integrated circuits data compression electrocardiography health care medical signal processing regression analysis	In this paper, we propose a novel low complexity on-chip ECG data compression methodology targeting remote health-care applications. This is to the best of our knowledge the first attempt for on-chip reliable data compression. The proposed methodology has been implemented targeting Application Specific Integrated Circuit platform at 1 MHz at Vdd 1.62 V for UMC 130 nm technology library with 16 bits system word-length. Furthermore the proposed methodology results in a faithful reconstruction which has been validated using MIT-BIH PTB-DB as well as our institute's health repository IITH-DB. On an average about 90% compression is achieved with more than 83% R2 statistics, 98% Cross Correlation and about 99% Regression between the original and the reconstructed data signifying the diagnostic accuracy. Subsequently the proposed methodology is capable of storing approximately 47 hrs of data in the same on-chip memory when compared to that of 5 hours of continuous data in the state of the art which would lead to enhanced diagnosis and prognosis in remote health-care.	application-specific integrated circuit;bounding interval hierarchy;cross reactions;data compression;decibel;forecast of outcome;greater than;mobile device;ninety nine;power (psychology);cellular targeting;db/db mouse	Bastin Joseph;Amit Acharyya;Pachamuthu Rajalakshmi	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944982	electronic engineering;computer science;electrical engineering;theoretical computer science	EDA	41.126111876919545	-5.366180591715796	8711
d1d9f2cc8db0a7df990fd1783ab735a4d2187f7f	unified decision combination framework	simulation ordinateur;combination of classifiers;metodo monte carlo;algorithm performance;methode parametrique;learning;etude experimentale;metodo parametrico;parametric method;methode monte carlo;prise decision;classification;teoria decision;fonction objectif;algorithme;aprendizaje;objective function;algorithm;simulation experiment;apprentissage;combining classifier;resultado algoritmo;theorie decision;monte carlo method;decision theory;performance algorithme;methode moyenne;pattern recognition;funcion objetivo;simulacion computadora;reconnaissance forme;reconocimiento patron;figure of merit;toma decision;computer simulation;estudio experimental;averaging method;clasificacion;metodo medio;algoritmo	Pattern recognition researchers are discovering that a judicious combination of classifiers, generally outperforms a single one. In this paper, we present a unified framework for decision combination. By looking at the combination problem from this new perspective, we gain an intuitive, perhaps better understanding of some of the known algorithms. A new parameterized combination method (pooled ranking figure of merit) is presented which is shown to be equivalent to three of the standard combination methods. Furthermore, our new decision combination method provides newer combination methods that provide interesting tradeoffs. These ideas are supported by simulation experiments. ( 1998 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved. Decision combination Voting Borda count Averaging Ranking figure of merit Pooled objective function Differential learning Combining classifiers	algorithm;approximation error;cross entropy;experiment;loss function;mean squared error;monte carlo method;numerical analysis;optimization problem;pattern recognition;simulation;unified framework;whole earth 'lectronic link	Khaled Al-Ghoneim;B. V. K. Vijaya Kumar	1998	Pattern Recognition	10.1016/S0031-3203(98)00030-2	computer simulation;figure of merit;decision theory;biological classification;computer science;artificial intelligence;mathematics;algorithm;statistics;monte carlo method	AI	11.02164559570982	-34.559901884097805	8725
d6790d8392db5265376a0cd4c0725db7101f2b4f	single-car routing in rail freight transport	single car rail freight transport;network design;nonlinear constraints;networks;routing;mixed integer linear programming;railroad transportation;linear programming;mixed integer programming;design;optimization;freight transportation;modeling;freight cars;lifting	Cars in rail freight service follow prescribed routes from their origin via intermediate shunting yards to their destination. The goal in designing such routes is to reduce the number of trains and their travel distances. Various real-world hard constraints make the problem difficult to formulate and also to solve. We present integer programming formulations for this car routing problem arising at the largest European railway company and discuss their pros and cons.	branch and price;column generation;computation;frank–wolfe algorithm;integer programming;michael j. fischer;microsoft outlook for mac;nonlinear system;numerical analysis;routing;scheduling (computing);solver;telecommunications network;uwe schöning	Armin Fügenschuh;Henning Homfeld;Hanno Schülldorf	2015	Transportation Science	10.1287/trsc.2013.0486	design;mathematical optimization;routing;simulation;integer programming;computer science;engineering;linear programming;operations management;mathematics;transport engineering	Theory	15.715984693440431	1.9009532127963853	8729
b18b8fc88da9c04a75372588fac19d20bdfb8b6f	shift scheduling to improve customer satisfaction, employee satisfaction and management satisfaction in service workplace where employees and robots collaborate		In this paper, shift scheduling method to improve customer satisfaction (CS), employee satisfaction (ES) and management satisfaction (MS) in service workplace where employees and robots collaborated is proposed. In service industry, it is important to introduce the labor force created as a result of operations efficiency improvement to the other business that creates added value. For this purpose, in recent years, it has been researched to introduce robots to service workplace. In restaurant business, it is necessary to improve CS, ES and MS together, because of increasing customers’ repeat and improving profitability. Therefore we started to research mentioned at the beginning. Since there are a trade-off relationship among CS, ES and MS, it is required to make a balanced plan. Therefore shift scheduling problem is modeled as multi-objective optimization problem so as to improve CS, MS, ES and formulated as a set cover problem. Finally, relationship of CS, ES, MS and method to create shift schedule to improve them are discussed based on numerical experiments.	robot;schedule (project management)	Takashi Tanizaki;Takeshi Shimmura;Nobutada Fujii	2017		10.1007/978-3-319-61240-9_2	scheduling (computing);job shop scheduling;tertiary sector of the economy;operations management;business;customer satisfaction;profitability index;job satisfaction;optimization problem;added value	Robotics	12.197494130837942	1.823434586615212	8735
c79b162d58cfb52d9bf28a30c85bc108c9fba914	reduced mdp representation of a logistical planning problem using petri-nets	dynamic programming;unfolding;programacion dinamica;deploiement;proceso markov;red petri;dynamic programming algorithm;despliegue;decision markov;intelligence artificielle;markov property;planificacion;processus markov;programmation dynamique;markov process;artificial intelligence;planning;markov decision;inteligencia artificial;markov decision process;planification;petri net;reseau petri	This paper describes a method for unfolding a Predicate-net representation of a logistical planning problem, such that it possesses the Markov property. The problem can then be easily converted into a Markov Decision Process (MDP) which can be solved in a tractable manner using standard Dynamic Programming algorithms.	algorithm;cobham's thesis;dynamic programming;logistics;markov chain;markov decision process;markov property;petri net;unfolding (dsp implementation)	Sanjeev Naguleswaran;Sarah L. Hickmott;Langford B. White	2005		10.1007/11589990_90	partially observable markov decision process;computer science;artificial intelligence;machine learning;dynamic programming;algorithm;statistics	ML	23.118097510257567	-14.528132732260367	8744
b9136ced9e13a78840bed3b738a81242eaf7f70b	chaotic analysis of seismic time series and short-term prediction with rbf neural networks	chaotic behavior;time series chaos earthquakes phase space methods radial basis function networks;chaos;earthquakes dynamics chaotic analysis seismic time series short term prediction rbf neural networks phase space reconstruction;training;phase space methods;chaotic time series;time series;earthquakes;radial basis function networks;phase space reconstruction;artificial neural networks;time series analysis;rbf neural network;prediction accuracy;correlation;chaos time series analysis neural networks extraterrestrial measurements algorithm design and analysis time measurement computer networks earthquakes mathematical model genetics;extraterrestrial measurements	By incorporating chaotic algorithm with the RBF neural network, a chaotic analysis approach was applied to a time series composed of seismic events occurred in Guangxi nearly three decades based on the theory of phase-space reconstruction. The dynamics of the earthquakes exhibit chaotic behavior. After the chaotic analysis, short term forecasting using an RBF Neural Network has been performed, and information about the nature of the underlying system has been gathered and aided the construction of the RBF neural network. The simulation results show that the method of chaotic time series has a better the non-linear fitting and higher prediction accuracy. Preliminary results indicate that this is a promising approach.	algorithm;chaos theory;computation;correlation dimension;long short-term memory;lyapunov fractal;matlab;neural networks;nonlinear system;radial basis function;simulation;time complexity;time series	Jinkui Zhang;Yi Chen;Ying Wang	2009	2009 Third International Conference on Genetic and Evolutionary Computing	10.1109/WGEC.2009.94	simulation;artificial intelligence;machine learning;time series;artificial neural network;statistics	Robotics	9.70789168449967	-21.37221385167484	8764
ca8eb44220d2b26d3d91b622a09b488f99228156	an organizational coevolutionary algorithm for classification	extraction information;organization;classification algorithm;evolutionary computation;bottom up;analisis datos;information extraction;pattern classification evolutionary computation search problems;indexing terms;algoritmo genetico;data mining;classification;coevolution;data analysis;algorithme coevolutionniste organisationnel pour classification;fouille donnee;target recognition;radar target recognition problems organizational coevolutionary algorithm classification algorithm bottom up search mechanism evolutionary operators selection mechanism fitness function multiplexer problems uci repository datasets;reconnaissance cible radar;prediction accuracy;radar target recognition;pattern classification;algorithme genetique;organization classification coevolution data mining evolutionary algorithms eas;algorithme evolutionniste;analyse donnee;genetic algorithm;algoritmo evolucionista;search problems;evolutionary algorithm;evolutionary process;coevolucion;busca dato;extraccion informacion;classification algorithms evolutionary computation multiplexing computational efficiency scalability data mining humans societies radar target recognition;fitness function;evolutionary algorithms eas	 Taking inspiration from the interacting process among organizations in human societies, a new classification algorithm, organizational coevolutionary algorithm for classification (OCEC), is proposed with the intrinsic properties of classification in mind. The main difference between OCEC and the available classification approaches based on evolutionary algorithms (EAs) is its use of a bottom-up search mechanism. OCEC causes the evolution of sets of examples, and at the end of the evolutionary process, extracts rules from these sets. These sets of examples form organizations. Because organizations are different from the individuals in traditional EAs, three evolutionary operators and a selection mechanism are devised for realizing the evolutionary operations performed on organizations. This method can avoid generating meaningless rules during the evolutionary process. An evolutionary method is also devised for determining the significance of each attribute, on the basis of which, the fitness function for organizations is defined. In experiments, the effectiveness of OCEC is first evaluated by multiplexer problems. Then OCEC is compared with several well-known classification algorithms on 12 benchmarks from the UCI repository datasets and multiplexer problems. Moreover, OCEC is applied to a practical case, radar target recognition problems. All results show that OCEC achieves a higher predictive accuracy and a lower computational cost. Finally, the scalability of OCEC is studied on synthetic datasets. The number of training examples increases from 100 000 to 10 million, and the number of attributes increases from 9 to 400. The results show that OCEC obtains a good scalability. Index Terms Data mining, classification, organization, evolutionary algorithms, coevolution. 1 Corresponding author, e-mail: neouma@163.com, telephone: 86-029-88202661 2	algorithmic efficiency;bottom-up proteomics;computation;email;evolutionary algorithm;experiment;fitness function;interaction;multiplexer;radar;scalability;synthetic intelligence;vii	Licheng Jiao;Jing Liu;Weicai Zhong	2006	IEEE Trans. Evolutionary Computation	10.1109/TEVC.2005.856068	mathematical optimization;genetic algorithm;index term;biological classification;coevolution;computer science;organization;artificial intelligence;machine learning;evolutionary algorithm;top-down and bottom-up design;data mining;mathematics;data analysis;fitness function;information extraction;algorithm;evolutionary computation	ML	7.151152536268806	-35.05915444256617	8809
8ed1a69d11c751f45f2f9a70de1f816c74357ba7	a portable multi-channel potentiostat for real-time amperometric measurement of multi-electrode sensor arrays	prototypes;diabetes;electrodes;graphical user interfaces;sensor arrays amperometric sensors electrochemical electrodes graphical user interfaces mathematics computing portable instruments prototypes readout electronics real time systems;portable multichannel potentiostat electrochemical validation real time data acquisition matlab graphical user interface pcb prototype readout channels hybrid multiplexed technique multielectrode sensor arrays real time amperometric measurement;electric potential;electrodes electric potential prototypes diabetes graphical user interfaces real time systems biosensors;biosensors;real time systems	This paper presents a compact and scalable architecture design of a multi-channel potentiostat. Utilizing a hybrid-multiplexed technique, the system is capable of driving multi-electrode array structures of large sizes with few readout channels. A 5-channel potentiostat with 80-electrode capability was fabricated into a portable 8.382×9.906 cm2 PCB prototype using discrete components. It features a dynamic current range of 126dB and 3.3V single-supply operation. Controlled by a MATLAB graphical user interface, the system demonstrates realtime data acquisition and achieves similar performance to a commercial potentiostat based on electrochemical validation.	data acquisition;electronic component;graphical user interface;matlab;multiplexing;potentiostat;prototype;real-time clock;scalability	Yaoxing Hu;Sanjiv Sharma;Jean Weatherwax;Anthony Cass;Pantelis Georgiou	2016	2016 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2016.7527488	embedded system;electronic engineering;computer science;engineering;electrical engineering;electrode;graphical user interface;prototype;electric potential;biosensor	Embedded	46.867212559337304	-1.061897224729569	8813
5bb4c931d2ef436d65ab73c97fc4bd024261cdcc	validation testing for temporal neural networks for rbf recognition	conference;meeting	A neuron can emit spikes in an irregular time basis and by averaging over a certain time window one would ignore a lot of information. It is known that in the context of fast information processing there is no sufficient time to sample an average firing rate of the spiking neurons. The present work shows that the spiking neurons are capable of computing the radial basis functions by storing the relevant information in the neurons' delays. One of the fundamental findings of the this research also is that when using overlapping receptive fields to encode the data patterns it increases the network’s clustering capacity. The clustering algorithm that is discussed here is interesting from computer science and neuroscience point of view as well as from a perspective.	algorithm;artificial neural network;cluster analysis;computer science;encode;information processing;neuron;radial (radio);radial basis function	Khaled E. A. Negm	2005			computer science;artificial intelligence;data mining;operations research	ML	7.018990733521139	-24.828929142686995	8816
2635cd4ca6b409b19783f2b97fd46fbbc1dbbe95	auction mechanism for spectrum allocation and profit sharing	retailing game theory incentive schemes profitability radio spectrum management;game theory;nash equilibrium;profit sharing;probability density function;resource management;retailing;cooperative game auction mechanism spectrum allocation profit sharing dynamic spectrum sharing multiple seller buyer interaction noncooperative game symmetric mixed strategy nash equilibrium risk neutral seller incentive expected profit maximization;spectrum;data mining;cooperative game;chromium frequency interference fcc nash equilibrium radio spectrum management switches temperature channel allocation us government;auction mechanism;noncooperative games;risk neutral seller incentive;multiple seller buyer interaction;chromium;games;mobile communication;symmetric mixed strategy nash equilibrium;expected profit maximization;radio spectrum management;profitability;incentive schemes;noncooperative game;dynamic spectrum sharing;spectrum allocation;mixed strategy	We examine the problem of designing an auction mechanism for dynamic spectrum sharing when there are multiple sellers and multiple buyers. First, we study the interaction among homogeneous buyers of spectrum as a noncooperative game and show the existence of a symmetric mixed-strategy Nash equilibrium (SMSNE). Second, we prove that there exists an incentive for risk neutral sellers of the spectrum to cooperate to maximize their expected profits at the SMSNEs of buyers' noncooperative game. Finally, we model the interaction among the sellers as a cooperative game and demonstrate that the core of the cooperative game is nonempty. This indicates that there exists a way for the sellers to share the profits in a such manner that no subset of sellers will deviate from cooperating with the remaining sellers.	frequency allocation;nash equilibrium	Sung Hyun Chun;Richard J. La	2009	2009 International Conference on Game Theory for Networks	10.1109/GAMENETS.2009.5137438	financial economics;double auction;microeconomics;business;commerce;forward auction	AI	-3.304646760180296	-4.161117417116324	8824
027d9c43f4ba9ef819512d04db37fc9c0d107123	addressing the problems of bayesian network classification of video using high-dimensional features	bayesian theory;belief networks;bayesian network;multiple labels assignment;high dimensionality;video signal processing;partitioning;image classification;multiple labels assignment bayesian network bayesian theory pattern classification video classification discrete bayes error content based retrieval dimension partitioning dimensionality reduction;dimensionality reduction;pattern classification;error statistics;classification accuracy;dimensional reduction;content based retrieval;error statistics pattern classification video signal processing image classification belief networks content based retrieval image retrieval image sequences;discrete bayes error;bayesian methods pattern classification data mining multidimensional systems partitioning algorithms pattern recognition machine learning inference mechanisms computational efficiency content based retrieval;image sequences;bayesian networks;image retrieval	Bayesian theory is of great interest in pattern classification. We present an approach to aid in the effective application of Bayesian networks in tasks like video classification, where descriptors originate from varied sources and are large in number. In order to extend the application of conventional Bayesian theory to the case of continuous and nonparametric descriptor space, dimension partitioning into attributes by minimizing the discrete Bayes error is proposed. The partitioning output goes to the dimensionality reduction module. A new algorithm for dimensionality reduction for improving the classification accuracy is proposed based on the class pair discriminative capacity of the dimensions. It is also shown how attributes can be weighed automatically in a single-label assignment based on comparing the class pairs. A computationally efficient method to assign multiple labels on the samples is also presented. Comparison with standard classification tools on video data of more than 4000 segments shows the potential of our approach in pattern classification.	algorithm;algorithmic efficiency;bayesian network;case-based reasoning;causality;dimensionality reduction;discrete mathematics;discretization;iteration;modified huffman coding;static single assignment form;weatherstar	Ankush Mittal;Loong Fah Cheong	2004	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2004.1269600	image retrieval;computer science;machine learning;pattern recognition;bayesian network;data mining	ML	28.160382654611777	-42.97615048027048	8838
96346d804805287cb233f6d7ebe956ba79a4e7a2	conservative likelihood inference for type i censored samples from the log-location-scale distributions	metodo estadistico;sample size;fiabilidad;reliability;quasi vraisemblance;exponential distribution;small sample;metodo monte carlo;probability;aplicacion;65c05;intervalo confianza;small sample size;loi probabilite;ley exponencial;ley probabilidad;censored sample;fonction repartition;62p30;loi exponentielle;tamano muestra;quasi likelihood;62f25;62e17;loi exacte;simulacion numerica;donnee censuree;methode monte carlo;taille echantillon;statistical method;probability of failure;asymptotic behavior;comportement asymptotique;echantillon censure;comportamiento asintotico;funcion distribucion;confidence interval;distribution function;62n02;censored data;time censoring;62g15;propriete asymptotique;methode statistique;monte carlo evaluation;fiabilite;monte carlo method;probability distribution;intervalle confiance;probabilidad;simulation numerique;probabilite;reliability data;62n99;asymptotic properties;pequena muestra;estimation statistique;monte carlo;60k10;exact distribution;monte carlo simulation;62n05;application;estimacion estadistica;60e05;statistical estimation;60k20;petit echantillon;62e15;62n01;numerical simulation;muestra censurada	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	D. S. Paolino	2007	Communications in Statistics - Simulation and Computation	10.1080/03610910701212876	computer simulation;econometrics;asymptotic analysis;calculus;mathematics;statistics;monte carlo method	Robotics	32.496034745649915	-20.69211756951617	8841
1be8bdcf3429529c8482c9ba6608c0a48dbb6ac6	a single-way ranging localization of auvs based on pso of outliers elimination		Localization of autonomous underwater vehicles (AUVs) is a very important and challenging task for the AUVs applications. In long baseline underwater acoustic localization networks, the accuracy of single-way range measurements is the key factor for the precision of localization of AUVs, whether it is based on the way of time of arrival (TOA), time difference of arrival (TDOA), or angle of arrival (AOA).The single-way range measurements do not depend on water quality and can be taken from long distances; however, there are some limitationswhich exist in thesemeasurements, such as the disturbance of the unknown current velocity and the outliers caused by sensors and errors of algorithm. To deal with these problems, an AUV self-localization algorithm based on particle swarm optimization (PSO) of outliers elimination is proposed, which improves the performance of angle of arrival (AOA) localization algorithm by taking account of effects of the current on the positioning accuracy and eliminating possible outliers during the localization process. Some simulation experiments are carried out to illustrate the performance of the proposed method compared with another localization algorithm.		Xinnan Fan;Zhongjian Wu;Jianjun Ni;Chengming Luo	2018	J. Robotics	10.1155/2018/1301497	computer vision;multilateration;outlier;time of arrival;artificial intelligence;computer science;ranging;angle of arrival;particle swarm optimization	Robotics	51.08662289517543	3.475265641469773	8843
30dc0d5089e4a13536a9a917d965a477ed98c0b0	on clock synchronization for multi-microphone speech processing in wireless acoustic sensor networks	cows;wireless acoustic sensor networks clock synchronization speech enhancement;clocks;array signal processing;wireless sensor networks array signal processing microphones speech processing synchronisation;clocks noise cows synchronization array signal processing cost benefit analysis;synchronization;mvdr beamformer clock synchronization multimicrophone speech processing wireless acoustic sensor networks wasn multimicrophone signal processing signal drift multimicrophone noise reduction clock synchronization algorithms;cost benefit analysis;noise	In wireless acoustic sensor networks (WASNs), clock synchronization is crucial for multi-microphone signal processing, since clock differences between capturing devices will cause signal drift. This in turn severely degrades the performance of multi-microphone signal processing. After a theoretical analysis of the effect of clock synchronization, we evaluate the use of three different clock synchronization algorithms in the context of multi-microphone noise reduction. Our experimental study shows that the achieved precision of clock synchronization enables sufficient accuracy of clock synchronization for the MVDR beamformer in ideal scenarios. However, in practical scenarios with measurement noise on the parameters of interest, time-stamp based clock synchronization algorithms get degraded, while signal based algorithms are still accurate enough for the MVDR beamformer, albeit at a much higher transmission cost.	acoustic cryptanalysis;algorithm;basic stamp;beamforming;clock synchronization;experiment;microphone;noise reduction;signal processing;speech processing	Yuan Zeng;Richard Christian Hendriks;Nikolay D. Gaubitch	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7177966	clock synchronization;synchronization;real-time computing;telecommunications;computer science;noise;cost–benefit analysis;self-clocking signal;digital clock manager	Embedded	49.735007377722724	3.4192867539701814	8857
24a4546de19cb3754e374b9e94b3a929f5bfd6df	philips 3d solutions: from content creation to visualization	optoelectronic devices;3d animation;three dimensional displays two dimensional displays animation standardization information geometry data visualization application software image converters laboratories image processing;video signal processing;philips 3d solutions;video processing;visualization;optoelectronic displays philips 3d solutions content creation visualization end to end 3d display solution video processing;end to end 3d display solution;three dimensional displays;signal processing;stereo image processing;video signal processing computer animation optoelectronic devices stereo image processing three dimensional displays;content creation;computer animation;3d video;3d display;optoelectronic displays	"""Philips is realizing an end-to-end 3D display solution from 3D content creation to visualization. This development fits in our long-standing tradition of combining expertise in video processing with our strength in display development to create the most exciting and best viewing experience. Philips developed several high-quality 3D displays, ranging in resolution, viewing angle, depth experience, and sizes from 4"""" to 40"""" and up. Backwards compatibility with 2D content is enabled via signal processing or opto-electronic 3D & 2D dual mode displays. Content creation and conversion methods are provided, which are a key factor for the success of 3D displays. Fully automatic conversion from monoscopic 2D content into 3D enables the re-use of all existing 2D video material. Further methods enable 3D animation/design, 2D to 3D conversion in post- production and live capture of new 3D content. Our efforts in MPEG standardization towards the """"2D-plus-depth"""" format for 3D video enables a flexible interface between the variety in 3D content creation methods and the range in 3D displays. Furthermore, the 3D format is compatible with existing 2D content, standards and infrastructure. Currently, Philips offers several commercial 3D products for professional use such as digital signage, and progress is being made towards consumer products such as 3DTV."""	2d to 3d conversion;2d-plus-depth;3d television;computer animation;digital signage;end-to-end principle;fits;moving picture experts group;signal processing;stereo display;video processing;viewing angle	André Redert;Robert-Paul Berretty;Chris Varekamp;Oscar Willemsen;Jos Swillens;Hans Driessen	2006	Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT'06)	10.1109/3DPVT.2006.107	computer vision;computer science;multimedia;computer graphics (images)	HCI	42.84922880686276	-21.316868972071713	8873
1380791c82cc28a5548b72f33b20a4df5cece47a	calibration approach-based product estimator of finite population total with subsampling of nonrespondents under single and two-phase sampling	negative correlation;nonresponse;calibration approach;product estimator;62d05;non response;subsampling;hansen and hurwitz estimator;sub sampling	Hansen and Hurwitz (1946) technique based estimator of population total is proposed using the calibration approach under the assumption that the auxiliary variable is negatively correlated with the study variable. The variance estimation is also considered. The two-phase sampling case is also explored. The theoretical results are demonstrated through empirical studies using both generated and real population data. The proposed estimator of population total outperforms the existing estimators in terms of the criteria of relative bias and relative root mean square error.	chroma subsampling;kernel density estimation;mean squared error;population;routh–hurwitz stability criterion;sampling (signal processing);two-phase commit protocol;two-phase locking	Rohan Kumar Raman;U. C. Sud;Hukum Chandra	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.936468	efficient estimator;minimax estimator;econometrics;mathematical optimization;minimum-variance unbiased estimator;non-response bias;mathematics;mean squared error;bias of an estimator;negative relationship;consistent estimator;statistics	ML	29.706781544307788	-22.89160570283388	8878
4a8850272223e9c477fe55d58ce850b9164a14d6	a quantitative study of learning and generalization in genetic programming	genetic program;call graph;theoretical computer science;computer science all;fitness function	The relationship between generalization and solutions functional complexity in genetic programming (GP) has been recently investigated. Three main contributions are contained in this paper: (1) a new measure of functional complexity for GP solutions, called Graph Based Complexity (GBC) is defined and we show that it has a higher correlation with GP performance on out-of-sample data than another complexity measure introduced in a recent publication. (2) A new measure is presented, called Graph Based Learning Ability (GBLA). It is inspired by the GBC and its goal is to quantify the ability of GP to learn “difficult” training points; we show that GBLA is negatively correlated with the performance of GP on out-of-sample data. (3) Finally, we use the ideas that have inspired the definition of GBC and GBLA to define a new fitness function, whose suitability is empirically demonstrated. The experimental results reported in this paper have been obtained using three real-life multidimensional regression problems.	blum axioms;computational complexity theory;fitness function;game boy camera;genetic programming;real life;symbolic regression	Mauro Castelli;Luca Manzoni;Sara Silva;Leonardo Vanneschi	2011		10.1007/978-3-642-20407-4_3	call graph;computer science;artificial intelligence;machine learning;mathematics;programming language;fitness function;algorithm;statistics	AI	21.212699215095586	-8.803067095437447	8882
ea7d06a4977e344bb9f0a0c0eaea47e67550bab2	forecasting exchange rates with fuzzy granular evolving modeling for trading strategies	granular computingevolving systemsexchange ratestradingforecasting	This paper addresses a fuzzy set based evolving modeling (FBeM) approach and the task of forecasting exchange rates in order to perform trading strategies. FBeM is a granular computing technique that uses fuzzy information granules to model nonstationary functions providing functional and linguistic approximations. As an application, this work considers the BRL/USD exchange rate market data for the period from January 2000 to October 2012. Comparisons in terms of goodness of fit and based on trading performance indicators includes the granular model against a Multi-Layer Perceptron (MLP), an autoregressive moving average (ARMA), a naïve strategy and some state of the art evolving fuzzy systems. Computational results suggest that the FBeM model statistically outperforms the alternative approaches.	approximation;artificial neural network;autoregressive model;computation;fuzzy control system;fuzzy set;granular computing;moving-average model;multilayer perceptron;naivety;nonlinear system;quad flat no-leads package;recursion	Leandro Maciel;Fernando A. C. Gomide;Rosangela Ballini	2013		10.2991/eusflat.2013.40	financial economics	ML	7.7629617522282635	-20.22338006452841	8908
d9c0310203179d5328c4f1475fa4d68c5f0c7324	face analysis in the wild		With the global demand for extra security systems, and the growing of human-machine interaction, facial analysis in unconstrained environments (in the wild) became a hot-topic in recent computer vision research.Unconstrained environments include surveillance footage, social media photos and live broadcasts.This type of images and videos include no control over illumination, position, size, occlusion, and facial expressions. Successful facial processing methods for controlled scenarios are unable to pledge with challenging circumstances. Consequently, methods tailored for handling those situations are indispensable for the face analysis research progress. This work presents a comprehensive review of state-of-the-art methods, drawing attention to the complications derived from in the wild scenarios and the behavior differences when applied to the controlled images.The main topics to be covered are: (1) face detection; (2) facial image quality; (3) head pose estimation; (4) face alignment; (5) 3D face reconstruction; (6) gender and age estimation; (7) facial expressions and emotions; and (8) face recognition. Finally, available code and applications for in the wild face analysis are presented,followed by a discussion on future directions.	closed-circuit television;computer vision;emoticon;face detection;facial recognition system;hidden surface determination;human–computer interaction;image quality;social media	Flávio H. de Bittencourt Zavan;Nathaly Gasparin;Julio Cesar Batista;Luan Porfirio e Silva;Vitor Albiero;Olga R. P. Bellon;Luciano Silva	2017	2017 30th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T)	10.1109/SIBGRAPI-T.2017.11	face detection;facial recognition system;image quality;pose;over-illumination;computer vision;facial expression;computer science;artificial intelligence	Vision	40.3151924567636	-44.73570090912336	8916
eb94a588276c8e2532dfef01ca9b1cbdff2a620a	introducing a large dataset of persian license plate characters	infrared cameras;optical character recognition;distortion;image acquisition;roads;cameras;visible radiation	A large dataset of Persian license plate characters is introduced. These extracted characters are provided by Bani Nick Pardazesh Company, which is a pioneer in the intelligent transportation systems field, and its license plate recognition system has been used for many applications in Iran. Natural scene vehicle images delivered from this company were in various conditions. Most of them were taken with visible light during day and a few of them by infrared light with 850 nm wavelength during night. The vehicle images were achieved by color, black and white, or infrared cameras from front view and back view of automobiles in ∼20 different indoor and outdoor locations, such as streets, roads, and parking lots, for different purposes, such as traffic control, issuing fines, etc. The images are different in size and angle and were taken in light and dark backgrounds, where the direction and intensity of the light varied. Also, some of the license plates were muddy and had parts that were shadowed. Out of all the available images, ∼20;145 Persian characters were extracted by an intelligent system and verified by human observers. The extracted images are different in size, and some of them suffer from elimination, distortion, rotation, and noise. © 2014 SPIE and IS&T [DOI: 10.1117/1.JEI.23.2.023015]	artificial intelligence;automatic number plate recognition;color;distortion;global illumination;illumination (image);image noise;line printer daemon protocol	Amir Ebrahimi Ghahnavieh;Mahmoud Enayati;Abolghasem A. Raie	2014	J. Electronic Imaging	10.1117/1.JEI.23.2.023015	computer vision;distortion;telecommunications;computer science;optical character recognition;computer graphics (images)	Vision	42.58648021893401	-43.67715698170968	8925
eff22bc6a6fd69c9765b6cf30d409d30f949c23a	mwedge3: marker-free model reconstruction and motion tracking from 3d voxel data	shape from silhouette;image motion analysis;shape from silhouette technique m sup 3 marker free model reconstruction motion tracking 3d voxel data computer animation human motion video capture motion parameters optical markers kinematic structure marker free optical motion capture marker free method kinematic model voxel volumes volume sequences multiview video data;computer animation image motion analysis image reconstruction optical tracking;motion tracking;human subjects;motion capture;optical tracking;image reconstruction;human motion;tracking humans animation motion estimation kinematics high speed optical techniques biological system modeling layout character generation optical recording;computer animation	In computer animation, human motion capture from video is a widely used technique to acquire motion parameters. The acquisition process typically requires an intrusion into the scene in the form of optical markers which are used to estimate the parameters of motion as well as the kinematic structure of the performer. Marker-free optical motion capture approaches exist, but due to their dependence on a specific type of a priori model they can hardly be used to track other subjects, e.g. animals. To bridge the gap between the generality of marker-based methods and the applicability of marker-free methods, we present a flexible non-intrusive approach that estimates both, a kinematic model and its parameters of motion from a sequence of voxel-volumes. The volume sequences are reconstructed from multi-view video data by means of a shape-from-silhouette technique. The described method is well-suited for but not limited to motion capture of human subjects.	voxel	Edilson de Aguiar;Christian Theobalt;Marcus A. Magnor;Holger Theisel;Hans-Peter Seidel	2004		10.1109/PCCGA.2004.1348340	iterative reconstruction;computer vision;finger tracking;facial motion capture;match moving;structure from motion;motion capture;simulation;quarter-pixel motion;computer science;motion interpolation;motion estimation;optical flow;computer animation;motion field;motion compensation;computer graphics (images)	Vision	50.459293595129246	-47.47912737078084	8934
d1bf29cb68eca9057bee1cdc2b5ecdfe4cf0327d	feature selection inspired classifier ensemble reduction	text;storage management data mining data reduction feature selection learning artificial intelligence pattern classification;training accuracy educational institutions complexity theory diversity reception nickel cybernetics;classification of information;qa75 electronic computers computer science;feature extraction;期刊论文;classification performance feature selection classifier ensemble reduction machine learning data mining multiple classifiers predictive performance ensemble system group diversity memory requirement storage requirement ensemble predictions artificial feature;artificial intelligence;harmony search classifier ensemble reduction feature selection;article	Classifier ensembles constitute one of the main research directions in machine learning and data mining. The use of multiple classifiers generally allows better predictive performance than that achievable with a single model. Several approaches exist in the literature that provide means to construct and aggregate such ensembles. However, these ensemble systems contain redundant members that, if removed, may further increase group diversity and produce better results. Smaller ensembles also relax the memory and storage requirements, reducing system's run-time overhead while improving overall efficiency. This paper extends the ideas developed for feature selection problems to support classifier ensemble reduction, by transforming ensemble predictions into training samples, and treating classifiers as features. Also, the global heuristic harmony search is used to select a reduced subset of such artificial features, while attempting to maximize the feature subset evaluation. The resulting technique is systematically evaluated using high dimensional and large sized benchmark datasets, showing a superior classification performance against both original, unreduced ensembles, and randomly formed subsets.	aggregate data;algorithm;benchmark (computing);cdisc sdtm evaluator terminology;class;data mining;entity name part qualifier - adopted;feature selection;generalization (psychology);genetic selection;harmony search;heuristic;information;inspiration function;interpreter (computing);large;machine learning;occam's razor;overhead (computing);projections and predictions;randomness;redundancy (engineering);regular language description for xml;requirement;sample variance;silo (dataset);solutions;subgroup;transformation matrix	Ren Diao;Fei Chao;Taoxin Peng;Neal Snooke;Qiang Shen	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2281820	random subspace method;cascading classifiers;feature extraction;computer science;artificial intelligence;machine learning;linear classifier;pattern recognition;data mining;ensemble learning	ML	11.175745444114595	-45.677840667547926	8937
f0c2e3f144bc8107d0387f701389a1f1ea37c4ed	approximation algorithms for the primer selection, planted motif search, and related problems			approximation algorithm;primer;planted motif search	Sudha Balla;Jaime Davila;Sanguthevar Rajasekaran	2007		10.1201/9781420010749.ch75	approximation algorithm;planted motif search;artificial intelligence;primer (molecular biology);mathematics;machine learning;bioinformatics	ECom	-0.2546681598264824	-51.43192696645042	8940
97ceb4bd56ed5d4d90e90589168dcad352054499	hiring above the m-th best candidate: a generalization of records in permutations	hiring problem;on line decision making;secretary problem;random permutation;external research report	The hiring problem is a simple model of on-line decisionmaking under uncertainty. As in many other such models, the input is a sequence of instances and a decision must be taken for each instance depending on the subsequence examined so far, while nothing is known about the future. One famous example of on-line decision-making the secretary problem, formally introduced in the early sixties. Broder et al. (2008) introduced the hiring problem as an extension of the secretary problem. Instead of selecting only one candidate, we are looking for selecting (hiring) many candidates to grow up a small company. In this context, a hiring strategy should meet two demands: to hire candidates at some reasonable rate and to improve the average quality of the hired staff. Soon afterwards, Archibald and Mart́ınez (2009) introduced a discrete model of the hiring problem where candidates seen so far could be ranked from best to worst without the need to know their absolute quality scores. Hence the sequence of candidates could be modeled as a random permutation. Two general families of hiring strategies were introduced: hiring above the m-th best candidate and hiring in the top P% quantile (for instance, P = 50 is hiring above the median). In this paper we consider only hiring above the m-th best candidate. We introduce new hiring parameters that describe the dynamics of the hiring process, like the distance between the last two hirings, and the quality of the hired staff, like the score of the best discarded candidate. While Archibald and Mart́ınez made systematic use of analytic combinatorics techniques (Flajolet, Sedgewick, 2008) in their analysis, we use here a different approach to study the various hiring parameters related associated to the hiring process. We are able to obtain explicit formulas for the probability distribution or the probability generating function of the random variables of interest in a rather direct way. The explicit nature of our results also allows a very detailed study of their asymptotic behaviour. Adding our new results to those of Archibald and Mart́ınez leads to a very precise ? This work started when the first author was visiting the third author in a short stay supported by an FPI grant from the Spanish Ministry of Science. The first and the second authors were supported by project TIN2010-17254 (FRADA) from the Spanish Ministry of Science and Innovation. The third author was supported by the Austrian Science Foundation FWF, grant S9608-N23. quantitative characterization of the hiring above the m-th best candidate strategy. This might prove very useful in applications of the hiring process, e.g., in data stream algorithms.	andrei broder;fixed-point iteration;need to know;online and offline;random permutation;secretary problem;streaming algorithm	Ahmed Helmi;Conrado Martínez;Alois Panholzer	2012		10.1007/978-3-642-29344-3_40	mathematical optimization;random permutation;simulation;mathematics;secretary problem;statistics	Theory	3.2537094441690257	-0.4650349519842327	8943
3b3d67a15b1293aec71830a7981778e862706422	a 'hands on' strategy for teaching genetic algorithms to undergraduates	analogical model;genetic algorithms;expe- riential learning;constructivism;genetic algorithm	Genetic algorithms (GAs) are a problem solving strategy that uses stochastic search. Since their introduction (Holland, 1975), GAs have proven to be particularly useful for solving problems that are ‘intractable’ using classical methods. The language of genetic algorithms (GAs) is heavily laced with biological metaphors from evolutionary literature, such as population, chromosome, crossover, cloning, mutation, genes and generations. For beginners studying genetic algorithms, there is quite an overhead in gaining comfort with these terms and an understanding of their parallel meanings in the unfamiliar computing milieu of an evolutionary algorithm.	crossover (genetic algorithm);evolutionary algorithm;genetic algorithm;milieu intérieur;mutation (genetic algorithm);overhead (computing);population;problem solving;stochastic optimization	Anne Venables;Grace Tan	2007	JITE	10.28945/3132	genetic algorithm;pedagogy;mathematics education;computer science	AI	25.950476116560253	-8.516078345899096	8946
a79b145e0977175acc68f1b6f42baaf93700f9ef	hmxt-gp: an information-theoretic approach to genetic programming that maintains diversity	diversity;genetic program;evolutionary computation;genetic programming;hmxt;genetic algorithm;information theoretic;information theory;conference proceeding;evolutionary computing	This paper applies a recent information--theoretic approach to controlling Genetic Algorithms (GAs) called HMXT to tree--based Genetic Programming (GP). HMXT, in a GA domain, requires the setting of selection thresholds in a population and the application of high levels of crossover to thoroughly mix alleles. Applying these in a tree--based GP setting is not trivial. We present results comparing HMXT--GP to Koza--style GP for varying amounts of crossover and over three different optimisation (minimisation) problems. Results show that average fitness is better with HMXT--GP because it maintains more diversity in populations, but that the minimum fitness found was better with Koza. HMXT allows straightforward tuning of population diversity and selection pressure by altering the position of the selection thresholds.	genetic algorithm;genetic programming;information theory;mathematical optimization;population;software release life cycle	Henry Santosa;John Milton;Paul J. Kennedy	2011		10.1145/1982185.1982420	evolutionary programming;genetic programming;mathematical optimization;crossover;genetic algorithm;computer science;bioinformatics;artificial intelligence;genetic operator;machine learning;genetic representation;evolutionary computation	HCI	26.1721156850302	-7.430518842615463	8948
4dafd76c2b3f5b94ea68bcf43abb51e788b45957	constrained-action pomdps for multi-agent intelligent knowledge distribution		This paper addresses a fundamental question of multi-agent knowledge distribution: what information should be sent to whom and when, with the limited resources available to each agent? Intelligent Knowledge Distribution is a framework that answers these questions. Communication requirements for multi-agent systems can be rather high when an accurate picture of the environment and the state of other agents must be maintained. To reduce the impact of multi-agent coordination on systems, including communications, this paper introduces the concept of action-based constraints on partially observable Markov decision processes, rewards based upon the value of information driven by Kullback-Leibler Divergence, and probabilistic constraint satisfaction through discrete optimization and Markov chain Monte Carlo analysis. Intelligent Knowledge Distribution is driven by determining the information content an agent believes another agent will obtain by receiving certain information, along with the importance or relevance of that information to the system objective. To perform constraint analysis on an infinite-horizon policy, policies are represented as a Finite State Controller allowing Markov chain Monte Carlo analysis to determine a probabilistic level of guarantee that the constraints will be satisfied. The analysis of performance for an example mission presented in this paper shows the constrained controllers, during the highest constraint seen in simulations, can be constructed to meet minimal constraint guarantees (80%) while impacting the optimal value less than 50%, where the unconstrained optimal controller only satisfied the constraint 10% of the time.	constraint satisfaction;discrete optimization;interaction;kullback–leibler divergence;markov chain monte carlo;markov decision process;mathematical optimization;monte carlo method;multi-agent system;optimal control;optimization problem;partially observable markov decision process;partially observable system;randomness;real-time clock;relevance;requirement;self-information;simulation	Michael C. Fowler;Pratap Tokekar;T. Charles Clancy;Ryan K. Williams	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8461118	mathematical optimization;monte carlo method;probabilistic logic;control engineering;engineering;markov decision process;discrete optimization;value of information;markov chain monte carlo;markov process;constraint satisfaction	AI	20.4573736437307	-15.772619486637435	8949
6fd3a1a5597d0b7901704f68d6924d51d94c4a9f	unfazed by both the bull and bear: strategic exploration in dynamic environments	global trends;strategic choice;dynamic environments;exploration and exploitation	People in a changing environment must decide between exploiting options they currently favor and exploring alternative options that provide additional information about the state of the environment. For example, drivers must decide between purchasing gas at their currently favored station (i.e., exploit) or risk a fruitless trip to another station to evaluate whether the price has been lowered since the last visit. Previous laboratory studies on exploratory choice have found that people choose strategically and explore alternative options when it is more likely that the relative value of competing options has changed. Our study extends this work by considering how global trends (which affect all options equally) influence exploratory choice. For example, during an economic crisis, global gas prices may increase or decrease at all stations, yet consumers should still explore strategically to find the best option. Our research question is whether people can maintain effective exploration strategies in the presence of global trends that are irrelevant in that they do not affect the relative value of choice options. We find that people explore effectively irrespective of global trends.	purchasing;relevance;value (ethics)	Peter S. Riefer;Bradley C. Love	2015	Games	10.3390/g6030251	economics;operations management;microeconomics;commerce	HCI	-4.382429074610383	-9.207857305887284	8950
8debe4e668adf2c83c561697ca1f1b6206c4bfec	on solving the multi-period single-sourcing problem under uncertainty	splitting variables;twin node family;non anticipativity constraints;satisfiability;branch and fix coordination;mixed 0 1 programs;fix and relax coordination;cost of production;computer experiment;two stage stochastic	We present a framework for solving the strategic problem of assigning retailers to facilities in a multi-period single-sourcing product environment under uncertainty in the demand from the retailers and the cost of production, inventory holding, backlogging and distribution of the product. By considering a splitting variable mathematical representation of the Deterministic Equivalent Model, we specialize the so-called Branch-and-Fix Coordination algorithmic framework. It exploits the structure of the model and, specifically, the non-anticipativity constraints for the assignment variables. The algorithm uses the Twin Node Family (TNF) concept. Our procedure is specifically designed for coordinating the selection of the branching TNF and the branching S3 set, such that the non-anticipativity constraints are satisfied. Some computational experience is reported.	algorithm;computation	Antonio Alonso-Ayuso;Laureano F. Escudero;Celeste Pizarro;H. Edwin Romeijn;Dolores Romero Morales	2006	Comput. Manag. Science	10.1007/s10287-005-0043-z	mathematical optimization;computer experiment;operations management;mathematics;algorithm;satisfiability	AI	1.010839469474749	-0.18991182630842	8973
892cde707adae2a1fb1b35942b33095014405b59	on sensitivity ranking using analytic hierarchy process	analytic hierarchy process;multi parameter sensitivity analysis;sensitivity ranking;sensitivity analysis	We present an application of the Analytic Hierarchy Process (AHP) in the sensitivity analysis of a multi-parameter decision support problem. In this problem, the Multi-Parametric Sensitivity Analysis (MPSA) appears appropriate and is employed. A drawback of MPSA is that it only produces a sensitivity comparison between every two parameters, and does not offer a systematic method for ranking all parameters at once. Without the overall sensitivity ranking, it is difficult to interpret the analysis result from MPSA, especially when the number of parameters is large. Our paper shows that AHP can be used to perform such ranking based on MPSA's pair-wise comparisons.		Hung Dang	2007	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488507004613	econometrics;analytic hierarchy process;computer science;machine learning;mathematics;sensitivity analysis;statistics	Robotics	-3.177781304682288	-18.30826889177244	8988
22d0cc0ad498361d515b27e217e63b180fac3d10	computer recognition of occluded curved line drawings	thesis or dissertation;computer vision;pattern recognition systems			Mark Ronald Adler	1978			computer vision;speech recognition;computer science;sketch recognition;computer graphics (images)	Vision	48.617642752645246	-50.6744243221853	8991
3148b807120b0272780e83744d6c8117e84f1f81	fault diagnosis of components and sensors in hvac air handling systems with new types of faults		Air handling systems are the key sub-systems of heating ventilation and air conditioning (HVAC) systems. They condition and deliver air to satisfy human thermal comfort requirements and provide acceptable indoor air quality. Faults in their components and sensors may lead to high-energy consumption, poor thermal comfort, and unacceptable indoor air quality. Additionally, new types of faults may falsely be identified as known types. Identifying failure modes and their severities with low false identification rates is thus critical to know what faults occur and how severe they are. However, this is challenging, since 1) classifying both failure modes and fault severities generates many categories of failures, leading to high computational requirements; 2) updating model parameters to adapt to changing environments requires accurate recursive equations that are hard to obtain; and 3) model errors and measurement noise may cause high false identification rates in detecting new types of faults. In this paper, failure modes are identified by hidden Markov models (HMMs) and fault severities are estimated by filtering methods, leading to a decrease in the number of HMM states and low computational requirements. To adapt to changing environments, a new online learning algorithm is developed. In this algorithm, HMM parameters are obtained based on their posterior distributions given new observations, thereby avoiding the need for accurate recurrence equations. To identify new fault types with low false identification rates, a robust statistical method is developed to compare current HMM observations with those expected from existing states to obtain potential new types, and then confirm new types by checking whether observations have a significant change. Physical knowledge is then used to find the reason for the new fault type. Experimental results show that failure modes and fault severities of both known and new types of faults are identified with high accuracy.	adobe air;algorithm;computation;failure cause;fault injection;hidden markov model;identity document forgery;markov chain;online machine learning;recurrence relation;recursion;requirement;sensor	Ying Jian Yan;Peter B. Luh;Krishna R. Pattipati	2018	IEEE Access	10.1109/ACCESS.2018.2806373	reliability engineering;hvac;filter (signal processing);atmospheric model;indoor air quality;hidden markov model;distributed computing;computer science	SE	14.91793349009644	-15.285909305799006	9002
3559fcf1a1da859dab2645fe937a4a25d1603b08	efficient deformable registration of multi-resolution surfel maps for object manipulation skill transfer	public demonstration deformable registration multiresolution surfel maps object manipulation skill transfer mobile manipulation robots generalization capability rgb d images object instances object model local rigid transformation grasp strategy motion strategy local transformation deformation field runtime efficiency registration method skill transfer approach;motion control image colour analysis image registration image resolution manipulators mobile robots;three dimensional displays deformable models kernel shape image resolution standards approximation methods	Endowing mobile manipulation robots with skills to use objects and tools often involves the programming or training on specific object instances. To apply this knowledge to novel instances from the same class of objects, a robot requires generalization capabilities for control as well as perception. In this paper, we propose an efficient approach to deformable registration of RGB-D images that enables robots to transfer skills between object instances. Our method provides a dense deformation field between the current image and an object model which allows for estimating local rigid transformations on the object's surface. Since we define grasp and motion strategies as poses and trajectories with respect to the object models, these strategies can be transferred to novel instances through local transformations derived from the deformation field. In experiments, we demonstrate the accuracy and runtime efficiency of our registration method. We also report on the use of our skill transfer approach in a public demonstration.	algorithm;central processing unit;coherence (physics);collaborative product development;displacement mapping;experiment;graphics processing unit;ground truth;instance (computer science);map;mobile manipulator;real-time clock;reference frame (video);robot;surfel;transfer-based machine translation;vergence	Jörg Stückler;Sven Behnke	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6906975	computer vision;simulation;computer science;machine learning	Robotics	52.531562816508	-45.56440862261706	9015
ed0ae554991d1e434d895b06aaf95459d40d8fed	hardware particle swarm optimization based on the attractive-repulsive scheme for embedded applications	software;particle swarm optimisation embedded systems floating point arithmetic microcontrollers;microcontrollers;swarm intelligence;generators;premature convergence;pso algorithm;floating point arithmetic global optimization swarm intelligence fpgas;hardware computer architecture software field programmable gate arrays optimization mathematical model generators;computer architecture;embedded systems;large scale;attractive repulsive scheme;particle swarm optimizer;embedded applications;particle swarm optimization;fpgas;dynamic range;mathematical model;global optimization;optimization;floating point arithmetic particle swarm optimization attractive repulsive scheme pso algorithm premature convergence embedded applications microcontrollers hardware implementation parallel self adaptive algorithm;floating point arithmetic;field programmable gate arrays;parallel self adaptive algorithm;particle swarm optimisation;hardware implementation;software implementation;hardware	Particle Swarm Optimization (PSO) algorithms have been proposed to solve engineering problems that require to find an optimal point of operation. However, the PSO algorithm suffers from \emph{premature convergence} and high elapsed time when solving multimodal and large scale engineering problems. This problem becomes an evident drawback for embedded applications in which the micro controllers often operates at low computational capacity. This paper proposes a hardware implementation of a parallel self-adaptive PSO algorithm based on an attractive-repulsive scheme and using the efficient floating-point arithmetic which performs computations with large dynamic range and high precision. The parallel capabilities of the PSO are exploited by implementing parallel particles directly in hardware in order to decrease the running time. In addition, the attractive-repulsive technique avoids the \emph{premature convergence} problem by self-adapting the swarm behavior according to its diversity. Synthesis and simulation results for benchmark test problems were performed, demonstrating the correctness of the proposed architectures. Finally, an elapsed time comparison between the hardware and software implementations shows the suitableness of the proposed architecture for embedded applications.	algorithm;benchmark (computing);computation;correctness (computer science);dynamic range;embedded system;fitness function;mathematical optimization;mobile robot;motion planning;multimodal interaction;particle swarm optimization;premature convergence;simulation;time complexity	Daniel M. Muñoz Arboleda;Carlos H. Llanos;Leandro dos Santos Coelho;Mauricio Ayala-Rincón	2010	2010 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2010.73	embedded system;mathematical optimization;parallel computing;swarm intelligence;computer science;theoretical computer science;particle swarm optimization;field-programmable gate array;global optimization	EDA	28.27229608718551	-1.9049821465192447	9034
a4543226f6592786e9c38752440d9659993d3cb3	pose-independent facial action unit intensity regression based on multi-task deep transfer learning		Facial expression recognition plays an increasingly important role in human behavior analysis and human computer interaction. Facial action units (AUs) coded by the Facial Action Coding System (FACS) provide rich cues for the interpretation of facial expressions. Much past work on AU analysis used only frontal view images, but natural images contain a much wider variety of poses. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) requires participants to estimate the AU occurrence and intensity under nine different pose angles. This paper proposes a multi-task deep network addressing the AU intensity estimation sub-challenge of FERA 2017. The network performs the tasks of pose estimation and pose-dependent AU intensity estimation simultaneously. It merges the pose-dependent AU intensity estimates into a single estimate using the estimated pose. The two tasks share transferred bottom layers of a deep convolutional neural network (CNN) pre-trained on ImageNet. Our model outperforms the baseline results, and achieves a balanced performance among nine pose angles for most AUs.	artificial neural network;bcs-facs;baseline (configuration management);computer multitasking;convolutional neural network;human computer;human–computer interaction;imagenet;job control (unix);overfitting;pose (computer vision);statistical classification	Yuqian Zhou;Jimin Pi;Bertram E. Shi	2017	2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)	10.1109/FG.2017.112	transfer of learning;convolutional neural network;artificial intelligence;facial action coding system;pose;computer vision;facial expression;computer science	Vision	29.564155995402714	-51.635808745105855	9042
37acc32cec736ed31c0e5b5390103e6d65fd3977	image encryption using binary bitplane	image encryption;security analysis;bit level permutation;bitplane decomposition	To enhance security of the bitplane decomposition based image encryption methods, this paper introduces a novel image encryption algorithm using a bitplane of a source image as the security key bitplane to encrypt images. Users have the flexibility to choose (1) any existing or newly generated image as the source image; (2) any decomposition method for generating the bitplane; (3) any decomposed bitplane as the security key bitplane; (4) any scrambling method for the bit-level permutation. As an example, this paper also proposes a bit-level scrambling algorithm to change bit positions. Simulations and security analysis are provided to demonstrate an excellent encryption performance of the proposed algorithm.	encryption	Yicong Zhou;Weijia Cao;C. L. Philip Chen	2014	Signal Processing	10.1016/j.sigpro.2014.01.020	computer science;theoretical computer science;internet privacy;security analysis;computer security	Arch	38.82797376273464	-9.114984294280509	9045
112deecd781b2b5046c09856db3f17cafd5b8550	dynamic pattern extraction of parameters in laser welding process	laser welding;decision maker;data mining;rough set;historical data	Tuning parameters is essential for the results of the welding process. In order to optimize the tuning process of welding parameters, we propose a system based on historical data of laser welding machines. On a given combination of materials, the system extracts patterns dynamically and classifies new cases with a relative accuracy, which depends on the selected data set. The analysis of the generated patterns helps decision makers to visualize important features in large databases and therefore, achieve optimal results.	pattern recognition	Gissel Velarde;Christian Binroth	2010		10.1007/978-3-642-14400-4_27	decision-making;rough set;computer science;machine learning;data mining;laser beam welding	Vision	1.2099293615322924	-31.494538420954992	9047
76a13d288122cf30f18d48c5995bb644f506cda5	structure optimization for deep multimodal fusion networks using graph-induced kernels		A popular testbed for deep learning has been multimodal recognition of human activity or gesture involving diverse inputs such as video, audio, skeletal pose and depth images. Deep learning architectures have excelled on such problems due to their ability to combine modality representations at different levels of nonlinear feature extraction. However, designing an optimal architecture in which to fuse such learned representations has largely been a non-trivial human engineering effort. We treat fusion structure optimization as a hyper-parameter search and cast it as a discrete optimization problem under the Bayesian optimization framework. We propose a novel graph-induced kernel to compute structural similarities in the search space of tree-structured multimodal architectures and demonstrate its effectiveness using two challenging multimodal human activity recognition datasets.	activity recognition;bayesian optimization;deep learning;discrete optimization;feature extraction;human factors and ergonomics;kernel (operating system);mathematical optimization;modality (human–computer interaction);multimodal interaction;nonlinear system;optimization problem;testbed	Dhanesh Ramachandram;Michal Lisicki;Timothy J. Shields;Mohamed R. Amer;Graham W. Taylor	2017	CoRR		machine learning;fusion;artificial intelligence;pattern recognition;computer science;graph	AI	26.116701769222065	-48.81097973927733	9051
c13a478468475dc4d414b4a8d61a9bbdef02eade	on optimizing search efforts (area effectively swept) allocation in the course of search and rescue operations	search and rescue;detection probability;keyword search;a priori information	A solution is considered for a problem of optimal distribution of search efforts among areas while running search and rescue  operations under limited search resources and probabilistic nature of a priori information about the location of an emergency  object.   	optimizing compiler	Andrey Makshanov;Victor Ermolaev	2009		10.1007/978-3-642-00304-2_20	simulation;engineering;data mining;world wide web	Theory	53.03999209308826	-26.318285981834997	9055
bb57ba1bffa174b5b6532fa2902dc1cfb0134eb3	a reinforcement learning technique with an adaptive action generator for a multi-robot system	autonomous specialization;learning process;reinforcement learning;multi robot system;learning environment;discriminant function;linear interpolation;environmental change;computer simulation;action search	A robust instance-based reinforcement learning (RL) approach for controlling autonomous multi-robot systems (MRS) is introduced in this chapter. Although RL has been proven to be an effective approach for behavior acquisition for an autonomous robot, it generates considerably sensitive results for the segmentation of the state and action spaces. This problem can yield severe results with increase in the complexity of the system. When segmentation is inappropriate, RL often fails. Even if RL obtains successful results, the achieved behavior might not be sufficiently robust. In conventional RL, human designers segment the state and action spaces by using implicit knowledge based on their personal experience, because there are no guidelines for segmenting the state and action spaces. Two main approaches for solving the abovementioned problem and for learning in a continuous space have been discussed. One of the methods applies function-approximation techniques such as artificial neural networks to the Q-function. Sutton (Sutton, 1996) used CMAC and Morimoto and Doya (Morimoto & Doya, 2000) used Gaussian softmax basis functions for function approximation. Lin represented the Q-function by using multi-layer neural networks called Q-net (Lin, 1993). However, these techniques have the inherent difficulty that a human designer must properly design their neural networks before executing RL. The other method involves the adaptive segmentation of the continuous state space according to the robots' experiences. Asada et al. proposed a state clustering method based on the Mahalanobis distance (Asada et al., 1996). Takahashi et al. used the nearest-neighbor method (Takahashi et al., 1996). However, these methods generally require large learning costs for tasks such as the continuous update of data classifications every time new data arrives. Our research group has proposed an instance-based RL method called the continuous space classifier generator (CSCG), which proves to be effective for behavior acquisition (Svinin et al., 2000). We have also developed a second instance-based RL method called Bayesiandiscrimination-function-based reinforcement learning (BRL) (Yasuda et al., 2005). Our preliminary experiments proved that BRL, by means of adaptive segmentation of state and action spaces, exhibits better performance as compared to CSCG. As we mentioned in the previous chapter, BRL has an extended form that accelerates the learning speed (Yasuda & Ohkura, 2010). Our focal point for the extension is the process of action searching. In a standard BRL, a robot performs a random action and stores an input-	adaptive grammar;approximation;artificial neural network;autonomous robot;basis function;cluster analysis;convex function;experiment;focal (programming language);layer (electronics);minimal recursion semantics;one-key mac;reinforcement learning;robustness (computer science);softmax function;state space;tomotaka takahashi;word lists by frequency	Toshiyuki Yasuda;Kazuhiro Ohkura	2008		10.1007/978-3-540-69134-1_25	computer simulation;error-driven learning;simulation;environmental change;computer science;artificial intelligence;machine learning;discriminant function analysis;linear interpolation;reinforcement learning	AI	18.728219981687392	-21.558320063341554	9056
22b5ecddb8f58f0fccaac29a02aca7b80883f2ed	a novel teaching-learning-based optimization algorithm for energy-efficient scheduling in hybrid flow shop		Hybrid flow shop scheduling problem (HFSP) has been extensively discussed and the main objectives are related to completion time. The reduction of energy consumption should be considered fully in HFSP in the era of green manufacturing. In this study, biobjective energy-efficient HFSP is considered, which is made up of three subproblems including scheduling, machine assignment, and speed selection. A three-string coding method is used to indicate solutions of three subproblems. A new teachers’ teaching-learning-based optimization (TTLBO) is proposed to minimize total energy consumption and total tardiness. Total tardiness is regarded as a key objective and a lexicographical method is adopted to compare solutions. TTLBO generates new solutions using a new optimization mechanism and is made up of the self-learning, interactive learning, and teaching of teachers. The learning phase of students are deleted from the algorithm. Multiple neighborhood searches are used to implement the self-learning of teachers and global search based on crossover is chosen to imitate other tivities of teachers. A number of experiments are conducted to test the impact of the new optimization meachanism on the performance of TTLBO and compare TTLBO with other algorithms from the literature. The computational results show that TTLBO is a competitive algorithm for the considered HFSP.	algorithm;computation;experiment;flow shop scheduling;lexicography;mathematical optimization;optimization mechanism;scheduling (computing)	Deming Lei;Liang Gao;Youlian Zheng	2018	IEEE Transactions on Engineering Management	10.1109/TEM.2017.2774281	engineering;tardiness;job shop scheduling;crossover;energy consumption;efficient energy use;scheduling (computing);algorithm;flow shop scheduling;interactive learning	AI	21.637771058346875	-0.5990515745744924	9069
8afb10761e9864bc43840ba06e42b4cb056b5640	wearable autonomous microsystem with electrochemical gas sensor array for real-time health and safety monitoring	environmental monitoring geophysics;air pollution measurement;air pollutants algorithms computer systems electrochemical techniques environmental monitoring equipment design explosive agents humans sulfur dioxide;silicon compounds;occupational health;electrochemical sensors;sensor fusion;gas sensors;health hazards;gas detectors sensor arrays gases real time systems sensor systems;sensor arrays;intelligent sensors;electrochemical sensors wearable autonomous microsystem electrochemical gas sensor array real time health monitoring real time safety monitoring airborne pollution explosive gases occupational safety multi analyte gas sensor system synergistic integration so 2 thumb drive sized prototype system so 2 sensor integration methodologies system design analyte mixture concentration estimation gas classification embedded sensor array signal processing algorithms single microelectronics chip signal condition functions multi mode electrochemical instrumentation circuit ionic liquids;silicon compounds air pollution measurement electrochemical sensors environmental monitoring geophysics gas sensors health hazards intelligent sensors occupational health sensor arrays sensor fusion	Airborne pollution and explosive gases threaten human health and occupational safety, therefore generating high demand for a wearable autonomous multi-analyte gas sensor system for real-time environmental monitoring. This paper presents a system level solution through synergistic integration of sensors, electronics, and data analysis algorithms. Electrochemical sensors featuring ionic liquids were chosen to provide low-power room-temperature operation, rapid response, high sensitivity, good selectivity, and a long operating life with low maintenance. The system utilizes a multi-mode electrochemical instrumentation circuit that combines all signal condition functions within a single microelectronics chip to minimize system cost, size and power consumption. Embedded sensor array signal processing algorithms enable gas classification and concentration estimation within a real-world mixture of analytes. System design and integration methodologies are described, and preliminary results are shown for a first generation SO2 sensor and a thumb-drive sized prototype system.	airborne ranger;algorithm;analog-to-digital converter;autonomous robot;biasing;cmos;embedded system;embedding;gases;instrument - device;ionic liquids;liquid substance;low-power broadcasting;national institute for occupational safety and health (u.s.);numerous;occupational diseases;potentiostat;prototype;real-time clock;selectivity (electronic);sensor web;signal processing;sulfur dioxide;synergy;usb flash drive;wearable computer;analyte;sensor (device)	Haitao Li;Xiaoyi Mu;Zhe Wang;Xiaowen Liu;Min Guo;Rong Jin;Xiangqun Zeng;Andrew J. Mason	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6345978	computer vision;electronic engineering;computer science;engineering;electrical engineering;nanotechnology;sensor fusion;occupational safety and health;electro-optical sensor;intelligent sensor	Robotics	47.2588623636349	-0.7251633151625579	9071
76cf3d837d8a2ba002b8d248a2429771314275d2	tutoring strategies in game-tree search	ciencias basicas y experimentales	Introduction. According to the analysis of grandmaster-like strategies in Shogi [Iida and Uiterwijk 1993], it is important for a teacher, at the beginning stages of teaching, to intentionally lose an occasional game against a novice opponent, or to play less than optimally in order to give the novice some prospects of winning, without this being noticed by the opponent. Such a strategy is called a tutoring strategy in game-tree search. In this work we consider a loss-oriented search strategy (LO-search for short), by which a player attempts to lose a game, and we show an algorithm for LOsearch based on the minimax strategy and OM-search (opponent-model search; see [Iida et al. 1993]). We further describe characteristics of LO-search, including the concept of intentional error. We next discuss the situation in which a player will notice such an intentional error. We then describe a tutoring search (TUsearch for short) by which a player attempts to lose a game without this being noticed by the opponent. LO-search is based on OM-search, since LO-search also takes the opponent model into account. OM-search is predicated on the assumption that one has complete knowledge of the opponent’s strategy and that this strategy is completely contained in the opponent’s evaluation function. In OM-search, two values are computed for all positions in a search tree. For clarity, the players are distinguished as a max player and a min player.	algorithm;evaluation function;manhunters;maxima and minima;minimax;search tree;tree traversal	Hiroyuki Iida;Ken'ichi Handa;Jos Uiterwijk	1995	ICGA Journal	10.3233/ICG-1995-18402	machine learning;game tree;artificial intelligence;computer science	AI	18.619215091089735	-12.638992688282176	9083
48e0f5a0411531aaff59003e0f53bf072f28a24a	list viterbi decoding of convolutional codes for efficient data hiding	discrete wavelet transforms;convolutional code;wavelet domain information hiding;watermarking;data hiding;convolutional codes;list viterbi algorithm;image coding;concatenated rcpc crc codes;data compression;decoding;information extraction;concatenated rcpc crc codes list viterbi decoding convolutional codes efficient data hiding list viterbi algorithm wavelet domain information hiding turbo codes digital images wavelet transformed images information embedding information extraction;wavelet transforms convolutional codes concatenated codes cyclic redundancy check codes image coding transform coding data compression viterbi decoding;information hiding;cyclic redundancy check codes;random number generation;turbo codes;transform coding;data mining;data encapsulation;wavelet transforms;viterbi algorithm decoding convolutional codes data encapsulation discrete wavelet transforms watermarking data mining digital images turbo codes random number generation;information embedding;viterbi algorithm;viterbi decoder;efficient data hiding;wavelet transformed images;concatenated codes;digital image;list viterbi decoding;digital images;viterbi decoding;turbo code	The decoding of convolutional codes using the list Viterbi algorithm is proposed for data hiding applications. The performance of this technique is evaluated for wavelet-domain information hiding and is shown in many cases to be advantageous in comparison to the widely used turbo codes for efficient extraction of information embedded in digital images.	convolutional code;viterbi decoder	Nikolaos Thomos;Nikolaos V. Boulgouris;Dimitrios Simitopoulos;Michael G. Strintzis	2002		10.1109/ICIP.2002.1039005	list decoding;turbo code;soft output viterbi algorithm;convolutional code;speech recognition;sequential decoding;viterbi algorithm;computer science;theoretical computer science;serial concatenated convolutional codes;pattern recognition;linear code;information hiding;error floor;information extraction;viterbi decoder;digital image;iterative viterbi decoding;statistics	NLP	42.15280108234525	-12.04979401304015	9100
fb2336fb1d6bd686b8483ffea23dc475b3ea2c88	the annealing evolution algorithm as function optimizer	minimisation;parallel genetic algorithm;algoritmo paralelo;minimization;evaluation performance;optimisation;parallel algorithm;algorithm performance;performance evaluation;optimizacion;simulated annealing algorithm;continuous variables;evaluacion prestacion;continuous variable;search strategy;minimizacion;simulated annealing;algoritmo genetico;global minimization;algorithme parallele;function optimization;recuit simule;resultado algoritmo;strategie recherche;continous variables;performance algorithme;algorithme genetique;evolutionary strategy;genetic algorithm;genetic algorithms;recocido simulado;optimization;estrategia investigacion	In this paper an annealing evolution algorithm is applied to the optimization of continuous problems. The algorithm uses an evolutionary strategy to guide the search in simulated annealing, Furthermore, we discuss an implementation of the algorithm and compare its performance with the conventional simulated annealing algorithm and the parallel genetic algorithm. The performance evaluation is carried out for a standard set of test functions from the literature. Here a breakthrough can be reported. The annealing evolution algorithm is able to find the global minimum of Rastrigin’s function of dimension 500 on the VAX 8600 machine.	distribution (mathematics);evolution;genetic algorithm;mathematical optimization;maxima and minima;performance evaluation;simulated annealing;vax 8000	Yong Liu;Lishan Kang;David J. Evans	1995	Parallel Computing	10.1016/0167-8191(94)00078-O	mathematical optimization;genetic algorithm;simulated annealing;computer science;artificial intelligence;hill climbing;adaptive simulated annealing;algorithm	Vision	27.813201407931416	1.5869764050997752	9110
ba97bee7254831a2a9dfb86c170a111c4c248413	online human interaction detection and recognition with multiple cameras	histograms;kernel;kernel trajectory cameras tracking histograms hilbert space geometry;geometry;object recognition behavioural sciences computing geometry hilbert spaces image fusion image sensors learning artificial intelligence object detection;hilbert space;human behavior analysis human interaction recognition segmentation detection kernel parity space multiple camera video surveillance;trajectory;segmentation detection human behavior analysis human interaction recognition kernel parity space multiple camera video surveillance;real time human behavior analysis online human interaction detection online human interaction recognition multiple cameras temporal trajectories kernel state space models kss models hilbert space linear operators geometry pairwise kernels kernel construction technique multiview learning technique single view publicly available data sets;cameras;tracking	We address the problem of detecting and recognizing online the occurrence of human interactions as seen by a network of multiple cameras. We represent interactions by forming temporal trajectories, coupling together the body motion of each individual and their proximity relationships with others, and also sound whenever available. Such trajectories are modeled with kernel state-space (KSS) models. Their advantage is being suitable for the online interaction detection, recognition, and also for fusing information from multiple cameras, while enabling a fast implementation based on online recursive updates. For recognition, in order to compare interaction trajectories in the space of KSS models, we design so-called pairwise kernels with a special symmetry. For detection, we exploit the geometry of linear operators in Hilbert space, and extend to KSS models the concept of parity space, originally defined for linear models. For fusion, we combine KSS models with kernel construction and multiview learning techniques. We extensively evaluate the approach on four single view publicly available data sets, and we also introduce, and will make public, a new challenging human interactions data set that we have collected using a network of three cameras. The results show that the approach holds promise to become an effective building block for the analysis of real-time human behavior from multiple cameras.	algorithm;computation;fragmented object;hilbert space;interaction;kernel (operating system);kernel method;linear model;real-time clock;real-time computing;recursion;sensor;state space	Saeid Motiian;Farzad Siyahjani;Ranya Almohsen;Gianfranco Doretto	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2606998	computer vision;kernel;kernel embedding of distributions;trajectory;machine learning;pattern recognition;histogram;mathematics;tracking;hilbert space	Vision	36.83323166965808	-48.25817507982968	9116
8e245a6db51d695880115943627f5085b590bca5	software fault inference based on expert opinion		Software fault prediction is a process which predicts that the software modules are faulty or not by using the software metrics and some soft computing methods. Software metrics are divided into two main categories such as object-oriented and method-level metrics. While class relationships and dependencies are covered by object-oriented metrics, behaviors of the classes can be also measured by method-level metrics. Actually, the complementary relationship between these metric groups is focused in this study and different predictive models are built by using different parameter sets. Each parameter set includes some object-oriented and some method-level metrics. Furthermore, Mamdani style fuzzy inference system (FIS) is employed here to predict faultiness. In contrast to data-driven methods, FIS does not require historical or previous data for modeling. In fact, it is a rule-based approach and rules are extracted with the help of domain experts. In this study, the dataset which consists of the method-level and the class-level metrics’ values that are collected from KC1 project of PROMISE repository is employed and most successful model whose performance is 0.8181 according to the evaluation criteria (the area under receiver operating characteristics (ROC) curve (AUC)) is built with the parameters of “coupling between object”, “line of code” and, “cyclomatic complexity”.	cyclomatic complexity;inference engine;logic programming;predictive modelling;receiver operating characteristic;serial ata;soft computing;software metric;source lines of code	Ezgi Erturk;Ebru Akcapinar Sezer	2015	JSW	10.17706/jsw.10.6.757-766	computer science;artificial intelligence;machine learning;data mining;database;algorithm;statistics	SE	3.576747906713236	-33.47008814602691	9137
09a4fff55e464e115c666c6261a5d46ecfa511d2	finding location using omnidirectional video on a wearable computing platform	bayesian framework;indoor navigation;context awareness;feature detection;bayes methods;mobile robotics;low resolution;wearable computers mobile robots robot sensing systems computer vision cameras feedback wearable sensors training data sensor systems robot vision systems;bayes methods feature extraction navigation notebook computers;navigation;condensation algorithm omnidirectional video wearable computing platform navigation system indoor environment bayesian framework posterior distribution image similarity;posterior distribution;feature extraction;indoor environment;state space;initial condition;condensation algorithm;time use;notebook computers;wearable computer;navigation system;image similarity	In this paper we present a framework for a navigation system in an indoor environment using only omnidirectional video. Within a Bayesian framework we seek the appropriate place and image from the training data to describe what we currently see and infer a location. The posterior distribution over the state space conditioned on image similarity is typically not Gaussian. The distribution is represented using sampling and the location is predicted and verified over time using the condensation algorithm. The system does not require complicated feature detection, but uses a simple metric between two images. Even with low resolution input, the system may achieve accurate results with respect to the training data when given favorable initial conditions.	360-degree video;augmented reality;condensation algorithm;feasible region;feature detection (computer vision);feature detection (web development);image resolution;initial condition;mobile robot;motion estimation;naruto shippuden: clash of ninja revolution 3;omnidirectional camera;sampling (signal processing);state space;statistical model;wearable computer	Wasinee Rungsarityotin;Thad Starner	2000	Digest of Papers. Fourth International Symposium on Wearable Computers	10.1109/ISWC.2000.888466	embedded system;computer vision;navigation;simulation;image resolution;wearable computer;feature extraction;computer science;state space;machine learning;feature detection;posterior probability;initial value problem	Mobile	50.875833569038555	-35.21845641983602	9153
77e96c2e442b53f827848fc27a32483eea740ba1	accurate distortion measurement using analytical model for the b-spline-based shape coding	analytical models;quantization;spline;image coding;quantization process;contour point distortion measurement analytical model b spline based shape coding quantization process parameterization process reconstruction quality assessment euclidean distance;reconstruction quality assessment;distortion measurement;euclidean distance;distortion measurement encoding shape analytical models quantization spline approximation methods;parameterization process;splines mathematics;contour point;splines mathematics distortion measurement image coding;b spline based shape coding;shape;approximation methods;encoding;analytical model	In this paper, we present a new model to measure the contour point distortion for the B-spline-based shape coding, called accurate distortion measurement using analytical model (ADMAM). It models the distortion as the shortest distance of associate contour point from the approximating B-spline. Observing that this shortest path is perpendicular to the tangent vector of the approximating B-spline, we construct a parametric cubic equation by setting their dot product to be zero and address it by using Cartan's formula. The theoretical property and the experiments presented demonstrate that ADMAM can lead to the smallest bit-rate among all the distortion measurements that can guarantee the admissible distortion, when the operational rate-distortion optimal shape coding framework is applied. Moreover, if the original contour has NC points, it takes only O(NC) time for both a peak and mean-squared segment distortion measuring paradigms, which is the lowest complexity among all the existing distortion measurements.	b-spline;cubic function;distortion;experiment;shortest path problem	Zhongyuan Lai;Zhen Zuo;Zhe Wang;Wenyu Liu	2011	2011 Data Compression Conference	10.1109/DCC.2011.69	spline;mathematical optimization;combinatorics;distortion;quantization;shape;euclidean distance;mathematics;geometry;encoding	Vision	50.21719101935207	-15.963700659715654	9170
58394a303bfc51ba6ace08327777505843a5708d	convex optimization approach for multi-label feature selection based on mutual information	time complexity;convex functions;redundancy;linear programming;mutual information;optimization;entropy	We propose a convex optimization approach for multi-label feature selection. The effective feature subset can be obtained through finding a global optima of a convex objective function for multi-label feature selection. However conventional greedy approaches are prone to suboptimal result. In this paper, the mathematical procedures and considerations for the optimization approach are presented for multi-label feature selection based on mutual information. We compared the proposed method with conventional greedy search based methods to show the potential of optimization based multi-label feature selection.	convex function;convex optimization;experiment;feature selection;greedy algorithm;loss function;mathematical optimization;multi-label classification;mutual information;optimization problem;selection (genetic algorithm)	Hyunki Lim;Dae-Won Kim	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899851	convex function;time complexity;entropy;mathematical optimization;convex optimization;nonlinear programming;linear programming;machine learning;pattern recognition;mathematics;redundancy;mutual information;feature selection;proper convex function	Vision	21.980336745790975	-39.105023019926854	9178
e102fd20143db61656b7011190fd6f0c0315e961	variable selection in high-dimensional varying-coefficient models with global optimality	model selection;truncated l 1 penalty;oracle property;difference convex programming;l subscript 0 regularization;large p small n;coordinate decent algorithm;truncated l subscript 1 penalty;l 0 regularization;nonparametric function;article;truncated l1 penalty;l0 regularization	The varying-coefficient model is flexible and powerful for modeling the dynamic changes of regression coefficients. It is important to identify significant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefficient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefficient modeling could be infinite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefficient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efficient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches.	approximation algorithm;coefficient;convex optimization;feature selection;global optimization;lasso;least squares;model selection;numerical analysis;rate of convergence;selection algorithm;simulation;smoothing;variable (computer science)	Lan Xue;Annie Qu	2012	Journal of Machine Learning Research		econometrics;mathematical optimization;mathematics;model selection;statistics	ML	29.231802437488327	-25.47387545184447	9183
0d19a9e5426847343d3f277093e63597f5aa7bcc	g-knn: an efficient document classification algorithm for sparse datasets on gpus using knn	gpu;data mining applications;evaluation;parallel algorithms	In nowadays we observe that there is more data than that can be effectively analyzed. Organizing this data has become one of the biggest problems in Computer Science. Many algorithms have been proposed for this purpose, highlighting those related to the Data Mining area, specifically the automatic document classification (ADC) algorithms. However, these algorithms are still a computational challenge because of the volume of data that needs to be processed. We found in the literature some proposals related to parallelization on graphics processing units (GPUs) to make these algorithms feasible. Still, most of the available parallel solutions ignore specific ADC challenges, such as high dimensionality and heterogeneity in the representation of the documents. In this context, we here present G-KNN, a GPU-based parallel version of the nearest neighbors algorithm (KNN), one of the most widely used ADC algorithms. In our evaluation using five different document collections, we show that the G-KNN can maintain the same classification effectiveness while increasing the efficiency by up to 12x faster than its sequential version using CPU and up to 3x faster than a CPU-based parallel implementation running with 6 threads. Moreover, our algorithm has a much lower memory consumption, enabling its use with large datasets.	baseline (configuration management);central processing unit;computation;computer graphics;computer science;data mining;data structure;document classification;graphics processing unit;k-nearest neighbors algorithm;parallel computing;sorting;sparse matrix;speedup;thrust	Leonardo C. da Rocha;Gabriel Spada Ramos;Rodrigo Chaves;Rafael Sachetto Oliveira;Daniel Madeira;Felipe Viegas;Guilherme Andrade;Sérgio Daniel;Marcos André Gonçalves;Renato A. C. Ferreira	2015		10.1145/2695664.2695967	computer science;theoretical computer science;evaluation;operating system;machine learning;data mining;database;analysis of parallel algorithms;parallel algorithm;cost efficiency	ML	-3.370858592860732	-39.869812650245414	9186
e12ebe98af607f5cb5d84a8f44b4695e3c6eb312	nadir compromise programming	stock market;optimal method;portfolio selection;multi objective optimization;decision maker;compromise programming	In problem of portfolio selection, financial Decision Makers (DMs) explain objectives and investment purposes in the frame of multi-objective mathematic problems which are more consistent with decision making realities. At present, various methods have introduced to optimize such problems. One of the optimization methods is the Compromise Programming (CP) method. Considering increasing importance of investment in financial portfolios, we propose a new method, called Nadir Compromising Programming (NCP) by expanding a CP-based method for optimization of multi-objective problems. In order to illustrate NCP performance and operational capability, we implement a case study by selecting a portfolio with 35 stock indices of Iran stock market. Results of comparing the CP method and proposed method under the same conditions indicate that NCP method results are more consistent with DM purposes. 2010 Elsevier Ltd. All rights reserved.	mathematical optimization;newton's method	Maghsoud Amiri;Mostafa Ekhtiari;Mehdi Yazdani	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.12.061	decision-making;multi-objective optimization;management science	AI	-3.3567966022175435	-15.844372823777581	9195
880379e7c8c2811617ca0b1509bc25a97447078e	internal topographical structure in training autonomous robot	robot sensing systems;legged locomotion;mobile robot;training;topographical structure;mobile robots;neurons robot sensing systems training vectors legged locomotion;vectors;hierarchical learning;self organizing map;neurocontrollers;e puck internal topographical structure autonomous robot training trainable controller mobile robot layered neural network learning phase;neurons;mobile robot self organizing map topographical structure hierarchical learning;learning artificial intelligence;neurocontrollers learning artificial intelligence mobile robots	In this research we propose a trainable controller for a mobile robot based on a layered neural network, in which the hidden layer is a topographical map. In this study we focus not only on building a general controller that can be embedded to mobile robots running in physical environment, but also on building controllers with good internal plausibility. We consider that internal plausibility of the physical functionality acquired by robots during their learning phase is important in increasing their usability in real world tasks. The internal plausibility in this study can be obtained by associating the topographical map formed internally with the actions of the robots. Here, we run some experiments using a small robot, e-puck, and report the preliminary results of our study.	artificial neural network;autonomous robot;e-puck mobile robot;embedded system;experiment;plausibility structure;topography;usability	Pitoyo Hartono;Thomas P. Trappenberg	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6083672	mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	50.2655218461139	-29.708867023438945	9202
4f9ce1b1be3b6750d8d771ff6b9d5148ba3a1e57	ranking systems: the pagerank axioms	multi agent system;e commerce;satisfiability;pagerank;internet technology;multi agent systems;social choice;ranking algorithm;representation theorem;axiomatic theory	This paper initiates research on the foundations of ranking systems, a fundamental ingredient of basic e-commerce and Internet Technologies. In order to understand the essence and the exact rationale of page ranking algorithms we suggest the axiomatic approach taken in the formal theory of social choice. In this paper we deal with PageRank, the most famous page ranking algorithm. We present a set of simple (graph-theoretic, ordinal) axioms that are satisfied by PageRank, and moreover any page ranking algorithm that does satisfy them must coincide with PageRank. This is the first representation theorem of that kind, bridging the gap between page ranking algorithms and the mathematical theory of social choice.	algorithm;axiomatic system;bridging (networking);design rationale;e-commerce;graph theory;ordinal data;pagerank	Alon Altman;Moshe Tennenholtz	2005		10.1145/1064009.1064010	axiomatic system;mathematical optimization;combinatorics;discrete mathematics;social choice theory;computer science;artificial intelligence;theoretical computer science;machine learning;multi-agent system;mathematics;algorithm;satisfiability	ECom	1.364735562290489	-19.995191822521896	9204
c7821e785f26cc2231e85878d4ae08df5e2538fd	high accuracy event detection for non-intrusive load monitoring		This paper proposes a new event detection algorithm for the use in Non-Intrusive Load Monitoring (NILM). This latter is a field where the main concern is to break down, in a non-intrusive manner, the global electrical energy consumption into individual appliances consumption. Detecting events is thus of importance for appliance clustering in event-based NILM systems. A simple and fast algorithm that detects the variations of the signal's envelope is proposed in this paper. Its main advantage is the high localization accuracy of the start times of events. Its performance is evaluated using simulated and real data and is compared to one of the recently proposed algorithms in the field. Simulations show that the proposed detection algorithm gives 100 % precision and 97.13 % recall at a Signal-to-Noise Ratio (SNR) of 50 dB.	algorithm;cluster analysis;computer simulation;signal-to-noise ratio	Mohamed Nait Meziane;Philippe Ravier;Guy Lamarque;Jean-Charles Le Bunetel;Yves Raingeaud	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952597	start times;steady state;real-time computing;cluster analysis;computer science;electric potential energy;detector;signal-to-noise ratio	Robotics	35.513196047284985	-31.613346246180058	9216
ce0b0e16df60fc3b187a1f986947ce700dbcd26b	classification of web documents using fuzzy logic categorical data clustering	web documents;dissimilarity measure;fuzzy logic;fuzzy clustering;hamming distance;minimum distance;smallest of;number of clusters;categorical data	We propose a categorical data fuzzy clustering algorithm to classify web documents. We extract a number of words for each thematic area (category) and then, we treat each word as a multidimensional categorical data vector. For each category, we use the algorithm to partition the available words into a number of clusters, where the center of each cluster corresponds to a word. To calculate the dissimilarity measure between two words we use the Hamming distance. Then, the classification of a new document is accomplished in two steps. Firstly, we estimate the minimum distance between this document and all the cluster centers of each category. Secondly, we select the smallest of the above minimum distance and we classify the document in the category that corresponds to this distance.	algorithm;categorical variable;cluster analysis;data point;electrical resistivity tomography;exception handling;fuzzy clustering;fuzzy logic;hamming distance;simulation;web page;whole earth 'lectronic link	George E. Tsekouras;Christos Anagnostopoulos;Damianos Gavalas;Economou Dafhi	2007		10.1007/978-0-387-74161-1_11	fuzzy logic;complete-linkage clustering;hamming distance;categorical variable;fuzzy clustering;computer science;machine learning;pattern recognition;data mining	ML	1.3459867280015014	-38.07572498814917	9224
ac6ed6121461b1bebf0903847ec0e19b80240ef4	modified lsb method using new cryptographic algorithm for steganography		Steganography is different from Cryptography, Steganography is the process of hiding the information so that no one will try to decrypt the information, where as in Cryptography it is obvious that the message is encrypted, so that any one will try decrypting the message. In this paper, we are suggesting new methods to improve the security in data hiding, perhaps by combining steganography and cryptography. In this work, we propose a new encryption method that provides the cipher text as the same size of the plain text. We also presented an extensive classification of various steganographic methods that have used in the field of Data Security. We analyze both security and performance aspects of the proposed methods by PSNR values and proved that in the cryptographic point of view. The proposed method is feasible in such a way that it makes to intricate the steganalyst to retrieve the original information from the Stego-image even if he detect the presence of digital steganography. An embedded message in this method is perceptually indiscernible under normal observation and thus our proposed method achieves the imperceptibility. The volume of data or message to be embedded in this method is comparatively large and proved in Experimental Results hence the high capacity is also achieved.	algorithm;least significant bit;steganography	R. Boopathy;M. Ramakrishnan;S. P. Victor	2012		10.1007/978-81-322-1602-5_63	steganography tools	Theory	39.28217434358845	-11.72880609551259	9250
f7630f80083a27ac1ea3e5818a70857723974f7d	large margin classifier based on hyperdisks	databases;quadratic programming;kernel;quadratic program;support vector machines;convex programming;approximation method;support vector machines classification convex hull hyperdisk kernel methods large margin classifier quadratic programming;training;kernel methods;computational geometry;classification;support vector machines databases kernel training approximation methods optimization accuracy;svm classifier large margin classifier hyperdisk convex program hyper plane line segment kernel trick multiclass classification problem support vector machine;accuracy;hyperdisk;multi class classification;pattern classification computational geometry convex programming;pattern classification;kernel method;optimization;approximation methods;support vector machine;convex hull;large margin classifier	This paper introduces a binary large margin classifier that approximates each class with an hyper disk constructed from its training samples. For any pair of classes approximated with hyper disks, there is a corresponding linear separating hyper plane that maximizes the margin between them, and this can be found by solving a convex program that finds the closest pair of points on the hyper disks. More precisely, the best separating hyper plane is chosen to be the one that is orthogonal to the line segment connecting the closest points on the hyper disks and at the same time bisects the line. The method is extended to the nonlinear case by using the kernel trick, and the multi-class classification problems are dealt with constructing and combining several binary classifiers as in Support Vector Machine (SVM) classifier. The experiments on several databases show that the proposed method compares favorably to other popular large margin classifiers.	approximation algorithm;closest pair of points problem;convex hull;convex optimization;database;displacement mapping;experiment;hyper-heuristic;kernel method;loose coupling;margin classifier;mathematical optimization;multiclass classification;nonlinear system;optimization problem;proximity problems;quadratic programming;quadratically constrained quadratic program;real-time clock;sparse matrix;statistical classification;support vector machine	Hakan Cevikalp	2011	2011 10th International Conference on Machine Learning and Applications and Workshops	10.1109/ICMLA.2011.86	margin classifier;support vector machine;kernel method;mathematical optimization;margin;computer science;machine learning;pattern recognition;mathematics;quadratic programming	ML	22.295640059984827	-39.653957388070964	9257
e6f08fd9ace51c6e0e308339093d6e4b6d410f06	msnet: multi-scale convolutional network for point cloud classification		Point cloud classification is quite challenging due to the influence of noise, occlusion, and the variety of types and sizes of objects. Currently, most methods mainly focus on subjectively designing and extracting features. However, the features rely on prior knowledge, and it is also difficult to accurately characterize the complex objects of point clouds. In this paper, we propose a concise multi-scale convolutional network (MSNet) for adaptive and robust point cloud classification. Both the local feature and global context are incorporated for this purpose. First, around each point, the spatial contexts of different sizes are partitioned as voxels of different scales. A voxel-based MSNet is then simultaneously applied at multiple scales to adaptively learn the discriminative local features. The class probability of a point cloud is predicted by fusing the features together across multiple scales. Finally, the predicted class probabilities of MSNet are optimized globally using the conditional random field (CRF) with a spatial consistency constraint. The proposed method was tested with data sets of mobile laser scanning (MLS), terrestrial laser scanning (TLS), and airborne laser scanning (ALS) point clouds. The experimental results show that the proposed method was able to achieve appreciable classification accuracies of 83.18%, 98.24%, and 97.02% on the MLS, TLS, and ALS data sets, respectively. The results also demonstrate that the proposed network has a strong generalization capability for classifying different kinds of point clouds under complex urban environments.	point cloud	Lei Wang;Yuchun Huang;Jie Shan;Liu He	2018	Remote Sensing	10.3390/rs10040612	discriminative model;computer vision;point cloud;geology;artificial intelligence;voxel;conditional random field;laser scanning;data set	Mobile	29.466321136951414	-49.900111180000344	9269
28b96e41551144ddcaf14de23a63cfc721704b4b	short documents clustering in very large text databases	busqueda informacion;estensibilidad;document clustering;extraction information;distributed system;algoritmo paralelo;base dato multidimensional;technologie communication;cluster algorithm;analyse amas;text;systeme reparti;electronic mail;parallel algorithm;high dimensionality;base de donnees multidimensionnelle;analisis datos;information extraction;information retrieval;base donnee tres grande;semantics;correo electronico;texte;data mining;multidimensional database;semantica;semantique;classification;algorithme parallele;data analysis;large scale;sistema repartido;cluster analysis;internet;data mining application;fouille donnee;recherche information;text database;estructura datos;analyse donnee;analisis cluster;structure donnee;extensibilite;scalability;communication technology;very large databases;texto;data structure;busca dato;clasificacion;extraccion informacion;tecnologia comunicacion;courriel	With the rapid development of the internet and communication technology, huge data is accumulated. Short text such as paper abstract and email is common in such data. It is useful to cluster such short documents to get the data structure or to help build other data mining applications. But almost all the current clustering algorithms become very inefficient or even unusable when handle very large (hundreds of GB) and high-dimensional text data. It is also difficult to get acceptable clustering accuracy since key words appear only few times in short documents. In this paper, we propose a frequent term based parallel clustering algorithm which can be used to cluster short documents in very large text database. A novel semantic classification method is also used to improve the accuracy of clustering. Our experimental study shows that our algorithm is more accurate and efficient than other clustering algorithms when clustering large scale short documents. Furthermore, our algorithm has good scalability and can be used to process even huge data.	database	Yongheng Wang;Yan Jia;Shuqiang Yang	2006		10.1007/11906070_8	information and communications technology;data stream clustering;scalability;the internet;document clustering;data structure;biological classification;computer science;data mining;database;semantics;parallel algorithm;cluster analysis;data analysis;world wide web;information extraction;clustering high-dimensional data	ML	-3.73537567870454	-33.374072947170326	9280
9c045a6d765c2f39577fa41d969ef4880d24ef22	reliable multiple-choice iterative algorithm for crowdsourcing systems	iterative learning;resource allocation;multiple choice;crowdsourcing	The appearance of web-based crowdsourcing systems gives a promising solution to exploiting the wisdom of crowds efficiently in a short time with a relatively low budget. Despite their efficiency, crowdsourcing systems have an inherent problem in that responses from workers can be unreliable since workers are low-paid and have low responsibility. Although simple majority voting can be a solution, various research studies have sought to aggregate noisy responses to obtain greater reliability in results through effective techniques such as Expectation-Maximization (EM) based algorithms. While EM-based algorithms get the limelight in crowdsourcing systems due to their useful inference techniques, Karger et al. made a significant breakthrough by proposing a novel iterative algorithm based on the idea of low-rank matrix approximations and the message passing technique. They showed that the performance of their iterative algorithm is order-optimal, which outperforms majority voting and EM-based algorithms. However, their algorithm is not always applicable in practice since it can only be applied to binary-choice questions. Recently, they devised an inference algorithm for multi-class labeling, which splits each task into a bunch of binary-choice questions and exploits their existing algorithm. However, it has difficulty in combining into real crowdsourcing systems since it overexploits redundancy in that each split question should be queried in multiple times to obtain reliable results.  In this paper, we design an iterative algorithm to infer true answers for multiple-choice questions, which can be directly applied to real crowdsourcing systems. Our algorithm can also be applicable to short-answer questions as well. We analyze the performance of our algorithm, and prove that the error bound decays exponentially. Through extensive experiments, we verify that our algorithm outperforms majority voting and EM-based algorithm in accuracy.	aggregate data;approximation;crowdsourcing;expectation–maximization algorithm;experiment;iterative method;message passing;the wisdom of crowds;web application	Donghyeon Lee;Joonyoung Kim;Hyunmin Lee;Kyomin Jung	2015		10.1145/2745844.2745871	multiple choice;weighted majority algorithm;resource allocation;computer science;artificial intelligence;machine learning;data mining;crowdsourcing;statistics;iterative learning control	Metrics	19.170104696888085	-43.722299901575276	9285
14b4755922c356529925b37b8548527da7164b4d	methodology to best extend amhs for site expansion	expansion plan;integrated vehicle;buildings;vehicle system;different approach;different use case;conveyor dynamic model;expansion analysis;site expansion;amhs;conveyors;conveyor model;segregated dynamic model;wafer transportation;vehicle dynamics;static tool;buildings (structures);automated material handling system;integrated vehicle-conveyor dynamic model;entire system;throughput prediction tool	As companies grow their capacity in multiple buildings there are increasing challenges with the automated material handling systems (AMHS) used to transport the wafers between two or more facilities. In some cases, the links used to perform these transports can become a constraint for the entire system. The problem grows more difficult as the expansion plan extends further into the future, making it harder to predict throughput requirements. This study discusses a particular throughput prediction tool as well as different approaches for evaluating designs. The approaches discussed include: integrated vehicle/conveyor model using static tool (Network approach), segregated dynamic models for conveyor/vehicle system, and integrated vehicle/conveyor dynamic model. The pros/cons of these approaches are discussed based on different use cases. The paper finishes by discussing the strategic advantages of factories performing expansion analysis early in the design of the factory and the importance to continue validating and improving these methods.	material handling;mathematical model;plan 9 from bell labs;requirement;throughput	Gabriel Gaxiola;Eric Christensen;Christian Hammel;Paul Stachura	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)		structural engineering;vehicle dynamics;simulation;engineering;transport engineering	HPC	11.077413860030312	1.8728546415584222	9295
606396085d525e71d3a92c908f8863159e4be33f	consistent latent position estimation and vertex classification for random dot product graphs	graph theory;random graph;latent space model;vectors stochastic processes estimation internet random variables pattern recognition encyclopedias;wikipedia consistent latent position estimation vertex classification random dot product graphs adjacency matrix eigendecomposition k nearest neighbors classification rule;bayes methods;random variables;universal consistency random graph k nearest neighbor latent space model;matrix algebra;pattern classification bayes methods graph theory learning artificial intelligence matrix algebra;internet;vectors;estimation;stochastic processes;pattern classification;pattern recognition;k nearest neighbor;learning artificial intelligence;encyclopedias;universal consistency	In this work, we show that using the eigen-decomposition of the adjacency matrix, we can consistently estimate latent positions for random dot product graphs provided the latent positions are i.i.d. from some distribution. If class labels are observed for a number of vertices tending to infinity, then we show that the remaining vertices can be classified with error converging to Bayes optimal using the $(k)$-nearest-neighbors classification rule. We evaluate the proposed methods on simulated data and a graph derived from .	adjacency matrix;classification;eigen (c++ library);graph (discrete mathematics);graph - visual representation;simulation;vertex (geometry);vertex (graph theory)	Daniel L. Sussman;Minh Tang;Carey E. Priebe	2014	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.135	random graph;random variable;estimation;combinatorics;the internet;graph theory;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;encyclopedia;statistics	ML	26.260882532786724	-29.23639138301141	9300
840b480518738031006580a9e5db8f9150cf78b1	an adaptive pruning algorithm for spoofing localisation based on tropical geometry.		The problem of spoofing attacks is increasingly relevant as digital systems are becoming more ubiquitous. Thus the detection of such attacks and the localisation of attackers have been objects of recent study. After an attack has been detected, various algorithms have been proposed in order to localise the attacker. In this work we propose a new adaptive pruning algorithm inspired by the tropical and geometrical analysis of the traditional Viterbi pruning algorithm to solve the localisation problem. In particular, the proposed algorithm tries to localise the attacker by adapting the leniency parameter based on estimates about the state of the solution space. These estimates stem from the enclosed volume and the entropy of the solution space, as they were introduced in our previous works.	arp spoofing;adaptive algorithm;digital electronics;feasible region;numerical analysis;spoofing attack;viterbi algorithm	Emmanouil Theodosis;Petros Maragos	2018	CoRR		theoretical computer science;viterbi algorithm;pruning;geometric analysis;spoofing attack;tropical geometry;computer science;algorithm	AI	40.480860978234915	-6.346284141204999	9315
45b3b8245c2536f6be2fda6adb47c6292732d68d	a frequent pattern mining method for finding planted (l, d)-motifs of unknown length	empirical study;frequent pattern mining;numerical method;search space;efficient algorithm;transcription regulation;molecular mechanics;breadth first search	Identification and characterization of gene regulatory binding motifs is one of the fundamental tasks toward systematically understanding the molecular mechanisms of transcriptional regulation. Recently, the problem has been abstracted as the challenge planted (l, d)- motif problem. Previous studies have developed numerous methods to solve the problem. But most of methods need to specify the length l of a motif in advance. In this study, we present an exact and efficient algorithm, called Apriori-Motif, without given l. The algorithm uses breadth first search and prunes the search space quickly by the downward closure property used in Apriori, a classical algorithm of frequent pattern mining. Empirical study shows that Apriori-Motif is better than some existing methods.	data mining	Caiyan Jia;Ruqian Lu;Lusheng Chen	2010		10.1007/978-3-642-16248-0_37	breadth-first search;molecular mechanics;numerical analysis;computer science;bioinformatics;data mining;mathematics;empirical research;algorithm;transcriptional regulation	ML	-0.4888424880205943	-50.29737042965627	9330
c0071a8590e5023169c9e3769e55aaee4c8eba23	on replacement models via a fuzzy set theoretic framework	dynamic programming;fuzzy set;theoretical framework;decision making under uncertainty;fuzzy number;fuzzy variable;maintenance cost;dynamic program;uncertainty handling;indexing terms;fuzzy set theory;human subjects;fuzzy sets uncertainty possibility theory fuzzy set theory costs decision making mathematical model humans computational efficiency arithmetic;triangular fuzzy number;model uncertainty;decision theory;finite horizon;possibility theory;economic cybernetics fuzzy set theory possibility theory dynamic programming uncertainty handling decision theory;ranking methods replacement models fuzzy set theory uncertainty revenue streams maintenance costs inflation mathematical framework decision making model human subjectivity possibility theory economic life asset calculation single asset replacement problem triangular fuzzy numbers fuzzy arithmetic dynamic programming fuzzy rewards vertex method;computational efficiency;economic cybernetics;replacement policy	"""| Uncertainty is present in virtually all replacement decisions due to unknown future events, such as revenue streams, maintenance costs, and in°ation. Fuzzy sets provide a mathematical framework for explicitly incorporating imprecision into the decision making model, especially when the system involves human subjectivity. This paper illustrates the use of fuzzy sets and possibility theory to explicitly model uncertainty in replacement decisions via fuzzy variables and fuzzy numbers. In particular, a fuzzy set approach to economic life of an asset calculation as well as a  ̄nite horizon single asset replacement problem with multiple challengers is discussed. Because the use of triangular fuzzy numbers provides a compromise between computational ef̄ciency and realistic modeling of the uncertainty, this discussion emphasizes fuzzy numbers. The algorithms used to determine the optimal replacement policy incorporate fuzzy arithmetic, dynamic programming with fuzzy rewards, the vertex method, and various ranking methods for fuzzy numbers. A brief history of replacement analysis, current conventional techniques, the basic concepts of fuzzy sets and possibility theory, and the advantages of the fuzzy generalization are also discussed. Keywords| Replacement analysis, fuzzy sets, possibility theory, fuzzy numbers, decision making under uncertainty. I. Economic Decision Analysis Economic decision analysis is a useful tool, o®ering individuals and organizations the techniques to model economic decision making problems, such as maintenance and replacement decisions, and determine an optimal decision. However, the accuracy of the model determines the validity of the conclusion. In many cases, the assumption of certainty in many models is made not so much for validity but the need to obtain simpler and more readily solvable formulations. Essentially, the tradeo® is between an inaccurate but solvable model and a more accurate but potentially unsolvable one. In most real-world systems, however, there are elements of uncertainty in the process or its parameters, which may lack precise de ̄nition or precise measurement, especially when the system involves human subjectivity. When developing a model of a system with uncertainty, the decision maker can either ignore the uncertainty, implicitly acknowledge it, or explicitly model it. Ignoring the uncertainty usually results in a deterministic model of the process with precise values for all parameters. Implicitly acknowledging the uncertainty may still result in a deterministic model in which sensitivity analysis or discount factors can be used to get an idea of how this uncertainty a®ects the outcome. Lastly, the decision maker can explicitly model the uncertainty using speci ̄c paradigms such as interval analysis, possibility theory, probability theory, or evidence theory [3]. The proper paradigm depends on the nature of the uncertainty. When the probabilities are speci ̄ed for the outcomes, then the theory of Von Neumann and Morgenstern [40] provides the tools necessary to determine the optimal decision. However, in many cases these probabilities are neither de ̄ned nor directly attainable. Under these circumstances, other theories are needed. The most common choice is the use of subjective probability distributions and the theory of choice due to Savage [34]. However, considerable debate on the use of subjective probabilities exists and is well documented in the literature [6], [16], [23], [25], [27]. From a psychological standpoint, the methods used to elicit these subjective probabilities and the validity of the subjective probabilities themselves have been the focus of research led by Tversky and Kahneman [37], [39], [38]. Their studies show that the heuristics employed to assess probabilities and predict values can sometimes lead to \severe and systematic errors"""" [38]. Because humans do not think naturally in probabilistic terms, they tend to  ̄nd the notions of fuzzy sets and their linguistic based approaches more user-friendly and appealing. We may view fuzzy set theory as a generalization of classical set theory since it provides us with a mathematical tool for describing sets that have no sharp transition from membership to nonmembership. Membership in a fuzzy set is de ̄ned by a generalized version of the classical indicator function called a membership function. Fuzzy sets allow the de ̄nition of vague or imprecise concepts such as \approximately 1000"""" where, for example, 1000 would have a membership of 1.0 and 975 a membership of 0.5 (see Figure 1). This theory has been developed and successfully applied to numerous areas such as control and decision making, engineering, and medicine. Its application to economic analysis is natural due to the uncertainty inherent in many  ̄nancial and investment decisions. As noted earlier, it provides a precise mathematical language to model uncertainty due to vagueness and imprecision in events or statements describing a system. More information on fuzzy set theory, particularly fundamental concepts such as fuzzy numbers which are invoked in our presentation, is included in the Appendix. II. Replacement Analysis One of the most practical and topical areas of engineering economics is replacement analysis. Mathematical models and analysis methods are used to determine the sequence of replacement decisions that provides a required service for a speci ̄ed time horizon in an optimal manner. It is assumed that maintenance and replacement decisions occur on a periodic basis. The decision maker chooses from various options, such as to keep, overhaul, or perform preventive maintenance on the existing asset or replace it with"""	algorithm;decision analysis;decision problem;decision theory;dynamic programming;fuzzy number;fuzzy set;heuristic (computer science);interval arithmetic;mathematical model;possibility theory;programming paradigm;savage;set theory;usability;vagueness;world-system	Augustine O. Esogbue;Warren E. Hearnes	1998	IEEE Trans. Systems, Man, and Cybernetics, Part C	10.1109/5326.725341	fuzzy logic;mathematical optimization;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	ML	0.8252408020075367	-12.732601238324465	9339
fa176951606dab046d27d70000bfd963d9282021	a lightweight genetic block-matching algorithm for video coding	motion estimation video coding image matching search problems genetic algorithms computational complexity;control overheads;full search;three step search algorithm;image processing;full search algorithm video coding block matching algorithm lightweight genetic search algorithm evolution schemes control overheads three step search algorithm simulation results performance computational complexity genetic motion estimation algorithms;complexite calcul;helium;image matching;implementation;performance;search algorithm;procesamiento imagen;three step search;motion estimation;indexing terms;algoritmo genetico;traitement image;genetics;evolution schemes;ejecucion;video coding;codificacion;complejidad computacion;vecteur mouvement;lightweight genetic search algorithm;computational modeling;senal video;signal video;computational complexity;motion vector;genetic motion estimation algorithms;video coding computational complexity computational modeling motion estimation genetic algorithms multimedia communication laboratories computer science computational efficiency;analyse performance;image sequence;multimedia communication;coding;performance analysis;algorithme genetique;full search algorithm;correspondencia bloque;video signal;block matching;genetic algorithm;genetic algorithms;secuencia imagen;mpeg;search problems;computer science;correspondance bloc;computational efficiency;simulation results;block matching algorithm;sequence image;codage;analisis eficacia	In this paper, a lightweight genetic search algorithm (LGSA) is proposed. Different evolution schemes are investigated, such that the control overheads are largely reduced. It is also shown that the proposed LGSA can be viewed as a novel expansion of the three-step search algorithm (TSS). It can be seen from the simulation results that the performance of LGSA is very similar to that of FSA, and the computational complexity is much lower than that of FSA and other previously proposed genetic motion estimation algorithms.	block-matching algorithm;computation;computational complexity theory;data compression;genetic algorithm;motion estimation;search algorithm;simulation	Chun-Iiung Lin;Ja-Ling Wu	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.709405	computer vision;simulation;genetic algorithm;image processing;cultural algorithm;computer science;theoretical computer science;machine learning;population-based incremental learning	Vision	46.661045357374825	-15.405245635433289	9341
787641a463fcdf464d4b99ed8a61bf7dd5fb5df8	the search for a cost matrix to solve rare-class biological problems	classification;machine learning;dissertation;local search;bioinformatics	The rare-class data classification problem is a common one. It occurs when, in a dataset, the class of interest is far outweighed by other classes, thus making it difficult to classify using typical classification algorithms. These types of problems are found quite often in biological datasets, where data can be sparse and the class of interest has few representatives. A variety of solutions to this problem exist with varying degrees of success. In this paper, we present our solution to the rare-class problem. This solution uses MetaCost, a cost-sensitive meta-classifier, that takes in a classification algorithm, training data, and a cost matrix. This cost matrix adjusts the learning of the classification algorithm to classify more of the rare-class data but is generally unknown for a given dataset and classifier. Our method uses three different types of optimization techniques (greedy, simulated annealing, genetic algorithm) to determine this optimal cost matrix. In this paper we will show how this method can improve upon classification in a large amount of datasets, achieving better results along a variety of metrics. We will show how it can improve on different classification algorithms and do so better and more consistently than other rare-class learning techniques like oversampling and undersampling. Overall our method is a robust and effective solution to the rare-class problem. Dedication To my friends and family, who have supported me throughout To Liqing, the best advisor a grad student could have To Christina, my love and inspiration GO HOKIES! iii Acknowledgments I would like to acknowledge all of the professors of my committee for their guidance and assistance in the creation of this dissertation.	genetic algorithm;gradient;greedy algorithm;mathematical optimization;oversampling;simulated annealing;sparse matrix;undersampling	Mark J. Lawson	2009			beam search;computer science;local search;data science;machine learning;data mining;iterated local search;incremental heuristic search;hyper-heuristic;guided local search	ML	10.54811275991366	-43.66561061098473	9342
67fca2b0abe9c7d53abe10bcbee96869f079333d	tsm resilient audio watermarking using imfs	detectors;watermarking;watermarking robustness signal to noise ratio transforms digital audio players detectors;hilbert transformation;signal to noise ratio tsm resilient audio watermarking imf ownership proof empirical mode decomposition hilbert huang transform audio signal decomposition mono component signals intrinsic mode functions pseudo random number signal processing attacks mp3 time scale modification resizing;intrinsic mode function;transforms;robustness;digital audio players;signal to noise ratio;hilbert transformation audio watermarking intrinsic mode function empirical mode decomposition;empirical mode decomposition;audio watermarking	With widespread of the Internet as well as the numerous applications that involve audio signals and systems, the issue of ownership proof become imperative. In this paper, we present an audio watermark scheme that is based on Empirical Mode Decomposition and Hilbert Huang Transform. The audio signal is decomposed into several mono-component signals or Intrinsic Mode Functions to serve as the addressee for watermark. The watermark is a pseudo random number which is added to the highest and lowest Intrinsic Mode Functions of the signal to generate what we call the modified IMFs which is in turn added to the remaining IMFs to produce the watermarked signal. Our experimental results show that the proposed method is robust against signal processing attacks such as MP3, time scale modification, resizing, and others. The obtained results meet the recommended imperceptibility signal to noise ratio.	digital watermarking;hilbert–huang transform;ibm spectrum protect (tivoli storage manager);imperative programming;internet;mp3;pseudorandomness;random number generation;signal processing;signal-to-noise ratio	Saif alZahir;Md Wahedul Islam	2012	2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2012.6334842	detector;electronic engineering;speech recognition;telecommunications;digital watermarking;computer science;electrical engineering;theoretical computer science;hilbert–huang transform;mathematics;signal-to-noise ratio;robustness	EDA	42.94916687940333	-8.882092647892076	9364
ebb16c7a66df82c0a8efaad24402c8a83831c94d	distributed optimization using ant colony optimization in a concrete delivery supply chain	ant colony optimization concrete supply chains supply chain management job shop scheduling routing costs job production systems collaboration optimization methods;optimal solution;optimisation;ant colony optimization;job shop scheduling;coordination mechanisms;combinatorial optimization problem;distributed optimization;supply chain performance;supply chain management concrete job shop scheduling optimisation;hybrid method;constructive heuristics distributed optimization ant colony optimization concrete delivery supply chain rapidly perishable goods ready mixed concrete scheduling routing job shop problems meta heuristics;job shop;supply chain;scheduling and routing;supply chain management;concrete	The timely production and distribution of rapidly perishable goods such as ready-mixed concrete is a complex combinatorial optimization problem in the context of supply chain management. The problem involves several tightly interrelated scheduling and routing problems that have to be solved considering a trade-off of production and delivery costs. This paper applies a novel supply chain management paradigm, the distributed optimization, to a real-world case of a concrete delivery supply chain. The production of concrete in several production centers and the distribution of concrete are modeled as job shop problems, where each problem is solved using Ant Colony Optimization. The management methodology consists of allowing each system to exchange information concerning intermediate optimization results through pheromone matrices. In this way, each system finds its own optimization solution based on the information provided by the other systems. A simulation example shows that the proposed coordination mechanism improves the supply chain performance, when compared to another management approach, where both problems are optimized using hybrid methods combining meta-heuristics with constructive heuristics.	ant colony optimization algorithms;combinatorial optimization;heuristic (computer science);mathematical optimization;optimization problem;program optimization;programming paradigm;routing;scheduling (computing);simulation;stochastic matrix	Jorge M. Faria;Carlos A. Silva;João Miguel da Costa Sousa;Michele Surico;Uzay Kaymak	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688292	optimization problem;job shop scheduling;mathematical optimization;multi-swarm optimization;ant colony optimization algorithms;supply chain management;simulation;concrete;computer science;supply chain;metaheuristic	Robotics	18.43496939609187	0.4108558242639035	9385
bcdc273735a574ee7f1cc81cd4eb83b5ce389bf3	joint pairwise learning and image clustering based on a siamese cnn		How to use a deep convolutional neural network (CNN) to efficiently and effectively learn representations of a large unlabeled set of images and group them into clusters remains a challenging problem. To address this problem, we propose a Siamese clustering CNN (SC-CNN) to iteratively learn discriminative representations for image clustering. Based on the proposed SC-CNN, we employ a mini-batch-based joint pairwise representation learning and clustering scheme to make the computation and storage cost efficient for large-scale image clustering on a personal computer with a commercial GPU graphic card. On top of SC-CNN, the proposed pairwise learning scheme effectively learns discriminative representations by appropriately selecting same-cluster and different-cluster image pairs from the results of each clustering iteration. Experimental results demonstrate that the proposed method outperforms start-of-the-art clustering schemes in clustering accuracy on public image sets.		Weng-Tai Su;Chih-Chung Hsu;Ziling Huang;Chia-Wen Lin;Gene Cheung	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451224	discriminative model;convolutional neural network;cluster analysis;computation;cost efficiency;feature extraction;pattern recognition;pairwise comparison;artificial intelligence;feature learning;computer science	Vision	26.618335664915193	-47.840036002798584	9387
556e8b03d7757fca39dce1deb3cfffa65337689a	using multiobjective metaheuristics to solve vrp with uncertain demands	real life optimization problems;optimal solution;optimisation;nsgaii multiobjective metaheuristics uncertain demands real life optimization problems uncertain environmental changes robust optimal solution vehicle routing problem stochastic demands vrpsd customers demands multiobjective evolutionary algorithms moea ibea moga;goods distribution;evolutionary computation;routing;vehicle routing problem;moea;multi objective evolutionary algorithm;uncertain environmental changes;robust optimization;nsgaii;stochastic demands;transportation customer satisfaction evolutionary computation goods distribution optimisation stochastic processes;customer satisfaction;optimization problem;stochastic processes;stochastic demand;vehicles robustness mathematical model entropy optimization object oriented modeling routing;transportation;mathematical model;robust optimal solution;robustness;optimization;entropy;vehicles;moga;vrpsd;customers demands;environmental change;ibea;object oriented modeling;multiobjective evolutionary algorithms;uncertain demands;object model;multiobjective metaheuristics	In real life optimization problems, it is very important to have high quality solutions (optimal). But when uncertainty becomes part of the optimization problem, solutions should be optimal and robust to the uncertain environmental changes. This paper focuses on finding robust optimal solution for the vehicle routing problem with stochastic demands VRPSD. In this case when the uncertainty of the customers demands enters this problem, the classical methods of VRP can not be used to obtain optimal solutions. We need new methods with new strategies to have robust optimal solution. For that we propose two bi-objective models, depending on the multi-objective evolutionary algorithms MOEAs: IBEA, MOGA and NSGAII. We compare the robustness degree of the two models and also we compare the performance of the three MOEAs over these two models.	display resolution;evolutionary algorithm;mathematical optimization;metaheuristic;optimization problem;powera;real life;vehicle routing problem	Dalia Sulieman;Laetitia Vermeulen-Jourdan;El-Ghazali Talbi	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586538	optimization problem;entropy;transport;mathematical optimization;routing;robust optimization;object model;environmental change;computer science;operations management;vehicle routing problem;machine learning;mathematical model;mathematics;customer satisfaction;robustness;evolutionary computation	Vision	16.376049324840654	-0.3262070723413605	9402
846e5b8cc93c659e224692bcb8b0969cfa38cdb7	the mixture of multi-kernel relevance vector machines model	incremental em learning;pattern clustering;support vector machines;time series;multi kernel;relevance vector machines;time series expectation maximisation algorithm learning artificial intelligence pattern clustering regression analysis support vector machines;incremental em learning relevance vector machines mixture models sparse prior multi kernel;regression analysis;hidden markov models kernel data models training mathematical model covariance matrix vectors;regression mixture model multikernel relevance vector machines rvm model sparsity enforcing property mixture model time series clustering problem kernel parameter weighted multikernel scheme maximum a posteriori approach map approach expectation maximization algorithm em algorithm closed form update equation model parameter incremental learning methodology parameter initialization problem;sparse prior;learning artificial intelligence;mixture models;expectation maximisation algorithm	We present a new regression mixture model where each mixture component is a multi-kernel version of the Relevance Vector Machine (RVM). In the proposed model, we exploit the enhanced modeling capability of RVMs due to their embedded sparsity enforcing properties. %The main contribution of this %work is the employment of RVM models as components of a mixture %model and their application to the time series clustering problem. Moreover, robustness is achieved with respect to the kernel parameters, by employing a weighted multi-kernel scheme. The mixture model is trained using the maximum a posteriori (MAP) approach, where the Expectation Maximization (EM) algorithm is applied offering closed form update equations for the model parameters. An incremental learning methodology is also presented to tackle the parameter initialization problem of the EM algorithm. The efficiency of the proposed mixture model is empirically demonstrated on the time series clustering problem using various artificial and real benchmark datasets and by performing comparisons with other regression mixture models.	algorithmic efficiency;benchmark (computing);closed-circuit television;cluster analysis;computer vision;curve fitting;embedded system;expectation–maximization algorithm;gaussian process;kernel (operating system);mixture model;object detection;relevance vector machine;sparse matrix;time series;video tracking	Konstantinos Blekas;Aristidis Likas	2012	2012 IEEE 12th International Conference on Data Mining	10.1109/ICDM.2012.34	support vector machine;computer science;machine learning;time series;pattern recognition;mixture model;mathematics;relevance vector machine;regression analysis;statistics	ML	29.43543786160094	-32.16043304622613	9415
57314e795facfde81985b8f923d95945ddc4121d	novel error concealment method with adaptive prediction to the abrupt and gradual scene changes	extrapolation video coding image sequences interpolation;interpolation;spatial dependence;error concealment;extrapolation;low complexity;indexing terms;video coding;layout video compression transform coding decoding change detection algorithms wireless communication b isdn asynchronous transfer mode detection algorithms interpolation;gradual scene change sequence abrupt scene changes adaptive prediction error concealment method low complexity scene change detection algorithm macroblock type information corrupt block concealment interpolation extrapolation;scene change detection;video communication;image sequences	In this paper, the impact of the scene change on the conventional error concealment method is addressed and a novel error concealment method is proposed to improve the insufficiency of conventional temporal error concealment algorithm due to the occurrence of scene change. Combining with the low complexity scene change detection algorithm using macroblock type information, the corrupt blocks resulting from bit errors are concealed either temporally or spatially depending on whether or not an abrupt scene change is found. In the case of gradual scene change, a novel error concealment method of interpolation and extrapolation is proposed to utilize the linear property of gradual scene change sequence, and effectively reduce the concealment error in comparison with the conventional algorithm. Great improvement about 3 to 5 dB PSNR in average and 6 to 8 dB in some cases is obtained with very little overhead memory and computation.	algorithm;authorization;computation;convergence insufficiency;dspace;decibel;error concealment;extrapolation;group of pictures;ieee xplore;interpolation;macroblock;overhead (computing);peak signal-to-noise ratio;propagation of uncertainty;simulation;software propagation	Soo-Chang Pei;Yu-Zuong Chou	2004	IEEE Trans. Multimedia	10.1109/TMM.2003.819749	computer vision;speech recognition;index term;spatial dependence;interpolation;computer science;theoretical computer science;extrapolation;statistics	Visualization	47.52024110223945	-16.628391466149985	9429
e3b7a8dfe0f518dd04fa53e63d1ee7eabefbfe45	hybrid particle swarm optimization with bat algorithm		In this paper, a communication strategy for hybrid Particle Swarm Optimization (PSO) with Bat Algorithm (BA) is proposed for solving numerical optimization problems. In this work, several worst individuals of particles in PSO will be replaced with the best individuals in BA after running some fixed iterations, and on the contrary, the poorer individuals of BA will be replaced with the finest particles of PSO. The communicating strategy provides the information flow for the particles in PSO to communicate with the bats in BA. Six benchmark functions are used to test the behavior of the convergence, the accuracy, and the speed of the approached method. The results show that the proposed scheme increases the convergence and accuracy more than BA and PSO up to 3% and 47% respectively.		Tien-Szu Pan;Thi-Kien Dao;Trong-The Nguyen;Shu-Chuan Chu	2014		10.1007/978-3-319-12286-1_5	multi-swarm optimization;bat algorithm;firefly algorithm	EDA	27.319316272178288	-4.128567666340989	9430
87840ca0fef603c0f2ab07921933cd505ab566d4	video security for ambient intelligence	intelligent building;security ambient intelligence intelligent structures application software indoor environments computer vision streaming media cameras image segmentation layout;video streaming;image segmentation;ambient intelligence;computer vision;building management systems;indoor environment;image sequences security building management systems computer vision image segmentation;security;condition index;security ambient intelligence people counting;video sequences video security ambient intelligence intelligent building computer vision video streams image segmentation;image sequences	Moving toward the implementation of the intelligent building idea in the framework of ambient intelligence, a video security application for people detection, tracking, and counting in indoor environments is presented in this paper. In addition to security purposes, the system may be employed to estimate the number of accesses in public buildings, as well as the preferred followed routes. Computer vision techniques are used to analyze and process video streams acquired from multiple video cameras. Image segmentation is performed to detect moving regions and to calculate the number of people in the scene. Testing was performed on indoor video sequences with different illumination conditions.	ambient intelligence;color;computer vision;image segmentation;real-time clock;real-time computing;streaming media	Lauro Snidaro;Christian Micheloni;C. Chiavedale	2005	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2004.838478	building management system;computer vision;ambient intelligence;computer science;information security;video tracking;multimedia;image segmentation;computer graphics (images)	Vision	44.05129479385034	-43.45659515708684	9448
c76ba54dbe25d07601c6b6a1c3f08aa8b8c18b73	the cramér-infogan and partial inverse filter system for unsupervised image classification		The scarcity of labelled data and the abundance of raw data have endowed unsupervised approaches to image classification with great significance. To enable learning in domains where labelled data are few, an unsupervised image classification system consisting of Cramér-InfoGAN and Partial Inverse Filter is proposed in this article. The former learns disentangled representation of data and builds a mapping from it to images, which is the main functionality of InfoGAN. Moreover, it combines Cramér GAN to improve the stability and convergence of training, also to monitor the reliability of learned representation by observing the energy distance of Cramér GAN. The latter classifies images by mapping it back to its representation. Furthermore, it is regularized by gradient penalty to suppress noise and by independence constraint to reduce entanglement of different dimensions. Experiments on MNIST and CIFAR-10 datasets show improved convergence and respectable classification accuracy of the system.	computer vision;gradient;inverse filter;mnist database;quantum entanglement;unsupervised learning	Shikun Zhang;Xiaoxue Feng;Yufeng Ji;Feng Pan	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00058	raw data;energy distance;big data;contextual image classification;inverse filter;mnist database;mathematics;pattern recognition;artificial intelligence;convergence (routing)	Robotics	29.115795465775047	-43.96158332969322	9452
a750e0777ea99a4913a68c88f0c907305c28fa68	algorithm 591: a comprehensive matrix-free algorithm for analysis of variance	iterative method;efficient algorithm;eigensystem improvement;linear model;analysis of variance;matrix eigensystems	Some of the work to be described is reminiscent of A A R D V A R K [5], an analysisof-variance package, at Iowa State University in the early 1960s. This program was used for most of the large number of analyses processed at Iowa State at that time; however, the program was definitely not transportable, being written in a mix of assembler and FORTRAN for a nonstandard IBM 7074. Subsequent versions, lacking some of its initial capabilities but adding others, were prepared for other machines, including the IBM 360 (see for example [7]). One distinctive feature of the program that greatly aided in its usability was algebraic specification of the statistical model by the user [14], [18]. This facility has now become common in statistical packages. The initial A A R D V A R K relied principally on balanced analysis-of-variance (AOV) algorithms and approximate statistical methods were applied to treat unbalanced data. An iterative AOV algorithm was later developed by Hemmerle [12], which permits using balanced analysis-of-variance algorithms to obtain exact statistical results for unbalanced data. This algorithm along with subsequent	algebraic specification;approximation algorithm;assembly language;fortran;ibm 7070;ibm system/360;iterative method;list of statistical packages;statistical model;unbalanced circuit;usability	William J. Hemmerle	1982	ACM Trans. Math. Softw.	10.1145/356012.356018	econometrics;mathematical optimization;analysis of variance;linear model;mathematics;iterative method;statistics	Graphics	35.60261808769545	-17.020354138614803	9455
0c0f287d4db67a1f8cd2516e70a2c83d3f3d2efa	a study of storage assignment and order picking route problem for the warehouse of distribution center - assuming synchronized zone picking operations				Chikong Huang;You-Jen Liu	2015			operations management;distribution center;warehouse;real-time computing;order picking;engineering	Theory	12.259253039328105	2.7542498515620637	9483
ea55ae41655d082269693724f17bf0397cac801f	people detection in image and video data	people detection;generic model;mean shift;adaboost classification;pedestrian detection;detection rate;implicit shape model;discriminative model;object detection	In this paper, we address the problem of people detection in real world videos/images. We propose an approach that combines a generative model with a discriminative model to utilize the advantages of both types of detections. The individual body parts of the person are detected based on discriminative approach and the evidence aggregation from part detectors is done in a generative model by incorporating a probability framework. The algorithm works on images that can be extended to video data also. The advantage of this framework is that it can handle occlusions and crowded scenarios, where motion information is ambiguous. The results obtained illustrate the ability of the algorithm to give good detection rates despite occlusions, poor illumination conditions and pose, scale variations of people in the scene.	algorithm;discriminative model;generative model;sensor	Supriya Rao;N. C. Pramod;Chaitanya Krishna Paturu	2008		10.1145/1461893.1461909	computer vision;object-class detection;geography;viola–jones object detection framework;machine learning;pattern recognition	Vision	42.01607668214786	-47.01475680032912	9484
3588b857f7e435d091998357fac42a747dac0356	a hybrid grasp - evolutionary algorithm approach to golomb ruler search	parallelisme;metodo adaptativo;fenotipo;hybrid evolutionary algorithm;genotype;prension;methode adaptative;gripping;algoritmo genetico;optimisation combinatoire;resolucion problema;aleatorizacion;greedy randomized adaptive search procedure;hybrid approach;parallelism;paralelismo;biomimetique;adaptive method;algorithme genetique;randomisation;algorithme evolutionniste;genetic algorithm;prehension;algoritmo evolucionista;evolutionary algorithm;randomization;combinatorial optimization;phenotype;problem solving;resolution probleme;genotipo;biomimetics;optimizacion combinatoria	We consider the problem of finding small Golomb rulers, a hard combinatorial optimization task. This problem is here tackled by means of a hybrid evolutionary algorithm (EA). This EA incorporates ideas from greedy randomized adaptive search procedures (GRASP) in order to perform the genotype-to-phenotype mapping. As it will be shown, this hybrid approach can provide high quality results, better than those of reactive GRASP and other EAs.	combinatorial optimization;constraint satisfaction;display resolution;evolutionary algorithm;grasp;golomb ruler;greedy algorithm;greedy randomized adaptive search procedure;hoc (programming language);mathematical optimization;mutation (genetic algorithm);randomized algorithm;reinforcement learning;string (computer science)	Carlos Cotta;Antonio J. Fernández	2004		10.1007/978-3-540-30217-9_49	randomization;biomimetics;greedy randomized adaptive search procedure;mathematical optimization;genetic algorithm;combinatorial optimization;computer science;artificial intelligence;phenotype;evolutionary algorithm;genotype;mathematics;algorithm	Robotics	25.84858969300568	1.519181293446073	9486
1c3376a3a5fdffdd27570095d73250c85aa5df60	multi-scale dynamic human fatigue detection with feature level fusion	traffic accidents;image features;histograms;fatigue;traffic accident;adaboost algorithm multi scale dynamic human fatigue detection traffic accidents feature level fusion driver fatigue detection facial image sequences gabor filters;road traffic;gabor filters;multi scale dynamic human fatigue detection;gabor filter;facial image sequences;humans fatigue computer vision image sequences road accidents face detection gabor filters histograms concatenated codes data mining;face recognition;feature extraction;image sequence;classification algorithms;road traffic face recognition gabor filters image sequences road safety;face;fusion rule;road safety;feature level fusion;adaboost algorithm;driver fatigue detection;image sequences	Driver fatigue is a significant reason for many traffic accidents. We propose a novel multi-scale dynamic feature with feature level fusion for driver fatigue detection from facial image sequences. First, Gabor filters are employed to extract multi-scale and multi-orientation features from each image. Features of the same scale are then fused according to a fusion rule to produce a single feature. To account for the temporal aspect of human fatigue, the fused image sequence is divided into dynamic units, and a histogram of each dynamic unit is computed and concatenated as dynamic features. Finally AdaBoost algorithm is applied to extract the most discriminative features and construct a strong classifier for fatigue detection. The test data contains 600 image sequences from thirty people. Experimental results show the validity of the proposed approach, and the average correct rate is 99.33% which is much better than the baselines.	adaboost;algorithm;baseline (configuration management);concatenation;gabor filter;test data	Xiao Fan;Yanfeng Sun;Baocai Yin	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813461	computer vision;feature detection;speech recognition;engineering;pattern recognition;feature	Vision	39.95040159561089	-45.81077835280321	9492
cff2ab96742568c33068dc44f457894df43a97cd	integrated production planning and scheduling for a mixed batch job-shop based on alternant iterative genetic algorithm	journal of the operational research society	An integrated optimization production planning and scheduling based on alternant iterative genetic algorithm is proposed here. The operation constraints to ensure batch production successively are determined in the first place. Then an integrated production planning and scheduling model is formulated based on non-linear mixed integer programming. An alternant iterative method by hybrid genetic algorithm (AIHGA) is employed to solve it, which operates by the following steps: a plan is given to find a schedule by hybrid genetic algorithm; in turn, a schedule is given to find a new plan using another hybrid genetic algorithm. Two hybrid genetic algorithms are alternately run to optimize the plan and schedule simultaneously. Finally a comparison is made between AIHGA and a monolithic optimization method based on hybrid genetic algorithm (MOHGA). Computational results show that AIHGA is of higher convergence speed and better performance than MOHGA. And the objective values of the former are an average of 12.2% less than those of the latter in the same running time. Journal of the Operational Research Society advance online publication, 8 October 2014; doi:10.1057/jors.2014.88	automated planning and scheduling;batch processing;computation;flip-flop (electronics);genetic algorithm;integer programming;iterative method;linear programming;mathematical optimization;memetic algorithm;nonlinear system;scheduling (computing);time complexity;two-hybrid screening	Hong-Sen Yan;Xiao-Qin Wan;Fuli Xiong	2015	JORS	10.1057/jors.2014.88	mathematical optimization;simulation;economics;computer science;operations management;operations research;population-based incremental learning	AI	18.036550724704927	3.3816614499915745	9498
523bbb1a34ac347a3a47c199f791aace0a63d52b	angle independent bundle adjustment refinement	realistic images computer vision image sequences;camera orientation independent cost function;error reduction;cost function;computer graphics;geometry;image sequences angle independent bundle adjustment refinement real world 3d scene computer vision computer graphics camera orientation independent cost function standard bundle adjustment cost function;layout;computer graphic;computer vision;computational modeling;3d model;angle independent bundle adjustment refinement;image reconstruction;image sequence;standard bundle adjustment cost function;realistic images;robustness;cameras image reconstruction layout cost function computer vision geometry robustness equations computer science computational modeling;computer science;cameras;bundle adjustment;image sequences;real world 3d scene	Obtaining a digital model of a real-world 3D scene is a challenging task pursued by computer vision and computer graphics. Given an initial approximate 3D model, a popular refinement process is to perform a bundle adjustment of the estimated camera position, camera orientation, and scene points. Unfortunately, simultaneously solving for both camera position and camera orientation is an ill-conditioned problem. To address this issue, we propose an improved, camera-orientation independent cost function that can be used instead of the standard bundle adjustment cost function. This yields a new bundle adjustment formulation which exhibits noticeably better numerical behavior, but at the expense of an increased computational cost. We alleviate the additional cost by automatically partitioning the dataset into smaller subsets. Minimizing our cost function for these subsets still achieves significant error reduction over standard bundle adjustment. We empirically demonstrate our formulation using several different size models and image sequences.	algorithmic efficiency;approximation algorithm;bundle adjustment;business architecture;camera resectioning;computation;computer graphics;computer vision;condition number;feature vector;loss function;mathematical optimization;neural coding;numerical analysis;polynomial;refinement (computing);structure from motion;time complexity	Jeffrey Zhang;Daniel G. Aliaga;Mireille Boutin;Robert Insley	2006	Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT'06)	10.1109/3DPVT.2006.30	computer vision;simulation;computer science;bundle adjustment;computer graphics (images)	Vision	51.62674365641137	-50.24727413605809	9503
53de33f54b754bfc071c39770a98c48bd3311c21	wireless transmission of images using jpeg2000	channel coding;optimisation;information extraction wireless image transmission jpeg2000 stream wireless channel block based coding structure optimized product code turbo code reed solomon code burst error;wireless channels;error correction codes;image coding;data compression;visual communication telecommunication channels block codes image coding reed solomon codes feature extraction optimisation product codes data compression error correction codes turbo codes;information extraction;product code;visual communication;reed solomon codes;turbo codes;feature extraction;decoding streaming media product codes reed solomon codes protection turbo codes information processing laboratories strontium informatics;experimental evaluation;telecommunication channels;block codes;reed solomon code;turbo code;product codes	A novel scheme is proposed for the transmission of JPEG2000 image streams over wireless channels. The proposed scheme exploits the block-based coding structure of the JPEG2000 streams and employs optimized product codes consisting of Turbo codes and Reed-Solomon codes in order to deal effectively with burst errors. The optimization is based on information extracted directly from the compressed JPEG2000 streams. Experimental evaluation demonstrates that the proposed scheme outperform other recent algorithms for the wireless transmission of images.	algorithm;burst error;jpeg 2000;mathematical optimization;reed–solomon error correction;turbo code	Nikolaos Thomos;Nikolaos V. Boulgouris;Michael G. Strintzis	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421616	turbo code;telecommunications;computer science;theoretical computer science;information extraction;statistics	Robotics	49.045987453808316	-16.700493619667554	9512
1ac244d106ef9db24a3ef2e5a5f05fd2e5ff16e4	similarity measures in formal concept analysis	cluster similarity;62h30;68t10;formal concept analysis	Formal concept analysis (FCA) has been applied successively in diverse fields such as data mining, conceptual modeling, social networks, software engineering, and the semantic web. One shortcoming of FCA, however, is the large number of concepts that typically arise in dense datasets hindering typical tasks such as rule generation and visualization. To overcome this shortcoming, it is important to develop formalisms and methods to segment, categorize and cluster formal concepts. The first step in achieving these aims is to define suitable similarity and dissimilarity measures of formal concepts. In this paper we propose three similarity measures based on existent set-based measures in addition to developing the completely novel zeros-induced measure. Moreover, we formally prove that all the measures proposed are indeed similarity measures and investigate the computational complexity of computing them. Finally, an extensive empirical evaluation on real-world data is presented in which the utility and character of each similarity measure is tested and evaluated.	computational complexity theory;formal concept analysis;semantic web;similarity measure;social network;software engineering	Faris Alqadah;Raj Bhatnagar	2010	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-011-9257-7	semantic similarity;discrete mathematics;computer science;formal concept analysis;theoretical computer science;data mining;mathematics	AI	-4.10470536590312	-30.724726834206077	9518
58ac70afc03633c896a8ad9ddb7e28eabb5e463b	a modified functional delta method and its application to the estimation of risk functionals	methode delta;functional delta method;metodo estadistico;hadamard derivative;analyse multivariable;62f12;convergencia debil;62g05;proceso empirico;multivariate analysis;fonctionnelle;censoring;melange loi probabilite;weighted empirical process;censored sample;melangeage;62m10;theoreme central limite;estimation non parametrique;60f05;62g20;donnee censuree;mixed distribution;distortion risk measure;statistical method;delta method;distribucion estadistica;asymptotic behavior;loi asymptotique;comportement asymptotique;central limit theorem functional delta method hadamard derivative weighted empirical process mixing censoring distortion risk measure;functional dependency;queue distribution;funcional;estimation parametrique;echantillon censure;non parametric estimation;comportamiento asintotico;censored data;distribution statistique;central limit theorem;cola distribucion;functional;58b10;risk estimation;methode statistique;37a25;processus empirique;empirical process;teorema central limite;mezcla ley probabilidad;analisis multivariable;asymptotic distribution;weak convergence;62g30;estimation risque;risk measure;estimacion no parametrica;estimation statistique;mixing;convergence faible;estimacion estadistica;mezclado;statistical estimation;statistical distribution;distribution tail;62n01;62e20;muestra censurada;62m99	The classical functional delta method (FDM) provides a convenient tool for deriving the asymptotic distribution of statistical functionals from the weak convergence of the respective empirical processes. However, for many interesting functionals depending on the tails of the underlying distribution this FDM cannot be applied since the method typically relies on Hadamard differentiability w.r.t. the uniform sup-norm. In this article, we present a version of the FDM which is suitable also for nonuniform sup-norms, with the outcome that the range of application of the FDM enlarges essentially. On one hand, our FDM, which we shall call the modified FDM, works for functionals that are ''differentiable'' in a weaker sense than Hadamard differentiability. On the other hand, it requires weak convergence of the empirical process w.r.t. a nonuniform sup-norm. The latter is not problematic since there exist strong respective results on weighted empirical processes obtained by Shorack and Wellner (1986) [25], Shao and Yu (1996) [23], Wu (2008) [32], and others. We illustrate the gain of the modified FDM by deriving the asymptotic distribution of plug-in estimates of popular risk measures that cannot be treated with the classical FDM.		Eric Beutner;Henryk Zähle	2010	J. Multivariate Analysis	10.1016/j.jmva.2010.06.015	econometrics;asymptotic analysis;calculus;mathematics;censoring;statistics	SE	32.43577055186702	-22.97042204936619	9523
987533045ddf50411ea8acb16f557c20426cc3a1	on computational complexity of the constructive-optimizer neural network for the traveling salesman problem		Abstract The authors formerly proposed the constructive-optimizer neural network (CONN) for the traveling salesman problem (TSP) to provide the best compromise between the solution quality and convergence speed. However, the computational complexity of CONN were cautiously reported as o( n 3 ). In this paper, by using a probabilistic analysis approach, we prove that the real computational complexity of CONN is of O( n 2 log n ). Three sets of benchmark TSPs from TSPLIB were used to evaluate the performance of CONN. We demonstrated that a polynomial of order n 2 log n provided the best fit to the CPU time of CONN versus the number of TSP cities. Also, CONN was further compared with a large number of state-of-the-art neural networks in terms of both solution quality and CPU time. We demonstrated that for ordinary TSPs, CONN may provide the best tradeoff between the CPU time and solution quality while for very large-scale TSPs, the memetic self-organizing map may be preferred.	artificial neural network;computational complexity theory;mathematical optimization;travelling salesman problem	Mehdi Saadatmand Tarzjan	2018	Neurocomputing	10.1016/j.neucom.2018.09.039	cpu time;probabilistic analysis of algorithms;machine learning;artificial neural network;travelling salesman problem;artificial intelligence;computational complexity theory;mathematics;polynomial;binary logarithm;compromise	ML	25.823881322931022	3.6299620502898535	9525
164e8f6f51d501c289e93dc56f4eae1aef5da1a7	combing spatial and temporal features for crowd counting with point supervision		In this paper, we present a new approach to count the number of people that cross a counting line from video images. This paper focuses on point-level annotation in training images and incorporate spatial features along with novel temporal features in training the structured random forest for estimating crowd density. By computing the crowd velocity, we model the crowd counting map as elementwise multiplication of crowd density map and crowd velocity map. Integrating over crowd counting map on the line of interest(LOI) locations leads to the instantaneous LOI counting numbers. We show that results are comparable to those obtained when using more complex and costly techniques.	algorithm;computation;maximum flow problem;optical flow;random forest;velocity (software development)	Haiying Jiang;Weidong Jin;Zhibin Yu;Peizhen Xu	2017	2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2017.8078489	computer vision;optical imaging;artificial intelligence;pattern recognition;feature extraction;random forest;computer science;combing;annotation;multiplication	Vision	40.73711232093751	-42.70982448605063	9528
1f7f5613cff5e56e749c038bd05d0d145c406bee	genetic programming with incremental learning for grammatical inference	genetic program;genetic operator;incremental learning;context free grammar;grammatical inference;evolutionary algorithm	We present an evolutionary algorithm for the inference of context-free grammars from positive and negative examples. The algorithm is based on genetic programming and uses a local optimization operator that is capable of improving the learning task. Ordinary genetic operators are modified so as to bias the search. The system was evaluated using Tomita¿s language examples and results were compared with another similar approach. Results show that the proposed approach is promising and more robust than the other one.	context-free grammar;context-free language;evolutionary algorithm;genetic operator;genetic programming;grammar induction;mathematical optimization	Ernesto Rodrigues;Heitor Silvério Lopes	2006	2006 Sixth International Conference on Hybrid Intelligent Systems (HIS'06)	10.1109/HIS.2006.29	evolutionary programming;natural language processing;genetic programming;computer science;genetic operator;machine learning;genetic representation;pattern recognition;population-based incremental learning	Robotics	6.732187357642405	-31.29684238700017	9538
ff90851d8c26eea768f1bf64d204adb4ce95d535	a parametric method to solve quadratic programming problems with fuzzy costs	quadratic optimization.;— fuzzy sets;decision making;fuzzy mathematical programming;quadratic program;objective function;quadratic optimization;fuzzy set	This work describes a novel fuzzy-sets-based method to solve a particular class of quadratic programming problems which have vagueness coefficients in the objective function. Quadratic programming problems are of utmost importance in an increasing variety of practical fields. In addition, as the ambiguity and vagueness are natural and ever-present in real-life situations requiring solutions, it makes perfect sense to attempt to address them using fuzzy quadratic programming problems. Also, two other methods to solve this kind of problems are briefly described. The proposal uses two phases to solve fuzzy quadratic programming problems. In the first, phase we parametrize the fuzzy problem in several classical alphaproblems with different cutting levels. In the second, phase each of these alpha-problems is solved by using conventional solving techniques. The final fuzzy solution to the problem is obtained by integrating all of these particular alpha-solutions. The results obtained using these two methods are compared with the two-phased proposal outlined above. Keywords— Fuzzy sets, decision making, fuzzy mathematical programming, quadratic optimization.	coefficient;fuzzy set;loss function;optimization problem;quadratic programming;real life;vagueness	Ricardo C. Silva;Carlos Cruz Corona;Akebo Yamakami	2009			fuzzy set operations;discrete mathematics;fuzzy transportation;second-order cone programming;sequential quadratic programming;quadratic programming;active set method;quadratically constrained quadratic program;mathematical optimization;fuzzy number;mathematics	ML	-0.6298378757374828	-18.129132236219085	9543
1bda44dfd7305417f193670e12dbd1804d82f6f3	analysis of stable prices in non-decreasing sponsored search auction		Most critical challenge of applying generalized second price (GSP) idea in multi-round sponsored search auction (SSA) is to prevent revenue loss for search engine provider (SEP). In this paper, we propose non-decreasing Sponsored Search Auction (NDSSA) to guarantee SEP’s revenue. Each advertiser’s bid increment is restricted by minimum increase price (MIP) in NDSSA. The MIP determination strategy influences bid convergence speed and SEP’s revenue. Fixed MIP strategy and Additive-Increase/Multiplicative-Decrease (AIMD) principle are applied to determine MIP values, and they are evaluated in this paper. For the convergence speed analysis, fixed MIP strategy converges faster than AIME in most instances. For SEP’s revenue, AIMD assists SEP to gain more revenue than fixed MIP strategy by experiments. Simultaneously, SEP’s revenue in Vickrey-Clarke-Groves auction (VCG) is the lower bound of that in AIMD.	sponsored search auction	ChenKun Tsung;Hann-Jang Ho;Sing-Ling Lee	2011		10.1007/978-3-642-34889-1_8	industrial organization;generalized second-price auction;economics;public economics;reverse auction;proxy bid;revenue equivalence;multiunit auction;dutch auction	ECom	-2.579473205437034	-3.494268418133514	9548
5b2285538dd83b836b520700387861cb730c1953	a pruning approach to optimize synaptic connections and select relevant input parameters for neural network modelling of solar radiation	optimal brain surgeon;optimization;neural network;solar radiation modelling	The accurate modelling of solar radiation is important for many applications including agriculture and energy management. Previous research has widely focused on the development of artificial neural network (ANN) models for this task. Since the inputs to these models include different meteorological parameters, it is usually time consuming and costly to acquire these parameters. Therefore, selection of most relevant input parameters is a key step in the construction of these models. The overall goal of this research is to develop ANN models with less number of parameters and at the same time, high modelling accuracy. To this aim, an algorithm for network pruning, the optimal brain surgeon (OBS), is proposed to achieve two main objectives, selection of most relevant input parameters and optimization of the network structure at synaptic level. Four meteorological parameters, namely: temperature ( T ), relative humidity (RH), wind speed (WS), and sunshine duration (SSD) are used to model solar radiation, the global horizontal irradiation (GHI), over Abu Dhabi, the United Arab Emirates. The results show that the least relevant input parameter is RH with a contribution of 15.2% in the modelling process as compared to 24.8% for WS, 47.8% for  T , and 54.7% for SSD. The parameter selection results coincide with recently used techniques in solar radiation research, J48 technique in Waikato Environment for Knowledge Analysis (WEKA) software and Automatic Relevance Determination (ARD) method. The modelling performance is compared with an all-connected ANN using all the four input parameters and ten hidden layer neurons with a total of 61 synaptic connections including biases. On the one hand, the proposed technique has successfully achieved an ANN with three inputs ( T   , WS, SSD), seven active hidden layer neuron, and 17 synaptic connections including biases. On the other hand, there are improvements observed in statistical evaluation metrics including mean absolute biased error (MABE), adjusted coefficient of determination (            R  ¯   2        ), Akaike information criterion (AIC), Akaike final prediction error (FPE), Rissanen's minimum description length (MDL), and cross entropy (CE).	artificial neural network;synaptic package manager	Sajid Hussain;Ali Al Alili	2017	Appl. Soft Comput.	10.1016/j.asoc.2016.09.036	mathematical optimization;simulation;computer science;artificial intelligence;machine learning;artificial neural network;statistics	Robotics	11.193620231904195	-19.711400037167902	9549
57c7e397315dac3b199b12082eb315433f8559de	additivity of uncertainty measures on credal sets	additivity;credal set;uncertainty measures;uncertainty;independence;incomplete information;credal sets;imprecise probability;probability distribution;imprecise probabilities;convex set	A general way of representing incomplete information is to use closed and convex sets of probability distributions, which are also called credal sets. Each credal set is associated with uncertainty, whose amount is quantified by an appropriate uncertainty measure. One of the requisite properties of uncertainty measures is the property of additivity, which is associated with the concept of independence. For credal sets, the concept of independence is not unique. This means that different definitions of independence lead to different definitions of additivity for uncertainty measures. In this paper, we compare the various definitions of independence, but our principal aim is to analyze those definitions that are employed in the most significant uncertainty measures established in the literature for credal sets.	convex set;discrete sine transform;mass effect trilogy;mass assignment vulnerability;uncertainty principle	Joaquín Abellán;George J. Klir	2005	Int. J. General Systems	10.1080/03081070500396915	probability distribution;independence;discrete mathematics;imprecise probability;uncertainty;probability box;data mining;mathematics;convex set;complete information;additive function;statistics	DB	1.1977904459960362	-19.341515992117202	9554
d76bba1f95bbc906f2ccd7bf685ca2b1c1315de3	efficient error recovery for multiple description video coding	data compression;motion compensation;decoding;image enhancement motion compensation video coding data compression decoding;video coding;image enhancement;error recovery performance multiple description coding mdc multihypothesis motion compensated prediction mhmcp enhanced error resilient video coding scheme multiple state video coding msvc video compression decoder;video coding motion estimation video compression decoding error correction redundancy programmable control yield estimation motion control streaming media	Multiple description coding (MDC) and multihypothesis motion-compensated prediction (MHMCP) are two promising error resilient video coding techniques. We propose an enhanced error resilient video coding scheme based on multiple state video coding (MSVC). It uses multihypothesis motion prediction in addition to normal motion-compensated video compression steps. Yielded additional motion vectors between frames of two descriptions can help efficiently and quickly estimate lost content in one description when the other one is received correctly. Negligible redundancy is added to originally coded MSVC bitstreams while complicated motion search is avoided at decoder when there is a need to estimate lost content. Simulation results show that the proposed scheme has better error recovery performance than previously used approaches in most situations, especially when multiple and complex motions appear.	data compression;encoder;multiple description coding;simulation	Guanjun Zhang;Robert L. Stevenson	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1419427	video compression picture types;data compression;scalable video coding;sub-band coding;computer vision;real-time computing;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;mathematics;block-matching algorithm;context-adaptive binary arithmetic coding;motion compensation;h.261;algorithm;statistics;multiview video coding	Vision	48.096969679447845	-16.837238751525227	9575
ed5efb92cff75bc4fdec1b5c89701a8e0190629d	comparing coordination schemes for miniature robotic swarms: a case study in boundary coverage of regular structures	distributed coordination;probabilistic model;reactive vs deliberative;exploration;self organization;swarm robotics;jet turbine inspection	We consider boundary coverage of a regular structure by a swarm of miniature robots, and compare a suite of three fully distributed coordination algorithms experimentally. All algorithms rely on boundary coverage by reactive control, whereas coordination of the robots high-level behavior is fundamentally different: random, self-organized, and deliberative with reactive elements. The self-organized coordination algorithm was designed using macroscopic probabilistic models that lead to analytical expressions for the algorithm’s mean performance. We contrast this approach with a provably complete, near optimal coverage algorithm, which is due to its assumption (noise-less sensors and actuators) infeasible on a real miniature robotic platform, but is considered to yield best-possible policies for an individual robot. Experimental results with swarms of up to 30 robots show that selforganization significantly improves coverage performance with increasing swarm size. We also observe that enforcing a provably complete policy on a miniature robot with limited hardware capabilities is highly sub-optimal as there is a trade-off between coverage throughput and time spent for localization and navigation.	as-interface;algorithm;experiment;high- and low-level;microbotics;robot;self-organization;self-organized criticality;swarm robotics;throughput	Nikolaus Correll;Samuel Rutishauser;Alcherio Martinoli	2006		10.1007/978-3-540-77457-0_44	control engineering;statistical model;swarm robotics;self-organization;simulation;exploration;computer science;engineering;artificial intelligence	Robotics	52.27585481252248	-25.613946729529022	9577
f3c74f43bcb3a43acbd5acd3b671f20ae49f9b65	towards learning domain-independent planning heuristics		Automated planning remains one of the most general paradigms in Artificial Intelligence, providing means of solving problems coming from a wide variety of domains. One of the key factors restricting the applicability of planning is its computational complexity resulting from exponentially large search spaces. Heuristic approaches are necessary to solve all but the simplest problems. In this work, we explore the possibility of obtaining domain-independent heuristic functions using machine learning. This is a part of a wider research program whose objective is to improve practical applicability of planning in systems for which the planning domains evolve at run time. The challenge is therefore the learning of (corrections of) domain-independent heuristics that can be reused across different planning domains.	artificial intelligence;artificial neural network;automated planning and scheduling;brute-force search;computational complexity theory;graph (discrete mathematics);heuristic (computer science);machine learning;run time (program lifecycle phase)	Pawel Gomoluch;Dalal Alrajeh;Alessandra Russo;Antonio Bucchiarone	2017	CoRR		computer science;machine learning;artificial intelligence;heuristics;computational complexity theory;heuristic	AI	19.12239797671958	-10.7621157780983	9591
6eb50cde29a3af20d1b203412d73a8ab10930680	real-time color stereo vision system for a mobile robot based on field multiplexing	vision system;image processing equipment robot vision stereo image processing image colour analysis video signal processing multiplexing image sensors mobile robots;real time color stereo vision system;image processing equipment;video signal processing;mobile robot;color;real time;video compression;correlation based ezdf method;mobile robots;image sensors;multiplexing;real time systems stereo vision mobile robots color monos devices machine vision signal processing video compression robot vision systems cameras;field multiplexing;robot vision;monos devices;image colour analysis;monochrome stereo video signals;machine vision;signal processing;stereo image processing;stereo vision;stereo tracking;stereo tracking real time color stereo vision system mobile robot field multiplexing monochrome stereo video signals correlation based ezdf method;robot vision systems;cameras;real time systems	This paper discusses a stereo vasion processing system which processes color and monochrome stereo video signals on a single vision processing board. Here, we propose a “field mixing” technique for multiplexing multiple video signals. A compact color stereo vision system based on this technique is developed for a mobile robot. This syst em can process multiple video signals simultaneously, and realizes a flexible color stereo vision processing. In order to show the feasibility of this vision system, we installed it on our mobile robot, and implemented correlation-based EZDF method f o r stereo tracking of a n object. The experimental result of the tracking is shown.	mobile robot;monochrome;multiplexing;real-time transcription;stereopsis	Yoshio Matsumoto;Tomohiro Shibata;Katsuhiro Sakai;Masayuki Inaba;Hirochika Inoue	1997		10.1109/ROBOT.1997.619071	computer stereo vision;mobile robot;embedded system;stereo cameras;stereo camera;computer vision;machine vision;computer science;artificial intelligence;computer graphics (images)	Robotics	45.470908932062855	-35.54885817434597	9597
3e99629a38b4b22bbcec1527b829abab778207f6	parallel parsing of mpeg video	video streaming;data compression;video signal processing;mpeg video;multimedia communication video signal processing feature extraction data compression video coding;group of picture;video coding;key features mpeg video video parsing video streams group of pictures multimedia information system video browsing;feature extraction;multimedia communication;streaming media video compression layout decoding transform coding redundancy information resources data mining indexing computer science;parallel processing;compressed video	Video parsing refers to the detection and classification of abrupt and gradual scene changes in a video stream and constitutes an important preprocessing step in applications that treat video streams as sources of information. Parallel processing is proposed as a means of dealing with the high computational demands of video parsing. Parallel versions of two algorithms that detect scene transitions in compressed video streams are proposed. Three granularities of parallelism are investigated; Group of Pictures (GOP), frame and slice. Results show that the GOP-level implementation, which represents the coarsest granularity of task and data decomposition, always performs the best. The slice and frame levels of granularity take the second and third place respectively. The speedup a's shown to be almost linear in the case of the GOP level of granularity, whereas the Synchronization overheads are seen to be high for the frame and slice levels of granularity.	algorithm;data compression;distributed memory;emoticon;glr parser;group of pictures;h.264/mpeg-4 avc;linear algebra;mpeg-1;mpeg-7;moving picture experts group;overhead (computing);parallel computing;parsing;preprocessor;scalability;scheduling (computing);shared memory;speedup;streaming media;video coding format	Suchendra M. Bhandarkar;Shankar R. Chandrasekaran	2001	International Conference on Parallel Processing, 2001.	10.1109/ICPP.2001.952091	video compression picture types;data compression;scalable video coding;parallel processing;computer vision;parallel computing;h.263;uncompressed video;feature extraction;computer science;theoretical computer science;video capture;video tracking;block-matching algorithm;multimedia;video processing;smacker video;motion compensation;video post-processing;h.261;video denoising;multiview video coding	HPC	44.05889947901479	-22.826804361782635	9600
7b0e464417646ee6b116621bbdecf4773c9ddc4d	fuzzy association rules discovered on effective reduced database algorithm	knowledge based fuzzy model;learning artificial intelligence computational complexity data mining database management systems fuzzy set theory knowledge based systems;fuzzy set;simple shaped fuzzy partition fuzzy association rules discovery reduced database algorithm continuous attributes numerical attributes supermarket basket analysis fuzzy sets space complexity time complexity knowledge based fuzzy model reduced database table;fuzzy association rules discovery;time complexity;database management systems;supermarket basket analysis;reduced database algorithm;simple shaped fuzzy partition;data mining;association rules databases fuzzy sets algorithm design and analysis shape fuzzy logic data engineering partitioning algorithms machine learning algorithms pattern analysis;fuzzy set theory;fuzzy sets;computational complexity;numerical attributes;space complexity;reduced database table;learning artificial intelligence;continuous attributes;knowledge based systems;fuzzy model;knowledge base;fuzzy association rules	Fuzzy association rules can deal with continuous (numerical) attributes in database, and hence provide alternative new approach for their applications, such as supermarket basket analysis. This new approach can not only find the relations of continuous attributes, but also discrete (nominal) attributes by using crisp sets as special fuzzy sets, moreover combine them together to get good rules for analysers. However, compared with traditional models, fuzzy models generally have space and time complexity problem. We therefore develop the effective reduced database algorithm with less space and time complexity, which effectively form the transparent and knowledge based fuzzy model -reduced database table, so that we can simply discover fuzzy association rules from the reduced database table	algorithm;association rule learning;bayesian network;decision tree;feature selection;fuzzy set;numerical analysis;table (database);time complexity	Dong Xie	2005	The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.	10.1109/FUZZY.2005.1452493	knowledge base;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	DB	-1.6304609760764572	-29.00590627919342	9601
c21f9ea0b4205449f2c33c869946af535a194ada	regularized semi-supervised latent dirichlet allocation for visual concept learning	low rank graph;visual concept learning;semi supervised learning;latent dirichlet allocation	Topic model is a popular tool for visual concept learning. Most topic models are either unsupervised or fully supervised. In this paper, to take advantage of both limited labeled training images and rich unlabeled images, we propose a novel regularized Semi-Supervised Latent Dirichlet Allocation (r-SSLDA) for learning visual concept classifiers. Instead of introducing a new complex topic model, we attempt to find an efficient way to learn topic models in a semi-supervised way. Our r-SSLDA considers both semi-supervised properties and supervised topic model simultaneously in a regularization framework. Furthermore, to improve the performance of r-SSLDA, we introduce the low rank graph to the framework. Experiments on Caltech 101 and Caltech 256 have shown that r-SSLDA outperforms both unsupervised LDA and achieves competitive performance against fully supervised LDA with much fewer labeled images.	concept learning;latent dirichlet allocation;semi-supervised learning	Liansheng Zhuang;Haoyuan Gao;Jiebo Luo;Zhouchen Lin	2013	Neurocomputing	10.1016/j.neucom.2012.04.043	semi-supervised learning;latent dirichlet allocation;unsupervised learning;dynamic topic model;computer science;machine learning;pattern recognition;data mining;mathematics	ML	23.53147263132699	-44.56854205000673	9618
cdfbd034737012c0ac23968977bdad270bbb8c54	functional envelope for model-free sufficient dimension reduction		In this article, we introduce the functional envelope for sufficient dimension reduction and regression with functional and longitudinal data. Functional sufficient dimension reductionmethods, especially the inverse regression estimation family ofmethods, usually involve solving generalized eigenvalue problems and inverting the infinite-dimensional covariance operator. With the notion of functional envelope, essentially a special type of sufficient dimension reduction subspace, we develop a generic method to circumvent the difficulties in solving the generalized eigenvalue problems and inverting the covariance directly. We derive the geometric characteristics of the functional envelope and establish the asymptotic properties of related functional envelope estimators undermild conditions. The functional envelope estimators have shown promising performance in extensive simulation studies and real data analysis. Published by Elsevier Inc.	dimensionality reduction;simulation;sufficient dimension reduction	Xin Zhang;Chong Wang;Yichao Wu	2018	J. Multivariate Analysis	10.1016/j.jmva.2017.09.010	estimator;mathematics;eigenvalues and eigenvectors;covariance operator;subspace topology;covariance;mathematical analysis;mathematical optimization;sufficient dimension reduction	ML	28.915960520651264	-25.97392748053472	9621
d5d2134d199de46b1149ac79e05139e46c80e985	a novel milp-based objective reduction method for multi-objective optimization: application to environmental problems	multi objective optimization;objective reduction;environmental engineering;reduction method;life cycle assessment;environmental problem	Multi-objective optimization has recently emerged as a useful technique in sustainability analysis, as it can assist in the study of optimal trade-off solutions that balance several criteria. The main limitation of multi-objective optimization is that its computational burden grows in size with the number of objectives. This computational barrier is critical in environmental applications in which decision-makers seek to minimize simultaneously several environmental indicators of concern. With the aim to overcome this limitation, this paper introduces a systematic method for reducing the number of objectives in multi-objective optimization with emphasis on environmental problems. The approach presented relies on a novel mixed-integer linear programming formulation that minimizes the error of omitting objectives. We test the capabilities of this technique through two environmental problems of different nature in which we attempt to minimize a set of life cycle assessment impacts. Numerical examples demonstrate that certain environmental metrics tend to behave in a non-conflicting manner, which makes it possible to reduce the dimension of the problem without losing information.	mathematical optimization;multi-objective optimization;objective collapse theory	Gonzalo Guillén-Gosálbez	2011	Computers & Chemical Engineering	10.1016/j.compchemeng.2011.02.001	mathematical optimization;engineering;multi-objective optimization;mathematics;management science	AI	13.243511231333768	-5.501581550371068	9643
843714f135dbd7d7cb09892f00f34bb3f8bc2abc	combined variable speed limit and lane change control for truck-dominant highway segment	road transportation safety automobiles aerospace electronics microscopy monte carlo methods;truck traffic;environmental impacts;travel time;capacity drop;highway safety;bottlenecks;monte carlo method;freeways;highway capacity;highway traffic control;microsimulation;velocity control environmental factors monte carlo methods road traffic control;lane changing;relief capacity drop lane change control truck dominant highway segment traffic demand highway networks highway traffic mobility variable speed limit control vsl travel time capacity drop microscopic monte carlo simulations i 710 freeway truck demand environmental impact travel time traffic conditions incident scenarios;variable speed limits	Traffic demand of trucks is rapidly increasing on highway networks, which harms highway traffic mobility, safety and the environment. Variable speed limit (VSL) control is considered to be able to improve the traffic condition in truck-dominant highway segments. However previous research reported rather mixed effects of VSL controller on traffic mobility. While some studies reported improvements in travel time due to the use of VSL control, others reported either no improvement or small deterioration in travel time. In this paper, we demonstrated that the lack of improvement on travel time is due to lane changes that are taking place close to the bottleneck leading to severe capacity drop. We developed a combined lane change and VSL control strategy that recommends lane changes in advance to relief capacity drop in addition to VSL. Microscopic Monte-Carlo simulations on I-710 freeway with high truck demand were used to demonstrate that this combined control strategy is able to generate consistent improvements with respect to travel time, safety and environmental impact under different traffic conditions and incident scenarios.	change control;computer simulation;control theory;freeway;monte carlo method;simulation	Yihang Zhang;Petros A. Ioannou	2015	2015 IEEE 18th International Conference on Intelligent Transportation Systems	10.1109/ITSC.2015.192	simulation;engineering;automotive engineering;transport engineering	Robotics	9.719857414495204	-9.825180458129699	9645
8479a8520fce54f1a7ebcaf76e95d1191cc5ba7f	a new topological clustering algorithm for interval data	interval data;clustering;self organizing map	Clustering is a very powerful tool for automatic detection of relevant sub-groups in unlabeled data sets. In this paper we focus on interval data: i.e. where the objects are defined as hyper-rectangles. We propose here a new clustering algorithm for interval data, based on the learning of a Self Organizing Map. The major advantage of our approach is that the number of clusters to find is determined automatically; no a priori hypothesis for the number of clusters is required. Experimental results confirm the effectiveness of the proposed algorithm when applied to interval data.	algorithm;cluster analysis;self-organizing map	Guénaël Cabanes;Younès Bennani;Renaud Destenay;André Hardy	2013	Pattern Recognition	10.1016/j.patcog.2013.03.023	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;self-organizing map;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;affinity propagation;clustering high-dimensional data	Vision	2.2743852198745134	-41.50702005356773	9650
4ce3beb05c0e92e534e87aba2dcf4326a2fa41a7	learning neural networks for detection and classification of synchronous recurrent transient signals	evaluation performance;likelihood ratio;deteccion optimal;performance evaluation;detection signal;neural networks;learning;optimum detection;time complexity;synchronous signal;evaluacion prestacion;signal detection;classification;aprendizaje;apprentissage;deteccion senal;signal processing;synchronous signals;network architecture;detection optimale;optimal detection;tree network;reseau neuronal;clasificacion;red neuronal;white noise;neural network	This paper proposes a neural network solution to the classical signal processing problem of detection of a synchronous recurrent transient signal in noise. If a signal exists, it is assumed to be one of M known signals which may sometimes occur (probabilistically) in successive intervals. Several neural network configurations are applied to this problem and compared with each other and with the optimum adaptive sequential detector. A novel efficient neural network detector is proposed using an XOR-1ree configuration with learning. Tests with synthetic and real noise, show the excellent performance of this approach as compared to the optimum adaptive detector and to other neural network techniques. With real (non-white) noise obtained from sonar data, the XOR-1ree network widely outperforms the likelihood ratio detector. We also discuss the learning time complexity of the XOR-1ree network and compare it to that of standard three layer network architectures. ( 1998 Elsevier Science B.V. All rights reserved.	akaike information criterion;algorithm;artificial neural network;discrete cosine transform;exclusive or;experiment;feedforward neural network;lr parser;mean squared error;multilayer perceptron;multitier architecture;sonar (symantec);signal processing;stochastic process;synthetic intelligence;time complexity;vergence;white noise	Erol Gelenbe;Kerem Harmaniota;Jeffrey L. Krolik	1998	Signal Processing	10.1016/S0165-1684(97)00193-X	time complexity;network architecture;telecommunications;biological classification;likelihood-ratio test;computer science;artificial intelligence;machine learning;mathematics;white noise;artificial neural network;statistics;detection theory	ML	12.198858399180049	-31.910889754903998	9652
bb9a83b049e530cc00d882af4fd0c5fba79ab04d	an evolutionary algorithm with spatially distributed surrogates for multiobjective optimization	nondominated sorting genetic algorithm;prediction error;radial basis function;spatial distribution;prediction accuracy;multiobjective optimization;evolutionary algorithm;rbf network;k means clustering	In this paper, an evolutionary algorithm with spatially distributed surrogates (EASDS) for multiobjective optimization is presented. The algorithm performs actual analysis for the initial population and periodically every few generations. An external archive of the unique solutions evaluated using the actual analysis is maintained to train the surrogate models. The data points in the archive are split into multiple partitions using k-Means clustering. A Radial Basis Function (RBF) network surrogate model is built for each partition using a fraction of the points in that partition. The rest of the points in the partition are used as a validation data to decide the prediction accuracy of the surrogate model. Prediction of a new candidate solution is done by the surrogate model with the least prediction error in the neighborhood of that point. Five multiobjective test problems are presented in this study and a comparison with Nondominated Sorting Genetic Algorithm II (NSGA-II) is included to highlight the benefits offered by our approach. EASDS algorithm consistently reported better nondominated solutions for all the test cases for the same number of actual evaluations as compared to a single global surrogate model and NSGA-II.	evolutionary algorithm;multi-objective optimization;program optimization;surrogates	Amitay Isaacs;Tapabrata Ray;Warren Smith	2007		10.1007/978-3-540-76931-6_23	mathematical optimization;radial basis function;computer science;artificial intelligence;multi-objective optimization;machine learning;evolutionary algorithm;mean squared prediction error;mathematics;k-means clustering	EDA	29.0176452711241	-6.529316419834118	9660
7f8bddd64613e7444118f3551d040a2a282c6405	large scale multikernel rvm for object detection	bayes estimation;scale model;model specification;modele reduit;scaling law;pertinencia;prior distribution;regression model;prior knowledge;intelligence artificielle;classification;detection objet;modelo reducido;detector proximidad;large scale;modelo regresion;estimacion bayes;ley escala;modele regression;pertinence;relevance vector machine;inferencia;artificial intelligence;inteligencia artificial;relevance;loi echelle;vector processor;clasificacion;large scale problem;inference;proximity detector;object detection;estimation bayes;bayesian model;processeur vectoriel;detecteur proximite	The Relevance Vector Machine(RVM) is a widely accepted Bayesian model commonly used for regression and classification tasks. In this paper we propose a multikernel version of the RVM and present an alternative inference algorithm based on Fourier domain computation to solve this model for large scale problems, e.g. images. We then apply the proposed method to the object detection problem with promising results.	algorithm;bayesian network;computation;multikernel;object detection;relevance vector machine	Dimitris Tzikas;Aristidis Likas;Nikolas P. Galatsanos	2006		10.1007/11752912_39	vector processor;prior probability;relevance;biological classification;computer science;machine learning;pattern recognition;relevance vector machine;bayesian inference;specification;scale model;regression analysis;statistics	AI	29.375204488971743	-31.76665907608754	9668
5bd147397d6b8c7adc4693499ee96ae169ad90bf	classification of functional data: a segmentation approach	classification automatique statistiques;analyse multivariable;47a10;multivariate analysis;fonctionnelle;analisis datos;analyse fonctionnelle;prior distribution;46xx;prior knowledge;spectrum;segmentation;linear discriminate analysis;ley a priori;public domain;funcional;discriminant analysis;analyse discriminante;data analysis;analisis discriminante;functional analysis;functional;62h30;heterogeneidad;feature extraction;53a45;statistical computation;calculo estadistico;discriminant;analisis multivariable;analyse donnee;calcul statistique;functional data;analyse vectorielle;support vector machine;spatial heterogeneity;60k30;segmentacion;vector analysis;loi a priori;heterogeneity;heterogeneite;analisis funcional	We suggest a classification and feature extraction method on functional data where the predictor variables are curves. The method, called functional segment discriminant analysis (FSDA), combines the classical linear discriminant analysis and support vector machine. FSDA is particularly useful for irregular functional data, characterized by spatial heterogeneity and local patterns like spikes. FSDA not only reduces the computation and storage burden by using a fraction of the spectrum, but also identifies important predictors and extracts features. FSDA is highly flexible, easy to incorporate information from other data sources and/or prior knowledge from the investigators. We apply FSDA to two public domain data sets and discuss the understanding developed from the study.		Bin Li;Qingzhao Yu	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.03.024	functional analysis;spectrum;support vector machine;econometrics;public domain;prior probability;vector calculus;feature extraction;heterogeneity;machine learning;pattern recognition;mathematics;multivariate analysis;linear discriminant analysis;data analysis;segmentation;statistics;spatial heterogeneity;discriminant	ML	34.76414508406108	-23.581925195436668	9676
510bc207fec452c1a02d74bc75bf563a8b316268	locating sensors to observe network arc flows: exact and heuristic approaches	traffic network;genetic algorithm;branch and bound;sensor location	The problem of optimally locating sensors on a traffic network to monitor flows has been object of growing interest in the past few years, due to its relevance in the field of traffic management and control. Sensors are often located in a network in order to observe and record traffic flows on arcs and/or nodes. Given traffic levels on arcs within range or covered by the sensors, traffic levels on unobserved portions of a network can then be computed. In this paper, the problem of identifying a sensor configuration of minimal size that would permit traffic on any unobserved arcs to be exactly inferred is discussed. The problem being addressed, which is referred to in the literature as the Sensor Location Problem (SLP), is known to be NP-complete, and the existing studies about the problem analyze some polynomial cases and present local search heuristics to solve it. In this paper we further extend the study of the problem by providing a mathematical formulation that up now has been still missing in the literature and present an exact branch and bound approach, based on a binary branching rule, that embeds the existing heuristics to obtain bounds on the solution value. Moreover, we apply a genetic approach to find good quality solutions. Extended computational results show the effectiveness of the proposed approaches in solving medium-large instances.	branch and bound;computation;heuristic (computer science);local search (optimization);np-completeness;polynomial;relevance;sensor;superword level parallelism;traffic analysis	Lucio Bianco;Carmine Cerrone;Raffaele Cerulli;Monica Gentili	2014	Computers & OR	10.1016/j.cor.2013.12.013	traffic generation model;mathematical optimization;simulation;genetic algorithm;computer science;branch and bound	AI	17.858214275492553	2.2358706757279845	9683
5b29f7a02cc7950fe5d50b3155f1ef9a2a3fdc31	a rate control algorithm based on adaptive r-q model for mpeg-1 to mpeg-4 transcoding in dct domain	adaptive rate control;conference;adaptive control;programmable control adaptive control mpeg 4 standard transcoding discrete cosine transforms;programmable control;transform coding;adaptive codes;dct domain;transcoded bitstream rate control algorithm adaptive r q model mpeg 1 to mpeg 4 transcoding dct domain adaptive rate control video transmission target bit rate;variable rate codes;rate control;video coding;mpeg 4 standard;discrete cosine transforms;transcoded bitstream;video transmission;variable rate codes video coding discrete cosine transforms transform coding adaptive control adaptive codes;rate control algorithm;mpeg 1 to mpeg 4 transcoding;adaptive r q model;target bit rate;transcoding	This paper proposes an adaptive rate control algorithm for MPEG-1 to MPEG-4 transcoder for video transmission over networks of different capacities and characteristics. It is based on an adaptive R-Q model to follow the new target bit-rate of a transcoded bitstream. In R-Q modeling, the R-Q characteristics predicted from a previous picture of the same picture type are used in combination with the R-Q data measured for the current picture. To overcome the quantizer mismatch between MPEG-1 and MPEG-4 syntax, a gradual adaptation function is applied. It is shown that the adaptive R-Q model-based rate control can efficiently control the output bit-rate to match the target bit budget.	algorithm;discrete cosine transform;mpeg-1	Kwang-deok Seo;Seong-cheol Heo;Jae-Kyoon Kim	2002		10.1109/ICC.2002.996827	computer vision;real-time computing;transform coding;transcoding;adaptive control;computer science;theoretical computer science;statistics	Robotics	47.26560271011723	-17.485075799464745	9684
98457d972049789b12c256b70db166dd9d3c9155	cluster ensembles a knowledge reuse framework for combining partitionings		It is widely recognized that combining multiple classification or regression models typically provides superior results compared to using a single, well-tuned model. However, there are no well known approaches to combining multiple non-hierarchical clusterings. The idea of combining cluster labelings without accessing the original features leads us to a general knowledge reuse framework that we call <i>cluster ensembles</i>. Our contribution in this paper is to formally define the cluster ensemble problem as an optimization problem and to propose three effective and efficient combiners for solving it based on a hypergraph model. Results on synthetic as well as real data sets are given to show that cluster ensembles can (i) improve quality and robustness, and (ii) enable distributed clustering.	cluster analysis;mathematical optimization;optimization problem;synthetic intelligence	Alexander Strehl;Joydeep Ghosh	2002				ML	20.57047158797921	-42.38056775805654	9686
403d1123df907a07746b796495b6599b9c7a3c61	a preliminary comparison of tree encoding schemes for evolutionary algorithms	network design;optimal communication spanning tree;encoding evolutionary computation tree graphs decoding genetic mutations genetic algorithms testing design optimization polynomials performance evaluation;tree codes;kruskal wallis nonparametric tests;network design problems;network design problems tree encoding schemes evolutionary algorithms genetic algorithm optimal communication spanning tree kruskal wallis nonparametric tests;multiple comparisons;tree encoding schemes;evolutionary algorithms;genetic algorithm;genetic algorithms;spanning tree;evolutionary algorithm;tree codes genetic algorithms	This paper presents a comparative study of six encodings which have been used to represent trees in evolutionary algorithms. The study has been divided into two steps: 1) The encoding methods have been evaluated taking into account the time necessary to perform operations such as decoding, crossover and mutation, the feasibility of solutions after those operations, and the corresponding heritability and locality; 2) The encoding methods have been employed in a genetic algorithm to solve three different instances (with 10, 25 and 50 nodes) of the optimal communication spanning tree problem. Finally, the results obtained with each of the encodings are statistically compared using Kruskal-Wallis non-parametric tests and multiple comparisons. The results of this study provide insight into the properties of current encoding schemes for network design problems.	evolutionary algorithm;file spanning;genetic algorithm;kruskal's algorithm;locality of reference;minimum spanning tree;network planning and design	Eduardo G. Carrano;Carlos M. Fonseca;Ricardo H. C. Takahashi;Luciano C. A. Pimenta;Oriane M. Neto	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414042	mathematical optimization;combinatorics;genetic algorithm;computer science;artificial intelligence;order statistic tree;machine learning;evolutionary algorithm;tree rearrangement;genetic representation;mathematics;distributed minimum spanning tree;tree traversal;algorithm	Robotics	25.202769808926213	-0.4986634541968891	9698
96ca0837c0c14de6db121be090523031825dc565	a monocular mobile robot reactive navigation approach based on the inverse perspective transformation	localization;visual navigation;feature traking;mobile autonomous robots	This paper presents an approach to visual obstacle avoidance and reactive robot navigation for outdoor and indoor environments. The obstacle detection algorithm includes an image feature tracking procedure followed by a feature classification process based on the IPT Inverse Perspective Transformation. The classifier discriminates obstacle points from ground points. Obstacle features permit to draw out the obstacle boundaries which are used to construct a local and qualitative polar occupancy grid, analogously to a visual sonar. The navigation task is completed with a robocentric localization algorithm to compute the robot pose by means of an EKF Extended Kalman Filter. The filter integrates the world coordinates of the ground points and the robot position in its state vector. The visual pose estimation process is intended to correct possible drifts on the dead-reckoning data provided by the proprioceptive robot sensors. The experiments, conducted indoors and outdoors, illustrate the range of scenarios where our proposal has proved to be useful, and show, both qualitatively and quantitatively, the benefits it provides.	mobile robot	Francisco Bonin-Font;Antoni Burguera;Alberto Ortiz;Gabriel Oliver	2013	Robotica	10.1017/S0263574712000252	computer vision;simulation;internationalization and localization;computer science;mobile robot navigation	Robotics	51.51468659126941	-38.155092191653836	9732
586c95531e87511f32f618c1d4b0f400af3c4b07	a neural network based breast cancer prognosis model with pca processed features	classification;principal component analysis;prediction model;prognosis;artificial neural network	Accurate identification of the diagnosed cases is extremely important for a reliable prognosis of breast cancer. Data analytics and learning based methods can provide an effective framework for prognostic studies by accurately classifying data instances into relevant classes based on the tumor severity. Accordingly, a multivariate statistical approach has been coupled with an artificial intelligence based learning technique to implement a prediction model. Principal components analysis pre-processes the data and extracts features in the most relevant form for training an artificial neural network that learns the patterns in the data for classification of new instances. The diagnostic data of the original Wisconsin breast cancer database accessed from the UCI machine learning repository has been used in the study. The proposed hybrid model shows promising results when compared with other classification algorithms used most commonly in the literature and can provide a future scope for creation of more sophisticated machine learning based cancer prognostic models.	algorithm;artificial intelligence;artificial neural network;machine learning;mathematical optimization;predictive modelling;principal component analysis;statistical classification	Smita Jhajharia;Harish Kumar Varshney;Seema Verma;Rajesh Kumar	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732327	biological classification;computer science;machine learning;pattern recognition;data mining;predictive modelling;artificial neural network;principal component analysis	ML	9.733758646390392	-46.74138013110501	9757
b52fa5a8913d42c907633cbb0dabbbbf29f22036	an empirical study of ai-based multiple domain models for selecting attributes that drive company wealth	domain model;empirical study;stock market;artificial intelligent;shareholder value;single domain;feature selection	This paper explores what attributes drive company wealth creation in the Miscellaneous Industrials sector of the Australian Stock Market. We examine traditional and artificial intelligent (AI) feature selection techniques, to select attributes that drive company wealth and observe if a multiple domain model outperforms single domain models with regards to predicting company wealth. Using a large number of calculated attributes, our empirical findings suggest that a multiple domain model was most effective. We find that Market Capitalisation, Debt Asset Ratio, ROI and Beta Value attributes are the main contributors to following the directional change in the shareholder value. This was using a J4.8 and GRNN multiple domain model.		Mark B. Barnes;Vincent Cheng-Siong Lee	2007	Soft Comput.	10.1007/s00500-007-0160-4	single domain;computer science;artificial intelligence;machine learning;domain model;empirical research;feature selection	AI	5.005656163910012	-18.954402524909977	9766
c6c5b7d7da374eb3da6facbe39a9ae17028cfa74	a low complexity method for real-time gaze tracking	high resolution;image segmentation;image resolution;neural networks;neural nets;real time;light emitting diodes;training;real time gaze tracking;real time processing;gaze tracking;low complexity;low complexity gaze tracking method;accuracy;artificial neural networks;tracking computational complexity face recognition image resolution image segmentation neural nets;face recognition;dual thresholds;infrared lamp houses;computational complexity;real time processing problems;face images;neural networks real time gaze tracking low complexity gaze tracking method real time processing problems face images infrared lamp houses dual thresholds;infrared;reflection training artificial neural networks cameras real time systems accuracy light emitting diodes;reflection;cameras;tracking;neural network;real time systems	The paper addresses a novel, fast and low complexity gaze tracking method, which remedies computation complexity and real-time processing problems of most existing gaze tracking methods. Face images are captured and pre-processed under infrared lamp-houses with dual thresholds. Pupil area is verified by the run-length in eight directions, and the gaze point is located by mapping the vectors of pupil and reflection point or using neural network. The experimental results show that the real-time processing of gaze tracking, with high resolution and high accuracy simultaneously, is achieved in this new system.	algorithm;artificial neural network;computation;computer vision;eye tracking;image resolution;information retrieval;lamp (software bundle);objectivity/db;real-time clock;real-time locating system;run-length encoding	Jing Zhang;Mengkai Zhao;Li Zhuo;Lansun Shen	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665198	computer vision;image resolution;computer science;machine learning;artificial neural network;computer graphics (images)	Robotics	46.901179435982606	-44.571938114912484	9769
fe7ab34e71d110d836a30ffb287306a02af5f189	mesh motion scale invariant feature and collaborative learning for visual recognition		Visual recognition has been gradually played important roles in many fields. An effective feature descriptor, with higher discrimination and higher descriptiveness for the different visual recognition tasks, is a challenging issue. In this paper, we propose a novel feature, called mesh motion scale invariant feature description, to facilitate the different visual task description and balance discrimination and efficiency. Then, a hierarchical collaborative feature learning model for multi-visual tasks in complex scenes is presented for obtaining the recognition results. Four large databases, FRGC, CASIA, BU-3DFE and 3D Online Action, are introduced to the performance comparison and the experimental results show a better performance for face recognition, expression recognition and activity recognition based on our proposed method.	activity recognition;database;facial recognition system;feature learning;visual descriptor	Yue Ming;Jiakun Shi	2018	Multimedia Tools and Applications	10.1007/s11042-018-5969-6	computer science;facial recognition system;artificial intelligence;pattern recognition;activity recognition;collaborative learning;scale invariance;feature learning	Vision	34.87886434241147	-51.2648244711396	9771
e9cb894af4008ffc66282c454907e01b97948221	a hierarchy control algorithm and its application in urban arterial control problem	control algorithm;road traffic;real time;traffic control;hierarchy control algorithm;cell transmission model;multiobjective control problem;traffic signal timing control problem;control problem;feeding delay;urban arterial control problem;traffic control timing signal design communication system traffic control control systems optimization methods delay estimation intelligent transportation systems cities and towns delay effects;multiobjective control problem urban arterial control problem hierarchy control algorithm traffic signal timing control problem feeding delay cell transmission model;traffic control road traffic;simulation environment;control strategy	The purpose of this paper is to investigate the application of a hierarchy control algorithm in solving the traffic signal timing control problem for an oversaturated traffic arterial. In the local control layer, the concept of feeding delay and non-feeding delay is introduced , and the cell transmission model is used to calculate the feeding delay and the non-feeding delay, then the control problem is formulated as an conflicted multi-objective control problem, and an IPNSGA-II based compatible controller installed at adjacent junctions is used to design signal timing plan for each junctions. Then in the coordination layer, based on the state of density on each link, three rules are proposed to define the range of the junctions required to be coordinated. Finally, the IPNSGA-II is used to design the traffic signal timing plan for the junction group. Simulation results show that the proposed algorithm is capable of dealing with real-time oversaturated traffic arterial control problem. The algorithm is tested in a simulation environment consisting of a core area of 7 junctions. It can be concluded that the proposed method is much more effective in relieving oversaturation in a network than the isolated intersection control strategy.	algorithm;concave function;control theory;core (optical fiber);emoticon;mathematical optimization;multi-objective optimization;pareto efficiency;real-time clock;simulation;software release life cycle	Juan Chen;Lihong Xu;Xiaoguang Yang;Changliang Yuan	2007	2007 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2007.4357673	control engineering;simulation;engineering;transport engineering	EDA	11.187872212176872	-8.554399853588059	9782
60ca3019712fdbb554441fabec28d86b8c1a07ea	prediction of amazon spot price based on chaos theory using anfis model		Spot Instance Market is the most recent advancement in cloud computing business models. It is introduced by Amazon's Elastic Compute Cloud (Amazon EC2) in order to utilize its idle resources more efficiently. The main characteristic of spot instance is its dynamic pricing. The hourly price for a spot instance fluctuates depending on the supply and demand for cloud resources. Users across the globe can bid for a spot instance using an online auction platform. The auction platform determines the current market price, a.k.a. “Spot price” and the users whose bids are above the spot price obtain the instance. Amazon publicizes current spot price but does not disclose how it is determined. The major challenge for the users in this new business model is to predict the spot price before bidding. In this paper, we propose a new spot price forecasting model based on chaos theory. The proposed method makes use of chaos time series analysis to verify the chaotic feature of Amazon spot price and to perform a prediction using Adaptive Neural Fuzzy Inference System (ANFIS). We perform extensive simulation experiments using real spot price traces and show that the proposed method can be a bright merit to predict Amazon spot price.	adaptive neuro fuzzy inference system;amazon elastic compute cloud (ec2);amazon kindle;chaos theory;cloud computing;experiment;simulation;time series;tracing (software)	Zohra Amekraz;Moulay Youssef	2016	2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2016.7945632	computer science;real-time computing;cloud computing;adaptive neuro fuzzy inference system;market price;elasticity (economics);dynamic pricing;bidding;spot contract;supply and demand	HPC	5.539900576708785	-14.91939666333551	9797
aa5b0e67b457cd468f8cc00c0db7f853300c7b74	generative and evolutionary design exploration		Expert designers, both architects and engineers, typically display a strategy of exploring design alternatives, albeit a relatively small number. Expert architects’ strategy in problem solving can be denoted breadth first, depth next, in comparison to novices, who typically display less breadth of exploration (Akin, 2001). Although engineers’ strategy is markedly different, design alternatives play a role that is as important, if not more important, than for artchitects (Akin, 2009). Where designers typically consider a very small number of alternatives in their work, this can be explained by cognitive limits, opening the door for computational support of design exploration. In particular, Woodbury and Burrow (2006) argued that exploration is a compelling model for designer action and that designers benefit from tools that amplify their abilities to represent goals and problems spaces, and to search for designs. Generative and evolutionary methods have proven to be strong catalysts for design exploration, and design optimization has served as a means to assist in this exploration. Recently there is a marked move toward using optimization to aid exploration. Optimization is rarely intended to yield an optimal solution per se, instead assisting in gaining insight in the solution space, thereby reducing the size of the solution space for exploration, possibly focusing attention toward the Pareto boundary. Even at the Pareto boundary, there are a large number of solutions worthy of further exploration. Exploration and optimization together lead to a better understanding of the complexities of design issues and help designers in their decision-making process, especially with multiple-objectives problems, which is a nature of many design problems. As such, the focus of attention in generative and evolutionary design is shifting from the techniques themselves, and their direct application, to the way we are using these techniques to assist and improve the design and engineering process. We might frame generative and evolutionary design from the point of view of a “conversation” in the sense of Donald Schön (1983); this is nothing uncommon for generative design, though it is for optimization. This type of conversation is between the designer (or design team) and the computer, and is digitally enhanced. As such, the aim is less on optimization per se and more on exploration: the results from optimization are about changing one’s way of thinking more than choosing a single design and then realizing it. We can then ask the question of how these types of conversation can unfold. How do they start and where do they end? What to do with thousands of similar solutions? The 11 papers in this Special Issue all address generative and evolutionary design exploration and contribute to the discussion of the interaction between design exploration and evolutionary design optimization. Julian Eichhoff and Dieter Roller start off with “A Survey on Automating Configuration and Parameterization in Evolutionary Design Exploration.” Focusing specifically on engineering design, they comprehensively review evolutionary design optimization approaches based on genetic algorithms (and genetic programming) addressing different design phases. Reformulating design problems as multiple-objective design optimization problems commonly reduces to parameterization, defining constants, and configuration, defining design variables, objective functions, and constraint functions. Methods from the fields of machine learning and natural language processing are reviewed to support these parameterization and configuration processes. Herm Hofmeyer and Juan Manuel Davila Delgado, in “Coevolutionary and Genetic Algorithm Based Building Spatial and Structural Design,” compare the use of a genetic algorithm with a coevolutionary method to collaboratively develop and optimize building spatial and structural designs. The genetic algorithm uses a finite element analysis method for evaluation of design alternatives. The coevolutionary method applies deterministic procedures to cyclically evaluate and improve the structural design via a finite element method and topology optimization, and adjust the spatial design according to the improved structural design and the initial spatial requirements. Both methods provide optimized building designs; however, the coevolutionary method yields Reprint requests to: Rudi Stouffs, Department of Building Technology, Faculty of Architecture, Delft University of Technology Postbus 5043, 2600 GA Delft, The Netherlands E-mail: r.m.f.stouffs@tudelft.nl Artificial Intelligence for Engineering Design, Analysis and Manufacturing (2015), 29, 329–331. # Cambridge University Press 2015 0890-0604/15 doi:10.1017/S0890060415000360	artificial intelligence;continuous design;engineering design process;evolutionary algorithm;expert system;feasible region;finite element method;genetic algorithm;genetic programming;julian day;machine learning;mathematical optimization;multidisciplinary design optimization;natural language processing;optimization problem;pareto efficiency;problem solving;program optimization;requirement;software release life cycle;topology optimization	Rudi Stouffs;Yaqub Rafiq	2015	AI EDAM	10.1017/S0890060415000360	engineering;generative grammar;systems engineering	EDA	32.842048899839305	0.32036737124710324	9806
8e4db0d6dddea4f4ea3d5f68b44eed2b6c30dd5e	a new stochastic search algorithm bundled honeybee mating for solving optimization problems	multi objective;stochastic search technique;eld;pareto dominance;vehbmo	This paper presents a new stochastic search technique to solve optimization problems. The new stochastic search of parallel vector evaluated honeybee mating optimization (VEHBMO) technique mimics the honeybee’s mating. The effectiveness of the proposed technique is compared with other stochastic optimization methods through standard benchmark functions. Also, the proposed VEHBMO is applied over real engineering problems of economic load dispatch and environmental/economic power dispatch problems. Obtained results confirm the validity of the proposed stochastic search technique.	benchmark (computing);dynamic dispatch;mathematical optimization;optimization problem;search algorithm;stochastic optimization	Oveis Abedinia;Morteza Dadash Naslian;Masoud Bekravi	2014	Neural Computing and Applications	10.1007/s00521-014-1682-1	mathematical optimization;stochastic optimization;machine learning;mathematics	AI	25.02048953766222	-3.872443577722802	9815
53b2ee0e1620635055f5b3cce15ec6c7daf1ec35	rk-eda: a novel random key based estimation of distribution algorithm	conference publications;cooling scheme;univariate model;estimation of distribution algorithm;random key;permutation problems	The challenges of solving problems naturally represented as permutations by Estimation of Distribution Algorithms (EDAs) have been a recent focus of interest in the evolutionary computation community. One of the most common alternative representations for permutation based problems is the Random Key (RK), which enables the use of continuous approaches for this problem domain. However, the use of RK in EDAs have not produced competitive results to date and more recent research on permutation based EDAs have focused on creating superior algorithms with specially adapted representations. In this paper, we present RK-EDA, a novel RK based EDA which uses a cooling scheme to balance the exploration and exploitation of a search space by controlling the variance in its probabilistic model. Unlike the general performance of RK based EDAs, RK-EDA is actually competitive with the best EDAs on common permutation test problems: Flow Shop Scheduling, Linear Ordering, Quadratic Assignment, and Travelling Salesman Problems.	computer cooling;estimation of distribution algorithm;evolutionary computation;flow shop scheduling;linux/rk;problem domain;resampling (statistics);statistical model;travelling salesman problem	Mayowa Ayodele;John A. W. McCall;Olivier Regnier-Coudert	2016		10.1007/978-3-319-45823-6_79	mathematical optimization;combinatorics;random permutation;estimation of distribution algorithm;computer science;artificial intelligence;random function;stochastic simulation;mathematics;statistics	Theory	28.546938994097598	-10.94789858726651	9822
85e4db03848a3dd5628000b0419fe41f25de7551	a scheme of wavelet based compression of 2d image			wavelet	Kamrul Hasan Talukder;Koichi Harada	2006			wavelet;computer vision;wavelet transform;mathematical optimization;image compression;stationary wavelet transform;second-generation wavelet transform;computer science;wavelet packet decomposition;discrete wavelet transform;lifting scheme;artificial intelligence	Robotics	42.480905333687666	-15.340917293647522	9827
303e1b8ae412c1d472ec040c44dc2870e484874b	an efficient multiple protein structure comparison method and its application to structure clustering and outlier detection	probability;measurement;molecular configurations;differential geometry;statistical analysis differential geometry molecular configurations probability proteins;proteins;shape;statistical analysis;heuristic algorithms;mathematical model;probability model multiple protein structure comparison method structure clustering mathematical framework statistical framework multiple global structure comparison formal geodesic distance clustering algorithm robust outlier detection algorithm;protein engineering;proteins shape protein engineering bioinformatics measurement mathematical model heuristic algorithms;bioinformatics	Despite many years of research, comparing multiple protein structures simultaneously (multiple structure comparison) is still a challenging problem. Most of the previous studies have focused on similarities among subsets of residues (or atoms) from a group of proteins (local structure alignment) by minimizing some similarity scores based on root mean square deviation (RMSD) of the aligned residues. In this paper, we designed a novel mathematical and statistical framework for multiple global structure comparison (MGSC). Under this framework, a formal geodesic distance is defined for any pair of protein structures and a mean structure can be estimated for a group of protein structures. The multiple structure comparison is then conveniently performed by comparing each individual structure with the mean structure. The formal distance facilitates consistent and accurate clustering of protein structures. An efficient clustering algorithm was designed based on the developed method. Probability models can be built for groups of protein structures and used in hypothesis testing. A robust outlier detection algorithm was designed to illustrate the potential applications of the framework.	algorithm;anomaly detection;cluster analysis;distance (graph theory);downstream (software development);mean squared error;statistical classification	Wei Wu;Anuj Srivastava;Jose Laborde;Jinfeng Zhang	2013	2013 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2013.6732463	differential geometry;combinatorics;global distance test;shape;bioinformatics;machine learning;probability;mathematical model;data mining;mathematics;protein engineering;measurement;statistics	Vision	5.9693912682385	-49.20546587965661	9844
aca87eaf0c449e408f88a0098f96224a966202c4	bayesian optimal single arrays for robust parameter design	bayes estimation;experimental design;site web;quality assurance;design of experiments process management continuous improvement;internal noise;hierarchy;design of experiments doe algorithm noise quality improvement qi variation parameter design bayesian methods;sintesis control;bayesian approach;hierarchized structure;methode taguchi;quality improvement;plan experiencia;grupo de excelencia;structure hierarchisee;crossed product;aseguracion calidad;estimacion bayes;plan experience;exchange algorithms;ciencias basicas y experimentales;synthese commande;robustesse;matematicas;jerarquia;robust parameter design;variation reduction;robustness;sitio web;hierarchie;assurance qualite;metodo taguchi;taguchi method;estructura jerarquizada;control synthesis;web site;estimation bayes;robustez;design of experiment	It is critical to estimate control-by-noise interactions in robust parameter design. This can be achieved by using a cross array, which is a cross product of a design for control factors and another design for noise factors. However, the total run size of such arrays can be prohibitively large. To reduce the run size, single arrays are proposed in the literature, where a modified effect hierarchy principle is used for the optimal selection of the arrays. In this article, we argue that effect hierarchy is a property of the system and cannot be altered depending on the objective of an experiment. We propose a Bayesian approach to develop single arrays which incorporate the importance of control-by-noise interactions without altering the effect hierarchy. The approach is very general and places no restrictions on the number of runs or levels or type of factors or type of designs. A modified exchange algorithm is proposed for finding the optimal single arrays. We also explain how to design experiments with internal noise factors. The advantages of the proposed approach are illustrated using several examples.	experiment;greedy algorithm;interaction;mass effect trilogy	Lulu Kang;V. Roshan Joseph	2009	Technometrics	10.1198/tech.2009.08057	quality assurance;mathematics;design of experiments;statistics	SE	34.33453246350103	-24.53941527560239	9851
5fd3c3c36f0c764580f7b44f44125459d9a8d940	a methodological proposal to eliminate ambiguities in the comparison of vehicle routing problem solving techniques		In the field of vehicle routing problems it is very common to use benchmarks (sets of problem instances) to evaluate new solving techniques or algorithms. The purpose of these benchmarks is to compare the techniques based on the results or solutions obtained. Typically, the benchmarks include the values of optimal solutions (if they have been obtained) or values of the best known solutions. In many cases, details of how these results were obtained are not described. This may generate controversy and difficults the comparisons of techniques. This paper shows an example of ambiguity in the results of an instance of the most used VRPTW (Vehicle Routing Problem with Time Windows) bechmark. We show that when analyzing the optimal solution and the best approximate solution of a specific problem, the two results are equivalent. Finally, we will propose a set of guidelines to consider when publishing the results obtained by a	approximation algorithm;benchmark (computing);microsoft windows;optimization problem;problem solving;vehicle routing problem	Eneko Osaba;Roberto Carballedo	2012			artificial intelligence;management science;algorithm	EDA	22.239014209883376	0.7941811366094715	9867
d27dad23ff990ce148e24e192b7e767684151161	data preprocessing in data based process modeling		Important process variables which give information about the final product quality cannot often be measured by a sensor. The alternative procedure is estimation of these difficult-to-measure process variables for which it is necessary to have an appropriate process model. Process model building is based on plant data, taken from the process database. Since the quality of the built model depends heavily on the modeling data informativity, a preparatory part of modeling, in which analysis and preprocessing of available measured data are performed, is a very important step in such process modeling. The analysis and preprocessing of plant data obtained from an oil distillation process are showed in the paper. The results show that, apart from the regression method applied, selection of easy-to-measure variables which will be used in the model building and filtering of easy-to-measure variables significantly affects process model prediction capabilities.	data pre-processing;estimation theory;finite impulse response;homeomorphism (graph theory);information;latent variable;nonlinear system;online and offline;preprocessor;process modeling;sensor;smoothing;wavelet	Drazen Sliskovic;Ratko Grbic;Emmanuel Karlo Nyarko	2009		10.3182/20090921-3-TR-3005.00096	computer science;data science;machine learning;data mining	AI	23.4228486837492	-23.663896171686805	9870
e432a3a7ed2d572064b75aca54cd0a17d5ddf66c	multi-view subspace clustering with intactness-aware similarity		Abstract Multi-view subspace clustering, which aims to partition a set of multi-source data into their underlying groups, has recently attracted intensive attention from the communities of pattern recognition and data mining. This paper proposes a novel multi-view subspace clustering model that attempts to form an informative intactness-aware similarity based on the intact space learning technique. More specifically, we learn an intact space by integrating encoded complementary information. An informative similarity matrix is simultaneously constructed, which enforces the constructed similarity to have maximum dependence with its latent intact points by adopting the Hilbert–Schmidt Independence Criterion (HSIC). A new explanation on the advantages of such intactness-aware similarity has been provided ( i.e. , the similarity is learned according to the local connectivity). To effectively and efficiently seek the optimal solution of the associated problem, a new ADMM based algorithm is designed. Moreover, to show the merit of the proposed joint optimization, we also conduct the clustering in two separated steps. Extensive experimental results on six benchmark datasets are provided to reveal the effectiveness of the proposed algorithm and its superior performance over other state-of-the-art alternatives.		Xiaobo Wang;Zhen Lei;Xiaojie Guo;Changqing Zhang;Hailin Shi;Stan Z. Li	2019	Pattern Recognition	10.1016/j.patcog.2018.09.009	machine learning;partition (number theory);cluster analysis;subspace topology;artificial intelligence;mathematics;pattern recognition;similarity matrix	Vision	24.11059193922606	-43.26208665851377	9873
03eca18d9a73a5540771934be0192282c71ec1cb	joint replacement optimization for multi-part maintenance problems	aerospace industry optimisation maintenance engineering markov processes;optimisation;optimal method;maintenance cost;maintenance engineering;aerospace industry;optimal policy;satisfiability;stochastic system;upper bound;average cost;costs jet engines sun intelligent systems intelligent networks automation computer networks storms silver explosions;markov processes;development time;necessary optimality condition;jet engine maintenance problem multi part maintenance problem joint replacement optimization long run average cost optimal joint replacement policy stochastic system dynamics markov decision approach two stage analysis method one stage analysis method;replacement policy	A model of multi-part asset with dependent maintenance cost is presented. The problem is to minimize the long-run average cost per time unit. To share some costs, a good policy may jointly replace multiple parts when an asset is maintained. However, it is difficult to obtain an optimal joint replacement policy in view of combinatorial explosion of the states and stochastic system dynamics. To obtain optimal policies for small problems, a novel method is built by recent developed time aggregation Markov decision approach, which leads to analytical and computational simplifications as compared with traditional Markov decision approaches. One-stage and two-stage analysis methods are developed for large problems. The upper bound of one-stage analysis method for single part problems is obtained to show the insight that it can achieve near or true optimal policy. For multi-part problems, they are proved to satisfy certain necessary optimality conditions. These conditions can significantly simplify their implementation. Numerical results show that they are more efficient and effective than other near optimal methods.	bellman equation;karush–kuhn–tucker conditions;markov chain;mathematical optimization;numerical method;optimal design;stochastic process;system dynamics	Tao Sun;Qianchuan Zhao;Peter B. Luh;Robert N. Tomastik	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389564	maintenance engineering;mathematical optimization;simulation;engineering;aerospace;markov process;upper and lower bounds;statistics;satisfiability	Robotics	6.136195356864061	-0.10096487362516966	9880
41773335447e7fd3b8ab16e5ea8218f09f8c5302	automatic mapping of gaze position coordinates of eye-tracking glasses video on a common static reference image		This paper describes a method for automatic semantic gaze mapping from video obtained by eye-tracking glasses to a common reference image. Image feature detection and description algorithms are utilized to find the position of subsequent video frames and map corresponding gaze coordinates on a common reference image. This process allows aggregate experiment results for further experiment analysis and provides an alternative for manual semantic gaze mapping methods.	eye tracking	Adam Bykowski;Szymon Kupinski	2018		10.1145/3204493.3208331	gaze;eye tracking;computer vision;feature detection;computer science;artificial intelligence	Vision	49.245853285633785	-45.72245278158668	9885
a5321483af9b776d11aa8964584803f8837ac173	embed and project: discrete sampling with universal hashing		We consider the problem of sampling from a probability distr ibution defined over a high-dimensional discrete set, specified for instance by a graphical model. We propose a sampling algorithm, called PAWS, based on embeddin g the set into a higher-dimensional space which is then randomly projecte d using universal hash functions to a lower-dimensional subspace and explore d using combinatorial search methods. Our scheme can leverage fast combinatorial opt mization tools as a blackbox and, unlike MCMC methods, samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution. We demonstrate that by using state-of-the-art combinatori al search tools, PAWS can efficiently sample from Ising grids with strong interact ions and from software verification instances, while MCMC and variational methods fail in both cases.	algorithm;black box;calculus of variations;combinatorial search;experiment;graphical model;hash function;ising model;local search (optimization);markov chain monte carlo;mathematical optimization;programming paradigm;pseudo-random number sampling;randomness;sampling (signal processing);software verification;time complexity;universal hashing;weight function;with high probability	Stefano Ermon;Carla P. Gomes;Ashish Sabharwal;Bart Selman	2013			combinatorics;discrete mathematics;theoretical computer science;machine learning;mathematics;statistics	ML	26.70157484132992	-28.54236906000365	9886
933ac977bd50cd18ee7bebdb7cadfeac4cf4eb8b	price discount based on early order commitment in a single manufacturer-multiple retailer supply chain	optimal solution;metodo polinomial;decentralization;zero one programming;logistique;temps polynomial;adquisicion por suscripcion;compra;modelo autorregresivo;fournisseur;price discount;preparacion pedido;gaming;programmation zero un;supplier;decentralisation;contrato;operations research;lead time;delai livraison;autoregressive model;detaillant;decentralized system;purchasing;polynomial time algorithm;programmacion cero uno;sistema coordenadas;economic order quantity;analisis regresion;logistics;contract;polynomial method;rebajas;polynomial time;plazo entrega;coste;sistema descentralizado;supply chain management early order commitment 0 1 programming supply chain coordination wholesale price discount contract;analyse regression;achat;coordinacion;supply chain;regression analysis;quantite economique a commander;0 1 programming;systeme decentralise;preparation commande;contrat;cantidad economica pedida;systeme coordonnee;wholesale price discount contract;discount;methode polynomiale;order picking;modele autoregressif;rabais;supply chain coordination early order commitment price discount gaming;retailers;early order commitment;supply chain coordination;delivery lead time;supply chain management;purchases;coordination;coordinate system;proveedor;logistica;cout;acquisition titre onereux;descentralizacion;tiempo polinomial	Early order commitment (EOC) is a strategy for supply chain coordination, wherein the retailer commits to purchasing from a manufacturer a fixed order quantity a few periods in advance of the regular delivery lead time. In this paper, we formulate and analyze the EOC strategy for a decentralized, two-level supply chain consisting of a single manufacturer and multiple retailers, who face external demands that follow an autocorrelated AR(1) process over time. We characterize the special structure of the optimal solutions for the retailers’ EOC periods to minimize the total supply chain cost and discuss the impact of demand parameters and cost parameters. We then develop and compare three solution approaches to solving the optimal solution. Using this optimal cost as the benchmark, we investigate the effectiveness of using the wholesale price-discount scheme for the manufacturer to coordinate this decentralized system. We give numerical examples to show the benefits of EOC to the whole supply chain, examine the efficiency of the discount scheme in general situation, and provide the special conditions when the full coordination is achieved. 2009 Elsevier B.V. All rights reserved.	analysis of algorithms;autocorrelation;autoregressive model;benchmark (computing);decentralised system;experiment;logistics;numerical analysis;purchasing;simulation	Jinxing Xie;Deming Zhou;Jerry C. Wei;Xiande Zhao	2010	European Journal of Operational Research	10.1016/j.ejor.2008.12.027	supply chain management;economics;marketing;operations management;mathematics;decentralization;commerce	AI	0.20816832984507253	-5.091795473452152	9888
7007373cdac029a224e4a69e6a4f75a0f6e489f2	the cooperative estimation of distribution algorithm: a novel approach for semiconductor final test scheduling problems	cooperative estimation of distribution algorithm;manufacturing management;flexible manufacturing systems;semiconductor final test scheduling problems	During the past several years, there have been a significant number of researches conducted in the area of semiconductor final test scheduling problems (SFTSP). As specific example of simultaneous multiple resources scheduling problem (SMRSP), intelligent manufacturing planning and scheduling based on meta-heuristic methods, such as Genetic Algorithm (GA), Simulated Annealing (SA), and Particle Swarm Optimization (PSO), have become the common tools for finding satisfactory solutions within reasonable computational times in real settings. However, limited researches were aiming at analyze the effects of interdependent relations during group decision-making activities. Moreover for complex and large problems, local constraints and objectives from each managerial entity, and their contributions towards the global objectives cannot be effectively represented in a single model. In this paper, we propose a novel Cooperative Estimation of Distribution Algorithm (CEDA) to overcome the challenges mentioned before. The CEDA is established based on divide-and-conquer strategy and a co-evolutionary framework. Considerable experiments have been conducted and the results confirmed that CEDA outperforms recent research results for scheduling problems in FMS (Flexible Manufacturing Systems).	automated planning and scheduling;computation;decision support system;disk partitioning;ecosystem;estimation of distribution algorithm;evolutionary algorithm;experiment;genetic algorithm;heuristic;interdependence;metaheuristic;particle swarm optimization;real-time transcription;scheduling (computing);semiconductor;simulated annealing	Xin-Chang Hao;Jei-Zheng Wu;Chen-Fu Chien;Mitsuo Gen	2014	J. Intelligent Manufacturing	10.1007/s10845-013-0746-x	fair-share scheduling;mathematical optimization;simulation;engineering	AI	19.22288144254368	-1.171097197346027	9891
05765e0dcb751e98d33d7b512c1eb5aa33953741	using genetic algorithms to schedule flow shop releases	genetic algorithm		genetic algorithm	Gary A. Cleveland;Stephen F. Smith	1989			flow shop scheduling	Theory	19.575367203119217	-0.7049783613014492	9899
cb0d3bb0c806840eef4b00d93bb30a08d7520167	robust vehicle tracking and detection from uavs	vehicle detection;forward and backward tracking fbt;unmanned aerial vehicle uav;vehicle tracking;learning tracking detection	Unmanned Aerial Vehicles have been used widely in the commercial and surveillance use in the recent year. Vehicle tracking from aerial video is one of commonly used application. In this paper, a self-learning mechanism has been proposed for the vehicle tracking in real time. The main contribution of this paper is that the proposed system can automatic detect and track multiple vehicles with a self-learning process leading to enhance the tracking and detection accuracy. Two detection methods have been used for the detection. The Features from Accelerated Segment Test (FAST) with Histograms of Oriented Gradient (HoG) method and the HSV colour feature with Grey Level Cooccurrence Matrix (GLCM) method have been proposed for the vehicle detection. A Forward and Backward Tracking (FBT) mechanism has been employed for the vehicle tracking. The main purpose of this research is to increase the vehicle detection accuracy by using the tracking results and the learning process, which can monitor the detection and tracking performance by using their outputs. Videos captured from UAVs have been used to evaluate the performance of the proposed method. According to the results, the proposed learning system can increase the detection performance.	aerial photography;aerial video;features from accelerated segment test;gradient;unmanned aerial vehicle;vehicle tracking system	Xiyan Chen;Qinggang Meng	2015	2015 7th International Conference of Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2015.7492814	computer vision;simulation;tracking system;vehicle tracking system;computer science	Robotics	41.536748771857944	-45.06571413814536	9910
b3cf22c0ff14a27bdbf124361e5f1ed974582213	incremental learning of subtasks from unsegmented demonstration	robot learning;robot sensing systems;learning;gaussian processes;mixture of experts;data models approximation methods gaussian processes robot sensing systems equations learning;mobile robots;finite state machines;incremental learning;multi robot systems;goal scorer incremental learning unsegmented demonstration robot learning finite state machine robot soccer;approximation methods;robot soccer;sport;learning artificial intelligence;sport finite state machines learning artificial intelligence mobile robots multi robot systems;unsegmented demonstration;finite state machine;goal scorer;data models	We propose to incrementally learn the segmentation of a demonstrated task into subtasks and the individual subtask policies themselves simultaneously. Previous robot learning from demonstration techniques have either learned the individual subtasks in isolation, combined known subtasks, or used knowledge of the overall task structure to perform segmentation. Our infinite mixture of experts approach instead automatically infers an appropriate partitioning (number of subtasks and assignment of data points to each one) directly from the data. We illustrate the applicability of our technique by learning a suitable set of subtasks from the demonstration of a finite-state machine robot soccer goal scorer.	algorithm;aliasing;data point;dynamic problem (algorithms);finite-state machine;gaussian process;ibm notes;multimap;programming paradigm;reinforcement learning;robot learning;sparse matrix	Daniel H. Grollman;Odest Chadwicke Jenkins	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650500	robot learning;simulation;computer science;artificial intelligence;sport;machine learning;finite-state machine	Robotics	48.987039595530035	-25.969003468818897	9916
62733bc711329c48887dcf9aad40121a2c30ab2c	integrated supply chain and product family architecture under highly customized demand	mass customization;product family architecture;supply chain configuration;generic bill of products	Mass customization efforts are challenged by an unpredictable growth or shrink in the market segments and shortened product life cycles which result in an opportunity loss and reduced profitability; hence we propose a concept of sustainable mass customization to address these challenges where an economically infeasible product for a market segment is replaced by an alternative superior product variant nearly at the cost of mass production. This concept provides sufficient time to restructure the product family architecture for the inclusion of a new innovative product variant while fulfilling the market segments with the customer delight and an extended profitability. To implement the concept of sustainable mass customization we have proposed the notions of generic-bill-Of-products (GBOP: list of product variants agreed for the market segments), its interface with generic-supply-chain-structure and strategic decisions about opening or closing of a market segment as an optimization MILP (mixed integer linear program) model including logistics and GBOP constraints. Model is tested with the varying market segments demands, sales prices and production costs against 1 to 40 market segments. Simulation results provide us an optimum GBOP, its respective segments and decisions on the opening or closing of the market segments to sustain mass customization efforts.		Kashif M. Shahzad;Khaled Hadj-Hamou	2013	J. Intelligent Manufacturing	10.1007/s10845-012-0630-0	mass customization;engineering;marketing;new product development	Robotics	1.8996753596559248	-7.408479986389178	9929
aec7101d4c572e7274ef3676cd3c1fca92070965	a stochastic pca and svd algorithm with an exponential convergence rate		We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variancereduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis.	algorithm;convex optimization;gradient descent;iteration;mathematical optimization;principal component analysis;singular value decomposition	Ohad Shamir	2015			mathematical optimization;machine learning;mathematics;statistics	ML	24.344167632979158	-33.942237296737424	9944
43d91f9b2ac367cd42f4219cce14d6b26136ecdb	control and pricing in stochastic networks with concurrent resource occupancy	stochastic networks;service system;resource control;average case analysis;fixed point;optimization problem;exclusion process;linear program;self organizing list;bill of material;telecommunication networks	Concurrent resource occupancy pervades most engineering and service systems. For example, a multi-leg plane trip requires seat reservation on several connecting flights; a configure-to-order product demands the simultaneous processing of all its components; a file transfer on the Internet needs band-width on all the links along its route from source to destination. The object of our study is a network with stochastic concurrent occupancy of resources. The network can be physical (e.g., a telecommunication network), or virtual (e.g., the Worldwide Web), or relational (e.g., the bill of materials of a product, representing its configuration of all components); and both the demand/order arrivals and their processing times required of the resources are stochastic. Our goal is to do revenue optimization in the network through two decisions: (a) <i>pricing:</i> to determine the price charged to each job class and its dynamic adjustment over time; and (b) <i>resource control:</i> to regulate the distribution of resources among the job classes, in particular, when to accept/reject a job and from which class.  Below, we highlight a new <i>fixed-point approximation</i> for a network operating under a set of thresholds that control the access of jobs from each class. With this fixed-point approximation, the resource control problem takes the form of setting the optimal thresholds, which can be formulated and solved as a linear program. To determine the optimal prices then amounts to solving another set of optimality equations on top of the linear program. Furthermore, we can show that our approach via solving optimization problems based on the fixed-point approximation is optimal in some asymptotic sense.	approximation algorithm;emoticon;file transfer;fixed point (mathematics);fixed-point arithmetic;internet;job stream;linear programming;mathematical optimization;telecommunications network;world wide web	Xuan Li;David D. Yao	2004	SIGMETRICS Performance Evaluation Review	10.1145/1035334.1035354	optimization problem;mathematical optimization;simulation;computer science;linear programming;self-organizing list;fixed point;computer network;service system	Metrics	4.018393864899327	1.8909344152630447	9957
5ada39ac5928b94912b537b16a3db5263620ca30	incomplete-case nearest neighbor imputation in software measurement data	incomplete case;nearest neighbor imputation;software measurement data;complete case	Missing values are commonly encountered in software measurement data, and k nearest neighbor imputation (kNNI) is one of the most popular imputation procedures used by researchers and practitioners in empirical software engineering. Imputation techniques are used to replace missing values with one or more alternatives. Traditionally, kNNI uses only complete cases as possible donors for imputation (called complete case kNNI or CCkNNI), however a variant of CCkNNI called incomplete case k nearest neighbor imputation (ICkNNI) is an attractive alternative which has received very little attention. We present a detailed comparative study of CCkNNI and ICkNNI with missing software measurement data, and demonstrate that using incomplete cases often increases the effectiveness of nearest neighbor imputation (especially at higher missingness levels), regardless of the type of missingness.	experimental software engineering;geo-imputation;k-nearest neighbors algorithm;missing data;software measurement	Jason Van Hulse;Taghi M. Khoshgoftaar	2007	2007 IEEE International Conference on Information Reuse and Integration	10.1016/j.ins.2010.12.017	pattern recognition;data mining;imputation;statistics	SE	5.7175388690691396	-43.32685959799114	9962
dd52a32389817e92734126a465ff6c9216dfc43f	new aspect ratio invariant visual secret sharing schemes using square block-wise operation	analisis imagen;partage secret;ombre;information loss;secret sharing;imagen fija;rapport aspect;visual secret sharing;sombra;fixed image;relacion dimensional;shadow;perdida informacion;pattern recognition;invariante;image fixe;image analysis;reconnaissance forme;reconocimiento patron;analyse image;perte information;reparto secreto;invariant;aspect ratio	An aspect ratio invariant visual secret sharing (ARIVSS) scheme is a perfectly secure method for sharing secret images, by expanding a secret pixel to m sub pixels in shadow images, with m being the pixel expansion; meantime the aspect ratio of the recovered secret image is fixed. The advantage of ARIVSS is that there is no loss of information when the shape of the secret image is our information; for example, a secret image of a circle is compromised to an ellipse, if m does not have a square value. Two ARIVSS schemes, based on processing one and four pixel blocks, respectively, were previously proposed. In this paper, we have generalized the square block-wise approach, to further reduce pixel expansion.	block cipher mode of operation;secret sharing	Ching-Nung Yang;Tse-Shih Chen	2005		10.1007/11559573_141	computer vision;shadow;aspect ratio;image analysis;computer science;theoretical computer science;invariant;shamir's secret sharing;mathematics;homomorphic secret sharing;secret sharing;computer security	Vision	39.93969432785105	-9.605282153468202	9976
c36d698b66f997619c3c7f5d1ffa2c70fa203d03	evaluating the progression and orientation of scratches on outer-raceway bearing using a pattern recognition method		Bearing faults are a major source of failure in an induction motor, and early detection of fault becomes necessary because of its industrial application. A range of analytical methods has been used to detect, identify, and diagnose bearing faults, including vibrational analysis. Most analyses have used pitting as the fault, whereas in the industrial environment, scratches are a more common problem. This paper investigates such scratches, applying two types of fault analysis: fault progression and fault orientation. A support vector machine (SVM) algorithm is used to classify and diagnose the different types of bearing fault. The frequency-domain features obtained from a fast Fourier transform of the load current is used to train the SVM algorithm. The proposed diagnostic method is tested experimentally using induced outer race faults under different load conditions. The method is shown to be successful in diagnosing faults, suggesting potential applications in real industrial settings.	algorithm;color gradient;experiment;fast fourier transform;mathematical induction;pattern recognition;race condition;support vector machine	Shrinathan Esakimuthu Pandarakone;Yukio Mizuno;Hisahide Nakamura	2019	IEEE Transactions on Industrial Electronics	10.1109/TIE.2018.2833025	support vector machine;fast fourier transform;engineering;control engineering;induction motor;pattern recognition;raceway;artificial intelligence;bearing (mechanical)	Robotics	37.28162539079859	-30.721801028134873	9978
32359283bb2b08d1b5fbd066ef506e94af6564ae	gender recognition using a min-max modular support vector machine with equal clustering	cluster algorithm;analyse amas;analisis estadistico;identificacion sexual;metodo minimax;minimax method;sexing;classification a vaste marge;cluster analysis;statistical analysis;identification sexe;clustering method;analyse statistique;methode minimax;pattern classification;analisis cluster;support vector machine;maquina ejemplo soporte;vector support machine;reseau neuronal;red neuronal;neural network;classification forme	Considering the fast respond and high generalization accuracy of the min-max modular support vector machine (M-SVM), we apply M-SVM to solving the gender recognition problem and propose a novel task decomposition method in this paper. Firstly, we extract features from the face images by using a facial point detection and Gabor wavelet transform method. Then we divide the training data set into several subsets with the ‘part-versus-part’ task decomposition method. The most important advantage of the proposed task decomposition method over existing random method is that the explicit prior knowledge about ages contained in the face images is used in task decomposition. We perform simulations on a real-world gender data set and compare the performance of the traditional SVMs and that of M-SVM with the proposed task decomposition method. The experimental results indicate that MSVM with our new method have better performance than traditional SVMs and M-SVM with random task decomposition method.	computer cluster;gabor wavelet;maxima and minima;simulation;support vector machine;test set;wavelet transform	Jun Luo;Bao-Liang Lu	2006		10.1007/11760023_31	correlation clustering;support vector machine;fuzzy clustering;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;cluster analysis;artificial neural network	ML	10.646406002758505	-34.24520288824311	9980
fa6630024b192651accf490145a12ff256ea90ff	development of telemetry data processing program	telemetry data processing synchronization missiles frequency modulation surface treatment;remote fast moving target telemetry data processing program development control subsystem tracking subsystem remote target telemetering telemetry subsystem telemetry data frame architecture telemetry data frame processing graphical user interfaced telemetry data processing program surface to air guided missile flight test data;supercommutation tt c telemetry irig standard distributed data driven bus commutation decommutation;data communication;graphical user interfaces;telemetry data communication graphical user interfaces;telemetry	Telemetry, tracking, and control subsystem has been typically applied to acquire data from a remote measurand, and the information is used to control the remote target precisely. According to the purpose of telemetering and telemetry applications, the composition of telemetry subsystem varies. In addition, the telemetry data frame architecture and processing schemes are different in accordance with the characteristics of remote signal sources. Telemetry data formats and the data processing schemes are investigated in this research, and the data processing program is implemented. The developed graphical user interfaced telemetry data processing program is applied to process the surface-to-air guided missile flight test data, and it shows easy way of acquiring information and analyzing data from a remote fast-moving target.	frame (networking);graphical user interface;test data	In Jong Kim;Sungpil Lee	2013	2013 International Conference on ICT Convergence (ICTC)	10.1109/ICTC.2013.6675398	embedded system;electronic engineering;computer hardware;computer science;portable telemetry	Robotics	16.30707386463743	-11.989266339638444	10012
9b22a82f74a9db704bc13969dcdb1aaaf465fbcb	multimodality and the linkage-learning difficulty of additively separable functions	separable functions;multimodality;linkage learning difficulty	Estimation of Distribution Algorithms (EDAs) have emerged from the synergy between machine-learning techniques and Genetic Algorithms (GAs). EDAs rely on probabilistic modeling for obtaining information about the underlying structure of optimization problems and implementing effective reproduction operators. The effectiveness of EDAs depends on the capacity of the model-building to extract reliable information about the problem. In this study we analyze additively separable functions and argue that the degree of multimodality of such functions defines their linkage-learning difficulty. Besides, by using entropy-based concepts and Jensen's inequality, we show how allelic pairwise independence may appear as a consequence of an increasing multimodality. The results characterize the linkage-learning difficulty of well-known functions, like the deceptive trap, bipolar and concatenated parity.	concatenation;estimation of distribution algorithm;genetic algorithm;jensen's inequality;linkage (software);machine learning;mathematical optimization;social inequality;synergy;trap (computing)	Jean Paulo Martins;Alexandre C. B. Delbem	2014		10.1145/2576768.2598281	mathematical optimization;artificial intelligence;machine learning;mathematics	Theory	23.30631497824807	-8.215733651707618	10014
1a22283b3af9c198caa7549105b5da6a4318c7e4	an on-chip trainable and the clock-less spiking neural network with 1r memristive synapses		Spiking neural networks (SNNs) are being explored in an attempt to mimic brain's capability to learn and recognize at low power. Crossbar architecture with highly scalable resistive RAM or RRAM array serving as synaptic weights and neuronal drivers in the periphery is an attractive option for the SNN. Recognition (akin to “reading” the synaptic weight) requires small amplitude bias applied across the RRAM to minimize conductance change. Learning (akin to “writing” or updating the synaptic weight) requires large amplitude bias pulses to produce a conductance change. The contradictory bias amplitude requirement to perform reading and writing simultaneously and asynchronously, akin to biology, is a major challenge. Solutions suggested in the literature rely on time-division-multiplexing of read and write operations based on clocks, or approximations ignoring the reading when coincidental with writing. In this paper, we overcome this challenge and present a clock-less approach wherein reading and writing are performed in different frequency domains. This enables learning and recognition simultaneously on an SNN. We validate our scheme in SPICE circuit simulator by translating a two-layered feed-forward Iris classifying SNN to demonstrate software-equivalent performance. The system performance is not adversely affected by a voltage dependence of conductance in realistic RRAMs, despite departing from linearity. Overall, our approach enables direct implementation of biological SNN algorithms in hardware.	algorithm;approximation;biological neural networks;classification;conductance (graph);crossbar switch;electronic circuit simulation;hl7publishingsubsection <operations>;multiplexing;neural network simulation;ram 7s protein, rat;resistive random-access memory;spice;scalability;simulators;spiking neural network;synapse;synapses;synaptic weight;anatomical layer	Aditya Shukla;Udayan Ganguly	2018	IEEE Transactions on Biomedical Circuits and Systems	10.1109/TBCAS.2018.2831618	architecture;conductance;machine learning;computer science;electronic engineering;spiking neural network;crossbar switch;scalability;electronic circuit simulation;linear system;synaptic weight;artificial intelligence	ML	39.099936824010115	-0.9427819681206805	10039
fca6b0c4d890dd2be2e88fc3e152d27d675aaca8	experiments in the piece-wise linear approximation of ultrasonic echoes for object recognition in manipulation tasks	decision tree classifier;trees mathematics piecewise linear techniques feature extraction manipulators object recognition pattern classification ultrasonic transducers sonar decision theory;object recognition;manipulators;piece wise linear approximation;ultrasonic echoes;time of flight;piecewise linear techniques;object rotations;temperature sensors;mobile robots;testing;trees mathematics;ultrasonic transducers;work cell environment;piecewise linear techniques object recognition temperature sensors mobile robots mechanical variables measurement position measurement sonar measurements acoustic reflection educational institutions testing;piece wise linear;monostatic sonar;feature extraction;decision theory;manipulation tasks;acoustic reflection;position measurement;success rate;pattern classification;feature extraction piece wise linear approximation ultrasonic echoes object recognition manipulation tasks work cell environment decision tree classifiers monostatic sonar object rotations;decision tree classifiers;real time application;mechanical variables measurement;sonar measurements;sonar	This paper describes a novel method of object recognition based upon the piece-wise linear approximation of ultrasonic echoes which, when tested with examples typical of a work cell environment, achieves classification success rates of 92-98%. The approach uses decision tree classifiers constructed from simple features derived from the echoes of a monostatic sonar and taken from a number of view points. The results illustrate the effect upon the method's classification success of the use of a single view point, the inclusion of additional view points, and the possibility of object rotations, for 22 classes of objects. The processes of feature extraction and classification are accomplished within a time comparable with the time of flight of the pulse, and hence the method has potential for real time applications.	linear approximation;outline of object recognition	Ian P. W. Sillitoe;Antonio Visioli;Francesco Zanichelli;Stefano Caselli	1996		10.1109/ROBOT.1996.503802	mobile robot;computer vision;time of flight;speech recognition;decision theory;decision tree learning;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;software testing;sonar	Robotics	37.73110085559907	-34.57120411658672	10046
0f348056a828acd578450b4ac5e0ac48be32b01e	learning semantic place labels from occupancy grids using cnns	robot sensing systems;semantics;two dimensional displays;laser radar;context;labeling	The goal of this paper is to develop a robot with a grounded spatial vocabulary. Such a vocabulary would allow it to give and follow directions, and would give it valuable additional information in aiding localization and navigation. We approach the problem by defining an ontology of space (including corridor, doorway, and room) and by creating a Convolutional Neural Network (CNN) that allows the robot to classify LIDAR sensor data accordingly. In particular, we propose a CNN architecture that performs comparably or better than existing methods based on engineered features. Training CNNs can be fickle; we describe several specific aspects of our approach that are important for good performance in this task.	convolutional neural network;robot;vocabulary	Robert Goeddel;Edwin Olson	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759589	lidar;computer vision;labeling theory;computer science;artificial intelligence;semantics	Robotics	30.070795051575757	-48.655698279152226	10047
1ed427993360e9eaed6408f626487ccaa9290899	a particle swarm optimization using local stochastic search and enhancing diversity for continuous optimization	diversity;continuous optimization problems;local stochastic search;particle swarm optimization	The particle swarm optimizer (PSO) is a swarm intelligence based on heuristic optimization technique that can be applied to a wide range of problems. After analyzing the dynamics of traditional PSO, this paper presents a new PSO variant on the basis of local stochastic search strategy (LSSPSO) for performance improvement. This is encouraged by a social phenomenon that everyone wants to first exceed the nearest superior and then all superior. Specifically, LSSPSO employs a local stochastic search to adjust inertia weight in terms of keeping a balance between the diversity and the convergence speed, aiming to improve the performance of traditional PSO. Experiments conducted on unimodal and multimodal test functions demonstrate the effectiveness of LSSPSO in solving multiple benchmark problems as compared to several other PSO variants. & 2014 Elsevier B.V. All rights reserved.	algorithm;benchmark (computing);continuous optimization;distribution (mathematics);experiment;heuristic;loss function;mathematical optimization;multimodal interaction;optimization problem;particle swarm optimization;phase-shift oscillator;stochastic optimization;swarm intelligence;test case;test suite	Jianli Ding;Jin Liu;Kaushik R. Chowdhury;Wensheng Zhang;Qiping Hu;Jeff Lei	2014	Neurocomputing	10.1016/j.neucom.2013.03.075	mathematical optimization;multi-swarm optimization;computer science;artificial intelligence;stochastic optimization;machine learning;mathematics;particle swarm optimization;metaheuristic	AI	26.04593007903715	-3.9876717350391018	10061
5abebea959d6555b4405736a5481a415c9308d06	speech compaction using vector quantisation and hidden markov models	speech intelligibility;speech compaction vector quantization hidden markov models signal processing australia systems engineering and theory testing fatigue sampling methods;hidden markov model;speech intelligibility speech coding vector quantisation hidden markov models signal reconstruction;speech coding;information presentation;hidden markov models;mean opinion score;signal reconstruction;vector quantisation;vq vector quantisation hidden markov models time compaction speech compaction synchronised overlap add compaction hierarchical temporal decomposition mean opinion score testing hmm	We present techniques for the time compaction of speech using vector quantisation and hidden Markov modelling. These aim to retain the most perceptiially important information present in the speech signal, while discarding redundant information. The methods are compared with the conventional technique using synchronised overlap-add (SOLA) compaction, and with a recently proposed hierarchical temporal decomposition (HTD) based method. Using Mean Opinion Score testing, they are found to give a better quality output than the SOLA method, and similar quality to the HTD.	curve-fitting compaction;data compaction;hidden markov model;markov chain;overlap–add method;vector quantization	David R. Cole;Sridha Sridharan	1999		10.1109/ISSPA.1999.818216	mean opinion score;signal reconstruction;speech recognition;computer science;machine learning;speech coding;pattern recognition;markov model;intelligibility;hidden markov model	HCI	49.23087079578124	-9.543586987395104	10067
9910f2b057ec21e33d09c3633c428c21c667af97	an image reduction method using mr codes	compression	The use of office automation machines, such as facsimile machines and copiers, is widespread. Thus technologies which use and store hand-written documents and drawings (binary image data) electronically are important. Image reduction methods are among such technologies and generally use a method in which the pixels are thinned out. However, this causes deterioration in the image quality, for example, missing characters and fine lines, because of its uniform processing regardless of the content of the image. In this paper, a new reduction method using MR codes is proposed to prevent deterioration in the image quality. The MR code is used to encode the image data in a facsimile, electronic filing, or other systems, and includes two dimensional characteristics of the image. Therefore, the proposed method enables one to process flexibly according to the content of the image. Furthermore, the proposed method reduces computational time by directly processing the image into MR code. Compared with conventional processing methods, in terms of processing time, and subjective and objective evaluations, it is confirmed that the proposed method enables one to improve image quality while saving processing time. © 1997 Scripta Technica, Inc. Syst Comp Jpn, 28(7): 1–10, 1997	code	Isao Nakanishi;Yoshio Itoh;Yutaka Fukui	1997	Systems and Computers in Japan	10.1002/(SICI)1520-684X(19970630)28:7%3C1::AID-SCJ1%3E3.0.CO;2-K	data compression;image quality;time complexity;feature detection;document processing;binary image;telecommunications;image processing;computer science;digital image processing;coding;compression;algorithm;statistics	HPC	40.27739501704901	-14.311425551998257	10084
758404dbcdf933ec9ca49a785ef3b25f5eaaa9e8	low relative speed moving vehicle detection using motion vectors and generic line features	filtering;motion compensation;vehicle detection;ego motion compensation low relative speed moving vehicle detection motion vectors generic line features monocular moving camera h 264 avc encoder;vehicles cameras video coding vehicle detection vectors filtering motion compensation;video coding;vectors;video coding cameras motion compensation motion estimation object detection;vehicles;cameras	This paper presents a new approach to the detection of a vehicle with low relative speed to a monocular moving camera, for complementing moving vehicle detection using motion vectors from H.264/AVC encoder. This method makes use of the generic horizontal line features that exist on most vehicles as a clue of localizing moving vehicles. Further filtering and grouping of these detected lines followed by ego motion compensation can effectively detect moving vehicle with low relative speed for application in advanced driver assistance system. Our test results show a high detection rate of over 90%.	encoder;h.264/mpeg-4 avc;internationalization and localization;motion compensation	Chup-Chung Wong;Wan-Chi Siu;Stuart Barnes;Paul Jennings	2015	2015 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2015.7066384	filter;computer vision;match moving;simulation;quarter-pixel motion;computer science;motion estimation;motion compensation;algorithm;computer graphics (images)	Robotics	44.74234165431589	-44.79994162616185	10094
e1efab13f8a8afcac34431cd42a0d04fcb0556c0	studying the effect of measured solar power on evolutionary multi-objective prediction intervals		While it is common to make point forecasts for solar energy generation, estimating the forecast uncertainty has received less attention. In this article, prediction intervals are computed within a multi-objective approach in order to obtain an optimal coverage/width tradeoff. In particular, it is studied whether using measured power as an another input, additionally to the meteorological forecast variables, is able to improve the properties of prediction intervals for short time horizons (up to three hours). Results show that they tend to be narrower (i.e. less uncertain), and the ratio between coverage and width is larger. The method has shown to obtain intervals with better properties than baseline Quantile Regression.		Rubén Martín-Vázquez;J. Huertas-Tato;Ricardo Aler;Inés María Galván	2018		10.1007/978-3-030-03496-2_18	computer science;statistics;prediction interval;artificial intelligence;multi-objective optimization;quantile regression;pattern recognition;solar energy;solar power	NLP	9.225824049433497	-16.953738174180273	10095
27327f4dba911340a2e40c1a7f35edb67d221cfa	visual secret sharing by random grids revisited	desciframiento;image encryption;tabla codificacion;partage secret;image segmentation;image processing;secret sharing;encryption;visual secret sharing;procesamiento imagen;decryptage;cifrado;securite donnee;traitement image;visual cryptography;cryptage;codebook;criptografia;human visual system;cryptography;table codage;decryption;segmentation image;cryptographie;random grids;formal analysis;security of data;reparto secreto	Recently, the visual secret sharing (VSS) technique by random grids (RG), proposed by Kafri and Keren in 1987, has drawn attention in academia again to remove the abovementioned drawbacks. However, Kafri and Keren's scheme is a 2-out-of-2 VSS scheme but neither n-out-of-n nor 2-out-of-n(n>2). In this paper, novel n-out-of-n (Method 1) and 2-out-of-n (Method 2) secret image sharing schemes based on RG are proposed to encrypt the secret into n cipher-grids without pixel expansion and additional codebook required. In the decryption phase, while participants superimpose all (Method 1) or at least two (Method 2) cipher-grids without any extra computation, the secret is recognizable by the human visual system. To our best knowledge, this paper is the first attempt in the literature to develop new RG-based VSS schemes by means of extending the basic 2-out-of-2 scheme to the n-out-of-n as well as 2-out-of-n ones. To demonstrate the correctness of the proposed schemes, the formal analysis is given while the experimental results show the proposed schemes do work well.	secret sharing	Tzung-Her Chen;Kai-Hsiang Tsao	2009	Pattern Recognition	10.1016/j.patcog.2008.11.015	computer vision;image processing;computer science;cryptography;theoretical computer science;codebook;homomorphic secret sharing;image segmentation;internet privacy;secret sharing;computer security;verifiable secret sharing;encryption	Vision	39.19902961931664	-9.348176629008874	10102
9343abf6d64a0879adae29feb993ee0ef4152b2c	human action recognition using integrated model	video	A novel action recognition framework based on integrated model is proposed in the paper. First, the covariance descriptor is utilized to extract features from video sequences, and then each class specific codebook is constructed and appended to the global codebook. A static model applying the template matching technique and a dynamic model employing the trigram model are learned to capture complementary information in an action. And lastly, an integrated model is used to estimate the confidence of the static and dynamic models and produces a reliable result. Comparative experiments show that our presented method achieves superior results over other state-of-the-art approaches.	codebook;experiment;mathematical model;template matching;trigram	Yang Yi;Yikun Lin	2013		10.1117/12.2030544	simulation;computer science;artificial intelligence;machine learning	Vision	34.687290344186216	-49.55856489209101	10105
54686962f531514d9b583573b55022a31a702812	removing the feature correlation effect of multiplicative noise		Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks. From an information perspective, we consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways, which leads to the observation that multiplicative noise tends to increase the correlation between features, so as to increase the signal-to-noise ratio of information pathways. However, high feature correlation is undesirable, as it increases redundancy in representations. In this work, we propose non-correlating multiplicative noise (NCMN), which exploits batch normalization to remove the correlation effect in a simple yet effective way. We show that NCMN significantly improves the performance of standard multiplicative noise on image classification tasks, providing a better alternative to dropout for batch-normalized networks. Additionally, we present a unified view of NCMN and shake-shake regularization, which explains the performance gain of the latter.	artificial neural network;computer vision;deep learning;dropout (neural networks);gradient;multiplicative noise;redundancy (engineering);shake;signal-to-noise ratio	Zijun Zhang;Yining Zhang;Zongpeng Li	2018			artificial intelligence;machine learning;redundancy (engineering);artificial neural network;normalization (statistics);mathematics;correlation;contextual image classification;regularization (mathematics);multiplicative noise	ML	22.878823871172063	-46.528606087923144	10112
295bc82821cfde872a8c1baeec8594f3bdb94ae4	structured weak semantic space construction for visual categorization		Visual features have been widely used for image representation and categorization. However, visual features are often inconsistent with human perception. Besides, constructing explicit semantic space is still an open problem. To alleviate these two problems, in this paper, we propose to construct structured weak semantic space for image representation. Exemplar classifier is first trained to separate each training image from other images for weak semantic space construction. However, each exemplar classifier separates one training image from other images, and it only has limited semantic separability. Besides, the outputs of exemplar classifiers are inconsistent with each other. We jointly construct the weak semantic space using structured constraint. This is achieved by imposing low-rank constraint on the outputs of exemplar classifiers with sparsity constraint. An alternative optimization procedure is used to learn the exemplar classifiers. Since the proposed method does not dependent on the initial image representation strategy, we can make use of various visual features for efficient exemplar classifier training (e.g., fisher vector-based methods and convolutional neural networks-based methods). We apply the proposed structured weak semantic space-based image representation method for categorization. The experimental results on several public image data sets prove the effectiveness of the proposed method.	area striata structure;artificial neural network;categorization;class;convolutional neural network;discriminative model;information privacy;linear separability;low-rank approximation;mathematical optimization;neural network simulation;regular expression;sparse matrix	Chunjie Zhang;Jian Cheng;Qi Tian	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2728060	machine learning;convolutional neural network;artificial intelligence;open problem;visualization;categorization;object detection;pattern recognition;computer science;semantics;contextual image classification;classifier (linguistics)	Vision	24.732815714233325	-46.76217202631747	10116
b5d24ff8c496f0a07a143fccdd3300543dc7bfaa	modelling the effect of quantizing affine motion vectors on rate and energy of difference macroblocks	image motion analysis;rate optimization macroblocks energy affine motion vectors quantization advanced motion models video coding methods;video coding;image motion analysis vector quantisation video coding;motion vector;vector quantisation;motion analysis video coding vector quantization encoding decoding large scale integration logic optimization methods motion compensation motion estimation	This paper derives the relationship between the energy of difference macroblock and affine motion vector quantization step size. This is an important step in the analysis of advanced motion models for future video coding methods, as it facilitates rate optimization for affine motion vectors. The derived model shows that the difference macroblock energy has a squared relationship with the affine motion vector quantization step size. In addition, experimental results are shown validating this result and providing further insight.	data compression;kinetic data structure;macroblock;mathematical optimization;vector quantization	Roman C. Kordasiewicz;Shahram Shirani;Michael Gallant	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529881	computer vision;quarter-pixel motion;theoretical computer science;coding tree unit;motion estimation;control theory;mathematics;block-matching algorithm;motion compensation;vector quantization	Robotics	46.842681807467734	-17.405487224949255	10123
7410d1f32b0d220852ed679fe7ccfc4ebfa6e7dc	combination of independent kernel density estimators in classification	independent kernel density estimators;machine learning algorithms;kernel bandwidth testing boosting computer science information technology mathematics information science classification algorithms data structures;kernel;classification algorithm;cross validation classification error independent kernel density estimators bandwidth parameter l bfgs b algorithm;training;error analysis;estimation;bandwidth parameter;kernel density estimate;classification error;bandwidth;optimization;cross validation;operating system kernels;cross validation classification error;l bfgs b algorithm;data structure	A new classification algorithm based on combination of two independent kernel density estimators per class is proposed. Each estimator is characterized by a different bandwidth parameter. Combination of the estimators corresponds to viewing the data with different “resolutions”. The intuition behind the method is that combining different views on the data yields a better insight into the data structure; therefore, it leads to a better classification result. The bandwidth parameters are adjusted automatically by the L-BFGS-B algorithm to minimize the cross-validation classification error. Results of experiments on benchmark data sets confirm the algorithm's applicability.	algorithm;bandwidth (signal processing);benchmark (computing);cross-validation (statistics);data structure;experiment;limited-memory bfgs	Mateusz Kobos	2009	2009 International Multiconference on Computer Science and Information Technology	10.1109/IMCSIT.2009.5352749	kernel density estimation;estimation;kernel;data structure;machine learning;pattern recognition;bandwidth;cross-validation;statistics	ML	17.450409544164952	-41.00303595149976	10126
c5954d129fcc40a4843bc6fc36183f1988b1b2ae	optimal design of contingency pricing in it-intensive commerce		We propose the use of quality contingent prices, where a firm announces quality-price pairs for various levels of quality instead of a single price, as a mechanism for mitigating quality uncertainty. Contingency pricing is especially applicable to IT-intensive commerce where quality uncertainty is prevalent. The modern IT infrastructure allows easy capture, verification, and dissemination of performance and quality data essential for the implementation of contingency pricing framework. Under very broad conditions, we show that when the market underestimates firm performance, it is optimal to design a full-rebate contingent contract. The optimal quality threshold is set at the quality level that maximizes the gap between market and actual performance probabilities, and the optimal market size is independent of the quality threshold. When contingency pricing is optimal, it is sufficient to consider two-part contingent contracts: two-part contract performs as well as any multipart contract. Use of contingent prices include IT-intensive settings such as ASP service levels, Internet connectivity, and transaction execution in financial services.		Hemant K. Bhargava;Shankar Sundaresan	2002			economics;marketing;management;commerce	ECom	-0.5798710358759145	-6.959117640872779	10147
9eff25d071b4ec11a13aeaeb207acd92ec287750	progressive visual secret sharing for general access structure with multiple decryptions		Visual secret sharing (VSS) for general access structure (GAS) owns wider applications than (k,n) threshold VSS. VSS with multiple decryptions realizes the functionalities of both OR-based VSS (OVSS) and XOR-based VSS (XVSS), which can broaden the applications compared to one recovery method-based VSS. In this paper, we propose a progressive VSS (PVSS) scheme for GAS with the features of both OR and XOR decryptions based on random grid (RG). The different regions of the secret image and corresponding genearted random bits are employed to gain progressive property as well as GAS with OR and XOR decryptions. For the qualified sets, we can reconstruct the secret by stacking. On the other hand, if a device with XOR operation is available, we can improve the visual quality of the recovered secret image as well as reconstruct the secret image losslessly when we collect all the n shares. In addition, our scheme has neither pixel expansion nor codebook design due to RG. The effectiveness of the proposed scheme is shown in terms of experimental results and analyses.	access structure;codebook;lossless compression;pixel;progressive scan;residential gateway;secret sharing;stacking;xor	Xuehu Yan;Yuliang Lu	2016	2016 8th International Conference on Information Technology in Medicine and Education (ITME)	10.1007/s11042-017-4421-7	theoretical computer science;secure multi-party computation;proactive secret sharing	Vision	38.82736600382521	-10.299702570088584	10158
6ab00120f7479c68aedcd4469d588813d8b83499	humanitarian logistics network design under mixed uncertainty	differential evolution;mixed possibilistic stochastic programming;humanitarian logistics;integrated stock prepositioning and relief distribution;article	In this paper, we address a two-echelon humanitarian logistics network design problem involving multiple central warehouses (CWs) and local distribution centers (LDCs) and develop a novel two-stage scenariobased possibilistic-stochastic programming (SBPSP) approach. The research is motivated by the urgent need for designing a relief network in Tehran in preparation for potential earthquakes to cope with the main logistical problems in preand post-disaster phases. During the first stage, the locations for CWs and LDCs are determined along with the prepositioned inventory levels for the relief supplies. In this stage, inherent uncertainties in both supply and demand data as well as the availability level of the transportation network’s routes after an earthquake are taken into account. In the second stage, a relief distribution plan is developed based on various disaster scenarios aiming to minimize: total distribution time, the maximum weighted distribution time for the critical items, total cost of unused inventories and weighted shortage cost of unmet demands. A tailored differential evolution (DE) algorithm is developed to find good enough feasible solutions within a reasonable CPU time. Computational results using real data reveal promising performance of the proposed SBPSP model in comparison with the existing relief network in Tehran. The paper contributes to the literature on optimization based design of relief networks under mixed possibilistic-stochastic uncertainty and supports informed decision making by local authorities in increasing resilience of urban areas to	algorithm;central processing unit;computation;differential evolution;inventory;logistics;mathematical optimization;network planning and design;principle of good enough;program lifecycle phase;row echelon form;stochastic programming	S. Tofighi;S. Ali Torabi;S. Afshin Mansouri	2016	European Journal of Operational Research	10.1016/j.ejor.2015.08.059	differential evolution;mathematical optimization;economics;humanitarian logistics;marketing;operations management;mathematics;management science;operations research	AI	11.898007183270895	-1.411953636037379	10165
dc16e1754f8734f6ab69b227d38cb3c787d576e5	seasonal energy storage operations with limited flexibility: the price-adjusted rolling intrinsic policy	energy storage operations;real options;natural gas industry;om practice	The value of seasonal energy storage depends on how the firm operates storage to capture seasonal price spreads. Energy storage operations typically face limited operational flexibility characterized by the speed of storing and releasing energy, which makes the optimal policy, in general, difficult to compute. A widely used practice-based heuristic, the rolling intrinsic (RI) policy, generally performs well compared with an optimal policy but can significantly underperform in some cases. In this paper, we aim to understand the gap between the RI policy and the optimal policy and leverage the resulting insights to improve the RI policy. A new heuristic policy, the price-adjusted rolling intrinsic (PARI) policy, is developed based on theoretical analysis of storage options. This heuristic adjusts certain prices before applying the RI policy to provide the RI policy with estimates of the values of various storage options. We evaluate the performance of the RI and PARI polices using actual data from the natural gas industry. Our results show that, on average, the PARI policy recovers about 67% of the value loss of the RI policy. Furthermore, when the value loss of the RI policy is larger, the PARI policy tends to recover a higher fraction of that value loss.		Owen Q. Wu;Derek D. Wang;Zhenwei Qin	2012	Manufacturing & Service Operations Management	10.1287/msom.1120.0386	simulation;economics;operations management;operations research	Robotics	1.3463303715604071	-5.932144996120041	10171
0f68d1a87cdb51ac2d1d7c200a56cb9ee13493ad	heuristic models of fuzzy time series for forecasting	intervalo tiempo;modelizacion;forecasting;prevision;fuzzy time series;approche heuristique;chen model;time series;time interval;modelisation;modele heuristique;serie temporelle floue;indexation;serie temporelle;serie temporal;enfoque heuristico;sistema difuso;systeme flou;heuristic approach;modeling;modele chen;fuzzy system;intervalle temps;heuristic model	Abstract   Song and Chissom first proposed the definitions of fuzzy time series and time-invariant and variant models of fuzzy time series. Chen then proposed arithmetic operations to replace the complex computations in Song and Chissom's models. This study proposes heuristic models by integrating problem-specific heuristic knowledge with Chen's model to improve forecasting. This is because Chen's model was easy to calculate, was straightforward to integrate heuristic knowledge, and forecast better than the others. Both university enrollment and futures index are chosen as the forecasting targets. The empirical analyses show that the heuristic models reflect the fluctuations in fuzzy time series better and provide better overall forecasting results than the previous models.	heuristic;time series	Kunhuang Huarng	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(00)00093-2	econometrics;systems modeling;forecasting;artificial intelligence;time series;mathematics;operations research;fuzzy control system;statistics	AI	6.547035085594398	-20.6252649058045	10174
4253bd5c6aa840ce41ae2247596827945ca9c8ca	characterizing a tunably difficult problem in genetic programming	genetic program	This paper examines the behavioral phenomena that occur with the tuning of the binomial-3 problem. Our analysis identifies a distinct set of phenomena that may be generalizable to other problems. These phenomena also bring into question whether GA theory has any bearing on GP theory.	behavioral pattern;genetic programming;software release life cycle	Omer A. Chaudhri;Jason M. Daida;Jonathan C. Khoo;Wendell S. Richardsons;Rachel Harrison;William J. Sloat	2000			computer science;artificial intelligence;machine learning;mathematics;algorithm	NLP	24.748384360259248	-8.79088600384809	10197
d006694fb3a2dbdb265ca9d5e183a32c36424e14	multiple- instance learning with empirical estimation guided instance selection		The embedding based framework handles the multiple-instance learning (MIL) via the instance selection and embedding. It is how to select instance prototypes that becomes the main difference between various algorithms. Most current studies depend on single criteria for selecting instance prototypes. In this paper, we adopt two kinds of instance-selection criteria from two different views. For the combination of the two-view criteria, we also present an empirical estimator under which the two criteria compete for the instance selection. Experimental results validate the effectiveness of the proposed empirical estimator based instance-selection method for MIL.	embedding;genetic selection;multiple-instance learning;software prototyping;algorithm	Liming Yuan;Xianbin Wen;Haixia Xu;Lu Zhao	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546304	support vector machine;supervised learning;estimator;artificial intelligence;embedding;instance selection;pattern recognition;computer science	AI	20.424448401248075	-41.752617267738316	10203
69f240b1ed77f897917685e3fe8f9be6a768a0b8	constant distortion rate control for h.264/avc high definition videos with scene change	first pass statistics;linear distortion quantizer model;h 264 avc;second pass encoding;quantization step;joint model rate control algorithm h 264 avc high definition videos scene change constant distortion rate control distortion information frame scene complexity first pass encoding second pass encoding quantization step first pass statistics linear distortion quantizer model;frame scene complexity;video coding computational complexity statistical analysis;rate control;video coding;statistical analysis;computational complexity;distortion information;rate distortion theory automatic voltage control high definition video layout encoding statistics quantization quadratic programming video sequences bit rate;first pass encoding;joint model rate control algorithm;high definition videos;constant distortion rate control;high definition;scene change	In this paper, we propose a constant distortion rate control algorithm for H.264/AVC high definition videos with scene change. With the rate and distortion information of each frame, the frame scene complexity is modeled and the scene change is detected in the first pass encoding. In the second-pass encoding, the expected distortion of each GOP under the constraint of target bits is derived based on the first-pass statistics. And the quantization step (Q-step) of each frame can be obtained with a linear distortion-quantizer (D-Q) model by considering the parameter update of this model at scene change. The experimental results show that our proposed algorithm can provide more constant quality among different video scenes, compared to traditional joint model (JM) rate control algorithm.	algorithm;data compression;distortion;group of pictures;h.264/mpeg-4 avc;quantization (signal processing)	Dongdong Zhang;Zhenzhong Chen;King Ngi Ngan	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4542213	computer vision;speech recognition;computer science;theoretical computer science;mathematics;computational complexity theory;algorithm;statistics	Robotics	46.926976447812095	-17.605814786726448	10213
091857dbfded8f7ce0ff8d23428760498249cfbf	probabilistic multi-modal people tracker and monocular pointing pose estimator for visual instruction of mobile robot assistants	human interaction;point estimation;tracking system;tracking control engineering computing man machine systems mobile robots neural net architecture pose estimation;mobile robot;neural net architecture;mobile robots;mobile robots target tracking mobile communication context human robot interaction head robot vision systems cameras image recognition robustness;control engineering computing;research framework;man machine systems;eye gaze;tracking;pose estimation;low cost cameras probabilistic multimodal people tracker monocular pointing pose estimator visual instruction mobile robot assistants human robot communication interface research framework perses interactive mobile robotic assistants multimodal people detection tracking system hierarchical neural architecture monocular images	In this paper, we present two important aspects of our human-robot communication interface which is being developed in the context of our long-term research framework PERSES dealing with the development of highly interactive mobile robotic assistants. First, we introduce a multi-modal people detection and tracking system, a fundamental prerequisite for the observation of a human interaction partner and his nonverbal instructions given by pointing poses, gestures, head pose and eye gaze. Based on this detection and tracking system, we present a hierarchical neural architecture that is capable of estimating a target point at the floor given a pointing pose, thus enabling a user to command his mobile robot to a specific target position in his local surroundings by means of pointing. In this context, we were especially interested in determining whether it is possible to accomplish such a target point estimator using only monocular images of low-cost cameras. Both the tracker and the target point estimator were implemented and experimentally investigated on our mobile robotic assistant HOROS. The achieved recognition results presented finally demonstrate that it is in fact possible to realize a user-independent pointing pose estimation using monocular images only, but further efforts are necessary to improve the robustness of this approach for everyday application.	algorithm;autostereogram;experiment;feature extraction;gabor filter;kalman filter;mobile robot;modal logic;multimodal interaction;real-time transcription;sensor;speech recognition;tracking system	Horst-Michael Groß;Jan Richarz;Steffen Müller;Andrea Scheidig;Christian Martin	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246971	mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	47.7947903153065	-40.31192639661136	10214
5affc232debee09208f493797ae809905e4d3bc1	where does the driver look? top-down-based saliency detection in a traffic driving environment	driving;road safety intelligent transportation systems object detection;top down guidance top down based saliency detection traffic driving environment dynamically changing scene traffic saliency detection driving environment intelligent transportation systems autonomous driving traffic sign detection driving training car collision warning visual attention models visual search driver gaze behavior complex traffic driving environment;attention;eye fixations;visualization;computational modeling;computer models;saliency detection traffic environment bottom up top down visual attention;roads;image color analysis;predictive models;image analysis;visual perception;vehicles;visualization computational modeling vehicles roads image color analysis predictive models	A traffic driving environment is a complex and dynamically changing scene. When driving, drivers always allocate their attention to the most important and salient areas or targets. Traffic saliency detection, which computes the salient and prior areas or targets in a specific driving environment, is an indispensable part of intelligent transportation systems and could be useful in supporting autonomous driving, traffic sign detection, driving training, car collision warning, and other tasks. Recently, advances in visual attention models have provided substantial progress in describing eye movements over simple stimuli and tasks such as free viewing or visual search. However, to date, there exists no computational framework that can accurately mimic a driver's gaze behavior and saliency detection in a complex traffic driving environment. In this paper, we analyzed the eye-tracking data of 40 subjects consisted of nondrivers and experienced drivers when viewing 100 traffic images. We found that a driver's attention was mostly concentrated on the end of the road in front of the vehicle. We proposed that the vanishing point of the road can be regarded as valuable top-down guidance in a traffic saliency detection model. Subsequently, we build a framework of a classic bottom-up and top-down combined traffic saliency detection model. The results show that our proposed vanishing-point-based top-down model can effectively simulate a driver's attention areas in a driving environment.	autonomous car;bottom-up proteomics;device driver;eye tracking;simulation;top-down and bottom-up design;vanishing point	Tao Deng;Kaifu Yang;Yongjie Li;Hongmei Yan	2016	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2535402	computer vision;image analysis;simulation;visualization;attention;visual perception;computer science;engineering;predictive modelling;computational model;computer graphics (images)	Robotics	41.58050834570817	-43.97488276827423	10228
1ab2f7e637fc285c5780581b7637b14fd2ab87f2	iterative re-weighted l1-norm principal-component analysis		We consider the problem of principal-component analysis of a given set of data samples. When the data set contains faulty measurements/outliers, the performance of classic L 2 principal-component analysis (L 2 -PCA) deteriorates drastically. Instead, L 1 principal-component analysis (L 1 -PCA) offers outlier resistance due to the L 1 -norm maximization criterion it adopts to compute the principal subspace. In this work, we present an iterative re-weighted L 1 -PCA method (IRW L 1 -PCA) that generates a sequence of Li-norm subspaces. In each iteration, the data set comformity of each sample is measured by the L 1 subspace calculated in the previous iteration and used to weigh the data sample before the L 1 subspace update. The approach automatically suppresses outliers in each iteration resulting in increasingly accurate subspace calculation. We provide convergence analysis and compare the proposed algorithm against benchmark algorithms in the literature. Experimental studies demonstrate the superiority of the proposed IRW L 1 -PCA procedure.	benchmark (computing);conformity;expectation–maximization algorithm;experiment;iteration;level of measurement;principal component analysis;taxicab geometry	Ying Liu;Dimitris A. Pados;Stella N. Batalama;Michael J. Medley	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335373	iterative method;mathematical optimization;linear subspace;outlier;dimensionality reduction;sample (statistics);computer science;principal component analysis;subspace topology;matrix decomposition	ML	26.040271602384617	-37.82738653057014	10232
18a7b41cdcfb8fd38d47ee1e1974d3725fb95e48	adaptive pattern recognition in real-time video-based soccer analysis		Computer-aided sports analysis is demanded by coaches and the media. Image processing and machine learning techniques that allow for “live” recognition and tracking of players exist. But these methods are far from collecting and analyzing event data fully autonomously. To generate accurate results, human interaction is required at different stages including system setup, calibration, supervision of classifier training, and resolution of tracking conflicts. Furthermore, the real-time constraints are challenging: in contrast to other object recognition and tracking applications, we cannot treat data collection, annotation, and learning as an offline task. A semi-automatic labeling of training data and robust learning given few examples from unbalanced classes are required. We present a real-time system acquiring and analyzing video sequences from soccer matches. It estimates each player’s position throughout the whole match in real-time. Performance measures derived from these raw data allow for an objective evaluation of physical and tactical profiles of teams and individuals. The need for precise object recognition, the restricted working environment, and the technical limitations of a mobile setup are taken into account. Our contribution is twofold: (1) the deliberate use of machine learning and pattern recognition techniques allows us to achieve high classification accuracy in varying environments. We systematically evaluate combinations of image features and learning machines in the given online scenario. Switching between classifiers depending on the amount of training data and available training time improves robustness and efficiency. (2) A proper human–machine interface decreases the number of required operators who are incorporated into the system’s learning process. Their main task reduces to the identification of players in uncertain situations. Our experiments showed high performance in the classification task achieving an average error rate of 3 % on three real-world datasets. The system was proved to collect accurate tracking statistics throughout different soccer matches in real-time by incorporating two human operators only. We finally show how the resulting data can be used instantly for consumer applications and discuss further development in the context of behavior analysis.	bit error rate;experiment;image processing;machine learning;online and offline;outline of object recognition;pattern recognition;real-time clock;real-time computing;real-time locating system;semiconductor industry;statistical classification;unbalanced circuit;user interface	Marc Schlipsing;Jan Salmen;Marc Tschentscher;Christian Igel	2014	Journal of Real-Time Image Processing	10.1007/s11554-014-0406-1	embedded system;computer vision;real-time computing;simulation;computer science;machine learning	ML	37.93645361835862	-44.32565609483783	10233
13baa33f190d9553c6c981f4f939c488923da015	a new data envelopment analysis method for priority determination and group decision making in the analytic hierarchy process	modelizacion;agregacion;multicriteria analysis;hierarchical structure;evaluation performance;analytic hierarchy process;analisis envolvimiento datos;judgment;performance evaluation;probabilidad subjetiva;processus hierarchie analytique;probabilite subjective;systeme aide decision;hierarchized structure;social decision;multiple criteria decision making;evaluacion prestacion;subjective probability;estimation non parametrique;priorite;prise de decision;structure hierarchisee;simple additive weighting;sistema ayuda decision;decision maker;aggregation;modelisation;non parametric estimation;jugement;decision support system;hierarchical classification;data envelopment analysis;proceso jerarquia analitico;classification hierarchique;agregation;group decision making;juicio;deahp;analisis multicriterio;estimacion no parametrica;analyse multicritere;data envelopment analysis analytic hierarchy process deahp group decision making multiple criteria decision making;decision colectiva;decision collective;toma decision;data envelope analysis;priority;prioridad;modeling;clasificacion jerarquizada;estructura jerarquizada;analyse enveloppement donnee	The DEAHP method for weight deviation and aggregation in the analytic hierarchy process (AHP) has been found flawed and sometimes produces counterintuitive priority vectors for inconsistent pairwise comparison matrices, which makes its application very restrictive. This paper proposes a new data envelopment analysis (DEA) method for priority determination in the AHP and extends it to the group AHP situation. In this new DEA methodology, two specially constructed DEA models that differ from the DEAHP model are used to derive the best local priorities from a pairwise comparison matrix or a group of pairwise comparison matrices no matter whether they are perfectly consistent or inconsistent. The new DEA method produces true weights for perfectly consistent pairwise comparison matrices and the best local priorities that are logical and consistent with decision makers (DMs)' subjective judgments for inconsistent pairwise comparison matrices. In hierarchical structures, the new DEA method utilizes the simple additive weighting (SAW) method for aggregation of the best local priorities without the need of normalization. Numerical examples are examined throughout the paper to show the advantages of the new DEA methodology and its potential applications in both the AHP and group decision making.	analytical hierarchy;data envelopment analysis	Ying-Ming Wang;Kwai-Sang Chin	2009	European Journal of Operational Research	10.1016/j.ejor.2008.01.049	decision support system;computer science;operations management;data mining;data envelopment analysis;mathematics;operations research	Vision	-0.8503431383859565	-15.800109146271494	10234
e28d19722cc757d8644d446effda7fa6a28f76c0	forced information maximization to accelerate information-theoretic competitive learning	forced information maximization;dead neurons mutual information maximization forced information information loss competitive learning winner take all;feature detection;dead neurons;information loss;artificial data method;competitive learning;acceleration neurons neural networks computer vision entropy data mining feature extraction uncertainty principal component analysis computer architecture;information theoretic competitive learning;principal component analysis feature extraction learning artificial intelligence;feature extraction;principal component analysis;feature detection forced information maximization information theoretic competitive learning artificial data method principal component analysis;mutual information maximization;mutual information;forced information;learning artificial intelligence;winner take all;information theoretic	Information-theoretic competitive learning has been proved to be more general and more flexible type of competitive learning. However, one of the major shortcomings of this method is that it is sometimes very slow in learning. To overcome this problem, we introduce forced information used to force networks to increase information by supposing maximum information. We applied the method to an artificial data as well as a student survey. In both cases, we observed that information was very rapidly increased to stable points. Compared with results by the principal component analysis, our method showed more clearly the main features of input patterns. In addition, we can more easily explain a main mechanism of feature detection by forced information.	competitive learning;expectation–maximization algorithm;feature detection (computer vision);feature detection (web development);principal component analysis;theory	Ryotaro Kamimura;Ryozo Kitajima	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371227	winner-take-all;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;feature detection;mutual information;competitive learning;principal component analysis	ML	15.492435376148038	-32.40440892360669	10238
5340e510579aeb1e66128c40e0c39a50fb8df3ac	distribution preserving learning for unsupervised feature selection		Abstract Selection of most relevant features from high-dimensional data is difficult especially in unsupervised learning scenario, this is because there is an absence of class labels that would guide the search for relevant features. In this work, we propose a distribution preserving feature selection (DPFS) method for unsupervised feature selection. Specifically, we select those features such that the distribution of the data can be preserved. Theoretical analysis show that our proposed DPFS method share some excellent properties of kernel method. Moreover, traditional “wrapper” and “filter” feature selection methods often involve an exhaustive search optimization, feature selection problem is treated as variable of optimization problem in our proposed method, the optimization is tractable. Extensive experimental results over various real-life data sets have demonstrated the effectiveness of the proposed algorithm.	feature selection	Ting Xie;Pengfei Ren;Taiping Zhang;Yuan Yan Tang	2018	Neurocomputing	10.1016/j.neucom.2018.02.032	machine learning;brute-force search;kernel method;unsupervised learning;feature selection;artificial intelligence;data set;optimization problem;mathematics;pattern recognition	AI	22.849613744876194	-41.72196919154421	10239
12a08ed15e93dfe974d71ce9782f09b17f3e25f3	network structure for tracking of jockeys in horse races	mean shift;contextual information;sports analysis;colour tracking	"""This article proposes a model to track contenders (jockeys) in horse races from broadcast videos based on the colour property of contenders. To overcome the effects of background noise and occlusion the proposed system merges contextual information of the horse race into a colour-based tracking framework (mean shift). There are two improvements. The first is the design of a model, specifically for horse races, to extract the contenders from the background. The second estimates reliability of a trajectory by building a model called """"network structure"""". The proposed algorithm is tested on eleven broadcast videos of horse races where the mean length in time of video is 51s. To evaluate the proposed model, the races are divided into five intervals and the percentage of correct tracking (PCT) is calculated at the end of each interval. A comparison study of the proposed method, with a mean shift tracking method, shows that network structure improves tracking in the videos."""	algorithm;mean shift	Mohammad Hedayati;Michael J. Cree;Jonathan B. Scott	2014		10.1145/2683405.2683422	computer vision;simulation;mean-shift	Vision	42.390184196533546	-47.34464360521474	10271
9b037ca8b663c8797d0001c11853d6dabc50a8d7	a parametric fuzzy logic approach to dynamic part routing under full routing flexibility	flexible manufacturing systems;takagi sugeno;system configuration;processing flexibility;rule based;routing flexibility;system performance;fuzzy logic;flexible process plan;simulation experiment;dynamic part routing;flexible manufacturing system;membership function;routing algorithm;process planning;manufacturing system	Manufacturing flexibility is a competitive weapon for surviving today’s highly variable and volatile markets. It is critical therefore, to select the appropriate type of flexibility for a given manufacturing system, and to design effective strategies for using this flexibility in a way to improve the system performance. This study focuses on full routing flexibility which includes not only alternative machines for operations but also alternative sequences of operations for producing the same work piece. Upon completion of an operation, an on-line dispatching decision called part routing is required to choose one of the alternatives as the next step. This study introduces three new approaches, including a fuzzy logic approach, for dynamic part routing. The fuzzy part routing system adapts itself to the characteristics of a given flexible manufacturing system (FMS) installation by setting the key parameters of the membership functions as well as its Takagi-Sugeno type rule base, in such a way to capture the bottlenecks in the environment. Thus, the model does not require a search or training for the parameter set. The proposed approaches are tested against several crisp and fuzzy routing algorithms taken from the literature, by means of extensive simulation experiments in hypothetical FMS environments under variable system configurations. The results show that the proposed fuzzy approach remains robust across different system configurations and flexibility levels, and performs favourably compared to the other algorithms. The results also reveal important characteristic behaviour regarding routing flexibility. 2007 Elsevier Ltd. All rights reserved.	algorithm;experiment;fuzzy logic;fuzzy routing;online and offline;rule-based system;simulation;type rule;volatile memory	Ümit Bilge;Murat Firat;Erinç Albey	2008	Computers & Industrial Engineering	10.1016/j.cie.2007.11.013	fuzzy logic;rule-based system;control engineering;routing domain;mathematical optimization;static routing;simulation;membership function;computer science;engineering;artificial intelligence;operations management;multipath routing;machine learning;computer performance	AI	10.126902177849052	2.010192077606369	10275
4a552d84b214a90f86da2465b86399265796fa43	a hybrid competent multi-swarm approach for many-objective problems	competent algorithm particle swarm optimization many objective estimation of density algorithm;hybrid competent multiswarm approach dtlz test functions c multi bayesian optimization algorithm rboa eda estimation of distribution algorithm i multi iterated multiswarm maop many objective optimization problems;particle swarm optimisation bayes methods;particle swarm optimization;convergence optimization bayes methods sociology statistics measurement linear programming;estimation of density algorithm;competent algorithm;many objective	Many-objective optimization problems (MaOPs) are a class of multi-objective problems that presents more than three functions to be optimized. As most Pareto based algorithms scale poorly according to the number of objectives, researchers are working on alternatives to overcome these limitations. An algorithm that has shown good results in solving MaOPs is the Iterated Multi-swarm (I-Multi) which presents a clever multi-swarm strategy to spread the solutions across different areas of the objective space while keeping a good convergence. As the I-Multi is a very recent algorithm, alternative approaches are yet to be explored. Here we investigate the use of an Estimation of Distribution Algorithm (EDA) in the multi-swarm stage of I-Multi. EDAs create a model based on the best solutions found and sample new solutions based in this model. An EDA that presents good performance is the rBOA which is a real-valued version of the Bayesian optimization algorithm. This work presents an algorithm called C-Multi consisting of a hybrid between the I-Multi and the rBOA with the aim to join the diversity strength of I-Multi and the convergence characteristic of rBOA. An experimental study is conducted using the seven well-known DTLZ test functions with 3, 5, 10, 15 and 20 objectives to evaluate the performance of the algorithms as the number of objectives scales up. The results point that the new algorithm presents superior convergence and diversity on hard problems.	adaptive algorithm;archive;bayesian optimization;computation;computational complexity theory;distribution (mathematics);estimation of distribution algorithm;experiment;iterated function;iteration;local optimum;machine learning;mathematical optimization;pareto efficiency;sampling (signal processing);scalability;swarm	Olacir Rodrigues Castro Junior;Aurora Trinidad Ramirez Pozo	2014	2014 Brazilian Conference on Intelligent Systems	10.1109/BRACIS.2014.82	mathematical optimization;multi-swarm optimization;meta-optimization;artificial intelligence;machine learning;mathematics;population-based incremental learning	AI	25.538605339996366	-4.17493895982081	10282
2635d7bf037029b94d16d75fd6310955d9a409c9	scalable learning of bayesian network classifiers	scalable bayesian classification;out of core learning;big data;feature selection	Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the k-dependence Bayesian classifier (KDB) that discriminatively selects a sub-model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on 16 large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression.	bayesian network;dspace;discriminative model;logistic regression;naive bayes classifier;nonlinear system;out-of-core algorithm;random forest;scalability;statistical classification	Ana M. Martínez;Geoffrey I. Webb;Shenglei Chen;Nayyar A. Zaidi	2016	Journal of Machine Learning Research		big data;computer science;machine learning;pattern recognition;data mining;feature selection	ML	17.324131966601712	-38.64660385232387	10297
3f525a6b4566bd37d8a44ade5db422d0d3d66d31	learning collaborative manipulation tasks by demonstration using a haptic interface	dynamic programming;teleoperation learning collaborative manipulation tasks haptic interface robot gaussian mixture regression hidden markov model;manipulators;gaussian mixture;telerobotics haptic interfaces hidden markov models human robot interaction learning by example manipulators;hidden markov model;collaboration;learning by imitation;human robot interaction;haptics;data mining;orbital robotics;statistical model;force;programming by demonstration;collaboration haptic interfaces robot kinematics hidden markov models humans orbital robotics dynamic programming robot control communication system control data mining;collaborative tasks;dynamic information;hidden markov models;learning by example;trajectory;robot control;telerobotics;humans;gaussian mixture regression;haptic interfaces;communication system control;robot;learning collaborative manipulation tasks;communication pattern;teleoperation;robot kinematics;haptic interface	This paper presents a method by which a robot can learn through observation to perform a collaborative manipulation task, namely lifting an object. The task is first demonstrated by a user controlling the robot's hand via a haptic interface. Learning extracts statistical redundancies in the examples provided during training by using Gaussian Mixture Regression and Hidden Markov Model. Haptic communication reflects more than pure dynamic information on the task, and includes communication patterns, which result from the two users constantly adapting their hand motion to coordinate in time and space their respective motions. We show that the proposed statistical model can efficiently encapsulate typical communication patterns across different dyads of users, that are stereotypical of collaborative behaviours between humans and robots. The proposed learning approach is generative and can be used to drive the robot's retrieval of the task by ensuring a faithful reproduction of the overall dynamics of the task, namely by reproducing the force patterns for both lift the object and adapt to the human user's hand motion. This work shows the potential that teleoperation holds for transmitting both dynamic and communicative information on the task, which classical methods for programming by demonstration have traditionally overlooked.	haptic technology;hidden markov model;humans;lambda lifting;markov chain;programming by demonstration;robot;statistical model;transmitter	Sylvain Calinon;Paul Evrard;Elena Gribovskaya;Aude Billard;Abderrahmane Kheddar	2009	2009 International Conference on Advanced Robotics		computer vision;simulation;computer science;communication	Robotics	36.965792159975074	-40.778585441920036	10304
842033e46f4999584ba45c4696305b98e412cc35	distributed compression in camera sensor networks	channel coding;networks;image coding;data compression;hidden feature removal;plenoptic function;correlation methods;sensor network;wavelet transforms;channel coding data compression image coding wireless sensor networks cameras correlation methods hidden feature removal;image compression;intelligent networks intelligent sensors signal processing wireless sensor networks digital cameras compression algorithms image coding layout educational institutions computer networks;slepian wolf;united kingdom;coding;video frames;slepian wolf achievable rate region distributed compression camera sensor network plenoptic function transmission rate common central receiver flexible allocation distributed coding approach general binary source linear channel code;wireless sensor networks;cameras;general binaries	We address the problem of distributed compression in camera sensor networks. Our approach uses some geometrical information in order to estimate the correlation in the visual data. This correlation, which is related to the structure of the plenoptic function, can then be used to reduce the overall transmission rate from the sensors to a common central receiver. Our approach allows for a flexible allocation of the bit-rates amongst the encoders and can be made resilient to a fixed number of occlusions. Finally, we show that our distributed coding approach can be extended to general binary sources. The technique we propose uses linear channel codes and can achieve any point of the Slepian-Wolf achievable rate region.	algorithm;code;encoder;image sensor	Nicolas Gehrig;Pier Luigi Dragotti	2004	IEEE 6th Workshop on Multimedia Signal Processing, 2004.	10.1109/MMSP.2004.1436555	computer vision;wireless sensor network;telecommunications;computer science;statistics	Mobile	49.84345175203835	-17.36218947793388	10308
648a13a6b2bb8058d79cb95f3820a2a5fbcfd400	super-resolution 3d tracking and mapping	3d;image resolution;mobile robots;robotics;robust estimation;computer vision;colour errors super resolution 3d tracking super resolution 3d mapping visual slam technique 6 degrees of freedom pose dof dense structure low resolution images super resolution techniques 3d translation dense localisation framework mapping framework image deformations 6d space depth errors;image resolution cameras three dimensional displays real time systems estimation vectors simultaneous localization and mapping;3d odometry;slam robots image resolution object tracking;object tracking;2d;andrew comport s homepage;augmented reality;visual servoing;slam robots;tracking;pose estimation	This paper proposes a new visual SLAM technique that not only integrates 6 degrees of freedom (DOF) pose and dense structure but also simultaneously integrates the colour information contained in the images over time. This involves developing an inverse model for creating a super-resolution map from many low resolution images. Contrary to classic super-resolution techniques, this is achieved here by taking into account full 3D translation and rotation within a dense localisation and mapping framework. This not only allows to take into account the full range of image deformations but also allows to propose a novel criteria for combining the low resolution images together based on the difference in resolution between different images in 6D space. Another originality of the proposed approach with respect to the current state of the art lies in the minimisation of both colour (RGB) and depth (D) errors, whilst competing approaches only minimise geometry. Several results are given showing that this technique runs in real-time (30Hz) and is able to map large scale environments in high-resolution whilst simultaneously improving the accuracy and robustness of the tracking.	image resolution;key frame;mathematical model;real-time clock;real-time computing;simultaneous localization and mapping;super-resolution imaging;while	Maxime Meilland;Andrew I. Comport	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6631399	mobile robot;computer vision;augmented reality;2d computer graphics;simulation;pose;image resolution;computer science;video tracking;tracking;robotics;visual servoing;computer graphics (images)	Robotics	52.68391226886582	-45.643019879864745	10314
9bcb9461da31910ad19218fab2e0786c16ea8054	improving fine-grained object classification using adversarial generated unlabelled samples		Recently Convolutional Neural Networks(CNNs) models have achieved remarkable results for fine-grained image classification. However, CNNs require a large amount of training data during supervised learning and labeling so much data is expensive in many cases. To address this issue, this paper innovatively presents a semi-supervised pipeline to improve fine-grained classification tasks without any extra data. We carefully combine CNNs with Generative Adversarial Nets(GANs) for classification, which shows that the result is affirmative. In addition, we propose a multi dimension label regularization(MDLR) method to train labeled images and unlabeled images simultaneously. First we use a pre-trained Yolo v2 object detection model to detect coarse-grained object on the original dataset. Second we feed cropped images to the generator of GAN to produce more generated data and assign a uniform label distribution to the generated images. Third we mix these origin real images and generated images. Then these mixed images are fed to a baseline CNN classifier and a feature-fused CNN classifier. We obtain competitive or state-of-the-art results: using feature-fused CNN model on Stanford Dogs dataset we set a new state-of-the-art result of 90.7%; on Oxford 102 Flowers dataset, we show consistent improvements over baseline.	baseline (configuration management);benchmark (computing);computer vision;convolutional neural network;experiment;generative adversarial networks;learning classifier system;object detection;semi-supervised learning;semiconductor industry;supervised learning	Enze Xie;Guangyao Li;Wenyu Liu	2018	2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2018.8499075	convolutional neural network;supervised learning;task analysis;feature extraction;object detection;contextual image classification;training set;real image;pattern recognition;artificial intelligence;computer science	Vision	27.815252150503188	-50.458847704527976	10322
ea7c2cffdae8397c7003b533d87d6c80f6517ae7	efficient multilateration tracking with concurrent offset estimation using stochastic filtering techniques	filtering theory;search radar;stochastic processes;surveillance;tracking;aerial surveillance;base station;concurrent offset estimation;filter step;hyperboloid;linear regression kalman filter;linear system model;multilateration tracking system;nonlinear measurement model;radio signal;receiver;secondary surveillance radar;signal transmitter;stochastic filtering technique;time difference of arrival measurement;aerial surveillance;estimation;multilateration;nonlinear filtering;tracking	Multilateration systems operate by determining distances between a signal transmitter and a number of receivers. In aerial surveillance, radio signals are emitted as Secondary Surveillance Radar (SSR) by the aircraft, representing the signal transmitter. A number of base stations (sensors) receive the signals at different times. Most common approaches use time difference of arrival (TDOA) measurements, calculated by subtracting receiving times of one receiver from another. As TDOAs require intersecting hyperboloids, which is considered a hard task, this paper follows a different approach, using raw receiving times. Thus, estimating the signal's emission time is required, captured as a common offset within an augmented version of the system state. This way, the multilateration problem is reduced to intersecting cones. Estimation of the aircraft's position based on a nonlinear measurement model and an underlying linear system model is achieved using a linear regression Kalman filter [1, 2]. A decomposed computation of the filter step is introduced, allowing a more efficient calculation.	approximation algorithm;closed-circuit television;computation;cross-covariance;experiment;gaussian blur;kalman filter;linear system;multilateration;nonlinear system;radio wave;sensor;stochastic control;television antenna;transmitter	Patrick Dunau;Ferdinand Packi;Frederik Beutler;Uwe D. Hanebeck	2010	2010 13th International Conference on Information Fusion		control engineering;wide area multilateration;electronic engineering;engineering;control theory	Robotics	51.02442046406735	3.203503449816194	10325
72bb9ff155e8950bab4599e75d8431fb722fc243	fuzzy state estimation of discrete event systems	duracion;methode recursive;fuzzy neural nets;systeme evenement discret;fuzzy set;fuzzy reasoning;estimation etat;red petri;logique floue;metodo recursivo;recursive method;logica difusa;reseau neuronal flou;intelligence artificielle;state estimation;duration;sistema acontecimiento discreto;fuzzy logic;identificacion sistema;systeme incertain;discrete event system;time petri net;system identification;artificial intelligence;inteligencia artificial;sistema incierto;petri net;estimacion estado;uncertain system;identification systeme;conversion;reseau petri;duree	This paper addresses state estimation of discrete event systems (DES) using a fuzzy reasoning approach; a method for approximating the current state of DES with uncertainty in the duration of activities is presented. The proposed method is based on a DES specification given as a fuzzy timed Petri net in which fuzzy sets are associated to places; a technique for the recursive computing of imprecise markings is given, then the conversion to discrete marking is presented.		Juan Carlos González-Castolo;Ernesto López-Mellado	2006		10.1007/11925231_9	fuzzy logic;defuzzification;discrete event dynamic system;system identification;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;duration;control theory;mathematics;fuzzy set;petri net;fuzzy set operations;algorithm	Robotics	8.848679559779244	-28.169586546622362	10329
69cdb42e4b5a1f56c28ba2ac004ef3643cffb73b	dropoutdagger: a bayesian approach to safe imitation learning		While imitation learning is becoming common practice in robotics, this approach often suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which uses the distribution over actions provided by the novice policy, for a given observation. Our method, which we call DropoutDAgger, uses dropout to train the novice as a Bayesian neural network that provides insight to its confidence. Using the distribution over the novice’s actions, we estimate a probabilistic measure of safety with respect to the expert action, tuned to balance exploration and exploitation. The utility of this approach is evaluated on the MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved performance and safety compared to other DAgger variants and classic imitation learning.	algorithm;artificial neural network;bayesian network;dropout (neural networks);exploit (computer security);iterative method;robotics	Kunal Menda;Katherine Rose Driggs-Campbell;Mykel J. Kochenderfer	2017	CoRR		artificial neural network;machine learning;computer science;iterative method;training set;imitation;artificial intelligence;probabilistic logic;robotics;bayesian probability	Robotics	21.654902250061806	-20.172663684458843	10330
69f2f171193f3f4ff0470c93f2caabb7dfaed760	simulation of scheduled ordering policies in distribution supply chains	scheduling;simulation;supply chain management;balanced ordering;decentralized distribution supply chain;end-customer demands;scheduled ordering policy;simulation;synchronized ordering	In this paper we study a decentralized distribution supply chain with one supplier and many newsvendor-type retailers that face exogenous end-customer demands. Using total supply chain cost as our primary measure of performance, we compare two scheduled ordering policies -- Balanced ordering and Synchronized ordering -- with the traditional newsvendor-type ordering behavior. Via the use of simulation, we evaluate the effectiveness of the two scheduled ordering policies, and identify how the performance of the scheduled ordering policies changes with different supply chain parameters, such as the number of retailers, the supplier's expediting cost, the supplier's capacity limit, etc.	newsvendor model;simulation;two-port network	Lucy Gongtao Chen;Srinagesh Gavirneni	2007	2007 Winter Simulation Conference		supply chain management;service management;computer science;supply chain;scheduling	EDA	0.1850667354793359	-5.317576647472288	10336
3b60d3b95bfc1aa2be17316d766260555cffe54a	evolutionary simulation of complex networks structures with specific topological properties	network design;simulated annealing;complex networks	Abstract   The expanding variety of observable real-world complex networks (CN) required development of mathematical models aimed to explain the nature of such constructions and to model their structure with certain precision. Existing models of CN seem to lack flexibility because of rigid modelling algorithm they are built upon. This might be inconvenient when there is a need to have an extended set of hypotheses about possible networks structures for some experiments.  In present work we consider heuristic approach to modelling complex networks structures based on simulated annealing algorithm and applying it to the problem of modelling small-world networks with specific properties. We demonstrate that this approach helps to simulate realistic structures with properties unobtainable by traditional models of complex networks.	simulation	Victor V. Kashirin	2014		10.1016/j.procs.2014.05.224	simulation;artificial intelligence;machine learning;mathematics	Theory	20.668252982350694	-8.84770197062129	10382
661e7e21f3db8c8faa6e4325f6c71377da7f188f	multiple object class detection with a generative model	image sampling;detectors;image recognition;object detection detectors image edge detection computer vision training data image sampling position measurement noise measurement size measurement image recognition;high dimensionality;generic model;size measurement;noise measurement;computer vision;probabilistic model;training data;multiple objectives;image edge detection;position measurement;object detection;matching method	In this paper we propose an approach capable of simultaneous recognition and localization of multiple object classes using a generative model. A novel hierarchical representation allows to represent individual images as well as various objects classes in a single, scale and rotation invariant model. The recognition method is based on a codebook representation where appearance clusters built from edge based features are shared among several object classes. A probabilistic model allows for reliable detection of various objects in the same image. The approach is highly efficient due to fast clustering and matching methods capable of dealing with millions of high dimensional features. The system shows excellent performance on several object categories over a wide range of scales, in-plane rotations, background clutter, and partial occlusions. The performance of the proposed multi-object class detection approach is competitive to state of the art approaches dedicated to a single object class recognition problem.	cluster analysis;clutter;codebook;generative model;machine learning;matching (graph theory);scalability;sensor;statistical model	Krystian Mikolajczyk;Bastian Leibe;Bernt Schiele	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.202	statistical model;computer vision;training set;detector;method;object-class detection;object model;computer science;noise measurement;viola–jones object detection framework;machine learning;pattern recognition;3d single-object recognition	Vision	40.64007647815901	-49.964011551779954	10395
e4528a09353cdd127959fb2e536f397347828d62	design of an improved quantum-inspired evolutionary algorithm for a transportation problem in logistics systems	vehicle routing problem with time windows;quantum inspired evolutionary algorithm;optimization;heuristics	Logistics faces great challenges in vehicle schedule problem. Intelligence Technologies need to be developed for solving the transportation problem. This paper proposes an improved Quantum-Inspired Evolutionary Algorithm (IQEA), which is a hybrid algorithm of Quantum-Inspired Evolutionary Algorithm (QEA) and greed heuristics. It extends the standard QEA by combining its principles with some heuristics methods. The proposed algorithm has also been applied to optimize a problem which may happen in real life. The problem can be categorized as a vehicle routing problem with time windows (VRPTW), which means the problem has many common characteristics that VRPTW has, but more constraints need to be considered. The basic idea of the proposed IQEA is to embed a greed heuristic method into the standard QEA for the optimal recombination of consignment subsequences. The consignment sequence is the order to arrange the vehicles for the transportation of the consignments. The consignment subsequences are generated by cutting the whole consignment sequence according to the values of quantum bits. The computational result of the simulation problem shows that IQEA is feasible in achieving a relatively optimal solution. The implementation of an optimized schedule can save much more cost than the initial schedule. It provides a promising, innovative approach for solving VRPTW and improves QEA for solving complexity problems with a number of constraints.	evolutionary algorithm;logistics;transportation theory (mathematics)	Lixing Wang;S. K. Kowk;Andrew W. H. Ip	2012	J. Intelligent Manufacturing	10.1007/s10845-011-0568-7	mathematical optimization;simulation;engineering;heuristics	Robotics	21.5811884912435	-0.02758325310292651	10416
41653c79084669b42e873b5414bffefa05e0daa7	iterative-parametric image models for video coding	fractals;image coding;image segmentation;image processing;280203;motion compensation;product code;iso standards;institute for integrated and intelligent systems;conference output;video coding;counting circuits;iec standards;video coding image reconstruction image coding counting circuits image segmentation iec standards iso standards motion compensation fractals predictive models;faculty of engineering and information technology;image reconstruction;predictive models;digital image;image modeling	This paper proposes a novel method for compressing digital image sequence based on iterative and parameterisable image models. By using proposed image models, the reconstructed image approximates the input image progressively by repeatedly segmenting each residual image obtained from previous iteration and the input image into objects or domains. Parameters characterising segmented domains are then encoded and transmitted. The proposed image model utilises product codes in an attempt to further improve the quality of the input image. Finally, applications of proposed images models are extended to video coding, which exploit similarity between successful frames, in addition to selfsimilarity within each frame.	code;data compression;digital image;iteration	Xuesong Le;Ruben Gonzalez	2005	Proceedings of the Eighth International Symposium on Signal Processing and Its Applications, 2005.	10.1109/ISSPA.2005.1580193	iterative reconstruction;image warping;image texture;computer vision;feature detection;fractal;binary image;image processing;computer science;theoretical computer science;digital image processing;free boundary condition;universal product code;predictive modelling;multimedia;image segmentation;motion compensation;automatic image annotation;top-hat transform;digital image	Vision	45.635011792892065	-16.476258518888663	10428
1ac82cff3d8f1919621095c9c5f09571e9d00b89	machine-awareness in indoor environment: a pseudo-3d vision-based approach combining multi-resolution visual information		The present paper describes a dual approach using pseudo-3D vision for Machine-Awareness in indoor environment. Provided by color and depth cameras of the a Kinect system, the aforementioned duality presents an appealing solution for robotsu0027 3D-vision. Placing the human-robot and in a more general way the human-machine interactions as a key outcome of the expected visual Machine-Awareness, the proposed vision-system aims proffering the machine the self-reliance in awareness about the surrounding environment in which the machine is supposed to evolve. Blend pseudo-3D vision and salient objectsu0027 detection algorithm, the investigated approach seeks an autonomous detection of relevant items in 3D environment. The pseudo-3D perception leads to reducing computational complexity inborn to the 3D vision context into a 2D computational task by processing 3D visual information within a 2D-imagesu0027 framework. The statistical foundation of the investigated approach proffers it a solid and comprehensive theoretical basis, holding out a bottom-up nature making the issued system unconstrained regarding prior hypothesis. We provide experimental results validating the proposed system.	algorithm;autonomous robot;computational complexity theory;interaction;kinect;nvidia 3d vision;top-down and bottom-up design	Kurosh Madani;Hossam Fraihat;Christophe Sabourin	2017	2017 9th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)	10.1109/IDAACS.2017.8095116	machine learning;robot;computer vision;visualization;artificial intelligence;duality (optimization);computational complexity theory;computer science;perception	Robotics	43.198860581611406	-39.596622808384694	10435
6e2eba1955d883304bfde0f360a9471a81fa941f	simulated annealing of neural networks: the 'cooling' strategy reconsidered	computer science;neural network;simulated annealing;samarium;binary search trees;learning artificial intelligence;artificial neural network;computational modeling;neural networks;thermodynamics;global optimization;perceptrons	"""The simulated annealing (SA) algorithm [12] [5] has been widely used to address intractable global optimizations in many elds, including training of articial neural networks. Implementations of annealing universally use a monotone decreasing, or \cooling"""", temperature schedule which is motivated by the algorithm's proof of optimality as well as analogies with statistical thermodynamics. In this paper, we challenge this motivation: the fact that cooling schedules are \optimal"""" in theory is not related to the practical performance of the algorithm. Our nding is based on a new \best-so-far"""" criterion for measuring the quality of annealing schedules. Motivated by studies of optimal schedules for small problems, we study highly nonstandard annealing schedules for training of feedforward perceptron networks on a real-world sensor classi cation benchmark. We nd clear evidence that optimal schedules do not necessarily decrease monotonically to zero."""	algorithm;artificial neural network;bean scripting framework;benchmark (computing);computer cooling;feedforward neural network;global optimization;mathematical optimization;norm (social);numerical analysis;perceptron;schedule (computer science);simulated annealing;simulation;stochastic hill climbing;monotone	Kenneth D. Boese;Andrew B. Kahng	1993			mathematical optimization;binary search tree;simulated annealing;computer science;artificial intelligence;perceptron;machine learning;samarium;computational model;adaptive simulated annealing;artificial neural network	ML	21.692099849688773	-32.17222304788782	10482
a75f766eff0ac296044fb5697d2ea689456d9805	object tracking based on camshift with multi-feature fusion	object tracking	It is very hard for traditional Camshift to survive of drastic interferences and occlusions of similar objects. This paper puts forward an innovative tracking method using Camshift with multi-feature fusion. Firstly, SIFT features and edge features of the Camshift in RGB space are counted to reduce the probability of disruption by occlusion and clutter. Then, the texture features are collected to resolve the problems of analogue interference, the texture similarity between current frame and previous frames are calculated to determine the object area. The paper also describes the GM(1,1) prediction model, which could solve the occlusion problems in a novel way. Finally, through the motion trajectory, it can anticipate the exact position of the object. The results of several tracking tasks prove that our method has solved problems of occlusions, interferences and shadows. And it performs well in both tracking robustness and computational efficiency.	algorithm;clutter;computation;denial-of-service attack;interference (communication);pixel;scale-invariant feature transform;scott continuity	Zhiyu Zhou;Dichong Wu;Xiaolong Peng;Zefei Zhu;Kaikai Luo	2014	JSW		computer vision;computer science;video tracking;computer graphics (images)	Vision	42.111936526875624	-45.626990076979475	10490
9a8cfb9a0422750be281cfb228406601afbf5536	superpixel-based class-semantic texton occurrences for natural roadside vegetation segmentation	vegetation segmentation;texton;superpixel;classification algorithm;object recognition	Vegetation segmentation from roadside data is a field that has received relatively little attention in present studies, but can be of great potentials in a wide range of real-world applications, such as road safety assessment and vegetation condition monitoring. In this paper, we present a novel approach that generates class-semantic color–texture textons and aggregates superpixel-based texton occurrences for vegetation segmentation in natural roadside images. Pixel-level class-semantic textons are learnt by generating two individual sets of bag-of-word visual dictionaries from color and filter bank texture features separately for each object class using manually cropped training data. For a testing image, it is first oversegmented into a set of homogeneous superpixels. The color and texture features of all pixels in each superpixel are extracted and further mapped to one of the learnt textons using the nearest distance metric, resulting in a color and a texture texton occurrence matrix. The color and texture texton occurrences are aggregated using a linear mixing method over each superpixel and the segmentation is finally achieved using a simple yet effective majority voting strategy. Evaluations on two datasets such as video data collected by the Department of Transport and Main Roads, Queensland, Australia, and a public roadside grass dataset show high accuracy of the proposed approach. We also demonstrate the effectiveness of the approach for vegetation segmentation in real-world scenarios.	categorization;dictionary;document-term matrix;experiment;filter bank;pixel;texton	Ligang Zhang;Brijesh Verma	2017	Machine Vision and Applications	10.1007/s00138-017-0833-7	computer vision	Vision	31.70211789301704	-46.12982815835398	10499
4e8e7838e3161f5843694285f360484c707d195e	measurement of relative operational efficiency of soeus in india using data envelopment analysis	benchmarking;dea;power supply;relative operational efficiency;bcc model;technical efficiency;data envelopment analysis;sensitivity analysis;state owned electricity utilities;efficiency measurement;data envelope analysis;ccr model;india	A nonparametric approach to frontier analysis, data envelopment analysis (DEA) is applied in this work to evaluate the relative operational efficiency of state owned electric utilities (SOEU) in India. Two different models viz CCR model and BCC model are applied to evaluate the overall efficiency and technical efficiency. Twenty nine SOEUs in India are considered for the analysis and the relative operational efficiency scores are calculated. The results indicate that the performance of several SOEUs are sub-optimal, suggesting a potential for significant improvements in the operation so as to improve the overall efficiency. Also sensitivity analysis is carried out to investigate the effect of changes in the solutions of the model.	bricx command center;data envelopment analysis;input/output;viz: the computer game	R. Meenakumari;N. Kamaraj;Tripta Thakur	2009	IJADS	10.1504/IJADS.2009.025377	economics;computer science;operations management;data envelopment analysis;mathematics;operations research;welfare economics	Web+IR	5.0438591562321955	-10.381579232939487	10508
5c9fb943c523bb0b75826c129b1aa90f486f7ac6	neighborhood features help detecting electricity theft in big data sets		Electricity theft is a major problem around the world in both developed and developing countries and may range up to 40% of the total electricity distributed. More generally, electricity theft belongs to non-technical losses (NTL), which are losses that occur during the distribution of electricity in power grids. In this paper, we build features from the neighborhood of customers. We first split the area in which the customers are located into grids of different sizes. For each grid cell we then compute the proportion of inspected customers and the proportion of NTL found among the inspected customers. We then analyze the distributions of features generated and show why they are useful to predict NTL. In addition, we compute features from the consumption time series of customers. We also use master data features of customers, such as their customer class and voltage of their connection. We compute these features for a Big Data base of 31M meter readings, 700K customers and 400K inspection results. We then use these features to train four machine learning algorithms that are particularly suitable for Big Data sets because of their parallelizable structure: logistic regression, k-nearest neighbors, linear support vector machine and random forest. Using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for Big Data sets for varying NTL proportions of 1%90%. This work can therefore be deployed to a wide range of different regions around the world. Keywords-Data Mining, Electricity Theft Detection, Feature Engineering, Feature Selection, Machine Learning, NonTechnical Losses, Time Series Classification.	big data;data mining;feature engineering;feature selection;k-nearest neighbors algorithm;logistic regression;machine learning;ntl;random forest;sensor;support vector machine;time series	Patrick O. Glauner;Jorge Augusto Meira;Lautaro Dolberg;Radu State;Franck Bettinger;Yves Rangoni;Diogo Duarte	2016	CoRR		simulation;machine learning;data mining;statistics	ML	1.579761115646923	-33.68990528157513	10510
ca8d33b48a36f8e0936ec80e15089b3e9fbd6972	knowledge discovery in large spatial databases: focusing techniques for efficient class identification	database system;spatial database;protein protein docking;satellite image;knowledge discovery in database;spatial access method;knowledge discovery	Both, the number and the size of spatial databases are rapidly growing because of the large amount of data obtained from satellite images, X-ray crystallography or other scientific equipment. Therefore, automated knowledge discovery becomes more and more important in spatial databases. So far, most of the methods for knowledge discovery in databases (KDD) have been based on relational database systems. In this paper, we address the task of class identification in spatial databases using clustering techniques. We put special emphasis on the integration of the discovery methods with the DB interface, which is crucial for the efficiency of KDD on large databases. The key to this integration is the use of a well-known spatial access method, the R*-tree. The focusing component of a KDD system determines which parts of the database are relevant for the knowledge discovery task. We present several strategies for focusing: selecting representatives from a spatial database, focusing on the relevant clusters and retrieving all objects of a given cluster. We have applied the proposed techniques to real data from a large protein database used for predicting protein-protein docking. A performance evaluation on this database indicates that clustering on large spatial databases can be performed, both, efficiently and effectively.	algorithm;class consciousness;cluster analysis;computer-aided design;data mining;datasheet;decibel;docking (molecular);feature vector;fuzzy clustering;geographic information system;heuristic;macromolecular docking;medoid;nethack;pattern recognition;performance evaluation;polyhedron;protein data bank;r* tree;randomness;relational database;sdb (debugger);sequence database;spatial database	Martin Ester;Hans-Peter Kriegel;Xiaowei Xu	1995		10.1007/3-540-60159-7_5	geography;bioinformatics;data science;data mining;spatiotemporal database	ML	4.527985828323225	-46.2989675689292	10512
e602ec6fe4b7f30d7f5bcea324386b1b418a03a3	three-state financial distress prediction based on support vector machine	multinomial logit analysis mla;support vector machine svm;multinomial logit;financial distress;financial state;support vector machine	This paper examines the three-state financial distress prediction using support vector machine (SVM) and compares the classification results with the one using multinominal logit analysis(MLA).The results show that SVM provides better three-state classification than MLA. The model using SVM has better generalization than the model using MLA.	distress (novel);support vector machine;three-state logic	Hongshan Yao	2009		10.1007/978-3-642-01510-6_47	support vector machine;econometrics;computer science;machine learning;pattern recognition;relevance vector machine;structured support vector machine;multinomial logistic regression	ML	9.78012047477594	-25.988156572602236	10525
685ae67edb46d51286b711b487a857c2696829f7	obstacle detection with a photonic mixing device-camera in autonomous vehicles	pmd camera;obstacle detection;autonomous vehicle;vehicle steering;photonic mixing device;steering commands;behaviour network;obstacle avoidance;collision avoidance;autonomous vehicles	In autonomous vehicles as well as in modern driver assistance systems, obstacle detection shows to be the most important task to be achieved. This paper presents a collision avoidance system, based on a modern Time-Of-Flight camera. These cameras allow a 3D perception of the environment, in which obstacles can be detected, independent of special features. Thus, the system is capable of all kinds of objects, including pedestrians as well as bicycles or vehicles. The used Photonic Mixing Device (PMD) camera has a measurement range of up to 50 m. The system is integrated into an autonomous vehicle, on which detected obstacles are investigated in detail. The vehicle steering commands are then generated by a behaviour network, depending on the presence of obstacles in the driving lane.	apriori algorithm;autonomous robot;donald becker;experiment;kalman filter;pmd;region growing;stereoscopy;technical support;time-of-flight camera	Thomas Schamm;Johann Marius Zöllner;Stefan Vacek;Joachim Schröder;Rüdiger Dillmann	2008	IJISTA	10.1504/IJISTA.2008.021294	control engineering;computer vision;simulation;computer science;engineering;artificial intelligence;obstacle avoidance	Robotics	50.30146589507655	-36.6656288392166	10544
bf589741cbe1c7958e95e5a18194f18be9addfdb	forecasting uncertain hotel room demand	economic system	Mihir Rajopadhye, Mounir Ben Ghaliay and Paul P. Wang Department of Electrical and Computer Engineering Duke University Durham, North Carolina 27708-0291 Timothy Baker and Craig V. Eister Bass Hotels and Resorts Atlanta, Georgia 30346-2149 Abstract Economic systems are characterized by increasing uncertainty in their dynamics. This increasing uncertainty is likely to incur bad decisions that can be costly in nancial terms. This makes forecasting of uncertain economic variables an instrumental activity in any organization. This paper takes the hotel industry as a practical application of forecasting using the Holt-Winters method. The problem here is to forecast the uncertain demand for rooms at a hotel for each arrival day. Forecasting is part of hotel revenue management system whose objective is to maximize the revenue by making decisions regarding when to make rooms available for customers and at what price. The forecast approach discussed in this paper is based on quantitative models and does not incorporate management expertise. Even though, forecast results are found to be satisfactory for certain days, this is not the case for other arrival days. It is believed that human judgment is important when dealing with external events that may a ect the variables being forecasted. Actual data from a hotel are used to illustrate the forecasting mechanism.	beneath a steel sky;computer engineering;mihir bellare	Mihir Rajopadhye;Mounir Ben Ghalia;Paul P. Wang;Timothy Baker;Craig V. Eister	2001	Inf. Sci.	10.1016/S0020-0255(00)00082-7	actuarial science;demand forecasting;computer science	AI	1.5435579904545464	-13.275928859702246	10555
2001b3d1c6f56ee00bce1ea0464e633250f14f6b	investigation of the structure of a networked system	sensitivity;structure	We present a general approach to investigate the Structure of a Networked System by ranking its elements through additive, first order, Importance and Sensitivity measures. The performances of the System are defined in term of Partial and Global Risks, which are referred to each User node and to the whole Network respectively. The ranking of the Edges (subjected to failure and repair events) by means of additive measures allows to rank the (unfaultable) User nodes. The System’s Structure is investigated with reference to the whole range [0,1] of the input variables. The interactions among them are investigated by analyzing the trend of the Total order importance measure for different values of the finite change of the input variables.	interaction;performance;utility functions on indivisible goods	Stefano La Rovere;Paolo Vestrucci	2012	Rel. Eng. & Sys. Safety	10.1016/j.ress.2012.06.013	reliability engineering;engineering;data mining;statistics	ML	-0.3406370256803511	-20.226895047142698	10561
e228eb2c2726f05a26337adfeebb521a85dafa70	a low-granularity classifier for data streams with concept drifts and biased class distribution	extraction information;modelizacion;estimacion sesgada;association statistique;grain size;ajustamiento modelo;concept drift;theorie locale;tratamiento transaccion;streaming;distribution donnee;mise a jour;local theory;data stream mining low granularity data streams classifier biased class distribution concept drifts;analisis datos;information extraction;transmision continua;data stream;concept drifts;evenement rare;securite informatique;statistical association;flux donnee;flujo datos;data mining;classification;data distribution;actualizacion;computer security;ajustement modele;modelisation;accuracy;data analysis;transmission en continu;asociacion estadistica;adaptation model;pattern classification data analysis data mining;fraud;association rule;rare event;fouille donnee;grosor grano;association rule classification data stream concept drift;seguridad informatica;forth;model matching;data stream mining;acontecimiento rara;classification algorithms;prediction accuracy;intrusion detection systems;pattern classification;low granularity data streams classifier;analyse donnee;predictive models;costs data mining monitoring accuracy predictive models training data decision trees association rules ubiquitous computing feedback;model updating;teoria local;fraude;synthetic data;transaction processing;data flow;decision trees;modeling;biased estimation;distribucion dato;busca dato;estimation biaisee;clasificacion;extraccion informacion;systeme detection intrusion;biased class distribution;updating;traitement transaction;data models;grosseur grain	Many applications track streaming data for actionable alerts, which may include, for example, network intrusions, transaction frauds, bio-surveilence abnormalities, and so forth. Some stream classification models are built for this purpose. Due to concept drifts, maintaining a model's up-to-dateness has become one of the most challenging tasks in mining data streams. State-of-the-art approaches, including both the incrementally updated classifiers and the ensemble classifiers, have proved that model update is a very costly process. In this paper, we show that reducing model granularity reduces the update cost, as models of fine granularity enable us to efficiently pinpoint local components in the model that are affected by the concept drift. It also enables us to derive new model components to reflect the current data distribution, thus avoiding expensive updates on a global scale. Furthermore, those actionable alerts being monitored are usually rare occurrences. The existing stream classifiers cannot handle this problem. We address this problem and show that the low-granularity classifier handles rare events on stream data with ease. Experiments on real and synthetic data show that our approach is able to maintain good prediction accuracy at a fraction of the model updating cost of state-of-the-art approaches.	british informatics olympiad;concept drift;ensemble kalman filter;low-rank approximation;rare events;streaming media;synthetic data	Peng Wang;Haixun Wang;Xiaochen Wu;Wei Wang;Baile Shi	2007	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2007.1057	intrusion detection system;data modeling;data flow diagram;association;systems modeling;association rule learning;transaction processing;biological classification;computer science;artificial intelligence;concept drift;machine learning;decision tree;data mining;database;accuracy and precision;predictive modelling;data stream mining;data analysis;forth;information extraction;statistics;grain size;synthetic data	DB	-2.5891938422759826	-33.347479216042764	10572
7d191f48f620e67984308addeca55b644348e12d	evaluation model for vehicle shift quality based on evidence theory and fnn	shift quality;power transmission mechanical;road vehicle shift quality evaluation model;fuzzy neural network;evidential reasoning road vehicle shift quality evaluation model evidence theory fuzzy neural network training control strategy self learning ability fuzzy logic automatic transmission;fuzzy neural nets;automobiles;fuzzy neural network training;self learning ability;training;evidence theory;fnn evaluation vehicle shift quality evidence theory;vehicle dynamics automobile industry automobiles case based reasoning fuzzy logic fuzzy neural nets learning artificial intelligence power transmission mechanical;evidential reasoning;fuzzy logic;fnn;artificial neural networks;computational modeling;evaluation metric;quality evaluation;cognition;automatic transmission;driver circuits;evaluation;vehicles;fuzzy neural networks instruments computational intelligence intelligent vehicles reliability theory aggregates fuzzy systems neural networks conferences computer industry;case based reasoning;learning artificial intelligence;vehicle;fuzzy neural networks;subjective evaluation;vehicle dynamics;automobile industry;control strategy;evaluation model	To achieve the shift quality evaluation effectively, reasonably and timely, a new model to evaluate shift quality based on evidence theory and fuzzy neural network (FNN) is proposed in the paper. A method combining subjective evaluation and objective evaluation is adopted. Subjective evaluation provided by various drivers is aggregated by means of evidence theory, which can improve the reliability of subjective evaluation. Objective evaluation metrics measured by instrument and the corresponding subjective evaluation are self-learned and trained with FNN. Shift quality evaluation system is established, which is used to convert evaluation metric to subjective rating. And the correlation between the developed evaluation model and subjective ratings given by drivers is proved. Computational and experimental results show that the proposed model is available and promising. The proposed model is a theory base for developing control strategies and improving shift quality.	artificial neural network;computation;evaluation function;neuro-fuzzy;software release life cycle;transportation science	Gang Chen;Weigong Zhang;Zongyang Gong;Wei Sun	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.118	fuzzy logic;case-based reasoning;vehicle dynamics;simulation;cognition;computer science;artificial intelligence;evaluation;machine learning;evidential reasoning approach;computational model;automatic transmission;artificial neural network	NLP	5.143465645313501	-23.7143033272536	10573
e4edc414773e709e8eb3eddd77b519637f26f9a5	scale out for large minibatch sgd: residual network training on imagenet-1k with improved accuracy and reduced time to train		For the past 5 years, the ILSVRC competition and the ImageNet dataset have attracted a lot of interest from the Computer Vision community, allowing for state-of-the-art accuracy to grow tremendously. This should be credited to the use of deep artificial neural network designs. As these became more complex, the storage, bandwidth, and compute requirements increased. This means that with a non-distributed approach, even when using the most high-density server available, the training process may take weeks, making it prohibitive. Furthermore, as datasets grow, the representation learning potential of deep networks grows as well by using more complex models. This synchronicity triggers a sharp increase in the computational requirements and motivates us to explore the scaling behaviour on petaflop scale supercomputers. In this paper we will describe the challenges and novel solutions needed in order to train ResNet50 in this large scale environment. We demonstrate above 90% scaling efficiency and a training time of 28 minutes using up to 104K x86 cores. This is supported by software tools from Intel’s ecosystem. Moreover, we show that with regular 90 120 epoch train runs we can achieve a top-1 accuracy as high as 77% for the unmodified ResNet-50 topology. We also introduce the novel Collapsed Ensemble (CE) technique that allows us to obtain a 77.5% top-1 accuracy, similar to that of a ResNet-152, while training a unmodified ResNet-50 topology for the same fixed training budget. All ResNet-50 models as well as the scripts needed to replicate them will be posted shortly. Keywords—deep learning, scaling, convergence, large minibatch, ensembles.	artificial neural network;baseline (configuration management);central processing unit;computer vision;deep learning;ecosystem;ensemble learning;flops;feature learning;flow network;image scaling;imagenet;knights;reinforcement learning;requirement;scalability;self-replication;server (computing);skylake (microarchitecture);snapshot (computer storage);supercomputer;synchronicity;x86	Valeriu Codreanu;Damian Podareanu;Vikram A. Saletore	2017	CoRR		residual;artificial intelligence;scalability;replicate;machine learning;scaling;synchronicity;x86;artificial neural network;computer science;feature learning	ML	17.278285902436686	-32.93859264282313	10577
6a560565e3e16a39ea3c537c3fdfe6c26238e708	optimal consumption and arbitrage in incomplete, finite state security markets	optimal solution;portfolio selection;duality;utility function;control problem;linear programming;linear program;trading constraints;optimal portfolio;incomplete market	We study a consistent treatment for both the multi-period portfolio selection problem and the option attainability problem by a dual approach. We assume that time is discrete, the horizon is finite, the sample space is finite and the number of securities is less than that of the possible securities price transitions, i.e. an incomplete security market. The investor is prohibited from investing stocks more than given linear investment amount constraints at any time and he maximizes an expected additive utility function for the consumption process. First we give a set of budget feasibility conditions so that a consumption process is attainable by an admissible portfolio process. To establish this relation, we used an algorithmic approach which has a close connection with the linear programming duality. Then we prove the unique existence of a primal optimal solution from the budget feasibility conditions. Finally, we formulate a dual control problem and establish the duality between primal and dual control problems.	linear programming;microsoft office project portfolio server;selection algorithm;utility functions on indivisible goods	Hiroshi Shirakawa;Hiromichi Kassai	1993	Annals OR	10.1007/BF02282058	financial economics;mathematical optimization;duality;economics;linear programming;mathematics;microeconomics;welfare economics;incomplete markets	Theory	1.2963716193172854	-2.170494977284629	10584
c5fdef7828bd562df9da6d914c64662f6795769a	coherent steps of mobile sensing agents in gaussian scalar fields	multirobot experiment mobile sensing agent gaussian scalar field noisy scalar field false walk explorable probability gradient climbing level curve tracking noise variance;probability;tracking gaussian processes mobile robots probability;gaussian processes;mobile robots;noise equations noise measurement robot sensing systems mathematical model;tracking	This paper develops fundamental theoretical results to describe exploration behaviors of mobile sensing agents in a noisy scalar field. We introduce concepts of coherent steps and incoherent steps of the sensing agents and prove that a step of an agent is coherent if and only if the probability of a false-walk is less than a threshold determined by the explorable probability. Among all possible coherent steps, gradient climbing and level curve tracking are the steps that can achieve local supremum explorable probabilities. We also prove that by increasing the number of agents that are collaborating, the incoherent steps of the collaborating groups can become coherent. Based on estimates of the noise variance, we propose an algorithm that estimates the minimum number of agents required to guarantee coherent steps and implement a strategy that allows the sensing agents to self-organize into groups with the estimated minimum number of agents. Results are demonstrated in multi-robot experiments.	algorithm;coherent;experiment;gradient;hill climbing;self-organization	Wencen Wu;Fumin Zhang	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6426609	mobile robot;simulation;theoretical computer science;machine learning;probability;gaussian process;mathematics;tracking;statistics	Robotics	53.124238559801036	3.1448322214289703	10590
51dd10caadd4d2f52f11cfac36a4550857f6a68c	discrete hopfield network combined with estimation of distribution for unconstrained binary quadratic programming problem	unconstrained binary quadratic programming problem;scatter search;quadratic program;hopfield network;benchmark problem;combinatorial optimization problem;simulated annealing;journal;hopfield neural network;memetic algorithm;unified model;np hard problem;estimation of distribution algorithm;tabu search;probability model;local minima;discrete hopfield neural network;estimation of distribution	Unconstrained binary quadratic programming problem (UBQP) consists in maximizing a quadratic 0-1 function. It is a well known NP-hard problem and is a unified model for a variety of combinatorial optimization problems. This paper presents a discrete Hopfield neural network (DHNN) combined with estimation of distribution algorithm (EDA) for the UBQP. The idea of EDA is combined with the DHNN in order to overcome the local minima problem of the network. Once the network is trapped in local minima, the perturbation based on EDA can generate a new starting point for the DHNN for further search, which is in a promising area characterized by a probability model. Thus, the proposed algorithm, named DHNN-EDA, can escape from local minima and further search better results. The DHNN-EDA is tested on a large number of benchmark problems with size up to 7000 variables. Simulation results on the UBQP show that the DHNN-EDA is better than the other improved DHNN algorithms such as multi-start DHNN and DHNN with random flips, and is better than or competitive with metaheuristic algorithms such as simulated annealing, tabu search, scatter search and memetic algorithm.	hopfield network;quadratic programming	Jiahai Wang	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.02.032	mathematical optimization;combinatorics;simulated annealing;estimation of distribution algorithm;tabu search;computer science;machine learning;maxima and minima;unified model;np-hard;mathematics;hopfield network;quadratic programming;memetic algorithm	ML	25.482307792133966	-0.860997372040011	10608
271df16c554a014974a442541660fb08ecd1836d	human-driven feature selection for a robotic agent learning classification tasks from demonstration		The state features available to a robot define the variables on which the learning computation depends. However, little prior work considers feature selection in the context of deploying a general-purpose robot able to learn new tasks. In this work, we explore human-driven feature selection in which a robotic agent can identify useful features with the aid of a human user, by extracting information from users about which features are most informative for discriminating between classes of objects needed for a given task (e.g. sorting groceries). The research questions examine (a) whether a domain expert is able to identify a subset of informative task features, (b) whether human selected features will enable the agent to classify unseen examples as accurately as using computational feature selection, and (c) if the interaction strategy used to elicit the information from the user impacts the quality of resulting feature selection. Toward that end, we conducted a user study with 30 participants on campus, given a multi-class classification task and one of five different approaches for conveying information about informative features to a robot learner. Our findings show that when features are semantically interpretable, human feature selection is effective in LfD scenarios because it is able to outperform computational methods when there is limited training data, yet still remains on-par with computational methods as the training sample size increases.	autonomous robot;baseline (configuration management);computation;emoticon;feature selection;general-purpose markup language;information;multiclass classification;selection algorithm;sorting;subject-matter expert;usability testing	Kalesha Bullard;Sonia Chernova;Andrea Lockerd Thomaz	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8461012	subject-matter expert;support vector machine;task analysis;sample size determination;feature selection;control engineering;computation;feature extraction;machine learning;engineering;artificial intelligence;sorting	Robotics	16.351014808164148	-35.44203992492972	10614
14e0743cff4cd7124724ec80d6b53bdb488bf33e	neural structures for visual motion tracking	motion tracking;neural networks;network ensembles;real-time systems	This paper addresses visual motion tracking by a connectionist method, and aims at showing how the flexibility and the generalization power of neural networks can enhance a tracking system's adaptiveness and effectiveness. The simple principle of operation widens the range of applicability. A set of tracking structures that exhibit increasing levels of integration and efficiency are described. We also show how multinetwork architectures for estimate averaging may greatly increase tracking stability. The validity of the basic mechanism was assessed on a simple domain; however, a specific difficult testbed made it possible to verify the effectiveness of the method.	artificial neural network;connectionism;issue tracking system;testbed	Davide Anguita;Giancarlo Parodi;Rodolfo Zunino	1995	Machine Vision and Applications	10.1007/BF01211489	generalization;simulation;computer science;artificial intelligence;machine learning;artificial neural network	Vision	48.97012422416544	-30.685925635777334	10620
871a1ecafbf93bdf3a580716392e3d344992e11b	design and analysis of a novel weightless artificial neural based multi-classifier	tk7882 p3 pattern recognition	(MCS), but this has rarely incorporated any utilisation of weightless neural systems(WNS) as the combiner of an MCS ensemble. This paper explores the application of weightless networks within the multi-classifier environment by introducing an intelligent multi-classifier system using a WNS called the Enhanced Probabilistic Convergent Neural Networks (EPCN). The paper explores the use of EPCN by illustrating its major features, such as the specification of disjoint or overlapping input subset to the MCS, and the inherently parallel nature of the design. Within the proposed system, the number of base classifiers per MCS could be specified manually or automatically.. The proposed MCS is problem-domain independent and, our investigation is performed on handwritten characters. The proposed MCS is adaptive, its combiner is capable of extracting absolute or weighted classification decision(output) from base classifier. Diversity is increased in the base classifier by injecting randomness into the system parameters. Two types of EPCN classifiers are proposed, fix-PCN and rand-PCN. These PCNs are independent and orthogonal. One uses a fixed method of forming connectivity while the other uses random method of forming connectivity. In order to verify the performance of the recognition system, tests were performed, off-line, on benchmark datasets of unconstrained handwritten numerals. Experimental results suggest that MCS outperforms single EPCN in classification of handwritten characters.	benchmark (computing);diplexer;learning classifier system;multi categories security;neural networks;online and offline;program composition notation;rand index;randomness;weightless (wireless communications)	Pierre Lorrentz;W. Gareth J. Howells;Klaus D. McDonald-Maier	2007			speech recognition;computer science;artificial intelligence;machine learning	ML	10.787078159043942	-41.72299826370056	10621
5c56b0a0897917436c9a288b74cf10c8c9f97ddd	dynamic ecological system analysis		In this article, a new mathematical method for the dynamic analysis of nonlinear compartmental systems is developed in the context of ecology. The method is based on the novel dynamic system and subsystem partitioning methodologies through which compartmental systems are decomposed to the utmost level. The dynamic system and subsystem partitioning enable the determination of the distribution of environmental inputs and intercompartmental system flows as well as the organization of the associated storages generated by these inputs and flows individually and separately within the system. Moreover, the transient and the dynamic direct, indirect, acyclic, cycling, and transfer (diact) flows and associated storages transmitted along a given flow path or from one compartment, directly or indirectly, to any other are analytically characterized, systematically classified, and mathematically formulated. Major flow- and stock-related concepts and quantities of the current static network analyses are also extended to nonlinear dynamic settings and integrated with the proposed dynamic measures and indices within the proposed unifying mathematical framework. This comprehensive methodology enables a holistic view and analysis of ecological systems.	directed acyclic graph;dynamical system;ecology;ecosystem;holism;multi-compartment model;nonlinear system	Huseyin Coskun	2018	CoRR	10.31219/osf.io/35xkb		Visualization	12.851296375782757	-7.030699851534851	10649
a4b22738e93ab3599a2f2ffbd8e433b0583f2ccb	neural networks in astronomy	mlp;analisis componente principal;self organizing maps;fuzzy set;neural networks;analisis datos;astronomia;bayes methods;methode bayes;soft computing;multilayer perceptrons;astronomie;astronomical database;national virtual observatory;time series;data mining;classification;virtual observatory;carte autoorganisatrice;artificial intelligent;perceptron multicouche;detection objet;data analysis;time series analysis;fouille donnee;bayesian learning;principal component analysis;object extraction;serie temporelle;analyse composante principale;serie temporal;analyse donnee;genetic algorithm;self organized map;data reduction;astronomy;busca dato;music;pca;clasificacion;object detection;neural network	In the last decade, the use of neural networks (NN) and of other soft computing methods has begun to spread also in the astronomical community which, due to the required accuracy of the measurements, is usually reluctant to use automatic tools to perform even the most common tasks of data reduction and data mining. The federation of heterogeneous large astronomical databases which is foreseen in the framework of the astrophysical virtual observatory and national virtual observatory projects, is, however, posing unprecedented data mining and visualization problems which will find a rather natural and user friendly answer in artificial intelligence tools based on NNs, fuzzy sets or genetic algorithms. This review is aimed to both astronomers (who often have little knowledge of the methodological background) and computer scientists (who often know little about potentially interesting applications), and therefore will be structured as follows: after giving a short introduction to the subject, we shall summarize the methodological background and focus our attention on some of the most interesting fields of application, namely: object extraction and classification, time series analysis, noise identification, and data mining. Most of the original work described in the paper has been performed in the framework of the AstroNeural collaboration (Napoli-Salerno).		Roberto Tagliaferri;Giuseppe Longo;Leopoldo Milano;Fausto Acernese;Fabrizio Barone;Angelo Ciaramella;Rosario De Rosa;Ciro Donalek;Antonio Eleuteri;Giancarlo Raiconi;Salvatore Sessa;Antonino Staiano;Alfredo Volpicelli	2003	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(03)00028-5	computer science;artificial intelligence;machine learning;time series;data mining;artificial neural network;statistics;principal component analysis	DB	6.321686020974702	-33.241258023170026	10651
ccba6298127dd15a472196b3a157d1048c63514f	game-tree search over high-level game states in rts games	starcraft;ai;rts;real time strategy games;bwapi;game tree search;bot	From an AI point of view, Real-Time Strategy (RTS) games are hard because they have enormous state spaces, they are real-time and partially observable. In this paper, we present an approach to deploy gametree search in RTS games by using game state abstraction. We propose a high-level abstract representation of the game state, that significantly reduces the branching factor when used for game-tree search algorithms. Using this high-level representation, we evaluate versions of alpha-beta search and of Monte Carlo Tree Search (MCTS). We present experiments in the context of StarCraft showing promising results in dealing with the large branching factors present in RTS games.	alpha–beta pruning;branching factor;experiment;high- and low-level;iteration;level of detail;monte carlo method;monte carlo tree search;partially observable system;point of view (computer hardware company);real-time clock;real-time transcription;search algorithm;simulation;starcraft;video game developer	Alberto Uriarte;Santiago Ontañón	2014			combinatorial game theory;simulation;computer science;artificial intelligence;monte carlo tree search;sequential game;algorithm	AI	19.22527711529208	-17.02660711499894	10653
06faf15f713c3d3afc62ac5b92cceb6c40cb6feb	a general pascal program for map overlay of quadtrees and related problems	maps;pascal language;quad tree;image processing;mapa;quad arbol;recubrimiento;procesamiento imagen;overlay;recouvrement;functional programming;carte;traitement image;algorithme;algorithm;algorritmo;estructura datos;quad arbre;structure donnee;programmation fonctionnelle;pascal;geografia;geographie;programacion funcional;data structure;geography			F. Warren Burton;Vassiliki J. Kollias;John G. Kollias	1987	Comput. J.	10.1093/comjnl/30.4.355	pascal;data structure;image processing;computer science;quadtree;overlay;programming language;functional programming;algorithm;pascal;computer graphics (images)	Theory	39.306002612063544	-19.27510196749656	10693
9035dc898c10b203589699063908073b2f1d2e2a	pop: person re-identification post-rank optimisation	image recognition;optimisation;human computer interaction;human computer interaction visual surveillance person re identification ranking manifold information retrieval;information retrieval;person re identification;optimisation image recognition;visual surveillance;ranking;i lids dataset person reidentification post rank optimisation visual ambiguity visual disparity exhaustive human eyeballing ranking performance time consuming post rank visual search error prone post rank visual search one shot post rank optimization method pop method sparse negative selections systematic behavioural study searching behaviour state of the art distance metric learning one shot feedback optimisation performance improvement viper dataset;probes visualization cameras optimization vegetation context training;manifold	"""Owing to visual ambiguities and disparities, person re-identification methods inevitably produce sub optimal rank-list, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likely-candidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank Optimization (POP) method, which allows a user to quickly refine their search by either """"one-shot"""" or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user's searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the state-of-the-art distance metric learning based ranking models, even with just """"one shot"""" feedback optimisation, by as much as over 30% performance improvement for rank 1 re-identification on the VIPeR and i-LIDS datasets."""	brute-force search;cognitive dimensions of notations;converge;linux intrusion detection system;mathematical optimization;post-hartree–fock;shadow volume;sparse matrix	Chunxiao Liu;Chen Change Loy;Shaogang Gong;Guijin Wang	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.62	computer vision;manifold;ranking;computer science;machine learning;data mining;mathematics	Vision	34.32892333557719	-45.33925145232098	10694
077a3b834207104836044e81e487422ddc57902b	local detection and estimation of multiple objects from images with overlapping observation areas		We propose a method for detecting and estimating multiple objects from multiple noisy images with partly overlapping observation areas. The goal is to detect the objects that are “locally” present in the individual observation areas and to estimate their states. Our method is based on a new closed-form expression of the marginal posterior probability hypothesis density (PHD) and admits a distributed implementation. Simulation results demonstrate performance gains over correlation-based and PHD-based methods that do not take advantage of the overlapping observation areas.		Rene Repp;Günther Koliander;Florian Meyer;Franz Hlawatsch	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953036	artificial intelligence;mathematical optimization;probability density function;pattern recognition;posterior probability;mathematics	Robotics	46.26442214767718	-50.94695200830171	10699
495534c4c195946394fea99cc3406da7b607bb77	unsupervised discretization using tree-based density estimation	discretisation;fonction vraisemblance;base donnee;score function;discrete data;unsupervised discretization;medicion densidad;estimacion densidad;density measurement;analisis datos;point coupure;punto corte;estimation densite;database;discretization;base dato;discretizacion;cut point;data mining;funcion verosimilitud;conference contribution;density estimation;data analysis;it value;histogram;histogramme;decouverte connaissance;mesure densite;descubrimiento conocimiento;analyse donnee;cross validation;computer science;histograma;likelihood function;knowledge discovery	This paper presents an unsupervised discretization method that performs density estimation for univariate data. The subintervals that the discretization produces can be used as the bins of a histogram. Histograms are a very simple and broadly understood means for displaying data, and our method automatically adapts bin widths to the data. It uses the log-likelihood as the scoring function to select cut points and the cross-validated log-likelihood to select the number of intervals. We compare this method with equal-width discretization where we also select the number of bins using the cross-validated log-likelihood and with equal-frequency discretization.	algorithm;discrete mathematics;discretization;kernel density estimation;naive bayes classifier;scoring functions for docking	Gabi Schmidberger;Eibe Frank	2005		10.1007/11564126_26	discretization error;computer science;machine learning;discretization;mathematics;knowledge extraction;discretization of continuous features;algorithm;statistics	Robotics	8.52902054117099	-35.09669784927005	10702
d05cbd657c8e949d7ecbcc0edb712101b4d9ae94	enhanced video compression with region-based texture models	texture;dynamics bit rate codecs video coding heuristic algorithms image edge detection pixel;texture warping;codecs;data compression;texture synthesis;video compression;motion estimation;bit rate;inloop video quality assessment;visual quality;synthesis;image texture;region based texture model;h 264 video coding video compression region based texture model texture warping translational motion estimation texture synthesis temporal artefact inloop video quality assessment;video coding;temporal artefact;dynamic texture;image edge detection;dynamics;heuristic algorithms;pixel;h 264 video coding;compression;video;synthesis video compression texture warping;video coding data compression image texture motion estimation;warping;video quality assessment;translational motion estimation	This paper presents a region-based video compression algorithm based on texture warping and synthesis. Instead of encoding whole images or prediction residuals after translational motion estimation, this algorithm employs a perspective motion model to warp static textures and uses a texture synthesis approach to synthesise dynamic textures. Spatial and temporal artefacts are prevented by an in-loop video quality assessment module. The proposed method has been integrated into an H.264 video coding framework. The results show significant bitrate savings, up to 55%, compared with H.264, for similar visual quality.	algorithm;data compression;h.264/mpeg-4 avc;motion estimation;texture synthesis	Fan Zhang;David R. Bull	2010	28th Picture Coding Symposium	10.1109/PCS.2010.5702560	video compression picture types;data compression;computer vision;quarter-pixel motion;computer science;block-matching algorithm;multimedia;motion compensation;texture compression;statistics;multiview video coding;computer graphics (images)	Vision	45.011501093150876	-18.755576022133262	10719
0c8e691c5a07421cb57d6b70d4037a27d5f5323f	the density of states - a measure of the difficulty of optimisation problems	fitness landscape;density of state	We introduce a classifying measure of tness landscapes-the density of states-for continuous and discrete problems, especially optimisation of sequences and graphs. By means of the Boltzmann strategy we obtain a simple algorithm to calculate the density of states for a given problem. Knowing the density of states we are able to approximate the optimal tness value of the problem which makes it feasible to assess the eeectivity of practical optimisations.	approximation algorithm;mathematical optimization	Helge Rosé;Werner Ebeling;Torsten Asselmeyer	1996		10.1007/3-540-61723-X_985	mathematical optimization;fitness landscape;computer science;mathematics;density of states	ML	27.19988517768307	3.1045047059986737	10721
d6219487ae16f1a0580fed16f801f15b6aa0c7df	a robust method for detecting planar regions based on random sampling using distributions of feature points	homography;random sampling;ransac	We propose a robust method for detecting local planar regions in a scene with an uncalibrated stereo. Here, we assume that the correspondences between the two images have been established. Our method is based on RANSAC for estimating homographies to the local planar regions in the scene. For doing this, we adopt double random sampling scheme by a uniform distribution and the local probability distribution of each pair, which is defined by the distances from the point to the others in one image. We first choose a pair as a seed by the uniform distribution, and then choose four pairs by the local probability distribution with respect to the seed. By introducing the local probability distribution, we can efficiently choose four potentially coplanar pairs in the scene. The same scheme can be applicable to detect line segments in an image. We demonstrate that our method is robust to the outliers in a scene by simulations and real image examples. © 2006 Wiley Periodicals, Inc. Syst Comp Jpn, 37(4): 11–22, 2006; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.20492	boson sampling;digi-comp i;homography (computer vision);image registration;john d. wiley;monte carlo method;random sample consensus;sampling (signal processing);sensor;simulation	Hiroshi Kawakami;Yoshihiro Ito;Yasushi Kanazawa	2006	Systems and Computers in Japan	10.1002/scj.20492	sampling;computer vision;ransac;homography;machine learning;mathematics;geometry;statistics	Vision	49.58866167757241	-50.77054340536132	10724
bbc4260a2b6adfc16e16295fe853bea1ed4d4202	a novel approach for synthesis of cost-optimal heat exchanger networks	optimal solution;heat exchanger network;benchmark problem;sequential quadratic programming;cost optimization;hybridization;heat exchanger network synthesis;optimization;harmony search	This study presents a new hybrid methodology for synthesis of cost-optimal heat exchanger networks (HENs). The problem is solved in a two-level approach. The upper level generates the structure of HENs using harmony search (HS) algorithm. To evaluate the minimum cost of each structure, it is sent to the lower level in which the heat load of units and stream-split fractions are optimized by a combination of the HS and sequential quadratic programming (SQP). Based on the cost obtained for each structure HS ranks the HENs and produces new structures until algorithm converges to optimal solution. For validation purpose, three benchmark problems are examined and also a real-world industrial-sized problem is solved to demonstrate the applicability of this method for large-sized problems. The results of this study show that the new approach is able to find more economical networks than those generated by other methods.	cost efficiency	R. Mohammadhasani Khorasany;M. Fesanghary	2009	Computers & Chemical Engineering	10.1016/j.compchemeng.2008.12.004	orbital hybridisation;mathematical optimization;harmony search;computer science;engineering;mathematics;sequential quadratic programming;algorithm	Logic	17.049330307829496	-2.3045288867596723	10726
23b2db1ff9f956086f1e7bdec229c77e2707cd98	a multiple objective particle swarm optimization approach using crowding distance and roulette wheel	multiple objective particle swarm optimization;social leader;multi objective optimization;roulette wheel;data mining;crowding distance;cognitive leader multiple objective particle swarm optimization crowding distance roulette wheel multiobjective optimization social leader;particle swarm optimization multiobjective optimization;multiple objectives;particle swarm optimization wheels proposals design optimization cultural differences intelligent systems differential equations evolutionary computation algorithm design and analysis search problems;particle swarm optimizer;cognitive leader;lead;particle swarm optimization;hypercubes;multiobjective optimization;optimization;search problems;proposals;particle swarm optimisation	This paper presents a multiobjective optimization algorithm based on Particle Swarm Optimization (MOPSO-CDR) that uses a diversity mechanism called crowding distance to select the social leaders and the cognitive leader. We also use the same mechanism to delete solutions of the external archive. The performance of our proposal was evaluated in five well known benchmark functions using four metrics previously presented in the literature. Our proposal was compared to other four multi objective optimization algorithms based on Particle Swarm Optimization, called m-DNPSO, CSS-MOPSO, MOPSO and MOPSO-CDLS. The results showed that the proposed approach is competitive when compared to the other approaches and outperforms the other algorithms in many cases.	algorithm;archive;benchmark (computing);cascading style sheets;crowding;mathematical optimization;multi-objective optimization;particle swarm optimization;vp/css	Robson A. Santana;Murilo Rebelo Pontes;Carmelo J. A. Bastos Filho	2009	2009 Ninth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2009.73	mathematical optimization;multi-swarm optimization;simulation;meta-optimization;derivative-free optimization;artificial intelligence;multi-objective optimization;metaheuristic	Robotics	25.501358932460217	-2.828480439405802	10747
c174204c3741c29d9be5d38a334e25b6497a42ed	adaptive skip intervals: temporal abstraction for recurrent dynamical models		We introduce a method which enables a recurrent dynamics model to be temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI), is based on the observation that in many sequential prediction tasks, the exact time at which events occur is irrelevant to the underlying objective. Moreover, in many situations, there exist prediction intervals which result in particularly easy-to-predict transitions. We show that there are prediction tasks for which we gain both computational efficiency and prediction accuracy by allowing the model to make predictions at a sampling rate which it can choose itself.		Alexander Neitz;Giambattista Parascandolo;Stefan Bauer;Bernhard Schölkopf	2018			machine learning;computer science;artificial intelligence;prediction interval;sampling (signal processing);abstraction	ML	20.10654429956283	-23.65602759987214	10755
76a233100c0e40358e221b1b055838a0b72d9238	from blind to quantitative steganalysis	embedding change rate;transform coding support vector machines training accuracy feature extraction kernel q factor;jpeg transform;kernel;message length estimation;computer forensics;spatial domain;support vector machines;training;support vector regression;transform coding;support vector machines computer forensics feature extraction message passing regression analysis steganography;quantitative steganalysis;feature vector;state of the art quantitative steganalyzer;accuracy;steganography;regression;general methods;embedding operation;feature extraction;steganographic algorithm;message passing;blind steganalysis;feature vector extraction;regression analysis;support vector machine;blind detector;message length;regression blind steganalysis message length estimation quantitative steganalysis;state of the art quantitative steganalyzer blind steganalysis quantitative steganalysis embedding operation message length forensic tool blind detector support vector regression feature vector extraction embedding change rate steganographic algorithm jpeg transform spatial domain;forensic tool;q factor	A quantitative steganalyzer is an estimator of the number of embedding changes introduced by a specific embedding operation. Since for most algorithms the number of embedding changes correlates with the message length, quantitative steganalyzers are important forensic tools. In this paper, a general method for constructing quantitative steganalyzers from features used in blind detectors is proposed. The core of the method is a support vector regression, which is used to learn the mapping between a feature vector extracted from the investigated object and the embedding change rate. To demonstrate the generality of the proposed approach, quantitative steganalyzers are constructed for a variety of steganographic algorithms in both JPEG transform and spatial domains. The estimation accuracy is investigated in detail and compares favorably with state-of-the-art quantitative steganalyzers.	steganalysis	Tomás Pevný;Jessica J. Fridrich;Andrew D. Ker	2012	IEEE Trans. Information Forensics and Security	10.1109/TIFS.2011.2175918	support vector machine;speech recognition;computer science;theoretical computer science;machine learning;pattern recognition;computer forensics	Crypto	39.64477467998467	-15.414086590874206	10758
6ce815e73150235402655dc97b899e0b0445d052	a study of the nonlinear relation between cpi and international oil price based on str model	education computing;conference;computer science	This paper uses STR (Smooth Transition Regression) model to study the nonlinear relation between CPI in China and international oil price. The results show that the change of CPI in current period has a positive effect to the next period, and the non-linear effect of international oil price almost completely reveals the changing characteristics of CPI.	nonlinear system	Kunming Li;Jianbao Chen	2011		10.1007/978-3-642-23339-5_59	library science;geography;media studies;engineering physics	Arch	3.7369189905778017	-13.73249183144473	10760
575804c9ff457d439e883ae1bbc2678f9eb70fa8	adaptive learning vector quantizers for image compression	learning algorithm;image coding;adaptive codes;image coding vector quantization impedance matching distortion measurement computer science computed tomography testing video compression costs statistical distributions;adaptive vector quantization;image compression;adaptive learning;learning algorithm adaptive learning vector quantizers image compression adaptive vector quantization gold washing code vectors codebook;learning artificial intelligence image coding vector quantisation adaptive codes;vector quantizer;learning artificial intelligence;vector quantisation	We investigate adaptive vector quantization for image compression based the idea of gold-washing. The technique is a mechanism for testing the usefulness of a code vector in a codebook. It thus provides a tool for developing new ways of creating code vectors dynamically based on the input data. In this paper, we propose a new algorithm to quantize an input for which a close enough code vector can not be found. It guarantees that the compressed result is within pre-set distortion. We also use a learning algorithm to produce new code vectors from useful existing ones.	image compression	Jianhua Lin	1996		10.1109/ICIP.1996.560530	computer vision;feature vector;learning vector quantization;image compression;computer science;theoretical computer science;machine learning;mathematics;fractal transform;linde–buzo–gray algorithm;adaptive learning;vector quantization;algorithm	ML	44.31284385381561	-13.675880132468405	10762
a35b664d4cfb96a4e842066281e91c49279cc439	improved particle swarm optimization with wavelet mutation: application to linear phase fir filter design	high pass;convergence;low pass;magnitude response;band pass;evolutionary optimization technique;fir filter;wavelet mutation;ipsowm;band stop;ipso	In this paper, various swarm based algorithms like conventional Particle Swarm Optimization PSO, Improved Particle Swarm Optimization IPSO and another novel Improved Particle Swarm Optimization with Wavelet Mutation IPSOWM have been applied for the optimal design of linear phase FIR filters. Real coded genetic algorithm RGA has also been adopted for the sake of comparison. IPSO uses new definition for the velocity vector. Whereas in addition to the above-mentioned new definition added in IPSO, IPSOWM incorporates a new definition of swarm updating with the help of wavelet mutation based on wavelet theory. Wavelet mutation enhances the PSO to explore the solution space more effectively compared to the other optimization methods. IPSOWM is apparently free from getting trapped at local optima and premature convergence. Low pass LP, high pass HP, band pass BP and band stop BS FIR filters are designed with the proposed IPSOWM and other afore-mentioned algorithms individually for comparative optimization performance. A comparison of simulation results reveals the optimization efficacy of the IPSOWM over the other optimization techniques for the solution of the multimodal, non-differentiable, highly non-linear, and constrained FIR filter design problems.	filter design;finite impulse response;linear phase;particle swarm optimization;wavelet	Suman Kumar Saha;Rajib Kar;Durbadal Mandal;Sakti Prasad Ghoshal	2012	Int. J. Hybrid Intell. Syst.	10.3233/HIS-2012-00158	mathematical optimization;multi-swarm optimization;frequency response;convergence;low-pass filter;computer science;finite impulse response;band-pass filter;high-pass filter	EDA	31.164917570441236	-4.45785748386528	10771
c20eacd6bfbfcd39c5666ff9ce196e8d1c2c82d0	a novel class noise estimation method and application in classification	class noise;learning with noise;会议论文;noise elimination	Noise in class labels of any training set can lead to poor classification results no matter what machine learning method is used. In this paper, we first present the problem of binary classification in the presence of random noise on the class labels, which we call class noise. To model class noise, a class noise rate is normally defined as a small independent probability of the class labels being inverted on the whole set of training data. In this paper, we propose a method to estimate class noise rate at the level of individual samples in real data. Based on the estimation result, we propose two approaches to handle class noise. The first technique is based on modifying a given surrogate loss function. The second technique eliminates class noise by sampling. Furthermore, we prove that the optimal hypothesis on the noisy distribution can approximate the optimal hypothesis on the clean distribution using both approaches. Our methods achieve over 87% accuracy on a synthetic non-separable dataset even when 40% of the labels are inverted. Comparisons to other algorithms show that our methods outperform state-of-the-art approaches on several benchmark datasets in different domains with different noise rates.	approximation algorithm;benchmark (computing);binary classification;image noise;loss function;machine learning;noise (electronics);opaque pointer;sampling (signal processing);signal-to-noise ratio;synthetic data;test set	Lin Gui;Qin Lu;Ruifeng Xu;Minglei Li;Qikang Wei	2015		10.1145/2806416.2806554	value noise;noise measurement;machine learning;pattern recognition;statistics	ML	16.815183399639125	-41.16868918143952	10773
3420c8dbff2e290b6dfb6970c641aaa00373bd43	a multi-objective evolutionary approach to image quality/compression trade-off in jpeg baseline algorithm	jpeg algorithm;multi objective evolutionary algorithm;image compression;mean square error;image quality;compression ratio;multi objective evolutionary algorithms;evolutionary algorithm	The JPEG algorithm is one of the most used tools for compressing images. The main factor affecting the performance of the JPEG compression is the quantization process, which exploits the values contained in two tables, called quantization tables. The compression ratio and the quality of the decoded images are determined by these values. Thus, the correct choice of the quantization tables is crucial to the performance of the JPEG algorithm. In this paper, a two-objective evolutionary algorithm is applied to generate a family of optimal quantization tables which produce different trade-offs between image compression and quality. Compression is measured in terms of difference in percentage between the sizes of the original and compressed images, whereas quality is computed as mean squared error between the reconstructed and the original images. We discuss the application of the proposed approach to well-known benchmark images and show how the quantization tables determined by our method improve the performance of the JPEG algorithm with respect to the default tables suggested in Annex K of the JPEG standard.	algorithm;baseline (configuration management);image quality;iterative and incremental development;jpeg	Beatrice Lazzerini;Francesco Marcelloni;Massimo Vecchio	2010	Appl. Soft Comput.	10.1016/j.asoc.2009.08.024	image quality;lossless jpeg;computer vision;data compression ratio;mathematical optimization;image compression;computer science;theoretical computer science;machine learning;evolutionary algorithm;compression ratio;jpeg;jpeg 2000;mean squared error;quantization	Vision	42.29255860286983	-14.363084526542329	10777
e648c973b8942a9676685e4217dedab15334e1bf	a particle swarm optimization inspired tracker applied to visual tracking	particle swarm;real visual rgb d information sequential particle swarm optimization inspired tracker visual tracking dynamic optimization evolutionary optimization approach pso algorithm system dynamics sir algorithm sampling importance resampling algorithm random walk constant velocity model pso inspired tracker spso;traitement du signal et de l image;video analysis;rgb d sensors;intelligence artificielle;vision par ordinateur et reconnaissance de formes;rgb d sensors visual tracking particle swarm particle filter video analysis;traitement des images;particle filter;heuristic algorithms vectors particle swarm optimization visualization target tracking optimization magnetic heads;synthese d image et realite virtuelle;visual tracking;particle swarm optimisation dynamic programming evolutionary computation image sampling importance sampling object tracking	Visual tracking is dynamic optimization where time and object state simultaneously influence the problem. In this paper, we intend to show that we built a tracker from an evolutionary optimization approach, the PSO (Particle Swarm optimization) algorithm. We demonstrated that an extension of the original algorithm where system dynamics is explicitly taken into consideration, it can perform an efficient tracking. This tracker is also shown to outperform SIR (Sampling Importance Resampling) algorithm with random walk and constant velocity model, as well as a previously PSO inspired tracker, SPSO (Sequential Particle Swarm Optimization). Experiments were performed both on simulated data and real visual RGB-D information. Our PSO inspired tracker can be a very effective and robust alternative for visual tracking.	algorithm;dynamic programming;mathematical optimization;particle filter;particle swarm optimization;system dynamics;velocity (software development);video tracking	Christophe Mollaret;Frédéric Lerasle;Isabelle Ferrané;Julien Pinquier	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025085	computer vision;multi-swarm optimization;particle filter;eye tracking;computer science;artificial intelligence;machine learning;mathematics;particle swarm optimization;metaheuristic;statistics	Robotics	45.64214299938309	-47.73270738117884	10781
886835241d58b83d57aea86685e7e308435cea64	demand based bidding strategies under interval demand for integrated demand and supply management		The penetration of renewable resources in the wholesale electricity market and the demand response in the retail market cause the demand and the supply to become more unpredictable. The ISO is hard to efficiently schedule the production and dispatch the demand. Furthermore, strategic bidding in a more competitive environment is an important problem for the generator. Forecasting the hourly market clearing price (MCP) in the day-ahead electricity market is one of essential task for any bidding decision making. But only a single predicted value of MCP cannot offer enough help for the generator to select the optimal bidding strategies. Aiming at challenge these tasks, we design a new wholesale mechanism in which the ISO declares an interval demand to the wholesale market. The interval demand is more robust than a single demand figure and enables the ISO to handle unpredictable demand under the DR programs. We also developed a forecasting model to forecast a MCP function under the interval demand and introduce the notion of confidence interval to the forecasting model. The confidence interval predicts the exact range of hourly MCP. Based on these work, the optimal bidding strategies for the generator under an interval demand is also illustrated.	artificial neural network;dynamic dispatch;feedforward neural network	Zixu Liu;Xiao-Jun Zeng;Abasola C. M. Simon	2018	2018 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2018.8477941	mathematical optimization;computer science;electricity market;market clearing;demand response;bidding;confidence interval;microeconomics;supply and demand	ECom	1.904523571790527	-6.659472317432167	10782
c85da291e937e6c38bfe6ffd37f923d71f56bf9f	fast mode decision algorithms for inter/intra prediction in h.264 video coding	h 264 avc;intra prediction;video coding;spatial correlation;fast mode decision;mode decision	In this paper, we propose fast mode decision algorithms for both intra prediction and inter prediction in H.264. In order to select the candidate modes for intra4x4 and intra16x16 prediction efficiently, we have used the spatial correlation and directional information. We have also applied an early block size selection method to reduce the searching time further. The fast inter mode decision is achieved by an early SKIP mode decision method, and a fast mode decision method for 16x16 and P8x8 modes. Extensive simulations on different test sequences demonstrate a considerable speed up by saving the encoding time up to 82% for intra prediction and 77% for inter prediction on average, compared to the H.264 standard, respectively. This is achieved at the cost of negligible loss in PSNR values and small increase in bit rates.	algorithm;h.264/mpeg-4 avc	Ling-Jiao Pan;Seung-Hwan Kim;Yo-Sung Ho	2007		10.1007/978-3-540-77255-2_18	spatial correlation;real-time computing;simulation;computer science;machine learning;statistics	ML	47.162274802892995	-19.249723103386753	10804
1bf30047060e4cd02e28c9e3241a726747e1cd6c	landmark-based geodesic computation for heuristically driven path planning	graph theory;heuristic;image coding;application software;path planning;front propagation;euclidean distance;data mining;communication conference;fast marching;computer vision;artificial intelligent;large scale;geophysics computing;geodesic;artificial intelligence;geophysics computing path planning computer vision artificial intelligence application software data mining euclidean distance large scale systems image coding graph theory;large scale systems	This paper presents a new method to quickly extract geodesic paths on images and 3D meshes. We use a heuristic to drive the front propagation procedure of the classical Fast Marching. This results in a modification of the Fast Marching algorithm that is similar to the A algorithm used in artificial intelligence. In order to find very quickly geodesic paths between any given couples of points, we advocate for the initial computation of distance maps to a set of landmark points and make use of these distance maps through a relevant heuristic. We show that our method brings a large speed up for large scale applications that require the extraction of geodesics on images and 3D meshes. We introduce two distortion metrics in order to find an optimal seeding of landmark points for the targeted applications. We also propose a compression scheme to reduce the memory requirement without impacting the quality of the extracted paths.	algorithm;artificial intelligence;computation;data structure;distortion;fast fourier transform;fast marching method;heuristic;heuristic (computer science);landmark point;map;motion planning;software propagation	Gabriel Peyré;Laurent D. Cohen	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.163	computer vision;application software;geodesic;heuristic;computer science;graph theory;theoretical computer science;machine learning;fast marching method;euclidean distance;mathematics;geometry;motion planning	Vision	48.21061555061155	-24.366740836482602	10811
e71c875e178c7fa789bdf4fa07de87bd20408463	formulation of customers' shopping path in shelf space planning: a simulation-optimization approach	simulation optimization;imperialist competitive algorithm;shelf space allocation;artificial intelligence;shopping path	Numerous studies confirm that customers’ shopping behavior can highly be managed by many in-store factors such that retail managers try to systematically consider them in order to achieve a well-established solution for shelf-space allocation problem. To assist them, we develop an approach based on two artificial intelligence techniques to facilitate well-designed shelf space management. We propose an iterative simulation-optimization approach that integrates customers’ shopping path in the potential demand and introduces it by simulation in the optimization. A profit-based integer programming is also presented that the related computer program, being able to solve small-sized instances, applies important factors including shelf level utility, attraction of store’ zones, allocated shelf space, number of product facings, and demand substitution effects. The problem is inherently a complex and large-sized problem; therefore, we develop two algorithms: GA and hybrid GA with imperialist competitive algorithm. The experimental results prove good performance of hybrid algorithm in terms of both the solution quality and computation time. By embedding this flexible and powerful framework in an expert tool, retail managers are capable of making effective decisions.	artificial intelligence;computation;computer program;decision support system;decision theory;gams;heuristic (computer science);hybrid algorithm;imperialist competitive algorithm;integer programming;iterative method;mathematical optimization;metaheuristic;numerical analysis;sequential structure alignment program;simulation;software release life cycle;space allocation problem;time complexity	Elaheh Ghazavi;M. M. Lotfi	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.01.043	simulation;computer science;artificial intelligence;machine learning;imperialist competitive algorithm	AI	16.95012175221435	-0.5893619180522344	10818
7632c9cc8695714c9ee40340f2cb88ba622aa423	an improved particle swarm optimization for complex optimization problems	general center particle;optimization problem;particle swarm optimization	An improved particle swarm optimization (IPSO) is proposed where a general center particle is incorporated into particle swarm optimization (PSO) with linearly decreasing inertia weight factor in this paper. The general center particle is formed by the center of the best-found positions of all particles in IPSO. It has potential capacity to get good positions and guide the search direction of the whole swarm because of frequently appearance as the best particle of the swarm. Numerical results and comparison on a set of benchmark optimization functions show the proposed algorithm is a promising optimization method in obtaining better solutions.	particle swarm optimization;program optimization	Kezong Tang;Binxiang Liu;Jia Zhao	2013		10.1007/978-3-642-42057-3_109	multi-swarm optimization;constrained optimization;meta-optimization;derivative-free optimization;imperialist competitive algorithm;particle swarm optimization;metaheuristic	Theory	27.515856023554967	-3.7320012231901845	10819
53c0c9c6d63808046b7561be9511f375a394146e	on multi-view active learning and the combination with semi-supervised learning	active learning;semi supervised learning;sample complexity	<i>Multi-view learning</i> has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view <i>active learning</i>. Under the α-<i>expansion</i> assumption, we get an exponential improvement in the sample complexity from usual <i>Õ</i>(1/<i>ε</i>) to <i>Õ</i>(log 1/<i>ε</i>), requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in R<i><sup>d</sup></i> nor strong assumption on hypothesis class such as linear separators through the origin. We also give an upper bound of the error rate when the α-<i>expansion</i> assumption does not hold. Then, we analyze the combination of multi-view active learning and semi-supervised learning and get a further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which verifies that the combination of multi-view active learning and semi-supervised learning is efficient.	active learning (machine learning);free viewpoint television;sample complexity;semi-supervised learning;semiconductor industry;supervised learning;time complexity	Wei Wang;Zhi-Hua Zhou	2008		10.1145/1390156.1390301	semi-supervised learning;unsupervised learning;empirical risk minimization;sample exclusion dimension;computer science;online machine learning;machine learning;pattern recognition;mathematics;active learning;stability;active learning;probably approximately correct learning;statistics;generalization error	ML	20.460051542704772	-33.306279087499455	10831
174c45505c2d26212273efe633279a65b1ce706b	statistical weather data analysis for wide area smart grid operations	smart power grids control engineering computing data analysis load regulation meteorology power engineering computing power markets regression analysis;simple regression models wide area smart grid system wasgs correlation models load variation;load control statistical weather data analysis wide area smart grid operations deregulated power market demand curves varying weather parameters probabilistic problem weather parameters nonautonomous wide area smart grid system independent weather data variables simple linear regression multilinear regression techniques statistical varying load model r statistical tool;smart grids load modeling humidity correlation analytical models power system stability	In deregulated power market, the variation of demand curves with the varying weather parameters is a probabilistic problem. The weather parameters, such as temperature, humidity, and precipitation affect the load requirements in non-autonomous wide area smart grid system. We modeled the relationships of the load with aforementioned weather parameters. Moreover, various correlation models, such as Pearson, Spearman, and Kendall are quantitatively analyzed for each independent weather data variables. Furthermore, simple linear regression and multi-linear regression techniques are applied for the statistical varying load model in the wide area smart grid system. The effect of each independent variable on the dependent variable (load) is critically observed using statistical tool R. The linear and non-linear varying patterns exhibited monitoring information for the load control and management of wide area smart grid system.	autonomous robot;load management;nonlinear system;requirement	Sahibzada Muhammad Ali;Chaudhary Arshad Mehmood;Ahsan Khawja;Rahat Nasim;Muhammad Jawad;Saeeda Usman;Sikandar Khan;Saqib Salahuddin;Mian Atif Ihsan	2014	IEEE International Conference on Electro/Information Technology	10.1109/EIT.2014.6871808	econometrics;real-time computing;simulation;engineering	Robotics	36.15372433506094	-1.077934994736121	10850
8aa97aa62f69fa08a7c0baed8c658ec56c29e10c	pattern recognition neural network as a tool for pest birds detection	neural networks;training;computational modeling;birds;feature extraction;data preprocessing	Various kinds of vermin have been considered as a huge problem since primeval times. Over this period, means of protection against vermin have developed to be very quick and efficient. However, new goals in protection have appeared recently which reflects legislative changes in most countries. Public opinion has shifted towards greater environment protection. Nowadays, vermin control systems have turned from being used globally into local applications and from being applied preventively into casual usage. Thus, accurate vermin detection units are becoming very important parts of vermin control systems. This situation is valid in agricultural areas (e.g. vineyards) which are protected against pest birds, too. Reflecting on the current situation, a feedforward multilayer artificial neural network, aimed on detection of European starling in vineyards, is presented in this paper. Except a description and validation of the detection method, the idea of the comprehensive protection system is also outlined in this paper.	artificial neural network;control system;emoticon;feature extraction;feedforward neural network;linear predictive coding;microprocessor;pattern recognition;sensor;starling;test set	Petr Doležel;Pavel Skrabanek;Lumir Gago	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7849988	simulation;engineering;artificial intelligence;data mining	Security	7.612614623871841	-22.640607418852113	10864
a933fa9387706a0432bbc5184f89e71f6790a3b4	robust euclidean alignment of 3d point sets: the trimmed iterative closest point algorithm	qa75 electronic computers computer science szamitastechnika;performance evaluation;szamitogeptudomany;point sets;registration;iterative closest point;robustness;least trimmed squares;iterative closest point algorithm	The problem of geometric alignment of two roughly pre-registered, partially overlapping, rigid, noisy 3D point sets is considered. A new natural and simple, robustified extension of the popular Iterative Closest Point (ICP) algorithm [1] is presented, called Trimmed ICP. The new algorithm is based on the consistent use of the Least Trimmed Squares approach in all phases of the operation. Convergence is proved and an efficient implementation is discussed. TrICP is fast, applicable to overlaps under 50%, robust to erroneous and incomplete measurements, and has easy-to-set parameters. ICP is a special case of TrICP when the overlap parameter is 100%. Results of a performance evaluation study on the SQUID database of 1100 shapes are presented. The tests compare TrICP and the Iterative Closest Reciprocal Point algorithm [2].	algorithm;iterative closest point;iterative method;performance evaluation	Dmitry Chetverikov;Dmitry Stepanov;Pavel Krsek	2005	Image Vision Comput.	10.1016/j.imavis.2004.05.007	computer vision;mathematical optimization;combinatorics;least trimmed squares;computer science;mathematics;iterative closest point;statistics;robustness	Vision	50.64780234852574	-51.17255823409051	10867
74aab677130c94d8a1d1def1e21b8e6b29e09281	achieving robustness by casting planning as adaptation of a reactive system	visual sensing robustness planning adaptation reactive system artificial intelligence kitting robot uncertain environment flexible robot plans puma 560 robot;formal model;planning artificial intelligence;robots planning artificial intelligence;artificial intelligent;dynamic environment;robots;reactive system;robustness casting robotics and automation production facilities artificial intelligence robot sensing systems uncertainty robotic assembly intelligent robots robot control	Classical AI planning is not sufficiently robust in uncertain and dynamic environments. Reactive approakes are robust in some envimnments namely those for which they have been preprogrammed. An approach is presented here that integrates a prtort planning with reaction to increase robustness. .4s motivation, a practical robot problem, the kitting robot, is presented. Solving this problem demands a system that can make timely and robust actions in an uncertain environmenl. A solution to the kitting robot problem is outlined in which planning is cast as adaptation of a reactive system to suit changes in the goals or environment. The reactivesystem (the rcacfor) is based on a formal model for representing flexible robot plans, the R S model. Thus, it was possible to formalize the mechanisms by which the planner improves the behavior of the reactor. This system was implemented to control Puma-560 robot equipped with visual sensing.		Damian M. Lyons;Antonius J. Hendriks;Sandeep Mehta	1991		10.1109/ROBOT.1991.131978	robot;control engineering;mobile robot;simulation;reactive system;computer science;engineering;artificial intelligence;social robot;robot control;reactive planning;personal robot	Robotics	51.83003020145443	-26.860057244966157	10878
125bbe33f6363318120f37bd288ba52ec32d989b	robust image transmission over energy-constrained time-varying channels using multiresolution joint source-channel coding	transformation ondelette;hierarchical system;quantization;channel coding;robustness image communication signal resolution energy resolution time varying channels source coding constellation diagram modulation coding channel state information transmitters;cuantificacion;image coding;time variable channel;image processing;image resolution;data compression;decoding;visual communication;subband decomposition;systeme hierarchise;digital transmission;procesamiento imagen;channel state information;multistate awgn channel robust image transmission energy constrained time varying channels multiresolution joint source channel coding multiresolution modulation constellations still image transmission channel state information receiver transmitter performance wavelet image decomposition subbands generalized gaussian distributions algorithm multiresolution source codebook decoding source resolution signal constellation resolution real time operation table lookups wavelet image representation reconstructed image quality channel optimized source coding;transform coding;indexing terms;quantification;analyse multiresolution;traitement image;data communication;wavelet transforms;joint source channel coding;sistema jerarquizado;codificacion;image transmission;descomposicion subbanda;image representation;image reconstruction;image quality;canal variable con el tiempo;coding;generalized gaussian distribution;transmision numerica;canal variant dans temps;time varying channel;source code;compresion dato;transformacion ondita;decomposition sous bande;transmission numerique;image decomposition;image reconstruction visual communication source coding channel coding image coding transform coding wavelet transforms time varying channels image resolution modulation gaussian distribution table lookup decoding image representation gaussian channels;table lookup;transmission image;multiresolution analysis;gaussian channels;gaussian distribution;wavelet transformation;analisis multiresolucion	We explore joint source-channel coding (JSCC) for time-varying channels using a multiresolution framework for both source coding and transmission via novel multiresolution modulation constellations. We consider the problem of still image transmission over time-varying channels with the channel state information (CSI) available at 1) receiver only and 2) both transmitter and receiver being informed about the state of the channel, and we quantify the effect of CSI availability on the performance. Our source model is based on the wavelet image decomposition, which generates a collection of subbands modeled by the family of generalized Gaussian distributions. We describe an algorithm that jointly optimizes the design of the multiresolution source codebook, the multiresolution constellation, and the decoding strategy of optimally matching the source resolution and signal constellation resolution “trees” in accordance with the time-varying channel and show how this leads to improved performance over existing methods. The realtime operation needs only table lookups. Our results based on a wavelet image representation show that our multiresolutionbased optimized system attains gains on the order of 2 dB in the reconstructed image quality over single-resolution systems using channel optimized source coding.	algorithm;channel capacity;channel state information;codebook;data compression;forward error correction;image quality;modulation;open-source software;transmitter;wavelet	Igor Kozintsev;Kannan Ramchandran	1998	IEEE Trans. Signal Processing	10.1109/78.668553	speech recognition;telecommunications;image processing;computer science;mathematics;statistics;source code	Mobile	47.97275469996787	-13.064756071476156	10893
e884de8cafcad7de68672fdfe231e6afd0ed8e5a	a robust multi-athlete tracking algorithm by exploiting discriminant features and long-term dependencies		This paper addresses multiple athletes tracking problem. Athletes tracking is the key to whether sports video analysis can be more effective and practical or not. One great challenge faced by multi-athlete tracking is that athletes, especially the athletes in the same team, share very similar appearance, thus, most existing MOT approaches are hardly applicable in this task. To address this problem, we put forward a novel triple-stream network which could capture long-term dependencies by exploiting pose information to better distinguish different athletes. The method is motivated by the fact that poses of athletes are distinct from each other in a period of time because they play different roles in the team thus could be used as a strong feature to match the correct athletes. We design our Multi-Athlete Tracking (MAT) model on top of the online tracking-by-detection paradigm whereby bounding boxes from the output of a detector are connected across video frames, and improve it from two aspects. Firstly, we propose a Pose-based Triple Stream Networks (PTSN) based on Long Short-Term Memory (LSTM) networks, which are capable of modeling and capturing more subtle differences between athletes. Secondly, based on PTSN, we propose a multi-athlete tracking algorithm that is robust to noisy detection and occlusion. We demonstrate the effectiveness of our method on a collection of volleyball videos by comparing it with recent advanced multi-object trackers.		Nan Ran;Longteng Kong;Yunhong Wang;Qingjie Liu	2019		10.1007/978-3-030-05710-7_34	pattern recognition;artificial intelligence;computer science;athletes;algorithm;discriminant;bittorrent tracker	Vision	32.90950344202256	-50.13550286347229	10903
be0cc445a1093095e7b97ed9f07475a89c40644c	an omp steganographic algorithm optimized by sfla	orthogonal matching pursuit;shuffled frog leaping algorithm;steganography;sparse decomposition	In this paper, we propose a novel steganographic method, which utilizes the sparsity and integrity of the image compressed sensing to reduce the risk of being detected by steganalysis. In the proposed algorithm, the message hiding process is integrated into the image sparse decomposition process without affecting the image perceptibility. First, the cover image is decomposed by the orthogonal matching pursuit algorithm of image sparse decomposition, and the shuffled frog leaping algorithm (SFLA) is used to select the optimal atom in each decomposition iteration. Then, different quantization bits are adopted to quantify the sparse decomposition coefficients. Finally, via LSB±k steganographic strategy, the secret message is embedded in the least significant bits of the quantized coefficients. Experimental results show that the embedded data are invisible perceptually. Simultaneously, experiments show that the new steganography has good expandability in embedding capacity, owing to less sensitivity to the embedding bits. The security of the proposed method is also evaluated comparatively, by using four steganalyzers with rich feature, which indicates superior performance of the proposed method comparing with other steganographies conducted in sparse decomposition domain and the LSB±k methods used in spatial domain and DCT domain.	algorithm;openmp;steganography	Chun-Juan Ouyang;Chang-Xin Liu;Ming Leng;Huan Liu	2017	IJPRAI	10.1142/S0218001417540015	mathematical optimization;computer science;theoretical computer science;machine learning;steganography;statistics;matching pursuit	HPC	40.66902286500555	-9.61926503550807	10906
625dbbeda8351f56f0bfcd6a5bdcea59e02f4867	precision and recall for time series		Classical anomaly detection is principally concerned with point-based anomalies, those anomalies that occur at a single point in time. Yet, many real-world anomalies are range-based, meaning they occur over a period of time. Motivated by this observation, we present a new mathematical model to evaluate the accuracy of time series classification algorithms. Our model expands the well-known Precision and Recall metrics to measure ranges, while simultaneously enabling customization support for domain-specific preferences.	algorithm;anomaly detection;autonomous car;detectors;experiment;formal language;mathematical model;precision and recall;sensor;time series;whole earth 'lectronic link	Nesime Tatbul;Tae Jun Lee;Stan Zdonik;Mejbah Alam;Justin Emile Gottschlich	2018			machine learning;artificial intelligence;anomaly detection;precision and recall;correctness;mathematics	ML	5.75319218224073	-35.72962737022089	10925
0bde1c7d16ad5234c276fa82442798e702d28638	a weight factor algorithm for activity recognition utilizing a lattice-based reasoning structure	uncertainty handling inference mechanisms minimisation sensor fusion;minimisation;lattices;uncertainty;inference process weight factor algorithm activity recognition lattice based reasoning structure lattice based evidential fusion smart environments lattice layers uncertainty information sensor context;dempster shafer theory of evidence;training;inference mechanisms;uncertainty handling;revised lattice structure;accuracy;sensor fusion smart environments activity recognition reasoning under uncertainty dempster shafer theory of evidence revised lattice structure;factorization method;reasoning under uncertainty;sensor fusion;smart environments;context;smart environment;context lattices uncertainty accuracy sensor fusion training;activity recognition	This paper introduces a new weight factor method for lattice-based evidential fusion for the purposes of activity recognition within smart environments. In calculating the weight factor between the lattice layers, the uncertainty information derived from sensors along with the sensor context has been taken into consideration. According to the experimental results, the proposed weight factor method has the ability to effectively incorporate the uncertainty into the inference process, and subsequently infer complex activities such as a preparing lunch activity with an accuracy of 65.20%.	activity recognition;algorithm;sensor;smart environment	Jing Liao;Yaxin Bi;Chris D. Nugent	2011	2011 IEEE 23rd International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2011.150	computer science;machine learning;pattern recognition;data mining;mathematics;smart environment;statistics;activity recognition	Robotics	1.5390246583584255	-31.534473029062575	10930
b9c5a4c6d595c42cd60de64951abd8d84a13a62a	high-performance region-of-interest image error concealment with hiding technique	error concealment;region of interest;high performance	Recently region-of-interest (ROI) based image coding is a popular topic. Since ROI area contains much more important information for an image, it must be prevented from error decoding while suffering from channel lost or unexpected attack. This paper presents an efficient error concealment method to recover ROI information with a hiding technique. Based on the progressive transformation, the low-frequency components of ROI are encoded to disperse its information into the high-frequency bank of original image. The capability of protection is carried out with extracting the ROI coefficients from the damaged image without increasing extra information. Simulation results show that the proposed method can efficiently reconstruct the ROI image when ROI bit-stream occurs errors, and the measurement of PSNR result outperforms the conventional error concealment techniques by 2 to 5 dB.	algorithm;bitstream;coefficient;discrete cosine transform;embedded system;error concealment;image quality;ll parser;multivariate interpolation;peak signal-to-noise ratio;region of interest;simulation	Shih-Chang Hsia;Szu-Hong Wang;Ming-Huei Chen	2010	J. Electrical and Computer Engineering	10.1155/2010/318137	computer vision;speech recognition;computer science;computer graphics (images);region of interest	EDA	41.33420623206531	-13.494943324498598	10937
ae0d17324a4fd6fa2f7fd3e237e5a93eadcc00cc	new simulation methodology for risk analysis: rare-event, heavy-tailed simulations using hazard function transformations, with applications to value-at-risk	risk analysis;stochastic simulation;value at risk;heavy tail;risk factors;simulation methods;rare event;hazard rate;random variable;hazard function;laplace distribution	We develop an observation that a simulation method introduced recently for heavy-tailed stochastic simulation, namely hazard-rate twisting, is equivalent to doing exponential twisting on a transformed version of the heavy-tailed random-variable; the transforming function is the hazard function. Using this approach, the paper develops efficient methods for computing portfolio value-at-risk (VAR) when changes in the underlying risk factors have the multivariate Laplace distribution.	failure rate;risk factor (computing);simulation;time complexity;value at risk	Zhi Huang;Perwez Shahabuddin	2003		10.1145/1030818.1030858	reliability engineering;random variable;econometrics;risk analysis;heavy-tailed distribution;engineering;failure rate;stochastic simulation;laplace distribution;mathematics;hazard ratio;risk factor;statistics;value at risk	Metrics	30.571048488346563	-19.669613145230436	10949
12ad8d612746e6f74358ddf25e76174574bbcb79	stability of vertically differentiated cournot and bertrand-type models when firms are boundedly rational	bifurcation;bertrand competition;bounded rationality;dynamic stability;cournot competition;vertical product differentiation	This paper compares the dynamic of Cournot and Bertrand duopolies with vertical product differentiation and under bounded rationality. We find that an increase in the product differentiation degree destabilizes the Nash equilibrium under quantity competition, while the Bertrand–Nash equilibrium becomes more stable. From a global dynamic analysis, we show that an increase in the firms’ adjustment speed constitutes a source of complexity in both models. There is a cascade of flip bifurcations leading to increasingly complex attractors, and there are global bifurcations generating complex basins of attraction. Copyright Springer Science+Business Media New York 2016	bertrand (programming language)	Joaquín Andaluz;Gloria Jarne	2016	Annals OR	10.1007/s10479-015-2057-4	industrial organization;cournot competition;bertrand competition;economics;bertrand paradox;microeconomics;mathematical economics;bounded rationality	Crypto	-3.678240179539695	-4.495135111154432	10960
6150173a200fd4fea71038bd7d843f9dcde48e65	outlier identification with the harmonic topographic mapping	topographic map;outlier detection;topology preservation	We review two versions of a topology preserving algorithm one of which we had previously [1] found to be more successful in defining smooth manifolds and tight clusters. In the context of outlier detection, however, the other is shown to be more successful. Nevertheless, we show that, by using local kernels for calculation of responsibilities, the first one can also be used in this manner.	algorithm;anomaly detection;topography	Marian Pena;Colin Fyfe	2006			topographic map;anomaly detection;computer science;machine learning;pattern recognition;data mining	ML	29.70208676624494	-37.307779268560594	10961
d1b3502591f367c38ab9cae8931d99d7c0d73a3e	problem complexity	software engineering;computational complexity	Undergraduates often encounter problems with running their first empirical studies. This study investigated which kind of instruction works best for performing an analogue task of constructing racing cars for different racing tracks. Corresponding to earlier studies (e.g., Burns & Volmeyer, 2002; Vollmeyer, Burns, & Holyoak, 1996), participants got either a specific instruction to construct a racing car for one specific track or they got the unspecific instruction to construct fastest cars for several tracks. The specific instruction should prepare participants best for the trained track, but not for any other tracks. In contrast, participants with unspecific instruction should be better prepared for unfamiliar tracks because of their better understanding of the underlying structures. Alternatively, one could argue that specific instruction reduces the problem space. Given a restricted number of trials, participants who have to focus on aspects of problem space should perform better for both tasks if the aspects they focused on were relevant for both tasks. Therefore, the better understanding hypothesis expects an interaction between instruction and task for performance. In contrast, the reduced problem space hypothesis expects a main effect of instruction only given that both task share important properties.	fastest;problem domain;the world of robert burns	Michael Jackson	1997		10.1109/ICECCS.1997.622318	computational problem;mathematical optimization;function problem;counting problem;polynomial-time reduction;overlapping subproblems;dynamic problem;computer science;software engineering;computational complexity theory;algorithm	PL	18.66791935227346	-11.599481855712462	10962
01f3d1379049d2b1c5142dcc7ae2a7c8b79daa29	joint-based multi-task sparse learning for human action recognition	multi task learning;action recognition;joint based covariance feature;sparse representation	Human action recognition from videos is a challenging computer vision task and it has wide applications, such as human robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. In this paper, we present a novel human action recognition method based on human joint position available video sequences. We use the joint-based covariance matrix over time as a discriminative descriptor for each human joint. Meanwhile, to encode the temporal information of the moving joints, multiple covariance matrices are deployed over sub-sequences that are different in temporal granularity. After that, we input the descriptor into multi-task sparse learning framework to obtain a more compact and discriminative action representation. The proposed method is evaluated on the sub-J-HMDB dataset. Experimental results show that the proposed method achieves a good performance and is superior to some state-of-the-art approaches.	computer multitasking;computer vision;discriminative model;encode;human metabolome database;information retrieval;sparse matrix	Wenjun Wu;Yanhua Yang;Ruishan Liu;Cheng Deng	2015		10.1145/2808492.2808535	computer vision;computer science;machine learning;pattern recognition	Vision	34.093146429815214	-49.50034684569125	10964
c45b13a8b7d4451030225c55b5a29d18638cf992	a non-parametric bayesian approach to inflectional morphology		Most of the slides will be from his recent dissertation defense.	galaxy morphological classification	Jason Eisner;Markus Dreyer	2011			morphology (linguistics);artificial intelligence;pattern recognition;nonparametric statistics;mathematics;bayesian probability	Vision	10.017249988450166	-27.14140779562576	10976
4efab5e3edb039e637f718bf38514850e4f6bfdf	improved quantizer for adaptive predictive coding of speech signals at low bit rates	quantization;predictive coding speech coding bit rate quantization encoding acoustic noise speech enhancement;quantization noise;speech coding;speech enhancement;bit rate;acoustic noise;encoding;predictive coding;level 1	Adaptive predictive coding of speech signals at bit rates lower than 10 kbits/sec often requires the use of 2-level (1 bit) quantization of the samples of the prediction residual. Such a coarse quantization of the prediction residual can produce audible quantizing noise in the reproduced speech signal at the receiver. This paper describes a new method of quantization for improving the speech quality. The improvement is obtained by center clipping the prediction residual and by fine quantization of the high-amplitude portions of the prediction residual. The threshold of center clipping is adjusted to provide encoding of the prediction residual at a specified bit rate. This method of quantization not only improves the speech quality by accurate quantization of the prediction residual when its amplitude is large but also allows encoding of the prediction residual at bit rates below 1 bit/sample.	adaptive predictive coding;quantization (signal processing)	Bishnu S. Atal;Manfred R. Schroeder	1980		10.1109/ICASSP.1980.1170967	codec2;linear predictive coding;speech recognition;full rate;quantization;harmonic vector excitation coding;computer science;speech coding;pattern recognition	ML	48.3960532609153	-8.99594062661153	10995
c145f9b108d9f6429366ee8835e0b2a0cb3a2407	a drawback and an improvement of the classical weibull probability plot	model selection;graphical approach;weibull transformations;journal;plotting position;parameter estimation;wpp plot	The classical Weibull Probability Paper (WPP) plot has been widely used to identify a model for fitting a given dataset. It is based on a match between the WPP plots of the model and data in shape. This paper carries out an analysis for the Weibull transformations that create the WPP plot and shows that the shape of the WPP plot of the data randomly generated from a distribution model can be significantly different from the shape of the WPP plot of the model due to the high non-linearity of the Weibull transformations. As such, choosing model based on the shape of the WPP plot of data can be unreliable. A cdf-based weighted least squares method is proposed to improve the parameter estimation accuracy; and an improved WPP plot is suggested to avoid the drawback of the classical WPP plot. The appropriateness and usefulness of the proposed estimation method and probability plot are illustrated by simulation and real-world examples.		R. Jiang	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.02.001	econometrics;probability plot correlation coefficient plot;data mining;mathematics;estimation theory;model selection;statistics;q–q plot	DB	28.738691624225083	-22.285255725544758	10999
34cd96fb5fd1899641c9a1bc419f50162d84502a	aggregating multiple classification results using fuzzy integration and stochastic feature selection	dimensionalidad;sample size;fuzzy set;computacion informatica;high dimensionality;small sample size;tamano muestra;computational intelligence;logique floue;taille echantillon;dimensionality;hombre;logica difusa;curse of dimensionality;intelligence artificielle;multidimensional analysis;resonancia magnetica;probabilistic approach;fuzzy integral;feature space;classification;fuzzy sets;fuzzy logic;multiple classifiers;analyse n dimensionnelle;magnetic resonance;ciencias basicas y experimentales;dimensionnalite;enfoque probabilista;approche probabiliste;human;analisis n dimensional;pattern recognition;fuzzy integrals;artificial intelligence;feature selection;inteligencia artificial;grupo a;resonance magnetique;data classification;clasificacion;homme	Classifying magnetic resonance spectra is often difficult due to the curse of dimensionality; scenarios in which a high-dimensional feature space is coupled with a small sample size. We present an aggregation strategy that combines predicted disease states from multiple classifiers using several fuzzy integration variants. Rather than using all input features for each classifier, these multiple classifiers are presented with different, randomly selected, subsets of the spectral features. Results from a set of detailed experiments using this strategy are carefully compared against classification performance benchmarks. We empirically demonstrate that the aggregated predictions are consistently superior to the corresponding prediction from the best individual classifier. Crown Copyright 2010 Published by Elsevier Inc. All rights reserved.	benchmark (computing);crown group;curse of dimensionality;experiment;feature selection;feature vector;parallel computing;randomness;resonance;selection algorithm;statistical classification;vaporware	Nicolino J. Pizzi;Witold Pedrycz	2010	Int. J. Approx. Reasoning	10.1016/j.ijar.2010.05.003	curse of dimensionality;computer science;artificial intelligence;machine learning;computational intelligence;mathematics;fuzzy set;feature selection;statistics	AI	9.716795177920075	-32.889218271003195	11010
d45181ebe20b7c45ab247f02c8507c81ad670567	combining multiple svm classifiers for radar emitter recognition	feature vectors;kernel;electronic intelligence systems;support vector machines;electronic support measures;uncertainty handling feature extraction pattern classification radar signal processing support vector machines;dempster shaffer theory;training;uncertainty handling;multiple svm classifiers;support vector machines support vector machine classification radar pulse modulation frequency shift keying space vector pulse width modulation signal analysis flowcharts radio frequency fuzzy systems;feature vector;reject rate;simulation experiment;classifier;feature extraction;modern elint system;radar emitter recognition;pattern classification;support vector machine classification;electronic intelligence systems multiple svm classifiers radar emitter recognition modern elint system modern esm system classifier feature vectors dempster shaffer theory reject rate electronic support measures;modern esm system;signal to noise ratio;radar signal processing;radar;new combination;modulation	Radar emitter recognition is of great importance in modern ELINT and ESM systems. The conventional methods for emitter recognition usually use one classifier. For specific emitter recognition, there are slight differences between the feature vectors from radars with the same type. So the recognition result of single classifier is unreliable and instable. In this paper we propose a new combining method of multiple SVM Classifiers based on Dempster-Shaffer theory. We use a new training scheme to increase the uncertainty of single classifiers by classes’ combination of the training data. This training scheme is not only accords with the character of specific radar emitter recognition, but also exerts the function of D-S theory. The simulation experiments on actual pulses of six radars with the same type verify the correctness and validity of this method, which can enhance the recognition rate and decrease the reject rate.	correctness (computer science);experiment;lambert's cosine law;radar;signals intelligence;simulation;statistical classification	Lin Li;Hongbing Ji	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.623	speech recognition;feature vector;computer science;machine learning;pattern recognition	Robotics	15.355842497583724	-50.25968843767036	11025
20efc0fc4dcbd1871dfc7e14ab6679fdb272d7a6	a mathematical model for the control of infectious diseases: effects of tv and radio advertisements		The broadcast of awareness programs through TV and radio advertisements (ads) makes people aware and brings behavioral changes among the individuals regarding the risk of infection and its control mechanisms. In this paper, we propose and analyze a nonlinear mathematical model for the control of infectious diseases due to impact of TV and radio advertisements. It is assumed that susceptible individuals are vulnerable to infection as well as information through TV and radio ads and they contract infection via direct contact with infected individuals. In the model formulation, it is also assumed that the growth rates in cumulative number of TV and radio ads are proportional to the number of infected individuals with decreasing function of aware individuals. Further, it is assumed that awareness among susceptible individuals induces behavioral changes and they form separate aware classes, which are fully protected from infection as they use precautionary measures for their protection during the infection per...	mathematical model	A. K. Misra;Rajanish Kumar Rai	2018	I. J. Bifurcation and Chaos	10.1142/S0218127418500372	mathematics;advertising;risk of infection;broadcasting	HCI	-4.262886551957145	-10.206175886886124	11035
181af1cd10fb11e719bc5b6b867abefdff1aee4e	cusntf: a scalable sparse non-negative tensor factorization model for large-scale industrial applications on multi-gpu		Given a high-order, large-scale and sparse data from big data and industrial applications, how can we acquire useful patterns in a real-time and low memory overhead manner? Sparse Non-negative tensor factorization (SNTF) possesses high-order representation, non-negativity and dimension reduction inherence. Thus, SNTF has become a useful tool to represent and analyze the sparse data, which has been incorporated with extra contextual information, i.e., time and location, etc, more than the matrix, which can only model the 2 ways data. However, current SNTF techniques suffer from a) non-linear time and space overhead, b) intermediate data explosion, and c) inability on GPU and multi-GPU. To address these issues, a single-thread-based SNTF is proposed, which involves the feature elements rather than on the whole factor matrices, and can avoid the forming of large-scale intermediate matrices. Then, a CUDA parallelizing single-thread-based SNTF (CUSNTF) model is proposed for industrial applications on GPU and multi-GPU (MCUSNTF). Thus, CUSNTF has linear computing and space complexity, and linear communication cost on multi-GPU. We implement CUSNTF and MCUSNTF on 8 P100 GPUs, and compare it with state-of-the-art parallel and distributed methods. Experimental results from several industrial datasets demonstrate that the linear scalability and efficiency of CUSNTF.	big data;cuda;dspace;dimensionality reduction;emoticon;graphics processing unit;negativity (quantum mechanics);nonlinear system;overhead (computing);parallel computing;real-time locating system;scalability;sparse matrix;the matrix;thread (computing);time complexity	Hao Li;Keqin Li;Ji-yao An;Keqin Li	2018		10.1145/3269206.3271749	data mining;computational science;tensor;sparse matrix;big data;scalability;dimensionality reduction;cuda;factorization;matrix (mathematics);computer science	HPC	16.51819434117733	-17.81944986810141	11051
6d08a178e5959129e8e349c8dd9e802bc8aa6e6a	identifying candidate disease genes using a trace norm constrained bipartite raking model	current state of the art methods candidate disease genes identification trace norm constrained bipartite raking model computational prediction human diseases candidate gene prediction task wise ordered observation model latent multitask regression function matrix variate gaussian process trace norm constrained variational inference approach multitask regression model gene disease association data sets;biology computing;genomics;regression analysis biology computing diseases gaussian processes genetics genomics;gaussian processes;diseases data models predictive models computational modeling gaussian processes genetics measurement;genetics;diseases;regression analysis	Computational prediction of genes that play roles in human diseases remains an important but challenging task. In this work, we formulate candidate gene prediction as a bipartite ranking problem combining a task-wise ordered observation model with a latent multitask regression function using the matrix-variate Gaussian process (MV-GP). We then use a trace-norm constrained variational inference approach to obtain the bipartite ranking model variables and the parameters of the underlying multitask regression model. We use this model to predict candidate genes from two gene-disease association data sets and show that our model outperforms current state-of-the-art methods. Finally, we demonstrate the practical utility of our method by successfully recovering well characterized gene-disease associations hidden in our training data.	calculus of variations;candidate disease gene;computation;computer multitasking;gaussian process;gene prediction;inference;mental association;normal statistical distribution;role playing disorder;the matrix	Cheng H. Lee;Oluwasanmi O Koyejo;Joydeep Ghosh	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610286	genomics;bioinformatics;machine learning;gaussian process;mathematics;regression analysis;statistics	ML	7.886121082862147	-51.93763803453049	11060
d47aeb204c9e3b879893510ac2b66883a42cf86d	on truth discovery in social sensing: a maximum likelihood estimation approach	truth discovery;sensory measurement;noisy nature;maximum likelihood estimation;expectation maximization problem;sensory data collection task;binary measurement;previous work;data collection paradigm;maximum likelihood estimation approach;heuristic manner;human performance;sensors;expectation maximization;majority voting;maximum likelihood estimate;silicon;ubiquitous computing;reliability;data collection	This paper addresses the challenge of truth discovery from noisy social sensing data. The work is motivated by the emergence of social sensing as a data collection paradigm of growing interest, where humans perform sensory data collection tasks. A challenge in social sensing applications lies in the noisy nature of data. Unlike the case with well-calibrated and well-tested infrastructure sensors, humans are less reliable, and the likelihood that participants' measurements are correct is often unknown a priori. Given a set of human participants of unknown reliability together with their sensory measurements, this paper poses the question of whether one can use this information alone to determine, in an analytically founded manner, the probability that a given measurement is true. The paper focuses on binary measurements. While some previous work approached the answer in a heuristic manner, we offer the first optimal solution to the above truth discovery problem. Optimality, in the sense of maximum likelihood estimation, is attained by solving an expectation maximization problem that returns the best guess regarding the correctness of each measurement. The approach is shown to outperform the state of the art fact-finding heuristics, as well as simple baselines such as majority voting.		Dong Wang;Lance M. Kaplan;Hieu Khac Le;Tarek F. Abdelzaher	2012		10.1109/IPSN.2012.6920960	machine learning;maximum likelihood;quasi-maximum likelihood;statistics	HCI	33.33650618394658	-34.052822913347484	11069
fad317493e15e0b2fa7dc48e398891266b656483	combination of text mining and corrective neural network in short-term load forecasting	text mining;autoregressive moving average arma;journal;load forecasting;short term load forecasting;artificial neural network;neural network	Short-term load forecasting refers to short period load prediction of utility ranging from one hour to several days ahead. It is meaningful in planning and dispatching the load to meet the electricity system demand. The inaccuracy load forecasting can increase the electricity operating costs. In this paper, a novel method is presented and discussed which combines text mining and corrective neural network (TM-CNN) methods. Subsequently, a numeric example of daily maximum load forecasting is used to illustrate the performance of TM-CNN method, and the experiment results also reveal that TM-CNN method outperforms the autoregressive moving average(ARMA) and BP Artificial Neural Network(BPNN) approaches.	artificial neural network;text mining	Dongxiao Niu;Jianjun Wang	2009	JCP	10.4304/jcp.4.12.1188-1194	computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	9.154246579438297	-18.02434658934944	11070
138e1abf49e23bca50a867aeb50d2eb9178f9369	fast and reliable two-view translation estimation	outlier ratio two view translation estimation two view epipolar geometry estimation outlier handling translating camera cost function reprojection errors time complexity robust ego motion estimation camera rotation;publikationer;datorseende och robotik autonoma system;konferensbidrag;artiklar;rapporter;motion estimation cameras computational complexity computational geometry;matematik;vectors cameras three dimensional displays estimation geometry complexity theory cost function	It has long been recognized that one of the fundamental difficulties in the estimation of two-view epipolar geometry is the capability of handling outliers. In this paper, we develop a fast and tractable algorithm that maximizes the number of inlier under the assumption of a purely translating camera. Compared to classical random sampling methods, our approach is guaranteed to compute the optimal solution of a cost function based on reprojection errors and it has better time complexity. The performance is in fact independent of the inlier/outlier ratio of the data. This opens up for a more reliable approach to robust ego-motion estimation. Our basic translation estimator can be embedded into a system that computes the full camera rotation. We demonstrate the applicability in several difficult settings with large amounts of outliers. It turns out to be particularly well-suited for small rotations and rotations around a known axis (which is the case for cellular phones where the gravitation axis can be measured). Experimental results show that compared to standard RANSAC methods based on minimal solvers, our algorithm produces more accurate estimates in the presence of large outlier ratios.	algorithm;apache axis;cobham's thesis;embedded system;epipolar geometry;loss function;map projection;mobile phone;monte carlo method;motion estimation;point of view (computer hardware company);random sample consensus;sampling (signal processing);time complexity;virtual reality headset	Johan Fredriksson;Olof Enqvist;Fredrik Kahl	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.208	computer vision;mathematical optimization;mathematics;geometry;statistics	Vision	52.502288396814	-48.67061219451424	11074
d41c6eef2586bf375975c20df6fab16fb2c939a4	a pso-based multi-objective optimization approach to the integration of process planning and scheduling	process planning job shop scheduling production particle swarm optimization resource management iterative methods automatic control automation optimization methods search methods;exploitation search pso based multiobjective optimization scheduling particle swarm optimization multiobjective integrated process planning high quality trade off solutions combinatorial optimization large solution space exact search method convergence discrete modeled problem local search algorithm;high quality trade off solutions;optimized production technology;convergence;local search algorithm;exact search method;job shop scheduling;combinatorial optimization problem;multi objective optimization;search method;exploitation search;discrete modeled problem;iterative methods;particle swarm optimizer;discrete model;scheduling;large solution space;particle swarm optimization;schedules;scheduling problem;search problems;process planning;numerical experiment;combinatorial optimization;particle swarm optimisation;combinatorial mathematics;multiobjective integrated process planning;search problems combinatorial mathematics convergence particle swarm optimisation process planning scheduling;pso based multiobjective optimization	This paper has presented a particle swarm optimization (PSO) based approach to handle a multi-objective integrated process planning and scheduling problem. The aim is to find a set of high-quality trade-off solutions. This is a combinatorial optimization problem with substantially large solution space, suggesting that it is highly difficult to find the best solutions with the exact search method. To account for it, a PSO-based algorithm is proposed by fully utilizing the capability of the exploration search and fast convergence. To fit the continuous PSO in the discrete modeled problem, a novel solution representation is introduced in the algorithm. Moreover, to improve the solution quality, a local search algorithm is used to perform on the stored elite solutions, which would facilitate the exploitation search in the regions with promising solutions. The numerical experiments have been performed to demonstrate the effectiveness of the proposed algorithm.	automated planning and scheduling;combinatorial optimization;experiment;feasible region;local search (optimization);mathematical optimization;multi-objective optimization;numerical analysis;optimization problem;particle swarm optimization;scheduling (computing);search algorithm	Yifa Wang;Yunfeng Zhang;Jerry Y. H. Fuh	2010	IEEE ICCA 2010	10.1109/ICCA.2010.5524365	beam search;optimization problem;job shop scheduling;mathematical optimization;convergence;schedule;combinatorial optimization;computer science;local search;multi-objective optimization;machine learning;mathematics;iterative method;particle swarm optimization;scheduling;metaheuristic	AI	20.32871872190896	-1.30601887039297	11075
ce29d69d9e90b98a47af8d1054e0200856d67bfd	introduction and elucidation of the quality of sagacity in the extended variable precision rough sets model	qa mathematics;h social sciences general;rough set	This paper introduces the quality of sagacity measure in the extended variable precision rough sets model VPRSl;u. The need for this measure is a direct consequence of the use of the associated l and u values. Moreover, di erent levels of miss-classi cation are allowed in the classi cation of objects to a decision class or its compliment. This measure attempts to take this into account, by acknowledging the classi cation of an object to the compliment of a decision class, as an appropriate (if not optimum) classi cation of that object. A consequence of the analysis is a discussion of the notion of open and closed worlds in VPRSl;u.	rough set	Malcolm J. Beynon	2003	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80703-5	rough set;computer science;artificial intelligence;data mining;mathematics	DB	-3.312211483070332	-23.229264087003695	11078
cb11589882d7164c07bcce5c097c3d81753eda1f	a compact algorithm for rectification of stereo pairs	3d reconstruction;epipolar geometry;perspective projection;stereo;rectification	 Abstract. We present a linear rectification algorithm for general, unconstrained stereo rigs. The algorithm takes the two perspective projection matrices of the original cameras, and computes a pair of rectifying projection matrices. It is compact (22-line MATLAB code) and easily reproducible. We report tests proving the correct behavior of our method, as well as the negligible decrease of the accuracy of 3D reconstruction performed from the rectified images directly.	3d projection;3d reconstruction;algorithm;computer stereo vision;correctness (computer science);display resolution;experiment;image rectification;matlab;pc bruno;rectifier	Andrea Fusiello;Emanuele Trucco;Alessandro Verri	2000	Machine Vision and Applications	10.1007/s001380050120	3d reconstruction;computer vision;perspective;computer science;mathematics;geometry;image rectification;stereophonic sound;epipolar geometry;rectification;computer graphics (images)	Vision	53.495993823296956	-48.583389648566836	11087
876a74207a0be733c2c7c7c6b46ea25507980a6b	interpreting the phase spectrum in fourier analysis of partial ranking data		Whenever ranking data are collected, such as in elections, surveys, and database searches, it is frequently the case that partial rankings are available instead of, or sometimes in addition to, full rankings. Statistical methods for partial rankings have been discussed in the literature. However, there has been relatively little published on their Fourier analysis, perhaps because the abstract nature of the transforms involved impede insight. This paper provides as its novel contributions an analysis of the Fourier transform for partial rankings, with particular attention to the first three ranks, while emphasizing on basic signal processing properties of transform magnitude and phase. It shows that the transform and its magnitude satisfy a projection invariance and analyzes the reconstruction of data from either magnitude or phase alone. The analysis is motivated by appealing to corresponding properties of the familiar DFT and by application to two real-world data sets.	3d projection;coefficient;discrete fourier transform;fourier analysis;partial index;projection-slice theorem;signal processing;spectral density	Ramakrishna Kakarala	2012	Adv. Numerical Analysis	10.1155/2012/579050	econometrics;mathematical optimization;data mining;mathematics;statistics	ML	34.0230813726171	-28.3182942506059	11094
5e1925cd93d66a5e18966c63c84993974374e0cc	a genetic algorithm-based controller for decentralized multi-agent robotic systems	robot sensing systems;task performance;control systems;probability;cognitive robotics;probability genetic algorithms optimal control decentralised control cooperative systems software agents intelligent control robots;mobile robots;intelligent control;simulated evolution;cognitive architecture;software agents;optimal control;mechanical engineering;simulated evolution genetic algorithm based controller decentralized multi agent robotic systems task performance evolution robot colony cognitive architecture mutation probability fitness scaling parameters;decentralised control;cooperative systems;genetic algorithm based controller;robots;mutation probability;decentralized multi agent robotic systems;genetic algorithm;fitness scaling parameters;genetic algorithms;genetic mutations;robot colony;robotics and automation;control systems robot sensing systems cognitive robotics robotics and automation wheels genetic algorithms mobile robots mechanical engineering laboratories genetic mutations;wheels;evolution	In this paper the results of evolution on the task performance of a robot colony are discussed. The cognitive architecture of individual robots of a colony are modified, using genetic algorithms, producing a generation of robots with superior task performance, compared with those of the initial robot population. The effects of mutation probability and fitness scaling parameters on simulated evolution are also studied in this paper.	cognitive architecture;fitness function;genetic algorithm;image scaling;robot	Arvin Agah;George A. Bekey	1996		10.1109/ICEC.1996.542403	control engineering;simulation;engineering;artificial intelligence	Robotics	25.52760076561653	-11.72776458739312	11096
e5e604d9a1f930682c31c87d43b3a2f1c3aeffb6	automatic 3 -- d digitization using a laser rangefinder with a small field of view	digitized object;surface digitization;path planning;laser rangefinder;mechanical sensors sensor phenomena and characterization collision avoidance layout automation surface emitting lasers image sensors navigation extraterrestrial phenomena sampling methods;3 d laser rangefinder;3 d digitization;laser ranging;image sensors;robot vision image sensors laser ranging path planning position control;path planning techniques;robot vision;position control;field of view;next best view;collision avoidance;digitized object surface digitization laser rangefinder 3 d digitization 3 d laser rangefinder collision avoidance path planning techniques	We address the problem of the automation of surface digitization using a precision 3-0 laser rangejnde Because of their small field of view, such sensors navigate closely to the digitized object and are subject to collisions. Unlike previous techniques that addressed only the exhaustiveness of digitization, this work focuses on collision avoidance. To safely identiJji empty space, shadow and occlusion phenomena are carefully analyzed, und sa is the effect of sampling. Then, the planning problem is solved using a hierarchical approach. A t low level, we digitize a single view and address collision avoidance using path planning techniques. At high level, the problem becomes the choice of the	collision detection;high-level programming language;motion planning;sampling (signal processing);sensor	Dimitri Papadopoulos-Orfanos;Francis Schmitt	1997		10.1109/IM.1997.603849	computer vision;simulation;field of view;image sensor;motion planning;computer graphics (images)	Robotics	52.584217618443105	-36.90347171877429	11109
174e72894ad2ed173c7ad541ac5c66f14af77784	deep neural network training with ipso algorithm		Deep learning-based methods are frequently preferred in many areas in recent years. Another issue, which is as important as deep neural networks applications, is the training of deep neural networks. Although many techniques are proposed in the literature for the training of deep nets, most of these techniques use gradient descent based approaches. In this study, differently from the conventional gradient method, Improved Particle Swam Optimisation (IPSO) algorithm is used for the training of deep neural networks. LeNet-5 network is preferred as network structure and MNIST is utilized as data set. Depending on the number of particles, a performance of up to 96.29% was achieved. In the cases after 20 particles, the average performance was over 90%.	algorithm;artificial neural network;best, worst and average case;deep learning;gradient descent;gradient method;ipso alliance;mnist database;mathematical optimization;particle filter	Mehmet Muzaffer Kosten;Murat Barut;Nurettin Acir	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404721	anomaly detection;computer science;deep learning;convolutional neural network;gradient method;artificial intelligence;artificial neural network;pattern recognition;mnist database;algorithm;gradient descent	ML	17.42152042551367	-32.87131407936079	11115
414ac07855d1b4fd691e5c75ab9ae05162270519	robust kernel canonical correlation analysis with applications to information retrieval		Canonical correlation analysis (CCA) is a powerful statistical tool quantifying correlations between two sets of multidimensional variables. CCA cannot detect nonlinear relationship, and it is costly to derive canonical variates for high-dimensional data. Kernel CCA, a nonlinear extension of the CCA method, can efficiently exploit nonlinear relations and reduce high dimensionality. However, kernel CCA yields the so called over-fitting phenomenon in the high-dimensional feature space. To handle the shortcomings of kernel CCA, this paper develops a novel robust kernel CCA algorithm (KCCA-ROB). The derived method begins with reformulating the traditional generalized eigenvalue–eigenvector problem into a new framework. Under this novel framework, we develop a stable and fast algorithm by means of singular value decomposition (SVD) method. Experimental results on both a simulated dataset and real-world datasets demonstrate the effectiveness of the developed method. © 2017 Elsevier Ltd. All rights reserved.	algorithm;big data;curse of dimensionality;document retrieval;experiment;feature vector;image retrieval;information retrieval;kernel (operating system);nonlinear system;overfitting;simulation;singular value decomposition;sparse matrix;time complexity	Jia Cai;Xiaolin Huang	2017	Eng. Appl. of AI	10.1016/j.engappai.2017.05.016	canonical correlation;kernel embedding of distributions;machine learning;computer science;kernel principal component analysis;feature vector;curse of dimensionality;reproducing kernel hilbert space;phenomenon;pattern recognition;artificial intelligence;variable kernel density estimation	AI	24.744190109063393	-40.615198110066416	11123
334df07cd64aff68966f8bc3ba5a1731ee86c876	integrating ensemble of intelligent systems for modeling stock indices	metodo directo;linear combination;levenberg marquardt;systeme intelligent;chaotic behavior;algoritmo busqueda;stock market;takagi sugeno;bolsa valores;chaos;algorithme recherche;sistema inteligente;search algorithm;caos;intelligence artificielle;aprendizaje probabilidades;algoritmo genetico;stock markets;bourse valeurs;stock exchange;resolucion problema;marche valeurs;estimation erreur;paradigm;combinacion lineal;error estimation;neuro fuzzy;indexation;machine exemple support;estimacion error;intelligent system;paradigme;algorithme genetique;apprentissage probabilites;artificial intelligence;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;systeme chaotique;inteligencia artificial;support vector machine;evolutionary algorithm;vector support machine;reseau neuronal;paradigma;algoritmo optimo;algorithme optimal;optimal algorithm;methode directe;red neuronal;chaotic systems;direct method;probability learning;combinaison lineaire;problem solving;resolution probleme;artificial neural network;neural network	The use of intelligent systems for stock market predictions has been widely established. In this paper, we investigate how the seemingly chaotic behavior of stock markets could be well-represented using ensemble of intelligent paradigms. To demonstrate the proposed technique, we considered Nasdaq-100 index of Nasdaq Stock Market and the S&P CNX NIFTY stock index. The intelligent paradigms considered were an artificial neural network trained using LevenbergMarquardt algorithm, support vector machine, Takagi-Sugeno neurofuzzy model and a difference boosting neural network. The different paradigms were combined using two different ensemble approaches so as to optimize the performance by reducing different error measures. The first approach is based on a direct error measure and the second method is based on an evolutionary algorithm to search the optimal linear combination of the different. Experimental results reveal that the ensemble techniques performed better than the individual methods and the direct ensemble approach seems to work well for the problem considered.	artificial intelligence;artificial neural network;chaos theory;evolutionary algorithm;mean squared error;support vector machine	Ajith Abraham;Andy Auyeung	2003		10.1007/3-540-44869-1_98	direct method;support vector machine;stock exchange;genetic algorithm;levenberg–marquardt algorithm;linear combination;computer science;artificial intelligence;neuro-fuzzy;machine learning;operations research;artificial neural network;algorithm;search algorithm	AI	10.237196556315272	-30.968738432908197	11129
c4ffcad9ee4a7046ec34a7da67fbd55734869adb	an intelligent system for video events detection	incremental svm;event detection;video surveillance image sequences learning artificial intelligence object detection video signal processing;image sequences intelligent system video event detection video surveillance system image processing incremental learning model;modeling event detection intelligent system incremental svm;intelligent system;modeling	The Event detection method from a video surveillance has received much attention in the image processing. In this paper, we present an overview of a new approach for event detection from video surveillance system based on incremental learning. In our approach, each event is modeled by a set of states, and each state is represented by a learning model containing a positive class (event) and a negative class (non-event). Experiments on real image sequences have shown encouraging results.	artificial intelligence;automaton;closed-circuit television;experiment;image processing;video	Yassine Aribi;Ali Wali;Adel M. Alimi	2013	2013 9th International Conference on Information Assurance and Security (IAS)	10.1109/ISIAS.2013.6947742	computer vision;object-class detection;computer science;machine learning;video tracking;pattern recognition	Robotics	39.162658686674554	-46.817881765591174	11141
7b7e8b0ca125d0fef22fc002d5f6ecda4409c3f7	automated analysis of wild fish behavior in a natural habitat	k nearest neighbor classifier;image processing;audio data classification;spectral peak track;region growing	This paper proposes a novel approach for the analysis of movement and behavior of the Plainfin midshipman (Porichthys notatus) in the wild. It is based on underwater video recordings of the fish in their natural habitat taken inside their nests during reproductive months. During this time, alpha male Plainfin midshipmen rarely leave their nests as they are guarding their eggs, so the proposed approach addresses the issue of detecting subtle motion and nesting behavior as the fish remains relatively sedentary. To the best of our knowledge, this is the first paper to propose an automated method to analyze subtle movements of a highly territorial animal in its natural habitat.  Motion detection uses the displacement of SURF (Interest point algorithm) key-point movements from frame to frame to analyze the amount of movement by the fish. K-means clustering and other outlier removal techniques are then used to differentiate fish motion from small moving objects in the background and foreground. The analysis of fish behavior uses similarity-based periodicity detection combined with the K-neighbors classifier. Experimental validation with respect to expert-annotated ground truth shows excellent performance for both motion and behavior detection approaches.	cluster analysis;displacement mapping;ground truth;habitat;k-means clustering;k-nearest neighbors algorithm;quasiperiodicity;sensor;speeded up robust features	Xin Ru Nancy Wang;Sarika Cullis-Suzuki;Alexandra Branzan Albu	2015		10.1145/2764873.2764875	computer vision;speech recognition;geography;communication	Vision	40.430766904692625	-48.25462665144091	11151
cb424c7232a708ebdec3bba7405eb6d32cd28305	learning possibilistic graphical models from data	reasoning context model learning from data possibilistic networks possibility theory probabilistic networks graphical models probability;graph theory;graphical models possibility theory markov random fields databases iterative algorithms probability calculus uncertainty graph theory humans;probability;high dimensionality;graph theory inference mechanisms learning artificial intelligence possibility theory probability;inference mechanisms;expectation maximization;graphical representation;probability theory;markov network;graphical model;possibility theory;learning artificial intelligence;probabilistic network;possibility distribution;approaches to learning	Graphical models—especially probabilistic networks like Bayes networks and Markov networks—are very popular to make reasoning in high-dimensional domains feasible. Since constructing them manually can be tedious and time consuming, a large part of recent research has been devoted to learning them from data. However, if the dataset to learn from contains imprecise information in the form of sets of alternatives instead of precise values, this learning task can pose unpleasant problems. In this paper we survey an approach to cope with these problems, which is not based on probability theory as the more common approaches like, e.g., expectation maximization, but uses possibility theory as the underlying calculus of a graphical model. We provide semantical foundations of possibilistic graphical models, explain the rationale of possibilistic decomposition as well as the graphical representation of decompositions of possibility distributions and finally discuss the main approaches to learn possibilistic graphical models from data.	bayesian network;curry–howard correspondence;design rationale;expectation–maximization algorithm;graphical model;markov chain;markov random field;possibility theory	Christian Borgelt;Rudolf Kruse	2003	IEEE Trans. Fuzzy Systems	10.1109/TFUZZ.2003.809887	probability theory;possibility theory;expectation–maximization algorithm;computer science;artificial intelligence;graph theory;machine learning;pattern recognition;probability;data mining;mathematics;graphical model;statistics	AI	31.99711956080416	-31.91585645829469	11155
e32ac5e6554a2ba993ea9519d4a069bd954ec226	non-linear improvement on hydraulic pump and motor models based on parameter optimization algorithms	operant conditioning;efficiency distribution;hydraulic pump and motor;parameter optimization algorithms;error analysis;non linear improvement;component model;complex operating conditions;parameter optimization	To solve the imprecise description of efficiency and improve the accuracy of the key hydraulic components models in complex operating conditions, the traditional hydraulic pump and motor model is discussed and improved. With the non-linear improvement and the parameter optimization algorithms, model parameters can be determined based on the experimental efficiency data of samples. Take a motor product sample for example, the efficiency distribution of the improved model is much closer to the experimental results than that of the traditional model. The mean value and the variance value of percentage error for the improved model are much smaller, and the error analysis proves that the improved model is much more suitable for the modeling in complex operating conditions.		Anlin Wang;Binnan Yue;Kaifei Jiang;Xiaotian Li	2010		10.1007/978-3-642-16527-6_3	econometrics;computer science;operant conditioning;control theory;component object model	ML	12.590437469955614	-19.751265497106512	11171
421cdf7fd85f70e31f7a274cb1acd82888328d2a	an analysis of  $n\!k$  landscapes: interaction structure, statistical properties, and expected number of local optima	complexity theory;measurement;computational modeling;vectors;mathematical model;algorithm design and analysis	Simulated landscapes have been used for decades to evaluate search strategies whose goal is to find the landscape location with maximum fitness. Understanding properties of landscapes is important for understanding search difficulty. This paper presents a novel and transparent characterization of NK landscapes and derives an analytic expression representing the expected number of local optima. We prove that NK landscapes can be represented by parametric linear interaction models where model coefficients have meaningful interpretations. We derive the statistical properties of the model coefficients, providing insight into how the NK algorithm parses importance to main effects and interactions. An important insight derived from the linear model representation is that the rank of the linear model defined by the NK algorithm is correlated with the number of local optima, a strong determinant of landscape complexity, and search difficulty. We show that the maximal rank for an NK landscape is achieved through epistatic interactions that form partially balanced incomplete block designs. Finally, an analytic expression representing the expected number of local optima on the landscape is derived, providing a way to quickly compute the expected number of local optima for very large landscapes.	algorithm;coefficient;fractal landscape;interaction;linear model;local optimum;maximal set;simulated annealing	Jeffrey S. Buzas;Jeffrey H. Dinitz	2014	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2013.2286352	algorithm design;mathematical optimization;combinatorics;computer science;machine learning;mathematical model;mathematics;computational model;measurement;statistics	ML	0.3229182123695192	-49.11684679457016	11175
3d89c9825b6e6ed85c8a5c65c7114e5df932cc08	the asymptotic core, nucleolus and shapley value of smooth market games with symmetric large players	mixed games;asymptotic nucleolus;asymptotic shapley value;oligopoly	We study the asymptotic nucleolus of a smooth and symmetric oligopoly with an atomless sector. We show that under appropriate assumptions, the asymptotic nucleolus of the TU market game coincides with the unique TU competitive payoff distribution x. This equivalence results from nucleolus of a finite game belonging to its core and the Aumann Core Equivalence, which holds for this economy due to the cut-throat competition among the identical large players. A comparison with the Shapley value yields that in some cases, the asymptotic Shapley value is more favorable for the large traders than the asymptotic nucleolus x . This may be interpreted by the ‘fairness property’ of Shapley Value which does not reflect the intense competition among the large traders, accounting for the relative importance of their marginal contribution. J.E.L. Classification numbers. C71, D40, D43.	asymptote;fairness measure;marginal model;stable marriage problem;traders;turing completeness	Avishay Aiche;Anna Rubinchik;Benyamin Shitovitz	2015	Int. J. Game Theory	10.1007/s00182-014-0422-1	financial economics;economics;microeconomics;shapley value;mathematical economics;oligopoly	ECom	-4.4461125340744045	-2.9326901850429707	11187
0955c04f4a2af86a894c323cbcf643415ab77229	scheduling heterogeneous delay tolerant tasks in smart grid with renewable energy	task arrival process heterogeneous delay tolerant tasks scheduling smart grid electricity grid generation harvested renewable energy dynamic electricity price renewable energy devices hvac systems energy harvesting device energy storage battery external grid electricity prices;renewable energy sources;pricing;cost reduction;smart power grids cost reduction delay tolerant networks hvac power system economics pricing renewable energy sources scheduling;delay tolerant networks;hvac;smart power grids;scheduling;power system economics;batteries delay electricity renewable energy resources power demand home appliances smart grids	The smart grid is the new generation of electricity grid that can efficiently facilitate new distributed sources of energy (e.g., harvested renewable energy), and allow for dynamic electricity price. In this paper, we investigate the cost minimization problem for an end-user, such as a home, community, or a business, which is equipped with renewable energy devices when electrical appliances allow different levels of delay tolerance. The varying price of electricity presents an opportunity to reduce the electricity bill from an end-user's point of view by leveraging the flexibility to schedule operations of various appliances and HVAC systems. We assume that the end user has an energy storage battery as well as an energy harvesting device so that harvested renewable energy can be stored and later used when the price is high. The energy storage battery can also draw energy from the external grid. The problem we formulate here is to minimize the cost of the energy from the external grid while usage of appliances are subject to individual delay constraints and a longterm average delay constraint. The resulting algorithm requires some future information regarding electricity prices, but achieves provable performance without requiring future knowledge of either the power demands or the task arrival process.	algorithm;grid computing;point of view (computer hardware company);provable security;rechargeable battery;scheduling (computing)	Shengbo Chen;Prasun Sinha;Ness B. Shroff	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6426013	renewable energy;pricing;embedded system;hvac;real-time computing;engineering;grid parity;stand-alone power system;smart grid;distributed generation;scheduling;intermittent energy source	HPC	2.4320770883326746	4.166803635270925	11194
6beed3029c71d2d24b0dbefe817e2a00b7d95f81	developing common set of weights with considering nondiscretionary inputs and using ideal point method		Data envelopment analysis (DEA) is used to evaluate the performance of decision making units (DMUs) with multiple inputs and outputs in a homogeneous group. In this way, the acquired relative efficiency score for each decision making unit lies between zero and one where a number of them may have an equal efficiency score of one. DEA successfully divides them into two categories of efficient DMUs and inefficient DMUs. A ranking for inefficient DMUs is given but DEA does not provide further information about the efficient DMUs. One of the popular methods for evaluating and ranking DMUs is the common set of weights (CSW) method.We generate a CSWmodel with considering nondiscretionary inputs that are beyond the control of DMUs and using ideal point method. The main idea of this approach is to minimize the distance between the evaluated decision making unit and the ideal decision making unit (ideal point). Using an empirical example we put our proposed model to test by applying it to the data of some 20 bank branches and rank their efficient units.	catalog service for the web;data envelopment analysis	Reza Kiani Mavi;Sajad Kazemi;Jay M. Jahangiri	2013	J. Applied Mathematics	10.1155/2013/906743	econometrics;mathematical optimization;mathematics	ML	2.5452607182865266	-16.907475003268825	11198
09fb440dd2daf2b93e36dd5df93950f0f3bda685	symmetric low-rank representation for subspace clustering	low rank matrix recovery;dimension reduction;symmetric low rank representation;spectral clustering;期刊论文;subspace clustering;affinity matrix	We propose a symmetric low-rank representation (SLRR) meth od for subspace clustering, which assumes that a data set is a pproximately drawn from the union of multiple subspaces. The proposed technique can reveal the membership of multiple su bspaces through the self-expressiveness property of the data. In pa rticular, the SLRR method considers a collaborative repres entation combined with low-rank matrix recovery techniques as a low-ran k representation to learn a symmetric low-rank representat io , which preserves the subspace structures of high-dimensional dat a. In contrast to performing iterative singular value decom p sition in some existing low-rank representation based algorithms, t he symmetric low-rank representation in the SLRR method can be calculated as a closed form solution by solving the symmetric lowrank optimization problem. By making use of the angular info rmation of the principal directions of the symmetric low-rank repre sentation, an a ffinity graph matrix is constructed for spectral clustering. Extensive experimental results show that it outperforms st ate-of-the-art subspace clustering algorithms.	algorithm;algorithmic efficiency;angularjs;benchmark (computing);cluster analysis;clustering high-dimensional data;computational complexity theory;database;effective method;feature extraction;iterative method;mathematical optimization;optimization problem;pa-risc;singular value decomposition;spectral clustering	Jie Chen;Haixian Zhang;Zhang Yi	2016	Neurocomputing	10.1016/j.neucom.2015.08.077	real representation;mathematical optimization;combinatorics;discrete mathematics;irreducible representation;computer science;machine learning;mathematics;spectral clustering;dimensionality reduction	ML	26.658561933283856	-40.61398613231953	11199
3c8da7d27ce822b608375e5a5dbcb28d99a71025	a fast reduced kernel extreme learning machine	extreme learning machine;kernel method;support vector machine;rbf network	In this paper, we present a fast and accurate kernel-based supervised algorithm referred to as the Reduced Kernel Extreme Learning Machine (RKELM). In contrast to the work on Support Vector Machine (SVM) or Least Square SVM (LS-SVM), which identifies the support vectors or weight vectors iteratively, the proposed RKELM randomly selects a subset of the available data samples as support vectors (or mapping samples). By avoiding the iterative steps of SVM, significant cost savings in the training process can be readily attained, especially on Big datasets. RKELM is established based on the rigorous proof of universal learning involving reduced kernel-based SLFN. In particular, we prove that RKELM can approximate any nonlinear functions accurately under the condition of support vectors sufficiency. Experimental results on a wide variety of real world small instance size and large instance size applications in the context of binary classification, multi-class problem and regression are then reported to show that RKELM can perform at competitive level of generalized performance as the SVM/LS-SVM at only a fraction of the computational effort incurred.	approximation algorithm;arabic numeral 0;big data;binary classification;computation;computational complexity theory;experiment;generalization (psychology);iterative method;kernel (operating system);learning disorders;least squares;matrix size;neural coding;nonlinear system;numerous;randomness;subgroup;supervised learning;support vector machine	Wan-Yu Deng;Yew-Soon Ong;Qing-Hua Zheng	2016	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2015.10.006	support vector machine;least squares support vector machine;kernel method;instance-based learning;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;online machine learning;machine learning;pattern recognition;data mining;graph kernel;mathematics;tree kernel;relevance vector machine;variable kernel density estimation;polynomial kernel;structured support vector machine	ML	19.83414591356813	-38.390726611510466	11200
77d0813fbdb516c56bf02d235850a98bbd64c1ac	design of a wireless sensor fusion system to analyze conditions inside bore wells	bore well;sensor fusion;rescue operation;deep well;atmospheric conditions	Bore well rescue operations are always challenging, as rescuing the child trapped inside bore wells must be done in the quickest possible time. In our earlier work, we have designed a bore well rescue device providing vital information about the conditions inside the bore well. In this paper, a wireless sensor fusion system is developed to collect the vital parameters such as humidity and temperature from the bore well at different climatic conditions. The sensor system is designed to provide information on the humidity level and concentration of gases inside the bore well which can aid the rescues operation. Experiments have conducted in open bore wells to access and analyses the atmospheric conditions inside bore well in the Mettupayam and Pollachi region, Tamil Nadu, India. We validated the proposed system in eight real rescue incidents with the help of Tamil Nadu Fire and Rescue department, India to save the child from the bore well at different places in Tamil Nadu, India. From our rescue operations, we observed that the monitoring the vital parameters such as humidity, temperature, oxygen level, CO and other gaseous level from the bore well using our proposed wireless sensor fusion system helps the rescue and paramedical team to take necessary actions immediately for rescuing the child safely in short time.		K. P. Sridhar;C. R. Hema;S. Deepa	2017	Wireless Personal Communications	10.1007/s11277-016-3299-4	simulation	Mobile	15.97173175181986	-13.666103384158806	11210
16537e5d44c2670d7fb3b65982a34ac486ab19a6	automated bug assignment: ensemble-based machine learning in large scale industrial contexts	mining software repositories;ensemble learning;computer and information science;industrial scale;classification;software engineering;natural sciences;large scale;machine learning;bug assignment;bug reports;computer science;programvaruteknik;data och informationsvetenskap;issue management	Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment.	cross-validation (statistics);experiment;machine learning;open-source software;software development;software maintenance;suicidegirls	Leif Jonsson;Markus Borg;David Broman;Kristian Sandahl;Sigrid Eldh;Per Runeson	2015	Empirical Software Engineering	10.1007/s10664-015-9401-9	natural science;biological classification;computer science;data science;data mining;database;ensemble learning;software regression	SE	15.178427483491454	-43.63008667051626	11230
79ac4ff5ac2fc3f846afb2c73938c18976b463e2	a novel hybrid multi-criteria decision-making model to solve ua-flp		The unequal area facility layout problem (UA-FLP) has been addressed by many approaches. Most of them only take quantitative aspects into consideration. In this paper, we will solve UA-FLP using a novel hybrid methodology that joins interactive evolutionary optimization and multi-criteria decision making. I particular, a combination of an interactive genetic algorithm and the analytic hierarchy process (AHP), is proposed. By means of this new approach, it is possible to consider both quantitative and qualitative (using the expert knowledge) criteria in order to reach an acceptable design. Our approach allows the decision maker (DM) to interact with the algorithm, guiding the search process and ranking the criteria that are more relevant in each design solution. In this way, the algorithm is adjusted to the DM’s preferences through his/her subjective evaluations of the representative solutions obtained by a clustering method, and also, to the quantitative criteria. A interesting real-world data set is analysed to empirically probe the robustness of this model. Relevant results are obtained, and interesting conclusions are drawn from the application of this novel intelligent framework.	user agent;windows fundamentals for legacy pcs	Laura García-Hernández;Lorenzo Salas-Morera;Henri Pierreval;Antonio Arauzo-Azofra	2018		10.1007/978-3-319-94649-8_35	machine learning;computer science;decision-making models;distributed computing;robustness (computer science);ranking;genetic algorithm;analytic hierarchy process;cluster analysis;joins;artificial intelligence	ECom	19.203673554506434	-6.253744658970174	11240
f4615ae853fa0e8effbc5b36f7455c43520345aa	on fairness and calibration		The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be “fair.” In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.	algorithm;binocular disparity;fairness measure;linear programming relaxation;machine learning;predictive modelling;statistical classification	Geoff Pleiss;Manish Raghavan;Felix Wu;Jon M. Kleinberg;Kilian Q. Weinberger	2017			computer science;machine learning;calibration;artificial intelligence;population;classifier (linguistics)	ML	19.029154189763027	-34.97530620779201	11243
108410f0880484b1cbfa5caa00603e02a61eb9fd	a framework for attention and object categorization using a stereo head robot	object recognition;saliency map;learning;degree of freedom;object categorization;robots electrical capacitance tomography associative memory eyes biological systems laboratories computer science feature extraction hardware feedback;attention;object recognition active vision robot vision stereo image processing feature extraction;robot vision;feature extraction;region of interest;stereo image processing;cognition;associative memory;associative memory attentional mechanism object categorization articulated stereo head robot pan tilt left verge right verge region of interest selection attention shifts saccadic movements feature extraction object identification object recognition incremental world map construction world map consistency maintenance perception salience map environment knowledge base;categorization;active vision;knowledge base	This work describes a framework for dealing with attention and categorization using a robot platform consisting of an articulated stereo-head with four degrees of freedom (pan, tilt, left verge, and right verge). As a practical result of this development, the system can select a region of interest, perform shifts of attention involving saccadic movements, perform an efficient feature extraction and identification/recognition, incrementally construct a world map, and keep this map consistent with a current perception of the world. Another important result for the attentional mechanism is that the system is capable to visit all regions of its restricted world, selecting one region at a time according to a salience map. For identification, the system starts without any knowledge of the environment and increases its knowledge base (associative memory) as necessary to deal with a current set of objects.	categorization;content-addressable memory;feature extraction;knowledge base;region of interest;robot;the verge	Luiz Marcos Garcia Gonçalves;Antonio A. F. Oliveira;Roderic A. Grupen	1999		10.1109/SIBGRA.1999.805719	psychology;computer vision;machine learning;communication	Robotics	48.39623454427734	-38.67320074367344	11246
97a709d402084508bb9fb79b2b8ad2b4a0173213	using simulated annealing to minimize fuel consumption for the time-dependent vehicle routing problem	time dependent;travel time;vehicle routing problem;vehicle routing;time dependent travel speeds;simulated annealing;satisfiability;fuel consumption;research paper;indexation;transportation;carbon emissions;experimental evaluation	0360-8352/$ see front matter 2010 Elsevier Ltd. A doi:10.1016/j.cie.2010.03.012 * Tel.: +886 6 2871663; fax: +886 6 2873536. E-mail address: yiyo@mail.hku.edu.tw The vehicle routing problem (VRP) has been addressed in many research papers. Only a few of them take time-dependent travel speeds into consideration. Moreover, most research related to the VRP aims to minimize total travel time or travel distance. In recent years, reducing carbon emissions has become an important issue. Therefore, fuel consumption is also an important index in the VRP. In this research a model is proposed for calculating total fuel consumption for the time-dependent vehicle routing problem (TDVRP) where speed and travel times are assumed to depend on the time of travel when planning vehicle routing. In the model, the fuel consumption not only takes loading weight into consideration but also satisfies the ‘‘non-passing” property, which is ignored in most TDVRP-related research papers. Then a simulated annealing (SA) algorithm is proposed for finding the vehicle routing with the lowest total fuel consumption. An experimental evaluation of the proposed method is performed. The results show that the proposed method provides a 24.61% improvement in fuel consumption over the method based on minimizing transportation time and a 22.69% improvement over the method based on minimizing transportation distances. 2010 Elsevier Ltd. All rights reserved.	discharger;fax;genetic algorithm;gradient;mathematical optimization;optimizing compiler;shape optimization;simulated annealing;vehicle routing problem	Yiyo Kuo	2010	Computers & Industrial Engineering	10.1016/j.cie.2010.03.012	transport;mathematical optimization;greenhouse gas;simulation;simulated annealing;computer science;engineering;operations management;vehicle routing problem;mathematics;transport engineering;fuel efficiency;satisfiability	Robotics	15.50778267868227	0.5817022199235043	11248
34098c46601b43ef0f945438f56de22a4ad284d5	effects of reduced precision on floating-point svm classification accuracy	real time;machine learning;gaussian kernel;perturbation analysis;floating point;support vector machine;floating point arithmetic;classification accuracy;embedded device	There is growing interest in performing ever more complex classification tasks on mobile and embedded devices in real-time, which results in the need for efficient implementations of the respective algorithms. Support vector machines (SVMs) represent a powerful class of nonlinear classifiers, and reducing the working precision represents a promising approach to achieving efficient implementations of the SVM classification phase. However, the relationship between SVM classification accuracy and the arithmetic precision used is not yet sufficiently understood. We investigate this relationship in floating-point arithmetic and illustrate that often a large reduction in the working precision of the classification process is possible without loss in classification accuracy. Moreover, we investigate the adaptation of bounds on allowable SVM parameter perturbations in order to estimate the lowest possible working precision in floating-point arithmetic. Among the three representative data sets considered in this paper, none requires a precision higher than 15 bit, which is a considerable reduction from the 53 bit used in double precision floating-point arithmetic. Furthermore, we demonstrate analytic bounds on the working precision for SVMs with Gaussian kernel providing good predictions of possible reductions in the working precision without sacrificing classification accuracy.	algorithm;benchmark (computing);double-precision floating-point format;embedded system;field-programmable gate array;fundamental fysiks group;mobile device;nonlinear acoustics;nonlinear system;polynomial kernel;quantization (image processing);quantization (physics);real-time clock;real-time computing;run time (program lifecycle phase);significant figures;support vector machine	Bernd Lesser;Manfred Mücke;Wilfried N. Gansterer	2011		10.1016/j.procs.2011.04.053	computer science;floating point;theoretical computer science;operating system;machine learning;pattern recognition;data mining;mathematics;extended precision;algorithm	SE	16.819375846303874	-16.363531979238626	11249
962830642664ae2e62a05cdc9aab1bf49b945ce9	moving tracking with approximate topological isomorphism	期刊论文	Today, tracking of moving objects in video becomes a highlight in multimedia. This paper proposes a novel method, which is suitable for applying on relatively high-resolution videos that moving objects can be distinguished from their color and shape information. This method matches and tracks multiple moving objects in video by extracting and combining multi-features. With the background reconstruction method we proposed, the moving objects are separated as sub images from the background, we first extract some valuable features from each sub image, especially the topological information. Then, features are applied to a strong classifier which is accumulated with weak feature classifiers. After that, by the initial matching of moving objects, we extract their kinematical features to reinforce the matching method. Finally, experimental results show the effectiveness of the novel algorithm.	approximation algorithm;experiment;image resolution;outline of object recognition	Weina Fu;Jiantao Zhou;Yingdong Ma	2015	Multimedia Tools and Applications	10.1007/s11042-015-2519-3	computer vision;computer science;theoretical computer science;machine learning	Vision	36.867183509879226	-50.80166086057583	11254
5a699f78b16a77e6ccdbb308a13c3d07bc07a45b	research of outlier mining based adaptive intrusion detection techniques	outlier mining;intrusion detection attacks;self adaptive;improved association rule algorithm;anomaly detection;rule based;intrusion detection;self adaptive artificial intelligence intrusion detection outlier mining anomaly detection;association rules;data mining;artificial intelligent;intrusion detection data mining clustering algorithms association rules computer science data engineering knowledge engineering electronic mail conference management knowledge management;adaptation model;association rule;heuristic algorithms;artificial intelligence intrusion detection attacks outlier mining based adaptive intrusion detection technique similarity coefficient improved association rule algorithm uci data set kdd99 data set;kdd99 data set;uci data set;clustering algorithms;artificial intelligence;outlier mining based adaptive intrusion detection technique;security of data;security of data artificial intelligence data mining;algorithm design and analysis;similarity coefficient	The traditional IDS can not effectively manage the new continuously changing intrusion detection attacks. To deal with the problem, data mining based intrusion detection methods have been the hot fields in intrusion detection research. An outlier mining based adaptive intrusion detection framework is proposed in this paper. In the proposed framework, the outliers are firstly detected by similarity coefficient. And then, the clusters are built on the detected outlier data set and the improved association rule algorithm is employed on the clusters. Finally, the rules generated by association rule algorithm will be adaptively added into the current intrusion detection rule base. The experiments performed on simulated data and KDD99 from UCI data set have shown the effectiveness of proposed methods.	algorithm;association rule learning;coefficient;data mining;experiment;intrusion detection system;jaccard index;rule-based system	Yu Ke Fang;Yan Fu;Junlin Zhou	2010	2010 Third International Conference on Knowledge Discovery and Data Mining	10.1109/WKDD.2010.51	anomaly-based intrusion detection system;anomaly detection;association rule learning;computer science;artificial intelligence;machine learning;pattern recognition;data mining	ML	4.659196422554795	-38.07503882584745	11258
ac0bb28bb90cad5165c0dc6330e89718803ef021	general schema theory for genetic programming with subtree-swapping crossover: part ii	specific crossover operator;crossover point;space n2;strongly-typed gp crossover;standard crossover;context-preserving crossover;exact schema theory;genetic programming;subtree-swapping crossover;gp crossover;schema theory;part ii;cartesian node reference system;gp schema;one-point crossover;general schema theory;size-fair crossover;two-part paper;probability distribution;lower bound	This paper is the second part of a two-part paper which introduces a general schema theory for genetic programming (GP) with subtree-swapping crossover (Part I (Poli and McPhee, 2003)). Like other recent GP schema theory results, the theory gives an exact formulation (rather than a lower bound) for the expected number of instances of a schema at the next generation. The theory is based on a Cartesian node reference system, introduced in Part I, and on the notion of a variable-arity hyperschema, introduced here, which generalises previous definitions of a schema. The theory includes two main theorems describing the propagation of GP schemata: a microscopic and a macroscopic schema theorem. The microscopic version is applicable to crossover operators which replace a subtree in one parent with a subtree from the other parent to produce the offspring. Therefore, this theorem is applicable to Koza's GP crossover with and without uniform selection of the crossover points, as well as one-point crossover, size-fair crossover, strongly-typed GP crossover, context-preserving crossover and many others. The macroscopic version is applicable to crossover operators in which the probability of selecting any two crossover points in the parents depends only on the parents' size and shape. In the paper we provide examples, we show how the theory can be specialised to specific crossover operators and we illustrate how it can be used to derive other general results. These include an exact definition of effective fitness and a size-evolution equation for GP with subtree-swapping crossover.	carcinoma, neuroendocrine;cartesian closed category;computer science;crossover (genetic algorithm);database schema;effective fitness;evolutionary computation;gain;genetic programming;holland's schema theorem;hot swapping;iterated function;iteration;marc (archive);mutation;node - plant part;numerical analysis;poli gene;paging;poly a;population parameter;ruizterania albiflora;software propagation;strong and weak typing;the offspring;theory;tree (data structure);yao graph	Riccardo Poli;Nicholas Freitag McPhee	2003	Evolutionary Computation	10.1162/106365603766646825	genetic programming;defining length;combinatorics;crossover;genetic algorithm;computer science;artificial intelligence;machine learning;schema;mathematics;upper and lower bounds;algorithm	ML	32.820835483283425	-0.41498701035588315	11259
3f1f0646c04d23654df18d871d197043536bd721	autonomous exploration with a low-cost quadrocopter using semi-dense monocular slam	jakob engel;members;engelj	Micro aerial vehicles (MAVs) are strongly limited in their payload and power capacity. In order to implement autonomous navigation, algorithms are therefore desirable that use sensory equipment that is as small, low-weight, and lowpower consuming as possible. In this paper, we propose a method for autonomous MAV navigation and exploration using a low-cost consumer-grade quadrocopter equipped with a monocular camera. Our vision-based navigation system builds on LSD-SLAM which estimates the MAV trajectory and a semidense reconstruction of the environment in real-time. Since LSD-SLAM only determines depth at high gradient pixels, texture-less areas are not directly observed. We propose an obstacle mapping and exploration approach that takes this property into account. In experiments, we demonstrate our vision-based autonomous navigation and exploration system with a commercially available Parrot Bebop MAV.	aerial photography;algorithm;automated planning and scheduling;autonomous robot;experiment;gradient;line-of-sight (missile);low-power broadcasting;map;motion planning;on-board data handling;parrot virtual machine;pixel;real-time clock;semiconductor industry;simultaneous localization and mapping;video tracking;voxel	Lukas von Stumberg;Vladyslav C. Usenko;Jakob Engel;Jörg Stückler;Daniel Cremers	2016	CoRR		computer vision;simulation;computer science;enumerated type;remote sensing	Robotics	53.661810533840075	-36.82155374225129	11262
31ab62812b7c8a901e3c230ba4e28a42586d181d	ensemble stream model for data-cleaning in sensor networks	mobile sensor networks;renewable energy;routing;bagging;data mining;power aware routing;randomforest;machine learning;sensor networks;feature extraction;data aggregation;data cleaning;classifiers;quality of service;netcoding;quality of data	ENSEMBLE STREAM MODEL FOR DATA-CLEANING IN SENSOR NETWORKS by Vasanth Iyer Florida International University, 2013. Miami, Florida Professor S.S. Iyengar, Co-Major Professor Professor Niki Pissinou, Co-Major Professor Ensemble Stream Modeling and Data-cleaning are sensor information processing systems have different training and testing methods by which their goals are crossvalidated. This research examines a mechanism, which seeks to extract novel patterns by generating ensembles from data. The main goal of label-less stream processing is to process the sensed events to eliminate the noises that are uncorrelated, and choose the most likely model without over fitting thus obtaining higher model confidence. Higher quality streams can be realized by combining many short streams into an ensemble which has the desired quality. The framework for the investigation is an existing data mining tool. First, to accommodate feature extraction such as a bush or natural forest-fire event we make an assumption of the burnt area (BA∗), sensed ground truth as our target variable obtained from logs. Even though this is an obvious model choice the results are disappointing. The reasons for this are two: One, the histogram of fire activity is highly skewed. Two, the measured sensor parameters are highly correlated. Since using non descriptive features does not yield good results, we resort to temporal features. By doing so we carefully eliminate the averaging effects; the resulting histogram is more satisfactory and conceptual knowledge is learned from sensor streams.	data mining;feature extraction;ground truth;information processing;overfitting;plasma cleaning;sensor;stream processing	Vasanth Iyer;S. Sitharama Iyengar;Niki Pissinou	2015	AI Matters	10.1145/2757001.2757006	computer science;machine learning;pattern recognition;data mining	AI	22.43639234923092	-28.532015192482042	11266
fff847b97f7dddd7094355eb6587ce8fcf4112be	testing for random effects in panel data under cross sectional error correlation - a bootstrap approach to the breusch pagan test	experimental design;time correlation;62f40;error aleatorio;infinite dimension;approximation asymptotique;panel data;metodo estadistico;finite sample;test statistique;bootstrap;analisis datos;cross correlation;05bxx;random effects;correlation temporelle;estadistica test;statistique test;62h20;correlation croisee;cross sectional study;test estadistico;simulation;echantillon fini;plan experiencia;statistical test;dimension infinie;loi conditionnelle;simulacion;statistical method;ley condicional;donnee panel;valor critico;statistical regression;size distortion;62jxx;breusch pagan test;erreur aleatoire;estudio transversal;contemporaneous error correlation;conditional test;wild bootstrap;data analysis;correlacion temporal;correlation spatiale;efecto aleatorio;estimation erreur;62k99;spatial correlation;correlacion espacial;plan experience;random effect;error estimation;methode statistique;regresion estadistica;statistical computation;estimacion error;calculo estadistico;analyse correlation;analyse donnee;seemingly unrelated regression;cross section;random error;calcul statistique;random effects model;asymptotic approximation;valeur critique;regression statistique;effet aleatoire;critical value;conditional distribution;test statistic;random times;analisis correlacion;dimension infinita;correlacion cruzada;aproximacion asintotica;correlation analysis;test conditionnel;etude transversale	When testing the pooled regression via the Breusch Pagan test model disturbances are often assumed to be i.i.d. over both the time and the cross section dimension.A bootstrap approach to generate critical values for the Breusch Pagan statistic is provided which is valid under heteroskedasticity and cross sectional correlation as typically formalized in the framework of seemingly unrelated regressions. Moreover, asymptotic results are derived for a finite cross section and infinite time dimension. Finite sample simulations show that ignoring cross sectional correlation may lead to large size distortions in practice. Conditional versions of the test statistic designed to cope with random time effects or spatial error correlation show empirical size distortions in case the source of contemporaneous error correlation is misspecified. Moreover, the bootstrap is robust if contemporaneous error correlation is induced by random time effects or in case of spatial error correlation. © 2005 Elsevier B.V. All rights reserved.	booting;bootstrapping (statistics);cross section (geometry);distortion;panel data;random effects model;simulation	Helmut Herwartz	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.08.003	econometrics;breusch–pagan test;calculus;mathematics;statistics;random effects model	ML	32.98306944881235	-21.97009720540877	11275
dd6e336499a2856987730cd88144a33d278993ee	a decision-making model of social shopping in franchising: assessing collaboration strategies	collaboration;social intermediary;social shopping;franchise;daily deal promotion	Our paper develops a decision-making model of social shopping in franchising to understand impacts of various collaboration strategies on pro ̄ts of a social intermediary, a franchisor, and a franchisee. Three decision variables are considered to make a daily deal promotion in a manner that results in optimal pro ̄ts: the social intermediary's advertising expense, the franchisee's service quality expense, and the franchisor's ̄nancial assistance to the franchisee. The analysis shows that while system-wide collaboration generates more total pro ̄t for all three parties than the pro ̄ts obtained when they seek to optimize their individual pro ̄t functions, the advantage of system-wide collaboration may depend on the parameter values of the pro ̄t function and collaboration cost. Since social shopping is just in the early stage of the business model development, most franchises and social intermediaries have not formed concrete collaboration strategies. Our model provides a methodology for di®erent collaborative needs of the franchises and social intermediaries.	algebraic equation;decision theory;e-commerce;interaction;nl (complexity);numerical analysis;numerical aperture;online advertising;open road tolling;purchasing;quality of service;social network;social shopping;web content development	In Lee;Choong Kwon Lee;Sangjin Yoo;Moo-Jin Choi	2015	International Journal of Information Technology and Decision Making	10.1142/S0219622014500898	public relations;economics;marketing;management;commerce;collaboration	HCI	-3.4159328166403102	-10.06634711076212	11277
a80e8e97296c268b7e0fa70e6e54865c81b3d864	multiobjective dynamic optimization of vaccination campaigns using convex quadratic approximation local search	susceptible infected recovered;differential equation;objective function;high dimension;local search;dynamic optimization problem;impulse control;dynamic optimization;evolutionary computing	The planning of vaccination campaigns has the purpose of minimizing both the number of infected individuals in a time horizon and the cost to implement the control policy. This planning task is stated here as a multiobjective dynamic optimization problem of impulsive control design, in which the number of campaigns, the time interval between them and the number of vaccinated individuals in each campaign are the decision variables. The SIR (Susceptible-Infected-Recovered) differential equation model is employed for representing the epidemics. Due to the high dimension of the decision variable space, the usual evolutionary computation algorithms are not suitable for finding the efficient solutions. A hybrid optimization machinery composed by the canonical NSGA-II coupled with a local search procedure based on Convex Quadratic Approximation (CQA) models of the objective functions is used for performing the optimization task. The final results show that optimal vaccination campaigns with different trade-offs can be designed using the proposed scheme.	approximation;local search (optimization)	André R. da Cruz;Rodrigo T. N. Cardoso;Ricardo H. C. Takahashi	2011		10.1007/978-3-642-19893-9_28	mathematical optimization;simulation;multi-objective optimization;mathematics;mathematical economics;metaheuristic	ML	16.078960822468	-2.0611863775316266	11302
02c3dd39994ef52b7c8c558099d88e4d648a3de5	false-positive free hilbert and multi-resolution-based image watermarking technique using firefly optimisation algorithm	optimisation;multi resolution transform;geometrical attacks;false positives detection;hilbert transform;firefly algorithm;principal component analysis;robustness;image watermarking;image processing attacks;image security;pca;watermark imperceptibility	To increase the robustness and imperceptibility of the image watermarking scheme, a novel optimisation technique is proposed. The presented watermarking scheme uses Hilbert and multi-resolution transform (DWT) with principal component analysis (PCA)-based subband selection method for determining the suitable location for embedding. The modified firefly optimisation algorithm is used to decide the optimal scaling values among the multiple scaling factors for embedding the watermark within the cover image in hybrid transform space. Since, the embedding process is done in Hilbert transform domain, the imperceptibility of the watermark is improved and it is free from false positive detection. The experimental results shows that the presented approach is more robust against different kinds of image processing attacks such as, Gaussian filter, Poisson noise, salt and pepper noise, median filter, contrast adjustment, histogram equalisation, row and column deletion, JPEG compression and geometrical attacks such a...	algorithm;digital watermarking;firefly (cache coherence protocol);mathematical optimization	Loganathan Agilandeeswari;Ganesan Kaliyaperumal	2016	IJIPSI	10.1504/IJIPSI.2016.10003007	computer vision;mathematical optimization;theoretical computer science;mathematics	Robotics	41.24303122729679	-10.42884290097939	11311
e539ae3d830298d2f912d8c83c153eb242cf4031	a data analysis method and its applications in excel	data analysis;linear regression	Data analysis is a quite important process and has been extensively employed in many areas. Especially in statistics, the distribution type test of data often needs to be handled. This paper presents that linear regression as a type of universal method can be applied to the distribution type test. In reliability engineering due to the failure data is commonly of non-linear relationship so that the linear regression method can not be directly employed. This difficulty can be resolved through linear transformation. Aimed for the linearization procedure of four kind of typical distributions, e.g., exponential distribution, normal distribution, logarithmic normal distribution and twoparameter Weibull distribution, their transformations are respectively different. After linearization transformations, correlation coefficients had been used as a criterion to choose the most matched distribution type. This method can be conveniently operated in MS EXCEL. Several samples and experiments illustrated the detailed transformation and the efficiency than other methods.	coefficient;experiment;nonlinear system;pedobarography;reliability engineering;time complexity	Jinlei Qin;Yuguang Niu;Zhijun Li	2014	JSW		gumbel distribution;mathematical optimization;chi-square test;computer science;linear regression;generalized linear model;bayesian linear regression;data analysis;chi-squared distribution;anderson–darling test;statistics;probability integral transform;three-point estimation	DB	29.675384784671696	-20.380527087062372	11313
590fb2b1dee70ea93c6091c95e9e78e073592d2a	pose robust face tracking by combining view-based aams and temporal filters	asm;kalman filter;face tracking;temporal matching filter;random forest;shape initialization;view based aam	Active appearance models (AAMs) are useful for face tracking for the advantages of detailed face interpretation, accurate alignment and high efficiency. However, they are sensitive to initial parameters and may easily be stuck in local minima due to the gradient-descent optimization, which makes the AAM based face tracker unstable in the presence of large pose deviation and fast motion. In this paper, we propose to combine the view-based AAMs with two novel temporal filters to overcome the limitations. First, we build a new view space based on the shape parameters of AAMs, instead of the model parameters controlling both the shape and appearance, for the purpose of pose estimation. Then the Kalman filter is used to simultaneously update the pose and shape parameters for a better fitting of each frame. Second, we propose a temporal matching filter which is twofold. The inter-frame local appearance constraint is incorporated into AAM fitting, where the mechanism of the active shape model (ASM) is also implemented in a unified framework to find more accurate matching points. Moreover, we propose to initialize the shape with correspondences found by a random forest based local feature matching. By introducing the local information and temporal correspondences, the twofold temporal matching filter improves the tracking stability when confronted with fast appearance changes. Experimental results show that our algorithm is more pose robust than basic AAMs and some state-of-art AAM based methods, and that it can also handle large expressions and non-extreme illumination changes in test video	active appearance model;active shape model;algorithm;automatic acoustic management;control theory;facial motion capture;gradient descent;kalman filter;mathematical optimization;maxima and minima;particle filter;random forest;unified framework;yaws	Chen Huang;Xiaoqing Ding;Chi Fang	2012	Computer Vision and Image Understanding	10.1016/j.cviu.2012.02.007	kalman filter;random forest;computer vision;facial motion capture;computer science;machine learning;pattern recognition;mathematics	Vision	47.62219063367982	-48.964041757790035	11319
e475aacf14c95a0bdc703dd716c86da27529086b	virtual progress: the effect of path characteristics on perceptions of progress and choice	goal oriented services;goal orientation;consumer choice;progress;behavioral decision making;transportation;path characteristics;choice;analytical model	Transportation services may be defined as services in which the consumer’s goal is to get transported from one well-defined state (start) to another (destination) state without concern for intermediate states. Examples of transportation services include commuter trains and shuttles, time bound construction contracts and turnkey projects. By definition, the evaluation of such services should depend on the price and the amount of time taken for completion but not on the path taken to reach the final destination. In this paper, however, we demonstrate that the characteristics of the specific path to the final destination influences its prospective evaluation and choice. Specifically, we argue that obstacles (like idle time and travel away from the final destination) are seen as obstacles in the progress towards the destination. Hence, the presence of such obstacles reduces the anticipated value of the service and hence the likelihood of choosing it. Further, we argue that the earlier such obstacles occur in the service, the lower is the choice probability. We present an analytical model of consumer choice behavior, and test the predictions emerging from it in a series of experiments. Our results show that in choosing between two services which cover the same displacement in the same time (i.e. identical actual progress), consumer choice is driven by the perception of progress towards the goal (i.e. by virtual progress). In a final experiment, we show that the effects of virtual progress may outweigh the effects of actual progress.	displacement mapping;experiment;prospective search;turnkey	Dilip Soman;Mengze Shi	2003	Management Science	10.1287/mnsc.49.9.1229.16574	transport;economics;marketing;operations management;goal orientation;management science;management;welfare economics	HCI	1.501155533856911	-7.877072607307787	11320
cdc4124ca02f3e7f58bcc8fc7e822783200a9b20	synthetically improved genetic algorithm on the traveling salesman problem in material transportation	traveling salesman problem;travelling salesman problems dispatching genetic algorithms goods distribution logistics materials handling stochastic programming;goods distribution;vehicle dispatching genetic algorithm traveling salesman problem material transportation logistics distribution rotation algorithm stochastic programming stochastic disturbance recovery theory;vehicle dispatching stochastic disturbance recovery theory rotation algorithm traveling salesman problem synthetically improved genetic algorithm;logistics;materials handling;travelling salesman problems;route choice;improved genetic algorithm;genetic algorithms;genetic algorithms traveling salesman problems transportation genetics optimization mathematical model cities and towns;stochastic programming;high speed;logistic distribution;dispatching	The research of traveling salesman problem is important in logistics distribution. There are many stochastic disturbance factors affecting logistics distribution in real life. So the result objectivity based on distribution and recovery theory and rotation algorithm to traveling salesman problem is not good. In consideration of stochastic factors affecting on logistics distribution, using stochastic programming theory and putting forward stochastic disturbance recovery theory and synthetically improved genetic algorithm with rotation algorithm we can solve the traveling salesman problem effectively. This way can avoid the higher difference between theoretical results and the ideal ones in order to promote the manipulation of the optimal plan. Last, we take the vehicle dispatch and route choice of material transportation in Hebei province as an example, and make the simulation test to verify the affectivity of the algorithm in the meantime. The result indicates that applying the synthetically improved genetic algorithm we can improve the material transportation efficiency, and achieve high-speed mathematical operation.	dynamic dispatch;genetic algorithm;logistics;objectivity/db;plan 9 from bell labs;real life;simulation;stochastic programming;travelling salesman problem	Huizi An;Wei Li	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023808	stochastic programming;logistic distribution;traveling purchaser problem;logistics;2-opt;mathematical optimization;simulation;genetic algorithm;computer science;mathematics;travelling salesman problem;3-opt;statistics;bottleneck traveling salesman problem	Robotics	15.960925938482807	-3.139731615482061	11332
5493ba8a71415c95c13f6e28c3f658099b2f8e54	a symbolic-numeric approach to multi-objective optimization in manufacturing design		Some important classes of optimization problems originating from the optimal design of semiconductor memories such as SRAM, aiming at boosting the yield rate, are studied. New optimization methods for the classes based on a symbolic algorithm called quantifier elimination, combined with numerical computation, are proposed. The total efficiency of the design process is improved by reducing the number of numerical yield-rate evaluations. In addition, useful information such as the explicit relations among design variables, objective functions, and the yield rate, is provided.	multi-objective optimization	Hidenao Iwane;Hitoshi Yanami;Hirokazu Anai	2011	Mathematics in Computer Science	10.1007/s11786-011-0097-y	mathematical optimization;discrete mathematics;test functions for optimization;mathematics;algorithm	Logic	15.050285260332462	-6.202749097013673	11342
9bc4b2ab15e4000e7181ced0b9036cec8decb151	customization competition between branded firms: continuous extension of product line from core product	game theory;pricing;marketing and manufacturing interfaces;product differentiation;selection cost;production	We study a competition of product customization between two branded firms by a game-theoretic approach. Firms produce products with two attributes: one attribute indicates a characteristic with regard to “function” or “design” of a product and the other indicates “taste” or “flavor” of the product, which reflects consumers’ brand/taste preferences. Two branded firms have their own specific core products and our customization is defined as a continuous extension of their product line from the core product only along the “function” attribute. In particular, we allow asymmetric positions of core products, which may create the position advantage/disadvantage between firms. We suppose that consumers incur their selection costs with regard to finding their most favorable item among a rich variety of products and firms incur their customizing costs with regard to extending their product lines. We first show that in the equilibrium, branded firms should fundamentally adopt their customizations to cover the center space in the market as far as possible, regardless of the position of the competitor’s core product. Therefore, the position of the core product contributes to the creation of a competitive advantage: when one firm’s core product is located more closely to the center of the market than the competitor’s, its customization can always cover more range of the center space in the market, while keeping its degree of customization smaller than the competitor’s. Furthermore, we show some implications of unit-cost improvement: in a short run, a firm is better off concentrating on the improvement of the unit selection cost rather than the unit customizing cost. In contrast, in a long run, both firms can benefit from the improvement of the unit customizing cost.		Noritsugu Takagoshi;Nobuo Matsubayashi	2013	European Journal of Operational Research	10.1016/j.ejor.2012.10.001	pricing;game theory;economics;marketing;operations management;product differentiation;core product;commerce	Theory	-0.004848973257666925	-7.819647719134301	11345
cd5a236cab29af8390cd64434e6c8a4950833992	gene expression programming with multiple chromosomes	gep;gene expression programming;mc gep;multi chromosomes	Gene expression programming (GEP) has been widely used in the areas of pattern recognition and knowledge discovery, however, when dealing with complicated problems, it is very time-consuming and the number of generations is large. In order to overcome these drawbacks, this paper proposes a multi-chromosomes GEP algorithm (MC-GEP). Firstly, the individual is composed of multiple chromosomes, each chromosome consists of one or more genes. Secondly, the expression of each chromosome or combinations of several chromosomes may be chosen to indicate the individual. Finally, chromosome recombination is changed and performed orderly like meiosis. Experimental results show that MC-GEP can reduce the running time and the number of generations with respect to the GEP.	gene expression programming	Bo Wang;Min Yao;Rong Zhu	2011	IJMIC	10.1504/IJMIC.2011.043145	computer science;bioinformatics;machine learning;gene expression programming	PL	4.180219934912927	-46.539693163622594	11357
bf5f5e60227f33097e2228925c9980df5bd7fda2	occlusion-aware hmm-based tracking by learning	discriminative methods;video signal processing hidden markov models tracking;hidden markov models;hidden markov models classification algorithms feature extraction adaptation models computer vision target tracking standards;occlusion handling;visual tracking occlusion aware hmm based tracking classifier bootstraps feature selection method discriminative power hidden markov model video sequences;occlusion handling tracking by detection discriminative methods hidden markov models;tracking by detection	Recently, an emerging class of methods, namely tracking by detection, achieved quite promising results on challenging tracking data sets. These techniques train a classifier in an online manner to separate the object from its background. These methods only take input location of the object and a random feature pool; then, a classifier bootstraps itself by using the current tracker state and extracted positive and negative samples. Following these approaches, a novel tracking system is proposed. A feature selection method is introduced to increase the discriminative power of the classifier. During tracking, a Hidden Markov Model (HMM) is utilized to filter the features that improve the performance. Moreover, a state of the proposed HMM is allocated to handle occlusions. The proposed tracker is tested on publicly available challenging video sequences and superior tracking results are achieved in real-time.	feature selection;hidden markov model;markov chain;naive bayes classifier;real-time clock;statistical classification;tracking system	Tughan Marpuc;A. Aydin Alatan	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025997	maximum-entropy markov model;speech recognition;computer science;machine learning;video tracking;pattern recognition;hidden markov model	Vision	34.35169580910692	-47.66474481986845	11385
a1b40e64a822b16b8a35ff8fb960d6779f3b9e68	a bilevel programming approach to double optimal stopping	bilevel programming problem;integral options;double optimal stopping problem	This paper treats a class of double optimal stopping problems arising in the pricing of integral options. Under certain conditions, we give an explicit form of the double stopping time for such type of optimal stopping problems. The present results are essentially derived by solving a certain nonlinear bilevel programming problem explicitly.	optimal stopping	Cloud Makasu	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.04.024	mathematical optimization;combinatorics;optimal stopping;mathematics;mathematical economics;odds algorithm	Theory	1.9547054408729942	-2.4028068297494025	11398
e77a4e6524a08ff79618858604680b9710c640de	optimization-based maneuver automata for cooperative trajectory planning of autonomous vehicles		In this paper, a major challenge in the field of trajectory planning for autonomous on-road vehicles is addressed: to calculate trajectories efficiently and reliably. The proposed method is based on a cooperative maneuver automaton, in which each maneuver is formulated as a discrete-time hybrid automaton that defines constraints on the behavior of a group of autonomous vehicles. For trajectory planning, these constraints enter an optimal control problem, which is formulated as a Mixed-Integer Quadratic Program (MIQP). Despite good practical performance, these programs have adverse worst-case run-time properties, which, however, are improved by the proposed approach. Also, calculation of controllable sets for the hybrid automata allows to determine both the feasibility of an optimal control problem for a given initial state and the required time to complete the maneuver in advance- without solving the optimization problem. The efficacy of the proposed method is demonstrated in an example scenario.	automata theory;autonomous robot;best, worst and average case;drug vehicle;hybrid automaton;integer (number);lymphoma, mixed-cell, follicular;mathematical optimization;motion planning;optimal control;optimization problem;quadratic programming;run time (program lifecycle phase)	Jan Eilbrecht;Olaf Stursberg	2018	2018 European Control Conference (ECC)	10.23919/ECC.2018.8550422	mathematical optimization;quadratic programming;vehicle dynamics;optimal control;hybrid automaton;trajectory;optimization problem;computer science	Robotics	53.137735789078725	-21.45448842975484	11401
382ffbfe1f99ce73edd1043b0e72fb6c913ee455	eyelight: light-and-shadow-based occupancy estimation and room activity recognition		This paper explores the feasibility of localizing and detecting activities of building occupants using visible light sensing across a mesh of light bulbs. Existing Visible Light activity sensing (VLS) techniques require either light sensors to be deployed on the floor or a person to carry a device. Our approach integrates photosensors with light bulbs and exploits the light reflected off the floor to achieve an entirely device-free and light source based system. This forms a mesh of virtual light barriers across networked lights to track shadows cast by occupants. The design employs a synchronization circuit that implements a time division signaling scheme to differentiate between light sources and a sensitive sensing circuit to detect small changes in weak reflections. Sensor readings are fed into indoor supervised tracking algorithms as well as occupancy and activity recognition classifiers. Our prototype uses modified off-the-shelf LED flood light bulbs and is installed in a typical office conference room. We evaluate the performance of our system in terms of localization, occupancy estimation and activity classification, and find a 0.89m median localization error as well as 93.7% and 93.78% occupancy and activity classification accuracy, respectively.	activity recognition;algorithm;internationalization and localization;prototype;reflection (computer graphics);sensor;von luschan's chromatic scale	Viet Nguyen;Mohamed Ibrahim;Siddharth Rupavatharam;Minitha Jawahar;Marco Gruteser;Richard E. Howard	2018	IEEE INFOCOM 2018 - IEEE Conference on Computer Communications	10.1109/INFOCOM.2018.8485867	occupancy;photodetector;light-emitting diode;computer vision;synchronization;distributed computing;visible spectrum;computer science;activity recognition;artificial intelligence;shadow	Mobile	46.449348555045816	-42.708534317661226	11406
16011e39d6d99fd97ec1f4c81cc017bcf9f24a02	a methodology for dynamic data mining based on fuzzy clustering	cluster algorithm;procesamiento informacion;algoritmo borroso;gestion trafic;algorithme amas;segmentation;traffic management;data mining;fuzzy clustering;dynamic data;fouille donnee;fuzzy algorithm;information processing;gestion trafico;algorithme flou;sistema difuso;systeme flou;traitement information;busca dato;segmentacion;fuzzy system	Dynamic data mining is increasingly attracting attention from the respective research community. On the other hand, users of installed data mining systems are also interested in the related techniques and will be even more since most of these installations will need to be updated in the future. For each data mining technique used, we need di1erent methodologies for dynamic data mining. In this paper, we present a methodology for dynamic data mining based on fuzzy clustering. Using the implementation of the proposed system we show its bene4ts in two application areas: customer segmentation and tra6c management. c © 2004 Elsevier B.V. All rights reserved.	application domain;cluster analysis;data mining;dynamic data;fuzzy clustering;mit engineering systems division	Fernando A. Crespo;Richard Weber	2005	Fuzzy Sets and Systems	10.1016/j.fss.2004.03.028	concept mining;mining feasibility study;active traffic management;text mining;dynamic data;information processing;fuzzy clustering;computer science;artificial intelligence;machine learning;data mining;data stream mining;operations research;segmentation;fuzzy control system	AI	-2.714577869426175	-32.49489275952144	11415
51913c230bd11995eeba440ef3768e850ae26a5b	application-oriented design of smart camera networks	belief networks;multi object tracking;occupancy sensing;single object tracking;stereo vision systems;bayes methods;power efficiency;low resolution;application oriented design;distributed bayesian filtering algorithm;stereo vision systems smart camera networks bayesian estimation target tracking occupancy sensing multi object tracking;image sensors;smart camera networks;wireless sensor network;system design;object tracking;stereo image processing;stereo vision;smart cameras;smart cameras measurement target tracking intelligent networks surveillance network topology cost function wireless sensor networks delay bayesian methods;bayesian estimator;wireless sensor networks distributed intelligence power efficiency smart cameras tracking;target tracking;distributed intelligence;camera network;wireless sensor networks belief networks cameras image sensors stereo image processing target tracking;wireless sensor networks;image sensor;intelligent sensors;filtering theory;cameras;tracking;design methodology;camera resolution	System design aspects must be considered to effectively map an application onto the constraints of a smart camera network. Therefore, we propose an application-driven design methodology that enables the determination of an output set of operation parameters given an input set of application requirements. We illustrate this approach utilizing distributed, sequential Bayesian estimation for several applications including target tracking, occupancy sensing and multi-object tracking. Observation models for single camera and stereo vision systems are introduced with a particular focus on low-resolution image sensors. Early simulation results indicate that (i) stereo vision can increase tracking accuracy by about a factor of five over single camera vision and (ii) doubling camera resolution can result in more than twice the accuracy.	experiment;image sensor;particle filter;period-doubling bifurcation;requirement;sequential consistency;simulation;smart camera;stereo camera;stereopsis	Stephan Hengstler;Hamid K. Aghajan;A. Goldsmith	2007	2007 First ACM/IEEE International Conference on Distributed Smart Cameras	10.1109/ICDSC.2007.4357500	computer stereo vision;smart camera;embedded system;computer vision;camera auto-calibration;simulation;wireless sensor network;image resolution;computer science;image sensor;optics	Robotics	51.51081210382006	-32.9279558964121	11421
197ea293ee5aeade2fb0d4ee452be871855b52d5	comparison of windowing in speech and audio coding	speech coding;perceptual modelling lapped orthogonal transform speech coding audio coding windowing;audio coding;speech codecs;perceptual quality speech coding audio coding speech paradigms audio paradigms audio codecs perceptual model transform domain speech codecs temporal continuity linear predictive filtering residual domain;speech coding audio coding speech codecs;transforms speech speech coding codecs audio coding delays quantization signal	Speech and audio coding have during the last decade converged to an increasingly unified technology. This contribution discusses one of the remaining fundamental differences between speech and audio paradigms, namely, windowing of the input signal. Audio codecs generally use lapped transforms and apply a perceptual model in the transform domain, whereby temporal continuity is achieved by windowing and overlap-add. Speech codecs on the other hand achieve temporal continuity by using linear predictive filtering, whereby windowing is applied in the residual domain. Despite these fundamental differences, we demonstrate that the two windowing approaches, combined with perceptual modeling, perform very similarly both in terms of perceptual quality and theoretical properties.	codec;lapped transform;overlap–add method;programming paradigm;scott continuity;speech coding	Tomas Bäckström	2013	2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics	10.1109/WASPAA.2013.6701853	voice activity detection;sub-band coding;adaptive multi-rate audio codec;codec2;g.729;linear predictive coding;speech recognition;mpeg-4 part 3;acoustics;bandwidth extension;computer science;speech coding;speech processing;acoustic model;psqm	Vision	47.89876642467447	-8.861340393494718	11426
85f616df46c819534b6dc58df082c59a2945923b	knowledge-based consistency index for fuzzy pairwise comparison matrices		Fuzzy AHP is today one of the most used Multiple Criteria Decision-Making (MCDM) techniques. The main argument to introduce fuzzy set theory within AHP lies in its ability to handle uncertainty and vagueness arising from decision makers (when performing pairwise comparisons between a set of criteria/alternatives). As humans usually reason with granular information rather than precise one, such pairwise comparisons may contain some degree of inconsistency that needs to be properly tackled to guarantee the relevance of the result/ranking. Over the last decades, several consistency indexes designed for fuzzy pairwise comparison matrices (FPCMs) were proposed, as will be discussed in this article. However, for some decision theory specialists, it appears that most of these indexes fail to be properly “axiomatically” founded, thus leading to misleading results. To overcome this, a new index, referred to as KCI (Knowledge-based Consistency Index) is introduced in this paper, and later compared with an existing index that is axiomatically well founded. The comparison results show that (i) both indexes perform similarly from a consistency measurement perspective, but (ii) KCI contributes to significantly reduce the computation time, which can save expert's time in some MCDM problems.		Sylvain Kubler;William Derigent;Alexandre Voisin;Jérémy Robert;Yves Le Traon	2017	2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2017.8015380	fuzzy logic;machine learning;artificial intelligence;fuzzy set operations;defuzzification;fuzzy set;type-2 fuzzy sets and systems;pairwise comparison;fuzzy classification;fuzzy number;mathematics	DB	-4.493769194932359	-21.01243578204452	11435
dfb5c92184c3bfc36ba7f33dd35269f9e183bf80	mission operations scheduling: complexity and resolution methods	satellite scheduling;simulation ground station scheduling satellite scheduling genetic algorithms constraint programming;job shop scheduling;processor scheduling;simulation;ground station scheduling;ground support systems;satellite broadcasting;scheduling;satellites;conference report;constraint programming;genetic algorithms mission operations scheduling resolution method satellite space missions management operations teams spacecraft systems european space agency nasa satellite mission intelligent systems ground station services satellite scheduling optimization objectives windows accessibility ground station scheduling;artificial satellites;space missions;genetic algorithms;scheduling artificial satellites genetic algorithms ground support systems;space vehicles;satellites space vehicles processor scheduling job shop scheduling satellite broadcasting space missions	Recently there has been a growing interest in mission operations scheduling problem. The problem, in a variety of formulations, arises in management of satellite/space missions requiring efficient allocation of user requests to make possible the communication between operations teams and spacecraft systems. Not only large space agencies, such as ESA (European Space Agency) and NASA, but also smaller research institutions and universities can establish nowadays their satellite mission, and thus need intelligent systems to automate the allocation of ground station services to space missions. In this paper, we survey some relevant formulations of the satellite scheduling viewed as a family of problems and identify various forms of optimization objectives. The main complexities, due highly constrained nature, windows accessibility and visibility, multi-objectives and conflicting objectives are examined. Then, we discuss the resolution of the problem through different heuristic methods. In particular, we focus on the version of ground station scheduling, for which we present some computational results for the case of the multi-ground stations scheduling obtained with Genetic Algorithms using the STK simulation toolkit.	accessibility;computation;esa;genetic algorithm;heuristic;mathematical optimization;microsoft windows;stk;scheduling (computing);simulation	Fatos Xhafa;Junzi Sun;Admir Barolli;Makoto Takizawa	2012	2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems	10.1109/CISIS.2012.23	nurse scheduling problem;real-time computing;simulation;dynamic priority scheduling;engineering;operations management	Robotics	14.30824770467286	2.21364775108085	11442
22a6db98ed4a248152a551079e3e1faba5e35549	self-sustained activity in attractor networks using neuromorphic vlsi	silicon;effective transfer function;protocols;neuromorphic vlsi;neural nets;vlsi analogue integrated circuits neural nets transfer functions;network on chip;transfer functions;neuromorphics;analog vlsi chips;semiconductor device measurement;system on a chip;neurons transfer functions system on a chip semiconductor device measurement silicon protocols neuromorphics;chip;self sustained activity;effective transfer function self sustained activity attractor networks neuromorphic vlsi analog vlsi chips on chip network address event representation based system mean field approximation;address event representation;analogue integrated circuits;parameter space;vlsi;analog vlsi;attractor neural network;address event representation based system;spike train;integrate and fire neuron;collective behaviour;neurons;on chip network;mean field approximation;attractor networks;equilibrium state	We describe and demonstrate the implementation of attractor neural network dynamics in analog VLSI chips [1]. The on-chip network is composed of an excitatory and an inhibitory population of recurrently connected linear integrate-and-fire neurons. Besides the recurrent input these two populations receive external input in the form of spike trains from an Address-Event-Representation (AER) based system. External AER input stimulates the attractor network and provides also an adequate background activity for the on-chip populations. We use the mean-field approximation of a model attractor neural network to identify regions of parameter space allowing for attractor states, matching hardware constraints. Consistency between theoretical predictions and the observed collective behaviour of the network on chip is checked using the ‘effective transfer function’ (ETF) [2]. We demonstrate that the silicon network can support two equilibrium states of sustained firing activity that are attractors of the dynamics, and that external stimulation can provoke a transition from the lower to the higher state.	analog-to-digital converter;approximation;artificial neural network;biological neuron model;conventional pci;embedded system;emulator;high- and low-level;network on a chip;neuromorphic engineering;population;quantum field theory;recurrent neural network;synaptic package manager;synthetic intelligence;transfer function;usability;very-large-scale integration	Patrick Camilleri;Massimiliano Giulioni;Maurizio Mattia;Jochen Braun;Paolo Del Giudice	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596342	chip;system on a chip;communications protocol;thermodynamic equilibrium;computer science;theoretical computer science;mean field theory;machine learning;transfer function;very-large-scale integration;network on a chip;parameter space;silicon;artificial neural network	ML	39.91914146617506	-1.423489086383237	11443
3e865666ca15bb87a781f8e5a356d09fc2bb3960	animal polarization imaging and implications for optical processing	animals;image processing;stokes parameters;zoology biological techniques biology computing bio optics image processing light polarisation optical design techniques;optical polarization;biophotonics;mantis shrimp animal polarization imaging optical processing dedicated filter designs hard wired sensory systems data specific efficiency task specific efficiency animal visual systems spatial imaging polarization image processing fiddler crabs cuttlefish octopus;animals visualization image color analysis sensitivity polarization image processing photoreceptors biosensors;stokes parameters animals biophotonics image processing optical polarization	Biologically inspired solutions for modern-day sensory systems promise to deliver both higher capacity and faster, more efficient processing of information than current computational approaches. Many animals are able to perform remarkable sensing tasks despite only being able to process what would be considered modest data rates and bandwidths. The key biological innovations revolve around dedicated filter designs. By sacrificing some flexibility, specifically matched and hard-wired sensory systems, designed primarily for single roles, provide a blueprint for data and task-specific efficiency. In this paper, we examine several animal visual systems designed to use the polarization of light in spatial imaging. We investigate some implications for artificial optical processing based on models of polarization image processing in fiddler crabs, cuttlefish, octopus, and mantis shrimp.	blueprint;computation;fiddler;image processing;polarization (waves)	Nicholas W. Roberts;Martin J. How;Megan L. Porter;Shelby E. Temple;Roy L. Caldwell;Samuel B. Powell;Viktor Gruev;N. Justin Marshall;Thomas W. Cronin	2014	Proceedings of the IEEE	10.1109/JPROC.2014.2341692	image processing;nanotechnology;optics;biophotonics;physics;stokes parameters	Visualization	46.55539581567284	-32.57510850787074	11448
8dd21b8ba68686c0af5360ee5e4aa1e9c244808e	optimizing individual activity personal plans through local search		Optimization through local search is known to be a powerful approach to confront complex optimization problems. In this article we tackle the problem of optimizing individual activity personal plans, that is, plans involving activities one person has to accomplish independently of others, taking into account complex constraints and preferences. Recently, this problem has been addressed adequately using an adaptation of the squeaky wheel optimization framework (SWO). In this article we demonstrate that further improvement can be achieved in the quality of the resulting plans, by coupling SWO with a post-optimization phase based on local search techniques. Particularly, we present a bundle of transformation methods to explore the neighborhood of the solution produced by SWO using either hill climbing or simulated annealing. Similar results can be obtained by employing local search only, starting from an empty plan, thus demonstrating the strength of the proposed local search techniques. We present several experiments that demonstrate an improvement on the utility of the produced plans, with respect to the solutions produced by SWO only, of more than 6% on average, which in particular cases exceeds 20%. Of course, this improvement comes at the cost of extra time.	local search (optimization);optimizing compiler	Anastasios Alexiadis;Ioannis Refanidis	2016	AI Commun.	10.3233/AIC-150680	mathematical optimization;simulation;artificial intelligence	AI	22.581218051127177	-2.8559497741810964	11458
0d0a82f665ea5e4dc7ef5d8d42dc1f735d721fb4	fuzzy rule extraction from id3-type decision trees for real data	decision tree;fuzzy rules;rule based;classifier system;rule extraction;data sets fuzzy rule extraction id3 type decision trees real data fuzzy rule based classifier system rule extraction gradient descent tuning performance based pruning;fuzzy logic;knowledge acquisition decision trees fuzzy systems fuzzy logic knowledge based systems;fuzzy rule base;data mining decision trees testing fuzzy systems classification tree analysis redundancy error correction data analysis clustering methods training data;gradient descent;knowledge acquisition;decision trees;fuzzy systems;knowledge based systems	This paper proposes a method to construct a fuzzy rule-based classifier system from an ID3-type decision tree (DT) for real data. The three major steps are rule extraction, gradient descent tuning of the rule-base, and performance-based pruning of the rule-base. Pruning removes all rules which cannot meet a certain level of performance. To test our scheme, we have used the DT generated by RIB3, an ID3-type classifier for real data. In this process, we made some improvements of RID3 to get a tree with less redundancy and hence a smaller rule-base. The rule-base is tested on several data sets and is found to demonstrate an excellent performance. Results obtained by the proposed scheme are consistently better than C4.5 across several data sets.		Nikhil R. Pal;Sukumar Chakraborty	2001	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.956036	rule-based system;membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;knowledge-based systems;machine learning;decision tree;pattern recognition;data mining;information fuzzy networks;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	3.866173079898862	-29.485179787088722	11468
245eb83b5e9cd87e35e443e1e97000cc7bbd18b3	articulated object registration using simulated physical force/moment for 3d human motion tracking	simulated physical force moment;3d registration;3d model;human motion;kinematic constraints;iterative closest point;iterative closest points;articulated human motion tracking	In this paper, we present a 3D registration algorithm based on simulated physical force/moment for articulated human motion tracking. Provided with sparsely reconstructed 3D human surface points from multiple synchronized cameras, the tracking problem is equivalent to fitting the 3D model to the scene points. The simulated physical force/ moment generated by the displacement between the model and the scene points is used to align the model with the scene points in an Iterative Closest Points (ICP) [1] approach. We further introduce a hierarchical scheme for model state updating, which automatically incorporates human kinematic constraints. Experimental results on both synthetic and real data from several unconstrained motion sequences demonstrate the efficiency and robustness of our proposed method.	algorithm;align (company);displacement mapping;experiment;hierarchical database model;iterative closest point;iterative method;kinesiology;simulation;synthetic intelligence	Bingbing Ni;Stefan Winkler;Ashraf A. Kassim	2007		10.1007/978-3-540-75703-0_15	computer vision;mathematical optimization;simulation;geography;iterative closest point	Vision	49.37114629750665	-49.15090948497116	11473
7650b60d9cf8703a1e1ad3e07b7de252e4145a7b	rgb-d camera pose estimation using deep neural network		This paper presents a study for RGB-D camera pose estimation using deep learning techniques. The proposed network architecture is composed of two components: the convolution neural network (CNN) for exploiting the vision information, and the Long Short-Term Memory (LSTM) block for incorporating the temporal information. The CNN, more precisely a RGB-D variant of GoogLeNet, functionalizes as a feature-oriented camera pose estimator, while the LSTM works as a temporal filter to model the pose transition. A modified loss function is also proposed to help regulate the convergence of the pose parameters. Experimental results show that the combination of CNN and LSTM can achieve a higher pose estimation accuracy, while the pipeline structure defined in the network can also provide flexibility for handling different scenarios.	3d pose estimation;artificial neural network;convolution;deep learning;long short-term memory;loss function;network architecture	Fei Guo;Yifeng He;Ling Guan	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8308674	estimator;convolutional neural network;computer vision;artificial neural network;rgb color model;pose;deep learning;network architecture;artificial intelligence;convergence (routing);computer science	Robotics	26.759048346402736	-51.2130327815429	11506
6d349658d916ff76c06f6bbfcbd64647c12ec051	modality constrained programming problems: a unified approach to fuzzy mathematical programming problems in the setting of possibility theory	optimisation sous contrainte;constrained optimization;optimisation;fuzzy mathematical programming;optimizacion;incertidumbre;uncertainty;algoritmo borroso;prise decision;optimizacion con restriccion;mathematical programming;fuzzy algorithm;algorithme flou;ambiguity;possibility theory;optimization;incertitude;toma decision;ambiguedad;programmation mathematique;programacion matematica;ambiguite	In this paper, fuzzy mathematical programming problems are formulated based on the idea analogous with the chance constrained programming problem. The difference in meaning between the ambiguity of the coefficients and that of the decision maker's preference is emphasized. The constraints with fuzzy coefficients are treated as the restriction that should be satisfied properly rather than perfectly. The objective functions with fuzzy coefficients are treated variously depending on the interpretations, i.e., the optimization of the modalities, the optimization of the fractile, the minimization of the ambiguity, and so forth. The deterministic equivalent constraints and the deterministic equivalent problems are shown when the constraints and the objective functions are linear. A numerical example is given to illustrate the proposed formulations.	mathematical optimization;possibility theory	Masahiro Inuiguchi;Hidetomo Ichihashi;Yasufumi Kume	1993	Inf. Sci.	10.1016/0020-0255(93)90086-2	stochastic programming;possibility theory;mathematical optimization;constrained optimization;uncertainty;computer science;artificial intelligence;mathematics;algorithm	AI	0.5332077074551306	-16.926019867831616	11524
6a19171027444d82776f06c313a042ddfb461c38	simultaneously applying multiple mutation operators in genetic algorithms	generation;fitness value;mutation ratio;genetics;next generation;genetic algorithm;dynamic mutation;offspring	The mutation operation is critical to the success of genetic algorithms since it diversifies the search directions and avoids convergence to local optima. The earliest genetic algorithms use only one mutation operator in producing the next generation. Each problem, even each stage of the genetic process in a single problem, may require appropriately different mutation operators for best results. Determining which mutation operators should be used is quite difficult and is usually learned through experience or by trial-and-error. This paper proposes a new genetic algorithm, the dynamic mutation genetic algorithm, to resolve these difficulties. The dynamic mutation genetic algorithm simultaneously uses several mutation operators in producing the next generation. The mutation ratio of each operator changes according to evaluation results from the respective offspring it produces. Thus, the appropriate mutation operators can be expected to have increasingly greater effects on the genetic process. Experiments are reported that show the proposed algorithm performs better than most genetic algorithms with single mutation operators.	complex systems;crossover (genetic algorithm);experiment;genetic algorithm;local optimum;mutation (genetic algorithm);mutation testing;next-generation network;norm (social)	Tzung-Pei Hong;Hong-Shung Wang;Wei-Chou Chen	2000	J. Heuristics	10.1023/A:1009642825198	dynamic mutation;mathematical optimization;generation;genetic algorithm;mutation;computer science;bioinformatics;genetic operator;genetic representation;offspring;premature convergence	AI	25.84511752906535	-6.369428105310339	11560
8f1b7e6d632d0035461e453cdbc439bc8204549f	regret bounds for meta bayesian optimization with an unknown gaussian process prior		• Assume there exist basis functions Φ = [φs]s=1 : X → R , mean parameter u ∈ R and covariance parameter Σ ∈ RK×K such that μ(x) = Φ(x)Tu and k(x, x′) = Φ(x)TΣΦ(x′), i.e. f = Φ(x)TW ∼ GP (μ, k),W ∼ N (u,Σ). • Assume M ≥ K, and Φ(x̄) has full row rank. The observation Yi = Φ(x̄)Wi + ̄i ∼ N (Φ(x̄)Tu,Φ(x̄)TΣΦ(x̄) + σI), and we estimate the weight vector as Ŵi = (Φ(x̄))Yi ∼ N (u,Σ + σ2(Φ(x̄)Φ(x̄)T)−1). Let W = [Ŵi]i=1 ∈ RN×K . • Unbiased GP prior parameter estimator: û = 1 NW 1N and Σ̂ = 1 N−1 (W − 1N û) T(W − 1N û). • Unbiased GP posterior estimator: μ̂t(x) = Φ(x)ût and k̂t(x) = Φ(x)Σ̂tΦ(x) where ût = û+ Σ̂Φ(xt)(Φ(xt) Σ̂Φ(xt)) (yt − Φ(xt)u), Σ̂t = N − 1 N − t− 1 ( Σ̂− Σ̂Φ(xt)(Φ(xt)Σ̂Φ(xt))Φ(xt)Σ̂ ) .	bayesian optimization;gaussian process;mathematical optimization;motion planning;online and offline;regret (decision theory);robot	Zi Wang;Beomjoon Kim;Leslie Pack Kaelbling	2018			artificial intelligence;bayesian optimization;estimator;machine learning;prior probability;motion planning;observational study;mathematics;regret;bayes' theorem;gaussian process	ML	27.569278109502587	-26.835129531249336	11571
605acfe7d92fce8c3e30120678d61e06641c4563	quality assessment of the jpeg 2000 compression standard			jpeg 2000	Úlfar Steingrímsson;Klaus Simon	2004			computer vision;artificial intelligence;compression (physics);lossless jpeg;computer science;jpeg 2000	Vision	42.28569212456169	-16.256973565909817	11584
0c316ea668226c770c11d018bcf57f03383ff110	dynamic assignment of crew reserve in airlines	dynamic programming;crew reserve assignment problem crap;airlines operations management;crew scheduling;combinatorial optimization	The Crew Reserve Assignment Problem (CRAP) considers the assignment of the crew members to a set of reserve activities covering all the scheduled flights in order to ensure a continuous plan so that operations costs are minimized while its solution must meet hard constraints resulting from the safety regulations of Civil Aviation as well as from the airlines internal agreements. The problem considered in this study is of highest interest for airlines and may have important consequences on the service quality and on the economic return of the operations. A new mathematical formulation for the CRAP is proposed which takes into account the regulations and the internal agreements. While current solutions make use of Artificial Intelligence techniques run on main frame computers, a low cost approach is proposed to provide on-line efficient solutions to face perturbed operating conditions. The proposed solution method uses a dynamic programming approach for the duties scheduling problem and when applied to the case of a medium airline while providing efficient solutions, shows good potential acceptability by the operations staff. This optimization scheme can then be considered as the core of an on-line Decision Support System for crew reserve assignment operations management. DOI: 10.4018/978-1-4666-2145-9.ch016	artificial intelligence;assignment problem;decision support system;dynamic programming;mainframe computer;mathematical optimization;online and offline;quality of service;scheduling (computing)	Walid El Moudani;Félix Mora-Camino	2011	Int. J. of Applied Metaheuristic Computing	10.4018/jamc.2011070103	mathematical optimization;simulation;combinatorial optimization;computer science;dynamic programming;mathematics;operations research	AI	13.720746956121134	2.1337352530329996	11587
814fe069e08c34b36c65151c46be8d86d5c54de7	single-objective and multi-objective formulations of solution selection for hypervolume maximization	evolutionary multi objective optimization;solution selection;optimization problem;evolutionary algorithm;solution set optimization;indicator based evolutionary algorithm	A new trend in evolutionary multi-objective optimization (EMO) is the handling of a multi-objective problem as an optimization problem of an indicator function. A number of approaches have been proposed under the name of indicator-based evolutionary algorithms (IBEAs). In IBEAs, the entire population usually corresponds to a solution of the indicator optimization problem. In this paper, we show how hypervolume maximization can be handled as single-objective and multi-objective problems by coding a set of solutions of the original multi-objective problem as an individual. Our single-objective formulation maximizes the hypervolume under constraint conditions on the number of nondominated solutions. On the other hand, our multi-objective formulation minimizes the number of non-dominated solutions while maximizing their Hypervolume.	entropy maximization;evolutionary algorithm;mathematical optimization;multi-objective optimization;optimization problem	Hisao Ishibuchi;Yuji Sakane;Noritaka Tsukamoto;Yusuke Nojima	2009		10.1145/1569901.1570187	optimization problem;mathematical optimization;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;mathematical economics	AI	19.92824890825331	-3.7456615368335036	11592
8863353f76372a5cf346a610add64e73b27e8c4e	septa - a new numerical tool for simultaneous targeting and design of heat exchanger networks	heat cascade;segregated problem table algorithm septa;tp chemical technology;heat exchanger network hen;pinch analysis;minimum utility targets;numerical approach	Pinch Analysis is an established insight-based methodology for design of energy-efficient processes. The Composite Curves (CCs) is a popular Pinch Analysis tool to target the minimum energy requirements. An alternative to the CCs is a numerical technique known as the Problem Table Algorithm (PTA). The PTA however, does not show individual hot and cold streams heat cascades and cannot be used for design of heat exchanger networks (HEN). This paper introduces the Segregated Problem Table Algorithm (SePTA) as a new numerical tool for simultaneous targeting and design of a HEN. SePTA shows profiles of heat cascade across temperature intervals for individual hot and cold streams, and can be used to simultaneously locate pinch points, calculate utility targets and perform SePTA Heat Allocation (SHA). The SHA can be represented on a new SePTA Network Diagram (SND) that graphically shows a heat exchanger network together with the amount of heat exchange on a temperature interval scale. This paper also shows that SePTA and SND can be a vital combination of numerical and graphical visualisation tools for targeting and design of complex HENs involving stream splitting, threshold problems and multiple pinches.	numerical analysis	Sharifah Rafidah Wan Alwi;Zainuddin A. Manan;Misrawati Misman;Wei Sze Chuah	2013	Computers & Chemical Engineering	10.1016/j.compchemeng.2013.05.008	simulation;engineering;pinch analysis;chemical engineering;engineering drawing	Logic	34.14844505643027	-4.945586885871913	11598
30b877c3aca4be2351d0620448738838d5f7a338	route selecting in the light of the theory of multimode transportation based on multi-agent	transportation networks;dynamic programming context aware services intelligent control educational institutions roads rail transportation pipelines cost function information technology forward contracts;service level;multi agent systems transportation supply and demand;route selection;supply and demand route selecting multimode transportation multi agent transportation market economy freightage market transportation corporations;market economy;multi agent systems;transportation;transport costs;supply and demand	In the context of market economy, various conflicts in freightage market surface as below: the conflict of competition and cooperation among transportation corporations, the conflict of supply and demand between consignors and transportation corporations, and the conflict of transport costs and service level, etc. Thus the theory of multimode transportation based on multi-agent is put forward to reconcile those conflicts in this paper, and in the light of the theory of multimode transportation based on multi-agent the problem of selecting the best route with the highest comprehension serving level showed by transport cost and transport time etc. is particularly elaborated here.	multi-agent system	Chuan-hua Zeng;Qing-song Li	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1400973	transport;service level;artificial intelligence;multi-agent system;supply and demand;advanced traffic management system;transportation planning	Robotics	11.378300835536344	-2.929000086018488	11617
c756fe553508caa529a2853e4868919cf9b5d2f7	combining color feature for scale adaptive correlation filter tracker		In order to improve the robustness of correlation filtering(CF) tracker, and solve the problems that the traditional CF methods cannot overcome the change of the object scale and the color information was not used. A scale adaptive correlation filtering tracker based on color feature fusion is proposed. Firstly, we transform the target search area of the image from RGB color space to Lab color space, and then extract the three channel Lab color features of the search area. Secondly, we combine the color feature and HOG feature into multi-channel feature. The multi-channel correlation filter was employed to locate the center of the target. Finally, the target scale model is established by Lab color feature. Acquiring different scales image block from the target position at the current frame. We get the optimal target scale by comparing with the scale model. Experiments were performed on 35 public benchmark challenging color sequences and compared with other 5 tracking methods. The results show that our proposed approach outperforms state-of-the-art tracking methods, and has a real-time tracking speed of 66 frames/s.	benchmark (computing);color space;experiment;real-time transcription	Cong Li;Cunyue Lu;Xun Zhao;Bao-min Zhang;Hongyu Wang	2017	2017 4th International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2017.8248465	robustness (computer science);control theory;kernel (linear algebra);filter (signal processing);computer science;computer vision;rgb color space;feature extraction;histogram;lab color space;scale model;artificial intelligence	Robotics	41.56386313735807	-50.17812159678516	11621
de888b1dc3c7b5f71c621f2a541c206eec23ebe3	multiple kernel discriminant analysis and decision fusion for robust sub-pixel hyperspectral target recognition	multi classifiers;kernel discriminant analysis;stepwise lda;image recognition;fisher linear discriminant analysis;kernel;classification algorithm;pixel mixing;mixing condition;high dimensionality;small sample size;reflectivity;dimensionality reduction multiple kernel discriminant analysis decision fusion subpixel hyperspectral target recognition hyperspectral image automatic target recognition hyperspectral reflectance signature principal components analysis fisher linear discriminant analysis stepwise lda divide and conquer method hyperspectral space partitioning pixel mixing robust recognition;pixel classification;kernel methods;kernel robustness target recognition hyperspectral sensors hyperspectral imaging linear discriminant analysis principal component analysis reflectivity pattern classification image analysis;hyperspectral sensors;curse of dimensionality;linear discriminate analysis;target recognition kernel methods pattern classification multi classifiers decision fusion;sensor fusion decision making geophysical signal processing image recognition object detection principal component analysis remote sensing;accuracy;subpixel hyperspectral target recognition;dimensionality reduction;principal components analysis;target recognition;geophysical signal processing;feature extraction;principal component analysis;remote sensing;hyperspectral reflectance signature;pixel;divide and conquer method;automatic target recognition;hyperspectral data;decision fusion;pattern classification;kernel pca;kernel method;robustness;image analysis;ground truth;sensor fusion;hyperspectral imaging;land cover classification system;robust recognition;linear discriminant analysis;dimensional reduction;hyperspectral image;hyperspectral space partitioning;multiple kernel discriminant analysis;divide and conquer;density functional;conditional distribution;object detection;spatial resolution	Hyperspectral image based automatic target recognition (ATR) systems often project the high dimensional hyperspectral reflectance signatures onto a lower dimensional subspace using techniques such as principal components analysis (PCA), Fisher's linear discriminant analysis (LDA) and stepwise LDA. In a general classification framework, these projections are sub-optimal, and in the absence of sufficient training data, are likely to be ill conditioned. In recent work, the authors proposed a divide and conquer approach that partitions the hyperspectral space into contiguous subspaces followed by multi-classifiers and decision fusion. Although this technique alleviated the small sample size problem and provided a good recognition performance in light and moderate pixel mixing, the performance significantly decreased under severe mixing conditions, as does with conventional ATR techniques. In this work, the authors propose a kernel discriminant analysis based projection in each subspace of the partition, followed by the multi-classifier, decision fusion framework to ensure robust recognition even in severe pixel mixing conditions. The performance of the proposed system (as measured by overall target recognition accuracies) is greatly superior to conventional dimensionality reduction techniques, as well as the more recently proposed divide-and-conquer technique.	automatic target recognition;condition number;dimensionality reduction;kernel (operating system);linear discriminant analysis;pixel;principal component analysis;stepwise regression;type signature	Saurabh Prasad;Lori M. Bruce	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4778923	computer vision;kernel method;image analysis;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;linear discriminant analysis;remote sensing;principal component analysis	Vision	29.876243874764388	-43.08571366156059	11625
489d75a918bc4d4d00baa408fca86b818150adb1	a new iterative model to simplify the knowledge extracted on a fuzzy rule-based learning algorithm	fuzzy set theory;iterative methods;classification genetic fuzzy systems sequential covering iterative rule learning approach;iterative model prediction capability complete rule base finding problem decomposition strategy sequential covering strategy fuzzy rule based learning algorithm knowledge extraction;learning artificial intelligence fuzzy set theory iterative methods knowledge based systems;learning artificial intelligence;knowledge based systems	Different fuzzy rule-based learning algorithms use the sequential covering strategy. This model applies a problem decomposition strategy, in which the task of finding a complete rule base is reduced to a sequence of subproblems in each of which the solution is to add a single rule. Now, we are interested in introducing additional capabilities in this strategy in order to review the knowledge previously extracted. Thus, our main objective is that in each iteration instead of only being able to add rules, we can propose three different options: to add the best rule that increases the prediction capability of the rule base, to add the best rule that replaces one or more rules previously learned without loosing accuracy or do not add any rule, if the rule base can not be improved. The experimental results show that this proposal maintains the accuracy of the model as well as the average number of rules but significantly reducing the number of conditions per database, which means that rules are more general. Therefore, this new iterative scheme improves the interpretability of the model obtained.	fuzzy rule;genetic algorithm;iteration;iterative and incremental development;logic programming;machine learning;rule-based system	David García;Antonio González Muñoz;Raúl Pérez	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622451	conflict resolution strategy;computer science;artificial intelligence;knowledge-based systems;machine learning;data mining;mathematics;iterative method;fuzzy set	DB	4.129893226701572	-30.643228966057315	11635
47c946fd80882fc1530b0cf4c70dbb99f63cfb08	an automatic and programmable optical sar data processor	spatial light modulator synthetic aperture radar automatic programmable optical sar data processor digital signal processing processor fourier transformation;optical filters;synthetic aperture radar optical filters optical imaging adaptive optics high speed optical techniques optical signal processing;high speed optical techniques;optical imaging;optical signal processing;spatial light modulator sar optical processor automatic programmable;synthetic aperture radar fourier transforms radar signal processing remote sensing spatial light modulators;adaptive optics;synthetic aperture radar	Synthetic Aperture Radar now is widely used in remote imaging which have many kinds of system. But as SAR data become larger and larger, the traditional Digital Signal Processing processor can not afford the high speed and high resolution process in rated size and power. But our optical processor for SAR can finish the data process in high speed because it can perform the Fourier transformation at the speed of light with low power consumption. Besides, this system is automatic and programmable which can be realized by computer and spatial light modulator.	digital signal processing;image resolution;modulation;optical computing;spatial light modulator	Chaobo Lin;Huanglong Wang;Yesheng Gao;Kaizhi Wang;Xingzhao Liu	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326490	computer vision;synthetic aperture radar;optical imaging;optical filter;optics;radar imaging;inverse synthetic aperture radar;adaptive optics;side looking airborne radar;physics;remote sensing	Arch	43.83184288169571	-3.406123800139699	11636
aee1bf149f35066900f01307b0a2e79b9c9c9b02	a tissue p system based evolutionary algorithm for multi-objective vrptw		Abstract Multi-objective vehicle routing problem with time windows (VRPTW) has important applications in engineering and computer science, and it is a NP-hard problem. In the last decade, numerous new methods for multi-objective VRPTW have sprung up. However, the calculation speed of most algorithms is not fast enough, and on the other hand, these algorithms did not give a complete Pareto optimal front, although their results are excellent. Hence, in this paper, a tissue P system with three cells based MOEA, termed PDVA, is proposed to solve the multi-objective VRPTW. In PDVA, two mechanisms, the discrete glowworm evolution mechanism (DGEM) and the variable neighborhood evolution mechanism (VNEM), are used as sub-algorithms in two cells respectively to balance the exploration and exploitation reasonably. Simultaneously, some special strategies are used to enhance the performance of the proposed algorithm. The following experiments are presented to test the proposed algorithm. First, the influence of the parameters on the performance of the algorithm is investigated. Second, the validity of the algorithm is highlighted when compared to the DGEM-VNEM algorithm. Third, the quality and diversity of the solutions are improved when compared to the other popular algorithms. These results and comparisons on test instances demonstrate the competitiveness of PDVA in solving multi-objective VRPTW in terms of both quantity and speed.	evolutionary algorithm;p system;vehicle routing problem	Wenbo Dong;Kang Zhou;Huaqing Qi;Cheng He;Jun Zhang	2018	Swarm and Evolutionary Computation	10.1016/j.swevo.2017.11.001	mathematical optimization;pareto principle;evolutionary algorithm;vehicle routing problem;p system;bioinformatics;computer science	AI	24.61070753512695	-3.331823352556569	11639
605a58ba31e5a1a5920cfd6fa0f853dfdacc7c5f	a object tracking method in two cameras with common view field		A object tracking method is proposed in this paper by utilizing the repetitive information in two cameras with common view field. First, the background will be built using Gaussian background modeling for the images of different cameras with common view field. Second, the foreground objects can be attained and extracted using the background subtraction method. SIFT(Scale-Invariant Feature Transform) feature points will be matched for the objects extracted from the images of the different cameras. Then RANSAC(RANdom SAmple Consensus) algorithm is used to filter the SIFT feature matching, and the same object of two images will be attained. The results demonstrate that this method is better for far field object from different cameras with common view field. It has potential and important applications in object matching and tracking of multicameras.	algorithm;background subtraction;feature model;feature vector;random sample consensus;robustness (computer science);scale-invariant feature transform	Yingjie Xia;Guangwei Li;Rui Chen;Peipei Yu	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302041	computer vision;ransac;feature extraction;video tracking;artificial intelligence;pattern recognition;near and far field;scale-invariant feature transform;gaussian;computer science;matched filter;background subtraction	Vision	41.87691979139404	-50.2119133840893	11650
1efcd0a4c98bca15d2a0faa55951ab0b5f1e13f7	modular neural network with fuzzy integration of responses for face recognition	fuzzy rules;fuzzy integral;face recognition;genetic algorithm;modular neural network;fuzzy system;sugeno integral	This paper presents a modular neural network with fuzzy integration of responses for face recognition. We describe its architecture  and simulation results using the ORL database. We show results from different integrators, such as the Gating Network, fuzzy  Sugeno integrals and type-1 fuzzy systems. We also show that the results with type-1 fuzzy systems are good, but we decided  to optimize this fuzzy system with genetic algorithms (MFs parameters and fuzzy rules) to improve the results.  	facial recognition system;modular neural network	Erika Ayala;Miguel Lopez;Patricia Melin	2009		10.1007/978-3-642-04514-1_8	defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;artificial intelligence;neuro-fuzzy;machine learning;pattern recognition;time delay neural network;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Vision	4.828104591709469	-26.382919538869103	11676
9fc3829f47e222c3d0ef50a5d370c0e38a185658	rectangular sets of probability measures	dynamic programming;time consistency;grupo de excelencia;risk averse optimization;ciencias basicas y experimentales;matematicas;rectangularity;multistage stochastic optimization;robust optimal control and markov decision processes;coherent risk measures;grupo a	In this paper we consider the notion of rectangularity of a set of probability measures, introduced in Epstein and Schneider [4], from a somewhat different point of view. We define rectangularity as a property of dynamic decomposition of a distributionally robust stochastic optimization problem and show how it relates to the modern theory of coherent risk measures. Consequently we discuss robust formulations of multistage stochastic optimization problems in frameworks of Stochastic Programming, Stochastic Optimal Control and Markov Decision Processes.	coherence (physics);markov chain;markov decision process;mathematical optimization;multistage amplifier;optimal control;optimization problem;risk measure;stochastic optimization;stochastic programming	Alexander Shapiro	2016	Operations Research	10.1287/opre.2015.1466	markov decision process;stochastic programming;mathematical optimization;discrete mathematics;economics;computer science;stochastic optimization;dynamic programming;mathematics;time consistency;statistics	ML	1.1209482188360909	-1.6372361525254613	11677
52deacd6abc7d46acc9759cdcb1815b26f120759	design automation for interwell connectivity estimation in petroleum cyber-physical systems	cyber physical systems;indexes;petroleum;mathematical model;production;optimization;entropy	In a petroleum cyber-physical system (CPS), interwell connectivity estimation is critical for improving petroleum production. An accurately estimated connectivity topology facilitates reduction in the production cost and improvement in the waterflood management. This paper presents the first study focused on computer-aided design for a petroleum CPS. A new CPS framework is developed to estimate the petroleum well connectivities. Such a framework explores an innovative water/oil index integrated with the advanced cross-entropy optimization. It is applied to a real industrial petroleum field with massive petroleum CPS data. The experimental results demonstrate that our automated estimations well match the expensive tracer-based true observations. This demonstrates that our framework is highly promising.	automation;computer-aided design;cross entropy;cyber-physical system;mathematical optimization;network topology	Xiaodao Chen;Dongmei Zhang;Lizhe Wang;Ning Jia;Zhijiang Kang;Yun Zhang;Shiyan Hu	2017	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2016.2584065	petroleum engineering;database index;entropy;mathematical optimization;computer science;engineering;mathematical model;cyber-physical system;waste management;petroleum;statistics	EDA	6.74128398083119	3.1730478093243453	11678
d9eada85155af1d2af157a399c54791b54182696	modeling stated preference for mobility-on-demand transit: a comparison of machine learning and logit models		Logit models are usually applied when studying individual travel behavior, i.e., to predict travel mode choice and to gain behavioral insights on traveler preferences. Recently, some studies have applied machine learning to model travel mode choice and reported higher out-of-sample prediction accuracy than conventional logit models (e.g., multinomial logit). However, there has not been a comprehensive comparison between logit models and machine learning that covers both prediction and behavioral analysis. This paper aims at addressing this gap by examining the key differences in model development, evaluation, and behavioral interpretation between logit and machine-learning models for travel-mode choice modeling. To complement the theoretical discussions, we also empirically evaluated the two approaches on stated-preference survey data for a new type of transit system integrating high-frequency fixed routes and micro-transit. The results show that machine learning can produce significantly higher predictive accuracy than logit models and are better at capturing the nonlinear relationships between trip attributes and mode-choice outcomes. On the other hand, compared to the multinomial logit model, the best-performing machinelearning model, the random forest model, produces less reasonable behavioral outputs (i.e. marginal effects and elasticities) when they were computed from a standard approach. By introducing some behavioral constraints into the computation of behavioral outputs from a random forest model, however, we obtained better results that are somewhat comparable with the multinomial logit model. We believe that there is great potential in merging ideas from machine learning and conventional statistical methods to develop refined models for travel-behavior research and suggest some possible research directions.	choice modelling;computation;machine learning;marginal model;multinomial logistic regression;nonlinear system;random forest	Xilei Zhao;Xiang Yan;Alan Yu;Pascal Van Hentenryck	2018	CoRR			ML	25.63203912516274	-22.36150783716504	11685
5cdd3f3388cef8a17f7e2dd5b05a7aa578e2576e	optimization of analog circuits via simulation and a lagrangian-type gradient-based method		We propose a new method for determining the physical sizes of components in an electrical circuit that maximize some primary performance measure while satisfying some conditions on the secondary performance measures. The proposed method is based on the observation that the performance measures are unimodal and smooth. Thus, it focuses on a local search and applies a Lagrangian method to search for a local optimum. The proposed method has advantages over existing methods because it does not rely on approximate formulas for the performance measures, like other equation-based methods do, and finds the “exact” optimal solution by calling an electrical circuit simulator, such as SPICE, at each iteration to evaluate the performance measures and to compute their gradients. The proposed method also enjoys fast convergence because it focuses on a local search rather than global searches. Numerical experiments illustrate the effectiveness of the proposed method in a one-stage operational amplifier.	analogue electronics;approximation algorithm;electronic circuit simulation;experiment;gradient;iteration;local optimum;local search (optimization);numerical method;operational amplifier;spice	Eunji Lim;Youngmin Kim;Jaehyouk Choi	2015	2015 Winter Simulation Conference (WSC)		mathematical optimization;simulation;computer science;theoretical computer science;mathematics;bandwidth;statistics	AI	31.27354026318977	-14.161741074506077	11686
f67e845d9403dd31d2112c49f9f910f65e44f96e	genetic algorithm application to a production-inventory model of imperfect process with deteriorating items under two dispatching policies	genetic algorithms dispatching inventory management costs optimized production technology conference management marketing management technology management engineering management application software;optimized production technology;inventory management;marketing management;application software;first in first out;lifo;genetic algorithm production inventory model imperfect process deteriorating items;biological system modeling;conference management;data mining;inventory model;production management;technology management;objective function;sensitivity analysis genetic algorithms inventory management production management;last in first out;deteriorating items;sensitivity analysis;engineering management;imperfect process;mathematical model;production inventory model;production;imperfect production process;genetic algorithm;genetic algorithms;fifo;sensitivity analyses;dispatching;gallium;genetic algorithm application;sensitivity analyses genetic algorithm application production inventory model imperfect production process fifo first in first out last in first out lifo inventory management	First-In-First-Out (FIFO) and the Last-In-First-Out (LIFO) are general presumption policies of inventory management. In this paper, the genetic algorithm is applied to solve an imperfect production-inventory problem under two inventory dispatching policies, FIFO and LIFO. Referred to mathematical models presented in (Sung et al.2008) and (Lin and Gong 2007), the cell reference function of Excel 2003 is firstly adopted to setup the corresponding objective functions. Sensitivity analyses with various combinations of parameters are further studies. Obtained results show that when a deteriorating item is produced by an imperfect process, the LIFO policy generally dominates the FIFO. However, there exists a cross point when the deterioration rate α under the in-control state is 0.03 and the minimum total cost presents a point of the break-even.	dominator (graph theory);evolver;fifo (computing and electronics);genetic algorithm;inventory theory;mathematical model;numerical analysis;spreadsheet	Yu-Su Shen;June-Chung Sung;Dah-Chuan Gong	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.253	marketing management;genetic algorithm;fifo and lifo accounting;computer science;technology management;operations management;management science;management;operations research;sensitivity analysis;statistics	Robotics	11.8510062432085	-3.6019467974637394	11703
26fa9d4c0c23b16d162b0f13476ee5a9d2b7e050	handhold object detection and event analysis using visual interaction clues			object detection	Jun-Wei Hsieh;Chia-Lung Lin;Jiun-Cheng Cheng;Pin Wu;Duan-Yu Chen	2011			computer science;computer vision;distributed computing;object detection;artificial intelligence	Vision	39.900818091089896	-47.26498869200465	11707
de50d22592185e29396f0e5792e45accaf387681	quadratic risk minimization in a regime-switching model with portfolio constraints	49n15;regime switching;convex constraints;93e20;variational analysis;duality synthesis;convex analysis;91b28;90a09;finite state markov chain	We study a problem of stochastic control in mathematical finance, for which the asset prices are modeled by Itô processes. The market parameters exhibit “regime-switching” in the sense of being adapted to the joint filtration of the Brownian motion in the asset price models and a given finite-state Markov chain which models “regimes” of the market. The goal is to minimize a general quadratic loss function of the wealth at close of trade subject to the constraint that the vector of dollar amounts in each stock remains within a given closed convex set. We apply a conjugate duality approach, the essence of which is to establish existence of a solution to an associated dual problem and then use optimality relations to construct an optimal portfolio in terms of this solution. The optimality relations are also used to compute explicit optimal portfolios for various convex cone constraints when the market parameters are adapted specifically to the Markov chain.	brownian motion;convex cone;convex conjugate;convex set;duality (optimization);loss function;markov chain;stochastic control	Catherine Donnelly;Andrew J. Heunis	2012	SIAM J. Control and Optimization	10.1137/100809271	convex analysis;subderivative;mathematical optimization;convex optimization;duality;variational analysis;mathematics;mathematical economics	Theory	1.6178947693779686	-2.141184298790017	11715
c56bca31334bcffd60249de101902e970f763619	offer stack optimization in electricity pool markets	dynamic programming;dynamic programming applications;journal article;natural resources;natural resources energy electricity pool markets;energy electricity pool markets dynamic programming;energy electricity pool markets;applications	We consider a generator making offers of energy into an electricity pool market. For a given time period, it must submit an offer stack, consisting of a fixed number of quantities of energy and prices at which it wants these quantities dispatched. We assume that the generator cannot offer enough power to substantially affect the market price, so the optimal response would be to offer energy at marginal cost. However, the market rules do not permit an arbitrary function, so the problem is to find an offer stack approximating marginal cost in a way that maximizes its profit. We give optimality conditions for this problem and derive an optimization procedure based on dynamic programming. This procedure is illustrated by applying it to several examples with different costs of production.	approximation algorithm;breakpoint;dalton pritchard;dynamic programming;linear function;local optimum;loss function;marginal model;markov chain;mathematical optimization;microwave;nonlinear programming;nonlinear system;optimization problem;piecewise linear continuation;point of view (computer hardware company);program optimization	Philip J. Neame;Andrew B. Philpott;Geoffrey Pritchard	2003	Operations Research	10.1287/opre.51.3.397.14955	mathematical optimization;economics;computer science;operations management;dynamic programming;natural resource;mathematics;microeconomics;information technology;commerce	ML	2.042951931767644	-2.6123930582861528	11734
53e1ab38cfd62e239d4b488c42c70c77953860ea	an effective wsn deployment algorithm via search economics	metaheuristic algorithm;deployment problem;wireless sensor network	The development of a powerful search mechanism to find a good solution is the current research direction of studies on metaheuristic algorithms; however, most of the developed mechanisms will search and check the possible solutions without knowledge of the overall landscape of the solution space during the convergence process. To make each search during the convergence process as effective as possible, this paper presents a new metaheuristic algorithm called search economics (SE) to solve the deployment problem of wireless sensor networks. The main distinguishing features of the SE are twofold: the first is its capability to depict the solution space based on the solutions that have been checked by the search algorithm, and second is its capability to use the knowledge thus obtained, i.e., the “landscape of the solution space,” during the search process. On the basis of these concepts, the investment in a search process will be more meaningful and thus less easy to fall into a local optimum during early iterations. The experimental results show that the proposed algorithm can provide a result for the deployment problem that is significantly better than those provided by the state-of-the-art metaheuristic algorithms evaluated in this study in terms of the quality. © 2016 Elsevier B.V. All rights reserved.	effective method;feasible region;global optimization;iteration;local optimum;lossless compression;lossy compression;mathematical optimization;metaheuristic;on the fly;parallel computing;search algorithm;simulation;software deployment	Chun-Wei Tsai	2016	Computer Networks	10.1016/j.comnet.2016.01.005	beam search;mathematical optimization;wireless sensor network;parallel metaheuristic;simulated annealing;beam stack search;tabu search;search-based software engineering;computer science;artificial intelligence;local search;machine learning;best-first search;guided local search;search algorithm	AI	22.944127713509793	-3.1959297307517214	11743
21e1527f6f093a52c628e3de808b59ba3cfed6b8	general trajectory prior for non-rigid reconstruction	image motion analysis;video signal processing;trajectory cameras image reconstruction discrete cosine transforms equations shape encoding;video signal processing discrete cosine transforms high pass filters image motion analysis image reconstruction image sequences;high pass filters;nonrigid structure from motion;motion capture sequences;trajectory;shape;truncated dct basis;general trajectory;discrete cosine transforms;image reconstruction;trajectory filter reconstruction;3d reconstruction error;image sequences general trajectory nonrigid structure from motion 3d reconstruction error reconstructability limit high pass filters truncated dct basis trajectory filter reconstruction motion capture sequences;reconstructability limit;encoding;cameras;image sequences	Trajectory basis Non-Rigid Structure From Motion (NRSFM) currently faces two problems: the limit of reconstructability and the need to tune the basis size for different sequences. This paper provides a novel theoretical bound on 3D reconstruction error, arguing that the existing definition of reconstructability is fundamentally flawed in that it fails to consider system condition. This insight motivates a novel strategy whereby the trajectory's response to a set of high-pass filters is minimised. The new approach eliminates the need to tune the basis size and is more efficient for long sequences. Additionally, the truncated DCT basis is shown to have a dual interpretation as a high-pass filter. The success of trajectory filter reconstruction is demonstrated quantitatively on synthetic projections of real motion capture sequences and qualitatively on real image sequences.	3d reconstruction;asymptotic computational complexity;discrete cosine transform;graphics pipeline;motion capture;principal component analysis;real-time clock;reference frame (video);regular expression;sparse matrix;structure from motion;synthetic data;time complexity	Jack Valmadre;Simon Lucey	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247826	iterative reconstruction;computer vision;mathematical optimization;shape;trajectory;mathematics;geometry;high-pass filter;encoding;computer graphics (images)	Vision	53.066908573186424	-50.731799195355734	11745
65bfde33c2ed900bcbcc73fc392d154b5f476b64	robust background subtraction for network surveillance in h.264 streaming video	video surveillance feature extraction image colour analysis image resolution image segmentation statistical analysis video coding video streaming;video surveillance;video streaming;image segmentation;image resolution;video surveillance background subtraction compressed domain algorithm h264 avc;vectors quantization signal discrete cosine transforms surveillance bit rate video coding indexes;video coding;statistical analysis;image colour analysis;feature extraction;pixel based approach robust background subtraction network surveillance h 264 streaming video advanced video coding avc perceptual quality mpeg proprietary codecs temporal statistics feature vector macroblock units foreground pixel quantization parameter low complexity technique color comparison pixel resolution segmentation	The H.264/Advanced Video Coding (AVC) is the industry standard in network surveillance offering the lowest bitrate for a given perceptual quality among any MPEG or proprietary codecs. This paper presents a novel approach for background subtraction in bitstreams encoded in the Baseline profile of H.264/AVC. Temporal statistics of the proposed feature vectors, describing macroblock units in each frame, are used to select potential candidates containing moving objects. From the candidate macroblocks, foreground pixels are determined by comparing the colors of corresponding pixels pair-wise with a background model. The basic contribution of the current work compared to the related approaches is that, it allows each macroblock to have a different quantization parameter, in view of the requirements in variable as well as constant bit-rate applications. Additionally, a low-complexity technique for color comparison is proposed which enables us to obtain pixel-resolution segmentation at a negligible computational cost as compared to those of classical pixel-based approaches. Results showing striking comparison against those of proven state-of-the-art pixel domain algorithms are presented over a diverse set of standardized surveillance sequences.	algorithm;background subtraction;codec;color;computational complexity theory;computer and network surveillance;feature vector;h.264/mpeg-4 avc;internet access;macroblock;moving picture experts group;pixel;quantization (signal processing);real-time computing;real-time transcription;requirement;streaming media;technical standard;the industry standard	Bhaskar Dey;Malay Kumar Kundu	2013	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2255416	video compression picture types;scalable video coding;computer vision;image resolution;background subtraction;feature extraction;computer science;video tracking;coding tree unit;block-matching algorithm;multimedia;image segmentation;motion compensation;h.261;multiview video coding;computer graphics (images)	Vision	45.043194198475646	-19.397986637637263	11757
9b8706052ef2e74c448039d1c502f699e51b6b6a	basic theoretical foundations and insights on bilevel models and their applications to power systems		Decision making in the operation and planning of power systems is, in general, economically driven, especially in deregulated markets. To better understand the participants’ behavior in power markets, it is necessary to include concepts of microeconomics and operations research in the analysis of power systems. Particularly, game theory equilibrium models have played an important role in shaping participants’ behavior and their interactions. In recent years, bilevel games and their applications to power systems have received growing attention. Bilevel optimization models, Mathematical Program with Equilibrium Constraints and Equilibrium Problem with Equilibrium Constraints are examples of bilevel games. This paper provides an overview of the full range of formulations of non-cooperative bilevel games. Our aim is to present, in an unified manner, the theoretical foundations, classification and main techniques for solving bilevel games and their applications to power systems.	bilevel optimization;game theory;ibm power systems;interaction;mathematical optimization;mathematical programming with equilibrium constraints;noise shaping;operations research;statistical classification	David Pozo;Enzo Sauma;Javier Contreras	2017	Annals OR	10.1007/s10479-017-2453-z	mathematical optimization;simulation;mathematics;mathematical economics;bilevel optimization	AI	1.8607162576124257	2.4786377004640903	11768
da65b75e2e92fe2f1298a269d0c7f3c1c60bbd54	an improved chaos-based image encryption scheme	image encryption;long period;cross correlation;normal distribution;logistic map;linear complexity;nonlinear transformation;chaos theory	Chaos theory provides a new approach to image encryption technology. The key stream generator is the key design issue of an image encryption system, it directly determines the security and efficiency of the system. This paper proposes an improved chaos-based key stream generator to enlarge the key space, extend the period and improve the linear complexity of the key stream under precision restricted condition so as to enhance the security of a chaos-based image encryption system. The generator is constructed by three Logistic maps and a nonlinear transform. The balance and correlation properties of the generated sequence are analyzed. The sequence is proved to be a binary Bernoulli stochastic sequence and the distribution of the differences between the amounts of 0 and 1 is analyzed. The side lobes of auto and cross correlation are proved to obey normal distribution N(0, 1/N). The experimental results indicate that the scheme has advantages of long period and strong anti various attack ability over conventional chaos-based encryption system.	bernoulli polynomials;chaos theory;cross-correlation;encryption;key space (cryptography);map;nonlinear system	Chong Fu;Zhen-chuan Zhang;Ying Chen;Xingwei Wang	2007		10.1007/978-3-540-72584-8_76	normal distribution;discrete mathematics;logistic map;theoretical computer science;cross-correlation;mathematics;chaos theory;deterministic encryption;probabilistic encryption;algorithm;statistics	Security	38.58015329001574	-8.593667311817864	11769
1c47aec9008e79b36b7cc70b97ef348921147b00	variance reduction in simulation	technical report	In the design and analysis of simulation experiments, it is generally difficult to estimate model performance parameters with adequate precision at an acceptable computing cost. This paper surveys the main variance reduction techniques that have been developed to improve the efficiency of simulation-based performance statistics.	experiment;simulation;variance reduction	James R. Wilson	1984			computer science;technical report	EDA	28.8388868680107	-16.769772310115087	11781
f1585991bad809fd80dc6ae573083c35aff8c6e1	dynamic obstacle detection based on probabilistic moving feature recognition	moving object;urban environment;obstacle detection;feature recognition;difference set;experimental evaluation	This paper presents a framework to detect moving objects based on the recognition of moving features in the images. The classification scheme is based on a complete probabilistic representation of feature locations that relates the vehicle motion with the visual information. Experimental evaluation under different settings in an outdoor, urban environment shows the performance of the proposed architecture.	comparison and contrast of classification schemes in linguistics and metadata;feature recognition;feedback;high- and low-level;image sensor;statistical classification	Roman Katz;Oliver Frank;Juan I. Nieto;Eduardo Mario Nebot	2007		10.1007/978-3-540-75404-6_8	feature recognition;computer vision;machine learning;pattern recognition;feature;difference set	Vision	41.79265277203873	-47.57012858529422	11783
bd9cc484dc870fa6551973bc1591143f967472cb	mixture weight influence on kernel entropy component analysis and semi-supervised learning using the lasso	data spectroscopy;kernel eigenvalues and eigenfunctions entropy clustering algorithms convolution vectors heart;pattern clustering;mixture densities;semi supervised learning;lasso framework mixture weight influence kernel entropy component analysis semisupervised learning spectral method cluster structure mixture weights cluster components semisupervised kernel eca classifier;kernel entropy component analysis;spectral dimensionality reduction;data spectroscopy spectral dimensionality reduction semi supervised learning kernel entropy component analysis cluster assumption mixture densities lasso;entropy;pattern clustering entropy learning artificial intelligence;learning artificial intelligence;lasso;cluster assumption	The aim of this paper is two-fold. First, we show that the newly developed spectral method known as kernel entropy component analysis (kernel ECA) captures cluster structure, which is very important in semi-supervised learning, and we provide an analysis showing how mixture weights influence kernel ECA in a mixture of cluster components setting. Second, we develop a semi-supervised kernel ECA classifier based on the Lasso framework, and report promising results compared to the state-of-the art.	kernel (operating system);lasso;semi-supervised learning;semiconductor industry;spectral method;supervised learning	Jonas Nordhaug Myhre;Robert Jenssen	2012	2012 IEEE International Workshop on Machine Learning for Signal Processing	10.1109/MLSP.2012.6349814	kernel;kernel method;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;machine learning;pattern recognition;mathematics;variable kernel density estimation;polynomial kernel;statistics	ML	25.501220239275607	-39.381018655223016	11789
4df08f7240b5034790e07b4204b6dbb16373989c	robust regression with twinned gaussian processes	model selection;predictive distribution;benchmark problem;heavy tail;robust inference;mixture model;robust method;robust regression;gaussian process	We propose a Gaussian process (GP) framework for robust infe renc in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approac h is a generalization of the mixture likelihood used in traditional robust GP regres sion, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmu s en and Ghahramani [2]. The value of this restriction is in its tractable ex p ctation propagation updates, which allow for faster inference and model selecti on, and better convergence than the standard mixture. An additional benefit over t he latter method lies in our ability to incorporate knowledge of the noise domain t o influence predictions, and to recover with the predictive distribution info rmation about the outlier distribution via the gating process. The model has asymptot ic c mplexity equal to that of conventional robust methods, but yields more confi de t predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which th ey fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended t o more general noise models. We also address similarities with the work of Goldbe rg et al. [3].	asymptote;benchmark (computing);cobham's thesis;gaussian process;mixture model;partial template specialization;sion's minimax theorem;smoothing;software propagation	Andrew Naish;Sean B. Holden	2007			econometrics;posterior predictive distribution;heavy-tailed distribution;machine learning;mixture model;gaussian process;mathematics;robust regression;model selection;statistics	ML	27.643122382896532	-28.87057109211317	11812
c1c8fec3f30a3708dd146ea056d54a2fcfdb5880	direct mining co-occurrence features for visual recognition: a branch and bound method	object recognition;itemsets;training;tree searching data mining object recognition;data mining;upper bound;visualization;boosting;visual recognition and or co occurrence features frequent itemset mining branch and bound;or co occurrence feature direct cooccurrence feature mining visual recognition branch and bound method object recognition scene recognition discriminative co occurrence feature discovery computational demanding task two stage frequent pattern mining cooccurrence feature mining algorithm optimal conjunction mining disjunction mining feature mining process multiclass boosting framework adaboost and co occurrence feature;frequent itemset mining;boosting visualization training decision trees benchmark testing upper bound itemsets;and or co occurrence features;tree searching;visual recognition;decision trees;branch and bound;benchmark testing	The co-occurrence features are the composition of base features that have more discriminative power than individual base features. Although they show promising performance in visual recognition applications such as object and scene recognition, the discovery of discriminative co-occurrence features is usually a computational demanding task. Unlike previous feature mining methods that fix the order of the co-occurrence features or rely on a two-stage frequent pattern mining to select the optimal feature co-occurrence, we propose a novel branch-and-bound based co-occurrence feature mining algorithm that can directly mine both optimal conjunctions (AND) and disjunctions (OR) of individual features at arbitrary orders simultaneously. This feature mining process is integrated into a multi-class boosting framework Adaboost. MH such that the weighted error is minimized by the discovered co-occurrence features in each boosting step. Experiments on the benchmark datasets and scene recognition dataset validate the advantages of our proposed method.	adaboost;algorithm;benchmark (computing);branch and bound;computation;data mining;decision tree;modified huffman coding;speedup	Chaoqun Weng;Yuning Jiang;Junsong Yuan	2013	2013 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2013.6607598	benchmark;visualization;feature;computer science;cognitive neuroscience of visual object recognition;machine learning;decision tree;pattern recognition;data mining;upper and lower bounds;branch and bound;feature;boosting	Vision	26.96953364860038	-46.9509231398881	11820
07c80339af2dc54c94c03c01db71a3d7d2bb9ea8	learning without forgetting		When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.	acclimatization;computer multitasking;convolutional neural network;feature extraction	Zhizhong Li;Derek Hoiem	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2773081	artificial neural network;retraining;semi-supervised learning;computer science;machine learning;artificial intelligence;pattern recognition;convolutional neural network;deep learning;feature extraction;multi-task learning;forgetting	Vision	21.248874095301236	-48.84268380851706	11850
25c6162b2bd002e77a7ea5083d0630591213b820	a first approach for cost-sensitive classification with linguistic genetic fuzzy systems in imbalanced data-sets	cost sensitive;learning process;pragmatics;measurement;genetic fuzzy systems fuzzy rule based classification systems imbal anced data sets cost sensitive;fuzzy rule based classification system;learning artificial intelligence fuzzy set theory genetic algorithms;fuzzy rule based classification systems;training;training measurement machine learning pragmatics genetics learning systems intelligent systems;rule learning;cost sensitive learning method cost sensitive classification linguistic genetic fuzzy system machine learning fuzzy rule based classification system;fuzzy set theory;genetics;learning systems;linguistic genetic fuzzy system;genetic fuzzy systems;imbal anced data sets;fuzzy rule base;machine learning;cost sensitive classification;classification system;intelligent systems;genetics based machine learning;genetic algorithms;cost sensitive learning;learning artificial intelligence;cost sensitive learning method;genetic fuzzy system;imbalanced data sets	Classification in imbalanced domains has become one of the most relevant problems within the area of Machine Learning at the present. This problem has raised in significance due to its presence in many real applications and it occurs when the distribution of the available examples to carry out the learning process is very different between the classes (often for binary class data-sets). Usually, the underrepresented class is the concept of the most interest for the problem, being the cost derived from a misclassification of these examples much higher than that of the remaining examples. In this work we analyze the behaviour of a cost-sensitive learning method for Fuzzy Rule Based Classification Systems in the scenario of high imbalanced data-sets. Specifically, we focus on one representative rule learning approach for Genetic Fuzzy Systems, the Fuzzy Hybrid Genetics-Based Machine Learning algorithm. The experimental results show how our cost-sensitive approach in this type of domains will help us to obtain very accurate solutions in shorter training times and also with a lower complexity with respect to other possibilities proposed for classification with imbalanced problems such as the use of preprocessing to rebalance the class distribution.	algorithm;algorithmic learning theory;fuzzy control system;fuzzy rule;genetic fuzzy systems;machine learning;preprocessor;self-balancing binary search tree	Victoria López;Alberto Fernández;Francisco Herrera	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687187	genetic algorithm;computer science;artificial intelligence;machine learning;pattern recognition;data mining;fuzzy set;measurement;pragmatics	AI	9.814342731066434	-38.910692716232575	11855
d70d68434af4af2f4cadd180ee43e61ad4b0cd48	service rate control of closed jackson networks from game theoretic perspective	queueing;game theory;nash equilibrium;service rate control;markov decision processes	Game theoretic analysis of queueing systems is an important research direction of queueing theory. In this paper, we study the service rate control problem of closed Jackson networks from a game theoretic perspective. The payoff function consists of a holding cost and an operating cost. Each server optimizes its service rate control strategy to maximize its own average payoff. We formulate this problem as a non-cooperative stochastic game with multiple players. By utilizing the problem structure of closed Jackson networks, we derive a difference equation which quantifies the performance difference under any two different strategies. We prove that no matter what strategies the other servers adopt, the best response of a server is to choose its service rates on the boundary. Thus, we can limit the search of equilibrium strategy profiles from a multidimensional continuous polyhedron to the set of its vertex. We further develop an iterative algorithm to find the Nash equilibrium. Moreover, we derive the social optimum of this problem, which is compared with the equilibrium using the price of anarchy. The bounds of the price of anarchy of this problem are also obtained. Finally, simulation experiments are conducted to demonstrate the main idea of this paper.	game theory;jackson	Li Xia	2014	European Journal of Operational Research	10.1016/j.ejor.2014.01.038	price of stability;jackson network;implementation theory;markov decision process;game theory;epsilon-equilibrium;mathematical optimization;traveler's dilemma;best response;economics;computer science;operations management;repeated game;stochastic game;strategy;chicken;normal-form game;mathematical economics;queueing theory;equilibrium selection;symmetric game;solution concept;price of anarchy;nash equilibrium	Robotics	0.03659158488630466	-1.5988505146055532	11910
69fd96240d25526675166ec5c547a0ed916cb13d	law discovery from financial data using neural networks	financial data processing;neural nets;generalization law discovery financial data neural networks experimental study market capitalization balance sheet items rf5 rule finder rf6 nominally conditioned polynomials mcv regularizer;neural networks polynomials artificial intelligence radio frequency laboratories asset management explosions robustness training data degradation;data mining;polynomials;financial data;generalisation artificial intelligence data mining financial data processing neural nets polynomials;generalisation artificial intelligence;neural network	This paper describes an experimental study for discovering underlying laws of market capitalization using BS (Balance Sheet) items. For this purpose, we apply law discovery methods based on neural networks: RF5 (Rule Finder) discovers a single numeric law from data containing only numeric values, RF6 discovers a set of nominally conditioned polynomials from data containing both nominal and numeric values, and MCV regularizer is used to improve both the generalization performance and the readability. Our preliminary experimental results show that these methods are promising for discovering underlying laws from financial data.	algorithm;artificial neural network;experiment;mobile television;polynomial	Kazumi Saito;Naonori Ueda;Shigeru Katagiri;Yutaka Fukai;Hiroshi Fujimaru;Masayuki Fujinawa	2000		10.1109/CIFER.2000.844627	computer science;artificial intelligence;machine learning;data mining	ML	12.79535818513721	-35.049383594454454	11913
14133d29faccebfff1b1c3762f8648b8e88d9cc2	learning the matching function		The matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels. This approach works under assumption, that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically. However, this might not be the case, if we try to match pixels over a large interval of time. In this paper we propose a method, which learns the matching function, that automatically finds the space of allowed changes in visual appearance, such as due to the motion blur, chromatic distortions, different colour calibration or seasonal changes. Furthermore, it automatically learns the importance of matching scores of contextual features at different relative locations and scales. Proposed classifier gives reliable estimations of pixel disparities already without any form of regularization. We evaluated our method on two standard problems stereo matching on KITTI outdoor dataset, optical flow on Sintel data set, and on newly introduced TimeLapse change detection dataset. Our algorithm obtained very promising results comparable to the state-of-the-art.	algorithm;computer stereo vision;correspondence problem;discriminative model;distortion;frame (video);gaussian blur;optical flow;pixel;statistical classification;stereo cameras	Lubor Ladicky;Christian Häne;Marc Pollefeys	2015	CoRR		computer vision;machine learning;pattern recognition;mathematics	Vision	32.2476919833343	-50.751924618176474	11914
379b8311a03066b876a029e180967f95d8c04e01	modeling and estimation of accident rate and trend in air transport	estimation;regression analysis;uncertainty;air transport;bayesian estimation;binomial distribution;bayesian methods;particle filtering;point estimation;particle filter;particle filters	An established approach in the evaluation of aviation accident statistics is to determine point estimates of the accident rate by dividing number of accidents by number of flights and to determine an uncertainty interval through evaluation of the underlying binomial distribution. The trend, however, is not estimated. Another established approach is to perform a regression analysis to estimate rate and trend, but then uncertainty is not estimated. In this paper we overcome these limitations of established approaches by studying the problem as one of Bayesian estimation of the joint conditional density function of accident rate and trend given accident and flight statistical data. Subsequently, a particle filter is used in order to perform numerical evaluations. The novel approach is shown to work well on commercial aviation accident data.	numerical analysis;particle filter	Henk A. P. Blom;Edwin A. Bloem	2010	2010 13th International Conference on Information Fusion		econometrics;engineering;forensic engineering;statistics	Robotics	29.62001603340581	-19.696341908419807	11916
2f95b393a66468d21b0e97bcda9811ed34dc728a	feature selection with  $\ell_{2,1-2}$  regularization		"""Feature selection aims to select a subset of features from high-dimensional data according to a predefined selecting criterion. Sparse learning has been proven to be a powerful technique in feature selection. Sparse regularizer, as a key component of sparse learning, has been studied for several years. Although convex regularizers have been used in many works, there are some cases where nonconvex regularizers outperform convex regularizers. To make the process of selecting relevant features more effective, we propose a novel nonconvex sparse metric on matrices as the sparsity regularization in this paper. The new nonconvex regularizer could be written as the difference of the <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula> norm and the Frobenius (<inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,2}$ </tex-math></inline-formula>) norm, which is named the <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1-2}$ </tex-math></inline-formula>. To find the solution of the resulting nonconvex formula, we design an iterative algorithm in the framework of ConCave–Convex Procedure (CCCP) and prove its strong global convergence. An adopted alternating direction method of multipliers is embedded to solve the sequence of convex subproblems in CCCP efficiently. Using the scaled cluster indictors of data points as pseudolabels, we also apply <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1-2}$ </tex-math></inline-formula> to the unsupervised case. To the best of our knowledge, it is the first work considering nonconvex regularization for matrices in the unsupervised learning scenario. Numerical experiments are performed on real-world data sets to demonstrate the effectiveness of the proposed method."""	algorithm;augmented lagrangian method;carbonyl cyanide m-chlorophenyl hydrazone;convergence (action);data point;embedded system;embedding;entity name part qualifier - adopted;experiment;feature selection;genetic selection;iterative method;local convergence;manifold regularization;matrix regularization;sparse dictionary learning;sparse matrix;subgroup;unsupervised learning	Yong Shi;Jianyu Miao;Zhengyu Wang;Peng Zhang;Lingfeng Niu	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2785403	artificial intelligence;machine learning;pattern recognition;feature selection;computer science;regularization (mathematics)	ML	23.84126529576442	-42.27067248992641	11930
92c88020526b5472ca7cbe63f601e2aae626bf76	incremental acquisition of multiple nonlinear forward models based on differentiation process of schema model	esquema;bayesian framework;test hypothese;bayes estimation;modelizacion;processus gauss;systeme nerveux central;principio modelo interno;learning;forward model;test hipotesis;computer model;simulation;modular learning;hombre;mosaic;simulacion;inverse modeling;non linear model;multiple internal models;modele non lineaire;proceso adquisicion;acquisition process;modele multiple;schema;aprendizaje;modelisation;sistema nervioso central;estimacion bayes;apprentissage;modelo no lineal;differentiation;learning methods;differenciation;multimodel;theory;teoria;human;principe modele interne;multiple model;internal model principle;diferenciacion;gaussian process;reseau neuronal;modelo multiple;proceso gauss;modeling;scheme;red neuronal;processus acquisition;central nervous system;internal model;theorie;estimation bayes;homme;neural network;hypothesis test	We introduce the schema model as an alternative computational model representing multiple internal models. The human central nervous system is believed to obtain multiple forward-inverse models. The schema model enables agents to obtain multiple nonlinear forward models incrementally. This model is based on hypothesis testing theory whereas most modular learning methods are based on a Bayesian framework. As a specific example, we describe a schema model with a normalized Gaussian network (NGSM). Simulation revealed that NGSM has two advantages over MOSAIC's learning method: NGSM can obtain multiple models incrementally and does not depend on the initial parameters of the forward models.	cell differentiation process;computation;computational model;increment;learning disorders;ncsa mosaic;nervous system structure;nonlinear system;normal statistical distribution;simulation	Tadahiro Taniguchi;Tetsuo Sawaragi	2008	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2007.10.007	statistical hypothesis testing;mosaic;scheme;internal model;systems modeling;semi-structured model;computer science;inverse problem;artificial intelligence;central nervous system;calculus;gaussian process;schema;mathematics;differentiation;theory;artificial neural network;algorithm;statistics	ML	20.059054361927725	-27.88536837258761	11931
3d5e320c29b6026fd9f9247db572923fa4a1d79b	exploring the neural algorithm of artistic style		In this work we explore the method of style transfer presented in [1]. We first demonstrate the power of the suggested style space on a few examples. We then vary different hyper-parameters and program properties that were not discussed in [1], among which are the recognition network used, starting point of the gradient descent and different ways to partition style and content layers. We also give a brief comparison of some of the existing algorithm implementations and deep learning frameworks used. To study the style space further, an idea similar to [2] is used to generate synthetic images by maximizing a single entry in one of the Gram matrices Gl and some interesting results are observed. Next, we try to mimic the sparsity and intensity distribution of Gram matrices obtained from a real painting and generate more complex textures. Finally, we propose two new style representations built on top of network’s features and discuss how one could be used to achieve local and potentially content-aware style transfer.	algorithm;deep learning;gradient descent;gramian matrix;sparse matrix;synthetic data	Yaroslav Nikulin;Roman Novak	2016	CoRR		computer science;artificial intelligence;machine learning;mathematics;algorithm	ML	23.25462372869974	-48.61015959454623	11936
08fc6dda0a5b1766dc1f78fcce001fafb96030b5	a parallel cellular genetic algorithm used in finite element simulation	parallel genetic algorithm;parallel simulated annealing;finite element simulation;search algorithm;universiteitsbibliotheek;np hard problem;genetic algorithm;load balance	In this paper we will formulate a framework for a parallel population based search process: an Abstract Cellular Genetic Algorithm (ACGA). Using the ACGA as a template, various parallel search algorithms can be formulated, e.g. parallel Genetic Algorithms and parallel Simulated Annealing. As a case study we will investigate the influence of locality on the behaviour of a Cellular Genetic Algorithm (CGA), that is constructed according to this framework. A neighbourhood structure is imposed upon the population, which results in overlapping local cell-populations. Using varying neighbourhood sizes, we will discuss experiments with CGAs ranging from maximally local to effectively global. The CGA has been applied to a load balancing problem: the NP-hard problem of mapping a process graph onto a processor topology in parallel finite element simulations.	genetic algorithm;simulation	Arjen Schoneveld;Jan F. de Ronde;Peter M. A. Sloot;Jaap A. Kaandorp	1996		10.1007/3-540-61723-X_1017	mathematical optimization;genetic algorithm;computer science;artificial intelligence;load balancing;theoretical computer science;np-hard;distributed computing;adaptive simulated annealing;algorithm;search algorithm	Robotics	23.14983214777285	1.6767773538015442	11973
c7a51e066986325558fe1f40eaac1cc51ae41cd9	multiview occlusion analysis for tracking densely populated objects based on 2-d visual angles	computational geometry;posterior probability;probability;particle filter;tracking;maximum likelihood estimation	A novel framework of multiview occlusion analysis is presented for tracking densely populated objects moving on two-dimensional plane. This paper explicitly models the spatial structure of the occlusion process between objects and its uncertainty, based on 2D silhouette-based visual angles from fixed viewpoints. The occlusion structure is defined as tangency combination between the objects and the edges of the visual angles, based on geometric constraints inherent in the visual angles. The problem is then formulated as recursive Bayesian estimation consisting of hypothesis generation/testing of the occlusion structure and the estimation of posterior probability distribution for the object states including position and posture, on each hypothesis of the occlusion structure. For implementing the proposed framework, we develop a novel type of particle filter that supports multiple state distributions. Experiments using synthetic and real data show the robustness of the framework even in the face of severe occlusions.	algorithm;ambient occlusion;experiment;particle filter;poor posture;population;recursion;robustness (computer science);synthetic data	Kazuhiro Otsuka;Naoki Mukawa	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.174	computer vision;particle filter;computational geometry;pattern recognition;probability;mathematics;tracking;maximum likelihood;posterior probability;statistics	Vision	46.220900646206296	-50.151195860221975	11979
714b3009f53add2b3cb898a6dc5939d5b492f2cd	anomaly detection using replicator neural networks trained on examples of one class	anomaly detection;artificial neural networks;replicator neural network;pattern recognition and data mining;neural evolutionary and fuzzy computation;one class learning;auto encoder	Anomaly detection aims to find patterns in data that are significantly different from what is defined as normal. One of the challenges of anomaly detection is the lack of labelled examples, especially for the anomalous classes. We describe a neural network based approach to detect anomalous instances using only examples of the normal class in training. In this work we train the net to build a model of the normal examples, which is then used to predict the class of previously unseen instances based on reconstruction error rate. The input to this network is also the desired output. We have tested the method on six benchmark data sets commonly used in the anomaly detection community. The results demonstrate that the proposed method is promising for anomaly detection. We achieve F-score of more than 90% on 3 data sets and outperform the original work of Hawkins et al. on the Wisconsin breast cancer set.		Anh Hoang Dau;Victor Ciesielski;Andy Song	2014		10.1007/978-3-319-13563-2_27	anomaly detection;computer science;artificial intelligence;machine learning;data mining;autoencoder	ML	13.578126554254133	-35.31826619174158	11987
dac3821f3deb65f1c434395999c63d98d0b21e49	method for determining whether or not text information is leaked from computer display through electromagnetic radiation	information security;wavelet transform;sparse decomposition;text information;electromagnetic radiation	Confidential information might be leaked through electro-magnetic radiation from a computer display. To detect the electromagnetic radiation that contains text information, this paper proposed an evaluation method without reconstructing the displayed image. In this method, sparse decomposition in wavelet is used to describe the characteristics of electromagnetic radiation signals contain text information. By using this method, it is easy to detect text information leakage in electromagnetic radiation from a computer display.		Degang Sun;Jun Shi;Dong Wei;Meng Zhang;Wei-qing Huang	2014		10.1007/978-3-319-21966-0_14	computer vision;electromagnetic radiation;speech recognition;computer science;information security;theoretical computer science;computer security;wavelet transform	HCI	37.15817099619167	-10.278040377819584	11989
3501996036e2a3577b03c2f033f561c6904987de	vector-space analysis of belief-state approximation for pomdps	expected utility;vector space;approximation scheme;value function;error bound;direct method	We propose a new approach to value-directed belief state approximation for POMDPs. The valuedirected model allows one to choose approximation methods for belief state monitoring that have a small impact on decision quality. Using a vector space analysis of the problem, we devise two new search procedures for selecting an approximation scheme that have much better computational properties than existing methods. Though these provide looser error bounds, we show empirically that they have a similar impact on decision quality in practice, and run up to two orders of magnitude more quickly.	approximation;decision quality;loose coupling	Pascal Poupart;Craig Boutilier	2001			direct method;mathematical optimization;expected utility hypothesis;vector space;machine learning;mathematics;bellman equation;statistics	AI	21.94968239308449	-16.178197522504586	11994
cdf5b1909828b1ab07f8239690a2eb6a93e57d6a	compressive sensing strategy for classification of bearing faults		Owing to the importance of rolling element bearings in rotating machines, condition monitoring of rolling element bearings has been studied extensively over the past decades. However, most of the existing techniques require large storage and time for signal processing. This paper presents a new strategy based on compressive sensing for bearing faults classification that uses fewer measurements. Under this strategy, to match the compressed sensing mechanism, the compressed vibration signals are first obtained by resampling the acquired bearing vibration signals in the time domain with a random Gaussian matrix using different compressed sensing sampling rates. Then three approaches have been chosen to process these compressed data for the purpose of bearing fault classification these includes using the data directly as the input of classifier, and extract features from the data using linear feature extraction methods, namely, unsupervised Principal Component Analysis (PCA) and supervised Linear Discriminant Analysis (LDA). Classification performance using Logistic Regression Classifier (LRC) achieved high classification accuracy with significantly reduced bandwidth consumption compared with the existing techniques.		Hosameldin O. A. Ahmed;Mou Ling Dennis Wong;Asoke K. Nandi	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952543	condition monitoring;compressed sensing;signal processing;pattern recognition;computer science;principal component analysis;feature extraction;machine learning;time domain;linear discriminant analysis;bearing (mechanical);artificial intelligence	Robotics	36.917978900196225	-30.769074696776624	12008
c8495df004597c4575b7b288c15cd2ee3a7220f0	partially linear transformation models with varying coefficients for multivariate failure time data	transformation model;multivariate failure time;期刊论文;estimating equations;delete a group jackknife;62h12;varying coefficients;local polynomials	This paper studies the estimation and inference of varying coefficients and parameters in the partially linear transformation models for multivariate failure time data. A profile martingale-based estimating method that includes global and local estimating equations is proposed. Asymptotic properties of the estimators are established. Some numerical simulations are given to show the performance of the estimation method in finite-sample situation. In order to reduce the computational burden, a simple and useful one-step estimator method is used. We further suggest a delete-a-group jackknife method to estimate asymptotic variance of estimators. A real data set from the Busselton Population Health Surveys is analyzed to illustrate the proposed methods.	coefficient	Zhiping Qiu;Yong Zhou	2015	J. Multivariate Analysis	10.1016/j.jmva.2015.08.008	econometrics;mathematical optimization;mathematics;statistics;estimating equations	HPC	29.60285284201387	-23.109796772194503	12021
29d67259f4f4d0d2361e21f2872de04c076305f3	particle swarm optimization - a survey	particle swarm optimization;swarm intelligence;objective function;search space	Particle Swarm Optimization (PSO) is a search method which utilizes a set of agents that move through the search space to find the global minimum of an objective function. The trajectory of each particle is determined by a simple rule incorporating the current particle velocity and exploration histories of the particle and its neighbors. Since its introduction by Kennedy and Eberhart in 1995, PSO has attracted many researchers due to its search efficiency even for a high dimensional objective function with multiple local optima. The dynamics of PSO search has been investigated and numerous variants for improvements have been proposed. This paper reviews the progress of PSO research so far, and the recent achievements for application to large-scale optimization problems. key words: particle swarm optimization, swarm intelligence		Keisuke Kameyama	2009	IEICE Transactions		biomimetics;mathematical optimization;multi-swarm optimization;simulation;information processing;swarm intelligence;computer science;artificial intelligence;speed;particle swarm optimization;metaheuristic;evolutionary computation;search algorithm	AI	26.156831909747183	-5.051355626686001	12031
16d27d2771419ac7c65732986f6ff56073e11f25	hdm-net: monocular non-rigid 3d reconstruction with learned deformation model		Monocular dense 3D reconstruction of deformable objects is a hard ill-posed problem in computer vision. Current techniques either require dense correspondences and rely on motion and deformation cues, or assume a highly accurate reconstruction (referred to as a template) of at least a single frame given in advance and operate in the manner of non-rigid tracking. Accurate computation of dense point tracks often requires multiple frames and might be computationally expensive. Availability of a template is a very strong prior which restricts system operation to a pre-defined environment and scenarios. In this work, we propose a new hybrid approach for monocular non-rigid reconstruction which we call Hybrid Deformation Model Network (HDM-Net). In our approach, deformation model is learned by a deep neural network, with a combination of domain-specific loss functions. We train the network with multiple states of a nonrigidly deforming structure with a known shape at rest. HDM-Net learns different reconstruction cues including texture-dependent surface deformations, shading and contours. We show generalisability of HDM-Net to states not presented in the training dataset, with unseen textures and under new illumination conditions. Experiments with noisy data and a comparison with other methods demonstrate robustness and accuracy of the proposed approach and suggest possible application scenarios of the new technique in interventional diagnostics and augmented reality.		Vladislav Golyanik;Soshi Shimada;Kiran Varanasi;Didier Stricker	2018		10.1007/978-3-030-01790-3_4	machine learning;robustness (computer science);3d reconstruction;artificial intelligence;strong prior;artificial neural network;shading;computer science;computation;monocular;augmented reality	Vision	27.076795670545792	-50.50669427901347	12036
c9988a37f06831167efde2e1271e142372601439	genetic network programming with acquisition mechanisms of association rules	evolutionary computation;association rules;data mining;association rule;genetic network programming			Kaoru Shimada;Kotaro Hirasawa;Jinglu Hu	2006	JACIII	10.20965/jaciii.2006.p0102	evolutionary programming;association rule learning;interactive evolutionary computation;computer science;machine learning;genetic representation;pattern recognition;data mining;evolutionary computation	Robotics	-0.15452788256473887	-33.17840297010888	12039
abdc18d80378d153a00f526b07a56516bf50b142	using diversity with three variants of boosting: aggressive, conservative, and inverse	multilayer perceptrons;aprendizaje probabilidades;classification;perceptron multicouche;mlp neural network;red multinivel;pattern recognition;apprentissage probabilites;multi layer perceptron;reconnaissance forme;multilayer network;reseau multicouche;reseau neuronal;reconocimiento patron;clasificacion;red neuronal;probability learning;neural network	We look at three variants of the boosting algorithm called here Aggressive Boosting, Conservative Boosting and Inverse Boosting. We associate the diversity measure Q with the accuracy during the progressive development of the ensembles, in the hope of being able to detect the point of “paralysis” of the training, if any. Three data sets are used: the artificial Cone-Torus data and the UCI Pima Indian Diabetes data and the Phoneme data. We run each of the three Boosting variants with two base classifier models: the quadratic classifier and a multi-layer perceptron (MLP) neural network. The three variants show different behavior, favoring in most cases the Conservative Boosting.	algorithm;artificial neural network;boosting (machine learning);cone;experiment;layer (electronics);memory-level parallelism;multilayer perceptron;quadratic classifier;quadratic function	Ludmila I. Kuncheva;Christopher J. Whitaker	2002		10.1007/3-540-45428-4_8	brownboost;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;multilayer perceptron;gradient boosting;artificial neural network	ML	11.695134723896082	-32.37789343032623	12042
ee402e4cc33c1f2a123fe95d490dd29c00853c81	generalization performance of radial basis function networks	local rademacher complexity;complexity theory radial basis function networks estimation error approximation error kernel;structural risk minimization srm;radial basis function networks approximation theory generalisation artificial intelligence;model selection generalization performance radial basis function networks rbf networks local rademacher complexities l 1 metric capacity rbf network complexity estimation estimation error bound approximation error bound hölder continuity l p loss function structural risk;structural risk minimization srm learning theory local rademacher complexity radial basis function rbf networks;learning theory;radial basis function rbf networks	This paper studies the generalization performance of radial basis function (RBF) networks using local Rademacher complexities. We propose a general result on controlling local Rademacher complexities with the L1 -metric capacity. We then apply this result to estimate the RBF networks' complexities, based on which a novel estimation error bound is obtained. An effective approximation error bound is also derived by carefully investigating the Hölder continuity of the lp loss function's derivative. Furthermore, it is demonstrated that the RBF network minimizing an appropriately constructed structural risk admits a significantly better learning rate when compared with the existing results. An empirical study is also performed to justify the application of our structural risk in model selection.	approximation error;generalization (psychology);hospital admission;loss function;model selection;norm (social);rademacher complexity;radial (radio);radial basis function network;scott continuity	Yunwen Lei;Lixin Ding;Wensheng Zhang	2015	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2014.2320280	mathematical optimization;radial basis function;machine learning;learning theory;mathematics;radial basis function network;rademacher complexity;statistics	ML	19.74373963058578	-30.240513686559442	12056
d62bf711858abb7678bc26f5035ae2b15d729d75	design and implementation of a vision-based motion capture system	motion compensation;bvhfile data;real time;client server systems;joints;animation humans costs production network servers joints cameras shape graphics information science;motion capture;client software;servers;client server;self designed vision based motion capture system;design and implementation;three dimensional displays;image color analysis;server software;animation;solid modelling client server systems computer animation motion compensation;game production self designed vision based motion capture system client server network structure client software server software 3d position data bvhfile data animation;humans;client server network structure;network structure;computer animation;game production;cameras;tracking;solid modelling;3d position data	Motion Capture has been widely used in many fields such as animation and game production. This paper describes the scheme and implementation of a self-designed vision-based motion capture system. This system is set up based on a Client/Server network structure. The client software tracks every marker attached to the actor’s body and sends the results to the server in real-time. The server software recovers every joint’s 3D position and shows the capturing results simultaneously. It can also convert the 3D position data into BVH file data.	algorithm;bounding volume hierarchy;client (computing);client–server model;hidden surface determination;kinesiology;motion capture;real-time clock;real-time computing;server (computing)	Zhenjiang Miao;Jia Li;Zhan Xu	2009	2009 Fifth International Conference on Image and Graphics	10.1109/ICIG.2009.187	embedded system;computer vision;real-time computing;simulation;computer science;server	Robotics	48.62263090032293	-44.75543429998335	12061
744afdfb6ae94a69d820677bae6999a9001eab13	retracted article: color image watermarking in big multimedia data applications		In recent years, digital watermarking technology has become an important technology to protect patient privacy in telemedicine. Because medical information is highly sensitive, the security of medical images is a key problem in telemedicine applications. Aiming at the key color image watermarking scheme and the non key color image watermarking algorithm, two targeted attack schemes are proposed to evaluate their security in telemedicine. The target scheme is based on SVD and QR based color image watermarking algorithm, and their embedding process is the same. The proposed attack takes advantage of the prior knowledge of the watermarking algorithm to make the exact embedding space change. Therefore, these changes will lead to an interruption of the extraction process. The experimental results show that the watermarking scheme based on Watermark Based on the scheme of key key safety. This is because the proposed target attack needs to tamper with the key based watermark image, instead of the key based image, to remove the embedded watermark. Our proposed targeted attack has more effective watermark removal performance than other general attacks such as JPEG compression, Gauss noise, and so on.	algorithm;color image;digital watermarking;embedded system;interrupt;jpeg;medical imaging;medical privacy;singular value decomposition	Ju Zhu;Chaofan Xiong;Hongwei Du;Ruxi Xiang;Yuan Li	2017	Multimedia Tools and Applications	10.1007/s11042-017-5604-y	computer vision;computer science;color image;digital watermarking;artificial intelligence;multimedia	EDA	38.71987762896472	-11.54727532344131	12074
a149162fb5baeeb0390da6e90815b77069a99e93	scalable framework for the analysis of population structure using the next generation sequencing data.		Genomic variant data obtained from the next generation sequencing can be used to study the population structure of the genotyped individuals. Typical approaches to ethnicity classification/clustering consist of several time consuming pre-processing steps, such as variant filtering, LD-pruning and dimensionality reduction of genotype matrix. We have developed a framework using R programming language to analyze the influence of various pre-processing methods and their parameters on the final results of the classification/clustering algorithms. The results indicated how to fine-tune the pre-processing steps in order to maximize the supervised and unsupervised classification performance. In addition, to enable efficient processing of large data sets, we have developed another framework using Apache Spark. Tests performed on 1000 Genomes data set confirmed the efficiency and scalability of the presented approach. Finally, the dockerized version of the implemented frameworks (freely available at: https://github.com/ZSI-Bio/popgen) can be easily applied to any other variant data set, including data from large scale sequencing projects or custom data sets from clinical laboratories.		Anastasiia Hryhorzhevska;Marek S. Wiewiórka;Michal J. Okoniewski;Tomasz Gambin	2017		10.1007/978-3-319-60438-1_46	data mining;artificial intelligence;machine learning;cluster analysis;computer science;scalability;dimensionality reduction;spark (mathematics);data set;variant call format;population;large-scale sequencing	NLP	7.19682262946764	-49.31846364579684	12076
44941659cd2ad5911f29b5a47b90027455f1ae49	a jam-absorption driving strategy for mitigating traffic oscillations	jam absorption driving strategy jad strategy traffic oscillation mitigation newell car following theory vehicle guide time space ending point estimation absorbing speed wave propagation prevention capacity drop prevention rubbernecking behavior;capacity drop traffic congestion traffic oscillation jam absorption driving jad;estimation theory road vehicle radar traffic wave propagation;oscillators;traffic control;acceleration;trajectory;estimation;vehicles;vehicles oscillators trajectory estimation acceleration numerical models traffic control;numerical models;traffic congestion traffic oscillation jam absorption driving jad capacity drop	To mitigate traffic oscillations that usually sustainably propagate upstream, this paper proposes a jam-absorption driving (JAD) strategy in the framework of Newell's car-following theory. The basic idea of the JAD strategy is to guide a vehicle to move slowly before being captured by an oscillation and terminate the slow movement when the vehicle would start to leave the jam if no such slow movement was implemented. To practically implement the idea, a two-step method is proposed to estimate the time–space ending point of the strategy, and a proper vehicle is selected to implement the JAD strategy based on a given expected absorbing speed and current traffic conditions. To test the JAD strategy, two simulated traffic scenarios are constructed based on a realistic data-driven car-following model. The first scenario, which only reproduces one oscillation, directly shows the effectiveness of the JAD idea in preventing wave propagation and capacity drop. The second scenario, which contains a series of traffic oscillations induced by the rubbernecking behavior, validates the proposed JAD strategy in more complicated and realistic conditions. It is indicated that the JAD strategy is able to absorb traffic oscillations; thus, the side effects incurred by the oscillations could be subsequently mitigated. The significance of this paper is to provide us a new idea to mitigate traffic oscillations, i.e., the JAD strategy.	bios;instability;jad;jam;mobile app;mobile phone;neural oscillation;simulation;software propagation;terminate (software);the void (virtual reality);upstream (software development)	Zhengbing He;Liang Zheng;Liying Song;Ning Zhu	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2587699	acceleration;estimation;simulation;telecommunications;engineering;trajectory;transport engineering;traffic wave;oscillation;quantum mechanics	Networks	9.683632677099347	-10.359057910188236	12079
45cf79e4c6ff1ce3e8035250bef7c24d71d2204c	autonomous configuration of parameters in robotic digital cameras	digital camera	In the past few years, the use of digital cameras in robotic applications has been increasing significantly. The main areas of application of these robots are the industry and military, where these cameras are used as sensors that allow the robot to take the relevant information of the surrounding environment and making decisions. To extract information from the acquired image, such as shapes or colors, the configuration of the camera parameters, such as exposure, gain, brightness or white-balance, is very important. In this paper, we propose an algorithm for the autonomous setup of the most important parameters of digital cameras for robotic applications. The proposed algorithm uses the intensity histogram of the images and a black and a white area, known in advance, to estimate the parameters of the camera. We present experimental results that show the effectiveness of our algorithms. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera.	algorithm;color balance;digital camera;image histogram;initial condition;robot;sensor	António J. R. Neves;Bernardo Cunha;Armando J. Pinho;Ivo Pinheiro	2009		10.1007/978-3-642-02172-5_12	smart camera;computer vision;camera auto-calibration;simulation;computer science;three-ccd camera;computer graphics (images)	Robotics	49.737106481207576	-40.6183573247056	12083
3e7ffb5658cf99968633ede18785c5cfdd6aa9eb	semi-supervised deep learning for monocular depth map prediction		Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.	autostereogram;color depth;deep learning;depth map;depth perception;encoder;experiment;flow network;ground truth;image noise;image segmentation;loss function;pixel;seamless3d;semi-supervised learning;semiconductor industry;sensor;sparse matrix;stereo camera;supervised learning;unsupervised learning	Yevhen Kuznietsov;Jörg Stückler;Bastian Leibe	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.238	computer vision;artificial intelligence;supervised learning;ground truth;deep learning;pattern recognition;computer science;monocular;training set;lidar;depth map	Vision	27.60200836543429	-49.75726238473056	12097
c373923ee8dec2097fed890507bcb13305005c5c	an inference method for fuzzy tree grammars	grammar;arbre graphe;fuzzy tree grammar;inference grammaticale;tree graph;grammaire expansive;expansive grammar;fuzzy grammar;grammaire;inferencia;grammaire arbre;fuzzy expansive tree grammar;grammatical inference;sistema difuso;systeme flou;grammaire floue;tree grammar;arbol grafo;gramatica;fuzzy system;inference	The inference for fuzzy tree grammars is an important and di cult work. This paper gives an inference method for fuzzy tree grammars, which is composed of two parts. First, a fuzzy expansive tree grammar is generated from a sample set; second, the grammar is simpli ed. c © 2000 Elsevier Science B.V. All rights reserved.	fuzzy logic	Lan Shu	2000	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00292-9	natural language processing;tree-adjoining grammar;l-attributed grammar;adaptive neuro fuzzy inference system;grammar induction;fuzzy classification;computer science;fuzzy number;machine learning;regular tree grammar;incremental decision tree;grammar;mathematics;tree structure;context-free grammar;ambiguous grammar;stochastic context-free grammar;fuzzy set operations;tree;algorithm;fuzzy control system	AI	3.1083281735809436	-23.618631658078783	12109
e2ed9d1a05058708eb84d9e5774b3aa43caa0888	single-visit policies for allocating a single resource in a stochastic environment	probability;stochastic development and evaluation of heuristics;resource allocation;markov good policies of simple structure;optimization method;stochastic model applications models for stochastic resource allocation;metodo optimizacion;dynamic programming optimal control;methode optimisation;programming scheduling;asignacion recurso;stochastic model;allocation ressource;modelo estocastico;modele stochastique	Conventional analyses of stochastic resource allocation problems based on Gittins' indices frequently yield policies which involve an unacceptable amount of switching of the resource from one option to another. This paper discusses a variety of methodologies aimed at solving this problem. They principally involve the development and analysis of stochastic resource allocation models incorporating switching costs together with a consideration of a new class of single-visit policies for which each option has a single (random) period during which it is in receipt of the key resource.		I. Benkherouf;Kevin D. Glazebrook;R. W. Owen	1994	Operations Research	10.1287/opre.42.6.1087	mathematical optimization;economics;resource allocation;stochastic modelling;operations management;stochastic optimization;probability;mathematics;mathematical economics;management;stochastic investment model;statistics	Robotics	4.11327858935072	-2.3617902141450746	12116
cfa63975faa8d829a642d96604ddbeca4abf302e	deep feature matching for dense correspondence		Image matching is a challenging problem as different views often undergo significant appearance changes caused by deformation, abrupt motion, and occlusion. In this paper, we explore features extracted from convolutional neural networks to help the estimation of image matching so that dense pixel correspondence can be built. As the deep features are able to describe the image structures, the matching method based on these features is able to match across different scenes and/or object appearances. We analyze the deep features and compare them with other robust features, e.g., SIFT. Extensive experiments on 5 datasets demonstrate the proposed algorithm performs favorably against the state-of-the-art methods in terms of visually matching quality and accuracy.	algorithm;artificial neural network;convolutional neural network;experiment;image registration;pixel;scale-invariant feature transform	Yang Liu;Jinshan Pan;Zhixun Su	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296390	convolutional neural network;computer vision;pixel;optical imaging;robustness (computer science);feature extraction;artificial intelligence;computer science;pattern recognition;scale-invariant feature transform	Vision	31.821840691984526	-51.19719056249669	12121
181ea7494d0e1b65f9f619252ed7e38f833ecd28	an efficient alternative to svm based recursive feature elimination with applications in natural language processing and bioinformatics	lenguaje natural;methode recursive;microprocessor;centro gravitacional;raisonnement base sur cas;text;razonamiento fundado sobre caso;ridge regression;pulga de dna;centre gravite;regresion ridge;puce a dna;book chapter;center of mass;langage naturel;metodo recursivo;recursive method;bioinformatique;tratamiento lenguaje;clasificador;intelligence artificielle;texte;straight forward method;microarray data sets;convex optimisation;microarray classification;hierarchical classification;regression pseudo orthogonale;classifier;verification task;natural selection;language processing;selection naturelle;scale factor;natural language;machine exemple support;genome;traitement langage;dna chip;classification hierarchique;classificateur;natural language processing systems;artificial intelligence;feature selection;microprocesseur;keywords computable limits;inteligencia artificial;recursive feature elimination;genoma;bioinformatica;maquina ejemplo soporte;vector support machine;case based reasoning;texto;clasificacion jerarquizada;facteur echelle;microprocesador;seleccion natural;natural language processing;factor escala;svm based recursive feature elimination rfe svm algorithm;bioinformatics	The SVM based Recursive Feature Elimination (RFE-SVM) algorithm is a popular technique for feature selection, used in natural language processing and bioinformatics. Recently it was demonstrated that a small regularisation constant C can considerably improve the performance of RFE-SVM on microarray datasets. In this paper we show that further improvements are possible if the explicitly computable limit C → 0 is used. We prove that in this limit most forms of SVM and ridge regression classifiers scaled by the factor 1 C converge to a centroid classifier. As this classifier can be used directly for feature ranking, in the limit we can avoid the computationally demanding recursion and convex optimisation in RFE-SVM. Comparisons on two text based author verification tasks and on three genomic microarray classification tasks indicate that this straightforward method can surprisingly obtain comparable (at times superior) performance and is about an order of magnitude faster.	algorithm;bioinformatics;computable function;converge;convex optimization;feature selection;mathematical optimization;microarray;natural language processing;recursion (computer science);text-based (computing)	Justin Bedo;Conrad Sanderson;Adam Kowalczyk	2006		10.1007/11941439_21	scale factor;center of mass;case-based reasoning;natural selection;speech recognition;dna microarray;classifier;computer science;artificial intelligence;machine learning;natural language;feature selection;tikhonov regularization;algorithm;statistics;genome	ML	9.391198902109606	-33.76659764346532	12123
73fbf3c6a6a1980c41bb89dbe2c0c2063775d5ee	operations research improves quality and efficiency in home care	home care;operations research;decision support system;scheduling;heuristics;health care	Elder care systems are facing increased costs, primarily because the elderly constitute a growing percentage of the population. Sweden publicly finances such systems; in 2005, the cost to taxpayers on a national level was 8.8 billion euros ($13 billion). The many customized aspects of scheduling home care workers to assist elderly and disabled citizens with their varying needs contribute to these costs. Laps Care, a system that was developed in 2002, uses operations research modeling to eliminate the manual planning of home care unit assignments. More than 200 units/organizations in Swedish municipalities use Laps Care each day to plan staff scheduling and routing for 4,000 home care workers. The system has increased operational efficiency by 10--15 percent; this corresponds to an annual savings of 20--30 million euros ($30--$45 million). In addition, the quality of home care for elderly citizens has improved. The City of Stockholm, with its 800,000 inhabitants, adopted Laps Care in 2006 and started a full implementation and rollout during 2008, thus adding 800 units and 15,000 home care workers to the system. The savings for the City of Stockholm will be 20--30 million euros ($30--$45 million).	operations research	Patrik Eveborn;Mikael Rönnqvist;Helga Einarsdóttir;Mats Eklund;Karin Lidén;Marie Almroth	2009	Interfaces	10.1287/inte.1080.0411	decision support system;computer science;operations management;heuristics;management;operations research;scheduling;health care	HCI	12.299925387283373	-1.4371172687314622	12124
ac0fd99c800160043c9d4eb38f369c81f6ae0097	automated safety control by video cameras	safety controls;public domains;ts technical sciences;computer vision;safety and security;defence safety and security;security systems;smart cameras;human operator;suspicious behaviours;ii intelligent imaging;artificial intelligence;multiple agents;surveillance systems;physics electronics;multiple agent networks	At this moment many surveillance systems are installed in public domains to control the safety of people and properties. They are constantly watched by human operators who are easily overloaded. To support the human operators, a surveillance system model is designed that detects suspicious behaviour in a non-public area. Its task is to alert the operators about suspicious events to give them the chance to investigate and take action. A prototype application has been implemented using state-of-the-art techniques from Computer Vision and Artificial Intelligence.	artificial intelligence;computer vision;operator overloading;prototype	Iulia Lefter;Léon J. M. Rothkrantz;Maarten Somhorst	2012		10.1145/2383276.2383320	smart camera;computer vision;simulation;computer science;artificial intelligence;computer security	AI	41.77469138475573	-43.58307804018717	12136
58e55e254356ac0dbcee658eeccdea45015b6c57	inne: a structured learning algorithm for noisy examples	structure learning;learning algorithm;learning systems;learning system;biology knowledge acquisition inne structured learning algorithm noisy examples graph description learning problems description language learning engine;learning systems knowledge acquisition knowledge based systems;knowledge acquisition;logic uncertainty learning systems algorithm design and analysis machine learning algorithms testing machine learning medical treatment diseases mechanical factors;learning problems;knowledge based systems	We present a technique to learn structural concepts from noisy examples. The reason why we made this choice is that, on real data, there is often noise or uncertainty knowledge we have to deal with. We use a graph description, close to Sowa's, which allows to: better manage learning problems defme new methods present results in a familiar and practical way. We have developed a learning system INNE which is based on this kind of description language. Our goal is to design a learning algorithm which allows the processing of important amount of data in a reasonable time. We adopt particular algorithm choices and selection criteria for this purpose. This kind of learning engine has been successfully tested on a real problem in Biology. Thus an experimental basis and a validation of the method have been acquired.	algorithm;image noise;john f. sowa;structured prediction	Michel Liquiere;Jean Sallantin	1989		10.1109/TAI.1989.65304	semi-supervised learning;natural language processing;unsupervised learning;robot learning;proactive learning;multi-task learning;instance-based learning;error-driven learning;algorithmic learning theory;wake-sleep algorithm;computer science;artificial intelligence;online machine learning;knowledge-based systems;machine learning;inductive transfer;learning classifier system;stability;competitive learning;computational learning theory;active learning;generalization error	ML	5.664587535945361	-30.36171786382345	12141
2071f918b0827a99287f345e8108d71c44cbdb47	position estimation and correction of mobile robot by model-based scene matching and optimization method	mobile robot;coordinate transformation;hash table;least square method;uncertainty analysis;cross correlation	This paper presents a method to estimate and correct the positions of an indoor mobile robot. Scene matching is based on geometric hashing[9, 101. An optimization method[l2] of maximizing cross-correlation of image regions is used to estimation correction. We assume that the robot is equipped with two or more video cameras, a 2 0 laser range finder and an odometry system. The 3 0 CAD model of indoor environment is known. First, the indoor environments are remodeled in a form convenient for scene matching. Some selected features, which are the model edge positions on the horizontal plane of the 2 0 laser range finder; are transformed into geometric invariants by system calibration and range data fusion. These geometric invariants of features are modeled off-line in hashing tables according to the proposed basis constraints[l3]. They are then used as indices to make scene matching by geometric hashing with weighted voting rule. After scene matching, the self-localization of mobile robot can be completed by coordinate transformation and a least square method. Furthermore, we use a two-step optimized descent method in a number of search directions to make estimation correction. Finally, the next position estimation can be made more accurate and eficient by combining uncertainty analysis[l] with geometric hashing.	computer-aided design;cross-correlation;geometric hashing;line level;mathematical optimization;mobile robot;odometry;online and offline;scene graph;shadow volume	Yi-Bing Yang;Hung-Tat Tsui	1996			coordinate system;computer vision;mathematics;hash table;artificial intelligence;cross-correlation;pattern recognition;geometric hashing;hash function;mobile robot;odometry;sensor fusion	Robotics	52.45822266247704	-38.94209703722258	12189
93b3ac926c9796629e33e386eca6a8915953007e	cluster-based probability model applied to image restoration and compression	image sampling;cluster based probability model;kernel;image coding;probability;error statistics image restoration image coding vector quantisation entropy codes probability image sampling coding errors;coding errors;statistical signal processing;performance;entropy coding;probabilistic model accuracy;image restoration;yield estimation;statistical signal processing system;maximum instantaneous errors;probabilistic model;training data;vq;vector quantization;image compression;signal processing;entropy codes;maximum instantaneous errors cluster based probability model image restoration image compression statistical signal processing system performance probabilistic model accuracy vector quantization vq entropy coding;error statistics;signal restoration;vector quantizer;probability model;vector quantisation;image restoration image coding kernel training data probability signal processing yield estimation signal restoration laboratories vector quantization	The performance of a statistical signal processing system is determined in large part by the accuracy of the probabilistic model it employs. Accurate modeling often requires working in several dimensions, but doing so can introduce dimensionality-related diiculties. A recently introduced model circumvents some of these diiculties while maintaining accuracy suucient to account for much of the high-order, nonlinear statistical interdependence of samples. Properties of this model are reviewed, and its power demonstrated by application to image restoration and compression. Also described is a vector quantization (VQ) scheme which employs the model in entropy coding a Z N-lattice. The scheme has the advantage over standard VQ of bounding maximum instantaneous errors.	circuit restoration;entropy encoding;image compression;image restoration;interdependence;nonlinear system;statistical model;statistical signal processing;tor messenger;vector quantization	Kris Popat;Rosalind W. Picard	1994		10.1109/ICASSP.1994.389408	image restoration;statistical model;training set;kernel;speech recognition;performance;image compression;computer science;entropy encoding;signal processing;pattern recognition;probability;mathematics;statistical signal processing;vector quantization;statistics	Vision	49.16107521854648	-11.693742456817736	12208
5b97e997b9b654373bd129b3baf5b82c2def13d1	3d face tracking and texture fusion in the wild		We present a fully automatic approach to real-time 3D face reconstruction from monocular in-the-wild videos. With the use of a cascaded-regressor based face tracking and a 3D Morphable Face Model shape fitting, we obtain a semi-dense 3D face shape. We further use the texture information from multiple frames to build a holistic 3D face representation from the video frames. Our system is able to capture facial expressions and does not require any person-specific training. We demonstrate the robustness of our approach on the challenging 300 Videos in the Wild (300-VW) dataset. Our real-time fitting framework is available as an open source library at http://4dface.org.	algorithm;holism;open-source software;real-time clock;real-time web;semiconductor industry;streaming media;super-resolution imaging	Patrik Huber;Philipp Kopp;Matthias Rätsch;William J. Christmas;Josef Kittler	2016	CoRR		computer vision;speech recognition;computer science;multimedia	Vision	41.35978555393721	-47.81176873710479	12209
371415b30d8fd06931a6dc8d89a3ef0466ca7024	real-time fpga-implementation for blue-sky detection	plasma displays;vision system;colored noise;powerpc altivec;blue sky detection;tensilica xtensa;drones;salient point extraction;hardware accelerator;embedded vision systems;fpga;machine vision signal processing algorithms optical computing energy consumption computer architecture optical signal processing image motion analysis iterative algorithms computer vision embedded system;hardware accelerator blue sky detection fpga;computer vision;tv plasma displays plasma materials processing colored noise hardware algorithm design and analysis real time systems field programmable gate arrays cameras signal processing;autonomous robots;16 bit floating point instructions;image stabilization;powerpc altivec embedded vision systems tensilica xtensa altera nios2 salient point extraction optical flow computation image stabilization drones autonomous robots 16 bit floating point instructions;image sequences computer vision digital signal processing chips;signal processing;digital signal processing chips;floating point;optical flow;tv;power consumption;field programmable gate arrays;plasma materials processing;optical flow computation;algorithm design and analysis;autonomous robot;cameras;hardware;real time systems;image sequences;altera nios2	Currently, television sets with flat plasma and LCD screens with improved resolutions and better color quality are emerging. To fully utilize their capabilities, lower resolution standard definition video material is enhanced. During such process, existing noise can become clearly visible, or additional artifacts may be introduced. These impairments are usually better visible in smooth image areas such as sky regions, motivating the development of special techniques for their removal. In this paper, we introduce a hardware accelerator for an existing pixel-accurate and spatially-consistent sky-detection algorithm. We describe the algorithmic and architectural design considerations of a resource-efficient real-time system, targeting an FPGA platform. Our results show that it is feasible to implement a simplified algorithm version by using only 5,756 logic-and 23,687 memory elements of the targeted device. A demonstrator setup using real-time camera signal, proves that images of up to 640times480 at a frame rate of 30 fps can be processed. Furthermore, according to our estimations, images with pixel rates up to 142 MHz, e.g. high definition TV, can be processed by the proposed system.	algorithm;computation;experiment;field-programmable gate array;gradient;hardware acceleration;liquid-crystal display;pipeline (computing);pixel;plasma display;real-time clock;real-time computing;real-time transcription;scheduling (computing);standard-definition television;streaming media;television set;texture mapping;thresholding (image processing);virtual camera system	Nhut Thanh Quach;Bahman Zafarifar;Georgi Gaydadjiev	2007	2007 IEEE International Conf. on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2007.4429961	embedded system;computer vision;parallel computing;computer hardware;computer science;electrical engineering;operating system;signal processing;field-programmable gate array	EDA	43.931119619057256	-34.54093230488519	12213
d7f99d148ab85bafa8f9bea243a120e8ef79a8b4	deep belief networks for image denoising		Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs’ ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.	additive white gaussian noise;bayesian network;mnist database;machine learning;mean squared error;noise reduction;sensor;utility functions on indivisible goods	Mohammad Ali Keyvanrad;Mohammad Pezeshki;Mohammad Ali Homayounpour	2013	CoRR		speech recognition;computer science;machine learning;pattern recognition	ML	22.190001855475842	-48.915760931148	12218
35dff0b80e6b519b36ae5b22062e404894f60109	conditional state space models for discriminative motion estimation	linear algebra;state space methods;convex programming;kalman filters;kalman filter;motion estimation;convex optimization;computer vision;probabilistic model;state space methods motion estimation density measurement time measurement predictive models application software computer vision graphical models inference algorithms filtering algorithms;human body;linear time;graphical model;synthetic data;state space methods computer vision convex programming image sequences kalman filters linear algebra motion estimation pose estimation;state space model;image sequences;pose estimation;silhouette videos conditional state space models discriminative motion estimation real valued multivariate states computer vision discriminative undirected graphical model measurement density inference algorithm measurement dimension kalman filtering convex optimization linear algebra human body pose estimation	We consider the problem of predicting a sequence of real-valued multivariate states from a given measurement sequence. Its typical application in computer vision is the task of motion estimation. State Space Models are widely used generative probabilistic models for the problem. Instead of jointly modeling states and measurements, we propose a novel discriminative undirected graphical model which conditions the states on the measurements while exploiting the sequential structure of the problem. The major benefits of this approach are: (1) It focuses on the ultimate prediction task while avoiding probably unnecessary effort in modeling the measurement density, (2) It relaxes generative models' assumption that the measurements are independent given the states, and (3) The proposed inference algorithm takes linear time in the measurement dimension as opposed to the cubic time for Kalman filtering, which allows us to incorporate large numbers of measurement features. We show that the parameter learning can be cast as an instance of convex optimization. We also provide efficient convex optimization methods based on theorems from linear algebra. The performance of the proposed model is evaluated on both synthetic data and the human body pose estimation from silhouette videos.	algorithm;computer vision;convex optimization;discriminative model;graph (discrete mathematics);graphical model;ibm notes;kalman filter;linear algebra;linear model;mathematical optimization;motion estimation;nonlinear system;silhouette (clustering);state space;synthetic data;time complexity	Minyoung Kim;Vladimir Pavlovic	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4408943	kalman filter;computer vision;convex optimization;computer science;machine learning;pattern recognition;mathematics;discriminative model;statistics	Vision	29.561650618559792	-34.77492000766418	12223
8e67ed6999e8b187804ac18039fa1dde115fc1f2	blast-induced ground vibration prediction using support vector machine	performance measure;blast vibration;environmental effect;mean absolute error;conventional vibration predictor equations;vibration control;coefficient of determination;support vector machine	Ground vibrations induced by blasting are one of the fundamental problems in the mining industry and may cause severe damage to structures and plants nearby. Therefore, a vibration control study plays an important role in the minimization of environmental effects of blasting in mines. In this paper, an attempt has been made to predict the peak particle velocity using support vector machine (SVM) by taking into consideration of maximum charge per delay and distance between blast face to monitoring point. To investigate the suitability of this approach, the predictions by SVM have been compared with conventional vibration predictor equations. Coefficient of determination (CoD) and mean absolute error were taken as a performance measure.	approximation error;blast;coefficient of determination;experiment;kerrison predictor;support vector machine;velocity (software development)	Manoj Khandelwal	2010	Engineering with Computers	10.1007/s00366-010-0190-x	structural engineering;control engineering;support vector machine;computer science;engineering;machine learning;vibration control;control theory;coefficient of determination;mean absolute error	ML	11.107550989342108	-19.236349984532033	12231
d69e644016042d1032995bc9f51e2d72a1c1cd93	beyond trees: adopting miti to learn rules and ensemble classifiers for multi-instance data	conference contribution;computer science;learning rule;miti	MITI is a simple and elegant decision tree learner designed for multi-instance classification problems, where examples for learning consist of bags of instances. MITI grows a tree in best-first manner by maintaining a priority queue containing the unexpanded nodes in the fringe of the tree. When the head node contains instances from positive examples only, it is made into a leaf, and any bag of data that is associated with this leaf is removed. In this paper we first revisit the basic algorithm and consider the effect of parameter settings on classification accuracy, using several benchmark datasets. We show that the chosen splitting criterion in particular can have a significant effect on accuracy. We identify a potential weakness of the algorithm—subtrees can contain structure that has been created using data that is subsequently removed—and show that a simple modification turns the algorithm into a rule learner that avoids this problem. This rule learner produces more compact classifiers with comparable accuracy on the benchmark datasets we consider. Finally, we present randomized algorithm variants that enable us to generate ensemble classifiers. We show that these can yield substantially improved classification accuracy.	benchmark (computing);decision tree;emoticon;ensemble kalman filter;ensemble learning;priority queue;randomized algorithm;tree (data structure)	Luke Bjerring;Eibe Frank	2011		10.1007/978-3-642-25832-9_5	computer science;machine learning;pattern recognition;data mining	ML	14.92187588469054	-40.103043408470015	12254
1ad651d68b136a718b0478f0cf466d7762fb5704	a new particle swarm optimization method enhanced with a periodic mutation strategy and neural networks	population based intelligence algorithm;diversity;convergence;diversity variety;neural nets;transonic flow aerodynamics aerospace components benchmark testing computational fluid dynamics neural nets particle swarm optimisation;computational time reduction;aerodynamics;local controlled diversity;particle swarm optimization pso;materials;computational time reduction particle swarm optimization method periodic mutation strategy population based intelligence algorithm optimization problems multifrequency vibrational pso benchmark test functions direct shape optimization airfoil transonic flow mutation application strategy diversity variety global random diversity local controlled diversity mutation operator objective function value artificial neural network;mutation operator;optimization problems;airfoil;computational fluid dynamics;transonic flow;benchmark test functions;vectors;objective function value;particle swarm optimization;particle swarm optimization pso airfoil diversity mutation neural nets;periodic mutation strategy;optimization;vectors optimization algorithm design and analysis materials equations particle swarm optimization convergence;direct shape optimization;multifrequency vibrational pso;particle swarm optimization method;aerospace components;particle swarm optimisation;algorithm design and analysis;benchmark testing;mutation;global random diversity;artificial neural network;mutation application strategy	Particle swarm optimization (PSO), a relatively new population-based intelligence algorithm, exhibits good performance on optimization problems. However, during the optimization process, the particles become more and more similar, and gather into the neighborhood of the best particle in the swarm, which makes the swarm prematurely converged most likely around the local solution. A new optimization algorithm called multifrequency vibrational PSO is significantly improved and tested for two different test cases: optimization of six different benchmark test functions and direct shape optimization of an airfoil in transonic flow. The algorithm emphasizes a new mutation application strategy and diversity variety, such as global random diversity and local controlled diversity. The results offer insight into how the mutation operator affects the nature of the diversity and objective function value. The local controlled diversity is based on an artificial neural network. As far as both the demonstration cases' problems are considered, remarkable reductions in the computational times have been accomplished.	algorithm;artificial neural network;benchmark (computing);computation;distribution (mathematics);mathematical optimization;optimization problem;particle swarm optimization;phase-shift oscillator;randomness;rendering (computer graphics);shape optimization	Y. Volkan Pehlivanoglu	2013	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2012.2196047	mutation;optimization problem;algorithm design;benchmark;mathematical optimization;multi-swarm optimization;simulation;meta-optimization;convergence;computational fluid dynamics;computer science;transonic;derivative-free optimization;artificial intelligence;machine learning;airfoil;imperialist competitive algorithm;particle swarm optimization;artificial neural network;metaheuristic	ML	28.545720373857023	-5.066181046585882	12257
6e092ae31fce272f8de42c1fd16d549f3a95cf2b	the negative binomial distribution as a trend distribution for circulation data in flemish public libraries	bibliometrie;circulacion documento;bibliotheque;belgica;europa;statistique;gestion fonds;document circulation;analisis estadistico;aplicacion;modele mathematique;journal contribution;loi probabilite;ley probabilidad;recoleccion dato;data gathering;negative binomial distribution;biblioteconomia;data collection;bibliometria;bibliotheconomie;livre;loi binomiale negative;flandres;modelo matematico;gestion fondos;distribucion estadistica;stock management;circulation document;belgique;belgium;probability law;statistical analysis;distribution statistique;ley binomial negativa;libro;analyse statistique;probability theory;public library;statistics;mathematical model;librarianship;bibliometrics;theorie probabilite;teoria probabilidad;flanders;europe;book;public libraries;application;collecte donnee;bibliotheque publique;biblioteca;statistical distribution;library;estadistica;biblioteca publica	Based on data collected by the authors in Flemish public libraries, we show how the negative binomial distribution (NBD) can be used as a trend distribution for library circulation data. Although actual data show more variation than simple statistics can explain, we recommend the use of the NBD for practical, managerial purposes. As a consequence we also recommend the teaching of these methods in introductory library management courses.	library (computing);network block device;public library	Marie-Jeanne Leemans;Marleen Maes;Ronald Rousseau;Christel Ruts	1992	Scientometrics	10.1007/BF02016846	operations research;statistics;data collection	Metrics	34.69565100444609	-20.423709392451936	12262
a26ee4dcc1011e3e03b4e74c9a6889baea471aad	optimal sequential diagnostic strategy generation considering test placement cost for multimode systems	biological patents;biomedical journals;text mining;europe pubmed central;diagnostic strategy;citation search;multimode system;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;sequential fault diagnosis;full text;and or graph;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Sequential fault diagnosis is an approach that realizes fault isolation by executing the optimal test step by step. The strategy used, i.e., the sequential diagnostic strategy, has great influence on diagnostic accuracy and cost. Optimal sequential diagnostic strategy generation is an important step in the process of diagnosis system construction, which has been studied extensively in the literature. However, previous algorithms either are designed for single mode systems or do not consider test placement cost. They are not suitable to solve the sequential diagnostic strategy generation problem considering test placement cost for multimode systems. Therefore, this problem is studied in this paper. A formulation is presented. Two algorithms are proposed, one of which is realized by system transformation and the other is newly designed. Extensive simulations are carried out to test the effectiveness of the algorithms. A real-world system is also presented. All the results show that both of them have the ability to solve the diagnostic strategy generation problem, and they have different characteristics.	algorithm;computer simulation;engineering;fault detection and isolation;graph traversal;search problem;software propagation;world-system;executing - querystatuscode	Shigang Zhang;Lijun Song;Wei Zhang;Zheng Hu;Yongmin Yang	2015		10.3390/s151025592	text mining;medical research;simulation;computer science;bioinformatics;engineering;electrical engineering;data mining;operations research;statistics	AI	19.06112787276147	-2.2787244456514304	12272
4abc49e74d3add87f05e40b1ca08e2a5d5edf84e	prediction of television audience rating based on fuzzy cognitive maps with forward stepwise regression	data mining;fuzzy cognitive maps;television audience rating;forward stepwise regression	The television audience rating is an important indicator of the quality of television programs and important reference for decision-television operator. As many factors that affect the ratings and the trends are complex, the article proposes a television rating mining predictive model based on fuzzy cognitive maps (FCMs) with forward stepwise regression. The FCMs use the causal relationship among various concept nodes to simulate the fuzzy reasoning, and enhance the dynamic behavior of the simulation system with its feedback mechanism, which is suitable for system to predict the trend of television audience rating. A FCM-based model for predicting television audience rating is proposed in this paper. The forward stepwise regression algorithm is used to obtain concept nodes of coarse weight matrix for FCMs, and then a training weight algorithm is used to refine the coarse weight matrix model. The FCM model is applied to mine the television audience rating, realizing to predict the television playback volum...	fuzzy cognitive map;stepwise regression	Nan Ma;Patrick Wang;Qin He;Wenjia Li;Ying Zheng;Zhang Huan	2017	IJPRAI	10.1142/S0218001417500203	pattern recognition;fuzzy logic;artificial intelligence;machine learning;operator (computer programming);stepwise regression;fuzzy cognitive map;computer science	NLP	5.006155943158321	-23.170441382541487	12275
6f7b882ed394ab4a64e351cf9da192f9b9378b44	an extended model for a spiking neuron class	integrate and fire;model validation;dynamic threshold;spiking neurons;information processing;integrate and fire neuron;artificial neural network;neural network	This paper proposes an extension to the model of a spiking neuron for information processing in artificial neural networks, developing a new approach for the dynamic threshold of the integrate-and-fire neuron. This new approach invokes characteristics of biological neurons such as the behavior of chemical synapses and the receptor field. We demonstrate how such a digital model of spiking neurons can solve complex nonlinear classification with a single neuron, performing experiments for the classical XOR problem. Compared with rate-coded networks and the classical integrate-and-fire model, the trained network demonstrated faster information processing, requiring fewer neurons and shorter learning periods. The extended model validates all the logic functions of biological neurons when such functions are necessary for the proper flow of binary codes through a neural network.	action potential;artificial neural network;artificial neuron;binary code;binary data;biological neuron model;boolean;chemical synapse;computation;exclusive or;experiment;hl7publishingsubsection <operations>;information processing;learning disorders;license;neuroscience discipline;nonlinear system;physiological processes;silicon;synapses;weight	Ana M. G. Guerreiro;Carlos A. Paz de Araujo	2007	Biological Cybernetics	10.1007/s00422-007-0169-x	winner-take-all;biological neural network;codi;neuroscience;types of artificial neural networks;random neural network;computer science;artificial intelligence;machine learning;regression model validation;artificial neural network;spiking neural network	ML	14.546036927037427	-27.342807694766588	12276
57fb2d2e9a35f7df4fa18cbaa1da5ca97d7ad0ae	theory and application of entropiograph based i-divergence estimation	entropy;pattern classification;pattern clustering;probability;trees (mathematics);mst;arbitrary contamination density;binary hypothesis test;clustering;entropic-graph based i-divergence estimation;entropic-graph information divergence estimate;entropic-graph methods;minimal spanning tree;mixture densities;probability density function estimation;robust classification;uniform contamination density	This paper addresses the problem of robust classification of mixture densities by using an entropic-graph information divergence estimate; this provides a means to robustly estimate I-divergence without using any explicit probability density function estimation procedure. We previously applied entropic-graph methods to clustering and classification for mixture densities having uniform contamination density. This paper describes an extension of our previous methods to mixture densities with arbitrary contamination density. Under the assumption that at least one of the pdf's can be estimated from a training sample, a binary hypothesis test is proposed for testing whether an independent target sample has identical distribution as the training sample. This test is based on thresholding an entropic-graph I-divergence estimate constructed from the Minimal Spanning Tree (MST) spanning the target sample on a transformed data space.	cluster analysis;dataspaces;file spanning;minimum spanning tree;portable document format;preprocessor;thresholding (image processing);whitening transformation	Olivier J. J. Michel;Alfred O. Hero	2002	2002 11th European Signal Processing Conference		econometrics;pattern recognition;mathematics;statistics	ML	32.20551492566776	-29.731421686127963	12291
6032ba697ab3c652528ad3e13f33adc2e66c3c86	compression of 3-d echocardiographic images using a modified 3-d set-partitioning-in-hierarchical-trees algorithm based on a 3-d wavelet packet transform	set partitioning in hierarchical trees;wavelet transforms;wavelet transform;wavelet packet transform;wavelet packet decomposition;compression ratio;wavelets	An efficient compression strategy is indispensable for 3-D digital echocardiography, which can provide more accurate diagnostic information than 2-D echocardiography without geometric assumption, but unfortunately requires a huge storage space. We describe a new set partitioning in hierarchical trees (SPIHT) algorithm that is based on the wavelet packet transform and use it to compress 3-D echocardiographic images. The new algorithm originates from the well-known SPIHT algorithm that is based on a wavelet transform but outperforms it for the compression of 3-D echocardiographic images. Experimental results are presented to verify the effectiveness of the new SPIHT algorithm. Results show that the new algorithm achieves a very high compression ratio while retaining good quality 3-D echocardiographic images. © 2006 SPIE and IS&T. DOI: 10.1117/1.2194467	algorithm;set partitioning in hierarchical trees;wavelet packet decomposition;wavelet transform;whole earth 'lectronic link	Xiyi Hang;Neil L. Greenberg;Yuan F. Zheng;James D. Thomas	2006	J. Electronic Imaging	10.1117/1.2194467	wavelet;mathematical optimization;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;theoretical computer science;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;set partitioning in hierarchical trees;wavelet transform	Graphics	42.381816736148814	-15.225020459146068	12300
cd29d18ad62ff06635d74175d099e36d990f313e	a new predictive model for the cyanotoxin content from experimental cyanobacteria concentrations in a reservoir based on the abc optimized support vector machine approach: a case study in northern spain	equipment;honey bee colonies;case studies;ecology;aquatic organisms;vectors;toxins;research projects;regression analysis;health;recreation;information	Cyanotoxins, a kind of poisonous substances produced by cyanobacteria, are responsible for health risks in surface waters used for drinking or for recreation. Consequently, anticipation of its presence is a matter of importance to prevent risks. The aim of this study is to build a cyanotoxin diagnostic model by using support vector machines (SVMs) in combination with the artificial bee colony (ABC) technique from cyanobacterial concentrations determined experimentally in the Trasona reservoir (Northern Spain), to forecast the cyanotoxins' presence in the Trasona reservoir (Northern Spain). The ABC–SVM model is aimed at highly nonlinear biological problems with sharp peaks and the tests carried out have proven its high performance. The results of the present study are two-fold. In the first place, the significance of each biological and physical–chemical variables on the cyanotoxin content in the reservoir is presented through the model. Secondly, a predictive model of the cyanotoxin content is obtained. The agreement of the ABC–SVM-based model with experimental data confirmed its good performance. Finally, conclusions of this innovative research work are exposed.	abc (stream cipher);coefficient of determination;ecosystem;emoticon;numerical analysis;radial basis function kernel;radial basis function network;support vector machine;synergy	P. J. García Nieto;J. R. Alonso Fernández;E. García-Gonzalo;C. Díaz Muñiz;Ricardo Mayo Bayón;Víctor M. González Suárez	2015	Ecological Informatics	10.1016/j.ecoinf.2015.09.010	recreation;information;computer science;mathematics;health;ecology;regression analysis;statistics	ML	11.811094286927963	-18.815586416450937	12313
a37f480cf340b6cd2c2fda4db31e7cc89bcddc8d	bayesian inference in monte-carlo tree search		Monte-Carlo Tree Search (MCTS) methods are drawing great interest after yielding breakthrough results in computer Go. This paper proposes a Bayesian approach to MCTS that is inspired by distributionfree approaches such as UCT [13], yet significantly differs in important respects. The Bayesian framework allows potentially much more accurate (Bayes-optimal) estimation of node values and node uncertainties from a limited number of simulation trials. We further propose propagating inference in the tree via fast analytic Gaussian approximation methods: this can make the overhead of Bayesian inference manageable in domains such as Go, while preserving high accuracy of expected-value estimates. We find substantial empirical outperformance of UCT in an idealized bandit-tree test environment, where we can obtain valuable insights by comparing with known ground truth. Additionally we rigorously prove on-policy and off-policy convergence of the proposed methods.	approximation;bayesian network;coat of arms;coefficient;computation;computer go;deployment environment;edward wegman;ground truth;machine learning;mark n. wegman;maxima and minima;monte carlo tree search;overhead (computing);scalability;simulation;speedup;tree testing;tree traversal;treewidth	Gerald Tesauro;V. T. Rajan;Richard Segal	2010			econometrics;frequentist inference;machine learning;data mining;mathematics;bayesian statistics;statistics	ML	26.678565784636387	-28.549154344766226	12317
5f4779e7fe7321e4842b20944da8d83b7e27755e	genetic nearest feature plane	clustering;genetic algorithm;nearest feature plane classifier	The problem addressed in this paper concerns the complexity reduction of the nearest feature plane classifier, so that it may be applied also in dataset where the training set contains many patterns. This classifier considers, to classify a test pattern, the subspaces created by each combination of three training patterns. The main problem is that in dataset of high cardinality this method is unfeasible. A genetic algorithm is here used for dividing the training patterns in several clusters which centroids are used to build the feature planes used to classify the test set. The performance improvement with respect to other nearest neighbor based classifiers is validated through experiments with several benchmark datasets.		Loris Nanni;Alessandra Lumini	2009	Expert Syst. Appl.	10.1016/j.eswa.2007.10.009	nearest-neighbor chain algorithm;genetic algorithm;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis	Vision	13.302827879774007	-42.101665153930774	12324
14770c14fd24e614c7daaf8e8f983bf419f552f4	freeze detection in 2d navigation video sequences by matching of extracted line segments	video signal processing;edge detection;image matching;matrix algebra;feature extraction;traffic engineering computing;error detection;video signal processing computerised navigation edge detection error detection feature extraction image matching image sequences matrix algebra traffic engineering computing;navigation eigenvalues and eigenfunctions video sequences vectors monitoring detection algorithms transforms;map jump freeze artifact detection 2d navigation video sequences extracted line segment matching artifact detection framework automated error detection framework infotainment system computerised navigation system field tests frozen frame geometric transformation matrix;image sequences;computerised navigation	In this paper, we present an automated error/ artifact detection framework that monitors the infotainment system of the car. Specifically, this paper focuses on the detection of freezing artifacts that could occur in the navigation system of the car during field tests. Knowing that the motion in the navigation system could also stop when the car stops, it is crucial to differentiate between this situation and a real freezing artifact. We propose an algorithm that reliably detects map jumps which occur after the freezing artifacts. The proposed algorithm extracts lines from the possibly frozen frame and the frame following the freeze event and matches the extracted lines together in order to output a geometric transformation matrix that describes the motion between both frames. If the motion is larger than normal, a map jump is detected and a freeze event is signaled.	algorithm;feature model;global positioning system;image registration;transformation matrix	Gilbert Yammine;Eugen Wige;Franz Simmet;Dieter Niederkorn;André Kaup	2012	2012 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2012)	10.1109/ICVES.2012.6294329	computer vision;simulation;computer science;computer graphics (images)	Robotics	44.46137395239089	-45.33226343693191	12325
88bfaf155e9d1a599ef4454029ad9cdf41469b02	a memetic-neural approach to discover resources in p2p networks	resource discovery;local search algorithm;book chapter;computational intelligence;population size;p2p;working conditions;memetic algorithm;objective function;multi layer perceptron;p2p networks;peer to peer;neural network	This chapter proposes a neural network based approach for solving the resource discovery problem in Peer to Peer (P2P) networks  and an Adaptive Global Local Memetic Algorithm (AGLMA) for performing in training of the neural network. The neural network,  which is a multi-layer perceptron neural network, allows the P2P nodes to efficiently locate resources desired by the user.  The necessity of testing the network in various working conditions, aiming to obtain a robust neural network, introduces noise  in the objective function. The AGLMA is a memetic algorithm which employs two local search algorithms adaptively activated  by an evolutionary framework. These local searchers, having different features according to the exploration logic and the  pivot rule, have the role of exploring decision space from different and complementary perspectives. Furthermore, the AGLMA  makes an adaptive noise compensation by means of explicit averaging on the fitness values and a dynamic population sizing  which aims to follow the necessity of the optimization process. The numerical results demonstrate that the proposed computational  intelligence approach leads to an efficient resource discovery strategy and that the AGLMA outperforms an algorithm classically  employed for executing the neural network training.  	memetics;peer-to-peer	Ferrante Neri;Niko Kotilainen;Mikko Vapa	2008		10.1007/978-3-540-70807-0_8	computer science;artificial intelligence;machine learning;data mining	ML	26.29011124220865	-4.905401393586162	12340
6a79d9518fbcf0358a0a1a6d5965a071b1ad6312	maximizing strike aircraft planning efficiency for a given class of ground targets		Strike planning is one of the fundamental tasks of an advanced Air Force and involves the assignment of strike aircraft to ground targets with a maximum level of efficiency. Therefore, planning an optimal strike based on the preferences of the decision maker is crucial. The efficiency of the strike plan in this paper implies attacking the maximum number of targets while considering target priority and the desired level of damage on each target. The other objective is to minimize the cost of the strike plan. This paper develops a methodology that maximizes the efficiency of the strike plan. Given this efficiency, the aircraft and weapon costs plus the distance flown is then minimized. The methodology also considers the capacities for different types of aircraft and weapons at each aircraft base to avoid assigning aircraft to targets from a base where there are insufficient resources to do so. Computational results are presented that analyze the sensitivity of the model solution times to solver optimality tolerance and aircraft and weapon capacities. Results also suggest substantial cost savings are possible while still maintaining the effectiveness of the strike package.	computation;cost efficiency;design of experiments;experiment;in-phase and quadrature components;numerical analysis;solver;symposium on principles of database systems	Necip Dirik;Shane N. Hall;James T. Moore	2015	Optimization Letters	10.1007/s11590-014-0844-5	simulation	AI	12.22717956717869	0.3977419908980627	12345
b3958148917567c4643cba071de2d0e13ecb28a4	a trajectory recommendation system via optimizing sensors utilization in airborne systems (demo paper)		Airborne sensory system is equipped on piloted or remotely-piloted aerial vehicles to collect and transmit imagery data back to the ground users. In traditional approaches where pilots to satisfy spatio-temporal tasks via image capturing, the pilot is required to manually decide an alternative trajectory to satisfy as many tasks as possible while maintaining a low deviation cost due to fuel constraint. Additionally, various constraints on tasks and original flight trajectory must be satisfied as well, such as temporal and Quality of Service constraints. We show a demo of a trajectory recommendation framework consists of two approaches to generate an optimized trajectory with the above goals by increasing sensor utilization via task aggregation and scheduling. We demonstrate a trajectory recommendation system that accepts user inputs and outputs visualization of intermediate processes and final trajectory.	airborne ranger;optimizing compiler;recommender system	San Yeung;Sanjay Kumar Madria;Mark Linderman	2015		10.1007/978-3-319-22363-6_31	embedded system;simulation;remote sensing	DB	52.45273101443447	-27.262388543416385	12359
5e1c082c0859b2412d0729768fa5dedeef33c2ea	kernel-based classification in complex-valued feature spaces for polarimetric sar data	markov random field mrf polarimetric sar reproducing kernel hilbert space rkhs support vector machine svm;polsar image kernel based classification complex valued feature spaces polarimetric sar data kernel based approach kernel hilbert spaces admissible kernel functions complex valued feature classification support vector machine svm classifier case specific interpretation maximum margin hyperplane complex vector space markov random fields case specific techniques powell numerical algorithms ho kashyap numerical algorithms sir c data;synthetic aperture radar feature extraction geophysical image processing geophysical techniques image classification remote sensing by radar;kernel synthetic aperture radar feature extraction support vector machine classification accuracy remote sensing	A kernel-based approach is proposed in this paper to address supervised classification of polarimetric SAR data. Relevant features extracted from such data are generally complex-valued (e.g., scattering coefficients, multilook covariance-matrix entries). First, based on the theory of complex reproducing kernel Hilbert spaces (RKHS's), a family of admissible kernel functions tailored to the classification of complex-valued features is proposed. Then, a support vector machine (SVM) classifier is developed using this family of kernels and a case-specific interpretation is discussed for the related notion of maximum-margin hyperplane in a complex vector space. Finally, a spatial-contextual classifier is introduced by integrating the proposed family of kernels with a recent combination of SVM and Markov random fields. Case-specific techniques, based on the Powell and Ho-Kashyap numerical algorithms, are incorporated in the proposed methods to automatically optimize their parameters. Experiments with SIR-C data are discussed.	algorithm;coefficient;hilbert space;kernel (operating system);linux;machine learning;markov chain;markov random field;numerical analysis;polarimetry;powell's method;supervised learning;support vector machine	Sebastiano B. Serpico	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6946661	computer vision;kernel method;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;machine learning;pattern recognition;mathematics;polynomial kernel	ML	30.016321861781552	-42.75420072413232	12366
5fb4c2b5ebb6c9cd70509535b23ef10e3f9b1a7a	context-aware recommendations with random partition factorization machines		Context plays an important role in helping users to make decisions. There are hierarchical structure between contexts and aggregation characteristics within the context in real scenarios. Exist works mainly focus on exploring the explicit hierarchy between contexts, while ignoring the aggregation characteristics within the context. In this work, we explore both of them so as to improve accuracy of prediction in recommender systems. We propose a Random Partition Factorization Machines (RPFM) by adopting random decision trees to split the contexts hierarchically to better capture the local complex interplay. The intuition here is that local homogeneous contexts tend to generate similar ratings. During prediction, our method goes through from the root to the leaves and borrows from predictions at higher level when there is sparseness at lower level. Other than estimation accuracy of ratings, RPFM also reduces the over-fitting by building an ensemble model on multiple decision trees. We test RPFM over three different benchmark contextual datasets. Experimental results demonstrate that RPFM outperforms state-of-the-art context-aware recommendation methods.	birch;benchmark (computing);big data;cluster analysis;dbscan;decision tree;disk partitioning;fm broadcasting;graphics processing unit;k-means clustering;lisp machine;neural coding;overfitting;parallel computing;rapid update cycle;recommender system;rock (processor);test set;tree (data structure)	Shaoqing Wang;Cuiping Li;Kankan Zhao;Hong Chen	2017	Data Science and Engineering	10.1007/s41019-017-0035-3	theoretical computer science;machine learning;data mining	Web+IR	-2.8945993146994953	-42.50988232825443	12388
c37d81519d3b14491c6c1d5f8a910aa2b92ba67f	vision-based pedestrian tracking system using color and motion cue	histograms tracking image color analysis hardware feature extraction radar tracking detectors;detectors;histograms;radar tracking;histogram pedestrian tracking hog color motion;image color analysis;feature extraction;video signal processing computational complexity computer vision image colour analysis image fusion image motion analysis image sensors object detection object tracking pedestrians road safety;histogram comparison technique vision based pedestrian tracking system color information motion cue intelligent vehicles road safety improvement moving cameras hog human detector pedestrian detection video frames color based similarity fusion motion based similarity fusion image regions histogram utilization high computational complexity histogram generation technique;tracking;hardware	Pedestrian tracking is becoming increasingly important for intelligent vehicles to improve the safety on road. Vision based pedestrian tracking with moving cameras faces notorious challenges. The classical background subtraction technique in surveillance applications is no longer applicable for Advanced Driver Assistance Systems (ADASs). In this paper, we propose a vision based pedestrian tracking algorithm which relies on color and motion information. First, pedestrians are detected using the HOG human detector in each image frame. The detected pedestrians are then associated over video frames based on fusion of color-based similarity and motion-based similarity between two image regions. Our studies reveal that the utilization of histogram in the proposed method contributes to high computational complexity. As such, preliminary results for a hardware-efficient implementation of histogram generation and comparison technique have been presented.	algorithm;background subtraction;clock rate;color;computation;computational complexity theory;eye tracking;prototype;real-time clock;real-time computing;tracking system;video tracking	Meiqing Wu;Siew Kei Lam;Thambipillai Srikanthan;Tushar Shah	2014	2014 International Symposium on Integrated Circuits (ISIC)	10.1109/ISICIR.2014.7029538	computer vision;simulation;color normalization;geography;video tracking;histogram equalization;image histogram;computer graphics (images)	Robotics	43.29981816930567	-44.83859475971813	12414
1647cc5aaf955e8346912cecb46ca4ed80151d15	variational nonparametric bayesian hidden markov model	dirichlet process;state space methods;variational nonparametric bayesian hidden markov model;gaussian processes;hidden markov model;bayes methods;variational techniques;bayesian methods;variational bayesian;speech;inference mechanisms;variational inference method;graphical models;real world application;hidden markov models;machine learning;hidden state space;variational inference;state space;variational techniques bayes methods gaussian processes hidden markov models inference mechanisms speech recognition;real world speech recognition application;bayesian methods hidden markov models state space methods speech recognition large scale systems pattern recognition machine learning graphical models gaussian distribution;pattern recognition;speech recognition;hidden state space variational nonparametric bayesian hidden markov model dirichlet process gaussian component variational inference method synthetic data real world speech recognition application;approximation methods;markov processes;synthetic data;speech recognition nonparametric bayesian hidden markov model variational inference;context modeling;nonparametric bayesian;gaussian distribution;gaussian component;large scale systems	The Hidden Markov Model (HMM) has been widely used in many applications such as speech recognition. A common challenge for applying the classical HMM is to determine the structure of the hidden state space. Based on the Dirichlet Process, a nonparametric Bayesian Hidden Markov Model is proposed, which allows an infinite number of hidden states and uses an infinite number of Gaussian components to support continuous observations. An efficient variational inference method is also proposed and applied on the model. Our experiments demonstrate that the variational Bayesian inference on the new model can discover the HMM hidden structure for both synthetic data and real-world applications.	experiment;hidden markov model;markov chain;speech recognition;state space;synthetic data;variational principle	Nan Ding;Zhijian Ou	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495125	normal distribution;bayesian probability;computer science;state space;speech;machine learning;hidden semi-markov model;free energy principle;pattern recognition;gaussian process;mathematics;context model;graphical model;markov process;markov model;hidden markov model;statistics;synthetic data	ML	29.444672335681016	-30.81240718078607	12417
f47fdcacd1fec0b5dfcf1e53972ef0b18d735170	medal: a cost-effective high-frequency energy data acquisition system for electrical appliances		Traditional energy measurement fails to provide support to consumers to make intelligent decisions to save energy. Non-intrusive load monitoring is one solution that provides disaggregated power consumption profiles. Machine learning approaches rely on public datasets to train parameters for their algorithms, most of which only provide low-frequency appliance-level measurements, thus limiting the available feature space for recognition.  In this paper, we propose a low-cost measurement system for high-frequency energy data. Our work utilizes an off-the-shelf power strip with a voltage-sensing circuit, current sensors, and a single-board PC as data aggregator. We develop a new architecture and evaluate the system in real-world environments. The self-contained unit for six monitored outlets can achieve up to 50 kHz for all signals simultaneously. A simple design and off-the-shelf components allow us to keep costs low. Equipping a building with our measurement systems is more feasible compared to expensive existing solutions. We used the outlined system architecture to manufacture 20 measurement systems to collect energy data over several months of more than 50 appliances at different locations, with an aggregated size of 15 TB.	algorithm;data acquisition;data aggregation;data quality;feature vector;ground truth;machine learning;requirement;sampling (signal processing);sensor;single-board computer;sound quality;system of measurement;systems architecture;systems design;terabyte	Thomas Kriechbaumer;Anwar Ul Haq;Matthias Kahl;Hans-Arno Jacobsen	2017		10.1145/3077839.3077844	telecommunications;engineering;electrical engineering	Mobile	11.511587556167347	-15.347887579582823	12427
6fdfe599b1e2f02afe62bab1cc4151dd556786ae	walsh–hadamard-based 3-d steganography for protecting sensitive information in point-of-care	electrocardiography electroencephalography privacy temperature sensors cryptography data mining;biomedical signal steganography walsh hadamard privacy preservation authenticity watermarking	Remote points-of-care has recently had a lot of attention for their advantages such as saving lives and cost reduction. The transmitted streams usually contain 1) normal biomedical signals (e.g., electrocardiograms) and 2) highly private information (e.g., patient identity). Despite the obvious advantages, the primary concerns are privacy and authenticity of the transferred data. Therefore, this paper introduces a novel steganographic mechanism that ensures 1) strong privacy preservation of private information by random concealing inside the transferred signals employing a key and 2) evidence of originality for the biomedical signals. To maximize hiding, fast Walsh–Hadamard transform is utilized to transform the signals into a group of coefficients. To ensure the lowest distortion, only less-significant values of coefficients are employed. To strengthen security, the key is utilized in a three-dimensional (3-D) random coefficients’ reform to produce a 3-D order employed in the concealing process. The resultant distortion has been thoroughly measured in all stages. After extensive experiments on three types of signals, it has been proved that the algorithm has a little impact on the genuine signals ( $<$1 %). The security evaluation also confirms that unlawful retrieval of the hidden information within rational time is mightily improbable.	3d computer graphics;algorithm;biologic preservation;coefficient;distortion;experiment;fast walsh–hadamard transform;hadamard transform;information sensitivity;patients;personally identifiable information;privacy;resultant;steganography;disease transmission	Alsharif Abuadbba;Ibrahim Khalil	2017	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2016.2631885	steganography;streams;artificial intelligence;real-time computing;computer vision;cryptography;information sensitivity;distortion;hadamard transform;digital watermarking;private information retrieval;computer security;computer science	Security	38.18595253178106	-10.790580731420901	12430
d4d5a97fdc361272f5f42a767317ae244233e5b6	machine learning task as a diclique extracting task	graph theory;instance based learning;decision tree learning;itemsets;pattern;training;prediction algorithms;inductive leaning task;data mining;training data;artificial neural networks;learning by example;inductive learning algorithm machine learning inductive learning pattern diclique diqlique extracting task;machine learning;bayesian learning;machine learning machine learning algorithms data mining bipartite graph decision trees genetic algorithms fuzzy systems informatics artificial neural networks bayesian methods;graph theoretical diclique extracting task;inductive learning;diclique;diqlique extracting task;genetic algorithm;genetic algorithms;inductive leaning task machine learning task solution graph theoretical diclique extracting task data mining decision tree learning artificial neural networks bayesian learning instance based learning genetic algorithms;machine learning task solution;artificial neural network;learning by example graph theory;inductive learning algorithm;hair	As we know there exist several approaches and algorithms for data mining and machine learning task solution, for example, decision tree learning, artificial neural networks, Bayesian learning, instance-based learning, genetic algorithms, etc. They are effective and well-known and their base algorithms and main ideology are published. In this paper we present a new approach for machine learning (ML) task solution, an inductive learning algorithm based on diclique extracting task. We show how to transform ML as inductive leaning task into the graph theoretical diclique extracting task, present an example and discuss about the problems related with that approach and effectiveness of the algorithm.	artificial neural network;data mining;decision tree learning;existential quantification;genetic algorithm;instance-based learning;machine learning	Rein Kuusik;Tarvo Treier;Grete Lind;Peeter Roosmann	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.453	multi-task learning;instance-based learning;error-driven learning;genetic algorithm;wake-sleep algorithm;computer science;machine learning;pattern recognition;data mining;task analysis;active learning;artificial neural network;generalization error	ML	13.370301200709811	-36.014549901226175	12440
b155f9ca0f7a43f972f9ec150d8ae75a9044f3ed	development of an efficient classifier using proposed sensitivity-based feature selection technique for intrusion detection system		Intrusion detection system protects an individual computer or network computer from suspicious data and protects the system from unauthorized access. In this paper, we propose a feature selection technique (FST) known as sensitivity based feature selection technique (SBFST) which selects relevant features from intrusion data based on the value of sensitivity. We compare various existing FSTs with the proposed SBFST from three different categories of NSL-KDD data set. Experimental results reveal that C4.5 with SBFST performs better than other existing FSTs and produce a high accuracy of 99.68% with 11 features and 99.95% accuracy with nine features for the multiclass and binary class problems respectively. It has also produced 99.64% accuracy for both multiclass and binary class problems respectively with six and seven features. The performance of proposed SBFST is also verified using the intersection of features, segment by segment with other FSTs and found to be better.	feature selection;intrusion detection system	H. S. Hota;Dinesh K. Sharma;Akhilesh Kumar Shrivas	2018	IJICS	10.1504/IJICS.2018.10010649	computer network;network computer;feature selection;classifier (linguistics);intrusion detection system;binary number;intrusion;computer science;pattern recognition;artificial intelligence	Security	6.681062741002303	-37.970860679345826	12450
e637d448f0cd2524d2f7d5a34c40bd8f8b78d92b	a cluster validity index based on frequent pattern	pattern clustering;fuzzy reasoning;benchmark dataset cluster validity index frequent pattern cv index fuzzy clustering algorithm fuzzy c means fcm logical reasoning artificial dataset;indexes integrated optics;会议论文;integrated optics;fuzzy set theory;indexes;fuzzy c means fcm frequent pattern cluster validity cv fuzzy cluster analysis;pattern clustering fuzzy reasoning fuzzy set theory	Since a clustering algorithm can produce as many partitions as desired, one need to assess their quality in order to select the partition that most represents the structure in the data. This is the rationale for the cluster-validity (CV) problem and indices. This paper proposes a CV index for fuzzy-clustering algorithm, such as the fuzzy c-means (FCM) or its derivatives. Given a fuzzy partition, this new index uses global information and is based on more logical reasoning than geometrical features. Experimental results on artificial and benchmark datasets are given to demonstrate the performance of the proposed index, as compared with traditional and recent indices.	algorithm;benchmark (computing);cluster analysis;design rationale;fuzzy clustering;fuzzy cognitive map;fuzzy concept	Hongyan Cui;Kuo Zhang;Xu Huang;Yunjie Liu	2013	2013 16th International Symposium on Wireless Personal Multimedia Communications (WPMC)		database index;defuzzification;fuzzy clustering;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	Robotics	2.040528048504314	-39.75968003899588	12451
a944085eaf8146a81c6242feac4cc921d6206058	self-improving visual odometry		We propose a self-supervised learning framework that uses unlabeled monocular video sequences to generate large-scale supervision for training a Visual Odometry (VO) frontend, a network which computes pointwise data associations across images. Our self-improving method enables a VO frontend to learn over time, unlike other VO and SLAM systems which require time-consuming hand-tuning or expensive data collection to adapt to new environments. Our proposed frontend operates on monocular images and consists of a single multi-task convolutional neural network which outputs 2D keypoints locations, keypoint descriptors, and a novel point stability score. We use the output of VO to create a self-supervised dataset of point correspondences to retrain the frontend. When trained using VO at scale on 2.5 million monocular images from ScanNet, the stability classifier automatically discovers a ranking for keypoints that are not likely to help in VO, such as t-junctions across depth discontinuities, features on shadows and highlights, and dynamic objects like people. The resulting frontend outperforms both traditional methods (SIFT, ORB, AKAZE) and deep learning methods (SuperPoint and LF-Net) in a 3D-to-2D pose estimation task on ScanNet.		Daniel DeTone;Tomasz Malisiewicz;Andrew Rabinovich	2018	CoRR			ML	29.487153157917078	-49.84531399185618	12455
5d7a391a342a79bb5ff683d2b88dfdf8595ac581	millimeter wave ultra wide band short range radar localization accuracy	millimeter wave radar ultra wideband radar vehicle safety bandwidth awgn radar signal processing millimeter wave technology laboratories road vehicles ultra wideband technology;frequency 77 ghz millimeter wave ultra wide band radar short range radar localization accuracy road vehicle radar high distance resolution toa technique tdoa technique doa technique impulse radio evaluation frequency 76 ghz;sensors;frequency 77 ghz;ultra wideband radar;radio direction finding;millimeter wave ultra wide band radar;radar resolution;millimeter wave radar;awgn;local system;road vehicle radar;high distance resolution;receivers;accuracy;distance measurement;short range radar localization accuracy;estimation;frequency 76 ghz;toa technique;bandwidth;ultra wideband radar direction of arrival estimation millimetre wave devices radar resolution radio direction finding road vehicle radar;vehicle safety;impulse radio evaluation;vehicles;millimeter wave;experimental evaluation;tdoa technique;millimeter wave technology;impulse radio;doa technique;millimetre wave devices;radar signal processing;ultra wideband technology;direction of arrival estimation;road vehicles;ultra wide band	In the coming years, it is expected that long and short range road vehicle radars (LRR, SRR) will all operate at millimeter wave. In several regions of the world, a one GHz bandwidth was selected between 76 and 77 GHz for LRR. To obtain a very high distance resolution for applications like brakeassist and pre-crash, SRR need an even larger bandwidth. Hence, a 4 GHz bandwidth was allocated to provide precise radial range information of objects with a range separation of approximately 5 cm to 10 cm. Directions of the targets are obtained using two or more separate sensors associated to localization techniques. The purpose of this paper concerns a simulation and experimental evaluation of an impulse radio, ultra wide band (IR-UWB) vehicle localization system. In this context, different localization techniques (TDOA, TOA, DOA) will be evaluated in terms of position error for scenarios using three distant, separate sensors. Our objective is to evaluate an optimum technique as a function of the localization range. Simulation results will be compared to experimental results obtained in laboratory. These results will show the interest of coupling two of the following TOA, TDOA and DOA techniques, at different ranges. Keywords-Radar; Ultra Wide Band; Pulse; Ranging; Localization; Vehicle	ambiguous name resolution;direction of arrival;front and back ends;internationalization and localization;linear algebra;multilateration;radar;radial (radio);simpl;sensor;simulation;software propagation;time of arrival;ultra-wideband	Nizar Obeid;Marc Heddebaut;Fouzia Elbahhar;Christophe Loyez;Nathalie Rolland	2009	VTC Spring 2009 - IEEE 69th Vehicular Technology Conference	10.1109/VETECS.2009.5073669	additive white gaussian noise;estimation;electronic engineering;telecommunications;sensor;ultra-wideband;mathematics;accuracy and precision;extremely high frequency;local system;bandwidth;statistics	Robotics	49.37631834090791	2.1782678969456675	12456
30caa21b7581170159e4184da2e62c19333c6ae3	power system bidding tournaments for a deregulated environment	anasynchronous bidding scheme;centrally dispatched power pool;power system control;iso bidding model;asynchronous bidding scheme;power transmission;asingle bid;feedback mechanism;electricity supply industry;power system;iso model;profit margins;power system security;independent system operator;iso bidding models;competitive market;independent generator;interruptible loads;independent generators;spot market;bidding process;transmission control;centrally-dispatched power pool;power system bidding tournaments;deregulated environment;iso;power generation;profitability;power industry;power systems;feedback	We describe certain tools for understanding and operating power systems in a deregulated environment. Many of the current models for this competitive market that employ an independent system operator (ISO) for controlling transmission, ensuring fair access and security, and providing a spot market for power are studied. This centrally dispatched power pool also ensures that generation meets demand based on bids submitted daily from independent generators (and from customers offering interruptible loads). Currently, most ISO bidding models allow only a single bid per day. We present an asynchronous bidding scheme as a possible alternative. In particular, we examine the effects of including a feedback mechanism such that upon receiving generation levels from the ISO, independent generators (IGs) be allowed to modify their bid if they so desire. This competitive or `sequential' bidding process should be allowed to take place each day for a predetermined period of time; in this way, IGs will have a chance to compete and hopefully optimize their profit margins. The paper also discusses the development tools necessary for examining the effects of different bidding processes on the ISO model and evaluating their capability of driving the market to an efficient state of operation		Eric Sakk;Robert J. Thomas;Ray Zimmerman	1997		10.1109/HICSS.1997.10069	power transmission;operations management;feedback;microeconomics;electric power system;commerce;profitability index	Robotics	-1.0607353044169965	-4.121864244285331	12459
8b5a1fede75258190fb860b2a3084b4eb95887c3	index tracking optimization with cardinality constraint: a performance comparison of genetic algorithms and tabu search heuristics		The aim of this study was to compare the performance of the well-known genetic algorithms and tabu search heuristics with the financial problem of the partial tracking of a stock market index. Although the weights of each stock in a tracking portfolio can be efficiently determined by means of quadratic programming, identifying the appropriate stocks to include in the portfolio is an NP-hard problem which can only be addressed by heuristics. Seven real-world indexes were used to compare the above techniques, and results were obtained for different tracking portfolio cardinalities. The results show that tabu search performs more efficiently with both real and artificial indexes. In general, the tracking portfolios obtained performed well in both in-sample and out-of-sample periods, so that these heuristics can be considered as appropriate solutions to the problem of tracking an index by means of a small subset of stocks.	atari portfolio;cardinality (data modeling);central processing unit;computation;genetic algorithm;heuristic (computer science);mpeg transport stream;mathematical optimization;mean squared error;np-hardness;quadratic programming;software release life cycle;tabu search;whole earth 'lectronic link	Fernando García;Francisco Guijarro;Javier Oliver	2017	Neural Computing and Applications	10.1007/s00521-017-2882-2	mathematical optimization;machine learning;mathematics	AI	21.793165156765763	0.27590762635250143	12473
775a4a9032bb5f1fb418517b7d9d33f2a21e6a1e	recovery from incorrect knowledge in soar	incorrect knowledge	Incorrect knowledge can be a problem for any intelligent system. Soar is a proposal for the underlying architecture that supports intelligence. It has a single representation of long-term memory and a single learning mechanism called chunking. This paper investigates the problem of recovery from incorrect knowledge in Soar. Recovery is problematic in Soar because of the simplicity of chunking: it does not modify existing productions, nor does it analyze the long-term memory during learning. In spite of these limitations, we demonstrate a domain-independent approach to recovery from incorrect control knowledge and present extensions to this approach for recovering from all types of incorrect knowledge. The key idea is to correct decisions instead of long-term knowledge. Soar’s architecture allows this corrections to occur in parallel with normal processing. This approach does not require any changes to the Soar architecture and because of Soar’s uniform representations for tasks and knowledge, this approach can be used for all tasks and subtasks in Soar.	artificial intelligence;shallow parsing;soar (cognitive architecture)	John E. Laird	1988			computer science;artificial intelligence;machine learning;algorithm	AI	5.535060461939807	-31.004138382913595	12493
d564a067d83338836b2d393e3e53a8dce9661db9	indistinguishable bandits dueling with decoys on a poset		We adress the problem of dueling bandits defined on partially ordered sets, or posets. In this setting, arms may not be comparable, and there may be several (incomparable) optimal arms. We propose an algorithm, UnchainedBandits, that efficiently finds the set of optimal arms of any poset even when pairs of comparable arms cannot be distinguished from pairs of incomparable arms, with a set of minimal assumptions. This algorithm relies on the concept of decoys, which stems from social psychology. For the easier case where the incomparability information may be accessible, we propose a second algorithm, SlicingBandits, which takes advantage of this information and achieves a very significant gain of performance compared to UnchainedBandits. We provide theoretical guarantees and experimental evaluation for both algorithms.	algorithm;coat of arms	Julien Audiffren;Liva Ralaivola	2016	CoRR		artificial intelligence;theoretical computer science;mathematics;algorithm	ML	-3.115078068131565	1.4662216832173356	12495
9a57f0233a616bab84fc35101637765510b1ed83	how good is my prediction? finding a similarity measure for trajectory prediction evaluation		The reliable prediction of traffic participants' trajectories is an important challenge for automated driving. Prediction methods that try to deal with this challenge need similarity measures for trajectories in order to evaluate the quality of their prediction. Currently there exists no commonly accepted similarity measure suitable for this task. In this paper we review common trajectory similarity measures and analyze them with regard to prediction evaluation. Further we introduce a new approach for synthesizing a hybrid measure that combines a set of similarity measures and provide a heuristic to determine the parameters for this approach.	autonomous car;heuristic;plausibility structure;similarity measure;weight function	Jannik Quehl;Haohao Hu;Ömer Sahin Tas;Eike Rehder;Martin Lauer	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317825	computer vision;artificial intelligence;similarity measure;machine learning;trajectory;engineering;heuristic	Robotics	10.604740709960774	-12.219438764879449	12502
ea6090cec68180742c301791e01b981e4b1b6c71	a novel block matching algorithmic approach with smaller block size for motion vector estimation in video compression	data compression;image matching;fss block matching algorithmic approach block size motion vector estimation video compression process computational complexity exhaustive search algorithm output quality new three step search ntss four step search;motion estimation;video coding;vectors algorithm design and analysis motion estimation prediction algorithms diamonds estimation video compression;computational complexity;search problems;video coding computational complexity data compression image matching motion estimation search problems;block matching algorithm fast motion vector estimation motion vector estimation	The most computationally expensive operation in entire video compression process is Motion Estimation. The challenge is to reduce the computational complexity and time of Exhaustive Search Algorithm without losing too much quality at the output. The proposed work is to implement a novel block matching algorithm for Motion Vector Estimation which performs better than other conventional Block Matching Algorithms such as Three Step Search (TSS), New Three Step Search (NTSS), and Four Step Search (FSS) etc.	analysis of algorithms;block size (cryptography);block-matching algorithm;computational complexity theory;data compression;flying-spot scanner;motion estimation;search algorithm	Suvojit Acharjee;Nilanjan Dey;Debalina Biswas;Poulami Das;Sheli Sinha Chaudhuri	2012	2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)	10.1109/ISDA.2012.6416617	data compression;mathematical optimization;quarter-pixel motion;computer science;theoretical computer science;machine learning;motion estimation;mathematics;block-matching algorithm;rate–distortion optimization;computational complexity theory;motion compensation	Robotics	48.49153397781891	-19.274302861300356	12503
841a6e1f74155f1fcb833ed9233fdeb6acd8a085	integration of knowledge-based systems and neural networks: neuro-expert petri net models and applications	knowledge based systems neural networks object oriented modeling petri nets fuzzy logic artificial neural networks artificial intelligence fuzzy neural networks neurons hybrid intelligent systems;fuzzy neural network;fuzzy neural nets;knowledge based system;fuzzy reasoning;neural networks;hybrid intelligent systems;learning;building block;inference mechanisms;intelligent control;fuzzy logic;artificial neural networks;artificial intelligence;intelligent control knowledge based systems fuzzy neural nets petri nets knowledge representation fuzzy logic inference mechanisms learning artificial intelligence;neurons;petri nets;learning artificial intelligence;knowledge representation;fuzzy neural networks;petri net;intelligent control knowledge based systems neuro expert petri net fuzzy logic fuzzy neural networks knowledge representation fuzzy reasoning learning;object oriented modeling;knowledge based systems;neuro expert petri net;artificial neural network;neural network;knowledge base	There is n o w a growing realization iii the intelligent s y s t ems c o m m u n i t y tha t m a n y complex problems require hybrid solutions. This paper identif ies and describes how knowledge based sy s t ems (KBSs), fu z zy logic (FL) and artificial neural ne tworks (ANNs) can be integrated, and provides a novel ( fuzzy) expert Pe t r i n e t ( E P N , F E P N ) based approach f o r the integration of KBSs and ANNs. A generic expert Pe tr i n e t model of single neuron is presented, and a two-layer generic Pe t r i n e t model f o r ( fuzzy) neural ne tworks neural ( fuzzy) expert Pe tr i ne t s (NEPN, N F E P N ) that use this neuron model a s a building block is described. T h e N E P N and N F E P N models can be used for representing a ( fuzzy) knowledge base a.nd ( fuzzy) reasoning. A n d also, they can be utilized to develop ANNl i ke multilayered Pe t r i n e t architectures of distributed hybrid intelligence having learning abili t y .Some application examples are illustrated for validat i o n of the proposed models.	artificial neural network;biological neuron model;chasys draw ies;execution unit;knowledge base;knowledge-based systems;ne (complexity);neural network software;petri net	Xuan F. Zha;Samuel Y. E. Lim;Sai Cheong Fok	1998		10.1109/ROBOT.1998.677304	computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;petri net;artificial neural network	NLP	5.889157793520402	-28.39608093055489	12518
034dc3d6681a6e43cab1a604a80965dd79ce1be6	simultaneous optimization of anfis-based fuzzy model driven to data granulation and parallel genetic algorithms	parallel genetic algorithm;comparative analysis;least square method;parameter identification;fuzzy inference system;fuzzy model	The paper concerns the simultaneous optimization for structure and parameters of fuzzy inference systems that is based on Hierarchical Fair Competition-based Parallel Genetic Algorithms (HFCGA) and information data granulation. HFCGA is used to optimize structure and parameters of ANFIS-based fuzzy model simultaneously. The granulation is realized with the aid of the C-means clustering. Through the simultaneous optimization mechanism to be explored, we can find the overall optimal values related to structure as well as parameter identification of ANFIS-based fuzzy model via HFCGA, C-Means clustering and standard least square method. A comparative analysis demon-strates that the proposed algorithm is superior to the conventional methods.	adaptive neuro fuzzy inference system;genetic algorithm	Jeoung-Nae Choi;Sung-Kwun Oh;Kisung Seo	2007		10.1007/978-3-540-72395-0_29	qualitative comparative analysis;mathematical optimization;defuzzification;adaptive neuro fuzzy inference system;neuro-fuzzy;machine learning;data mining;mathematics;least squares	HPC	6.509197200341584	-25.990918263593347	12521
4eb6bd6dbc6fd8cea4595f8aefdc27aa590ae3ff	human action recognition in table-top scenarios: an hmm-based analysis to optimize the performance	hidden markov model;motion capture data;computer vision;abstract hidden markov model;action recognition;pattern recognition;optimization	Hidden Markov models have been extensively and successfully used for the recognition of human actions. Though there exist well-established algorithms to optimize the transition and output probabilities, the type of features to use and specifically the number of states and Gaussian have to be chosen manually. Here we present a quantitative study on selecting the optimal feature set for recognition of simple object manipulation actions pointing, rotating and grasping in a table-top scenario. This study has resulted in recognition rate higher than 90%. Also three different parameters, namely the number of states and Gaussian for HMM and the number of training iterations, are considered for optimization of the recognition rate with 5 different feature sets on our motion capture data set from 10 persons.		Pradeep Reddy Raamana;Daniel Grest;Volker Krüger	2007		10.1007/978-3-540-74272-2_13	computer vision;speech recognition;feature;computer science;machine learning;pattern recognition;3d single-object recognition;hidden markov model;signature recognition	Vision	36.117998055550984	-48.68757489555996	12531
e52521cbd83a34aea6629a49e66a4000bb55f492	kernel online learning algorithm with scale adaptation		Kernel adaptive filtering is implemented by evaluating the inner product between the kernel function-based vector and the coefficient vector. In this brief, the coefficient vector is decomposed into the direction vector and the scale, which are updated using the steepest descent method and thus generate a novel online learning method, namely kernel online learning algorithm with scale adaptation (KOL-SA). In addition, the convergence of KOL-SA is proved and an upper bound of steady-state mean square error is therefore obtained. Simulation results confirm that the proposed KOL-SA achieves desirable filtering performance from the aspects of the filtering accuracy and stability.	adaptive filter;algorithm;coefficient;gradient descent;kernel (operating system);mean squared error;overfitting;simulation;steady state	Shiyuan Wang;Lujuan Dang;Badong Chen;Chengxiu Ling;Lidan Wang;Shukai Duan	2018	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2017.2765523	direction vector;kernel (statistics);kernel (linear algebra);online machine learning;population-based incremental learning;method of steepest descent;filter (signal processing);algorithm;mathematics;artificial intelligence;pattern recognition;adaptive filter	ML	23.335512682886225	-35.906313271914726	12554
aa6f4940cda2e8351f60cfa8993bcf4db2fa5cce	an rfid-based inventory management framework for efficient emergency relief operations	tracking systems;inventory management;telecommunication network planning;resource allocation;vital commodity consumption rfid based inventory management framework efficient emergency relief operations harm reduction emergency disaster management emergency shelters superdome shelter new orleans whybark stored vital commodity management monitoring systems humanitarian emergency inventory management system emergency supply real time tracking radiofrequency identification devices offline planning strategy online control techniques transportation system;disaster preparedness;disasters and emergency operations;vectors;transportation;safety;inventory management transportation radiofrequency identification planning inventory control safety vectors;radio frequency identification;planning;new orleans louisiana;crisis management;inventory control;telecommunication network planning disasters radiofrequency identification telecommunication network management;radiofrequency identification;disasters;telecommunication network management	Emergency disaster management has emerged as a vital tool to reduce the harm and alleviate the suffering the disasters can cause to their victims. A significant task of planners involved in the emergency disaster management is the ability to plan for and satisfy the vital needs of the people located in the emergency shelters, such as the Superdome shelter at New Orleans. As clearly stated by Whybark [1], the management of the stored vital commodities can be improved within the emergency disaster management if there were more effective tracking and monitoring systems. This study proposes a comprehensive framework for the development of a humanitarian emergency inventory management system based on the real-time tracking of emergency supplies and demands through the integration of emerging technologies such as Radio Frequency Identification Devices (RFID). The proposed approach combines an offline planning strategy with online control techniques in a unified framework that is robust with respect to disruptions to the transportation system and unexpected peaks in consumption of vital commodities in the aftermath of a disaster.	broadcast delay;convergence insufficiency;hp superdome;half-life 2: episode one;inventory control;online and offline;planning;radio frequency;radio-frequency identification;real life;real-time clock;real-time transcription;safety stock;unified framework	Eren Erman Ozguven;Kaan Ozbay	2012	2012 15th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2012.6338812	simulation;emergency;engineering;operations management;computer security	Robotics	12.720167045999982	-7.568868363333201	12569
6f6ef7ad0c034e805f0aaa7bb2c1b15a79005566	clustering generalised instances set approaches for text classification	k means;text classification;clustering method;k nearest neighbour;k means clustering;generalised instances set	This paper introduces three new text classi ̄cation methods: Clustering-Based Generalised Instances Set (CB-GIS), Multilevel Clustering-Based Generalised Instances Set (MLC GIS) and Multilevel Clustering-Based k Nearest Neighbours (MLC-kNN). These new methods aim to unify the strengths and overcome the drawbacks of the three similaritybased text classi ̄cation methods, namely, kNN, centroid-based and GIS. The new methods utilise a clustering technique called spherical K-means to represent each class by a representative set of generalised instances to be used later in the classi ̄cation. The CB-GIS method applies a °at clustering method while MLC-GIS and MLC-kNN apply multilevel clustering. Extensive experiments have been conducted to evaluate the new methods and compare them with kNN, centroid-based and GIS classi ̄ers on the Reuters-21578(10) benchmark dataset. The evaluation has been performed in terms of the classi ̄cation performance and the classi ̄cation e±ciency. The experimental results show that the top-performing classi ̄cation method is the MLC-kNN classi ̄er, followed by the MLC-GIS and CB-GIS classi ̄ers. According to the best micro-averaged F1 scores, the new methods (CB-GIS, MLC-CIS, MLC-kNN) have improvements of 4.48%, 4.65% and 4.76% over kNN, 1.84%, 1.92% and 2.12% over the centroidbased and 5.26%, 5.34% and 5.45% over GIS respectively. With respect to the best macro-averaged F1 scores, the new methods (CB-GIS, MLC-CIS, MLC-kNN) have improvements of 10.29%, 10.19% and 10.45% over kNN, respectively, 0.1%, 0.03% and 0.29% over the centroid-based and 3.75%, 3.68% and 3.94% over GIS respectively.	benchmark (computing);cluster analysis;document classification;emoticon;experiment;geographic information system;k-means clustering;k-nearest neighbors algorithm;multi-level cell;nl (complexity);numerical aperture;preprocessor;test set	Hassan Najadat;Rasha Obeidat;Ismail Hmeidi	2011	JIKM	10.1142/S0219649211002857	computer science;machine learning;pattern recognition;data mining;k-means clustering	Web+IR	8.08732975104071	-41.535343993269805	12577
4d34797cf95e35f5596cc3ee6b3cce9a7c74e702	sample size calculation for comparing time-averaged responses in k-group repeated-measurement studies	biological patents;sample size;biomedical journals;text mining;europe pubmed central;citation search;citation networks;generalized estimating equation gee;research articles;abstracts;open access;longitudinal outcome;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Many clinical trials compare the efficacy of K (≥3) treatments in repeated measurement studies. However, the design of such trials have received relatively less attention from researchers. Zhang & Ahn (2012) derived a closed-form sample size formula for two-sample comparisons of time-averaged responses using the generalized estimating equation (GEE) approach, which takes into account different correlation structures and missing data patterns. In this paper, we extend the sample size formula to scenarios where K (≥3) treatments are compared simultaneously to detect time-averaged differences in treatment effect. A closed-form sample size formula based on the noncentral χ(2) test statistic is derived. We conduct simulation studies to assess the performance of the proposed sample size formula under various correlation structures from a damped exponential family, random and monotone missing patterns, and different observation probabilities. Simulation studies show that empirical powers and type I errors are close to their nominal levels. The proposed sample size formula is illustrated using a real clinical trial example.	epilepsy, generalized;estimated;matthews correlation coefficient;missing data;power (psychology);probability;simulation;exponential;monotone	Song Zhang;Chul Ahn	2013	Computational statistics & data analysis	10.1016/j.csda.2012.08.013	sample size determination;econometrics;text mining;computer science;data mining;mathematics;statistics	AI	29.928307552195403	-21.701047555819905	12594
a86036e13e344264a12966b353ee44402cdd280a	online multi-kernel learning with orthogonal random features		Kernel-based methods have well-appreciated performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. To cope with this limitation, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops an online multi-kernel learning scheme to infer the intended nonlinear function ‘on the fly.’ Performance analysis shows that the novel algorithm can afford sublinear regret. Numerical tests on real datasets are carried out to showcase the effectiveness of the proposed algorithms.	algorithm;approximation;dictionary;kernel (operating system);nonlinear system;numerical method;regret (decision theory)	Yanning Shen;Tianyi Chen;Georgios B. Giannakis	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461509	mathematical optimization;task analysis;kernel (linear algebra);approximation algorithm;function approximation;regret;sublinear function;nonlinear system;popularity;computer science	Robotics	25.095176429263027	-31.835290921808397	12596
ae99540d16be0e224f674acfa29a0d4ad7e8c1b5	hierarchical classification for solving multi-class problems: a new approach using naive bayesian classification	liverpool;repository;university	A hierarchical classification ensemble methodology is proposed as a solution to the multi-class classification problem where the output from a collection of classifiers, arranged in a hierarchical manner, are combined to produce a better composite global classification (better than when the classifiers making up the ensemble operate in isolation). A novel topology for arranging the classifiers in the hierarchy is proposed such that the leaf classifiers act as binary classifiers and the remaining classifiers (those at the root and intermediate nodes) address groupings of classes. The main challenge is how to address the general drawback of the hierarchical model, that is if a record is miss-classified early on in the classification process (near the root of the hierarchy) it will continue to be miss-classified at deeper levels too. Three different approaches, founded on Naive Bayes classification, are proposed whereby Bayesian probability values are used to indicate whether single or multiple paths should be followed within the hierarchy. Reported experimental results demonstrate that the proposed mechanism can improve classification performance, in terms of average AUC, in the context of selected data sets.	bayesian network;binary classification;hierarchical database model;multiclass classification;naive bayes classifier	Esra'a Alshdaifat;Frans Coenen;Keith Dures	2013		10.1007/978-3-642-53914-5_42	computer science;data science;machine learning;data mining	ML	15.0892091177496	-39.99093998432946	12614
7c4f3a70127d42ad0a901cab58b2b6cd0358e3c8	multi-view unit intact space learning		Multi-view learning is a hot research topic in different research fields. Recently, a model termed multi-view intact space learning has been proposed and drawn a large amount of attention. The model aims to find the latent intact representation of data by integrating information from different views. However, the model has two obvious shortcomings. One is that the model needs to tune two regularization parameters. The other is that the optimization algorithm is too timeconsuming. Based on the unit intact space assumption, we propose an improved model, termed multi-view unit intact space learning, without introducing any prior parameters. Besides, an efficient algorithm based on proximal gradient scheme is designed to solve the model. Extensive experiments have been conducted on four real-world datasets to show the effectiveness of our method.	algorithm;experiment;mathematical optimization;proximal gradient method;proximal gradient methods for learning;time complexity	Kun-Yu Lin;Chang-Dong Wang;Yu-Qin Meng;Zhi-Lin Zhao	2017		10.1007/978-3-319-63558-3_18	artificial intelligence;machine learning;cluster analysis;computer science;regularization (mathematics)	AI	24.492136604906673	-42.39327642797301	12636
a92aa15fc9a397210c2575961ea2532eb574f51a	solving large scale crew scheduling problems by using iterative partitioning	partitioning methods;large scale;crew scheduling;scheduling problem;large scale optimization	This paper deals with large-scale crew scheduling problems arising at the Dutch railway operator, Netherlands Railways (NS). NS operates about 30,000 trains a week. All these trains need a driver and a certain number of conductors. No available crew scheduling algorithm can solve such huge instances at once. A common approach to deal with these huge weekly instances, is to split them into several daily instances and solve those separately. However, we found out that this can be rather inefficient. In this paper, we discuss several methods to partition huge instances into several smaller ones. These smaller instances are then solved with the commercially available crew scheduling algorithm TURNI. We compare these partitioning methods with each other, and we report several results where we applied different partitioning methods after each other. The results show that all methods significantly improve the solution. With the best approach, we were able to cut down crew costs with about 2% (about 6 million euro per year).	algorithm;crew scheduling;iterative method;scenario analysis;schedule;scheduling (computing)	Erwin J. W. Abbink;Joel Van't Wout;Dennis Huisman	2007			mathematical optimization;simulation;computer science;operations management	AI	15.011677454345762	2.679473589980215	12638
5a021bb28e8c62a8c21fffa1ff35929ef2edce8d	trajectory aligned features for first person action recognition	egocentric vision;video segmentation;video indexing and analysis;action and activity recognition	Egocentric videos are characterized by their ability to have the first person view. With the popularity of Google Glass and GoPro, use of egocentric videos is on the rise. With the substantial increase in the number of egocentric videos, the value and utility of recognizing actions of the wearer in such videos has also thus increased. Unstructured movement of the camera due to natural head motion of the wearer causes sharp changes in the visual field of the egocentric camera causing many standard third person action recognition techniques to perform poorly on such videos. Objects present in the scene and hand gestures of the wearer are the most important cues for first person action recognition but are difficult to segment and recognize in an egocentric video. We propose a novel representation of the first person actions derived from feature trajectories. The features are simple to compute using standard point tracking and do not assume segmentation of hand/objects or recognizing object or hand pose unlike in many previous approaches. We train a bag of words classifier with the proposed features and report a performance improvement of more than 11% on publicly available datasets. Although not designed for the particular case, we show that our technique can also recognize wearer's actions when hands or objects are not visible. & 2016 Elsevier Ltd. All rights reserved.	bag-of-words model;baseline (configuration management);experiment;feature vector;first-person (video games);glass;homography (computer vision);shake;virtual camera system	Suriya Singh;Chetan Arora;C. V. Jawahar	2017	Pattern Recognition	10.1016/j.patcog.2016.07.031	computer vision;multimedia	Vision	33.95780616859025	-49.90662011812858	12653
fb74048fc54a32a52ba281b3229c0f5b15e3017d	hierarchical mcmc sampling	mcmc methods;long range interaction;porous media;computational complexity;energy minimization;random field	We maintain that the analysis and synthesis of random fields is much faster in a hierarchical setting. In particular, complicated longrange interactions at a fine scale become progressively more local (and therefore more efficient) at coarser levels. The key to effective coarsescale activity is the proper model definition at those scales. This can be difficult for locally-coupled models such as Ising, but is inherent and easy for those models, commonly used in porous media, which express constraints in terms of lengths and areas. Whereas past methods, using hierarchical random fields for image estimation and segmentation, saw only limited improvements, we find reductions in computational complexity of two or more orders of magnitude, enabling the investigation of models at much greater sizes and resolutions.	computational complexity theory;interaction;ising model;markov chain monte carlo	Paul W. Fieguth	2004		10.1007/978-3-540-30125-7_15	econometrics;mathematical optimization;random field;mathematics;computational complexity theory;energy minimization;porous medium;statistics	AI	38.629945588174294	-22.685695731086817	12672
25c9f33aceac6dcff357727cbe2faf145b01d13c	keeping the neural networks simple by minimizing the description length of the weights	gaussian noise;neural network	Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of intereating schemes for encoding the weights.	monte carlo method;neural networks;noise (electronics);nonlinear system;simulation	Geoffrey E. Hinton;Drew van Camp	1993		10.1145/168304.168306	stochastic neural network;gaussian noise;computer science;artificial intelligence;theoretical computer science;machine learning;time delay neural network;artificial neural network	ML	17.540365439153398	-30.626889003623777	12673
281008ce1315fe8465555cffc352e86748e9190e	correlation filter-based visual tracking via adaptive weighted cnn features fusion		Visual object tracking is an important and challenging task in computer vision. In this study, the authors propose a novel visual tracking approach by decomposing the tracking task into translation and scale estimation. In translation estimation, they employ multiple adaptive correlation filters with features of hierarchical convolutional neural networks (CNNs) to more accurately estimate the target location. To make full use of multi-level features from different CNN layers, they propose an adaptive weighted algorithm to fuse correlation response maps. In scale estimation, a one-dimensional correlation filter with histogram of oriented gradient (HOG) features is employed to estimate the scale variation. Extensive experimental results on 50 challenging benchmark video sequences demonstrate that the proposed algorithm outperforms state-of-the-art algorithms.	video tracking	Zhaohui Hao;Guixi Liu;Haoyang Zhang	2018	IET Image Processing	10.1049/iet-ipr.2017.0443	artificial intelligence;convolutional neural network;computer vision;fuse (electrical);fusion;video tracking;mathematics;histogram;pattern recognition;eye tracking;correlation	Vision	34.87968712739748	-49.833846052420796	12676
d86d294b11a00d0daa5f6aa8a8bed49b03ab1c5a	data embedding in digital images using critical functions		In this paper, ‘‘uniform embedding’’ (independent of image contents and pixel correlations while embedding) and ‘‘adaptive embedding’’ (depend on image contents and pixel correlations while embedding) in image steganography are investigated. A compact steganographic embedding function is proposed to ensure the correctness and efficiency, and a pixel correlation function is utilized to discriminate the image smoothness. Two feasible image steganographic frameworks using these critical functions are presented, and some wellknown image steganographic methods can be derived from the proposed frameworks. The effectiveness of the proposed frameworks is experimentally validated by constructing and testing some special data hiding methods in the case of four neighboring pixel as a processing unit. Experimental results show that the proposed methods can achieve better visual performance and statistical undetectability compared with the prior works. Another promising merit of our work is the potential to provide steganographers general-purpose strategies to acquire new image steganographic methods. © 2017 Elsevier B.V. All rights reserved.	correctness (computer science);digital image;experiment;general-purpose markup language;pixel;statistical model;steganography	Xin Liao;Zheng Qin;Liping Ding	2017	Sig. Proc.: Image Comm.	10.1016/j.image.2017.07.006	steganography;information hiding;artificial intelligence;theoretical computer science;computer vision;pixel;digital image;smoothness;correctness;computer science;embedding;correlation function	ML	40.13620708066739	-11.378063865826018	12697
531c35ee1b0ec04ac8745dbefbd63340b61082da	credit scoring model based on neural network with particle swarm optimization	modelizacion;banking;swarm intelligence;modeling technique;credit scoring;procesamiento informacion;scoring credit;intelligence en essaim;feed forward neural network;redundancia;gestion red;secteur bancaire;credit;calcul analogique;modelisation;particle swarm optimizer;redundancy;credito;information processing;gestion reseau;valuacion credito;pattern classification;network management;reseau neuronal;traitement information;modeling;inteligencia de enjambre;red neuronal;redondance;methode score;neural network;analog calculus;calculo analogico;classification forme	Credit scoring has gained more and more attentions both in academic world and the business community today. Many modeling techniques have been developed to tackle the credit scoring tasks. This paper presents a Structuretuning Particle Swarm Optimization (SPSO) approach for training feed-forward neural networks (NNs). The algorithm is successfully applied to a real credit problem. By simultaneously tuning the structure and connection weights of NNs, the proposed algorithm generates optimized NNs with problem-matched information processing capacity and it also eliminates some ill effects introduced by redundant input features and the corresponding redundant structure. Compared with BP and GA, SPSO can improve the pattern classification accuracy of NNs while speeding up the convergence of training process.	algorithm;artificial neural network;feedforward neural network;information processing;particle swarm optimization;software release life cycle	Liang Gao;Chi Zhou;Hai-Bing Gao;Yong-Ren Shi	2006		10.1007/11881070_11	network management;feedforward neural network;systems modeling;information processing;swarm intelligence;computer science;artificial intelligence;machine learning;data mining;redundancy;artificial neural network	ML	10.264462942119627	-30.584887019153477	12711
d80875d496a97c9de9a919ff48728d6177a8ec32	maximum variance sparse mapping	manifold learning;objective function;optimization problem;sub manifold;mvsm;sparse representation	In this paper, a multiple sub-manifold learning method oriented classification is presented via sparse representation, which is named maximum variance sparse mapping. Based on the assumption that data with the same label locate on a sub-manifold and different class data reside in the corresponding sub-manifolds, the proposed algorithm can construct an objective function which aims to project the original data into a subspace with maximum sub-manifold distance and minimum manifold locality. Moreover, instead of setting the weights between any two points directly or obtaining those by a square optimal problem, the optimal weights in this new algorithm can be approached using L1 minimization. The proposed algorithm is efficient, which can be validated by experiments on some benchmark databases.		Bo Li;Jin Liu;Wenyong Dong	2011		10.1007/978-3-642-21090-7_1	optimization problem;mathematical optimization;computer science;machine learning;pattern recognition;sparse approximation;mathematics;nonlinear dimensionality reduction;manifold alignment	AI	26.416465685834467	-40.25816666572896	12713
a322df5ede6faa2b2baf54e706245e8dffa4b0fb	an improved particle swarm optimization algorithm	analisis numerico;matematicas aplicadas;mathematiques appliquees;65kxx;optimum global;optimization method;global optimum;metodo optimizacion;analyse numerique;information sharing;algorithme;49xx;algorithm;improved particle swarm optimization;numerical analysis;particle swarm optimizer;particion;hydrologic model;mathematical programming;particle swarm optimization;partition;methode optimisation;global optimization;parameters calibration;applied mathematics;particle swarm optimization algorithm;programmation mathematique;optimo global;programacion matematica;algoritmo	Because the variable inertia weight particle swarm optimization algorithm is easy to fall into the local optimum, this paper introduces the improved simulated annealing operator, chaotic disturbance operator and Cauchy mutation operator to the former and proposes an improved particle swarm optimization algorithm; Then, two typical Benchmark functions are used to test the performance of basic the proposed algorithm; Finally, the relations of population size and particle dimension to performance of the proposed algorithm is analyzed. Simulation results show that while maintains the superiorities of simple structure, few parameters and the ease of implement, the proposed algorithm improves the convergence precision largely.	benchmark (computing);genetic algorithm;ipso alliance;local optimum;mathematical optimization;particle swarm optimization;phase-shift oscillator;simulated annealing;simulation;software release life cycle	Yan Jiang;Tiesong Hu;Chongchao Huang;Xianing Wu	2007	2011 International Conference on Electronics, Communications and Control (ICECC)	10.1016/j.amc.2007.03.047	partition;mathematical optimization;multi-swarm optimization;numerical analysis;artificial intelligence;mathematics;global optimum;particle swarm optimization;algorithm;global optimization	Robotics	27.729827646425203	0.09071341681938709	12718
10ec30f3b0e1f15fed267cf16bfb2d7c04e911ea	fuzzy regression model with interval-valued fuzzy input-output data	goodness of fit;least squares approximations;fuzzy regression;regression analysis data handling fuzzy set theory least squares approximations nonlinear programming;least squares method;nonlinear programming;goodness of fit interval valued fuzzy number fuzzy regression least squares method;interval valued fuzzy regression model interval valued fuzzy input output data least squares method nonlinear programming model;fuzzy set theory;interval valued fuzzy number;regression analysis;data handling;soil fuzzy sets data models predictive models fuzzy set theory pollution measurement	A novel approach is introduced to construct a fuzzy regression model when both input data and output data are interval-valued fuzzy numbers. Using a distance on the space of interval-valued fuzzy numbers, a least-squares method is developed. Also, a nonlinear programming model is proposed to estimate the crisp parameters for the interval-valued fuzzy regression model. A real example demonstrates the feasibility and efficiency of the proposed method. Moreover, two goodness of fit indices are introduced and employed for more evaluation of such fuzzy interval-valued regression models.	fuzzy set;least squares;nonlinear programming;nonlinear system;programming model	Mohammad Reza Rabiei;Naser Reza Arghami;S. Mahmoud Taheri;Bahram Sadeghpour Gildeh	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622315	total least squares;mathematical optimization;membership function;defuzzification;adaptive neuro fuzzy inference system;nonlinear programming;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;machine learning;group method of data handling;fuzzy measure theory;mathematics;fuzzy set;goodness of fit;least squares;fuzzy set operations;regression analysis;nonlinear regression;statistics	Robotics	4.020119174623993	-22.80524095724245	12721
2416a023aa5e950c3ade7ea8aa373014c51d9301	sorting-based multiple description quantization	cuantificacion senal;evaluation performance;multiple description;communication system;performance evaluation;redundancia;sorting;internet sorting multidimensional multiple description quantization side information redundancy multiple description scalar quantization packet networks;implementation;packet loss;evaluacion prestacion;quantisation signal;multi dimensional;sorting internet quantisation signal;internet;redundancy;signal quantization;quantification signal;indexation;multiple description coding;flexible structure;estructura flexible;packet networks;structure flexible;implementacion;side information;telekommunikation;redondance;quantization sorting transmitters random variables redundancy flexible structures multidimensional systems ip networks robustness multimedia communication;telecommunications	We introduce a new method for multiple description quantization (MDQ), based on sorting a frame of samples and transmitting, as side-information/redundancy, an index that describes the resulting permutation. The sorting-based approach has a similar performance to multiple description scalar quantization and a flexible structure, providing straightforward implementation of multidimensional MDQ	quantization (signal processing);redundancy (information theory);sorting;transmitter	Niklas Wernersson;Mikael Skoglund	2006	IEEE Transactions on Communications	10.1109/TCOMM.2006.881186	electronic engineering;discrete mathematics;the internet;telecommunications;computer science;sorting;theoretical computer science;multiple description coding;redundancy;packet loss;implementation;communications system	Vision	48.064754144796105	-11.534644985175019	12727
79ebaca41c02d58bf96240269c64705c4e50589f	integrative clustering by nonnegative matrix factorization can reveal coherent functional groups from gene profile data	budding yeast;clustering algorithms informatics data integration matrix decomposition gene expression dh hemts bioinformatics;pattern clustering bioinformatics cellular biophysics data integration genetics matrix decomposition microorganisms molecular biophysics;pattern clustering;gene dataset fusion;joint prediction model;related gene group identification;heterogeneous dataset integration;s cerevisiae;nonnegative matrix factorization nmf;gene cluster development;gene profile data;genome wide data acquisition;gene profile merging;high cluster quality;data fusion;gene function prediction;dh hemts;genetics;gene expression;gene cluster inference;matrix decomposition;clustering;nonnegative matrix factorization;molecular biology;molecular biophysics;gene cluster fusion;coherent functional group;similar gene profile;clustering algorithms;gene profile merging integrative clustering nonnegative matrix factorization coherent functional group gene profile data molecular biology genome wide data acquisition gene function prediction gene dataset fusion joint prediction model related gene group identification similar gene profile profile based clustering gene cluster inference clustering model consensus gene cluster development data source gene cluster fusion budding yeast s cerevisiae heterogeneous dataset integration high cluster quality;informatics;gene set enrichment;integrative clustering;data source;gene profiling;microorganisms;cellular biophysics;clustering model consensus;profile based clustering;data integration;bioinformatics	Recent developments in molecular biology and techniques for genome-wide data acquisition have resulted in abundance of data to profile genes and predict their function. These datasets may come from diverse sources and it is an open question how to commonly address them and fuse them into a joint prediction model. A prevailing technique to identify groups of related genes that exhibit similar profiles is profile-based clustering. Cluster inference may benefit from consensus across different clustering models. In this paper, we propose a technique that develops separate gene clusters from each of available data sources and then fuses them by means of nonnegative matrix factorization. We use gene profile data on the budding yeast S. cerevisiae to demonstrate that this approach can successfully integrate heterogeneous datasets and yield high-quality clusters that could otherwise not be inferred by simply merging the gene profiles prior to clustering.	cluster analysis;coherent;computation;data acquisition;data pre-processing;design of experiments;functional genomics;fuse device component;gene ontology term enrichment;genetic heterogeneity;inference;molecular biology;non-negative matrix factorization;preprocessor;saccharomyces cerevisiae;saccharomycetales;stemming;systems biology;tracer;statistical cluster	Sanja Brdar;Vladimir S. Crnojevic;Blaz Zupan	2015	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2014.2316508	correlation clustering;fuzzy clustering;computer science;bioinformatics;data science;machine learning;data mining;cluster analysis;molecular biophysics	Comp.	5.5626995849912815	-50.68857616511399	12732
5f0a28af607c379af15f41287ee42b085e2301d8	a novel template-based learning model	learning model;feature space;onion;science learning;human visual system;convex hull	This article presents a model which is capable of learning and abstracting new concepts based on comparing observations and finding the resemblance between the observations. In the model presented here, the new observations are compared with the templates which have been derived from the previous experiences. In the first stage, the objects are first represented through a geometric description which is used for finding the object boundaries and a descriptor which is inspired by the human visual system and then they are fed into the model. Next, the new observations are identified through comparing them with the previously-learned templates and are used for producing new templates. The comparisons are made based on measures like Euclidean or correlation distance. The new template is created by applying onion-pealing algorithm. The algorithm consecutively uses convex hulls which are made by the points representing the objects. If the new observation is remarkably similar to one of the observed categories, it is no longer utilized in creating a new template. In order to identify the new observations in each stage, all the previous observations and the learned templates are utilized. The existing templates are used to provide a description of the new observation. This description is provided in the templates space. Each template represents a dimension of the feature space. The degree of the resemblance each template bears to each object indicates the value associated with the object in that dimension of the templates space. In this way, the description of the new observation becomes more accurate and detailed as the time passes and the experiences increase. We have used this model for learning and recognizing the new polygons in the polygon space. Representing the polygons was made possible through employing a geometric method and a method inspired by human visual system. Various implementations of the model have been compared. The evaluation results of the model prove its efficiency in learning and deriving new templates.	algorithm;experience;feature vector;human visual system model	Mohammadreza Abolghasemi-Dahaghani;Farzad Didehvar;Alireza Nowroozi	2011	CoRR		computer vision;feature vector;computer science;artificial intelligence;convex hull;machine learning;mathematics;human visual system model	Vision	23.161767999403875	-41.158772675290855	12750
c508b255e2eee85d20a2fc1b2df27d83a02c845e	applying two-stage neural network based classifiers to the identification of mixture control chart patterns for an spc-epc process		The effective controlling and monitoring of an industrial process through the integration of statistical process control (SPC) and engineering process control (EPC) has been widely addressed in recent years. However, because the mixture types of disturbances are often embedded in underlying processes, mixture control chart patterns (MCCPs) are very difficult for an SPC-EPC process to identify.This can result in problems when attempting to determine the underlying root causes of process faults. Additionally, a large number of categories of disturbances may be present in a process, but typical single-stage classifiers have difficulty in identifying large numbers of categories of disturbances in an SPC-EPC process.Therefore, we propose a two-stage neural network (NN) based scheme to enhance the accurate identification rate (AIR) forMCCPs by performing dimension reduction on disturbance categories. The two-stage scheme includes a combination of aNN, support vectormachine (SVM), andmultivariate adaptive regression splines (MARS). Experimental results reveal that the proposed scheme achieves a satisfactory AIR for identifying MCCPs in an SPC-EPC system.	artificial neural network;computer experiment;dimensionality reduction;electronic product code;embedded system;multivariate adaptive regression splines;random forest;receiver operating characteristic;smoothing spline;software patents under the european patent convention;statistical classification;time complexity	Yuehjen E. Shao;Po-Yu Chang;Chi-Jie Lu	2017	Complexity	10.1155/2017/2323082	support vector machine;machine learning;multivariate adaptive regression splines;artificial neural network;artificial intelligence;statistical process control;dimensionality reduction;engineering design process;control chart;mathematics;pattern recognition	ML	10.388288009908784	-36.85541446021713	12755
72771463c261dbd6387f203a1f9a0ba97b667611	using multi-attribute predicates for mining classification rules	classification;feature extraction;classification rules;knowledge acquisition;learning artificial intelligence knowledge acquisition classification;group identity;learning artificial intelligence;data mining relational databases marketing and sales feature extraction spatial databases costs decision trees association rules transaction databases stock markets;data tuples classification rules group identity multi attribute extraction feature combination phase feature extraction training set	In order to improve the e ciency of deriving classication rules from a large training dataset, we develop in this paper a two-phase method for multi-attribute extraction. A feature that is useful in inferring the group identity of a data tuple is said to have a good inference power to that group identity. Given a large training set of data tuples, the rst phase, referred to as feature extraction phase, is applied to a subset of the training database with the purpose of identifying useful features which have good inference powers to group identities. In the second phase, referred to as feature combination phase, these extracted features are evaluated together and multi-attribute predicates with strong inference powers are identi ed. A technique on using match index of attributes is devised to reduce the processing cost.		Ming-Syan Chen	1998		10.1109/CMPSAC.1998.716745	feature extraction;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;database;collective identity;feature	DB	-0.855591578308451	-32.04989071950119	12775
7478772963cb9ca63219e7c0b3d31cd7a39e616a	a sensor data fusion system based on k-nearest neighbor pattern classification for structural health monitoring applications	sensors;data fusion;active system;machine learning;piezoelectric;damage classification;article	Civil and military structures are susceptible and vulnerable to damage due to the environmental and operational conditions. Therefore, the implementation of technology to provide robust solutions in damage identification (by using signals acquired directly from the structure) is a requirement to reduce operational and maintenance costs. In this sense, the use of sensors permanently attached to the structures has demonstrated a great versatility and benefit since the inspection system can be automated. This automation is carried out with signal processing tasks with the aim of a pattern recognition analysis. This work presents the detailed description of a structural health monitoring (SHM) system based on the use of a piezoelectric (PZT) active system. The SHM system includes: (i) the use of a piezoelectric sensor network to excite the structure and collect the measured dynamic response, in several actuation phases; (ii) data organization; (iii) advanced signal processing techniques to define the feature vectors; and finally; (iv) the nearest neighbor algorithm as a machine learning approach to classify different kinds of damage. A description of the experimental setup, the experimental validation and a discussion of the results from two different structures are included and analyzed.	excite;health occupations;k-nearest neighbors algorithm;magee1 gene;machine learning;pattern recognition;piezoelectricity;preparation;signal processing;single linkage cluster analysis;solutions;super high material cd;sensor (device);spiromustine	Jaime Vitola;Francesc Pozo;Diego Alexander Tibaduiza Burgos;Maribel Anaya Vejar	2017		10.3390/s17020417	structural engineering;embedded system;computer science;engineering;sensor;piezoelectricity;data mining;sensor fusion;forensic engineering;physics	AI	37.38386581332661	-32.98013389279035	12811
51b089355b0c96e5c482e1d18575b0d179a27b41	a comparative study between visibility-based roadmap path planning algorithms	graph theory;motion control;probability;performance evaluation;path planning;robot motion planning visibility based roadmap path planning art gallery based roadmap algorithm visibility based motion planning visibility graph visibility based probabilistic roadmap graph cardinality art gallery theorem;configuration space;robot motion planning;visibility graph performance evaluation robot motion planning art gallery based roadmap visibility based probabilistic roadmap;comparative study;motion control path planning probability graph theory;motion planning;path planning motion planning orbital robotics art robot motion joining processes computer science computational efficiency robustness motion control	The aim of this paper is to evaluate the performance of our proposed art gallery-based roadmap algorithm against well-known and frequently cited visibility-based motion planning algorithms: the visibility graph and the visibility-based probabilistic roadmap. The comparison involves several criteria among which are: the cardinality of the graph, the completeness of the algorithm, coverage and connectivity of the free configuration space (CS/sub free/) and computational cost. Our proposed algorithm is robust and fast as it generally covers the whole CS/sub free/, based on the well-known art gallery theorem. It efficiently seeks to construct a roadmap that contains the smallest possible number of nodes (called guards) as opposed to generating a large number of nodes when compared to other motion planning approaches. The simulation results demonstrate that our proposed algorithm outperforms both algorithms and proves to not only combine the attractive features of both algorithms but also eliminate the drawbacks of each one.	algorithm;algorithmic efficiency;analysis of algorithms;art gallery problem;code coverage;computation;guard (computer science);lvm;motion planning;probabilistic roadmap;simulation;statistical relational learning;visibility graph;visual instruction set;whole earth 'lectronic link	Leena Lulu;Ashraf Elnagar	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545545	computer vision;visibility graph;simulation;any-angle path planning;computer science;artificial intelligence;graph theory;theoretical computer science;motion planning;quantum mechanics	Robotics	52.01508177212295	-24.063902756881888	12817
3dc284bb5227b97e2b44265a54e11bafe7206962	solving the multiple competitive facilities location problem	market share;multiple facilities;simulated annealing;heuristic algorithms;facility location problem;competitive facility location;h social sciences;heuristic algorithm;facility location	In this paper we propose five heuristic procedures for the solution of the multiple competitive facilities location problem. A franchise of several facilities is to be located in a trade area where competing facilities already exist. The objective is to maximize the market share captured by the franchise as a whole. We perform extensive computational tests and conclude that a two-step heuristic procedure combining simulated annealing and an ascent algorithm provides the best solutions.	algorithm;computation;facility location problem;heuristic;simulated annealing;times ascent	Tammy Drezner;Zvi Drezner;Saïd Salhi	2002	European Journal of Operational Research	10.1016/S0377-2217(01)00168-0	mathematical optimization;simulation;computer science;operations management;facility location problem;mathematics;1-center problem	AI	17.537128040819322	3.098946189197933	12823
26fd4db6536749c79e7201ea9f143c7b00e5f80d	accurate localization with respect to moving objects via multiple-body registration	service robots;multiple body registration generalized icp method robot localization mobile manipulation tasks iterative closest point algorithm;computational modeling;three dimensional displays iterative closest point algorithm service robots computational modeling simultaneous localization and mapping;three dimensional displays;simultaneous localization and mapping;mobile robots iterative methods;iterative closest point algorithm	Many mobile manipulation tasks require the robot to be accurately localized with respect to the object where the manipulation has to be executed. These tasks include autonomous docking and positioning as well as pick and place or logistics tasks. State-of-the-art approaches to the problem commonly assume that the environment is static and localize the robot with respect to predetermined locations. In this paper, we present an approach that relaxes the static assumption and enables a robot to accurately localize with respect to a reference object that could be moved in the environment. The core of the paper is an extension of the generalized ICP method to handle multiple rigid bodies that move independently to each others. Experiments in both simulated and real world scenarios show that our approach is able to localize the robot with respect to a moved object with an accuracy of less than one centimeter.	algorithm;autonomous robot;docking (molecular);experiment;internationalization and localization;logistics;mathematical model;mobile manipulator;mobile robot;monte carlo localization;smt placement equipment;simulation	Jörg Röwekämper;Benjamin Suger;Wolfram Burgard;Gian Diego Tipaldi	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353901	computer vision;mathematical optimization;simulation;computer science;artificial intelligence;computational model;iterative closest point;simultaneous localization and mapping	Robotics	53.68194798621293	-40.61695926794853	12825
f9754bda4a0ed0dc36f4d9c6c9f5e898d111c792	a linguistic lattice-valued approach for fuzzy multi-objective decision making	l fuzzy set;science general;incomparable information;aggregation;multi objective decision making;lattice implication algebra;sets	Most of the present methods for multi-objective decision making can only deal with linearly ordered preference information. In this paper, we focus on investigating methods for multi-objective decision making when the preference information set includes incomparable natural language terms. A logical algebraic structure of lattice implication algebra is then applied to represent both comparable and incomparable information simultaneously. We present a model for multi-objective decision making in Which the preference information set is a kind of linguistic-valued lattice implication algebras. And we extend the model to deal with the multi-objective decision making when the preference information set is a generalized linguistic-valued lattice. In these cases, decision makers can supply lattice information on their preference and weights of the individual objectives.		Xiaobing Li;Da Ruan;Jun Liu;Yang Xu	2008	Multiple-Valued Logic and Soft Computing		combinatorics;discrete mathematics;influence diagram;delegation;decision field theory;decision engineering;computer science;mathematics;programming language;weighted sum model;business decision mapping	AI	-2.8751499198730417	-22.14332202804736	12829
b9fa8aa5143f6a3ba87c6f1626d6c62ac1f7547e	a nonconvex variational approach for robust graphical lasso		In recent years, there has been a growing interest in problems in graph estimation and model selection, which all share very similar matrix variational formulations, the most popular one being probably GLASSO. Unfortunately, the standard GLASSO formulation does not take into account noise corrupting the data: this shortcoming leads us to propose a novel criterion, where the regularization function is decoupled in two terms, one acting only on the eigenvalues of the matrix and the other on the matrix elements. Incorporating noise information into the model has the side-effect to make the cost function non-convex. To overcome this difficulty, we adopt a majorization-minimization approach, where at each iteration a convex approximation of the original cost function is minimized via the Douglas-Rachford procedure. The achieved results are very promising w.r.t. classical approaches.	approximation;calculus of variations;iteration;lasso;loss function;model selection;the matrix;variational principle	Alessandro Benfenati;Emilie Chouzenoux;Jean-Christophe Pesquet	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462421	lasso (statistics);mathematical optimization;model selection;eigenvalues and eigenvectors;matrix (mathematics);symmetric matrix;estimation of covariance matrices;matrix similarity;regularization (mathematics);computer science	Robotics	27.157844548065242	-34.924777561374306	12832
30effbf1dd232c3d96782beeafc827723d97fb54	feature-aided tracking of ground vehicles using passive acoustic sensor arrays		Tracking of a moving ground target using acoustic signals obtained from a passive sensor network is a difficult problem as the signals are contaminated by wind noise and are hampered by road conditions, terrain and multipath, etc., and are not deterministic. Multiple target tracking becomes even more challenging, especially when some of the vehicles are light (e.g., wheeled) and some are heavy (e.g., heavy wheeled vehicles like trucks, tracked vehicles like tanks, etc.). In such cases the stronger acoustic signals from the heavy vehicles can mask those from the light vehicles, leading to poor detection of such targets. The full position estimates of emitters (targets), obtained following the association of the DoA angle estimates from multiple sensor arrays at each time scan, are used for target tracking. However, because of the particular challenges encountered in multiple ground vehicle scenarios, this association using kinematic (DoA angle) measurements only is not always reliable and can lead to lost as well as false tracks. In this paper we propose a new feature-augmented static association algorithm where feature augmented DoA angle measurements from multiple sensors are associated to localize targets and obtain composite measurements (position estimates) using a static multidimensional assignment (MDA) framework. We present a novel DoA detection scheme followed by a feature extraction technique designed from and for real data. Dynamic S-D and feature-aided S-D (multidimensional) assignment algorithms are presented to assign composite measurements and feature-augmented composite measurements, respectively, to tracks. The techniques are developed based on real data sets and tested on real data based on a field experiment.	acoustic cryptanalysis;acoustic fingerprint;algorithm;correspondence problem;experiment;feature extraction;google map maker;least-angle regression;mixture model;multipath propagation;passive optical network;sensor web	Vishal Cholapadi Ravindra;Yaakov Bar-Shalom;Thyagaraju Damarla	2010	J. Adv. Inf. Fusion		computer vision;mathematics;machine learning;artificial intelligence	Robotics	50.17181162725051	-33.44932051619023	12839
15941dd3a13133e5a7741e8fcf794e1120016f44	stereovision based 3d hand gesture recognition for pervasive computing applications	soc stereovision 3d hand gesture recognition pervasive computing applications human computer interface 3d stereo imaging techniques template matching segmentation problems template noise skin tone detection simple state machine stereo cameras system on chips;visual perception gesture recognition human computer interaction stereo image processing system on chip ubiquitous computing;human computer interaction;system on chip;stereo image processing;ubiquitous computing;visual perception;cameras gesture recognition three dimensional displays image color analysis face skin pervasive computing;gesture recognition	Hand gesture recognition, being one of the most intuitive means of Human Computer Interface has spawned many applications in the area of pervasive computing devices. This paper presents a solution for hand gesture recognition problem based on 3D stereo imaging techniques employing a low complexity algorithm on a low cost sensor. This method significantly improves upon conventional 2D techniques which are limited in their approach due to various reasons which include but are not limited to occlusion, difficulty in establishing classifiers, segmentation problems using template matching and noise. The innovation here is the use of a hybrid method that combines a simple state machine, skin tone detection and stereo cameras to provide a fast method for detecting simple hand gestures that can be implemented on most low cost System on Chips (SoC). The experimental results from the test setup are promising.	algorithm;depth map;finite-state machine;gesture recognition;human computer;human–computer interaction;sensor;stereo cameras;stereopsis;system on a chip;template matching;ubiquitous computing;vii	H Shenoy VikramShenoy;Pankaj Bongale;Vineet Roy;David S. Sumam	2013	2013 9th International Conference on Information, Communications & Signal Processing	10.1109/ICICS.2013.6782787	system on a chip;stereo cameras;computer vision;visual perception;computer science;gesture recognition;ubiquitous computing;computer graphics (images)	Robotics	47.39917835990352	-42.82824487457813	12854
890fd788f8dfb0144d462dfcf7844798cb25f7eb	quality assessment of individual classifications in machine learning and data mining	extraction information;measuring machine;fiabilidad;reliability;confiance;learning algorithm;confidence level;algorithmique;analisis estadistico;medicion densidad;estimacion densidad;intervalo confianza;density measurement;analisis datos;information extraction;estimation densite;intelligence artificielle;typicalness;algorithme apprentissage;probabilistic approach;data mining;classification;transduction;density estimation;data analysis;confidence interval;confidence;quality assessment;statistical analysis;machine learning;confianza;algorithmics;fouille donnee;algoritmica;machine mesure;enfoque probabilista;approche probabiliste;fiabilite;intervalle confiance;analyse statistique;kernel density estimate;controle qualite;mesure densite;artificial intelligence;analyse donnee;inteligencia artificial;quality control;algoritmo aprendizaje;busca dato;maquina medida;clasificacion;extraccion informacion;control calidad	Although in the past machine learning algorithms have been successfully used in many problems, their serious practical use is affected by the fact that often they cannot produce reliable and unbiased assessments of their predictions' quality. In last few years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we describe typicalness and transductive reliability estimation frameworks and propose a joint approach that compensates the above-mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into a joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense. We perform series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary method as well as with kernel density estimation. We show that the proposed method performs as well as proprietary methods and significantly outperforms density estimation methods.	algorithm;data mining;kernel density estimation;machine learning;problem domain;randomness;transduction (machine learning)	Matjaz Kukar	2005	Knowledge and Information Systems	10.1007/s10115-005-0203-z	econometrics;confidence interval;computer science;artificial intelligence;algorithmics;information extraction;statistics	ML	9.277452210846159	-32.292129702640416	12865
4564c4554057dded3b19ec13d36de06e7175e272	detection of multiple human location and direction by integrating different kinds of sensors	location tracking;success rate;ubiquitous computing;direct detection;experimental evaluation;contextual awareness;individual identification;electromagnetic waves	As ubiquitous services become available, information on human location and direction have so important that many detection methods have been proposed. However, a lot of them generate electromagnetic waves or light rays. They are unsuited for use in detection of children's location because parents are wary of health damage caused by radiated electromagnetic waves. The accuracy of a lot of methods which doesn't generate electromagnetic waves is not always high indoors. In this paper, we propose a detection method of indoor multiple human location and direction by using a stereovision camera and direction detection sensors. With a stereovision camera, it is possible to detect human location with high accuracy, but individuals cannot be identified. Direction detection sensors make it possible to identify an individual wearing them. By integrating these sensing devices based on direction information, detection of location and direction become possible with high accuracy. Moreover, with this method it is possible to identify an individual. We built a prototype and conducted an evaluation experiment. We conducted the experiment in a room divided by a ferroconcrete wall. We conducted a total of 10 experiments, changing the number of subjects. The experimental evaluation result shows the average error of location was 0.11[m] and the average error of direction was 0.39[rad]. The success rate of identifying an individual was 90[%].	experiment;mean squared error;prototype;ray (optics);sensor;stereopsis;the wall street journal	Tsuyoshi Shinno;Kazunori Hashizume;Junichi Tajima;Shigeo Kaneda;Hirohide Haga	2008		10.1145/1389586.1389685	computer vision;electromagnetic radiation;simulation;human–computer interaction;computer science;ubiquitous computing	HCI	42.184991231357834	-38.33866532563141	12867
71bd92aa4b05eacda7c50102ce8cb1b94596a349	a framework of quantum-inspired multi-objective evolutionary algorithms and its convergence condition	stochastic convergence;multi objective evolutionary algorithm;pareto optimal set;quantum computer;multi objective evolutionary algorithms;quantum computing;pareto optimality	A general framework of quantum-inspired multi-objective evolutionary algorithms as well as one of its sufficient convergence conditions to Pareto optimal set is proposed.	evolutionary algorithm;pareto efficiency	Zhiyong Li;Günter Rudolph	2007		10.1145/1276958.1277138	evolutionary programming;mathematical optimization;theoretical computer science;machine learning;mathematics;quantum computer	AI	25.137179976736412	-6.046063039734481	12871
315c8c660383a1dc4ac8b8b3f830a5e1eaa7515b	application of evolutionary computation techniques to the optimal short-term scheduling of the electrical energy production	energy production;mixed integer program;power system;genetic algorithm;evolutionary computing	In this paper, an evolutionary technique applied to the optimal short-term scheduling (24 hours) of the electric energy production is presented. The equations that define the problem lead to a nonlinear mixed-integer programming problem with a high number of real and integer variables. Consequently, the resolution of the problem based on combinatorial methods is rather complex. The required heuristics, introduced to assure the feasibility of the constraints, are analyzed, along with a brief description of the proposed genetic algorithm. Finally, results from realistic cases based on the Spanish power system are reported, revealing the good performance of the proposed algorithm, taking into account the complexity and dimension of the problem.	crossover (genetic algorithm);evolutionary computation;genetic algorithm;heuristic (computer science);integer programming;long short-term memory;mutation (genetic algorithm);nonlinear system;scheduling (computing);software release life cycle	Alicia Troncoso Lora;José Cristóbal Riquelme Santos;José Luís Martínez Ramos;Jesús Riquelme Santos;Antonio Gómez Expósito	2003		10.1007/978-3-540-25945-9_65	evolutionary programming;mathematical optimization;theoretical computer science;mathematics;algorithm	AI	17.084409372299234	-1.8789624348542235	12882
37c8d8c92cf52d621e15a40c9347eca3401a20e6	damage detection in structural health monitoring using kernel pls based glr		The objective of this paper is to extend the applicability of the GLR method to a wide range of practical systems. Most real systems are nonlinear, multivariate, and are best represented by input-output type of models. Kernel partial least squares (KPLS) models have been widely used to represent such systems. Therefore, in this paper, kernel PLS-based GLR method will be utilized in practice to improve damage detection in Structural Health Monitoring (SHM). The developed kernel PLS-based GLR technique combines the benefits of the multivariate input-output kernel PLS model and the statistical fault detection GLR statistic which showed performance in the cases where process models are not available. GLR is a well-known statistical detection method that relies on maximizing the detection probability for a given false alarm rate. To calculate the kernel PLS model, we use the data collected from the complex 3DOF spring-mass-dashpot system. The simulation results show improved performance of kernel PLS-based GLR in damage detection compared to the classical kernel PLS method.	fault detection and isolation;glr parser;kernel (operating system);kernel principal component analysis;nonlinear system;partial least squares regression;simulation	Marwa Chaabane;Majdi Mansouri;Hazem N. Nounou;Mohamed N. Nounou;Mohamed Ben Slima;Ahmed Ben Hamida	2017	2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2017.8075555	kernel (linear algebra);principal component analysis;partial least squares regression;statistics;structural health monitoring;statistic;multivariate statistics;computer science;fault detection and isolation;constant false alarm rate;artificial intelligence;pattern recognition	Robotics	23.657903469070455	-24.54468406640085	12891
49589ef52325fafd1d20b046b0a9552f175dea31	tracking and video surveillance activity analysis	background modeling;activity analysis;video surveillance;video streaming;video processing;low complexity;tracking	"""The explosion in the number of cameras surveilling the environment in recent years is generating a need for systems capable of analysing video streams for important events. This paper outlines a system for detecting noteworthy behaviours (from a security or surveillance perspective) which does not involve the enumeration of the event sequences of all possible activities of interest. Instead the focus is on calculating a measure of the abnormality of the action taking place. This raises the need for a low complexity tracking algorithm robust to the noise artefacts present in video surveillance systems. The tracking technique described herein achieves this goal by using a """"future history"""" buffer of images and so delaying the classification and tracking of objects by the time quantum which is the buffer size. This allows disambiguation of noise blobs and facilitates classification in the case of occlusions and disappearance of people due to lighting, failures in the background model etc."""	algorithm;blob detection;closed-circuit television;computer security;connected component (graph theory);future history;requirement;sensor;streaming media;tracking system;while;word-sense disambiguation	Michael Cheng;Binh Pham;Dian Tjondronegoro	2006		10.1145/1174429.1174491	computer vision;simulation;computer science;video tracking;tracking;multimedia;video processing;computer graphics (images)	Vision	39.720219511978414	-46.580324595665516	12908
cac753fa972d7a801ba6b1be4fcd79eead9a576f	research of traffic flow forecasting based on the information fusion of bp network sequence	traffic volume forecasting;bp neural network;information fusion	Traffic flow forecasting is an important aspect of the ITS as accurate traffic predication can alleviate congestion, save traveling time and reduce economical loses. The forecasting process may rely on historical data, current data, or both, to forecast the traffic volume in the future. In this paper, we compare three different approaches in traffic forecasting, study the input data and output data for these approaches, as well as some general insights, and also propose BPi¾źneurali¾źnetwork to estimate accurate traffic flow for a roadway section. By means of three layers-BP neutral network model, in which mechanism algorithm are used to preprocess the multi-source data, error data is eliminated, multi-source data fusion is realized and accurate traffic forecasting is achieved.		Wei Zhang;Ridong Xiao;Jing Deng	2015		10.1007/978-3-319-23862-3_54	traffic generation model;simulation;geography;data mining;operations research;network traffic simulation	Vision	8.661675946655992	-14.31459188352496	12922
cca0b2538ffaf888c714c0694372ae5e0e603049	hierarchical mixture models for nested data structures	grade of membership;mixture model;finite mixture model;data structure;model based clustering	A hierarchical extension of the finite mixture model is presented that can be used for the analysis of nested data structures. The model permits a simultaneous model-based clustering of lowerand higher-level units. Lower-level observations within higher-level units are assumed to be mutually independent given cluster membership of the higher-level units. The proposed model can be seen as a finite mixture model in which the prior class membership probabilities are assumed to be random, which makes it very similar to the grade-of-membership (GoM) model. The new model is illustrated with an example from organizational psychology.	cluster analysis;data structure;industrial and organizational psychology;mixture model	Jeroen K. Vermunt;Jay Magidson	2004		10.1007/3-540-28084-7_26	econometrics;pattern recognition;statistics	ML	30.451741781424364	-25.993248613524774	12928
f55a0f91b88a6ba30a3dccc7fee38b495cbc9243	probabilistic sampling-based testing for accelerated reliability assessment		A relevant objective of software reliability assessment is to get unbiased estimates with an acceptable trade-off between the number of tests required and the variance of the estimate. A low variance is desirable to increase the confidence in the estimate, but too many tests may be required by conventional reliability assessment testing techniques based solely on the operational profile. This article presents probabilistic sampling-based testing, a new technique using unequal probability sampling to exploit auxiliary information about the software under test so as to assess reliability unbiasedly and efficiently. The technique expedites the assessment process assuming the availability of some prior belief about input regions failure proneness. The evaluation by simulation and experimentally shows promising results in terms of estimate accuracy and efficiency.	acceptance testing;experiment;gauss;program structure tree;sampling (signal processing);simulation;software reliability testing;test case	Roberto Pietrantuono;Stefano Russo	2018	2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)	10.1109/QRS.2018.00017	reliability engineering;sampling (statistics);software;probabilistic logic;software quality;exploit;computer science	SE	28.70620830460222	-18.198528890158975	12931
cb4cee2a26e43e351e05e9f51473935657ff58dc	neural networks for data quality monitoring of time series.	time series;data quality;neural network	Time series play an important role in most of large data bases. Much of the information comes in temporal patterns which is often used for decision taking. Problems with missing and noisy data arise when data quality is not monitored, generating losses in many fields such as economy, customer relationship and health management. In this paper we present a neural network based system used to provide data quality monitoring for time series data. The goal of this system is to continuously adapt a neural model for each monitored series, generating a corridor of acceptance for new observations. Each rejected observation may be substituted by its estimated value, so that data quality is improved. A group of four diverse time series was tested and the system proved to be able to detect the induced outliers.	data quality;database;neural networks;signal-to-noise ratio;time series	Augusto Cesar Heluy Dantas;José Manoel de Seixas	2007			data quality;computer science;machine learning;time series;time delay neural network;artificial neural network	DB	6.112393376728538	-21.940048383615434	12935
0db2eb97b685b4b4fe32e963c7e55cbf83aebd6e	on the ordering property and law of importation in fuzzy logic	law of importation;ordering property;fuzzy implications;fuzzy logic;compositional rule of inference;full implication triple i method	In this paper, the authors investigate the ordering property (OP), x y I x y ≤ ⇔ ( ) = , 1 , together with the general form of the law of importation(LI), i.e., I T x y z I x I y z , , , , ( ) ( ) = ( ) ( ) , whereT is a t-norm and I is a fuzzy implication for the four main classes of fuzzy implications. The authors give necessary and sufficient conditions under which both (OP) and (LI) holds for S-, R-implications and some specific families of QL-, D-implications. Following this, the paper proposes the sufficient condition under which the equivalence between CRI and triple I method for FMP can be established. Moreover, this conclusion can be viewed as a unified triple I method, a generalized form of the known results proposed by Wang and Pei. One of the classical logic tautologies that have attracted maximum attention from researchers is the law of importation: x y z x y z ∧ ( ) → ≡ → → ( ) ( ) , the general form of it is: I T x y z I x I y z , , , , ( ) ( ) = ( ) ( ) (LI) In the framework of fuzzy logic, (LI) has been studied in isolation(Jayaram, 2008) as well DOI: 10.4018/jalr.2010070102 18 International Journal of Artificial Life Research, 1(3), 17-30, July-September 2010 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. as with some related properties, for instance, Baczyński (2001) and Bouchon-Meunier and Kreinovich (1996) have studied (LI) with the general form of the following distributive property: x y z x y x z → ∧ ( ) ≡ → ( )∧ → ( ) . However, the law of importation has not been studied in conjunction with the ordering property: x y I x y ≤ ⇔ ( ) = , 1 (OP) which means the implication I defines an ordering on propositions (Dubois & Prade, 1991). In this paper, we investigate the conditions under which the four most established and well-studied classes of fuzzy implications, S-, R-, QLand D-implications, satisfy both (OP) and (LI) and explore their potential applications in the field of approximate reasoning along the following lines. The full implication triple I method, or simply the triple I method, proposed by Wang (1999) is one of the most established methods of approximate reasoning. This method may bring approximate reasoning within the framework of logical semantic implication, and it may be considered as a reasonable alternative or complement for the compositional rule of inference(CRI) proposed by Zadeh (1973). In the past few years, there were a lot of discussion centred on the unified form of triple I algorithm. However, these algorithms were constructed only for some special implications. Both Wang and Fu (2005) and Pei (2008) designed several algorithms for the uniform of the triple I method, but mostly based on residuated implications. Song, Feng, and Lee (2002) investigated the triple I method by using Zadeh’s implication operatorR Z , and obtained the computational formula for fuzzy modus pones (FMP). Moreover, their proofs tend to be very complicated. We propose a further generalized algorithm for the uniform of the triple I method by investigating when the inference obtained from the CRI and triple I method become equivalent. Toward this end, we give some sufficient conditions on the operators employed in the inference, conditions that highlight the significant role played by the ordering property and the law of importation. Contrast with the complicated proofs in the literatures we mentioned before, the proof of our conclusion is greatly simplified. The paper is organized as follows. First, we review some basic notations and definitions of fuzzy logic connectives,fuzzy implications, including S-, R-, QLand D-implications and some preliminary properties regarding these fuzzy implications. We then prove the sufficient and necessary conditions for an S-implication generated by any t-conorm and a strong fuzzy negation, an Rimplication generated by a left-continuous t-norm, QLand D-implications generated by continuous t-conorms, any t-norm and strong fuzzy negations to satisfy both (OP) and (LI). We also show the sufficient conditions under which the equivalence between the inference in the CRI and triple I method can be obtained. Furthermore, we propose the generalized algorithm for the uniform of the triple I method and highlight its advantages.	approximation algorithm;artificial life;execution unit;finite model property;fuzzy logic;logical connective;semantic html;t-norm;turing completeness;vladik kreinovich	Huiwen Deng;Huan Jiang	2010	IJALR	10.4018/jalr.2010070102	fuzzy logic;computer science;artificial intelligence;fuzzy number;mathematics;algorithm	AI	0.6192293691290774	-23.541056595916285	12939
149a57ec380c38762f3e392560ef3ad3e1ab8ebd	learning iterative image reconstruction in the neural abstraction pyramid	contrast enhancement;contrast enhanced;neural abstraction pyramid;occlusion;backpropagation;backpropagation through time;image reconstruction;superresolution	Successful image reconstruction requires the recognition of a scene and the generation of a clean image of that scene. We propose to use recurrent neural networks for both analysis and synthesis. The networks have a hierarchical architecture that represents images in multiple scales with different degrees of abstraction. The mapping between these representations is mediated by a local connection structure. We supply the networks with degraded images and train them to reconstruct the originals iteratively. This iterative reconstruction makes it possible to use partial results as context information to resolve ambiguities. We demonstrate the power of the approach using three examples: superresolution, fill-in of occluded parts, and noise removal / contrast enhancement. We also reconstruct images from sequences of degraded images.	artificial neural network;iterative method;iterative reconstruction;recurrent neural network;super-resolution imaging	Sven Behnke	2001	International Journal of Computational Intelligence and Applications	10.1142/S1469026801000342	iterative reconstruction;computer vision;computer science;artificial intelligence;backpropagation;machine learning;superresolution	Vision	25.69869622496286	-51.053261165995615	12942
42875619afbd2053655b489dfb13bb359985194c	designing of classifiers based on immune principles and fuzzy rules	fuzzy classification;clonal selection;immune algorithm;fuzzy rules;data mining;membership function;pattern classification;clonal selection principle;fuzzy systems;fuzzy system	This paper proposed an algorithm to design a fuzzy classification system based on immune principles. The proposed algorithm evolves a population of antibodies based on the clonal selection and hypermutation principles. The membership function parameters and the fuzzy rule set including the number of rules inside it are evolved at the same time. Each antibody (candidate solution) corresponds to a fuzzy classification rule set. We compared our algorithm with other classification schemes on some benchmark datasets. The results demonstrated the effectiveness of the proposed immune algorithm. 2007 Elsevier Inc. All rights reserved.	algorithm;benchmark (computing);fuzzy classification;fuzzy rule	Zhang Lei;Li Ren-hou	2008	Inf. Sci.	10.1016/j.ins.2007.11.019	membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	4.032519300083115	-29.623969785532438	12945
2cdd9e445e7259117b995516025fcfc02fa7eebb	temporal exemplar-based bayesian networks for facial expression recognition	belief networks;bayesian network;image recognition;estimation theory;cmu;facial expression recognition;probability;conference_paper;inference engines;probability estimation temporal behavior exemplar based bayesian network facial expression recognition historical observation layer prior knowledge layer;prior knowledge;prior knowledge layer;probability belief networks estimation theory face recognition;computational modeling;face recognition;hidden markov models;estimation;historical observation layer;distributed parameter networks;bayesian methods face recognition hidden markov models facial features image recognition image sequences application software deformable models principal component analysis feature extraction;facial features;temporal behavior exemplar based bayesian network;intelligent networks;facial expression;gesture recognition;probability estimation;image sequences	We present a Temporal Exemplar-based Bayesian Networks (TEBNs) for facial expression recognition. The proposed Bayesian Networks (BNs) consists of three layers: Observation layer, Exemplars layer and Prior Knowledge layer. In the Exemplars layer, exemplar-based model is integrated with BNs to improve the accuracy of probability estimation. In the Prior Knowledge layer, static BNs is extended to Temporal BNs by considering historical observations to model temporal behavior of facial expression. Experiment on CMU expression database illustrates that the proposed TEBNs is very efficient in modeling the evolution of facial deformation.	bayesian network;jacobian matrix and determinant;k-nearest neighbors algorithm;newton;newton's method;the matrix	Lifeng Shang;Kwok-Ping Chan	2008	2008 Seventh International Conference on Machine Learning and Applications	10.1109/ICMLA.2008.9	computer vision;intelligent network;estimation;computer science;machine learning;pattern recognition;probability;bayesian network;gesture recognition;estimation theory;computational model;facial expression;inference engine;statistics	Vision	45.356776985416225	-49.486535576628626	12948
79502964cfd689f4a3cc1529d903a49350f1ca21	forecasting the volatility of a combined multi-country stock index using gwma algorithms		Globalization has increased the volatility of international financial transactions, particularly those related to international stock markets. An increase in the volatility of one countryu0027s stock market spreads throughout the globe, affecting other countriesu0027 stock markets. In particular, the Dow Jones Industrial Average plays an extremely important role in the international stock market. This paper uses the generally weighted moving average method and data from the Dow Jones Industrial Average, the National Association of Securities Dealers Automated Quotations, Japanu0027s Nikkei 225, the Korea Composite Stock Price Index, and the Hong Kong Hang Seng Index to predict the performance of the Taiwan Capitalization Weighted Stock Index. This paper attempts to find the smallest prediction error using the optimal combination of generally weighted moving average model parameters and combinations of various international stock market data and compares the results to that found using the exponentially weighted moving average model to explore differences between the two types of forecasting models.		Shey-Huei Sheu;Cheng-Yi Lin;Shin-Li Lu;Hsin-Nan Tsai;Yan-Chun Chen	2018	Expert Systems	10.1111/exsy.12248	globalization;data mining;stock market index;computer science;financial economics;moving average;stock market;ewma chart;price index;financial transaction;volatility (finance)	NLP	6.528433241248912	-18.927600228568277	12949
33336b69823bb7536be925a356f11de65d197366	a flexible-input, desired-output motor controller for engineering design classes	control engineering education;engineering design;robot competition engineering design motor control;72 mhz flexible input desired output motor controller engineering design classes wireless motor controller radio transmitter power motors radio controlled devices bidirectional control signals;design engineering;power engineering education control engineering education design methodology radio transmitters;radio transmitters machine control power engineering education control engineering education design engineering control system synthesis;radio transmitters;machine control;control system synthesis;teaching assistant;power engineering education;motor control	The flexible-input, desired-output (FIDO) motor controller is a robust wireless motor controller specifically designed for engineering design classes where students design and build radio-controlled devices that are typically featured in an end-of-semester celebration or competition. The controller receives bidirectional control signals from a standard 72-MHz band hobby radio transmitter and proportionally drives four motors with up to 3 A of current in each. The controller is designed to accommodate changes to the battery voltage as well as motors with different voltage ratings. It is also designed to be built easily by teaching assistants and can be reused year after year. These capabilities are designed to lower the cost of running this type of class and reduce the organizational work load required of the instructors. Specifically, this motor controller enables the use of commercial cordless drill batteries as inexpensive, portable sources to power motors at a wide range of voltage ratings. The drills are also used during the class as construction tools and can be used by the student after the class. The FIDO motor controller has been successfully tested in the 2.007-Design and Manufacturing classes at the Massachusetts Institute of Technology (MIT) Cambridge, and its utility and acceptance are confirmed by a class survey. Detailed circuit schematics are provided in the Appendix.	attachments;autonomous robot;circuit diagram;electronic design automation;engineering design process;h bridge;mechatronics;micro instrumentation and telemetry systems;microprocessor;model–view–controller;open architecture;radio control;rechargeable battery;schematic;signal processing;transmitter	Hongshen Ma;Alexander H. Slocum	2006	IEEE Transactions on Education	10.1109/TE.2005.863423	control engineering;open-loop controller;motor control;transmitter;electronic engineering;controller;engineering;electrical engineering;instrumentation and control engineering;control theory;engineering design process	Mobile	53.412738514334364	-11.499192489037572	12952
c7daa19064c110f88750efd49d55ff9b8e40dd4b	reducing vehicular traffic congestion using available forward road capacity detection	detectors;traffic control;transportation road traffic backpressure;mobile robots;junctions;roads;v2x communication vehicular traffic congestion reduction forward road capacity detection road congestion urban road systems traffic light control traffic controller afrc detection backpressure methods upstream traffic guidance enhanced downstream communication automatic congested region identification commercial traffic simulation software tools va systems fuel efficiency fuel waste autonomous vehicles;vehicles;vehicles junctions timing roads detectors traffic control mobile robots;vehicular ad hoc networks fuel economy road traffic control telecommunication congestion control traffic engineering computing;timing	Road congestion is a complex, significant and continuing problem. This project is concerned with urban road systems and traffic light control. The objective is to create a novel traffic controller which uses available forward road capacity (AFRC) detection and locality awareness in conjunction with existing backpressure methods as a means to reduce the duration of congestion. The controller is an ongoing development, the initial version considers only local AFRC but is being actively developed to communicate to multiple downstream controllers and to upstream traffic guidance. Enhanced downstream communication will automatically identify a junction where congestion starts and actively postpone the onset, automatically identify congested region exits and actively resolve the congestion in a shorter time. The controller measures the degree of congestion and is capable of providing information to reduce the influx of traffic into an already congested region. The effectiveness of the controller is shown through simulation using commercial traffic simulation software tools. Initial results are very promising, delays have been reduced to less than 15% of comparative timed and VA systems, the performance improvement increases as the road becomes more congested. Consequences of reducing overall congestion duration include increased fuel efficiency, reduced fuel waste and a reduction in the negative effects on environment and health. The benefits of the system are equally applicable to all powered transport systems regardless of fuel source or control method including autonomous vehicles and vehicles equipped with v2x communication.	autonomous robot;dvd region code;downstream (software development);locality of reference;network congestion;onset (audio);simulation software	James Hardy;Lu Liu	2015	2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing	10.1109/CIT/IUCC/DASC/PICOM.2015.311	mobile robot;traffic engineering;detector;network traffic control;simulation;floating car data;vehicle information and communication system;computer science;artificial intelligence;traffic congestion reconstruction with kerner's three-phase theory;traffic bottleneck;computer security;computer network;traffic optimization	Robotics	9.569439100320826	-9.005768596914102	12962
1b4ba76d294c5fbbae614e3b25d8b3ed4b08f9af	negative examples for sequential importance sampling of binary contingency tables	analisis estadistico;relacion orden;tabla contingencia;ordering;metodo secuencial;probabilistic approach;sequential method;algorithme;algorithm;relation ordre;degree sequence;statistical analysis;markov chain monte carlo;sequential importance sampling;enfoque probabilista;approche probabiliste;analyse statistique;echantillonnage importance;methode sequentielle;contingency table;importance sampling;table contingence;sequential monte carlo;algoritmo	The sequential importance sampling (SIS) algorithm has gained considerable popularity for its empirical success. One of its noted applications is to the binary contingency tables problem, an important problem in statistics, where the goal is to estimate the number of 0/1 matrices with prescribed row and column sums. We give a family of examples in which the SIS procedure, if run for any subexponential number of trials, will underestimate the number of tables by an exponential factor. This result holds for any of the usual design choices in the SIS algorithm, namely the ordering of the columns and rows. These are apparently the first theoretical results on the efficiency of the SIS algorithm for binary contingency tables. Finally, we present experimental evidence that the SIS algorithm is efficient for row and column sums that are regular. Our work is a first step in determining rigorously the class of inputs for which SIS is effective.	contingency table;importance sampling;particle filter	Ivona Bezáková;Alistair Sinclair;Daniel Stefankovic;Eric Vigoda	2006		10.1007/11841036_15	econometrics;particle filter;markov chain monte carlo;contingency table;importance sampling;order theory;mathematics;algorithm;statistics	EDA	32.017106724713315	-21.559063273765528	12974
7b5e2e9730f53930ce0d6fd67b2c3615f8e7e400	a dimension reduction shannon-wavelet based method for option pricing		We present a robust and highly efficient dimension reduction Shannon-wavelet method for computing European option prices and hedging parameters under a general jump-diffusion model with square-root stochastic variance and multi-factor Gaussian interest rates. Within a dimension reduction framework, the option price can be expressed as a two-dimensional integral that involves only (i) the value of the variance at the terminal time, and (ii) the time-integrated variance process conditional on this value. A Shannon wavelet inverse Fourier technique is developed to approximate the conditional density of the time-integrated variance process. Furthermore, thanks to the excellent approximation properties of Shannon wavelets, the overall pricing procedure is reduced to the evaluation of just a single integral that involves only the density of the terminal variance value. This single integral can be accurately evaluated, since the density of the variance at the terminal time is known in closed-form. We develop sharp approximation error bounds for the option price and hedging parameters. Numerical experiments confirm the robustness and impressive efficiency of the method.	dimensionality reduction;shannon (unit);shannon wavelet	Duy-Minh Dang;Luis Ortiz-Gracia	2018	J. Sci. Comput.	10.1007/s10915-017-0556-y	mathematical optimization;fourier transform;robustness (computer science);mathematics;wavelet;statistics;dimensionality reduction;conditional probability distribution;valuation of options;shannon wavelet;approximation error	Logic	32.401064650943496	-15.862050255298481	12979
c35f32a1a7b670bc0bc7255e548fafdf70211254	a rough set paradigm for unifying rough set theory and fuzzy set theory	mereologie;rough set theory;logique floue;logica difusa;fuzzy set theory;rough mereology;fuzzy logic;mereology;point of view;rough set;ensemble approximatif;rough inclusion	In this plenary address, we would like to discuss rough inclusions defined in Rough Mereology, a joint idea with A. Skowron, as a basis for common models for rough as well as fuzzy set theories. We would like to justify the point of view that tolerance (or, similarity) is the leading motif common to both theories and in this area paths between the two lie.	fuzzy set;mereology;motif;rough set;set theory	Lech Polkowski	2003		10.1007/3-540-39205-X_9	discrete mathematics;rough set;computer science;machine learning;mathematics;algorithm;dominance-based rough set approach	AI	-1.1789632591119108	-23.90237367671669	12984
20915fb69ee44a17b8b36c05ce06b4a59107f700	feature clustering for accelerating parallel coordinate descent	parallel computing;classification;machine learning;sparse data	Large-scale ` 1 -regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learning, including classification and regression problems. High-performance algorithms and implementations are critical to efficiently solving these problems. Building upon previous work on coordinate descent algorithms for ` 1 -regularized problems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-Greedy. We give a unified convergence analysis for the family of block-greedy algorithms. The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small. Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications. We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale ` 1 -regularization problems.	cluster analysis;compressed sensing;computation;coordinate descent;greedy algorithm;heuristic;load balancing (computing);matrix regularization;maximal set;parallel computing;randomized algorithm;slowly changing dimension;statistical classification;supervised learning;xtx	Chad Scherrer;Ambuj Tewari;Mahantesh Halappanavar;David J. Haglin	2012			mathematical optimization;sparse matrix;biological classification;computer science;theoretical computer science;machine learning;mathematics	ML	25.051621719084693	-36.95520413825028	12986
c325cf7e06626ac882df7067b7517086579c271b	convexity issues in system identification	convexity issues dynamical systems model estimation measured input output data convex formulations machine learning statistical learning theory sparsity regularization subspace method algebraic techniques model parameterization system identification algorithm;control engineering;data models vectors mathematical model estimation covariance matrices numerical models kernel;reglerteknik;statistical analysis;identification;statistical analysis identification learning artificial intelligence;learning artificial intelligence	System Identification is about estimating models of dynamical systems from measured input-output data. Its traditional foundation is basic statistical techniques, such as maximum likelihood estimation and asymptotic analysis of bias and variance and the like. Maximum likelihood estimation relies on minimization of criterion functions that typically are non-convex, and may cause numerical search problems. Recent interest in identification algorithms has focused on techniques that are centered around convex formulations. This is partly the result of developments in machine learning and statistical learning theory. The development concerns issues of regularization for sparsity and for better tuned bias/variance trade-offs. It also involves the use of subspace methods as well as nuclear norms as proxies to rank constraints. A quite different route to convexity is to use algebraic techniques manipulate the model parameterizations. This article will illustrate all this recent development.	algorithm;convex hull;dynamical system;machine learning;matrix regularization;numerical analysis;sparse matrix;statistical learning theory;system identification	Lennart Ljung;Tianshi Chen	2013	2013 10th IEEE International Conference on Control and Automation (ICCA)	10.1109/ICCA.2013.6565206	identification;econometrics;mathematical optimization;algorithmic learning theory;system identification;computer science;machine learning;mathematics;statistics	Robotics	27.335064147067023	-33.180000633595526	12993
e0a47f3cc9f601096798f306366014c5fe064ba1	a parametric classification rule based on the exponentially embedded family	pattern classification estimation theory exponential distribution monte carlo methods;training data vectors data models estimation neural networks statistics testing;neural networks;testing;monte carlo method parametric classification rule exponentially embedded family eef order estimation probability density function reference distribution power quality disturbance classification;training data;vectors;estimation;statistics;parametric classification rule exponentially embedded family eef multivariate gaussian classification;data models	In this paper, we extend the exponentially embedded family (EEF), a new approach to model order estimation and probability density function construction originally proposed by Kay in 2005, to multivariate pattern recognition. Specifically, a parametric classifier rule based on the EEF is developed, in which we construct a distribution for each class based on a reference distribution. The proposed method can address different types of classification problems in either a data-driven manner or a model-driven manner. In this paper, we demonstrate its effectiveness with examples of synthetic data classification and real-life data classification in a data-driven manner and the example of power quality disturbance classification in a model-driven manner. To evaluate the classification performance of our approach, the Monte-Carlo method is used in our experiments. The promising experimental results indicate many potential applications of the proposed method.	appendix;artificial neural network;biological neural networks;class;classification;exptime;electric power quality;embedded system;experiment;graph embedding;kullback–leibler divergence;latent variable;learning disorders;mbnl1 gene;mixture model;model-driven architecture;model-driven integration;monte carlo method;naive bayes classifier;nearest neighbor search;normal statistical distribution;ordinal position;ordinal data;pq tree;pattern recognition;portable document format;real life;simulation;single linkage cluster analysis;synthetic data;exponential;superoxide-generating nadph oxidase	Bo Tang;Haibo He;Quan Ding;Steven Kay	2015	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2014.2383692	data modeling;training set;estimation;computer science;machine learning;classification rule;pattern recognition;mathematics;software testing;artificial neural network;statistics	ML	29.102730478751326	-30.0254234982848	13025
510bfb623129585e06ce0c9ff5d9f2425178411e	content-based image retrieval with multinomial relevance feedback	engineering;dirichlet distribution;electrical electronic;technology;reinforcement learning;theory methods;science technology;artificial intelligence;computer science;content based image retrieval;relevance feedback;image retrieval	The paper considers an interactive search paradigm in which at each round a user is presented with a set of k images and is required to select one that is closest to her target. Performance is measured by the number of rounds needed to identify a specific target image or to find an image among the t nearest neighbours to the target in the database. Building on earlier work we assume a multinomial user model with the probabilities of response proportional to a function of the distance to the target. The conjugate prior Dirichlet distribution is used to model the problem motivating an algorithm that trades exploration and exploitation in presenting the images in each round. Experimental results verify the fit of the model with the problem as well as show that the new approach compares favourably with previous work.	content-based image retrieval;experiment;heuristic;image scaling;k-nearest neighbors algorithm;multinomial logistic regression;programming paradigm;relevance feedback	Dorota Glowacka;John Shawe-Taylor	2010			dirichlet distribution;image retrieval;computer science;artificial intelligence;machine learning;data mining;mathematics;reinforcement learning;statistics;technology	Vision	44.421209098017314	-28.530233323680942	13032
9db327c1a950828f9993d97015631db20b647da6	the indoor localization and tracking estimation method of mobile targets in three-dimensional wireless sensor networks	biological patents;biomedical journals;text mining;europe pubmed central;localization;citation search;citation networks;three dimensional deployment;research articles;wsns;abstracts;期刊论文;open access;life sciences;clinical guidelines;full text;calibration;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Indoor localization is a significant research area in wireless sensor networks (WSNs). Generally, the nodes of WSNs are deployed in the same plane, i.e., the floor, as the target to be positioned, which causes the sensing signal to be influenced or even blocked by unpredictable obstacles, like furniture. However, a 3D system, like Cricket, can reduce the negative impact of obstacles to the maximum extent and guarantee the sensing signal transmission by using the line of sight (LOS). However, most of the traditional localization methods are not available for the new deployment mode. In this paper, we propose the self-localization of beacons method based on the Cayley-Menger determinant, which can determine the positions of beacons stuck in the ceiling; and differential sensitivity analysis (DSA) is also applied to eliminate measurement errors in measurement data fusion. Then, the calibration of beacons scheme is proposed to further refine the locations of beacons by the mobile robot. According to the robot's motion model based on dead reckoning, which is the process of determining one's current position, we employ the H∞ filter and the strong tracking filter (STF) to calibrate the rough locations, respectively. Lastly, the optimal node selection scheme based on geometric dilution precision (GDOP) is presented here, which is able to pick the group of beacons with the minimum GDOP from all of the beacons. Then, we propose the GDOP-based weighting estimation method (GWEM) to associate redundant information with the position of the target. To verify the proposed methods in the paper, we design and conduct a simulation and an experiment in an indoor setting. Compared to EKF and the H∞ filter, the adopted STF method can more effectively calibrate the locations of beacons; GWEM can provide centimeter-level precision in 3D environments by using the combination of beacons that minimizes GDOP.	anatomic node;cfh gene;calibration;capsule endoscopy;dead reckoning;deploy;dilution of precision (navigation);entity name part qualifier - adopted;extended kalman filter;gauss;gauss–newton algorithm;iterative method;least squares;least-squares analysis;magma;menger sponge;mobile robot;newton;newton's method;node - plant part;simulation;vena cava filters;centimeter;establishment and maintenance of localization	Zixi Jia;Chengdong Wu;Zhao Li;Yunzhou Zhang;Bo Guan	2015		10.3390/s151129661	embedded system;text mining;calibration;simulation;internationalization and localization;telecommunications;computer science;bioinformatics;engineering;electrical engineering;data mining	Robotics	49.48349604618873	0.599477605846393	13038
095eb65d46778300f5923adf26ddb0e8a544e75b	implementation of string recognition algorithm based on the principle of artificial immunology	string recognition;two stage r row matching algorithm;detectors;complexity theory;artificial immune system;string matching artificial immune systems data structures learning artificial intelligence;generic algorithm;artificial immunology;cloning;detector set training string recognition algorithm artificial immune system two stage r row matching algorithm sliding window data structure self set mechanism;arrays;artificial immune systems detectors immune system object detection system testing data structures monitoring computer science computational modeling computer simulation;monitoring;string recognition algorithm;data structures;immune system;detector set training;self set mechanism;learning artificial intelligence;string matching;data structure;artificial immune systems;algorithm design and analysis;immune response artificial immunology string recognition sliding window;sliding window;immune response;partitioning algorithms	Artificial immune system simulates the principle of lymphocytes in the natural immune system, detects abnormality in objects to be tested through the immune response mechanism. Based on the r-row-matching algorithm, we propose a two-stage r-row-matching algorithm. According to the concept of immunity, the immunity process and mechanism, we design self-set and detector-set by using a sliding window. On this basis, the self-set, the detector-set and other generating algorithm, as well as the algorithm of detector-set training are implemented. Finally, based on the data structure of sliding window, the string recognition algorithm is implemented. Experiments show that the algorithm can effectively carry out string identification.	algorithm;artificial immune system;data structure	Junmin Ye;Junjie Wang;Wei Dong;Zhichang Qi	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.200	immune system;data structure;computer science;artificial intelligence;theoretical computer science;machine learning;programming language;artificial immune system	Robotics	3.816453282627848	-31.397581801738298	13039
64871cbcce04a999a0f887a9e7f16174ed61b0f0	motion perception using spatiotemporal frequency analysis	spatiotemporal frequency analysis;motion analysis;image motion analysis;fourier transform;frequency analysis;motion analysis spatiotemporal phenomena image motion analysis data mining layout frequency domain analysis information analysis fourier transforms image analysis image sequence analysis;frequency domain analysis;textured objects;image sequence analysis;motion estimation;layout;data mining;image texture;motion parameters;motion perception;filtering theory motion estimation image sequences image texture frequency domain analysis fourier transforms parameter estimation;motion information;fourier transforms;image sequence;spatiotemporal phenomena;optical flow information;spatiotemporal frequency domain analysis;image analysis;optical flow;temporal signature;domain analysis;parameter estimation;fourier transform properties;spatiotemporal filtering;dynamic scene;spatiotemporal filtering spatiotemporal frequency analysis motion perception motion information temporal signature textured objects spatiotemporal frequency domain analysis optical flow information image sequence motion parameters dynamic scene fourier transform properties depth parameters;information analysis;filtering theory;depth parameters;dynamic scenes;image sequences	Motion information is extracted by identifying the temporal signature associated with the textured objects in the scene. In this paper, we present a new computational framework for motion perception. Our methodology considers spatiotemporal frequency (STF) domain analysis to extract the optical flow information. First, we show that a sequence of image frames can be used to extract the motion parameters for the different regions in a dynamic scene using the basic Fourier transform properties in the STF analysis approach. A detailed analytical description of this model to interchangably extract motion and depth parameters and results to highlight their salient properties are presented.	domain analysis;frequency analysis;optical flow	Gopalan Ravichandran;Mohan Manubhai Trivedi	1994		10.1109/ICIP.1994.413575	fourier transform;computer vision;image analysis;speech recognition;computer science;pattern recognition;motion estimation;mathematics	Vision	39.167214115105516	-50.473705252237146	13041
d0ce2e6588d1e55fcbaa0b7606abfe6762635192	representability of binary relations through fuzzy numbers	engineering;intervalo;preference theory;fuzzy set;procesamiento informacion;fuzzy numbers;numero difuso;fuzzy number;fuzzy relation;nombre flou;analisis decision;conjunto difuso;relation binaire;ensemble flou;intervalle;binary number;decision analysis;ingenierie;preference modelling;interval;numero binario;triangular fuzzy number;theorie preference;information processing;nombre flou triangulaire;preferencia;decision;binary relation;ingenieria;preference;sistema difuso;teoria preferencia;systeme flou;relation floue;traitement information;analyse decision;relacion difusa;fuzzy system;interval order;nombre binaire	We analyse the representability of different classes of binary relations on a set by means of suitable fuzzy numbers. In particular, we show that symmetric triangular fuzzy numbers can be considered as the best codomain to represent interval orders. We also pay attention to the representability of other classes of acyclic binary relations. © 2005 Elsevier B.V. All rights reserved.	directed acyclic graph;fuzzy logic;fuzzy number;interval arithmetic;numerical analysis	María J. Campión;Juan Carlos Candeal;Esteban Induráin	2006	Fuzzy Sets and Systems	10.1016/j.fss.2005.06.018	discrete mathematics;information processing;decision analysis;computer science;artificial intelligence;fuzzy number;mathematics;algorithm;fuzzy control system	AI	1.6214177815807254	-22.01409993771649	13072
057a4c0747e4149236caa513ef8115a58c05d517	evolution of the cpg with sensory feedback for bipedal locomotion	modelizacion;animacion por computador;humanoid robot;genetic program;walking;caminata;legged locomotion;computer graphics;computer model;locomotion avec jambes;gait;marcha;sensory feedback;robotics;algoritmo genetico;construction mecanique;modelisation;mechanical engineering;construccion mecanica;posture;neural system;marche a pied;locomocion bipedo;neurofisiologia;postura;neurophysiologie;algorithme genetique;robotica;algorithme evolutionniste;genetic algorithm;coordinacion;algoritmo evolucionista;robotique;neurophysiology;evolutionary algorithm;reseau neuronal;bipedal walking;computer animation;allure;modeling;grafico computadora;infographie;locomotion bipede;red neuronal;optimization model;coordination;neural network;animation par ordinateur	This paper shows how the computational model, which simulates the coordinated movements of human-like bipedal locomotion, can be evolutionally generated without the elaboration of manual coding. In the research on bio-mechanical engineering, robotics and neurophysiology, the mechanism of human bipedal walking is of major interest. It can serve as a basis for developing several applications such as computer animation and humanoid robots. Nevertheless, because of the complexity of human's neuronal system that interacts with the body dynamics making the walking movements, much is left unknown about the control mechanism of locomotion, and researchers were looking for the optimal model of the neuronal system by extensive efforts of trial and error. In this work, genetic programming is utilized to induce the model of the neural system automatically and its effectives are shown by simulating a human bipedal gait with the obtained model. The experimental results show some promising evidence for evolutionary generation of the human-like bipedal locomotion.	central pattern generator	Sooyol Ok;DuckSool Kim	2005		10.1007/11539117_102	simulation;systems modeling;genetic algorithm;computer science;humanoid robot;artificial intelligence;bio-inspired robotics;evolutionary algorithm;robot locomotion;gait;computer animation;robotics;computer graphics;neurophysiology;artificial neural network	Robotics	23.818220836298	-13.018359468469315	13074
f26d3ceb866316bfe95a38dbe80cfbc343e6ad88	exploring discrepancies in findings obtained with the kdd cup '99 data set	intrusion detection;machine learning;kdd cup 99 data set;methodology	The KDD Cup '99 data set has been widely used to evaluate intrusion detection prototypes, most based on machine learning techniques, for nearly a decade. The data set served well in the KDD Cup '99 competition to demonstrate that machine learning can be useful in intrusion detection systems. However, there are discrepancies in the findings reported in the literature. Further, some researchers have published criticisms of the data (and the DARPA data from which the KDD Cup '99 data has been derived), questioning the validity of results obtained with this data. Despite the criticisms, researchers continue to use the data due to a lack of better publicly available alternatives. Hence, it is important to identify the value of the data set and the findings from the extensive body of research based on it, which has largely been ignored by the existing critiques. This paper reports on an empirical investigation, demonstrating the impact of several methodological differences in the publicly available subsets, which uncovers several underlying causes of the discrepancy in the results reported in the literature. These findings allow us to better interpret the current body of research, and inform recommendations for future use of the data set.	data mining;sigkdd	Vegard Engen;Jonathan Vincent;Keith Phalp	2011	Intell. Data Anal.	10.3233/IDA-2010-0466	intrusion detection system;computer science;data science;machine learning;methodology;data mining;computer security	ML	6.255915119911774	-47.10032211377522	13079
