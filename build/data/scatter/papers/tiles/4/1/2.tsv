id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
59f62db83340aa3dfa0a2ac3606ffa9fe87a38dc	sustainable implementation-level workflow for automating nfv operation		Network function virtualization is expected to enhance automated operation such as service deployment and fault recovery. As an effort to achieve automated operation for NFV, the ZOOM project has been launched by TM Forum. However, standardization of the business process framework (eTOM) is still limited to the abstracted process level, and an implementation-level process needs to be created by each organization considering vendor-specific implementations of control and management. As a result, implementation-level operation tasks are diverging and the cost of process maintenance is increasing accordingly. On the other hand, NFV MANO standardized by ETSI is expected to reduce implementation variations of control and management. From the above background, the harmonization of both standard models eTOM and ETSI to achieve a telecom operation map that can drive an automated onboarding NFV infrastructure should be considered. The contribution of this paper is to show how NFV MANO standards can derive a versatile implementation-level eTOM workflow by abstracting resources and functions of NFV. The paper describes the challenges of defining eTOM through mapping of MANO operations and eTOM processes.	business process;divergence (computer science);network function virtualization;page zooming;software deployment	Tatsuji Miyamoto;Masanori Miyazawa;Michiaki Hayashi	2017	2017 IFIP/IEEE Symposium on Integrated Network and Service Management (IM)	10.23919/INM.2017.7987394	enhanced telecom operations map;computer science;implementation;process control;computer network;software deployment;standardization;maintenance engineering;onboarding;workflow	OS	-30.561497858105785	55.266849008513724	5625
36a8ce7f97aa036d659c8c5c66a2e2078000469d	canonical abstraction for outerjoin optimization	search space;data summarization;cartesian product;efficient implementation;clustering;incremental data bubbles	Outerjoins are an important class of joins and are widely used in various kinds of applications. It is challenging to optimize queries that contain outerjoins because outerjoins do not always commute with inner joins. Previous work has studied this problem and provided techniques that allow certain reordering of the join sequences. However, the optimization of outerjoin queries is still not as powerful as that of inner joins.An inner join query can always be canonically represented as a sequence of Cartesian products of all relations, followed by a sequence of selection operations, each applying a conjunct in the join predicates. This canonical abstraction is very powerful because it enables the optimizer to use any join sequence for plan generation. Unfortunately, such a canonical abstraction for outerjoin queries has not been developed. As a result, existing techniques always exclude certain join sequences from planning, which can lead to a severe performance penalty.Given a query consisting of a sequence of inner and outer joins, we, for the first time, present a canonical abstraction based on three operations: outer Cartesian products, nullification, and best match. Like the inner join abstraction, our outerjoin abstraction permits all join sequences, and preserves the property of both commutativity and transitivity among predicates. This allows us to generate plans that are very desirable for performance reasons but that couldn't be done before. We present an algorithm that produces such a canonical abstraction, and a method that extends an inner-join optimizer to generate plans in an expanded search space. We also describe an efficient implementation of the best match operation using the OLAP functionalities in SQL:1999. Our experimental results show that our technique can significantly improve the performance of outerjoin queries.	algorithm;alljoyn;cartesian closed category;database;join (sql);mathematical optimization;online analytical processing;predicate (mathematical logic);sql;sql:1999;vertex-transitive graph	Jun Rao;Hamid Pirahesh;Calisto Zuzarte	2004		10.1145/1007568.1007643	computer science;theoretical computer science;cartesian product;database;cluster analysis;sort-merge join;algorithm	DB	-23.57834754134286	11.678708715943483	5626
07616ac5872d581b93a82bbcfce1760812fbe506	design, implementation, and evaluation of the constraint language cc(fd)	institutional repositories;fedora;vital;finite domain;vtls;ils	This paper describes the design, implementation, and applications of the constraint logic language cc(FD). cc(FD) is a declarative nondeterministic constraint logic language over nite domains based on the cc framework 28], an extension of the CLP scheme 17]. Its constraint solver includes (non-linear) arithmetic constraints over natural numbers which are approximated using domain and interval consistency. The main novelty of cc(FD) is the inclusion of a number of general-purpose combinators, in particular cardinality, constructive disjunction, and blocking implication, in conjunction with new constraint operations such as constraint entailment and generalization. These combinators signiicantly improve the operational expressiveness, extensibility, and exibility of CLP languages and allows issues such as the deenition of non-primitive constraints and disjunctions to be tackled at the language level. The implementation of cc(FD) (about 40,000 lines of C) includes a WAM-based engine 37], optimal arc-consistency algorithms based on AC-5 35], and incremental implementation of the combinators. Results on numerous problems, including scheduling, resource allocation, sequencing, packing, and hamiltonian paths are reported and indicate that cc(FD) comes close to procedural languages on a number of com-binatorial problems. In addition, a small cc(FD) program was able to nd the optimal solution and prove optimality to a famous 10/10 disjunctive scheduling problem 24], which was left open for more than 20 years and nally solved in 1988.	approximation algorithm;blocking (computing);combinatory logic;disjunctive normal form;extensibility;general-purpose modeling;interval arithmetic;local consistency;nonlinear system;scheduling (computing);set packing;solver;warren abstract machine	Pascal Van Hentenryck;Vijay A. Saraswat;Yves Deville	1998	J. Log. Program.	10.1016/S0743-1066(98)10006-7	constraint logic programming;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-18.18424762884659	19.303273907006005	5630
dba0c83ce7fb5dea508cf896121111c7d74798a4	object-oriented co-specification for embedded systems	hardware software co design;embedded system;object oriented;embedded computing;real time systems	Hardware-software co-design is the simultaneous design of the software which implements a set of functions and the hardware platform on which the software executes. Co-design is often necessary in the design of embedded systems to meet their performance and cost requirements. In most embedded systems, some implementation choices are easily predictable while others are not. Existing specification methodologies have not considered by the necessary uncertainty of the implementation of critical functions. Objectoriented specification techniques are, however, well-suited to the partitioning of concerns necessary for co-specification. This paper presents some techniques for adapting object-oriented specification to co-specification and illustrates their use with an example.	embedded system;requirement	Wayne H. Wolf	1996	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(95)01071-8	embedded system;embedded operating system;software requirements specification;computer architecture;real-time computing;computer science;object-oriented programming;avionics software	EDA	-41.66992810771756	34.59021228954646	5639
97e98c471fe36f2acb2c5845366f58c70c007982	an implementation of electronic shopping cart on the web system using component-object technology	web system;session management;efficient communications electronic shopping cart system world wide web client server system component object technology context data maintenance user sessions reliability safety session management context data storage mechanism main memory performance;information resources;electronic commerce;distributed object management home shopping electronic commerce information resources internet software reliability safety client server systems subroutines;client server systems;web server client server systems safety memory management costs application software technology management web sites access protocols file servers;home shopping;client server;internet;safety;distributed object management;world wide web;software reliability;subroutines	We propose a new mechanism for implementing the electronic shopping cart system (shortly, the shopping cart system) on the World Wide Web system (the Web system). The electronic shopping cart system is one of typical clientserver systems, and it includes essential tasks to be implemented in the typical Web based client-server system. The most important task is to maintain context data between successive user sessions. Although several methods, which can be applied to implement the electronic shopping cart on the Web system, have been proposed, any of them can not attain the task of maintaining context data sufficiently. In this paper we analyze the task and point out the following three difJiculties ( d l ) reliabilio, (d2) safety, and (d3) session management. We then propose a new mechanism (called the Context Data Storing (CDS) mechanism) to solve all of (d l ) , (d2) and (d3). In the proposed CDS mechanism, the context data for the session management is stored in the main memory of the client computer As a result, the CDS mechanism can achieve both high reliability and high safety as well as the management capability of user sessions. Next, we use component object technology to implement the CDS mechanism. Then, we compared the performance of the electronic shopping cart system using the proposed CDS mechanism with the one using the previous methods. The result showed that our proposed mechanism has solved all diflculties (d l ) , (d2) and (d3) and has attained efJicient communications between clients and Web servers.	client (computing);client–server model;computer data storage;server (computing);session (computer science);shopping cart software;web server;world wide web	Satoru Uehara;Osamu Mizuno;Tohru Kikuno	2001		10.1109/WORDS.2001.945116	real-time computing;computer science;database;world wide web	Web+IR	-27.30931376104584	48.49619415806779	5652
f109c94cd07700e93437d608930471dbe20a8813	information modelling and simulation in large interdependent critical infrastructures in irriis	information model;dependence analysis;federated simulation;semantic model;ci dependencies;scada system;information modelling;information system;ci dependability;critical infrastructure;simulation environment	Critical Infrastructures (CIs) and their protection play a very important role in modern societies. In the recent years, many studies and projects around this subject have investigated the growing (inter) dependencies and their mutual influence. In February 2006 the EU project IRRIIS [1] was started to increase the dependability, survivability and resilience of informationbased critical infrastructures (CIs). One of the main issues here is how to model information about critical infrastructures with special emphasis to their interdependencies. Many different aspects have to be taken into account for this purpose: systems with their components and their interactions, their behaviours, the services they provide, events and actions influencing them, risks to be considered, etc. Today, there are sophisticated information systems to manage CIs. They take special views on the respective CIs – but leave aside interdependencies to other systems. To some extend this is quite natural because interdependent systems can be quite different and need different kinds of information to be managed. For interdependency analysis and management of critical infrastructures we need information focussing exactly on the interdependency aspects – in relation to all other relevant kinds of information. This is the aim of the IRRIIS information model. It is based on the assumption that there is a generic approach to CI interdependencies – regardless of the concrete CIs considered. They all need systems and behaviours, states and transitions, events and actions. In order to deal with these complex but interrelated kinds of information the IRRIIS Information Model is based on semantic information modelling techniques. They provide the necessary expressivity, clear structures, and the needed formalization. This Generic Information Model allows us to integrate information from CIs – from real ones as in SCADA systems, or from simulations in order to manage their interdependencies. In the simulation case, the behaviour of CIs will be simulated by their native simulation approaches. IRRIIS simulates interdependencies taking these native CI simulations into account – using simulation federation based on the IRRIIS Generic Information Model. This paper gives an overview of the IRRIIS information model and the way it is used for the analysis of interdependent infrastructures.	action potential;decision support system;dependability;design rationale;financial risk modeling;it risk management;incidence matrix;information model;information system;integrated project support environment;interaction;interdependence;microsoft outlook for mac;simulation;strategic management	Ruediger Klein;Erich Rome;Césaire Beyel;Ralf Linnemann;Wolf Reinhardt;Andrij Usov	2008		10.1007/978-3-642-03552-4_4	semantic data model;information model;computer science;systems engineering;knowledge management;critical infrastructure;data mining;computer security;information system;dependence analysis;scada	Metrics	-54.05720695319339	19.59711306350362	5655
c2c5654847368fb1016ee50b68544ac9dba837d2	an approach to manage large inheritance networks within a dbs supporting nested relations	database system;knowledge based system;large scale;tree structure;relational database system;natural language processing;knowledge base	When developing large scale knowledge-based systems concepts are required for handling large knowledge bases as well as for guaranteeing the integrity of the knowledge base contents In this paper we present an approach for managing structured inheritance networks in an andvanced database system. Actually, we develop concepts for handling a KL-ONE like formalisms in an extended relational database system supporting nested relations. Of special importance is the handling of the general graph structure provided by KL-ONE like formalisms within the tree structure offered by nested relations. The described approach is oriented towards the specific requirements which have to be met in a natural language processing environment.		Rudi Studer;Stefan Börner	1987		10.1007/3-540-51171-7_30	natural language processing;relational database;computer science;data science;data mining;database schema;database design	DB	-31.266520152424203	10.860778185590425	5657
762a3dcd9cc180099cd2471cbda3187e478da2a8	a taxonomy of issues in name systems design and implementation	tolerancia falta;distributed system;systeme reparti;interrogation base donnee;interrogacion base datos;systematique;sistema repartido;sistematica;design and implementation;complex system;system design;fault tolerance;taxonomy;temporal properties;communication;comunicacion;database query;tolerance faute	In the last decade, name systems have grown from a single centrally-controlled server providing only host name to physical address mapping, to a complex system consisting of multiple and distributed servers, providing not only name mapping, but also general directory lookup services. These advances are due in part to the increase in size, complexity and heterogeneity of distributed systems. This paper presents a taxonomy of design and implementation issues in building a name system.	complex system;complexity;directory (computing);distributed computing;evolutionary taxonomy;lookup table;physical address;server (computing);taxonomy (general)	Ann-Kian Yeo;Akkihebbal L. Ananda;E. K. Koh	1993	Operating Systems Review	10.1145/155870.155872	root name server;fault tolerance;real-time computing;computer science;operating system;database;distributed computing;name server;arpa;domain name system;taxonomy;systems design	Networks	-28.768619631717634	45.69186285650611	5658
a806856348d2e458077e6dd507d4e880433108b1	belief logic programming with cyclic dependencies	expressive power;logic programs;expert system	Our previous work [26] introduced Belief Logic Programming (BLP), a novel form of quantitative logic programming with correlation of evidence. Unlike other quantitative approaches to logic programming, this new theory is able to provide accurate conclusions in the presence of uncertainty when the sources of information are not independent. However, the semantics defined in [26] is not sufficiently general—it does not allow cyclic dependencies among beliefs, which is a serious limitation of expressive power. This paper extends the semantics of BLP to allow cyclic dependencies. We show that the new semantics is backward compatible with the semantics for acyclic BLP and has the expected properties. The results are illustrated with examples of inference in a simple diagnostic expert system.	algorithm;backward compatibility;directed acyclic graph;expert system;expressive power (computer science);fixed point (mathematics);legendre transformation;logic programming;theory (mathematical logic)	Hui Wan	2009		10.1007/978-3-642-05082-4_11	higher-order logic;horn clause;stable model semantics;computer science;artificial intelligence;theoretical computer science;programming language;well-founded semantics;logic programming;expert system;expressive power;algorithm	AI	-15.776330454137275	10.0308825777655	5659
9371c0ad7d33fbf2ad68606bbef89d4c8cf018f9	numerical library reuse in parallel and distributed platforms	linear algebra;design model;code reusability;numerical method;scientific workflow;parallel and distributed computing;code reuse;object oriented;object oriented approach;numerical experiment;large scale distributed systems;numerical library	In the context of parallel and distributed computation, the currently existing numerical libraries do not allow code reuse. Besides, they are not able to exploit the multi-level parallelism offered by many numerical methods. A few linear algebra numerical libraries make use of object oriented approach allowing modularity and extensibility. Nevertheless, those which offer modularity together with sequential and parallel code reuse are almost non-existent. We analyze the lacks in existing libraries and propose a design model based on a component approach and the strict separation between computation operations, data definition and communication control of applications. We present then an implementation of this design using YML scientific workflow environment jointly with the object oriented LAKe (Linear Algebra Kernel) library. Some numerical experiments on GRID5000 platform validate our approach and show its efficiency.	code reuse;computation;data definition language;distributed computing;experiment;extensibility;library (computing);linear algebra;numerical method;parallel computing;yaml	Nahid Emad;Olivier Delannoy;Makarem Dandouna	2010		10.1007/978-3-642-19328-6_26	reusability;parallel computing;numerical analysis;computer science;theoretical computer science;linear algebra;distributed computing;programming language;object-oriented programming;algebra	HPC	-9.592365826402734	36.074057824110085	5671
2ded135dd81fa5dc18e6547004bca2434b660d29	r and relevance principle revisited	humanidades;filosofia etica	This paper first shows that some versions of the logic R of Relevance do not satisfy the relevance principle introduced by Anderson and Belnap, the principle of which is generally accepted as the principle for relevance. After considering several possible (but defective) improvements of the relevance principle, this paper presents a new relevance principle for (three versions of) R, and explains why this principle is better than the original and others.	relevance	Eunsuk Yang	2013	J. Philosophical Logic	10.1007/s10992-012-9247-1	philosophy;epistemology;calculus;mathematics	NLP	-11.855114058410921	4.293934907144519	5683
36ad6359c8dbddc697566f3d64dc0573b991d98d	classes of one-argument recursive functions		"""GRZEGORCZYK'S [l] sequence 8"""" (n 2 0) of increasing classes of functions has the following properties: 8 n + l contains gn, gn+l + &; each gn+l(x, x), where gn(x, y) is the function by which Bn is defined, majorizes the classe 8; of all the one argument functions in 8""""; US"""" is the class of all primitive recursive functions. RITCHIE has proved in 121 that 82, n 2 2, is the smallest class of functions containing s(~), E(x) , [x1j2] and fn(x, x) (ACKERMANN'S function), which is closed under addition, multiplication, composition and limited iteration. Here, we define and study a sequence Sn (n 2 0) of classes of one-argument functions the definition of which seems more simple than this of 8; and we prove that 3'n+1 = &gtl for n 2 2. In the first section below, we define the classes S,,, n 2 0, and develop some of their properties. In the second section, we consider $Po, S1 and di"""", more precisely. In the last section we establish the equality of 9'n+l and 8;+* for n 2 2."""	ackermann function;emoticon;iteration;primitive recursive function;recursion (computer science);recursive language	Nadejda V. Georgieva	1976	Math. Log. Q.	10.1002/malq.19760220117	combinatorics;discrete mathematics;recursion;mathematics	Theory	-5.49433944577716	14.819531042443987	5687
04d9169d77ae02c7b75bdf836d6ee3484122a340	designing a distributed queue	analytical model distributed queue distributed computing producer consumer model performance;queueing theory;distributed processing;performance;distributed computing;software performance evaluation;distributed queue;data structures;distributed computing analytical models stochastic processes buffer storage information science probes predictive models computational modeling parallel programming scheduling algorithm;producer consumer model;analytical model;software performance evaluation queueing theory data structures distributed processing	A common paradigm for distributed computing is the producer consumer model One set of processes produce objects that are consumed by another set of processes These objects might be data resources or tasks We present a simple algorithm for implementing a distributed queue This algorithm has several parameters that need to be tuned such as the number of probes to nd an object the amount of bu ering and the connectivity between the producers and the consumers We provide an analytical model that predicts performance and based on the analytical model we provide recommendations for setting the parameters Our analytical model is validated by a comparison to simulation results	algorithm;blocking (computing);centralized computing;distributed computing;elegant degradation;producer–consumer problem;programming paradigm;simulation;throughput;unbalanced circuit	Theodore Johnson	1995		10.1109/SPDP.1995.530699	distributed algorithm;parallel computing;real-time computing;data structure;performance;computer science;theoretical computer science;distributed computing;fork–join queue;distributed design patterns;queueing theory	Metrics	-16.844395637499183	58.72852860719817	5692
2c920df2f7cc076209b9ecfeddcbed8d3a15ee8e	a framework for verbalizing unconscious knowledge based on inductive logic programming	inductive logic programming;knowledge base		inductive logic programming	Koichi Furukawa	1995			natural language processing;knowledge base;description logic;statistical relational learning;computer science;artificial intelligence;machine learning;functional logic programming;programming paradigm;inductive programming;fifth-generation programming language;logic programming;algorithm;autoepistemic logic	AI	-19.10099534594095	11.331337306396017	5700
4744067e827a8a212d1cebec19f6fdc5cfed01cc	secure self-certified cots	malicious code detection;memory safety;information security;maintenance;program compilers security of data;contracts;technology planning;computer security;assembly;certifying compiler cots security certified code type safety memory safety malicious code detection secure self certified code;safety;certifying compiler;secure self certified code;safety computer science ip networks computer security information security costs maintenance technology planning contracts assembly;distributed systems security;ip networks;computer science;cots;program compilers;security;security of data;certified code;type safety	With the advent and the rising popularity of networks, Internet, intranets and distributed systems, security is becoming one of the major concerns in IT research. An increasing number of approaches have been proposed to ensure the safety and security of programs. Among those approaches, certified code seems to be the most promising. Unfortunately, as of today, most of the research on certified code have focused on simple type safety and memory safety, rather than security issues. We therefore propose to extend this approach to the security aspects of a program. Our intention is to use such an approach as an efficient and realistic solution to the problem of malicious code detection in COTS. In this paper, we present our progress in defining and implementing a certifying compiler that produces a secure self-certified code that can be used to ensure both safety and security of the code. 1. Motivation and Background Nowadays, there are many information infrastructures based on the so-called Commercial Off-The-Shelf (COTS) components. Actually, many organizations are undergoing a remarkable move from legacy systems towards COTSbased systems. The main motivation underlying such a migration is to take advantage of cutting-edge technologies and also to lower the program life-cycle costs of computer systems. Nevertheless, this migration phenomenon poses major and yet very interesting challenges to the currently established computer system technologies in terms of security, reliability, integration, interoperability, maintenance, planning, etc. This research is funded by a research contract from the Defense Research Establishment, Valcartier, DREV, Quebec, Canada. In our current research, we are concerned with malicious code that could exist in COTS software products. In a preliminary study of the domain [2], we considered different approaches suitable to address this problem. Among those approaches, certified code seems to be the most promising. Given a certified program provided by an untrusted source, a host can determine with certainty that this program may be safely executed. Unfortunately, as of today, most of the research on certified code have focused on simple type safety and memory safety, rather than security issues. We therefore propose to extend this approach to the security aspects of a program. In this paper, we present our progress in defining and implementing a certifying compiler that produces a secure self-certified code that can be used to ensure both the safety and security of the code. We used LCC [3], a C compiler, as the starting point for our certifying compiler to which we added a type system based on TALx86 [5]. The rest of the paper is organized as follows. Section 2 presents the certified code approach, which guarantees that COTS components may be safely executed. Section 3 focuses on the code generation process that produces annotated assembly code. Section 4 follows with the verification process, which aims at formally certifying that the generated annotated code satisfies well defined safety properties. Section 5 proposes an extension to both the generation and verification processes to include security properties. Section 6 presents related work. Finally, a few concluding remarks and a discussion of future research are ultimately sketched as a conclusion in Section 7.	assembly language;code generation (compiler);compiler;computer security;distributed computing;formal verification;internet;interoperability;intranet;intrusion detection system;legacy system;malware;memory safety;modal logic;model checking;planning;reliability engineering;system integration;system migration;type safety;type system	Mourad Debbabi;E. Giasson;Béchir Ktari;Frédéric Michaud;Nadia Tawbi	2000		10.1109/ENABL.2000.883726	memory safety;type safety;computer science;information security;software engineering;database;assembly;computer security	Logic	-58.057327661424495	45.435766212892496	5725
57887866ec16ec4fd1a0eb523fe9c27482565eac	using structural computing to support information integration	relationship management;software engineering;information integration	Software engineers face a difficult task in managing the many different types of relationships that exist between the documents of a software development project. We refer to this task as information integration, since establishing a relationship between two documents typically means that some part of the information in each document is semantically related. A key challenge in information integration is providing techniques and tools that manage and evolve these relationships over time. The structural computing domain provides a set of principles to derive new techniques and tools to help with these tasks of relationship management and evolution. We present a prototype information integration environment, InfiniTe, and describe how we are exploiting structural computing principles in the design of its infrastructure services.	customer relationship management;prototype;software development;software engineer	Kenneth M. Anderson;Susanne A. Sherba	2001		10.1007/3-540-45844-1_15	computer science;systems engineering;knowledge management;information integration;data mining;system integration	SE	-55.81382007037237	17.922785114839687	5729
58ccba64f14b68be50d12db8221e0224b23bd258	a target logical schema: the acs	conceptual schema;data structure	This extended abstract describes a data structure (ACS) intended to serve as a target for conceptual schema languages and as a source for implementations. It indicates some of the uses to which it has been put.	conceptual schema;data structure	Peter M. Stocker;R. Cantié	1983			data structure;logical schema;computer science;conceptual schema;document structure description;star schema;database;database schema	Theory	-31.687101679944124	9.285826419089757	5730
39201eb57e0615b557fa276cd3df1236a55e7ac1	an evaluation of set-associativity in two-level caches for shared memory multiprocessors	processor sharing;cache performance;simulation study;trace driven simulation;least recently used;shared memory multiprocessor	In this paper we study the behavior of a two level cache hierarchy for multiprocessors, where the processors share a common second level cache. Cache performance is studied as a function of set associativity of primary and second level caches, using trace driven simulations. The simulation uses numerical benchmarks compiled using a parallelizing and vectorizing compiler. Simulation study shows that a replacement scheme based on least recently used blocks gives better performance than a replacement scheme based on multilevel inclusion. Invalidations to ensure multilevel inclusion in the two level cache hierarchy do not significantly affect cache performance and can be further reduced by increasing set associativity of second level cache. Furthermore, the study shows that coherency traffic and cache interference are sensitive to set associativity of the cache. In spite of reduction in coherency traffic, improvement in performance with set associativity is largely due to increase in hit ratio of primary and second level caches.	shared memory	Senthil Krishnamoorthy;Alok N. Choudhary	1992		10.1007/3-540-55599-4_122	bus sniffing;uniform memory access;distributed shared memory;shared memory;cache coherence;computer architecture;parallel computing;real-time computing;distributed memory;data diffusion machine;cache-only memory architecture;memory map;non-uniform memory access	HPC	-10.361667717370851	50.28751740498546	5734
c7d7ed5b75c82965c4b2e5aabf298c1d5df67806	large-eddy simulations of turbulent flows, from desktop to supercomputer	large eddy sim ulation;large eddy simulation;turbulent flow	In this paper, a general introduction to the large-eddy simulation (LES) technique will be given. Modeling and numerical issues that are under study will be described to illustrate the capabilities and requirements of this techniques. A palette of applications will then be presented, chosen on the basis both of their scientific and technological importance, and to highlight the application of LES on a range of machines, with widely different computational capabilities.	computation;computer scientist;computer simulation;desktop computer;large eddy simulation;numerical analysis;palette (computing);requirement;strong generating set;supercomputer;throughput;turbulence;unstructured grid;workstation	Ugo Piomelli;Alberto Scotti;Elias Balaras	2000		10.1007/3-540-44942-6_45	turbulence;computational science;simulation;computer science;theoretical computer science;large eddy simulation	HPC	-7.356576208557215	37.64752801531736	5743
c3402a2f9396067cf545b3df34995f979e94bcf0	functional tests of the radic fault tolerance architecture	radicmpi;fully distributed structure;fault tolerant;functional testing;message passing system;programming model;fault tolerant computing;parallel architectures fault tolerant computing message passing;parallel architectures;message passing;radic fault tolerance architecture;functional tests;radicmpi functional tests radic fault tolerance architecture message passing programming model fully distributed structure;testing fault tolerance fault tolerant systems message passing frequency scalability distributed control control systems collaboration protocols	Clusters with thousand of nodes are a reality and the current trend indicates that they are becoming larger. Such large clusters are subject to a relatively high fault frequency so a fault-tolerance scheme is mandatory to assure the correct application completion. Message passing is the programming model often used in large clusters and the current implementations used to achieve fault tolerance in message passing systems do not focus in an architecture that simultaneously attends to scalability, transparency and independence of stable/central elements. The RADIC architecture was proposed and design as a fully distributed structure in order to achieve such requirements. Such architecture defines a fully distributed fault tolerance controller implemented by a set of system processes, which collaborate in order to perform all the basic functions of a fault tolerance protocol. This paper presents the test methodology used to verify the functionality of the RADIC architecture using RADICMPI, a prototype on the MPI semantic	blocking (computing);code;compiler;computation;correctness (computer science);fault tolerance;mpich;matrix multiplication;message passing interface;messaging pattern;mike lesser;multiplication algorithm;node (computer science);non-blocking algorithm;overhead (computing);programmer;programming model;prototype;requirement;runtime system;scalability;transaction processing system;transparency (graphic)	Angelo Duarte;Dolores Rexachs;Emilio Luque	2007	15th EUROMICRO International Conference on Parallel, Distributed and Network-Based Processing (PDP'07)	10.1109/PDP.2007.45	parallel computing;message passing;real-time computing;computer science;stuck-at fault;functional testing;distributed computing;programming language;general protection fault;software fault tolerance	HPC	-13.440589118464253	43.591385814924635	5745
183538279d77655b9f8074ffd67aa5a3706657d5	formal validation of data parallel programs: introducting the assertional approach	data parallel;weakest precondition	We present a proof system for a simple data parallel kernel language. This proof system is based on two-component assertions, where the current extent of parallelism is explicitly described. We define a weakest preconditions (WP) calculus and discuss the associated definability property. Thanks to this weakest preconditions calculus, we establish the completeness of the proof system. We finally discuss other approaches.		Luc Bougé;David Cachera;Yann Le Guyadec;Gil Utard;Bernard Virot	1996		10.1007/3-540-61736-1_51	parallel computing;computer science;distributed computing;predicate transformer semantics;programming language	HPC	-14.398158607393693	19.532078431633174	5763
8162a8a06adf687479a08bdf90de624d7bccbf44	calana: a general-purpose agent-based grid scheduler	grid scheduling;software agents grid computing resource allocation scheduling;agent based;resource allocation;bidding strategies;job shop scheduling grid computing processor scheduling computational modeling resource management information systems resource virtualization computer industry computer architecture software agents;software agents;scheduling;information system;grid computing;resource provider calana agent based grid scheduler grid resource allocation centralized information system lightweight scheduling system	Grid resource allocation is a complex task usually solved by systems relying on a centralized information system. In order to create a lightweight scheduling system, we investigated the potential of auctions for resource allocation. Each resource provider runs an agent bidding on the execution of software with respect to local restrictions. This way, the information system becomes obsolete. In addition, each provider can implement different bidding strategies in order to reflect his preferences.	agent-based model;centralized computing;general-purpose markup language;grid computing;information system;scheduling (computing)	Mathias Dalheimer;Franz-Josef Pfreundt;Peter Merz	2005	HPDC-14. Proceedings. 14th IEEE International Symposium on High Performance Distributed Computing, 2005.	10.1109/HPDC.2005.1520974	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;dynamic priority scheduling;resource allocation;computer science;software agent;operating system;two-level scheduling;database;distributed computing;scheduling;human resource management system;information system;drmaa;grid computing	HPC	-29.16178677127521	49.817571844315374	5771
3ea561441adb63642db47c0759794823838ef08a	dynamic binding and scheduling of firm-deadline tasks on heterogeneous compute resources	heterogeneous compute resources;rtos;heterogeneous task sets;heterogeneity complicates scheduling;processor scheduling mathematical model real time systems dynamic scheduling program processors computational modeling;multiprocessor;heterogeneous computing;processor scheduling;earliest deadline first;heterogeneous real time scheduling method;real time;task execution time;embedded system;firm deadline task scheduling;task analysis embedded systems scheduling;multi resource;dynamic binding;embedded systems;computational modeling;homogeneous multiresource system;heterogeneous;deadline miss rates firm deadline task scheduling task binding heterogeneous compute resources embedded system homogeneous multiresource system heterogeneity complicates scheduling task execution time heterogeneous real time scheduling method heterogeneous task sets;deadline miss rates;task binding;scheduling;task analysis;real time scheduling;mathematical model;multi resource rtos real time scheduling multi core multiprocessor heterogeneous;program processors;algorithm design;multi core;dynamic scheduling;real time systems;least laxity first	Embedded systems increasingly include heterogeneous compute resources. Yet the vast majority of real-time scheduling methods are designed for single-resource or homogeneous multi-resource systems. Heterogeneity complicates scheduling; task execution time is resource-dependent. Furthermore, the best resource for one task may not necessarily be the best resource for all tasks, so one resource may not be universally more valuable than another. This paper presents new algorithms designed specifically for heterogeneous real-time scheduling. We evaluate the algorithms’ deadline miss rates for heterogeneous task sets that represent a variety of execution scenarios, and show that two of our algorithms have lower deadline miss rates than the Earliest Deadline First or Least Laxity First approaches. We also discuss how task set and system characteristics affect the schedulers’ abilities to achieve a quality schedule.	algorithm;earliest deadline first scheduling;embedded system;late binding;real-time clock;real-time operating system;real-time transcription;run time (program lifecycle phase);scheduling (computing)	Hsiang-Kuo Tang;Kyle Rupnow;Parameswaran Ramanathan;Katherine Compton	2010	2010 IEEE 16th International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2010.29	multi-core processor;algorithm design;parallel computing;real-time computing;earliest deadline first scheduling;multiprocessing;real-time operating system;dynamic priority scheduling;computer science;operating system;mathematical model;task analysis;distributed computing;computational model;scheduling;symmetric multiprocessor system	Embedded	-10.056838558339525	58.79202608253796	5774
ab97968e80677fcb412d5501be9f8cd826011a14	a secure group-oriented framework for intelligent virtual environments	intelligent virtual environments;multiagent systems	In this paper a Multiagent System for Intelligent Virtual Environments using the Magentix Multiagent Platform is presented. It is based on a previous framework which has been improved with agent groups, security and efficiency concerns. Therefore, the framework presented can be used in common Intelligent Virtual Environment domains such as education, commercial games and simulation.	agent-based model;authentication;complex system;map;message passing;multi-agent system;performance evaluation;programmer;scalability;simulation;virtual reality;virtual world;world wide web	Jose M. Such;Juan M. Alberola;Antonio Barella;Ana García-Fornes	2011	Computing and Informatics		simulation;computer science;knowledge management;artificial intelligence;multi-agent system;distributed computing	AI	-39.41931281738981	20.594981058304047	5776
0c3495c25fb1cbc58627de8502943adb98430492	step-indexing: the good, the bad and the ugly	logical relation;004;program transformation;indexation;source language;step indexing logical relations low level languages compiler correctness	Over the last decade, step-indices have been widely used for the construction of operationally-based logical relations in the presence of various kinds of recursion. We first give an argument that stepindices, or something like them, seem to be required for defining realizability relations between high-level source languages and lowlevel targets, in the case that the low-level allows egregiously intensional operations such as reflection or comparison of code pointers. We then show how, much to our annoyance, step-indices also seem to prevent us from exploiting such operations as aggressively as we would like in proving program transformations.	computational model;high- and low-level;intensional logic;logical relations;program transformation;recursion	Nick Benton;Chung-Kil Hur	2010			computer science;theoretical computer science;programming language;algorithm	PL	-19.030282064365956	22.533179951352476	5783
de229a559a29e44437368f71c5b533102aec2b2f	astrolabe: a grid operating environment with full-fledged usability	grid portal;server side components;grid operating environment;jsr 168 specification;client server architecture;grid computing environment;astrolabe;dynamic portlet container;client server systems;null;workflow management software client server systems grid computing problem solving;eclipse rcp;workflow management software;eclipse rcp grid operating environment problem solving environments feature rich grid operating environment client server architecture astrolabe dynamic portlet container local resource accessability jsr 168 specification workflow designer server side components;feature rich grid operating environment;workflow designer;grid computing;problem solving environment;usability grid computing collaborative work portals containers packaging computers problem solving runtime research and development;problem solving;problem solving environments;local resource accessability	Based on a comprehensive analysis of existing grid portals and problem-solving environments, Astrolabe, a feature-rich grid operating environment built on a fine- tuned client/server architecture is presented in this paper. In Astrolabe, a dynamic portlet container is deployed at the client side to provide portlets with more powerful runtime capabilities such as rich interfaces, local resource access- ability and offline usage, comparing to the JSR-168 specification. A workflow designer is built upon the portlet container to provide integrated workflow modeling capability. Moreover, server-side components are also employed to enable portability and collaboration among users. The main contribution of this paper is bringing the aforementioned features as a full package, which could improve the usability of current grid computing environments. Eclipse RCP is utilized for the implementation of Astrolabe to leverage the state of the art of rich client technologies.	astrolabe;client-side;client–server model;eclipse;fat client;grid computing;java portlet specification;online and offline;operating environment;portals;problem solving;server (computing);server-side;software feature;usability	Hailue Lin;Kun Chen;Xiaohui Yan	2007	Sixth International Conference on Grid and Cooperative Computing (GCC 2007)	10.1109/GCC.2007.46	computer science;operating system;database;distributed computing;law;world wide web;client–server model;grid computing	HPC	-35.62605619428668	42.08665926248536	5789
6f0872b50a43da4988eb1acf9bffc095dd52fdf1	semantic foundations for embedding hol in nuprl	theorem prover;type theory;functional programming language	We give a new semantics for Nuprl's constructive type theory that justiies a useful embedding of the logic of the HOL theorem prover inside Nuprl. The embedding gives Nuprl eeective access to most of the large body of formalized mathematics that the HOL community has amassed over the last decade. The new semantics is dramatically simpler than the old, and gives a novel and general way of adding set-theoretic equivalence classes to untyped functional programming languages.	automated theorem proving;functional programming;hol (proof assistant);intuitionistic type theory;nuprl;programming language;set theory;turing completeness	Douglas J. Howe	1996		10.1007/BFb0014309	computer science;automated theorem proving;programming language;functional programming;type theory;algorithm	PL	-14.626728158655194	18.170600581010103	5797
6ab0e6a87aae4241adad5e5b7e28ce0ac0336b61	an approach for developing natural language interface to databases using data synonyms tree and syntax state table	natural language;natural language interface	The basic idea addressed in this research is developing a generic, dynamic, and domain independent natural language interface to databases. The approach consists of two phases; configuration phase and operation phase. The former builds data synonyms tree based on the database being implemented. The idea behind this tree is matching the natural language words with database elements. The tree hierarchy contains the database tables, attributes, attribute descriptions, and all possible synonyms for each description. The latter phase contains a technique that implements syntax state table to extract the SQL components from the natural language user request. As a result the corresponding SQL statement is generated without interference of human experts. Index-terms: Natural language processing, Natural language interface, Database interface, Synonyms tree, Syntax table.	natural language user interface	Safwan Shatnawi;Rajeh Khamis	2009		10.1007/978-90-481-9112-3_87	natural language processing;language identification;interface description language;data definition language;natural language programming;data manipulation language;natural language user interface;data control language;computer science;database;programming language;abstract syntax tree	ML	-32.49363486919871	8.62103353091067	5802
bf19cbff2a0ef66bf3f759b1fb2e04e0a8a14335	performance/energy efficiency of variable line-size caches for intelligent memory systems	on chip bus;energy efficient;cache memory;chip;memory access;memory systems;spatial locality;point of view;memory bandwidth	Integrating main memory (DRAM) and processors into a single chip, or merged DRAM/logic LSI, makes it possible to exploit high on-chip memory bandwidth by widening on-chip bus and on-chip DRAM array. In addition, from energy consumption point of view, the integration brings a significant improvement by decreasing the number of off-chip accesses. For merged DRAM/logic LSIs having on-chip cache memory, we can exploit the high bandwidth by means of replacing a whole cache line at a time. This approach tends to increase the cache-line size if we attempt to exploit the attainable high bandwidth. A large cache-line size gives a benefit of prefetching effect if programs have rich spatial locality. Otherwise, however, it will bring the following disadvantages due to poor spatial locality:	cpu cache;central processing unit;computer data storage;dynamic random-access memory;locality of reference;memory bandwidth;point of view (computer hardware company);principle of locality	Koji Inoue;Koji Kai;Kazuaki Murakami	2000		10.1007/3-540-44570-6_13	uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;sense amplifier;memory refresh;computer hardware;physical address;computer memory;memory controller;conventional memory;extended memory;flat memory model;registered memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-8.284280156652462	53.506984610638895	5815
3e6484f66f7df853dfd97d3e165aae565067d5bf	faceted search over ontology-enhanced rdf data	owl 2;faceted search;algorithms;sparql;ontology;rdf	An increasing number of applications rely on RDF, OWL 2, and SPARQL for storing and querying data. SPARQL, however, is not targeted towards end-users, and suitable query interfaces are needed. Faceted search is a prominent approach for end-user data access, and several RDF-based faceted search systems have been developed. There is, however, a lack of rigorous theoretical underpinning for faceted search in the context of RDF and OWL 2. In this paper, we provide such solid foundations. We formalise faceted interfaces for this context, identify a fragment of first-order logic capturing the underlying queries, and study the complexity of answering such queries for RDF and OWL 2 profiles. We then study interface generation and update, and devise efficiently implementable algorithms. Finally, we have implemented and tested our faceted search algorithms for scalability, with encouraging results.	automated planning and scheduling;data access;faceted classification;first-order logic;first-order reduction;resource description framework;sparql;scalability;search algorithm;web ontology language	Marcelo Arenas;Bernardo Cuenca Grau;Evgeny Kharlamov;Sarunas Marciuska;Dmitriy Zheleznyakov	2014		10.1145/2661829.2662027	rdf/xml;computer science;sparql;artificial intelligence;rdf;ontology;database;web ontology language;world wide web;information retrieval;rdf schema	Web+IR	-34.530242082375246	4.288431611712973	5819
c415ffd624a30d82283e8fb655c7abd7599118b5	trace-based simulation of message passing parallel programs.	message passing;parallel programs			W. D. Garrett;William E. Cohen;Rhonda Kay Gaede	2000			parallel computing;message passing;computer science;distributed computing;programming language	Arch	-10.388313836349587	42.62863192648775	5823
44dda77aa68120e4f87a0638081c9dfe23687f8e	better debugging via output tracing and callstack-sensitive slicing	debugging;output tracing and attribution;point of failure tools;computer crashes;software reliability program debugging program slicing;slice intersection;testing;failure analysis;slice intersection callstack sensitive slicing program debugging program slicing point of failure tools buggy programs;sensitivity analysis;programming profession;static program slicing;buggy programs;output tracing and attribution static program slicing callstack sensitive analysis points of failure;linux;program debugging;points of failure;program slicing;callstack sensitive analysis;callstack sensitive slicing;software reliability;debugging programming profession computer crashes failure analysis testing linux	Debugging often involves 1) finding the point of failure (the first statement that produces bad output) and 2) finding and fixing the actual bug. Print statements and debugger break points can help with step 1. Slicing the program back from values used at the point of failure can help with step 2. However, neither approach is ideal: Debuggers and print statements can be clumsy and time-consuming and backward slices can be almost as large as the original program. This paper addresses both problems. We present callstack-sensitive slicing, which reduces slice sizes by leveraging the series of calls active when a program fails. We also show how slice intersections may further reduce slice sizes. We then describe a set of tools that identifies points of failure for programs that produce bad output. Finally, we apply our point-of-failure tools to a suite of buggy programs and evaluate callstack-sensitive slicing and slice intersection as applied to debugging. Callstack-sensitive slicing is very effective: On average, a callstack-sensitive slice is about 0.31 time the size of the corresponding full slice, down to just 0.06 time in the best case. Slice intersection is less impressive, on average, but may sometimes prove useful in practice.	best, worst and average case;call stack;crash (computing);debugger;debugging;overhead (computing);programmer;reliability engineering	Susan Horwitz;Ben Liblit;Marina Polishchuk	2010	IEEE Transactions on Software Engineering	10.1109/TSE.2009.66	failure analysis;program slicing;real-time computing;computer science;operating system;software engineering;software testing;programming language;debugging;sensitivity analysis;linux kernel;software quality	SE	-60.44175420120229	36.80210784355971	5830
c168db52f767e763ba0899fc93d0d08e4b6ad250	distributed resolution for expressive ontology networks	004 informatik;web ontology language;parallel computer;semantic web;description logic	The Semantic Web is commonly perceived as a web of partially interlinked machine readable data. This data is inherently distributed and resembles the structure of the web in terms of resources being provided by different parties at different physical locations. A number of infrastructures for storing and querying distributed semantic web data, primarily encoded in RDF have been developed but almost all the work on description logic reasoning as a basis for implementing inference in the Web Ontology Language OWL still assumes a centralized approach where the complete terminology has to be present on a single system and all inference steps are carried out on this system. We propose a distributed reasoning method that preserves soundness and completeness of reasoning under the original OWL import semantics. The method is based on resolution methods for ALCHIQ ontologies that we modify to work in a distributed setting. Results show a promising runtime decrease compared to centralized reasoning and indicate that benefits from parallel computation trade off the overhead caused by communication between the local reasoners.	centralized computing;computation;description logic;human-readable medium;ontology (information science);overhead (computing);parallel computing;resolution (logic);semantic web;web ontology language	Anne Schlicht;Heiner Stuckenschmidt	2009		10.1007/978-3-642-05082-4_7	web service;knowledge representation and reasoning;web modeling;description logic;semantic web rule language;data web;ontology inference layer;computer science;semantic reasoner;ontology;artificial intelligence;theoretical computer science;semantic web;social semantic web;data mining;semantic web stack;database;programming language;web ontology language;owl-s	AI	-36.66691865093796	5.639650675557722	5834
605f234c8bfb372e86abe0237e74258dc81f9a7f	formal development of an embedded verifier for java card byte code	software metrics;formal specification;java card;formal model;java virtual machine;b method;formal method;embedded systems;formal verification;smart cards;automatic code translation embedded verifier java card byte code b method java card language metrics formal development;java smart cards virtual machining protection information systems data security error correction codes prototypes;security policy;security of data;software metrics formal specification formal verification java security of data smart cards embedded systems;java	The Java security policy is implemented by security components such as the Java Virtual Machine (JVM), the API, the verifier, the loader. It is of prime importance to ensure that the implementation of these components is in accordance with their specifications. Formal methods can be used to bring the mathematical proof that the implementation of these components corresponds to their specification. In this paper, we introduce the formal development of a complete byte code verifier for Java Card and its on-card integration. In particular, one aims to focus on the model and the proof of the complete type verifier for the Java Card language. The global architecture of the verification process implemented in this real industrial case study is described and the detailed specification of the type verifier is discusses as well as its proof. Moreover, this paper presents a comparison between a formal and a traditional development and concludes on the benefits and the inconvenient of using formal methods in industry.	application programming interface;byte;embedded system;formal methods;high- and low-level;java card;java virtual machine;overhead (computing);prototype;reference implementation;requirement;smart card;software bug	Ludovic Casset;Lilian Burdy;Antoine Requet	2002		10.1109/DSN.2002.1028886	b-method;smart card;java card;real-time computing;formal methods;jsr 94;formal verification;computer science;security policy;operating system;formal specification;programming language;java;software metric;java annotation	PL	-54.85631834094417	54.284692182807945	5849
1e563f5fe7269abf2f0e9cfd6259c1a13c32036f	reasoning on the web with assumption-based argumentation		This tutorial provides an overview of computational argumentation, focusing on abstract argumentation and assumption-based argumentation, how they relate, as well as possible uses of the latter in Web contexts, and in particular the Semantic Web and Social Networks. The tutorial outlines achievements to date as well as (some) open issues.	aba problem;description logic;semantic web;world wide web	Francesca Toni	2012		10.1007/978-3-642-33158-9_10	artificial intelligence;theoretical computer science;mathematics;algorithm	AI	-21.544700137919172	8.136651212175728	5859
9d9afd5e0d5a10530118b32f7cc94f56422c44a0	a modeling framework for developing networked agents applications	multiagent system;multi agent system;state revision;sensor network application networked agent application multi agent system agent collective task agent architecture agent continuous state agent discrete state;sensor network;receivers;computer architecture;multi agent systems;state revision networked agents agent architecture hybrid state representation;vectors;lead;batteries;transmitters;agent architecture;hybrid state representation;vectors transmitters multiagent systems lead batteries receivers computer architecture;multiagent systems;networked agents	This paper presents a modeling framework for developing multi-agent systems composed by networked identical agents that interact to perform a collective task. An agent architecture for addressing this kind of distributed problems is presented. Then the component devoted to represent and revise the agent's hybrid (continuous and discrete) state is detailed. The framework is illustrated using a case study dealing with a sensor network application.	agent architecture;dynamical system;interaction;multi-agent system	Berenice Gudiño-Mendoza;Ernesto López-Mellado	2011	2011 8th International Conference on Electrical Engineering, Computing Science and Automatic Control	10.1109/ICEEE.2011.6106588	control engineering;real-time computing;engineering;distributed computing	Robotics	-39.704375363154156	21.28760268103953	5864
87a81ef03a6bf222c6d12ebcdb798646c4a5ccb0	performance characteristics of hybrid mpi/openmp scientific applications on a largescale multithreaded bluegene/q supercomputer	hybrid mpi openmp;multithreaded;performance analysis;bluegene q	Many/multi-core supercomputers provide a natural programming paradigm for hybrid MPI/OpenMP scientific applications. In this paper, we investigate the performance characteristics of five hybrid MPI/OpenMP scientific applications (two NAS Parallel benchmarks Multi-Zone SP-MZ and BT-MZ, an earthquake simulation PEQdyna, an aerospace application PMLB and a 3D particle-in-cell application GTC) on a large-scale multithreaded BlueGene/Q supercomputer at Argonne National laboratory, and quantify the performance gap resulting from using different number of threads per node. We use performance tools and MPI profile and trace libraries available on the supercomputer to analyze and compare the performance of these hybrid scientific applications with increasing the number OpenMP threads per node, and find that increasing the number of threads to some extent saturates or worsens performance of these hybrid applications. For the strong-scaling hybrid scientific applications such as SP-MZ, BT-MZ, PEQdyna and PLMB, using 32 threads per node results in much better application efficiency than using 64 threads per node, and as increasing the number of threads per node, the FPU percentage decreases, and the MPI percentage (except PMLB) and IPC per core (except BT-MZ) increase. For the weak-scaling hybrid scientific application such as GTC, the performance trend (relative speedup) is very similar with increasing number of threads per node no matter how many nodes (32, 128, 512) are used.	blue gene;extensible authentication protocol;floating-point unit;haplogroup bt;image scaling;library (computing);message passing interface;multi-core processor;nas parallel benchmarks;openmp;particle-in-cell;programming paradigm;scalability;sharp mz;simulation;speedup;supercomputer;thread (computing)	Xingfu Wu;Valerie E. Taylor	2013	IJNDC	10.2991/ijndc.2013.1.4.3	computational science;computer architecture;parallel computing	HPC	-5.81853098251013	39.69218702340026	5879
f22887a5aefd50676dc6297835e5b197cad0a080	defining a collaborative platform to report machine state		Nowadays, we are seeing the evolution of Industry, and with it, the development of technological solutions that can assure sustainability and competitiveness in the manufacturing environment. Along this evolution, Cyber Physical Systems were developed with the goal to merge both physical and computational processes and to allow predictive, proactive and collaborative maintenance of industrial machines. The work here presented has been integrated in the Cyber Physical System based Proactive Collaborative Maintenance (MANTIS) project and has the main goal of proposing a collaborative platform to report current machine state. This way, it will be possible to facilitate and support the interaction between all stakeholders that will participate in the collaborative decision-making process. With this approach we believe to be possible to reduce machine down-time and the unnecessary waste of machine components and workhand while attempting to solve different machine problems.	industry 4.0;international symposium on fundamentals of computation theory;item unique identification;mantis bug tracker	Diogo Martinho;João Carneiro;Asif Mohammed;Ana Vieira;Isabel Praça;Goreti Marreiros	2018		10.1007/978-3-319-77712-2_26	systems engineering;merge (version control);group decision-making;cyber-physical system;multi-agent system;sustainability;engineering	AI	-61.6069934937676	12.248205463697351	5882
264a1ba0c92d976f6c6e9523b7e6a05b0446614d	a mobile and portable trusted computing platform	signal image and speech processing;information systems applications incl internet;communications engineering networks	The mechanism of establishing trust in a computing platform is tightly coupled with the characteristics of a specific machine. This limits the portability and mobility of trust as demanded by many emerging applications that go beyond the organizational boundaries. In order to address this problem, we propose a mobile and portable trusted computing platform in a form of a USB device. First, we describe the design and implementation of the hardware and software architectures of the device. We then demonstrate the capabilities of the proposed device by developing a trusted application.	software architecture;software portability;trusted computing	Surya Nepal;John Zic;Dongxi Liu;Julian Jang	2011	EURASIP J. Wireless Comm. and Networking	10.1186/1687-1499-2011-75	embedded system;direct anonymous attestation;computer science;operating system;distributed computing;computer security;trusted client;computer network	Mobile	-47.32368107764985	56.780050731091926	5890
c5828c166e00214b494b404059ca304303fe1587	software-level scheduling to exploit non-uniformly shared data cache on gpgpu	gpgpu;scheduling;cache performance	Data cache is introduced to GPUs to mitigate the irregular memory access problem. But few studies have investigated how to exploit its full potential. In this work, we consider some important GPU applications that feature data sharing across thread blocks. We show that the sharing is not well exploited because current GPU runtime ignores such a factor when scheduling threads. We then present an application-level transformation to remap thread blocks to data on the fly. With the software-level scheduler, thread blocks with much data sharing are scheduled to share the cache on a streaming multiprocessor (SM). Experiments on four benchmarks show 1.23X speedup on average.	benchmark (computing);cpu cache;feature data;general-purpose computing on graphics processing units;graphics processing unit;multiprocessing;on the fly;scheduling (computing);speedup;thread pool	Bo Wu;Weilin Wang;Xipeng Shen	2013		10.1145/2492408.2492421	parallel computing;real-time computing;cache;computer science;operating system;cache algorithms;cache pollution	Arch	-9.739134680758413	51.572244489039726	5905
c4ddb02a74a262945f81035f9e968b3f882e435e	process models for the software development and performance engineering tasks	risk analysis;performance related risks;development process;software performance engineering;research paper;object oriented;software development;performance analysis;performance model;process model;performance modeling	"""This research paper investigates and evaluates the currently available process models for software development with respect to representation of software performance engineering (SPE) tasks. The deficits identified point to the need to expand existing process models to include this task. On the basis of an process model for object-oriented development, a so called PM-OOPE is proposed to illustrate the subject, which allocates necessary SPE activities to the development process in a phase-related manner. """"Support processes"""" required to implement these activities are also identified. The support processes take account of the necessary organisational framework, identification of the resources required for SPE activities based on risk analysis, applicable SPE methods, and performance related informations needed to carry out the task."""	it risk management;performance engineering;process modeling;software development	Andreas Schmietendorf;Evgeni Dimitrov;Reiner R. Dumke	2002		10.1145/584369.584400	personal software process;simulation;risk analysis;software engineering process group;performance engineering;computer science;systems engineering;engineering;package development process;software development;software engineering;process modeling;object-oriented programming;empirical process;goal-driven software development process;software development process	SE	-56.667783298272965	22.705231201500442	5924
547faff7bb0a011205b2d94d71f5937d4ba20026	multi-agent based cargo auction	zinātniskās publikācijas;rīgas tehniskā universitāte;multi agent system;automated marketplace;auction protocols;izdevums rtu zinātniskie raksti;multi agent negotiation mechanisms;rtu;cargo transportation and logistics	AbstractThe paper presents a mechanism and an implemented tool for multi-agent based cargo auctions. The auction automates negotiations between a particular client and multiple logistics companies. The user can define his priorities in terms of six different criteria and find the most appropriate offer as a result of an auction. The developed marketplace is part of the eINTERASIA e-logistics portal and, thus, available online for both the client and the logistics companies.		Egons Lavendelis;Janis Grundspenkis	2015	Appl. Comput. Syst.	10.1515/acss-2015-0006	eauction;marketing;business;commerce	Theory	-51.97849382162289	12.335803047361043	5935
357611fbcf20ac0655219f33ce31770fef86ff59	bpa bisimilarity is exptime-hard	basic process algebra;bisimilarity;computational complexity	Given a basic process algebra (BPA) and two stack symbols, the BPA bisimilarity problem asks whether the two stack symbols are bisimilar. We show that this problem is EXPTIME-hard.	bisimulation;exptime;oracle bpa suite;process calculus	Stefan Kiefer	2013	Inf. Process. Lett.	10.1016/j.ipl.2012.12.004	combinatorics;discrete mathematics;computer science;mathematics;computational complexity theory;algorithm	DB	-5.061758295666979	23.05727710650297	5944
757cba91f3213f3a6dafb427381ee8d17ad61e59	decision algorithms for fragments of real analysis. i. continuous functions with strict convexity and concavity predicates	canonical model;satisfiability;decision problem	In this paper we address the decision problem for a fragment of unquantified formulae of real analysis, which, besides the operators of Tarski’s theory of reals, includes also strict and non-strict predicates expressing comparison, monotonicity, concavity, and convexity of continuous real functions over possibly unbounded intervals. The decision result is obtained by proving that a formula of our fragment is satisfiable if and only if it admits a parametric “canonical” model, whose existence can be tested by solving a suitable unquantified formula, expressed in the decidable language of Tarski’s theory of reals and involving the numerical variables of the initial formula plus various other parameters. This paper generalizes a previous decidability result concerning a more restrictive fragment in which predicates relative to infinite intervals or stating strict concavity and convexity were not expressible. c © 2006 Elsevier Ltd. All rights reserved.	algorithm;canonical model;concave function;convex function;decision problem;numerical analysis;recursive language;strict function	Domenico Cantone;Gianluca Cincotti;Giovanni Gallo	2006	J. Symb. Comput.	10.1016/j.jsc.2006.02.003	combinatorics;mathematical analysis;discrete mathematics;decision problem;canonical model;mathematics;algebra;satisfiability	Logic	-8.530013316597465	16.28387628045217	5945
4b67822f15b32fb1692585eb5e89cb1928cbd970	web service indexing for efficient retrieval and composition	syntactic matching;web service discovery;web services indexing information retrieval;availability;web and internet services;information retrieval;web service indexing;web service;runtime;graph traversing web service indexing web service discovery web service composition syntactic matching semantic matching;web service composition;web services indexing ontologies availability runtime costs web and internet services xml nonvolatile memory;indexing;nonvolatile memory;indexation;web services;xml;graph traversing;ontologies;service discovery;semantic matching	The success and widespread use of Web service technology is bound to the availability of tools for rapidly discovering services and for composing their operations. We present a system for service discovery and composition based on syntactic and semantic matching of the published service interfaces. The system relies on accurate preprocessing of the available services, that is, on the indexing time of the services' descriptions. The composition task is performed by index lookup and an efficient heuristic for graph traversing. The presented system is the VitaLab entry for the syntactic and semantic WS-challenge 2006	heuristic;lookup table;preprocessor;semantic matching;service discovery;web service	Marco Aiello;Christian Platzer;Florian Rosenberg;Huy Tran;Martin Vasko;Schahram Dustdar	2006	The 8th IEEE International Conference on E-Commerce Technology and The 3rd IEEE International Conference on Enterprise Computing, E-Commerce, and E-Services (CEC/EEE'06)	10.1109/CEC-EEE.2006.96	web service;computer science;database;world wide web;information retrieval	Robotics	-40.48966813073466	8.191679944539091	5958
8bc5ccb4afe5cba4777289474ebf4030c8fabf4d	an adaptive casteship mechanism for developing multi-agent systems	modelizacion;metodo adaptativo;multiagent system;caste;ciclo desarrollo;caste determination;agent based systems;multi agent system;life cycle;semantica formal;adaptive agents;mas;methode adaptative;intelligence artificielle;formal semantics;systeme adaptatif;semantique formelle;adaptive casteship;modelisation;multi agent systems;determinismo casta;agent intelligent;adaptive method;mas design;adaptive system;intelligent agent;cycle developpement;sistema adaptativo;artificial intelligence;agente inteligente;inteligencia artificial;sistema multiagente;mas modelling;modeling;systeme multiagent;determinisme caste	In this paper, we propose an adaptive casteship mechanism for modelling and designing adaptive Multi-Agent Systems (MAS). In our approach, caste is the modular unit and abstraction that specify agents’ behaviour. Adaptive behaviours of agents are captured as the change of castes during their lifecycles by executing ‘join’, ‘quit’, ‘activate’ and ‘deactivate’ operations on castes. The formal semantics of caste operations are rigorously defined. The properties of agent’s adaptive behaviours are formally specified and proved. A graphical notation of caste transition diagrams and a number of rules for check consistency are designed. An example is also presented throughout the paper.	adaptive grammar;agent-oriented programming;agent-oriented software engineering;complex systems;diagram;enterprise information system;graphical user interface;high-level programming language;internet;join (sql);linearizability;mobile agent;mobile computing;multi-agent system;self-management (computer science);self-organization;semantics (computer science);software system;temporal logic	XinJun Mao;Lijun Shan;Hong Zhu;Ji Wang	2008	IJCAT	10.1504/IJCAT.2008.017716	biological life cycle;simulation;systems modeling;computer science;engineering;artificial intelligence;multi-agent system;formal semantics	AI	-39.57270067660308	24.824072875974455	5968
fd5ecb170f955c3dc9215a0847e84208b941c412	amalgamating language and meta-language for composing logic programs.	logic programs	"""Logic programming is extended with expressions of the form A in Pexp both in top-level goals and in clause bodies. A in Pexp is a meta-level feature that denotes the truth of a formula A with respect to a \virtual"""" set of clauses denoted by the program expression Pexp. Pexp involves named collections of clauses and composition operations over them. Both the operational and xpoint semantics of the language are given along with its meta-level deenition. A set of examples provides evidence of expressiveness of the language."""	3d xpoint;logic programming	Antonio Brogi;Chiara Renso;Franco Turini	1994			description logic;axiomatic semantics	PL	-16.660343625144854	16.688552046917493	5979
0a05daef94a7db5bee6901955bfdc2a6c8d1e2dc	improving region selection in dynamic optimization systems	optimising compilers;cache storage;program diagnostics;software fault diagnosis;code optimization;program diagnostics cache storage optimising compilers;magnetohydrodynamic power generation design optimization runtime tail program processors counting circuits frequency virtual machining optimizing compilers design methodology;selection mechanism;next executing tail;last executed iteration;vliw;last executed iteration dynamic optimization systems hp dynamo code optimization code caching selection mechanism next executing tail;binary translation;commitment;dynamic optimization systems;code caching;trace prediction;hp dynamo;optimizing compilers;cache memories;dynamic optimization	The performance of a dynamic optimization system depends heavily on the code it selects to optimize. Many current systems follow the design of HP Dynamo and select a single interprocedural path, or trace, as the unit of code optimization and code caching. Though this approach to region selection has worked well in practice, we show that it is possible to adapt this basic approach to produce regions with greater locality, less needless code duplication, and fewer profiling counters. In particular, we propose two new region-selection algorithms and evaluate them against Dynamo¿s selection mechanism, Next-Executing Tail (NET). Our first algorithm, Last-Executed Iteration (LEI), identifies cyclic paths of execution better than NET, improving locality of execution while reducing the size of the code cache. Our second algorithm allows overlapping traces of similar execution frequency to be combined into a single large region. This second technique can be applied to both NET and LEI, and we find that it significantly improves metrics of locality and memory overhead for each.	algorithm;cpu cache;duplicate code;dynamic programming;iteration;locality of reference;mathematical optimization;overhead (computing);program optimization;tracing (software)	David Hiniker;Kim M. Hazelwood;Michael D. Smith	2005	38th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'05)	10.1109/MICRO.2005.22	parallel computing;real-time computing;computer science;very long instruction word;theoretical computer science;operating system;program optimization;programming language	Arch	-18.375909804886767	36.85694881355388	5987
b776f665753cf5df00053dbde1e6f66bf3e63f05	x.400 use of extended character sets		This RFC defines a suggested method of using GeneralText in order tonharmonize as much as possible the usage of this body part. [STANDARDS-nTRACK]		Harald Tveit Alvestrand	1993	RFC	10.17487/RFC1502	computer science;data mining;world wide web;engineering drawing	Theory	-52.75229141839444	6.8666065591680185	5990
27ef1be5da2f559d3a20b6bfa284f66f04c2de19	a design and implementation method for embedded systems using communicating sequential processes with an event-driven and multi-thread processor	multi threading;sensors;development cost reduction event driven processor multithread processor cyberworlds embedded system implementation embedded system design agent system state transition diagram communicating sequential process models c like language radio controlled car tablet development time reduction;mobile robots;embedded system;receivers;software agents;embedded systems;communicating sequential processes;object oriented way;system recovery;telerobotics communicating sequential processes embedded systems mobile robots multiprocessing systems multi threading notebook computers software agents;event driven and mullti thread processor embedded system communicating sequential processes agent system object oriented way state transition diagram;event driven and mullti thread processor;agent system;process control;telerobotics;notebook computers;object oriented modeling receivers embedded systems wheels process control system recovery sensors;multiprocessing systems;state transition diagram;object oriented modeling;wheels	Recently, because embedded systems that play important roles in cyber worlds involve complexly intertwined functions, it becomes difficult to develop them using conventional technologies. In this paper, a new method is introduced for designing and implementing embedded systems. The design phase uses an agent system, a state transition diagram, and communicating sequential process models by descending from an abstract and general level to a more concrete and specific level. The implementation phase uses an event-driven and multi-thread processor and a C-like language with enhanced processes communication channels. A radio-controlled car operated using a tablet is developed using the proposed design and implementation method. The development time and cost is considerably reduced.	communicating sequential processes;embedded system;event (computing);event-driven finite-state machine;event-driven programming;experimental system;radio control;state diagram;state transition table;tablet computer;thread (computing)	Ryo Mizutani;Kenji Ohmori	2012	2012 International Conference on Cyberworlds	10.1109/CW.2012.38	telerobotics;mobile robot;embedded system;real-time computing;state diagram;multithreading;computer science;sensor;artificial intelligence;software agent;operating system;communicating sequential processes;process control;distributed computing	EDA	-32.38352204215613	36.468196348006416	5992
d35332a0e0c10eb0bd101619de5fb2422176ab26	combinatorial approach for automated platform diversity testing	combinatorial testing approach;software platform;approximation algorithms;platform testing;testing;data mining;diversity reception;production testing middleware production engineering computing;production engineering computing;interaction fault detection;feature extraction;product line engineering;diversity testing;middleware;automatic testing taxonomy programming costs scheduling sorting databases uncertainty collaborative software project management;production testing;interaction fault detection platform testing diversity testing product line engineering combinatorial testing approach philips medical systems products;philips medical systems products;medical diagnostic imaging	In recent years, product line engineering has been used effectively in many industrial setups to create a large variety of products. One key aspect of product line engineering is to develop re-usable assets often referred to as a platform. Such software platforms are inherently complex due to requirements of providing diverse functionalities, thereby leading to combinatorial test data explosion problem while validating these platforms. In this paper, we present a combinatorial approach for testing varied features and data diversity present within the platform. The proposed solution effectively takes care of complex interdependencies among diversity features and generates only valid combinations for test scenario. We also developed a prototype tool based on our proposed approaches to automate the platform testing. As part of our case study, we have used our prototype to validate a software platform widely being used across Philips Medical Systems (PMS) products. Initial results confirm that our approach significantly improves the overall platform testing process by reducing testing effort and improve the quality of the platform by detecting all interaction faults.	care-of address;interdependence;interface message processor;prototype;requirement;scenario testing;sensor;spatial variability;test automation;test data	Rajendra Sisodia;Vijaykumar Channakeshava	2009	2009 Fourth International Conference on Software Engineering Advances	10.1109/ICSEA.2009.28	simulation;feature extraction;computer science;systems engineering;engineering;operating system;software engineering;middleware;software testing;approximation algorithm	SE	-58.17247450981572	31.386091220690968	5997
81adf45ad32eef6216a23a49e0bcc4c7641ff041	an implementation of flush channels based on a verification methodology	distributed system;protocols;verification methodology;distributed processing;distributed computing;out of order;asynchronous message passing protocols;formal verification;formal argument flush channels verification methodology asynchronous message passing protocols distributed system;flush channels;programming profession;message passing;bandwidth;protocols distributed processing formal verification message passing;circuits;computer science;circuits delay protocols computer science educational institutions programming profession bandwidth message passing distributed computing out of order;formal argument	Flush channels generalize more conventional asynchronous message passing protocols. A distributed system that uses flush channels allows a programmer the flexibility of specifying the delivery order of each message in relation to other messages transmitted on the channel. An implementation technique that follows directly from a verification methodology for flush channels is presented. A relatively formal argument in support of the technique is included. >	allocate-on-flush;verification and validation	Phil Kearns;Tracy Camp;Mohan Ahuja	1992	Parallel Processing Letters	10.1109/ICDCS.1992.235023	communications protocol;electronic circuit;message passing;real-time computing;formal verification;computer science;out-of-order execution;theoretical computer science;distributed computing;bandwidth	EDA	-31.684687913575512	32.672609678533995	6006
cbf56fb578698dc07772ef095b16d0ec303542a1	designing and implementing b2b applications using argumentative agents	business process;autonomous agent	This paper presents a framework for modeling and deploying Businessto-Business ( B2B) applications, with autonomous agents exposing the individual components that implement these applications. This framework consists of three levels identified by strategic, application, and resource, with focus here on the first two levels. The strategic level is about the common vision that independent businesses define as part of their decision of partnership. The application level is about the business processes that get virtually integrated as result of this common vision. Since conflicts are bound to arise among the independent applications/agents, the framework uses a formal model based upon computational argumentation theory through a persuasion protocol to detect and resolve these conflicts. In this protocol, agents can reason about partial information using partial arguments , partial attack andpartial acceptability. Agents can then jointly find arguments supporting a new solution for their conflict, which is not known by any of them individually. Termination, soundness, and completeness properties of this protocol are presented. Distributed and centralized coordination strategies are also supported in this framework, which is illustrated with a simple online purchasing case study.	autonomous robot;business process;centralized computing;formal language;image scaling;ontology (information science);population;purchasing;web service	Jamal Bentahar;Nanjagud Narenda;Zakaria Maamar;Rafiul Alam;Philippe Thiran	2008		10.3233/978-1-58603-916-5-165	electrical network;systems engineering;battery (electricity);flashlight;computer science;electric potential energy	AI	-42.919180277926436	19.425054309473396	6008
08b0f1f177aa8005bddc469c1a578ae9e98ffba4	a bad-block test design for multiple flash-memory chips		Flash-memory has become a popular alternative in storage systems, due to its characteristics in non-volatility, shock-resistance, and low-power consumption. When a flashmemory chip is created priori to shipping, its initial bad blocks should be identified, where a bad block means that data stored in the block could be unreliable and can not be used. For safety and reliable reasons, any bad blocks should be efficiently identified by considering the operational model of flash-memory chips. In this paper, we will propose a bad-block test design for multiple flash-memory chips to efficiently identify bad blocks. The objective is to exploit execution parallelism and provide QoS guarantees. We will define a real-time task model and provide the schedulability analysis. We further provide a reasonable execution time setup for real-time tasks and show that the execution time setup can provide a schedulable result. In the experiments, a real case is discussed and measured such that the bad-block test design can be realized by configuring the corresponding computation time, the period, and the longest non-preemption time.	computation;experiment;flash memory;low-power broadcasting;non-volatile memory;parallel computing;preemption (computing);real-time clock;run time (program lifecycle phase);scheduling analysis real-time systems;test design;time complexity;volatility	Chin-Hsien Wu	2012	J. Inf. Sci. Eng.		automatic test equipment	Embedded	-9.897349589031355	58.09297694345187	6009
ab22cde9001fb54b36810307a23b9c70660324a1	evolution of the product manager	group and organization interfaces;design;user/machine systems;user interfaces;management;the computer industry	Better education needed to develop the discipline		Ellen Chisa	2014	Commun. ACM	10.1145/2669480	product management	Graphics	-60.4805977002129	5.219163900560178	6028
dcd5223ab16f93e8d2e761cabdba487774e5103c	exact query reformulation with first-order ontologies and databases	standard theorem;safe-range formula;effective approach;arbitrary first-order logic ontology;exact query reformulation;general framework;database signature;safe-range first-order equivalent reformulation;sql query;first-order ontology	standard theorem;safe-range formula;effective approach;arbitrary first-order logic ontology;exact query reformulation;general framework;database signature;safe-range first-order equivalent reformulation;sql query;first-order ontology	database;first-order predicate;ontology (information science)	Enrico Franconi;Volha Kerhet;Nhung Ngo	2012		10.1007/978-3-642-33353-8_16	sargable;query optimization;query by example;theoretical computer science;data mining;database;mathematics	DB	-25.501003506478305	9.094369403570079	6041
26ee9073e64d4bcbcafe3f20064a9067b69eb8b7	meta-programming with built-in type equality	programming language;data type;design space;formal reasoning;equality types;meta programming;type system;meta language	We report our experience with exploring a new point in the design space for formal reasoning systems: the development of the programming language Ωmega. Ωmega is intended as both a practical programming language and a logic. The main goal of Ωmega is to allow programmers to describe and reason about semantic properties of programs from within the programming language itself, mainly by using a powerful type system. We illustrate the main features of Ωmega by developing an interesting meta-programming example. First, we show how to encode a set of well-typed simply typed λ-calculus terms as an Ωmega data-type. Then, we show how to implement a substitution operation on these terms that is guaranteed by the Ωmega type system to preserve their well-typedness.	encode;metaprogramming;programmer;programming language;simply typed lambda calculus;type system	Tim Sheard;Emir Pasalic	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2007.11.012	metaprogramming;fourth-generation programming language;first-generation programming language;type conversion;natural language programming;very high-level programming language;language primitive;type system;object language;specification language;programming domain;data type;type safety;metalanguage;computer science;theoretical computer science;functional logic programming;programming paradigm;low-level programming language;inductive programming;fifth-generation programming language;programming language theory;programming language;programming language specification;high-level programming language;algorithm	PL	-23.942145638865817	23.31807856070974	6046
096785781170092828172a4f98822c450b9d6d45	profile context management in ubiquitous computing	profile context;context dependency;context management;ad hoc networks;ubiquitous computing;pervasive systems;mobile communications;context aware services	By utilising various types of contextual data acquired from users' devices, enhanced context-aware services and applications can be developed and deployed. But current context management systems are based on a server-client approach, which hinders their widespread adoption in an ad-hoc environment. In addition, performance issues introduced because of context dependency in a ubiquitous environment still need to be addressed. This paper presents the idea of profile context to address the problem of context dependency, and also proposes an open framework for context acquisition, management and distribution in a ubiquitous environment. Bringing together context information and context updates from various sources, support for context-aware decisions can be implemented efficiently in a mobile environment by addressing the issues related to context dependency using profile context.	ubiquitous computing	Raheel Ali Baloch;Noël Crespi	2012	IJAHUC	10.1504/IJAHUC.2012.050437	wireless ad hoc network;human–computer interaction;computer science;knowledge management;internet privacy;world wide web;ubiquitous computing	HCI	-43.120005406750494	43.99374461562934	6047
6a4407dd940cab3ae4a8fe95afb4054382a321b8	the design of large real-time systems: the time-triggered approach	data sharing;time triggered;independent development;certification;distributed processing;timeliness;control error propagation;computer architecture;ime triggered approach;distributed real time system;error propagation;time triggered architecture;real time systems computer architecture control systems safety hardware synchronization application software communication system control clocks protocols;certification real time systems ime triggered approach time triggered architecture spatial partitioning temporal firewall control error propagation independent development distributed architecture timeliness validation;computer architecture distributed processing real time systems;validation;temporal firewall;distributed architecture;spatial partitioning;real time systems	The time-triggered(TT) architecture approach supports the spatial partitioning of a large, distributed real-time system into a set of autonomous subsystems with small control-free data-sharing interfaces between them. This paper presents such a TT architecture and gives a detailed description of the interface between an autonomous time-triggered communication subsystem based on the TTP protocol and the host computer within a node of this architecture. This interface acts as a temporal firewall that eliminates the possibility of control error propagation from one subsystem to another subsystem. It thus facilitates the independent development and validation of the subsystems and supports the composability of the distributed architecture with respect to timeliness, validation, and certification.	autonomous robot;composability;distributed computing;firewall (computing);host (network);propagation of uncertainty;real-time clock;real-time computing;real-time operating system;real-time transcription;software propagation;space partitioning;trusted third party	Hermann Kopetz;Martin Oberkönig;Christian Ebner;Andreas Krüger;Dietmar Millinger;Roman Nossal-Tüyeni;Anton V. Schedl	1995		10.1109/REAL.1995.495208	embedded system;real-time computing;computer science;space partitioning;propagation of uncertainty;operating system;distributed computing;interface control document;certification;systems architecture	Embedded	-32.61507424992856	38.203782451304	6052
e2536c1946c60dca6866faf1154f220357447c09	poster: r4platform: a reliable data platform for continuous performance auditing in buildings		This poster presents the R4Platform which provides an integrated set of tools and services that can be used as an enabler for continuous performance auditing in the built environment. The objective is to provide a modular set of components that when composed can provide an end-to-end auditing system for building managers, ESCOs and energy providers. The poster will present the platform architecture and its application in a real world environment, this will include a visual demonstration of the platform.		Alan McGibney;Jean Michel Rubillon;Susan Rea	2017			real-time computing;performance audit;built environment;architecture;computer science;modular design;audit	DB	-34.254151426442455	57.31196587597824	6057
8fd4f5093bfc685b4d8d132fd306dc515ad2dc9f	supporting virtual organisations using bdi agents and constraints	commerce electronique;multiagent system;belief;comercio electronico;e commerce;software agent;agent logiciel;desir;constraint satisfaction;deseo;software agents;satisfaction contrainte;croyance;bdi agents;agent intelligent;virtual organisation;intelligent agent;agente inteligente;satisfaccion restriccion;creencia;sistema multiagente;desire;electronic trade;systeme multiagent	Virtual organisations underpin many important activities in distributed computing, including e-commerce and e-science. This paper describes a new technique by which software agents can intelligently form virtual organisations to meet some pre-specified requirements. Our approach builds on work in BDI agents and constraint satisfaction techniques. Using a realistic service-providing scenario, we show how an agent can use constraint solving techniques to explore possible virtual organisation alliances with other agents, based on its beliefs and desires. The agent can choose the best among several possible virtual organisations to form in order to meet a customer’s requirements. Our approach is to use a deliberative process to construct possible worlds, each corresponding to a potential virtual organisation, and each configured using constraint satisfaction techniques. We also show how an agent can take account of pre-existing virtual organisation relationships in its deliberations.	constraint satisfaction problem;distributed computing;e-science;e-commerce;possible world;requirement;software agent;virtual enterprise;virtual organization	Stuart W. Chalmers;Peter M. D. Gray;Alun D. Preece	2002		10.1007/3-540-45741-0_20	simulation;computer science;knowledge management;artificial intelligence;software agent;intelligent agent	AI	-39.55475955333476	16.342903031457244	6069
7ed784381785fb9280ea1625b026177a98bdcccb	purity and side effect analysis for java programs	expresion regular;verificacion modelo;java programming;localization;verification modele;useful information;langage java;informacion util;localizacion;interpretacion abstracta;program verification;analisis programa;program optimization;verificacion programa;marcador;localisation;capture;model checking;pointer;side effect;estructura datos;expression reguliere;pointeur;lenguaje java;captura;analyse information;structure donnee;optimisation programme;program analysis;interpretation abstraite;analyse programme;abstract interpretation;verification programme;data structure;information analysis;regular expression;information utile;java language;optimizacion programa	We present a new purity and side effect analysis for Java programs. A method is pure if it does not mutate any location that exists in the program state right before the invocation of the method. Our analysis is built on top of a combined pointer and escape analysis, and is able to determine that methods are pure even when the methods mutate the heap, provided they mutate only new objects. Our analysis provides useful information even for impure methods. In particular, it can recognize read-only parameters (a parameter is readonly if the method does not mutate any objects transitively reachable from the parameter) and safe parameters (a parameter is safe if it is read-only and the method does not create any new externally visible heap paths to objects transitively reachable from the parameter). The analysis can also generate regular expressions that characterize the externally visible heap locations that the method mutates. We have implemented our analysis and used it to analyze several applications. Our results show that our analysis effectively recognizes a variety of pure methods, including pure methods that allocate and mutate complex auxiliary data structures.	data structure;escape analysis;ibm notes;iterative method;java package;pointer (computer programming);program analysis;pure function;read-only memory;regular expression;state (computer science)	Alexandru Salcianu;Martin C. Rinard	2005		10.1007/978-3-540-30579-8_14	program analysis;model checking;real-time computing;pointer;internationalization and localization;data structure;computer science;theoretical computer science;program optimization;data analysis;programming language;side effect;regular expression;algorithm	PL	-20.244614511267848	29.686824679722605	6075
22961b74914d026cca0c38c52822908e01a648d0	security and access control for a human-centric collaborative commerce system	electronic commerce;access;c commerce systems;e commerce;collaboration;distributed computing;computer security;collaborative environment;e commerce c commerce systems security access control human centric computing collaboration;it security;control;access control;access control collaboration business collaborative work information security distributed computing electronic commerce control systems protection privacy;security;human centric computing;knowledge base	The rise of globally distributed computer based workspaces has enabled the incorporation of collaboration in electronic commerce (e-Commerce) systems. Working in collaborative environments with e-Commerce technologies leads to the subject of collaborative commerce (or c-Commerce). C-Commerce creates dynamic collaboration and harnesses organizations’ information and knowledge base into a computer-based framework to support personalized access to potentially all participants and information in a given community. One of the main concerns in such a system is security and control of access. Many distributed organizations and individuals want to work together and share their information and knowledge in the process. At the same time, they need to protect their privacy and sensitive information and establish proper protocols for access and sharing activities. This paper discusses a human-centered collaborative commerce system (HCCS) and its security and access control design. Specifically, it presents three security modules and components that will support collaborative exchange and processes. We, then, introduce an improved access control method and algorithm which is role-, group-, and task-based (RGT-based access control) that ensures information and resources access efficiently. Developing further access control algorithms and implementations will be considered in a variety of case studies in future work.	access control;algorithm;computer security;e-commerce;information sensitivity;knowledge base;personalization;privacy;workspace	Seung-yun Kim;Jian Zhu;Waleed W. Smari;William K. McQuay	2006	International Symposium on Collaborative Technologies and Systems (CTS'06)	10.1109/CTS.2006.62	e-commerce;computer access control;physical access;computer science;knowledge management;access control;role-based access control;distributed system security architecture;network access control;world wide web;computer security;scientific control;collaboration	Security	-45.9996588285623	58.524421811913484	6076
68ec5cd59ea53897c7f2d8e7d800bc8f9d10db26	an indexing scheme for rdf and rdf schema based on suffix arrays	directed acyclic graph;resource description framework;suffix array;indexation;next generation;semantic web;world wide web;path expressions	The Semantic Web is a candidate for the next generation of the World Wide Web. It is anticipated that the number of metadata written in RDF (Resource Description Framework) and RDF Schema will increase as the Semantic Web becomes popular. In such a situation, demand for querying metadata described with RDF and RDF Schema will also increase, and therefore effective query retrieval of RDF data is important. To this end, we propose an indexing scheme for RDF and RDF Schema. In our (proposed) scheme, we first extract four kinds of DAGs (Directed Acyclic Graphs) from an RDF data, and extract all path expressions from the DAGs. Then, we generate four kinds of suffix arrays based on the path expressions. Using the indices, we can achieve efficient processing of query retrievals on RDF data including schematic information defined by RDF Schema (for example, classes and/or properties).	database;directed acyclic graph;mathematical optimization;next-generation network;path expression;query optimization;rdf schema;resource description framework;schematic;semantic web;suffix array;triplestore;world wide web	Akiyoshi Matono;Toshiyuki Amagasa;Masatoshi Yoshikawa;Shunsuke Uemura	2003			rdf/xml;cwm;semantic web rule language;bibliographic ontology;computer science;sparql;simple knowledge organization system;semantic web;rdf;linked data;data mining;database;rdf query language;web ontology language;world wide web;blank node;information retrieval;semantic analytics;directed acyclic graph;rdf schema	Web+IR	-34.82439413057188	5.411139845799319	6078
bcda42491038518ba353371f3a1edc91836f169d	validation of system behavior from an integrated semantics of use case and design models	use case	This paper summarizes an approach how to specify use cases and how to solve the problem of validating the conformance between the use case model and the design model. An integrated semantics of the two models is proposed. We employ UMLand OCL-based techniques as well as ideas from graph transformation. This research contributes to model transformation within the area of Model Driven Development (MDD).	conformance testing;graph rewriting;model transformation;model-driven engineering;object constraint language	Duc-Hanh Dang	2007			theoretical computer science;use-case analysis;systems engineering;use case diagram;computer science;model transformation;graph rewriting;semantics;use case	SE	-53.599507461201945	25.505650601039065	6086
63851edc8018f62770a2cb990d0c51feb04734be	sharing and reusing data and analytic methods with learnsphere.				Ran Liu;Kenneth R. Koedinger;John C. Stamper;Philip I. Pavlik	2017			artificial intelligence;computer engineering;computer science;machine learning;reuse	ML	-56.990584564784115	5.223855485749623	6087
09531f39a454b4c9e8cb0b36bab4c4452b82d520	hierarchical modeling and simulation environment for intelligent transportation systems	simulation ordinateur;traffic simulation;systeme gestion trafic technologie avancee;sistema de transporte;systeme intelligent;road network;intelligent transport system;teoria sistema;road traffic;gestion trafic;sistema inteligente;traffic management;i3d2 transportation simulation system;model abstraction method;red carretera;devs formalism;atis the next generation;trafic routier;systems theory;ses mb;theorie systeme;intelligent system;gestion trafico;reseau routier;systeme transport;trafico carretera;simulacion computadora;information system;computer simulation;transportation system;systeme information;hierarchical model;simulation environment;sistema informacion	ion is a key to model construction for simulation [21-24]. Abstraction, as a process, refers to a method or algorithm applied to a model to simplify its complexity while preserving its validity. The amount of detail in a model can be taken as the “product” of its scope and resolution. Scope refers to how much of the real world is represented; resolution refers to the number of variables in the model and their precision or granularity. Given a fixed amount of resources and a model complexity that exceeds this limit, there is a trade-off between scope and resolution. We may be able to represent some aspects of a system accurately, but only a few components will be presentable. Or, we may be able to provide a comprehensive view on the system but only at a relatively low resolution. Such an abstraction process Volume 80, Number 2 SIMULATION 63 © 2004 Simulation Councils Inc.. All rights reserved. Not for commercial use or unauthorized distribution. at PENNSYLVANIA STATE UNIV on April 17, 2008 http://sim.sagepub.com Downloaded from	algorithm;authorization;complexity;image resolution;resolution (logic);simulation	Jong-Keun Lee;Ye-Hwan Lim;Sung-Do Chi	2004	Simulation	10.1177/0037549704042860	traffic generation model;computer simulation;active traffic management;simulation;computer science;engineering;transport engineering;systems theory;advanced traffic management system;information system;hierarchical database model;network traffic simulation	AI	-32.02771715339578	15.603912323962193	6088
272f6f31722655f35779cd5ca174981563d68ca4	feature-based modelling by integrating design and recognition approaches	feature based model;bibliotheque;concepcion asistida;computer aided design;architecture systeme;feature recognition;reconocimiento;feature conversion;systeme integre;sistema integrado;integrated design;recognition;conception assistee;arquitectura sistema;geometric model;feature modelling;system architecture;reconnaissance;integrated system;biblioteca;library	Previous work on feature-based modelling has emphasized generating features either in the design phase (design by features) or in the later product-development phases (feature-recognition). Recently, some attempts have been made to integrate both strategies, with the major aim of combining the positive aspects while reducing the drawbacks. The paper presents a system architecture for feature-based modelling which is founded on this integration that is obtained through the definition of a common feature library and an intermediate model, which plays the role of communication link between the geometric model and the feature-based model		Teresa De Martino;Bianca Falcidieno;Franca Giannini;Stefan Haßinger;Jivka Ovtcharova	1994	Computer-Aided Design	10.1016/0010-4485(94)90107-4	feature recognition;simulation;library;engineering;artificial intelligence;geometric modeling;computer aided design;feature model;systems architecture;mechanical engineering	EDA	-57.22773280365925	9.656560791283198	6091
9d477c6a427e64f243833a7853bee48cdfe2fcd9	online-autotuning of parallel sah kd-trees	parallel performance optimization online autotuning parallel construction surface area heuristic sah kd tree spatial data structure computer graphics performance portability performance evaluation;online autotuning;tree data structures computer graphics optimisation parallel processing software performance evaluation spatial data structures;geometry;parallel performance optimization spatial data structures online autotuning;tuning;data structures;tuning data structures ray tracing geometry hardware parallel algorithms rendering computer graphics;ray tracing;spatial data structures;rendering computer graphics;hardware;parallel performance optimization;parallel algorithms	We explore the benefits of using online-autotuning to find an optimal configuration for the parallel construction of Surface Area Heuristic (SAH) kD-trees. Using a quickly converging autotuning mechanism, we achieve a significant performance improvement of up to 1.96x. The SAH kD-tree is a spatial data structure and a fundamental tool in the domain of computer graphics and simulations. The parallel construction of these trees is influenced by several parameters, controlling various aspects of the algorithm. However, the parameter configurations advocated in the literature are hardly ever portable. To boost portability, we apply online-autotuning to four state-of-the-art variants of parallel kD-tree construction. We show that speedups over the variants' standard configurations are possible with low programmer effort. We further demonstrate the performance portability of our approach by evaluating performance on varying multicore platforms and both static and dynamic geometries.	algorithm;application programming interface;auto-tune;computer graphics;data structure;experiment;heuristic;multi-core processor;open research;parallel computing;programmer;simulation;software portability;speedup;thread (computing);undefined behavior;unordered associative containers (c++)	Martin Tillmann;Philip Pfaffe;Christopher Kaag;Walter F. Tichy	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.31	ray tracing;computer architecture;parallel computing;data structure;computer science;theoretical computer science;operating system;parallel rendering;distributed computing;parallel algorithm	Visualization	-6.906904727430736	46.017230725201905	6102
4f3bd1e4cc798682cd332c13767eb6644c83d876	ontology-driven software engineering	model development and transformation;software systems;semantics;software engineering;model development;ontology	This workshop represents the 6th in a series of OOPSLA workshops focusing on the impact of semantics on the development and re-engineering of software systems. These workshops include three on 'Semantics of Enterprise Integration' (2001-2003), one on 'Legacy Transformation' (2004) and one on 'Semantic-Based Systems Development' (2007).	enterprise integration;software engineering;software modernization;software system	Sergio de Cesare;Guido L. Geerts;Grant Holland;Mark Lycett;Christopher Partridge	2009		10.1145/1639950.1639983	personal software process;verification and validation;computer science;knowledge management;package development process;social software engineering;software development;iterative and incremental development;ontology;software construction;semantics;systems development life cycle;software analytics;goal-driven software development process;software development process;software system;software peer review	SE	-51.878719880924315	26.276678594547068	6126
86094f4cd8fea93097417a75c045ff93e90b2834	a model based transformation paradigm for cross-language collaborations	cross language transformation;clt rom;model based paradigm;online collaboration;recursive object model	Online collaboration is a big challenge in the field of international product development in a cross-language environment. It serves two purposes: cross-language translation and design requirement clarification. Though many approaches and tools are developed for each of the purposes, not a solution serves both of them well. Especially, the traditional statistical methods for cross-language translation cannot preserve the whole semantic information, which intend to incur misunderstanding and ineffective collaboration. This results in potential problems in clarifying the design requirements. In this paper, we proposed a method to online collaboration, named Cross-Language Transformation based on Recursive Object Model (CLT-ROM). The proposed method consists of two steps. Firstly, a natural language sentence is transformed into a source ROM diagram. Secondly, a corresponding target ROM diagram is generated by a transformation algorithm. The proposed method is a model-based communication tool which facilitates collaborations. Since the ROM has been proven effective in requirements clarification, some examples are given to illustrate that the CLT-ROM has a good capability of semantic preserving in requirement engineering for product development.	programming paradigm	Kunmei Wen;Suo Tan;Jie Wang;Ruixuan Li;Yuan Gao	2013	Advanced Engineering Informatics	10.1016/j.aei.2012.10.007	computer science;engineering;artificial intelligence;software engineering;machine learning;data mining;management	DB	-56.33619497692465	24.21000896514635	6136
d124f301d92e104fe8eb339621d0939e8de3068d	from raw project data to business intelligence	software;standards;query processing;risk analysis;vtml;software development tim traceability information mode vtml visual trace modeling language sql queries trace links software engineering;sql queries;software engineering;project stakeholders business intelligence raw project data visual trace modeling language vtml;visualization;visual languages competitive intelligence project management;traceability information mode;visual trace modeling language;software development;unified modeling language;trace links;tim;visual analytics;data models;unified modeling language visualization data models software development software engineering visual analytics query processing	VTML (Visual Trace Modeling Language) empowers project stakeholders to issue useful queries. The Web extra at https://youtu.be/RH4rvFgj8lQ is an audio podcast in which author Jane Cleland-Huang provides an audio recording of the Requirements column, in which she discusses how VTML (Visual Trace Modeling Language) empowers project stakeholders to issue useful queries.	jane (software);modeling language;podcast;vtml;world wide web	Patrick Mäder;Jane Cleland-Huang	2015	IEEE Software	10.1109/MS.2015.92	unified modeling language;data modeling;visual analytics;visualization;risk analysis;computer science;software development;software engineering;data mining;database;programming language	SE	-51.79425846981705	21.527144273900294	6154
2fbd86228fed0e5f90147dfe97e0641218834a3a	software management of hybrid main memory systems			computer data storage;software project management	Ahmad Hassan	2016				DB	-19.24118715692507	51.76227643534746	6171
0fd39cf08c78e81a4038c85dfdb5b22ba4d795a9	caterpillars, context, tree automata and tree pattern matching	pattern matching;tree automata	We present a novel, yet simple, technique for the speciication of context in struc-tured documents that we call caterpillar expressions. Although we are primarily applying this technique in the speciication of context-dependent style sheets for HTML, SGML and XML documents, it can also be used for query speciication for structured documents, as we shall demonstrate, and for the speciication of computer program transformations. From a conceptual point of view, structured documents are trees, and one of the oldest and best-established techniques to process trees and, hence, structured documents are tree automata. We present a number of theoretical results that allow us to compare the expressive power of caterpillar expressions and caterpillar au-tomata, their companions, to the expressive power of tree automata. In particular, we demonstrate that each caterpillar expression describes a regular tree language that is, hence, recognizable by a tree automaton. Finally, we employ caterpillar expressions for tree pattern matching. We demonstrate that caterpillar automata are able to solve tree-pattern-matching problems for some, but not all, types of tree inclusion that Kilpell ainen investigated in his PhD thesis. In simulating tree pattern matching with caterpillar automata, we reprove some of Kilpell ainen's results in a uniform framework.	computer program;context-sensitive language;html;pattern matching;program transformation;regular tree grammar;simulation;standard generalized markup language;tree automaton;xml	Anne Brüggemann-Klein;Derick Wood	1999		10.1142/9789812792464_0023	red–black tree;combinatorics;discrete mathematics;tree rotation;trie;k-ary tree;tree structure;search tree;tree;tree traversal	PL	-24.737125594103073	12.553690038766389	6193
248c206515cb8dcb05df98ed16ef49b31cffb917	increasing off-chip bandwidth in multi-core processors with switchable pins	bandwidth limitation issue;dynamic pin switch technique;processor design;surplus pins;microprocessor chips;processor performance;off-chip pins;off-chip memory bandwidth;many-cores;switchable pins;multiprocessing systems;memory intensive stages;power delivery;integrated circuit design;memory intensive phases;processor signal communication;multicore processors;switches	Off-chip memory bandwidth has been considered as one of the major limiting factors to processor performance, especially for multi-cores and many-cores. Conventional processor design allocates a large portion of off-chip pins to deliver power, leaving a small number of pins for processor signal communication. We observed that the processor requires much less power than that can be supplied during memory intensive stages. This is due to the fact that the frequencies of processor cores waiting for data to be fetched from off-chip memories can be scaled down in order to save power without degrading performance. In this work, motivated by this observation, we propose a dynamic pin switch technique to alleviate the bandwidth limitation issue. The technique is introduced to dynamically exploit the surplus pins for power delivery in the memory intensive phases and uses them to provide extra bandwidth for the program executions, thus significantly boosting the performance	central processing unit;memory bandwidth;multi-core processor;processor design	Shaoming Chen;Yue Hu;Ying Zhang;Lu Peng;Jesse Ardonne;Samuel Irving;Ashok Srivastava	2014	2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)		multi-core processor;embedded system;computer architecture;parallel computing;computer hardware;network switch;processor design;computer science;operating system;integrated circuit design	Arch	-6.737870034159567	53.68630543036309	6199
660cf5937f55f452754fe6025c12b8b4a9b02471	a service-oriented middleware for context-aware applications	context awarness;dynamic self adaptive cbse;aspect	Context awareness has emerged as an important element in distributed computing. It offers mechanisms that allow applications to be aware of their environment and enable these applications to adjust their behavior to the current context. Considering the dynamic nature of context, the data flow of relevant contextual information can be significant. In order to keep track of this information flow, a flexible service mechanism should be available for the client applications. In this document we present a service-oriented middleware for context-aware applications. This middleware provides support to leverage the development of context-aware applications by providing a scripting-like approach for context-aware application development; allowing the subscription of rules containing context-based events and conditions and a notification to be sent when the specified context holds. Moreover, a domain-specific language has been developed to express these context-based rules.	context awareness;dataflow;distributed computing;domain-specific language;service-oriented device architecture;service-oriented middleware	Luiz Olavo Bonino da Silva Santos;Remco Poortinga;Peter Vink	2007		10.1145/1376866.1376873	middleware;real-time computing;computer science;middleware;database;context model;world wide web	HCI	-40.12347945534581	41.623397036292594	6202
52f0fbd428911b7e43477092ec195d40c5be685f	swift: a fast dynamic packet filter	packet filtering;code optimization;perforation;instruction set architecture;application program interface;single instruction multiple data;commercial off the shelf;operating system;object oriented;processing speed;complex instruction set computer	This paper presents Swift, a packet filter for high performance packet capture on commercial off-the-shelf hardware. The key features of Swift include (1) extremely low filter update latency for dynamic packet filtering, and (2) Gbps high-speed packet processing. Based on complex instruction set computer (CISC) instruction set architecture (ISA), Swift achieves the former with an instruction set design that avoids the need for compilation and security checking, and the latter by mainly utilizing SIMD (single instruction, multiple data). We implement Swift in the Linux 2.6 kernel for both i386 and x86 64 architectures. The Swift userspace library supports two sets of application programming interfaces (APIs): a BPF-friendly API for backward compatibility and an object oriented API for simplifying filter coding. We extensively evaluate the dynamic and static filtering performance of Swift on multiple machines with different hardware setups. We compare Swift with BPF (the BSD packet filter)—the de facto standard for packet filtering in modern operating systems—and hand-coded optimized C filters that are used for demonstrating possible performance gains. For dynamic filtering tasks, Swift is at least three orders of magnitude faster than BPF in terms of filter update latency. For static filtering tasks, Swift outperforms BPF up to three times in terms of packet processing speed, and achieves much closer performance to the optimized C filters.	application programming interface;bsd;backward compatibility;central processing unit;compiler;computational model;data rate units;experiment;firewall (computing);linux;mmx (instruction set);modern operating systems;network packet;operating system;pf (firewall);packet analyzer;streaming simd extensions;swift (programming language);systems design;user space;x86;x86-64	Zhenyu Wu;Mengjun Xie;Haining Wang	2008			parallel computing;real-time computing;simd;application programming interface;computer hardware;computer science;complex instruction set computing;operating system;program optimization;instruction set;database;distributed computing;object-oriented programming;computer security;computer network	Networks	-4.903235917002336	46.866430948083924	6208
062dff2b2e1846f9e28707b528aa3d245e24dfbe	fragment-based genetic programming for fully automated multi-objective web service composition		Web services have become increasingly popular in recent years, given their modular nature and reusability potential. A particularly promising application is in Web service composition, where multiple individual services with specific functionalities are composed to accomplish a more complex task. Researchers have proposed evolutionary computing techniques for creating compositions that are not only feasible, but also have the best possible Quality of Service (QoS). Some of these works employed multi-objective techniques to tackle the optimisation of compositions with conflicting QoS attributes, but they are not fully automated, i.e. they assume the composition workflow structure is already known. This assumption is often not satisfied, as the workflow is often unknown. This paper proposes a genetic programming-based method to automatically generate service compositions in a multi-objective context, based on a novel fragmented tree representation. An evaluation using benchmark datasets is carried out, comparing existing methods adapted to the multi-objective composition problem. Results show that the fragmented method has the lowest execution time overall. In terms of quality, its Pareto fronts are equivalent to those of one of the approaches but inferior to those of the other. More importantly, this work provides a foundation for future investigation of multi-objective fully automated service composition.	benchmark (computing);evolutionary computation;genetic programming;mathematical optimization;multi-objective optimization;pareto efficiency;quality of service;run time (program lifecycle phase);service composability principle;web service	Alexandre Sawczuk da Silva;Yi Mei;Hui Ma;Mengjie Zhang	2017		10.1145/3071178.3071199	evolutionary computation;mathematical optimization;machine learning;artificial intelligence;computer science;web service;pareto principle;quality of service;genetic programming;data mining;modular design;workflow;reusability	Web+IR	-45.921852794938935	14.964862485019573	6210
e0ae9276eeb89662042d10de439d5dbe028e06b8	stormed hybrid games	control problem;hybrid system	We introduce STORMED hybrid games ( SHG), a generalization of STORMED Hybrid Systems [21], which have natural specificati ons, allow rich continuous dynamics and admit various properties to be deci dable. We solve the control problem forSHGusing a reduction to bisimulation on game graphs. This reduction generalizes to a greater family of games, which in ludes o-minimal hybrid games [5]. We also solve the optimal-cost reachability problem for Weighted SHGand prove decidability of WCTL for Weighted STORMED hybrid s ystems.	bisimulation;reachability problem;segmented hyper graphics	Vladimeros Vladimerou;Pavithra Prabhakar;Mahesh Viswanathan;Geir E. Dullerud	2009		10.1007/978-3-642-00602-9_39	combinatorics;discrete mathematics;mathematics;algorithm	ECom	-4.765424061303541	22.55804265403183	6213
7c1f2d6778085d8e4b0511e95087c0b5baf8dd26	distributed architectures for high performance and privacy-aware content generation and delivery	content management;distributed architectures;ubiquitous computing content management data privacy internet;ubiquitous web access;perforation;information privacy;privacy aware content generation;web accessibility;web contents;user profile;privacy aware content delivery;internet;service oriented architecture topology computer architecture computational efficiency data privacy content management information management information security irrigation character generation;data privacy;next generation;ubiquitous computing;high performance;information privacy distributed architectures privacy aware content generation privacy aware content delivery web contents user profile management ubiquitous web access;distributed architecture;user profile management	The current trend in the evolution of the Web is towards an every increasing demand for personalized Web contents. Tailoring Web resources to the characteristics of heterogeneous client devices and user preferences opens two main novel issues in the research area of content generation and delivery. First, the high computational cost characterizing most adaptation services requires efficient and scalable support systems. Second, adaptation services typically rely on user information that may include sensitive data. When system scalability is achieved through architectures that are distributed over a geographical area, privacy issues concerning the user profiles management become particularly critical. In this paper, we propose a distributed architecture for the ubiquitous Web access that guarantees high performance and privacy of sensitive user information by splitting the adaptation services over the nodes of a two-level topology. We evaluate the performance and the overheads of the proposed architecture in order to understand to which extent it is convenient to distribute adaptation services over multiple nodes that are geographically distributed	algorithmic efficiency;centralized computing;distributed computing;geographic coordinate system;information sensitivity;intel core (microarchitecture);overhead (computing);perceived performance;personalization;privacy;response time (technology);scalability;user (computing);user profile;web resource;world wide web	Claudia Canali;Michele Colajanni;Riccardo Lancellotti	2006	2006 Second International Conference on Automated Production of Cross Media Content for Multi-Channel Distribution (AXMEDIS'06)	10.1109/AXMEDIS.2006.24	computer science;database;internet privacy;world wide web	DB	-42.13198103871924	59.0546669412889	6223
bd70209049f5736e5454647bc65d8736416a3af2	opportunities and challenges for future generation grid research	distributed application;complex network;grid middleware;web service;large scale;computer experiment;web portal;middleware;service oriented architecture;grid system;geographic distribution	Grid-based applications are the hallmark of 21st century E-science. Developed by collaborative, virtual communities, they form a new generation of applications that combine scientific instruments, distributed data archives, sensors, and computing resources to solve complex scientific problems. In the growing Escience community, there are on-going research efforts aimed at exploiting the vast bandwidth of fiber optic networks to interconnect such resources and enable a number of high-performance applications. Central to this development are the research activities carried out in the area of Optical Control Plane with results promising to extend the concept of application-driven networking into the optical arena.	archive;control plane;e-science;optical fiber;research data archiving;sensor;virtual community	Dennis Gannon	2006		10.1007/11945918_2	web service;middleware;computer experiment;semantic grid;computer science;operating system;service-oriented architecture;middleware;data grid;database;distributed computing;world wide web;complex network;grid computing;computer network	HPC	-30.623656419280685	50.80270430718362	6252
2d996ff0d2d05a5bbb841250ec2c8d4bc68f0d36	safl: increasing and accelerating testing coverage with symbolic execution and guided fuzzing		Mutation-based fuzzing is a widely used software testing technique for bug and vulnerability detection, and the testing performance is greatly affected by the quality of initial seeds and the effectiveness of mutation strategy. In this paper, we present SAFL1, an efficient fuzzing testing tool augmented with qualified seed generation and efficient coverage-directed mutation. First, symbolic execution is used in a lightweight approach to generate qualified initial seeds. Valuable explore directions are learned from the seeds, thus the later fuzzing process can reach deep paths in program state space earlier and easier. Moreover, we implement a fair and fast coverage-directed mutation algorithm. It helps the fuzzing process to exercise rare and deep paths with higher probability. We implement SAFL based on KLEE and AFL and conduct thoroughly repeated evaluations on real-world program benchmarks against state-of-the-art versions of AFL. After 24 hours, compared to AFL and AFLFast, it discovers 214% and 133% more unique crashes, covers 109% and 63% more paths and achieves 279% and 180% more covered branches.  Video link: https://youtu.be/LkiFLNMBhVE	algorithm;seeds (cellular automaton);software bug;software testing;state (computer science);state space;symbolic execution;test automation;american fuzzy lop	Mingzhe Wang;Jie Liang;Yuanliang Chen;Yu Jiang;Xun Jiao;Hao Liu;Xibin Zhao;Jia-Guang Sun	2018	2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)	10.1145/3183440.3183494	real-time computing;computer engineering;hafnium oxide;state space;global positioning system;software;computer science;symbolic execution;fuzz testing	SE	-59.87438590439037	36.76356563917662	6258
1ca1876eef647cd94f73f052eef34c63c64b586c	unrolling lists	unrolled list;efficient compile-time analysis;data item;functional program;functional language;car field;unrolling list;cons cell;data dependency;cdr field;data structure	Lists are ubiquitous in functional programs, thus supporting lists efficiently is a major concern to compiler writers for functional languages. Lists are normally represented as linked <italic>cons</italic> cells, with each <italic>cons</italic> cell containing a <italic>car</italic> (the data) and a <italic>cdr</italic> (the link); this is inefficient in the use of space, because 50% of the storage is used for links. Loops and recursions on lists are slow on modern machines because of the long chains of control dependencies (in checking for <italic>nil</italic>) and data dependencies (in fetching <italic>cdr</italic> fields). We present a data structure for “unrolled lists”, where each cell has several data items (<italic>car</italic> fields) and one link  (<italic>cdr</italic>). This reduces the memory used for links, and it significantly shortens the length of control-dependence and data-dependence chains in operations on lists. We further present an efficient compile-time analysis that transforms programs written for “ordinary” lists into programs on unrolled lists. The use of our new representation requires no change to existing programs. We sketch the proof of soundness of our analysis—which is based on <italic>refinement types</italic>—and present some preliminary measurements of our technique.	compile time;compiler;control flow;data dependency;data structure;dependence analysis;functional programming;recursion;refinement (computing);skip list;soundness (interactive proof)	Zhong Shao;John H. Reppy;Andrew W. Appel	1994		10.1145/182409.182453	real-time computing;difference list;computer science;theoretical computer science;programming language;algorithm	PL	-19.02625185403077	32.35093362997226	6266
f2f0fa9677da10a478ae9aa339c977f8876330ba	implementation, performance, and science results from a 30.7 tflops ibm bladecenter cluster	big red;science gateways;us teragrid;access big red;system management;tflops ibm e1350 bladecenter;certain application community;certain challenge;e1350 bladecenter architecture;end-to-end scientific workflows;tflops ibm bladecenter cluster;science result	This paper describes Indiana University's implementation, performance testing, and use of a large high performance computing system. IU's Big Red, a 20.48 TFLOPS IBM e1350 BladeCenter cluster, appeared in the 27th Top500 list as the 23rd fastest supercomputer in the world in June 2006. In spring 2007, this computer was upgraded to 30.72 TFLOPS. The e1350 BladeCenter architecture, including two internal networks accessible to users and user applications and two networks used exclusively for system management, has enabled the system to provide good scalability on many important applications while being well manageable. Implementing a system based on the JS21 Blade and PowerPC 970MP processor within the US TeraGrid presented certain challenges, given that Intel-compatible processors dominate the TeraGrid. However, the particular characteristics of the PowerPC have enabled it to be highly popular among certain application communities, particularly users of molecular dynamics and weather forecasting codes. A critical aspect of Big Red's implementation has been a focus on Science Gateways, which provide graphical interfaces to systems supporting end-to-end scientific workflows. Several Science Gateways have been implemented that access Big Red as a computational resource—some via the TeraGrid, some not affiliated with the TeraGrid. In summary, Big Red has been successfully integrated with the TeraGrid, and is used by many researchers locally at IU via grids and Science Gateways. It has been a success in terms of enabling scientific discoveries at IU and, via the TeraGrid, across the US. Copyright © 2009 John Wiley & Sons, Ltd.	flops;ibm bladecenter	Craig A. Stewart;Matthew R. Link;D. Scott McCaulay;Greg Rodgers;George W. Turner;David Y. Hancock;Faisal Saied;Marlon E. Pierce;Ross Aiken;Matthias S. Müller;Matthias Jurenz;Matthias Lieber;Jenett Tillotson;Beth Plale	2010	Concurrency and Computation: Practice and Experience	10.1002/cpe.1539	supercomputer;parallel computing;simulation;computer science;operating system;database;distributed computing;computer graphics (images)	HPC	-27.98701964726402	52.2145764391541	6295
e902b5516433cd271de28c639490d5b903277a7f	examining random and designed tests to detect code mistakes in scientific software	software testing;tolerance problem;random testing;computational software	Successfully testing computational software to detect code mistakes is impacted by multiple factors. One factor is the tolerance accepted in test output. Other factors are the nature of the code mistake, the characteristics of the code structure, and the choice of test input. We have found that randomly generated test input is a viable approach to testing for code mistakes and that simple structural metrics have little predictive power in the type of testing required. We provide further evidence that reduction of tolerance in expected test output has a much larger impact than running many more tests to discover code mistakes.		Diane Kelly;Robert Gray;Yizhen Shao	2011	J. Comput. Science	10.1016/j.jocs.2010.12.002	non-regression testing;random testing;development testing;regression testing;test data generation;code review;fuzz testing;white-box testing;manual testing;computer science;software reliability testing;theoretical computer science;functional testing;dynamic testing;linear code sequence and jump;risk-based testing;smoke testing;software testing;code coverage;test management approach;algorithm;static program analysis	Logic	-61.94370093439336	34.690317618183215	6306
0957e9d6c6ffde6a74a5ea88c1facf58effe2be6	methods and tools for aerospace operations modeling and simulation: modeling the space shuttle	modeling and simulation;probability distribution;discrete event simulation	We summarize our methodology for modeling space shuttle processing using discrete event simulation. Why the project was initiated, what the overall goals were, how it was funded, and who were the members of the project team are identified. We describe the flow of the space shuttle flight hardware through the supporting infrastructure and how the model was created to accurately portray the space shuttle. The input analysis methodology that was used to populate the model elements with probability distributions for process durations is described in the paper. Verification, validation, and experimentation activities are briefly summarized.	experiment;population;simulation	Grant R. Cates;Martin J. Steele;Mansooreh Mollaghasemi;Ghaith Rabadi	2002			probability distribution;real-time computing;simulation;computer science;engineering;discrete event simulation;modeling and simulation;statistics	SE	-35.55440927961741	27.078469094630343	6310
936b0ed7134fe39650babe5a9809c60a679aaf31	towards efficient data-flow test data generation using klee		Dataflow coverage, one of the white-box testing criteria, focuses on the relations between variable definitions and their uses. Several empirical studies have proved data-flow testing is more effective than control-flow testing. However, data-flow testing still cannot find its adoption in practice, due to the lack of effective tool support. To this end, we propose a guided symbolic execution approach to efficiently search for program paths to satisfy data-flow coverage criteria.We implemented this approach on KLEE and evaluatedwith 30 program subjects which are constructed by the subjects used in previous data-flow testing literature, SIR, SV-COMP benchmarks. Moreover, we are planning to integrate the data-flow testing technique into the new proposed symbolic execution engine, SmartUnit, which is a cloud-based unit testing service for industrial software, supporting coverage-based testing. It has successfully helped several well-known corporations and institutions in China to adopt coverage-based testing in practice, totally tested more than one million lines of real code from industry.	cloud computing;control flow;dataflow;symbolic execution;systemverilog;test data generation;unit testing;white-box testing	Chengyu Zhang;Ting Su;Yichen Yan;Ke Wu;Geguang Pu	2018	CoRR		data mining;empirical research;theoretical computer science;computer science;dataflow;unit testing;software;symbolic execution;test data generation;cloud computing;data flow diagram	SE	-59.73459990936255	34.927085287651764	6315
b083fa1a850efee27a92bd8e1fb426a33b50b7e9	assessing and optimizing microarchitectural performance of event processing systems	benchmarking;instruction cache;performance;data processing;tuning;aggregation operator;complex event processing;high throughput;data structure;fraud detection;performance tuning;supply chain management	Event Processing (EP) systems are being progressively used in business critical applications in domains such as algorithmic trading, supply chain management, production monitoring, or fraud detection. To deal with high throughput and low response time requirements, these EP systems mainly use the CPU-RAM sub-system for data processing. However, as we show here, collected statistics on CPU usage or on CPU-RAM communication reveal that available systems are poorly optimized and grossly waste resources. In this paper we quantify some of these inefficiencies and propose cache-aware algorithms and changes on internal data structures to overcome them. We test the before and after system both at the micro-architecture and application level and show that: i) the changes improve micro-architecture metrics such as clocksper-instruction, cache misses or TLB misses; ii) and that some of these improvements result in very high application level improvements such as a 44% improvement on stream-to-table joins with 6-fold reduction on memory consumption, and order-of-magnitude increase on throughput for moving aggregation operations.	algorithm;algorithmic trading;attribute–value pair;cpu cache;central processing unit;complex event processing;data structure;expectation propagation;garbage collection (computer science);information;java;mathematical optimization;microarchitecture;multi-core processor;optimizing compiler;parallel computing;pattern recognition;query plan;random-access memory;requirement;response time (technology);storage model;throughput;translation lookaside buffer	Marcelo R. N. Mendes;Pedro Bizarro;Paulo Marques	2010		10.1007/978-3-642-18206-8_16	embedded system;parallel computing;real-time computing;computer science	OS	-11.83715130573688	51.896845942341166	6327
6e7a986d07b3fe43e430d1ceb41bf8d4529d9215	integrating software engineering into an intermediate programming class	software engineering;computer science education	The emphasis on software engineering topics in the newest report on the second computer science class will have considerable impact on computer science education. Teaching issues arising from the incorporation of software engineering topics are discussed. Ideas for projects and group activities, together with approaches to encourage students to include ease of maintenance and user-friendly interfaces are provided.	computer science;software engineering;usability	Laurie Honour Werth	1988		10.1145/52964.52979	personal software process;computing;software engineering process group;computer science;software design;social software engineering;informatics engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;resource-oriented architecture;computer-aided software engineering;software requirements;software system;computer engineering;software peer review	SE	-59.07501134763227	27.19520295859634	6342
11eab15c5108956f9abf1672b19e04a6c9642947	performance and energy consumption estimation for commercial off-the-shelf component system design	formal model;life cycle;uml;component;real time;performance;embedded real time systems;state machine;embedded system;embedded systems;time petri net;commercial off the shelf;complex system;energy consumption;system design;system development;early detection;embedded software	Nowadays, component-based embedded real- time systems have been used to improve the system development as well as to keep cost down through the reuse of embedded software applications. Besides, the use of semi-formal models has been widely adopted in the embedded real-time system component and system life cycle due to their friendly and intuitive notations. However, the ever more complex systems of today require modeling methods that allow early detection of potential problems in the initial phases of development. This paper presents the mapping process of UML state machine diagram into a time Petri net with energy constraints so as to estimate execution time and energy consumption in early phases of the embedded real-time component development life cycle. The estimates obtained from the model show that the proposed approach is indeed a good approximation to the respective measures obtained from the real hardware platform.	approximation;complex systems;component-based software engineering;embedded software;embedded system;formal system;modeling and analysis of real time and embedded systems;petri net;real-time clock;real-time computing;real-time transcription;requirement;run time (program lifecycle phase);semiconductor industry;software development process;state diagram;system lifecycle;systems design;uml state machine;unified modeling language	Ermeson Carneiro de Andrade;Paulo Romero Martins Maciel;Tiago Falcão;Bruno Costa e Silva Nogueira;Carlos Araújo;Gustavo Rau de Almeida Callou	2009	Innovations in Systems and Software Engineering	10.1007/s11334-009-0110-7	unified modeling language;embedded system;biological life cycle;complex systems;real-time computing;simulation;embedded software;performance;computer science;engineering;operating system;component;finite-state machine;systems design	Embedded	-41.61503466638422	33.48785505295105	6343
609afc625a5eda80a7e69e8edea03cb0fd6c811f	equivalence is in the eye of the beholder	algebraic specification;first in first out;software engineering;formal method;distributed algorithm	"""We analyze in what sense the two algorithms are and are not equivalent. There is no one notion of equivalence appropriate for all purposes and thus the \insubstantiality of processes"""" may itself be in the eye of the beholder. There are other issues where we disagree with Lamport. In particular, we give a direct equivalence proof for two programs without representing them by means of temporal formulas."""	algorithm;eye of the beholder;temporal logic;turing completeness	Yuri Gurevich;James K. Huggins	1997	Theor. Comput. Sci.	10.1016/S0304-3975(96)00315-5	lamport timestamps;distributed algorithm;combinatorics;discrete mathematics;fifo and lifo accounting;computer science;mathematics;algorithm;statistics;algebra	Theory	-12.119645685575993	21.29948803443578	6345
9350a4be56359afd8f34fdb6ccb1d9463fafc180	application cluster service scheme for near-zero-downtime services	application cluster service apcs;reliability engineering;degradation;failover scheme;state recovery scheme;performance evaluation;application software;availability;distributed computing;control engineering;near zero downtime service application cluster service apcs failover scheme state recovery scheme performance evaluator pev;performance evaluator pev;operating system;distributed computing system;application software availability distributed computing middleware manufacturing reliability engineering control engineering operating systems computer errors degradation;design pattern;manufacturing;middleware;computer errors;operating systems;near zero downtime service	The required reliability in applications of a distributed computer system is continuous service for 24 hours a day, 7 days a week. However, computer failures due to exhaustion of operating system resources, data corruption, numerical error accumulation, and so on, may interrupt services and cause significant losses. Hence, this work proposes an application cluster service (APCS) scheme. The proposed APCS provides both a failover scheme and a state recovery scheme for failure management. The failover scheme is designed mainly to automatically activate the backup application for replacing the failed application whenever it is sick or down. Meanwhile, the state recovery scheme is intended primarily to provide an inheritable design pattern to support applications with state recovery requirements. An application simply needs to inherit and implement this design pattern, and then can accomplish the task of state backup and recovery. Furthermore, a performance evaluator (PEV) that can detect performance degradation and predict time to failure is developed in this study. By using these detection and prediction capabilities, the APCS can perform the failover process before node breakdown. Thus, applying APCS and PEV can enable a distributed computer system to provide services with near-zero-downtime.	backup;cluster analysis;computer;distributed computing;downtime;elegant degradation;failover;high availability;interpreter (computing);numerical analysis;numerical error;numerical stability;operating system;requirement;software design pattern;tree accumulation	Fan-Tien Cheng;Shang-Lun Wu;Ping-Yen Tsai;Yun-Ta Chung;Haw Ching Yang	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570743	embedded system;availability;application software;real-time computing;degradation;computer science;engineering;operating system;middleware;design pattern;manufacturing	HPC	-23.00106088637381	50.64226018546338	6355
8b2d0e9ebff34a8aae3fa96e8425d192cffc4afc	secure personal authentication system for home network	personal computing;software;server client system personal authentication embedded system home aplliances;sensors;authorisation;prototypes;authentication;real time operating system;satisfiability;embedded system;home network;embedded systems;servers;global positioning system;personal computing authorisation embedded systems local area networks microprocessor chips;server client system;intelligent systems;home aplliances;embedded mpu;secure personal authentication system;real time operating system secure personal authentication system home network embedded mpu;personal authentication;authentication servers prototypes sensors global positioning system intelligent systems software;local area networks;microprocessor chips	This paper proposed a personal authentication system using small resources for home use. The following three goals were achieved at the same time by this system: (1) identification of each individual, (2) unique existence of each individual in the specified time and space, and (3) no contradiction in the existence of each individual in view of the space and the time before and after an event. Considering use in homes, the evaluation system consists of an embedded MPU and a real-time operating system, using little resources. This evaluation system showed correct fundamental operation satisfying all above three conditions.	authentication;embedded system;experiment;mpu-401;prototype;real-time clock;real-time operating system	Takako Nonaka;Yuta Uesugi;Tomohiro Hase	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687099	local area network;embedded system;real-time computing;real-time operating system;global positioning system;intelligent decision support system;computer science;sensor;authentication;prototype;authorization;computer security;server;satisfiability	Robotics	-40.10541469274927	51.35362532781475	6358
129bf5f2533798abf1fd2489ef471d6c57f5b8a0	monitoring and visualizing multiprocessor interconnection network contention	interconnection network	Multistage Interconnection Networks (MIN) have been widely used for building large-scale shared-memory multiprocessor systems. Complex interactions between many processors and memory modules through the MIN, (such as interprocessor communication, process scheduling and synchronization and remote-memory access) result in a significantly large space of possible performance behavior and potential performance bottlenecks. To provide inslght into dynamk system performance, we have developed an integrated data collection, analysis, and visualization environment for a MIN-based multiprocessor system, called MINGraph. The MIN-Graph is a graphical instrumentation monitor to aid users in investigating performance problems and in determining an effective way to exploit the high performance capabilities of interconnection network multiprocessor systems. Interconnection network contention is a maJor bottleneck of parallel computing on MIN-based multiprocessors. This paper focuses on evaluating the contention behavior through performance monitoring and visualization. Four sets of system and sclentlfic application programs with different programming and scheduling models and different memory access patterns are monitored and tested to observe the various network contention behaviors. The MIN-Graph is implemented on the BBN GPl000 and the BBN TC2000.	bottleneck (software);central processing unit;dimm;graphical user interface;inter-process communication;interaction;multiprocessing;multistage interconnection networks;parallel computing;performance engineering;performance tuning;scheduling (computing);shared memory	Hiaodong Zhang;Beichang Wang	1993	Concurrency - Practice and Experience	10.1002/cpe.4330050805	parallel computing;real-time computing;computer science;operating system;database;distributed computing	HPC	-9.285189840293905	46.43341546912678	6360
208ec0c3809c18486e29e61bf1cbc4861eab499d	concurrency control and recovery in transactional process management	concurrency control;transaction processing;partial order	The uni ed theory of concurrency control and recovery integrates atomicity and isolation within a common framework, thereby avoiding many of the shortcomings resulting from treating them as orthogonal problems. This theory can be applied to the traditional read/write model as well as to semantically rich operations. In this paper, we extend the uni ed theory by applying it to generalized process structures, i.e., arbitrary partially ordered sequences of transaction invocations. Using the extended uni ed theory, our goal is to provide a more exible handling of concurrent processes while allowing as much parallelism as possible. Unlike in the original uni ed theory, we take into account that not all activities of a process might be compensatable and the fact that these process structures require transactional properties more general than in traditional ACID transactions. We provide a correctness criterion for transactional processes and identify the key points in which the more exible structure of transactional processes implies di erences from traditional transactions.	acid;atomicity (database systems);concurrency (computer science);concurrency control;correctness (computer science);parallel computing	Heiko Schuldt;Gustavo Alonso;Hans-Jörg Schek	1999		10.1145/303976.304007	partially ordered set;optimistic concurrency control;transactional memory;real-time computing;transaction processing;computer science;concurrency control;database;distributed computing;multiversion concurrency control;serializability	DB	-23.61553865258567	47.52845344095919	6366
2c60b2aa26380135fbb0e8682bde735bc299f43f	probabilistic model checking and non-standard multi-objective reasoning		Probabilistic model checking is a well-established method for the automated quantitative system analysis. It has been used in various application areas such as coordination algorithms for distributed systems, communication and multimedia protocols, biological systems, resilient systems or security. In this paper, we report on the experiences we made in inter-disciplinary research projects where we contribute with formal methods for the analysis of hardware and software systems. Many performance measures that have been identified as highly relevant by the respective domain experts refer to multiple objectives and require a good balance between two or more cost or reward functions, such as energy and utility. The formalization of these performance measures requires several concepts like quantiles, conditional probabilities and expectations and ratios of cost or reward functions that are not supported by state-ofthe- art probabilistic model checkers. We report on our current work in this direction, including applications in the field of software product line verification.	model checking;statistical model	Christel Baier;Clemens Dubslaff;Sascha Klüppelholz;Marcus Daum;Joachim Klein;Steffen Märcker;Sascha Wunderlich	2014		10.1007/978-3-642-54804-8_1	reliability engineering;computer science;theoretical computer science;data mining	Logic	-59.726263437238366	32.08170117501498	6372
92e7252b3bfc01cd097c077dc4a7acb122c1c81d	a study of publish/subscribe systems for real-time grid monitoring	whi;instruments;design and development;quasi real time response;publish subscribe system;real time;distributed computing;testing;middleware publish subscribe systems real time grid monitoring distributed scientific instruments quasi real time response electrical power grid grid computing;indexing terms;publish subscribe systems;standards publication;conference paper;computer architecture;standards development;power engineering computing;power grids grid computing message passing middleware power engineering computing;monitoring;publish subscribe;message passing;publish subscribe systems monitoring real time systems distributed computing grid computing;middleware;real time systems monitoring instruments power grids grid computing standards development standards publication middleware computer architecture testing;power grids;distributed scientific instruments;electric power;real time grid monitoring;grid computing;electrical power grid;geographic distribution;real time systems	Monitoring and controlling a large number of geographically distributed scientific instruments is a challenging task. Some operations on these instruments require real-time (or quasi real-time) response which make it even more difficult. In this paper, we describe the requirements of distributed monitoring for a possible future electrical power grid based on real-time extensions to grid computing. We examine several standards and publish/subscribe middleware candidates, some of which were specially designed and developed for grid monitoring. We analyze their architecture and functionality, and discuss the advantages and disadvantages. We report on a series of tests to measure their real-time performance and scalability.	grid computing;middleware;publish–subscribe pattern;real-time clock;real-time computing;real-time transcription;requirement;scalability	Chenxi Huang;Peter R. Hobson;Gareth A. Taylor;Paul Kyberd	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370550	message passing;real-time computing;electric power;index term;computer science;operating system;middleware;database;distributed computing;software testing;publish–subscribe pattern;grid computing	Embedded	-31.652748779738566	48.25657500338338	6380
2e9ee5198cb621b0a74e8584c8b35c8837ce6388	mining associations between quality concerns and functional requirements		The cost and effort of developing software systems in a new technical area can be extensive. An organization must perform a domain analysis to discover competing products, analyze their architectures and features, and ultimately discover and specify product requirements. However, delivering high quality products, depends not only on gaining an understanding of functional requirements, but also of qualities such as performance, reliability, security, and usability. Discovering such concerns early in the requirements process drives architectural design decisions. This paper extends our prior work on mining functional requirements from large collections of domain documents, by proposing and evaluating a new technique for discovering and specifying quality concerns related to specific functional components. We evaluate our approach against three domains of Positive Train Control, Electronic Health Records, and Medical Infusion Pumps, and show that it significantly outperforms a basic information retrieval approach. Finally we classified the forms of retrieved information, discussed the utility of different types, and conducted a small study with an experienced engineer to investigate the quality of requirements produced using our approach.	computer performance;display resolution;domain analysis;functional requirement;information retrieval;software system;usability	Xiaoli Lian;Jane Cleland-Huang;Xiang Lin	2017	2017 IEEE 25th International Requirements Engineering Conference (RE)	10.1109/RE.2017.68	data mining;software system;non-functional testing;systems engineering;domain analysis;usability;functional requirement;computer science;positive train control	SE	-60.188171562013004	26.00543664731599	6387
abc4930a2ce9320bfee3578e7fbbeb44d4471299	targeted and anonymized smartphone-based public health interventions in a participatory sensing system	public healthcare sensors data collection data privacy servers privacy conferences;hpsn smartphone based public health intervention participatory sensing system mobile computing telecommunication technology data privacy data security mobile device anonymous public health data collection devices health participatory sensing network;smart phones biomedical communication data privacy health care information dissemination medical information systems mobile computing security of data	Public health interventions comprising information dissemination to affect behavioral adjustment have long been a significant component of public health campaigns. However, there has been limited development of public health intervention systems to make use of advances in mobile computing and telecommunications technologies. Such developments pose significant challenges to privacy and security where potentially sensitive data may be collected. In our previous work we identified and demonstrated the feasibility of using mobile devices as anonymous public health data collection devices as part of a Health Participatory Sensing Network (HPSN). An advanced capability of these networks extended in this paper would be the ability to distribute, apply, report on and analyze the usage and effectiveness of targeted public health interventions in an anonymous way. In this paper we describe such a platform, its place in the HPSN and demonstrate its feasibility through an implementation.	aggregate data;data collection;digital distribution;health information systems;information system;mobile computing;mobile device;participatory sensing;privacy;smartphone;anonymized	Andrew Clarke;Robert Steele	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944421	engineering;internet privacy;world wide web;computer security	Mobile	-43.292952379168874	60.19067931161645	6388
647df467454a6055ec86db56435c80a145557fa5	bioprocess control: advances and challenges	chemical process industry;advanced process control;real time;data management;bioprocess control;sterilization;research and development;process control;alarm management;biological systems;tools and techniques;bioprocess instrumentation	Most commercial process control tools and techniques used in the chemical process industries are applicable to bioprocesses (e.g. processes that make certain medicines). While bioprocesses include several different unit operations, the focus of most of this paper will be the control of bioreactors (i.e. fermentors) in which a near optimal environment is desired for microorganisms to grow, multiply, and produce a desired product. Bioreactor control provides special challenges due to significant process variability, the complexity of biological systems, the need, in many cases, to operate in a sterile environment, and the relatively few real-time direct measurements available that help define the state of the culture. This has led to some creative solutions as well as the identification of topics where further research and development and vendor software functionality are needed.		Joseph S. Alford	2006	Computers & Chemical Engineering	10.1016/j.compchemeng.2006.05.039	advanced process control;data management;engineering;industrial engineering;instrumentation and control engineering;process engineering;process control;control theory;bioprocess engineering;biochemical engineering;sterilization	SE	-57.62866637598468	8.465913468240059	6400
ce47bdef8ae44ab83a6ede5a9815a69c1238fd2f	measuring maintainability of dpra models: a pragmatic approach.		Dynamic Probabilistic Risk Assessment (DPRA) is a powerful concept that is used to evaluate design and safety of complex industrial systems. A DPRA model uses a conceptual system representation as a formal basis for simulation and analysis. In this paper we consider an adaptive maintenance of DPRA models that consist in modifying and extending a simplified model to a real-size DPRA model. We propose an approach for quantitative maintainability assessment of DPRA models created with an industrial modeling tool called PyCATSHOO. We review and adopt some metrics from conceptual modeling, software engineering and OO design for assessing maintainability of PyCATSHOO models. On the example of well-known ”Heated Room” test case, we illustrate how the selected metrics can serve as early indicators of model modifiability and complexity. These indicators would allow experts to make better decisions early in the DPRA model development life cycle.	conceptual system;design pattern;risk assessment;scalability;self-replication;simulation;software development process;software engineering;test case	Irina Rychkova;Fabrice Boissier;Hassane Chraibi;Valentin Rychkov	2017			reliability engineering;maintainability;computer science	SE	-57.338530467759824	25.596465423518545	6425
ae146bb053d75205e8761981dd412fa1c1736153	automated management of dynamic component dependency for runtime system reconfiguration	dynamic component dependency;runtime system reconfiguration;program diagnostics;instruments;instruments runtime context heuristic algorithms component architectures synchronization dynamics;component based system automated management dynamic component dependency runtime system reconfiguration;dynamic dependency mealy machine;dynamic dependency mealy machine runtime system reconfiguration dynamic component dependency;object oriented programming;runtime;program diagnostics object oriented programming operating systems computers;dynamics;synchronization;heuristic algorithms;operating systems computers;context;component architectures	Runtime reconfigurations of component-based systems must be undertaken with careful considerations of dependency between components. The safer and less disruptive a reconfiguration strategy is, the more accurate dependency information it needs. This paper proposes to manage dynamic dependency between components with mealy machine automatically derived from the implementation of components. To maintain the current dependency information for a component at runtime, the corresponding machine is instrumented into the component implementation in such a way that it is always synchronized with the execution of the component. We implemented a prototypical tool for this approach and evaluated it with a realistic benchmark application. The results show that our approach achieves a high accuracy and keeps low overheads without introducing any manual work.	benchmark (computing);component-based software engineering;java;mealy machine;program analysis;run time (program lifecycle phase);runtime system;symbolic execution;thread (computing)	Ping Su;Chun Cao;Xiaoxing Ma;Jian Lu	2013	2013 20th Asia-Pacific Software Engineering Conference (APSEC)	10.1109/APSEC.2013.66	synchronization;dynamics;parallel computing;real-time computing;computer science;distributed computing;dependency inversion principle;programming language;object-oriented programming	SE	-22.12231921376885	37.08387215565916	6435
e89f85384bc31760382c07155d5eb67ead66cea9	trust management for distributed heterogeneous systems by using linguistic term sets and hierarchies, aggregation operators and mechanism design	linguistic fuzzy term sets;trust management;mechanism design;heterogeneity	The current trend in system development is integrating already-existing systems in order to realize large-scale infrastructures. Several of these infrastructures exhibit stringent security requirements that must be handled by properly managing the trust relationships within both the different systems involved, and all the external entities interacting with the integrated infrastructure. Trust management is made complex by the intrinsic heterogeneity characterizing the integrated systems. To handle such a heterogeneity, we propose the application of fuzzy logic, combining it with proper means to deal with heterogeneous fuzzy sets. We present a technique to combine qualitative and quantitative specifications of trust scores aiming at periodically computing a new trust degree, by also considering reputation scores collected from other systems within the infrastructure. Last, we applied the game theory in order to promote truth-telling behavior during the process of reputation information collection. We finally present some experimental results showing the effectiveness of our approach in heterogeneous distributed environments.	trust management (managerial science)	Christian Esposito;Aniello Castiglione;Francesco Palmieri;Massimo Ficco	2017	Future Generation Comp. Syst.	10.1016/j.future.2015.12.004	fuzzy logic;game theory;operator (computer programming);computational trust;mechanism design;computer science;management science;distributed computing;software;fuzzy set;reputation	OS	-37.643200568521685	20.146541009402448	6438
8c581854139c628a8c16e36bf48dc5b65d3e26d0	rowhammer.js: a remote software-induced fault attack in javascript	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	A fundamental assumption in software security is that a memory location can only be modified by processes that may write to this memory location. However, a recent study has shown that parasitic effects in DRAM can change the content of a memory cell without accessing it, but by accessing other memory locations in a high frequency. This so-called Rowhammer bug occurs in most of today’s memory modules and has fatal consequences for the security of all affected systems, e.g., privilege escalation attacks. All studies and attacks related to Rowhammer so far rely on the availability of a cache flush instruction in order to cause accesses to DRAM modules at a sufficiently high frequency. We overcome this limitation by defeating complex cache replacement policies. We show that caches can be forced into fast cache eviction to trigger the Rowhammer bug with only regular memory accesses. This allows to trigger the Rowhammer bug in highly restricted and even scripting environments. We demonstrate a fully automated attack that requires nothing but a website with JavaScript to trigger faults on remote hardware. Thereby we can gain unrestricted access to systems of website visitors. We show that the attack works on off-the-shelf systems. Existing countermeasures fail to protect against this new Rowhammer attack.	algorithm;cpu cache;differential fault analysis;dynamic random-access memory;generic programming;image scaling;javascript;machine code;physical access;row hammer;software bug;superuser	Daniel Gruss;Clémentine Maurice;Stefan Mangard	2016		10.1007/978-3-319-40667-1_15	embedded system;computer science;world wide web;computer security;algorithm	Security	-55.487665922590416	57.62837405226529	6443
3a1c3d0c976f15ad0f6050ceea03d3b9b281a805	a methodology and tool support for widget-based web application development	widgetizing methodology;web application reengineering;web widgets;web development;community information systems	Due to the rapid evolution of Web technologies and standards like WebRTC for the real-time Web, there is a reengineering pressure on many existing Web applications for not getting outdated and for reducing costs and maintainance efforts. In this paper, we propose a methodology designed to support developers through an application reengineering process for achieving modular and scalable Web applications, by bridging the old and the new: a RESTful microservice architecture with a presentation layer composed from widgets. The methodology is based on empirical studies conducted with the help of the widget developer community and proposes an agile development cycle and guidelines for the redesign activities. Based on its principles, we developed a tool that has been used in the methodology evaluation for modeling the widget-based Web applications. The obtained results show that the approach and the proposed architecture are suitable for enabling the future generation of widget-based applications.	web application development	Petru Nicolaescu;Ralf Klamma	2015		10.1007/978-3-319-19890-3_33	web service;web application security;web development;web application;web modeling;data web;web analytics;web mapping;web-based simulation;web design;human–computer interaction;web standards;computer science;software engineering;web navigation;web intelligence;web engineering;programming language;web 2.0;world wide web;mashup	HCI	-49.073845153067715	21.25200395485903	6447
0875af06d43846d4d4fb85b3e9a860e9f7377a2f	programming rational agents in a modal action logic	reasoningwith incomplete knowledge;reasoning about action;logic based agents;rational agent;reasoning with incomplete knowledge;frame problem;logic programming;web based system;reasoning about actions;logic programs;modal and multimodal logic	In this paper we describe a language for reasoning about actions that can be used for modelling and for programming rational agents. We propose a modal approach for reasoning about dynamic domains in a logic programming setting. Agent behavior is specified by means of complex actions which are defined using modal inclusion axioms. The language is able to handle knowledge producing actions as well as actions which remove information. The problem of reasoning about complex actions with incomplete knowledge is tackled and the temporal projection and planning problems is addressed; more specifically, a goal directed proof procedure is defined, which allows agents to reason about complex actions and to generate conditional plans. We give a non-monotonic solution for the frame problem by making use of persistency assumptions in the context of an abductive characterization. The language has been used for implementing an adaptive web-based system.	abductive reasoning;action algebra;conditional (computer programming);frame problem;logic programming;modal logic;modal operator;rational agent;web application	Matteo Baldoni;Alberto Martelli;Viviana Patti;Laura Giordano	2004	Annals of Mathematics and Artificial Intelligence	10.1023/B:AMAI.0000031196.24935.b5	dynamic logic;knowledge representation and reasoning;rational agent;description logic;frame problem;computer science;artificial intelligence;non-monotonic logic;model-based reasoning;machine learning;mathematics;reasoning system;automated reasoning;deductive reasoning;logic programming;multimodal logic;algorithm;abductive logic programming;temporal logic of actions	AI	-17.91137424613466	8.11757409214333	6454
46dde34089a45533b399a44508d1e49a27c6d40b	formal verification and testing based on p systems	articulo;formal verification;p system;formal verification and testing based on p systems	In this paper it is surveyed the set of formal verification methods and testing approaches used so far for applications based on P systems.	formal verification;p system	Marian Gheorghe;Florentin Ipate;Ciprian Dragomir	2009		10.1007/978-3-642-11467-0_5	verification;formal methods;formal verification;software verification;formal specification;formal equivalence checking;refinement;intelligent verification;functional verification	Logic	-45.25922533166161	30.89379811044742	6466
521638d497c6d95c941b88d2d5d17bd0fc9bedda	revisiting software zero-copy for web-caching applications with twin memory allocation	performance improvement;lightweight software zero-copy mechanism;zero-copying data;twin memory allocation;lightweight data protection mechanism;cpu consumption;specific zcopy memory allocator;vanilla linux;revisiting software zero-copy;web-caching application;twin memory allocator;key concern	A key concern with zero copy is that the data to be sent out might be mutated by applications. In this paper, focusing specially on web-caching application, we observe that in most cases the data to be sent out is not supposed to be mutated by applications, while the metadata around it does get mutated. Based on this observation, we propose a lightweight software zero-copy mechanism that uses a twin memory allocator to allocate spaces for zerocopying data, and ensures such data is unchanged before being sent out with a lightweight data protection mechanism. The only change required to an application is to allocate zero-copying data through a specific ZCopy memory allocator. To demonstrate the effectiveness of ZCopy, we have designed and implemented a prototype based on Linux and ported two applications with very little effort. Experiments with Memcached and Varnish shows that show that ZCopy can achieve up to 41% performance improvement over the vanilla Linux with less CPU consumption.	central processing unit;information privacy;linux;memcached;memory management;multi-core processor;protection mechanism;prototype;slab allocation;web cache;zero-copy	Xiang Song;Jicheng Shi;Haibo Chen;Binyu Zang	2012			embedded system;real-time computing;computer hardware;computer science;operating system	OS	-16.40695711675994	53.03097150688751	6469
363f1932a907492f7a06b4c0ee7ab27a178b4d33	on-the-fly model checking of rctl formulas	verification symbolique;design process;concepcion sistema;symbolic verification;sistema informatico;computer system;specification language;formal verification;model checking;system design;on the fly;verification formelle;systeme informatique;lenguaje especificacion;langage specification;regular expression;reachability analysis;conception systeme;analyse atteignabilite	The specification language RCTL, an extension of CTL, is defined by adding the power of regular expressions to CTL. In addition to being a more expressive and natural hardware specification language than CTL, a large family ofRCTL formulas can be verified on-the-fly (during symbolic reachability analysis). On-the-fly model checking, as a powerful verification paradigm, is especially efficient when the specification is false and extremely efficient when the computation needed to get to a failing state is short. It is suitable for the inherently gradual design process since it detects a multitude of bugs at the early verification stages, and paves the way towards finding the more complex errors as the design matures. It is shown that for every erroneous finite computation, there is an RCTL formula that detects it and can be verified on-the-fly. On-thefly verification of RCTL formulas has moved model checking in IBM into a different class of designs inaccessible by prior techniques.	computation;failure;model checking;programming paradigm;reachability;regular expression;ronin;software bug;specification language;tali'zorah;usability	Ilan Beer;Shoham Ben-David;Avner Landver	1998		10.1007/BFb0028744	model checking;design process;specification language;formal verification;computer science;theoretical computer science;programming language;regular expression;algorithm;systems design	Logic	-15.829632009007462	27.908802393809665	6472
6aa21e1609459db01666c917ded4add4100d4238	on constructive cut admissibility in deduction modulo	rewrite rule;theoretical framework;cut elimination;proof search;automated theorem proving	Deduction modulo is a theoretical framework which allows the introduction of computational steps in deductive systems. This approach is well suited to automated theorem proving. We describe a proofsearch method based upon tableaux for Gentzen’s intuitionistic LJ extended with rewrite rules on propositions and terms . We prove its completeness with respect to Kripke structures. Then we give a soundness proof with respect to cut-free LJ modulo. This yields a constructive proof of semantic cut elimination, which we use to characterize the relation between tableaux methods and cut elimination in the intuitionistic case.	automated theorem proving;free variables and bound variables;intuitionistic logic;kripke structure (model checking);lightweight java;logic programming;long division;method of analytic tableaux;modulo operation;natural deduction;normalisation by evaluation;normalization property (abstract rewriting);rewrite (programming);rewriting;semantic analysis (compilers);usability	Richard Bonichon;Olivier Hermant	2006		10.1007/978-3-540-74464-1_3	computer science;automated theorem proving;programming language;algorithm	Logic	-14.529416418812247	17.383960607378782	6474
af70c60e3d4deea9becbf8263ef704e705efffee	ordered semiring-based trust establish model with risk evaluating	credential graph;privacy;risk evaluation;semiring;trust management	Distributed trust management supports the provision of the required levels in a flexible and scalable manner by locally discriminating between the entities with which a principal should interact. However, there is a tension between the preservation of privacy and the controlled release of information when an entity submits credentials for establishing and verifying trust metric where it may disclose too much information of the credentials. Furthermore, trust delegation will require some levels of risk to be tolerated. In this paper, we propose a trust model with privacy protecting and risk evaluating. Our model is based on an ordered semiring framework. In proposed ordered semiring framework, credential graph is flexible enough to express trust relationship which adapts to the trust and privacy risk aggregating and concentrating in decentralized network systems. We describe a solution model to establish trust with risk evaluating with the trust opinion and privacy opinion, and also provide the minimized privacy disclosure credential search algorithm based on ordered semiring model.	case preservation;chain of trust;credential;entity;privacy;scalability;search algorithm;trust management (information system);trust metric;verification and validation	Mingwu Zhang;Bo Yang;Shenglin Zhu;Wenzheng Zhang	2009	I. J. Network Security		trust anchor;computer science;semiring;data mining;internet privacy;privacy;computer security	Security	-43.43386872859305	57.45069131421954	6480
0d8af0068ca4861193d843e21dc2f2b292ec3855	a kripke logical relation between ml and assembly	logical relation;verification;transition state;ml language;langage de bas niveau;sistema transicion;state transition system;gestion memoire;lenguaje ensamblador;mundos posibles e imposibles;mondes possibles et impossibles;modele kripke;compilateur;langage ml;storage management;ramasse miettes;langage evolue;lambda calculus;codigo mutante;program verification;compiler;garbage collection;transition system;low level language;recogemigas;kripke model;gestion memoria;verificacion programa;step indexed kripke logical relations;systeme transition;langage assembleur;compositional compiler correctness;estado transitorio;theory;indexation;garbage collector;lenguaje de bajo nivel;possible and impossible worlds;lambda calculo;lenguaje evolucionado;biorthogonality;runtime system;modelo kripke;assembler;verification programme;lambda calcul;high level language;localized state;possible worlds;languages;self modifying code;etat transition;programme audomodifiable;compilador	"""There has recently been great progress in proving the correctness of compilers for increasingly realistic languages with increasingly realistic runtime systems. Most work on this problem has focused on proving the correctness of a particular compiler, leaving open the question of how to verify the correctness of assembly code that is hand-optimized or linked together from the output of multiple compilers. This has led Benton and other researchers to propose more abstract, compositional notions of when a low-level program correctly realizes a high-level one. However, the state of the art in so-called """"compositional compiler correctness"""" has only considered relatively simple high-level and low-level languages.  In this paper, we propose a novel, extensional, compiler-independent notion of equivalence between high-level programs in an expressive, impure ML-like λ-calculus and low-level programs in an (only slightly) idealized assembly language. We define this equivalence by means of a biorthogonal, step-indexed, Kripke logical relation, which enables us to reason quite flexibly about assembly code that uses local state in a different manner than the high-level code it implements (e.g. self-modifying code). In contrast to prior work, we factor our relation in a symmetric, language-generic fashion, which helps to simplify and clarify the formal presentation, and we also show how to account for the presence of a garbage collector. Our approach relies on recent developments in Kripke logical relations for ML-like languages, in particular the idea of possible worlds as state transition systems."""	assembly language;compiler correctness;correctness (computer science);garbage collection (computer science);high- and low-level;lambda calculus;local variable;logical relations;possible world;runtime system;self-modifying code;state transition table;turing completeness	Chung-Kil Hur;Derek Dreyer	2011		10.1145/1926385.1926402	computer science;theoretical computer science;lambda calculus;garbage collection;programming language;algorithm	PL	-19.03925509669278	22.391935715111803	6481
0d0d8d6e6620b11054723f543caac3b3843244eb	designing a bloom filter for differential file access	differential file;design process;bloom filter;database design	The use of a differential file for a database update can yield integrity and performance benefits, but it can also present problems in providing current data to subsequent accessing transactions. A mechanism known as a Bloom filter can solve these problems by preventing most unnecessary searches of the differential file. Here, the design process for a Bloom filter for an on-line student database is described, and it is shown that a very effective filter can be constructed with a modest expenditure of system resources.	bloom filter;online and offline	Lee L. Gremillion	1982	Commun. ACM	10.1145/358628.358632	real-time computing;design process;computer science;bloom filter;database;programming language;world wide web;database design	DB	-20.683813252357197	48.35435644580446	6484
5813c168299d16851cab054a402c3105d9192348	smatch model: extending rbac sessions in virtualization environment	virtual machine;virtualization;authorisation;switch;authentication;security management;virtualization orbac session switch;context model;virtual machines;virtual machines authorisation;access control;organizations;virtual environment;virtual machines smatch model virtualization environment rbac sessions secure management of switch first order logic access control authentication policies;switches context access control organizations authentication context modeling;switches;context modeling;session;context;first order logic;orbac	This paper extends RBAC sessions with share ability, reusability and switch ability properties. We define the Smatch (Secure Management of switch) model in which authorized users can join, leave, reopen and reuse dynamic sessions. In Smatch, subjects can also share sessions and dynamically switch their role or function with other subjects from the same or die rent organizations. Subjects can authenticate using their function which will automatically activate the set of roles associated with this function. The Smatch model is based on first order logic with actions. It provides means to specify contextual access control and authentication policies which apply to control functional behavior of dynamic sessions. We suggest an implementation of Smatch using virtual machines.	access control list;authentication;authorization;first-order logic;hardware virtualization;role-based access control;virtual machine	Nora Cuppens-Boulahia;Frédéric Cuppens;Marie Nuadi	2011	2011 Sixth International Conference on Availability, Reliability and Security	10.1109/ARES.2011.13	computer science;distributed computing;computer security;computer network	Security	-49.10293090824328	53.98666477628963	6486
07d0a2821d39ad886e2457e200b5f6ec4438390c	practical experience report: the performance of paxos in the cloud		This experience report presents the results of an extensive performance evaluation conducted using four opensource implementations of Paxos deployed in Amazon’s EC2. Paxos is a fundamental algorithm for building fault-tolerant services, at the core of state-machine replication. Implementations of Paxos are currently used in many prototypes and production systems in both academia and industry. Although all protocols surveyed in the paper implement Paxos, they are optimized in a number of different ways, resulting in very different behavior, as we show in the paper. We have considered a variety of configurations and failure-free and faulty executions. In addition to reporting our findings, we propose and assess additional optimizations to existing implementations.	algorithm;cloud computing;correctness (computer science);experiment;fault tolerance;heart rate variability;library (computing);open-source software;paxos (computer science);performance evaluation;software prototyping;state machine replication;throughput	Parisa Jalili Marandi;Samuel Benz;Fernando Pedone;Kenneth P. Birman	2014	CoRR		real-time computing;operating system;distributed computing;computer security	OS	-23.617272961187055	52.767323847433836	6493
8cf1d5e31a51e5262f115d207d071b3551d04115	virtual leashing: internet-based software piracy protection	internet computer crime protection application software web server network servers delay cryptography computer science coprocessors;software splitting;software splitting virtual leashing internet based software piracy protection;application software;software piracy;internet based software piracy protection;computer crime;software engineering;coprocessors;protection;network servers;virtual leashing;internet;cryptography;software engineering computer crime internet;web server;computer science;reverse engineering	Software-splitting is a technique for protecting software from piracy by removing code fragments from an application and placing them on a remote trusted server. The server provides the missing functionality but never the missing code. As long as the missing functionality is hard to reverse-engineer, the application cannot run without validating itself to the server. Current software-splitting techniques scale poorly to the Internet because interactions with the remote server are synchronous: the application must frequently block waiting for a response from the server. Perceptible delays due to network latency are unacceptable for many kinds of highly-reactive applications, such as games or graphics applications. This paper introduces virtual leashing, the first non-blocking software-splitting technique. Virtual leashing ensures that the application and the server communicate asynchronously, so the application's performance is independent (within reason) of large or variable network latencies. Experiments show that virtual leashing makes only modest demands on communication bandwidth, space, and computation	blocking (computing);computation;experiment;graphics;interaction;internet;non-blocking algorithm;reverse engineering;server (computing)	Ori Dvir;Maurice Herlihy;Nir Shavit	2005	25th IEEE International Conference on Distributed Computing Systems (ICDCS'05)	10.1109/ICDCS.2005.85	application software;computer science;cryptography;operating system;database;distributed computing;internet privacy;world wide web;computer security;web server;reverse engineering;computer network	Embedded	-54.16650279814574	58.71420576862334	6504
87f5f63949797a1c7777714dbe5d9ea163c751d4	transactional memory scheduling using machine learning techniques	machine learning algorithms;history;support vector machines;hidden markov model;training;hidden markov model lazy snapshot algorithm transactional memory fairness values support vector machine k nearest neighbor;hidden markov models;scheduling;hidden markov model transactional memory scheduling shared memory multicore systems hardware techniques software techniques performance parallel computation thread synchronization locks overhead lazy snapshot algorithm fairness decisions k nearest neighbor machine learning technique fairness decisions accuracy support vector machine model;k nearest neighbor;fairness values;lazy snapshot algorithm;support vector machine;hidden markov models support vector machines training scheduling machine learning algorithms throughput history;synchronisation hidden markov models learning artificial intelligence pattern classification shared memory systems support vector machines;transactional memory;throughput	Current shared memory multi-core systems require powerful software and hardware techniques to support the performance parallel computation and consistency simultaneously. The use of transactional memory results in significant improvement of performance by avoiding thread synchronization and locks overhead. Also, transactions scheduling apparently influences the performance of transactional memory. In this paper, we study the fairness of transactions' scheduling using Lazy Snapshot Algorithm. The fairness of transactions' scheduling aims to balance between transactions types which are read-only and update transactions. Indeed, we support the fairness of the scheduling procedure by a machine learning technique. The machine learning techniques improve the fairness decisions according to transactions' history. The experiments in this paper show that the throughput of the Lazy Snapshot Algorithm is improved with a machine learning support. Indeed, our experiments show that the learning significantly affects the performance if the durations of update transactions are much longer than read-only ones. We also study several machine learning techniques to investigate the fairness decisions accuracy. In fact, K-Nearest Neighbor machine learning technique shows more accuracy and more suitability, for our problem, than Support Vector Machine Model and Hidden Markov Model.	experiment;fairness measure;hidden markov model;k-nearest neighbors algorithm;lazy evaluation;lock (computer science);machine learning;markov chain;model of computation;multi-core processor;overhead (computing);parallel computing;read-only memory;scheduling (computing);shared memory;snapshot algorithm;snapshot isolation;supervised learning;support vector machine;synchronization (computer science);throughput;transactional memory;unsupervised learning	Basem Assiri;Costas Busch	2016	2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)	10.1109/PDP.2016.21	support vector machine;instance-based learning;parallel computing;computer science;theoretical computer science;machine learning;distributed computing;active learning;hidden markov model	DB	-12.34545920456849	57.55647009950904	6505
0dec650fd9942a1789b16db969951b517644f2a6	mobility patterns	databases;moving object;location server;protocols;state based location update protocol;databases protocols mobile computing cities and towns computer science computer networks cellular phones personal digital assistants personal communication networks data models;personal communication networks;moving object database;moving objects database;location aware services;location update;location information;moving objects database location aware services location information location server location update strategy mobility pattern state based mobility model state based location update protocol;linear functionals;computer networks;personal digital assistants;spatiotemporal phenomena distributed databases mobile computing protocols;distributed databases;spatiotemporal phenomena;cities and towns;mobility pattern;location awareness;computer science;distributed database management systems;mobility model;mobile computing;state based mobility model;cellular phones;location update strategy;data models	We present a data model for tracking mobile objects and reporting the result of continuous queries. The model relies on a discrete view of the spatio-temporal space, where the 2D space and the time axis are respectively partitioned in a finite set of user-defined areas and in constantsize intervals. We define a query language to retrieve objects that match mobility patterns describing a sequence of moves and discuss evaluation techniques to maintain incrementally the result of queries.	data model;optic axis of a crystal;prototype;query language;relevance;result set;sql;web application	Cédric du Mouza;Philippe Rigaux	2003		10.1109/ICPP.2003.1240628	data modeling;communications protocol;computer science;operating system;data mining;database;mobility model;mobile computing;world wide web;computer network	DB	-30.916420103488136	16.82226334766437	6511
c1e42f479626335238f2ec984de03328a9fb574b	evaluation of the unix host for a model development environment	rapid prototyping;model development;gpss;technical report	The needs for model development and the functionality of UNIX are reviewed in order to evaluate the capability of UNIX for either hosting an environment or supporting the development of an environment. Based on an ideal system comparison, the deficiencies of UNIX serve to define the second iteration in a rapid prototyping effort.	iteration;rapid prototyping;unix	Richard E. Nance;Osman Balci;Robert L. Moose	1984			unix architecture;embedded system;computer hardware;computer science;technical report;gpss;operating system;world wide web	SE	-32.012797755939786	27.658024466506795	6514
bcd63b33dd9d5c624b8fa7a1daea3d8e551c519a	towards application portability in platform as a service	companies;runtime;comparison;portability;runtime ecosystems companies middleware abstracts standardization;portability cloud computing platform as a service paas ecosystem comparison;ecosystems;abstracts;platform as a service;paas;middleware;standardization;ecosystem;cloud computing	Cloud Computing has been one of the most vibrant topics in the last years. Especially Platform as a Service (PaaS) is said to be a game changer for future application development. Taking away most of the configuration work, it pledges to foster rapid application development which seems even more important in a world of complex scalable distributed systems. Whereas Infrastructure as a Service (IaaS) is in the process of consolidation and standardization, the PaaS market is largely fragmented offering varying ecosystem capabilities. In this situation, application portability is a major concern for companies utilizing PaaS to avoid vendor lock-in and to retain the ability for future strategical decisions. To categorize portability problems of PaaS, we define a model of current PaaS offerings and identify different portability perspectives. Starting from the model, we derive a standardized profile with a common set of capabilities that can be found among PaaS providers and matched with one another to check application portability based on ecosystem capabilities. We validate our findings with a comprehensive data set of 68 PaaS offerings together with a web-based application for portability matching. We also identify further portability problems by porting the application to different PaaS vendors, validating ecosystem portability and giving hints for future research directions.	application programming interface;categorization;cloud computing;distributed computing;ecosystem;high- and low-level;on-premises software;platform as a service;rapid application development;requirement;scalability;semiconductor consolidation;software portability;user interface;vendor lock-in;web application	Stefan Kolb;Guido Wirtz	2014	2014 IEEE 8th International Symposium on Service Oriented System Engineering	10.1109/SOSE.2014.26	software portability;ecosystem;computer science;operating system;software engineering;database;portability testing;world wide web	Arch	-29.81393784282184	56.04515339863249	6522
a3c8ef29ff9d6085389efe75a081cda93e2d9d18	concurrent structures in game semantics		HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Concurrent structures in game semantics Simon Castellan	causal filter;computation;concurrency (computer science);concurrent computing;database normalization;denotational semantics;game semantics;intensional logic;non-deterministic turing machine;parallel language;programming language;vergence	Simon Castellan	2017	Bulletin of the EATCS		discrete mathematics;nondeterministic algorithm;theoretical computer science;mathematics;operational semantics;denotational semantics of the actor model;action semantics;well-founded semantics;game semantics;denotational semantics;concurrency	Logic	-16.9740109752607	15.525478026821563	6535
6766cf5f4d344fe7e8d37229619c71a9a0b9c3d7	acute stress response for self-optimizing mechatronic systems	objeto de conferencia;self optimizing;real time;software architecture;information processing;acute stress;ciencias informaticas;mechatronic systems;real time and embedded systems	Self-optimizing mechatronic systems have the ability to adjust their goals and behavior according to changes of the environment or system by means of complex real-time coordination and reconfiguration in the underlying software and hardware. In this paper we sketch a generic software architecture for mechatronic systems with selfoptimization and outline which analogies between this architecture and the information processing in natural organisms exist. The architecture at first exploits the ability of its subsystems to adapt their resource requirements to optimize its performance with respect to the usage of available computational resources. Secondly, the architecture achieves, inspired by the acute stress response of a natural being, that in the case of an emergency it makes all recources available to address a given threat in a self-coordinated manner.	computational resource;dependability;information processing;mathematical optimization;mechatronics;real-time transcription;requirement;software architecture	Holger Giese;Norma Montealegre;Thomas Müller;Simon Oberthür;Bernd Schulz	2006		10.1007/978-0-387-34733-2_16	control engineering;real-time computing;simulation;engineering	Embedded	-42.88648945700236	38.417039029417495	6538
13169003d325449c88bf71a485e1b06d80323bb6	knowledgebase compilation for efficient logical argumentation	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments. A common assumption for logic-based argumentation is that an argument is a pair 〈Φ, α〉 where Φ is minimal subset of the knowledgebase such that Φ is consistent and Φ entails the claim α. Different logics are based on different definitions for entailment and consistency, and give us different options for argumentation. For a variety of logics, in particular for classical logic, the computational viability of generating arguments is an issue. Here, we present a solution that involves compiling a knowledgebase ∆ based on the set of minimal inconsistent subsets of ∆, and then generating arguments from the compilation. Whilst generating a compilation is expensive, generating arguments from a compilation is relatively inexpensive.	compiler;knowledge base;while	Philippe Besnard;Anthony Hunter	2006			computer science;data mining;algorithm	AI	-16.354347270935893	7.638745976136913	6547
72ef716a8c91075bd17099f20334982121b45660	an operational domain-theoretic treatment of recursive types	inverse limit;domain theory;recursive types;operational semantics;realisable functor;denotational semantic;denotational semantics;algebraic compactness;generic approximation lemma;operational domain theory;fpc;contextual equivalence	We develop an operational domain theory for treating recursive types with respect to contextual equivalence. The principal approach taken here deviates from classical domain theory in that we do not produce the recursive types via usual inverse limits constructions we have it for free by working directly with the operational semantics. By extending type expressions to functors between some ‘syntactic’ categories, we establish algebraic compactness. To do this, we rely on an operational version of the minimal invariance property, of which a purely operational proof is given.	domain theory;email;goto;linear algebra;observational equivalence;operational semantics;recursion (computer science);simpson's rule;turing completeness;walter pitts	Weng Kin Ho	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.04.013	combinatorics;discrete mathematics;computer science;domain theory;pure mathematics;mathematics;inverse limit;programming language;operational semantics;denotational semantics	Logic	-12.417598544944603	17.06045787416517	6555
a27563fbd845d433687c64c37de3c9986959d986	"""remarks on gregory's """"actually"""" operator"""	complexite;theorem;arthur prior;operator;prior a n;simulation;complexity;universiteitsbibliotheek;actualite;nominals;gregory d;modal logic;preuve;logique modale;operateur;theoreme;actuality;hybrid logic;completeness;sahlqvist theory;decidabilite;completude;proof;logique hybride;theorie sahlqviste;actually operators;decidability	In this note we show that the classical modal technology of Sahlqvist formulas gives quick proofs of the completeness theorems in [8] (D. Gregory, Completeness and decidability results for some propositional modal logics containing “actually” operators, Journal of Philosophical Logic 30(1): 57–78, 2001) and vastly generalizes them. Moreover, as a corollary, interpolation theorems for the logics considered in [8] are obtained. We then compare Gregory's modal language enriched with an “actually” operator with the work of Arthur Prior now known under the name of hybrid logic. This analysis relates the “actually” axioms to standard hybrid axioms, yields the decidability results in [8], and provides a number of complexity results. Finally, we use a bisimulation argument to show that the hybrid language is strictly more expressive than Gregory's language.		Patrick Blackburn;Maarten Marx	2002	J. Philosophical Logic	10.1023/A:1015726824270	modal logic;decidability;discrete mathematics;complexity;theorem;philosophy;epistemology;potentiality and actuality;completeness;computer science;operator;proof;mathematics;algorithm	Theory	-10.725182804628592	13.968899647818782	6572
cc8786711f75e957fbe81e798db07c2fefce644f	supporting gpu sharing in cloud environments with a transparent runtime consolidation framework	virtual machine;virtualization;paper;tesla c2050;performance;cloud;gpu;cuda;performance improvement;nvidia;algorithms;computer science;cloud computing;operating systems;open source	Driven by the emergence of GPUs as a major player in high performance computing and the rapidly growing popularity of cloud environments, GPU instances are now being offered by cloud providers. The use of GPUs in a cloud environment, however, is still at initial stages, and the challenge of making GPU a true shared resource in the cloud has not yet been addressed.  This paper presents a framework to enable applications executing within virtual machines to transparently share one or more GPUs. Our contributions are twofold: we extend an open source GPU virtualization software to include efficient GPU sharing, and we propose solutions to the conceptual problem of GPU kernel consolidation. In particular, we introduce a method for computing the affinity score between two or more kernels, which provides an indication of potential performance improvements upon kernel consolidation. In addition, we explore molding as a means to achieve efficient GPU sharing also in the case of kernels with high or conflicting resource requirements. We use these concepts to develop an algorithm to efficiently map a set of kernels on a pair of GPUs. We extensively evaluate our framework using eight popular GPU kernels and two Fermi GPUs. We find that even when contention is high our consolidation algorithm is effective in improving the throughput, and that the runtime overhead of our framework is low.	algorithm;cloud computing;emergence;fermi (microarchitecture);graphics processing unit;kernel (operating system);open-source software;overhead (computing);processor affinity;requirement;semiconductor consolidation;supercomputer;throughput;virtual machine;x86 virtualization	Vignesh T. Ravi;Michela Becchi;Gagan Agrawal;Srimat T. Chakradhar	2011		10.1145/1996130.1996160	parallel computing;real-time computing;cloud computing;computer science;operating system;distributed computing	HPC	-16.063350262682384	52.20486730324821	6583
24f4884681cf724e449d995e5394757a1ed97b34	a collective intelligence based approach for satisfying the actors requirements in web services composition	social network services;standards;collaboration web service interactive composition requirement end user collective intelligence interaction;collaboration;semantics;web services;web services human computer interaction service oriented architecture;web services collaboration social network services standards context semantics;context;collective intelligence based approach end user satisfaction iwsc interactive web service composition soc service oriented computing	Service-Oriented Computing (SOC) is a new computing paradigm that utilizes service to support the development of rapid, low-cost and easy composition. SOC promotes creation of new services by composition. In the composition process, requirements are described by requestor and Web service offered by the provider, a provider is the owner of service his role is to create service and publish it to make it available to customers and partners. A number of Web services compositions approaches have been presented to satisfy the end user's requirements. Interactive Web services compositions (IWSC) creates new value by adapting the end-user's requirements, the end-user corresponds to the person requesting the service and who will search and invoke the service. With the emergence of collective intelligence (CI), IWSC allows a better end-user's satisfaction. In this paper, we propose a new approach that supports Collective intelligence for satisfying the actor's requirements that suggests a model to help the web services composition. Our approach uses the beneficial roles of collaboration as a key for future services composition. It uses also the interactivity between the actors, user interaction during a service composition will contribute in the satisfaction degree of the actors.	collective intelligence;emergence;interactivity;programming paradigm;requirement;service composability principle;service-oriented device architecture;web service	Ameni Youssfi Nouira;Yassine Jamoussi;Henda Hajjami Ben Ghézala	2015	2015 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2015.242	web service;web modeling;differentiated service;computer science;knowledge management;service delivery framework;ws-policy;service design;ws-addressing;semantics;multimedia;services computing;web intelligence;web 2.0;world wide web;service system;collaboration	Visualization	-46.46557928491921	14.636378959127702	6588
0d885481f31d8a8209036bb407dd730782883ebc	aplicative programming with naperian functors		Array-oriented programming languages such as APL [1] and J [2] pay special attention to manipulating array structures: rank-one vectors (sequences of values), rank-two matrices (which can be seen as rectangular sequences of sequences), rank-three cuboids (sequences of sequences of sequences), rank-zero scalars, and so on. One appealing consequence of this unification is the prospect of rank polymorphism [7]—that a scalar function may be automatically lifted to act element-by-element on a higher-ranked array, a scalar binary operator to act pointwise on pairs of arrays, and so on. For example, numeric function square acts not only on scalars, but also on vectors:	apl;cuboid;programming language;unification (computer science)	Jeremy Gibbons	2017		10.1007/978-3-662-54434-1_21	adjoint functors;natural transformation;functor category;derived functor;tor functor;calculus of functors	Theory	-11.449689701695581	31.841349632435698	6589
f720507fc8f42f6f9257fe13e6a14f6c17ea7021	evaluating and improving the performance and scheduling of hpc applications in cloud	virtualisation cloud computing parallel machines software performance evaluation;virtualization;cloud computing clouds supercomputers virtualization benchmark testing jacobian matrices bandwidth;cloud;characterization hpc cloud performance analysis economics job scheduling application awareness;hpc;clouds;performance analysis;characterization;bandwidth;intelligent application aware dynamic scheduling performance improvement performance evaluation hpc application scheduling cloud computing supercomputers high performance computing applications heterogeneous resources virtualization environments processor configurations cloud virtualization mechanisms online application aware job scheduling multiplatform environments cloudsim;economics;jacobian matrices;job scheduling;application awareness;supercomputers;benchmark testing;cloud computing	Cloud computing is emerging as a promising alternative to supercomputers for some high-performance computing (HPC) applications. With cloud as an additional deployment option, HPC users and providers are faced with the challenges of dealing with highly heterogeneous resources, where the variability spans across a wide range of processor configurations, interconnects, virtualization environments, and pricing models. In this paper, we take a holistic viewpoint to answer the question-why and whoshould choose cloud for HPC, for what applications, and how should cloud be used for HPC? To this end, we perform comprehensive performance and cost evaluation and analysis of running a set of HPC applications on a range of platforms, varying from supercomputers to clouds. Further, we improve performance of HPC applications in cloud by optimizing HPC applications' characteristics for cloud and cloud virtualization mechanisms for HPC. Finally, we present novel heuristics for online application-aware job scheduling in multi-platform environments. Experimental results and simulations using CloudSim show that current clouds cannot substitute supercomputers but can effectively complement them. Significant improvement in average turnaround time (up to 2X)and throughput (up to 6X) can be attained using our intelligent application-aware dynamic scheduling heuristics compared tosingle-platform or application-agnostic scheduling.	cloud computing;cloudsim;heuristic (computer science);holism;job scheduler;scheduling (computing);simulation;software deployment;spatial variability;speaker wire;supercomputer;throughput;web application;x86 virtualization	Abhishek Gupta;Paolo Faraboschi;Filippo Gioachin;Laxmikant V. Kalé;Richard Kaufmann;Bu-Sung Lee;Verdi March;Dejan S. Milojicic;Chun Hui Suen	2016	IEEE Transactions on Cloud Computing	10.1109/TCC.2014.2339858	supercomputer;parallel computing;real-time computing;cloud computing;computer science;operating system;distributed computing	HPC	-22.294572204739733	59.91464864983688	6600
fdaa1438d70360a63b9edaabbf787887a912cc62	the unified extensional versioning model	developpement logiciel;eficacia sistema;architecture systeme;system configuration;sistema informatico;performance systeme;computer software maintenance;computer system;organisation systeme;system performance;organizacion sistema;unified model;maintenance logiciel;desarrollo logicial;software development;arquitectura sistema;systeme informatique;programvaruteknik;system architecture	Versioning of components in a system is a well-researched field where various adequate techniques have already been established. In this paper, we look at how versioning can be extended to cover also the structural aspects of a system. There exist two basic techniques for versioning intentional and extensional and we propose a unified extensional versioning model for versioning of both components and structure in the same way. The unified model is described in detail and three different policies that can be implemented on top of the general model are exemplified/illustrated by three prototype tools constructed by the authors. The model is analysed with respect to the number of versions and configurations it generates and has to manage. Finally, the unified extensional model is compared to more traditional intentional models on some important parameters. The conclusions are that the unified model is indeed viable. It not only provides the functionality offered by the intentional model with respect to flexibility during development and management of combinatoric complexity, but also offers a framework for management of configurations that enables systems to provide much more advanced support than is commonly available.	configuration management;entity;intelligence explosion;prototype;repeatability;software versioning;traceability;unified model;version control	Ulf Asklund;Lars Bendix;Henrik Bærbak Christensen;Boris Magnusson	1999		10.1007/3-540-48253-9_8	simulation;computer science;software development;unified model;computer performance;systems architecture	DB	-42.70320456336989	26.396107703657407	6616
a3684c35378b21af4d1e9e980eb6d5aa6312320f	artist: the android runtime instrumentation and security toolkit		With the introduction of Android 5 Lollipop, the Android Runtime (ART) superseded the Dalvik Virtual Machine (DVM) by introducing ahead-of-time compilation and native execution of applications, effectively deprecating seminal works such as TaintDroid that hitherto depend on the DVM. In this paper, we discuss alternatives to overcome those restrictions and highlight advantages for the security community that can be derived from ART's novel on-device compiler dex2oat and its accompanying runtime components. To this end, we introduce ARTist, a compiler-based application instrumentation solution for Android that does not depend on operating system modifications and solely operates on the application layer. Since dex2oat is yet uncharted, our approach required first and foremost a thorough study of the compiler suite's internals and in particular of the new default compiler backend called Optimizing. We document the results of this study in this paper to facilitate independent research on this topic and exemplify the viability of ARTist by realizing two use cases. In particular, we conduct a case study on whether taint tracking can be re-instantiated using a compiler-based app instrumentation framework. Overall, our results provide compelling arguments for the community to choose compiler-based approaches over alternative bytecode or binary rewriting approaches for security solutions on Android.	ahead-of-time compilation;android;binary recompiler;deprecation;exemplification;foremost;machine code;operating system;optimizing compiler;rewriting;taint checking;virtual machine	Michael Backes;Sven Bugiel;Oliver Schranz;Philipp von Styp-Rekowsky;Sebastian Weisgerber	2017	2017 IEEE European Symposium on Security and Privacy (EuroS&P)	10.1109/EuroSP.2017.43	embedded system;real-time computing;compiler correctness;computer science;operating system;compiler construction;functional compiler;computer security;algorithm	Security	-23.53354440760382	38.59767965124302	6617
61c993346ec2f9318ba09458c17ba14b338bb3bc	a portable compiler for integrating hilog into prolog systems	portable compiler;prolog system		compiler;hilog;prolog	Konstantinos Sagonas;David Scott Warren	1994			computer architecture;programming language	Arch	-22.316977013627625	33.66148431194655	6623
392a9c9069dd60ffe1c18725f04cb1514b49c3b6	distributed programming with a logic channel based coordination model	programmation;distributed programs;software engineering;distributed computer systems;programacion;genie logiciel;coordinacion;systeme informatique reparti;coordination model;programming;coordination	We present a new coordination model and a small set of programming notations for distributed programming, which can be integrated into very different programming languages (imperative, declarative or object-oriented). Together they allow the development of distributed programs in a compositional way, by assembling different independent pieces of (possibly pre-existing and heterogeneous) code. This approach is similar to many other proposals such as Linda, PCN, CC++, for example, allowing multiparadigm and multilingual integration, and provides a powerful set of concurrent programming techniques, inherited from Concurrent Logic Languages (CLLs), which can be efficiently implemented in distributed systems. The coordination model is based on logic channels; these evolved from the concept of shared logic variables used in CLLs which, with the same expressive power, can be more efficiently implemented in distributed systems. We introduce this coordination model, giving some illustrative examples to show its expressiveness; some implementation issues are also commented on.	distributed computing	Manuel Díaz;Bartolomé Rubio;José M. Troya	1996	Comput. J.	10.1093/comjnl/39.10.876	programming;declarative programming;computer science;theoretical computer science;functional logic programming;database;programming paradigm;inductive programming;fifth-generation programming language;programming language;algorithm	Security	-28.365367939231323	31.134706818967512	6627
889db104c79dc672409f932e7ad74f8302879bae	automated offloading of android applications for computation/energy optimizations		Our work presents a methodology and a tool to optimize Android applications using mobile computation offloading techniques. Our demo provides a study about the compatibility of being offloaded of Android methods and a methodology to autonomously classify Android methods based on their functions.	android;computation offloading	Alessandro Zanni;Se-Young Yu;Stefano Secci;Rami Langar;Paolo Bellavista;Daniel F. Macedo	2017	2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOMW.2017.8116525	embedded system;humanoid robot;theoretical computer science;android (operating system);computation;server;computation offloading;mobile telephony;computer science	Visualization	-35.664633691063976	52.59803089335029	6629
0ec8fcbad6c5bf5d08bca61c21d1da7cb30ece6e	a compliant assurance model for assessing the trustworthiness of cloud-based e-commerce systems	e commerce cloud assurance model seal trustworthiness;legislation;cloud based e commerce assurance models compliant assurance model trustworthiness assessment cloud based e commerce systems cloud based e commerce stores information security information privacy web assurance models online consumer trust e commerce legislation e commerce standards adaptive legislation attributes adaptive iso standards advanced user security website availability intelligent cooperative rating analytical hierarchy process page ranking techniques e commerce sites cloud based sites;analytic hierarchy process;electronic commerce;e commerce;iso standards;authentication;cloud;biometrics;misalignment;trustworthiness;trusted computing;distortion;assurance;data privacy;web sites;model;fingerprint;business adaptation models seals legislation security availability analytical models;face;web sites analytic hierarchy process data privacy electronic commerce iso standards legislation security of data trusted computing;security of data;noise;seal	Many cloud-based e-commerce stores aim to attract and retain customers in order to be competitive. However, they are all faced with a challenge regarding gaining and maintaining consumer trust in a volatile cloud-based e-commerce environment where risks pertaining to information security, privacy of information and inadequate monitoring of compliance to applicable laws are prevalent. The pervasiveness of these risks has indirectly propelled the development of web assurance models, which were designed in an attempt to encourage online consumer trust. Regrettably, many of these models have been inadequate in certain areas, such as being unable to provide online real-time assurance on a comprehensive set of attributes, which include a check of compliance to the applicable e-commerce legislation or standards in a cloud-based environment. The aim of this research was to examine whether the integration of the attributes of adaptive legislation, adaptive ISO standards, policies, advanced user security and website availability can be used to develop a compliant assurance model. The model uses an intelligent cooperative rating based on the analytical hierarchy process and page ranking techniques to improve the level of cloud-based trustworthiness. We illustrated in an empirical explanatory survey conducted with 15 test samples from IEEE, Science Direct databases and real life data captured from E-commerce sites that the proposed compliant model strongly contributes to the improvement of cloud-based sites, as well as enhancing the trustworthiness of these websites. The findings of this research study can be used as a reference guide to understand the effectiveness of cloud-based e-commerce assurance models, as well as to enhance the trustworthiness of these models.	analytical hierarchy;cloud computing;database;e-commerce;information security;pagerank;real life;real-time locating system;trust (emotion)	Thembekile O. Mayayise;Isaac Olusegun Osunmakinde	2013	2013 Information Security for South Africa	10.1109/ISSA.2013.6641042	e-commerce;face;fingerprint;trustworthiness;analytic hierarchy process;distortion;cloud computing;computer science;noise;authentication;internet privacy;trustworthy computing;world wide web;computer security;biometrics	Web+IR	-48.95193352513513	58.584686482948484	6631
99eb5b6f10716b80ca6ae5e54f1630ba90347bd1	syntax and semantics of linear dependent types		A type theory is presented that combines (intuitionistic) linear types with type dependency, thus properly generalising both intuitionistic dependent type theory and full linear logic. A syntax and complete categorical semantics are developed, the latter in terms of (strict) indexed symmetric monoidal categories with comprehension. Various optional type formers are treated in a modular way. In particular, we will see that the historically much-debated multiplicative quantifiers and identity types arise naturally from categorical considerations. These new multiplicative connectives are further characterised by several identities relating them to the usual connectives from dependent type theory and linear logic. Finally, one important class of models, given by families with values in some symmetric monoidal category, is investigated in detail.	dependent type	Matthijs Vákár	2014	CoRR		discrete mathematics;mathematics;type theory;algorithm	Logic	-8.367343900635738	15.129188027554058	6635
206d77511e616d1fc2499eeae614cff2e835f4ec	combining fuzzy description logics and fuzzy logic programs	uncertainty;uncertainty handling;ontologies artificial intelligence;fuzzy logic;fuzzy logic uncertainty semantic web ontologies fuzzy reasoning fuzzy sets knowledge representation intelligent agent niobium web sites;horn logic;logic programming;fuzzy logic semantic web rules ontologies description logic horn logic description logic programs uncertainty;cognition;semantic web;ontologies;description logic;uncertainty handling fuzzy logic logic programming ontologies artificial intelligence semantic web;knowledge representation;description logic programs;embedded dl query fuzzy description logic fuzzy logic program ontology semantic web uncertainty handling description logic program fhdlp program uncertainty representation fuzzy hybrid knowledge base fuzzy hybrid rule;knowledge based systems;rules	Integrating rules and ontologies has become a key requirement for applications in the Semantic Web. Web applications in general have also motivated another requirement, that of handling uncertainty, an intrinsic feature of the real world. In this paper, we present a fuzzy extension to Description Logic Programs (DLP), called fhDLP. fhDLP not only combines DL with LP, as in DLP, but also supports uncertainty representation. More specifically, fuzzy hybrid knowledge bases layered on fhDLP consist of fuzzy hybrid rules with embedded DL queries to fuzzy DL concepts, roles, and axioms.	description logic;digital light processing;embedded system;fuzzy logic;logic programming;ontology (information science);semantic web;web application	Jidi Zhao;Harold Boley	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.363	fuzzy logic;t-norm fuzzy logics;knowledge representation and reasoning;fuzzy electronics;description logic;cognition;uncertainty;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;ontology;artificial intelligence;fuzzy number;theoretical computer science;neuro-fuzzy;knowledge-based systems;machine learning;semantic web;data mining;fuzzy associative matrix;logic programming;fuzzy set operations;fuzzy control language	AI	-22.42181946776697	6.145814861921049	6639
13a3ef1a836664a48522c0af4fb50ea29c1e335d	better lemmas with lambda extraction		In Satisfiability Modulo Theories (SMT), the theory of arrays provides operations to access and modify an array at a given index, e.g., read and write. However, common operations to modify multiple indices at once, e.g., memset or memcpy of the standard C library, are not supported. We describe algorithms to identify and extract array patterns representing such operations, including memset and memcpy.We represent these patterns in our SMT solver Boolector by means of compact and succinct lambda terms, which yields better lemmas and increases overall performance. We describe how extraction and merging of lambda terms affects lemma generation, and provide an extensive experimental evaluation of the presented techniques. It shows a considerable improvement in terms of solver performance, particularly on instances from symbolic execution.	ansi c;algorithm;c standard library;c string handling;gnu c library;modulo operation;satisfiability modulo theories;solver;symbolic execution	Mathias Preiner;Aina Niemetz;Armin Biere	2015	2015 Formal Methods in Computer-Aided Design (FMCAD)		database index;benchmark;cognition;computer science;theoretical computer science;programming language;algorithm	Logic	-18.33957136061703	23.492302132255276	6640
e5b8ea9d4763b13552ff185e71882a08baf99b4f	tackling the bus turnaround overhead in real-time sdram controllers		Synchronous dynamic random access memories (SDRAMs) are widely employed in multi- and many-core platforms due to their high-density and low-cost. Nevertheless, their benefits come at the price of a complex two-stage access protocol, which reflects their bank-based structure and an internal level of explicitly managed caching. In scenarios in which requestors demand real-time guarantees, these features pose a predictability challenge and, in order to tackle it, several SDRAM controllers have been proposed. In this context, recent research shows that a combination of bank privatization and open-row policy (exploiting the caching over the boundary of a single request) represents an effective way to tackle the problem. However, such approach uncovered a new challenge: the data bus turnaround overhead. In SDRAMs, a single data bus is shared by read and write operations. Alternating read and write operations is, consequently, highly undesirable, as the data bus must remain idle during a turnaround. Therefore, in this article, we propose a SDRAM controller that reorders read and write commands, which minimizes data bus turnarounds. Moreover, we compare our approach analytically and experimentally with existing real-time SDRAM controllers both from the worst-case latency and power consumption perspectives.	best, worst and average case;bus (computing);cache (computing);experiment;manycore processor;overhead (computing);random access;real-time clock;real-time transcription	Leonardo Ecco;Rolf Ernst	2017	IEEE Transactions on Computers	10.1109/TC.2017.2714672	computer science;parallel computing;memory bus;latency (engineering);control theory;real-time computing;cas latency;idle;system bus;memory bandwidth;random access	Embedded	-9.237528375745503	53.54168180295652	6641
4e2244494d429967f52235939c1dc667bf34f88b	aggro: boosting stm replication via aggressively optimistic transaction processing	software;distributed system;transaction processing concurrency control replicated databases software architecture;protocols;aggressively optimistic concurrency control scheme;concurrent computing;striking performance gain aggro boosting stm replication aggressively optimistic transaction processing software transactional memory disruptive programming model stm system atomic broadcast based active replication protocol aggressively optimistic concurrency control scheme serialization order compliant optimistic message delivery order oab service a priori knowledge;optimistic message delivery order;boosting stm replication;serialization order compliant;aggressively optimistic transaction processing;distributed transactions;striking performance gain;distributed systems distributed transactional memories replication protocols dependability;replication protocols;programming model;aggro;software architecture;a priori knowledge;disruptive programming model;software transactional memory;database systems;dependability;concurrency control;atomic broadcast based active replication protocol;schedules;optimistic concurrency control;protocols concurrency control schedules database systems concurrent computing software;atomic broadcast;simulation study;stm system;distributed systems;transaction processing;distributed transactional memories;oab service;replicated databases	Software Transactional Memories (STMs) are emerging as a potentially disruptive programming model. In this paper we are address the issue of how to enhance dependability of STM systems via replication. In particular we present AGGRO, an innovative Optimistic Atomic Broadcast-based (OAB) active replication protocol that aims at maximizing the overlap between communication and processing through a novel AGGRessively Optimistic concurrency control scheme. The key idea underlying AGGRO is to propagate dependencies across uncommitted transactions in a controlled manner, namely according to a serialization order compliant with the optimistic message delivery order provided by the OAB service. Another relevant distinguishing feature of AGGRO is of not requiring a-priori knowledge about read/write sets of transactions, but rather to detect and handle conflicts dynamically, i.e. as soon (and only if) they materialize. Based on a detailed simulation study we show the striking performance gains achievable by AGGRO (up to 6x increase of the maximum sustainable throughput, and 75% response time reduction) compared to literature approaches for active replication of transactional systems.	atomic broadcast;boosting (machine learning);concurrency (computer science);dependability;optimistic concurrency control;programming model;response time (technology);serialization;simulation;software transactional memory;throughput;transaction processing	Roberto Palmieri;Francesco Quaglia;Paolo Romano	2010	2010 Ninth IEEE International Symposium on Network Computing and Applications	10.1109/NCA.2010.10	communications protocol;software architecture;optimistic concurrency control;parallel computing;real-time computing;atomic broadcast;a priori and a posteriori;concurrent computing;transaction processing;distributed transaction;schedule;computer science;operating system;concurrency control;software transactional memory;dependability;database;distributed computing;programming paradigm	Arch	-22.1921518378329	48.34591942626536	6643
3446d0eefdff9b2d4bf60e7001713e948bfcee76	recursive in a generic real		There is a comeager set C contained in the set of 1-generic reals and a first order structure M such that for any real number X, there is an element of C which is recursive in X if and only if there is a presentation of M which is recursive in X.	recursion (computer science)	Juichi Shinoda;Theodore A. Slaman	2000	J. Symb. Log.		first-order logic;recursive language;mathematics;μ operator	Theory	-5.737672762973338	17.836103797197353	6644
0268324ffbec81f78b5fba3a80bbec1b0539edc1	on non-local propositional and weak monodic quantified ctl	ctl;branching time temporal logic;temporal logic;predicate temporal logic;journal article;non local semantics;first order;first order logic;decidability	In this paper we prove decidability of two kinds of branching time temporal logics. First we show that the non-local version of propositional PCTL , in which truth values of atoms may depend on the branch of evaluation, is decidable. Then we use this result to establish decidability of various fragments of quantified PCTL , where the next-time operator can be applied only to formulas with at most one free variable, all other temporal o perators and path quantifiers are applicable only to sentences, and the first-order construct follow the pattern of any of several decidable fragments of first-order logic.	computational complexity theory;decision problem;display resolution;expressive power (computer science);first-order logic;first-order predicate;free variables and bound variables;german research centre for artificial intelligence;probabilistic ctl	Sebastian Bauer;Ian M. Hodkinson;Frank Wolter;Michael Zakharyaschev	2004	J. Log. Comput.	10.1093/logcom/14.1.3	predicate logic;dynamic logic;zeroth-order logic;decidability;discrete mathematics;linear temporal logic;description logic;higher-order logic;tautology;many-valued logic;interval temporal logic;computation tree logic;computer science;intermediate logic;artificial intelligence;first-order logic;mathematics;predicate variable;ctl*;propositional variable;well-formed formula;algorithm	Logic	-12.574403055003392	14.167545302821628	6647
4a68f287c7ba840efc90171ada597d220b276a14	a double auction economic model for grid services	economie;parallelisme;outil logiciel;economia;software tool;markets;mercado;compra;distributed computing;langage java;economic model;grid;modelo economico;qa75 electronic computers computer science;ciencias economicas;parallelism;modele economique;internet;paralelismo;rejilla;marche;grid service;subasta;bidding;grille;calculo repartido;achat;lenguaje java;vente;sales;sciences economiques;economy;enchere;economics;venta;herramienta software;grid computing;calcul reparti;double auction;purchases;java language	"""The use of a service-based approach in Grid computing will lead to such services becoming valuable economic commodities. Cur- rent economics models on the Internet are concerned with the creation, sale and purchase of commodities (generally information resources), and ways in which buyers and sellers interact in markets for them. Although several initiatives are engaged in the development of Grid technologies, Grid economy issues are yet to be fully addressed. A """"Grid market"""" architecture based on the double auction economic model is proposed. Components of the design, and a prototype of the double auction model framework, which makes use of the Globus and Java CoG toolkits, are subsequently presented."""		Liviu Joita;Omer F. Rana;W. Alex Gray;John C. Miles	2004		10.1007/978-3-540-27866-5_53	the internet;simulation;bidding;computer science;artificial intelligence;economic model;operating system;database;distributed computing;double auction;grid;computer security;auction theory;grid computing	ECom	-39.56507193198035	15.9934745645644	6651
0f1b5bfdd607e7a1a8d7a812f713ab96a64e9fa2	a run-time environment for concurrent objects with asynchronous method calls	distributed system;incremental development;object oriented language;operational semantics;nondeterministic rewrite strategies;distributed environment;object oriented;rewriting logic;object orientation;pseudo random number generator;asynchronous method calls	A distributed system may be modeled by objects that run concurrently, each with its own processor, and communicate by remote method calls. However objects may have to wait for response to external calls; which can lead to inefficient use of processor capacity or even to deadlock. This paper addresses this limitation by means of asynchronous method calls and conditional processor release points. Although at the cost of additional internal nondeterminism in the objects, this approach seems attractive in asynchronous or unreliable distributed environments. The concepts are illustrated by the small object-oriented language Creol and its operational semantics, which is defined using rewriting logic as a semantic framework. Thus, Creol specifications may be executed with Maude as a language interpreter, which allows an incremental development of the language constructs and their operational semantics supported by testing in Maude. However, for prototyping of highly nondeterministic systems, Maude’s deterministic engine may be a limitation to practical testing. To overcome this problem, a rewrite strategy based on a pseudo-random number generator is proposed, providing Maude with nondeterministic behavior.	deadlock;distributed computing;iterative and incremental development;maude system;nondeterministic algorithm;operational semantics;pseudorandom number generator;pseudorandomness;random number generation;rewrite (programming);rewriting	Einar Broch Johnsen;Olaf Owe;Eyvind W. Axelsen	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.06.012	computer science;theoretical computer science;distributed computing;programming language;object-oriented programming	PL	-25.885092151307724	35.81210420212029	6653
5f5b8531339db451ddbef3c29141de07c2f8df7e	flatness is not a weakness	logica temporal;temporal logic;logical programming;program verification;infinite word;verificacion programa;formal verification;model checking;programmation logique;informatique theorique;verification formelle;logic in computer science;verification programme;programacion logica;logique temporelle;reachability analysis;analyse atteignabilite;computer theory;informatica teorica	We propose an extension, called L + p , of the temporal logic LTL, which enables talking about nitely many register values: the models are innnite words over tuples of integers (resp. real numbers). The formulas of L + p are at: on the left of an until, only atomic formulas or LTL formulas are allowed. We prove, in the spirit of the correspondence between automata and temporal logics, that the models of a L + p formula are recognized by a piecewise at counter machine; for each state q, at most one loop of the machine on q may modify the register values. Emptiness of (piecewise) at counter machines is decidable (this follows from a result in 9]). It follows that satissability and model-checking the negation of a formula are decidable for L + p. On the other hand, we show that inclusion is undecidable for such languages. This shows that validity and model-checking positive formulas are undecidable. logic in computer science.	atomic formula;automata theory;counter machine;logic in computer science;model checking;temporal logic;undecidable problem	Hubert Comon-Lundh;Véronique Cortier	2000		10.1007/3-540-44622-2_17	model checking;discrete mathematics;temporal logic;formal verification;computer science;artificial intelligence;mathematics;programming language;algorithm	Logic	-11.622482732516014	24.209213257272072	6658
259d2fcf4a093fde1dcefbcc5704decfd75b26ef	investigating the safe evolution of software product lines	refinement;product line;software product line;software product lines;product line safe evolution	The adoption of a product line strategy can bring significant productivity and time to market improvements. However, evolving a product line is risky because it might impact many products and their users. So when evolving a product line to introduce new features or to improve its design, it is important to make sure that the behavior of existing products is not affected. In fact, to preserve the behavior of existing products one usually has to analyze different artifacts, like feature models, configuration knowledge and the product line core assets. To better understand this process, in this paper we discover and analyze concrete product line evolution scenarios and, based on the results of this study, we describe a number of safe evolution templates that developers can use when working with product lines. For each template, we show examples of their use in existing product lines. We evaluate the templates by also analyzing the evolution history of two different product lines and demonstrating that they can express the corresponding modifications and then help to avoid the mistakes that we identified during our analysis.	evolution;feature model;software product line	Laís Neves;Leopoldo Teixeira;Demóstenes Sena;Vander Alves;Uirá Kulesza;Paulo Borba	2011		10.1145/2047862.2047869	computer science;refinement;product design;programming language;new product development;product engineering	SE	-57.7934249276039	27.69957324705085	6662
e407d0421ff38eb6e559ab77f636c52c61593541	business domain knowledge libraries to support software maintenance activities	software understanding;software maintenance;domain knowledge reuse;requirements recovery;domain knowledge;business domain knowledge;software reuse	The crisii in software development and maintenance has been widely stated. A great deal of effort has, and is, being made towards its solution (CSM 1985 to 1990). However, the probkm persists and it is tending to get worse (Pressman, 1987). Considering the dimension of the software crisis, the authors believe that the solutions required must cover a wide spectrum. The software maintenance component of software development has been recognrzed * asthemain factor responsible for the actual crisis. Domain knowledge is a key to requirements recovery and, therefore, for software maintenance. However, it is usually difficult to assimilate because of its hue, its dispersion and the lack of availability of its sources. Commercial business software accounts for the vast majority of existing software and consequently the hancial burden is high in maintainiing such software. A proposal (purely theoretical) is postulated of a global system for busioesS dOmai0 knowledge capture for hture reuse. The aim is to make business knowledge maximally reusable by making it publicly accessible in a knowledge repository. The proposed system, the Business Knowledge Library System (BKLS), consists of two t y p of library:	business domain;business software;community climate system model;fourth-generation programming language;knowledge management;library (computing);requirement;software crisis;software development;software maintenance	H. M. C. L. Mendes-Moreira;C. G. Davies	1993	Journal of Software Maintenance	10.1002/smr.4360050305	domain analysis;software engineering process group;software mining;business requirements;computer science;systems engineering;engineering;knowledge management;package development process;software development;body of knowledge;software engineering;domain engineering;knowledge-based systems;knowledge engineering;software asset management;management science;procedural knowledge;knowledge extraction;personal knowledge management;business software;software maintenance;knowledge value chain;software deployment;domain knowledge	SE	-59.327906884999315	23.18293197061832	6664
52f39ed16f8b7a2130b61c0f847fdef77094f115	role-based viewing envelopes for information protection in collaborative modeling	engineering;computer aided design;tecnologia electronica telecomunicaciones;user needs;computacion informatica;obscuration;security model;intellectual property;information security;computer graphics;removal;geometry;grupo de excelencia;multi user;blocking;three dimensional;computer programs;protection;3d model;level of detail;ciencias basicas y experimentales;information assurance;collaborative virtual prototyping;multi resolution modeling;computer access control;teams personnel;collaborative distributed design;access control;tecnologias;interactive graphics;multi resolution;collaborative design;role based viewing;distributed collaboration;concurrent engineering;distributed design	Information security and assurance are new frontiers for collaborative design. In this context, information assurance (IA) refers to methodologies to protect engineering information by ensuring its availability, confidentiality, integrity, non-repudiation, authentication, access control, etc. In collaborative design, IA techniques are needed to protect intellectual property, establish security privileges and create “need to know” protections on critical features. Aside from 3D watermarking, research on how to provide IA to distributed collaborative engineering teams is largely non-existent. This paper provides a framework for information assurance within collaborative design, based on a technique we call role-based viewing, in which information security relationships are roles assigned to users based on their permissions and privileges. Role-based viewing is achieved through integration of multi-resolution geometry and with the security model. In this way, 3D models are geometrically partitioned, and the partitioning is used to create multi-resolution mesh hierarchies that obscure, obfuscate, or remove sensitive material from the view of users without appropriate permissions. This approach is the basis for our prototype system FACADE (the Framework for Access-control in Computer-Aided Design Environments), a synchronous, multi-user collaborative modeling environment. In FACADE , groups of users worked in a shared 3D modeling environment in which each user viewing and modeling privileges are managed by a central access control mechanism. In this manner, individual actors see only the data they are allowed to see, at the level of detailed they are permitted to see it.	3d modeling;access control matrix;assembly language;authentication;computer-aided design;confidentiality;design rationale;digital 3d;facade pattern;information assurance;information security;knowledge management;level of detail;multi-user;need to know;non-repudiation;non-uniform rational b-spline;prototype;requirement;role hierarchy;role-based access control	Christopher D. Cera;Taeseong Kim;JungHyun Han;William C. Regli	2004	Computer-Aided Design	10.1016/j.cad.2003.09.014	computer security model;three-dimensional space;simulation;computer access control;computer science;engineering;information security;access control;computer aided design;level of detail;computer graphics;world wide web;computer security;blocking;intellectual property;concurrent engineering;mechanical engineering	EDA	-49.696669250365844	53.88097474160205	6667
e7d20c9c44d67ad46eab21ba0c7dca8671250205	performance evaluation of linked stream data processing engines for situational awareness applications			performance evaluation	Fadwa Lachhab;Mohamed Bakhouya;Radouane Ouladsine;Mohammed Essaaidi	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4380	computer engineering;computer science;distributed computing;database;situation awareness;data processing	OS	-30.459437874639757	39.061767277346235	6688
a006ca124562db8aa28876bbf439364a7ac50f79	an on-line multiprocessing interactive computer system for neurophysiological investigations	digital computers;theoretical model;ucla;data collection;data processing;time sharing;input output;system integration;batch process;on line programming;data reduction;neurophysiology;on line control;man machine systems;conferences	The principal dependencies of neurophysiologists upon the computer are for data collection and analysis, experimental control, and the development of theoretical models. One possible system providing these functions is one that allows several investigators to on-line time-share a moderate sized digital computer capable of performing input, output, and computational functions in a simple interpretive language that is easy to understand and use in a fast decision experimental environment. A community of neurophysiologists in the UCLA Brain Research Institute share such a computer in its data processing laboratory (DPL) by means of remote console stations in the investigators' laboratories connected to the DPL by a direct cabling system. A larger computer facility, available to a larger community of health scientists, is used for batch processing where problems do not need continuous interaction with the investigator for on-line control or analysis, or do need greater computational capability. The two facilities possess compatible I/O formats, thus making some problems soluable by the combination of both computers, and giving other problems the flexibility of either approach. Essentially the DPL is a multiprocessing system with an emphasis on I/O functions and an interpretive system appropriate for neurophysiological investigation and with some unique solutions to resource allocation and system integrity in its temporal-spatial (core) algorithm. The economic advantage of such a system is not argued, nor is CPU economy necessarily maximized with present use, though from the standpoint of I/O devices which are so important for such research, a central facility may possess some advantages. Reliability and demands of time-critical users must be realistically estimated for neurophysiological users for whom the on-line aspect may be with reference to the integrity of their experiments.	algorithm;batch processing;central processing unit;computation;computer;experiment;input/output;interpreted language;multiprocessing;online and offline;privilege level;system integrity;window of opportunity	Frederick D. Abraham;Laszlo Betyar;Richard Johnston	1968		10.1145/1468075.1468127	simulation;computer science;operations management	DB	-34.92461663705191	58.48528511810903	6694
66d6bb715ce380e7e2252bd6f3f13ceb3b2184cc	hardware/software synthesis of formal specifications in codesign of embedded systems	formal specification;embedded system;design technique;codesign;software synthesis;register transfer level;formal language;hardware and software synthesis	CoDesign aims to integrate the design techniques of hardware and software. In this work, we present a CoDesign methodology based on a formal approach to embedded system specification. This methodology uses the Templated T-LOTOS language to specify the system during all design phases. Templated T-LOTOS is a formal language based on CCS and CSP models. Using Templated T-LOTOS, a system can be specified by observing the temporal ordering in which the events occur from the outside. In this paper we focus on the synthesis of system specified by Templated T-LOTOS. The proposed synthesis algorithm takes advantage of peculiarities of Templates T-LOTOS. Hardware modules are translated into a register transfer-level language that manages some signals in order to drive synchronization, while the software models are translated into C according to a finite state model whose operations are controlled by a scheduler. The synthesis of the Templated T-LOTOS specification is based on the direct translation of the language operators to ensure that the implemented system is the same as the specified one.	algorithm;calculus of communicating systems;communicating sequential processes;embedded system;formal language;formal specification;register-transfer level;scheduling (computing)	Vincenza Carchiolo;Michele Malgeri;Giuseppe Mangioni	2000	ACM Trans. Design Autom. Electr. Syst.	10.1145/348019.348093	co-design;embedded system;computer architecture;formal language;parallel computing;real-time computing;formal methods;specification language;computer science;formal specification;programming language;register-transfer level	EDA	-33.96050060226752	33.4294945748457	6700
64c9e1612b8264f8373204c07ab92d2393541588	bit: a very compact scheme system for microcontrollers	esquema;experimental design;programa;modulo;random access memory;estacion trabajo;compilateur;program;systeme embarque;scheme language;collecteur;station travail;real time;plan experiencia;conception;microcontroleur;space time;compiler;byte code;espacio tiempo;embedded system;garbage collection;schema;algorithme;algorithm;workstation;plan experience;temps reel;garbage collector;diseno;programme;collector;tiempo real;design;real time garbage collection;bytecode;colector;module;scheme;compact scheme;read only memory;espace temps;compilador;microcontroller;algoritmo	We present a compact implementation of Scheme for microcontrollers that includes a real-time garbage collector. The compiler runs on a normal workstation and produces byte-code from the source program. A smart linker links the byte-code with the runtime module. We demonstrate that with this system it is clearly possible to run realistic Scheme programs on a microcontroller with as little as 3 to 4 KB of RAM. Programs that access the whole Scheme library require only 13 KB of ROM. As a byproduct of this research, we designed a novel space-efficient real-time GC algorithm.	algorithm;byte;compiler;computational complexity theory;date and time representation by country;earthbound;embedded system;fastest;garbage collection (computer science);kernel (operating system);kilobyte;memory management;microcontroller;random-access memory;read-only memory;real-time clock;real-time transcription;robotics;scheme;subroutine;virtual machine;workstation	Danny Dubé;Marc Feeley	2005	Higher-Order and Symbolic Computation	10.1007/s10990-005-4877-4	parallel computing;real-time computing;scheme;computer science;operating system;garbage collection;programming language	Embedded	-17.670306290939738	39.56040532813677	6707
0d7dd449f13a56263d0b8f3d0134e140f24f0022	decomposition ordering as a tool to prove the termination of rewriting systems		Decomposition ordering is a well-founded monotonic ordering on terms. Because it has the subterm and the deletion properties, decomposition ordering is useful to prove termination of rewriting systems. An algorithm comparing two terms is given.	algorithm;rewriting;well-founded semantics	Pierre Lescanne	1981				Logic	-11.06402515505833	17.44463929754291	6711
b5321d4f44a6049e24236d6e6fb97d7e937fb59e	exploration of temperature-aware refresh schemes for 3d stacked edram caches	3d microprocessors;cache;edram;temperature;refresh interval	Recent studies have shown that embedded DRAM (eDRAM) is a promising approach for 3D stacked lastlevel caches (LLCs) rather than SRAM due to its advantages over SRAM; (i) eDRAM occupies less area than SRAM due to its smaller bit cell size; and (ii) eDRAM has much less leakage power and access energy than SRAM, since it has much smaller number of transistors than SRAM. However, different from SRAM cells, eDRAM cells should be refreshed periodically in order to retain the data. Since refresh operations consume noticeable amount of energy, it is important to adopt appropriate refresh interval, which is highly dependent on the temperature. However, the conventional refresh method assumes the worst-case temperature for all eDRAM stacked cache banks, resulting in unnecessarily frequent refresh operations. In this paper, we propose a novel temperature-aware refresh scheme for 3D stacked eDRAM caches. Our proposed scheme dynamically changes refresh interval depending on the temperature of eDRAM stacked last-level cache (LLC). Compared to the conventional refresh method, our proposed scheme reduces the number of refresh operations of the eDRAM stacked LLC by 28.5% (on 32 MB eDRAM LLC), on average, with small area overhead. Consequently, our proposed scheme reduces the overall eDRAM LLC energy consumption by 12.5% (on 32 MB eDRAM LLC), on average. © 2016 Elsevier B.V. All rights reserved.	best, worst and average case;bit cell;cpu cache;dynamic random-access memory;edram;embedded system;lunar lander challenge;overhead (computing);spectral leakage;static random-access memory;transistor;vertical blanking interval	Young-Ho Gong;Jae Min Kim;Sung Kyu Lim;Sung Woo Chung	2016	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2016.01.010	embedded system;parallel computing;real-time computing;cpu cache;temperature;cache;computer science;operating system	Arch	-7.355259088237076	55.13643546045059	6722
18a0371bee93b5383de964b9faad81a8a475701f	a web-based architecture for e-gov application development	application development	In this paper we present a web-based architecture for e-gov application development. We propose to study and characterize a software development environment of “electronic governance in cities”, with emphasis on: legacy system integration, heterogeneous data base integration and development of an integrated webbased environment for the provision of e-gov services. This architecture is based on a 4-layer MVC model, the typical MVC (Model, View, Controller) plus a data layer, this architecture imposes few requirements to its clients, mainly a HTML web-browser, which allows this architecture to be platform-independent.	database;e-government;html;integrated development environment;legacy system;model–view–controller;requirement;software development;system integration;web application	Marcelo Tilli;André Marcelo Panhan;Osman Lima;Leonardo S. Mendes	2008			enterprise architecture framework;computer science;applications architecture;service-oriented modeling;rapid application development;management;systems design	SE	-54.863073112759075	14.986856363739859	6738
7fd3d78ad865d0c5dac166dd9b5069ccc39b8096	performance modeling of multi-tiered web applications with varying service demands	analytical models;concurrent computing;throughput servers load modeling testing concurrent computing analytical models time factors;testing;mean value analysis;servers;time factors;predicted throughput multitiered transactional web applications varying service demands enterprise based systems post deployment performance queuing networks mean value analysis models multiserver queues multicore cpu multiserver mva model spline interpolation mvasd multitier vehicle insurance registration e commerce web applications mean deviations;multi tier applications performance modeling queuing networks mean value analysis spline interpolation;multi tier applications;queuing networks;splines mathematics electronic commerce internet interpolation queueing theory;load modeling;performance modeling;spline interpolation;throughput	Multi-tiered transactional web applications are frequently used in enterprise based systems. Due to their inherent distributed nature, pre-deployment testing for high-availability and varying concurrency are important for post-deployment performance. Accurate performance modeling of such applications can help estimate values for future deployment variations as well as validate experimental results. In order to theoretically model performance of multi-tiered applications, we use queuing networks and Mean Value Analysis (MVA) models. While MVA has been shown to work well with closed queuing networks, there are particular limitations in cases where the service demands vary with concurrency. This is further contrived by the use of multi-server queues in multi-core CPUs, that are not traditionally captured in MVA. We compare performance of a multi-server MVA model alongside actual performance testing measurements and demonstrate this deviation. Using spline interpolation of collected service demands, we show that a modified version of the MVA algorithm (called MVASD) that accepts an array of service demands, can provide superior estimates of maximum throughput and response time. Results are demonstrated over multi-tier vehicle insurance registration and e-commerce web applications. The mean deviations of predicted throughput and response time are shown to be less the 3% and 9%, respectively. Additionally, we analyze the effect of spline interpolation of service demands as a function of throughput on the prediction results.	central processing unit;concurrency (computer science);e-commerce;extrapolation;genetic algorithm;high availability;maximum throughput scheduling;model–view–adapter;multi-core processor;multitier architecture;performance prediction;response time (technology);server (computing);software deployment;software performance testing;spline (mathematics);spline interpolation;web application	Ajay Kattepur;Manoj K. Nambiar	2015	2015 IEEE International Parallel and Distributed Processing Symposium Workshop	10.1109/IPDPSW.2015.28	spline interpolation;mean value analysis;throughput;parallel computing;real-time computing;simulation;concurrent computing;computer science;operating system;distributed computing;software testing;programming language;server;statistics;computer network	Metrics	-21.860410585408626	56.89252093259566	6749
38275db74384524dc6de9a0957addc6b8f8b9579	a new zero-one law and strong extension axioms	finite model theory;first order logic	One of the previous articles in this column was devoted to the zero-one laws for a number of logics playing prominent role in finite model theory: first-order logic FO, the extension FO+LFP of first-order logic with the least fixed-point operator, and the infinitary logic L∞,ω. Recently Shelah proved a new, powerful, and surprising zero-one law. His proof uses so-called strong extension axioms. Here we formulate Shelah’s zero-one law and prove a few facts about these axioms. In the process we give a simple proof for a “large deviation” inequality à la Chernoff. 1 Shelah’s Zero-One Law Quisani: What are you doing, guys? Author: We2 are proving a zero-one law which is due to Shelah. Q: Didn’t Shelah prove the law? A: Oh yes, he proved it all right, and even wrote it down [14]. Q: So what is the problem? Can’t you read his proof? 1Mathematics, University of Michigan, Ann Arbor, MI 48109-1109, USA; partially supported by a grant from Microsoft Research. 2The record of the conversation was simplified by blending the two authors into one who prefers “we” to “I”.	alpha compositing;chernoff bound;first-order logic;first-order predicate;fixed-point combinator;linear algebra;microsoft research;social inequality	Andreas Blass;Yuri Gurevich	2000	Bulletin of the EATCS		discrete mathematics;zero–one law;mathematics;infinitary logic;operator (computer programming);inequality;calculus;first-order logic;axiom;finite model theory;extension by definitions	Logic	-8.964558983085482	11.666810406078987	6761
ef5a16448030bff47e359509520a542cd7b3ad77	privacy preservation of semantic trajectory databases using query auditing techniques		Existing approaches that publish anonymized spatiotemporal traces of mobile humans deal with the preservation of privacy operating under the assumption that most of the information in the original dataset can be disclosed without causing any privacy violation. However, an alternative strategy considers that data stays in-house to the hosting organization and privacy-preserving mobility data management systems are in charge of privacy-aware sharing of the mobility data. Furthermore, human trajectories are nowadays enriched with semantic information by using background geographic information and/or by user-provided data via location-based social media. This new type of representation of personal movements as sequences of places visited by a person during his/her movement poses even greater privacy violation threats. To facilitate privacy-aware sharing of mobility data, we design a semantic-aware MOD engine were all potential privacy breaches that may occur when answering a query, are prevented through an auditing mechanism. Moreover, in order to improve user friendliness and system functionality of the aforementioned engine, we propose Zoom-Out algorithm as a distinct component, whose objective is to modify the initial query that cannot be answered at first due to privacy violation, to the ‘nearest’ query that can be possibly answered with ‘safety’.	algorithm;location-based service;privacy;social media;tracing (software);usability	Despina Kopanaki;Nikos Pelekis	2017			information retrieval;database;query language;data mining;trajectory;audit;computer science	DB	-41.903427014545336	59.70347375549625	6764
de9551f4668ce7f390dd7a64de5fa23add097995	service information blueprint: a scheme for defining service information requirements		Access to high quality information is essential for effective design, development and delivery of service. In product-service systems (PSS), this includes both information to support the lifecycles of physical products and associated services and information to support the management of services. Service blueprinting is a widely used technique for visualisation and mapping of service activities. This paper introduces the concept of Service Information Blueprint, which builds on and extends the existing concept of service blueprint by including definition of service information needs and capture of service information in PSS environment. Three types of information associated with a service activity is captured in the Service Information Blueprint-input, process and output information. Applicability of the Service Information Blueprint to define technical support services is demonstrated using a case study on machine maintenance and repair service contract scenario. In addition, the impact of changes in service contract type on service definition and information requirements is demonstrated.	blueprint;requirement	Saikat Kundu	2015	JoSSR	10.1007/s12927-015-0002-3	service provider;reliability engineering;service level requirement;service level objective;mobile qos;service catalog;service product management;differentiated service;systems engineering;engineering;knowledge management;basic service;service delivery framework;service design;service guarantee;service desk;data as a service;customer service assurance;service system	Crypto	-49.22281404951754	16.185828245913754	6767
2b4786d1bc356efead07f82d7dc02c395485ac9c	dealing with some conceptual data model requirements for biological domains	conceptual data modeling;biology computing;genomics;conceptual data model;data integrity;biological system modeling;data models biological system modeling object oriented modeling bioinformatics genomics relational databases object oriented databases biology computing laboratories knowledge representation;conceptual model;simulation languages biology computing knowledge representation sensor fusion;conceptual data model requirements;biological domains;conceptual modeling languages;conceptual modeling tools;data integration conceptual data model requirements biological domains knowledge representation conceptual modeling tools domain complexity conceptual data modeling conceptual modeling languages ad hoc modeling approach database design;ad hoc modeling approach;simulation languages;relational databases;object oriented databases;sensor fusion;database design;knowledge representation;object oriented modeling;data integration;domain complexity;data models;bioinformatics	Knowledge representation in the biological domain suffers from two main difficulties that make it hard to apply traditional conceptual modeling tools: the inherent domain complexity and its constant evolution. The former requires complex types and associations, whereas the latter calls for an expressive language. We list in this paper a set of requirements for conceptual data modeling that should fulfil biologists' needs. We discuss these requirements by analyzing representation problems using traditional conceptual modeling languages. We claim that the biological domain deserves an ad-hoc modeling approach that would help with activities such as data integration and database design.	biological system;conceptual schema;data (computing);data model;data modeling;database design;hoc (programming language);knowledge representation and reasoning;modeling language;requirement	José Antônio Fernandes de Macêdo;Fábio Porto;Sérgio Lifschitz;Philippe Picouet	2007	21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07)	10.1109/AINAW.2007.142	data modeling;conceptual model;genomics;domain;data model;relational database;computer science;conceptual schema;conceptual model;theoretical computer science;data integration;domain model;data integrity;data mining;database;sensor fusion;database design	DB	-31.86711065415753	12.037968302967066	6769
176a485c766a7a67344130946281016b39862556	symmetric datalog and constraint satisfaction problems in logspace	complexity theoretic assumptions;query processing computational complexity constraint theory datalog operations research;query processing;two element domain;operations research;constraint satisfaction problems;expressive power;datalog;first order;logarithmic space;computational complexity;constraint languages;complexity theoretic assumptions constraint satisfaction problems symmetric datalog queries logarithmic space constraint languages two element domain;constraint theory;constraint satisfaction problem;databases polynomials computer science cloning constraint theory constraint optimization graph theory artificial intelligence logic algebra;symmetric datalog queries	We introduce symmetric Datalog, a syntactic restriction of linear Datalog and show that its expressive power is exactly that of restricted symmetric Krom monotone SNP. The deep result of Reingold [17] on the complexity of undirected connectivity suffices to show that symmetric Datalog queries can be evaluated in logarithmic space. We show that for a number of constraint languages Gamma, the complement of the constraint satisfaction problem CSP(Gamma) can be expressed in symmetric Datalog. In particular, we show that if CSP(Gamma) is first-order definable and Lambda is a finite subset of the relational clone generated by Gamma then notCSP(Lambda) is definable in symmetric Datalog. Over the two-element domain and under standard complexity-theoretic assumptions, expressibility of notCSP(Gamma) in symmetric Datalog corresponds exactly to the class of CSPs computable in logarithmic space. Finally, we describe a fairly general subclass of implicational (or 0/1/all) constraints for which the complement of the corresponding CSP is also definable in symmetric Datalog. Our results provide preliminary evidence that symmetric Datalog may be a unifying explanation for families of CSPs lying in L.	computable function;constraint satisfaction problem;cryptographic service provider;datalog;first-order predicate;graph (discrete mathematics);l (complexity);nl (complexity);sl (complexity);theory;monotone	László Egri;Benoit Larose;Pascal Tesson	2007	22nd Annual IEEE Symposium on Logic in Computer Science (LICS 2007)	10.1109/LICS.2007.47	discrete mathematics;computer science;theoretical computer science;mathematics;programming language;constraint satisfaction problem;algorithm	Logic	-7.385160184644124	16.488541981354473	6791
80b0c7e8f2d50417666f4d0021e29b7300651412	cloud forensic technical challenges and solutions: a snapshot	google;digital forensics;cloud technologies cloud forensic technical challenges cloud computing digital forensics;silicon carbide;cloud computing digital forensics servers silicon carbide google computer crime;cloud;computer crime;law;servers;digital forensics cloud computing;law cloud digital forensics;cloud computing	"""As cloud computing becomes more prevalent, there is a growing need for forensic investigations involving cloud technologies. The field of cloud forensics seeks to address the challenges to digital forensics presented by cloud technologies. This article reviews current research in the field of cloud forensics, with a focus on """"forensics in the cloud""""- that is, cloud computing as an evidence source for forensic investigations."""	cloud computing;computer forensics;snapshot isolation	Ben Martini;Kim-Kwang Raymond Choo	2014	IEEE Cloud Computing	10.1109/MCC.2014.69	cloud computing;computer science;digital forensics;operating system;internet privacy;world wide web;computer security	HPC	-49.60598443272771	58.414473334718146	6792
8a1750e3488519d0f90203084f2558f0f9d7d335	mobile code in .net: a porting experience	distributed system;virtual machine;movilidad;systeme reparti;agent mobile;building block;mobility;agente movil;langage java;mobilite;machine virtuelle;sistema repartido;mobile code;code mobility;lenguaje java;mobile agent;point of view;maquina virtual;java language	Mobile code systems typically rely on the Java language, since it provides many of the necessary building blocks. Nevertheless, Microsoft recently released the .net platform, which includes at its core a virtual machine supporting multi-language programming, and a new language called C#. The competition between .net and Java is evident, and so are the analogies between these two technologies. From the point of view of code mobility, a natural question to ask is then whether .net supports mobile code, and how the mechanisms provided compare with those available in Java. This paper aims at providing a preliminary set of answers to this simple question. The work we report about was not driven by the goal of providing a thorough comparison. Instead, it was driven by the practical need to port an existing toolkit for code mobility written in Java, μCode, to the .net environment. This approach forced us to verify our mobile code design on a concrete example, rather than just think about the problem in abstract. The resulting software artifact constitutes, to the best of our knowledge, the first implementation of a mobile code system written for .net. In the paper, we provide an overview of the .net mechanisms supporting mobile code, show how they are exploited in our port, and discuss similarities and differences with the Java platform.	artifact (software development);code mobility;java;open-source license;open-source software;virtual machine	Márcio Eduardo Delamaro;Gian Pietro Picco	2002		10.1007/3-540-36112-X_2	real-time computing;.net framework;code access security;computer science;virtual machine;artificial intelligence;operating system;common intermediate language;mobile agent;database;distributed computing;real time java;programming language;java;mobile computing;computer security;code mobility;algorithm;managed code;java annotation;source code	PL	-27.52524843501645	41.78697614688584	6795
8e134ef76ba2c82d3762bdf1e7ef48a4c8362057	control of cyberphysical systems using passivity and dissipativity based methods		In cyberphysical systems, where compositionality of design is an important requirement, passivity and dissipativity based design methods have shown a lot of promise. Although these concepts are classical, their application to cyberphysical systems poses new and interesting challenges. The aim of this paper is to summarize some of the ongoing work in this area by the authors.		Panos J. Antsaklis;Bill Goodwine;Vijay Gupta;Michael J. McCourt;Yue Wang;Po Wu;Meng Xia;Han Yu;Feng Zhu	2013	Eur. J. Control	10.1016/j.ejcon.2013.05.018	control engineering;engineering;artificial intelligence;control theory	EDA	-45.40163927961956	37.37479347207864	6799
356d6accbbb902dbffb4217bedaf9c7f2150ac18	experiences developing and maintaining software in a multi-platform environment	software metrics;software portability;programming environments;software products hewlett packard laserjet printer products business computer markets conditional compilation complexity industrial source code multi platform parallel development model performance needs portable scaleable software systems software development software maintenance;software measurement;printers;multi platform maintenance;multi platform parallel development model;software maintenance;software complexity;multi platform development;software systems;parallel programming;software portability laser printers parallel programming printers program compilers programming environments software maintenance software metrics;industrial source code;software products;lessons learned;software development;conditional compilation complexity;laser printers;business computer markets;hewlett packard;performance needs;portable scaleable software systems;source code;program compilers;laserjet printer products	The computer market demands that companies develop families of software products that can be scaled to meet the functional and performance needs of the personal and business computer markets. To support a family of LaserJet printer products, Hewlett-Packard defined the multi-platform parallel development model for software development. This model allows HP to simultaneously develop a family of LaserJet printers that have different features and run on different processors, while shortening the development. In this paper we discuss our experiences using a technique called conditional compilation, within the multi-platform parallel development model, to create portable, scaleable software systems. We describe and share a new tool that was developed to help understand code containing conditional compilation. Examples of using the tool on industrial source code, and lessons learned while managing con&tional compilation complexity, are provided.	central processing unit;compiler;conditional compilation;printer (computing);software development;software system	T. Troy Pearse;Paul W. Oman	1997		10.1109/ICSM.1997.624254	software portability;computer architecture;computer science;engineering;software development;software engineering;programming language;software maintenance;software measurement;programming complexity;software metric;software system;computer engineering;source code	SE	-61.10039915685939	31.904166495843132	6805
7c8651895b8c8a0c93f5897bc25c16af516e07b3	a formal design language for real-time systems with data	software tool;real time;specification language;g500 information systems;data model;critical system;g600 software engineering;concurrent systems;real time systems	AORTA has been proposed as an implementable real-time language for concurrent systems where event times, rather than values of data, are critical. In this paper we describe how to use AORTA with a formal data model, allowing integration with a variety of model-based data speci cation languages. Example de nitions are given of time-critical systems with important data attributes. A development technique and supporting software tools for AORTA are also described. c © 2001 Elsevier Science B.V. All rights reserved.	concurrency (computer science);data model;process calculus;real-time clock;real-time computing;window of opportunity	Steven Bradley;William Henderson;David Kendall;Adrian Robson	2001	Sci. Comput. Program.	10.1016/S0167-6423(00)00025-3	data modeling;real-time computing;formal methods;specification language;data model;computer science;software development;formal specification;systems development life cycle;programming language;systems design	SE	-41.78977421124998	30.504306051699245	6812
06f031d1db3019627e9d1dc3c4a7ec7b9eb1318d	new expressive languages for ontological query answering	computer science and information systems	Ontology-based data access is a powerful form of extending database technology, where a classical extensional database (EDB) is enhanced by an ontology that generates new intensional knowledge which may contribute to answer a query. Recently, the Datalog± family of ontology languages was introduced; in Datalog±, rules are tuple-generating dependencies (TGDs), i.e., Datalog rules with the possibility of having existentially-quantified variables in the head. In this paper we introduce a novel Datalog± language, namely sticky sets of TGDs, which allows for a wide class of joins in the body, while enjoying at the same time a low query-answering complexity. We establish complexity results for answering conjunctive queries under sticky sets of TGDs, showing, in particular, that ontological conjunctive queries can be compiled into first-order and thus SQL queries over the given EDB instance. We also show some extensions of sticky sets of TGDs, and how functional dependencies and so-called negative constraints can be added to a sticky set of TGDs without increasing the complexity of query answering. Our language thus properly generalizes both classical database constraints and most widespread tractable description logics.	bus (computing);cobham's thesis;compiler;conjunctive query;data access;datalog;description logic;first-order predicate;functional dependency;intensional logic;relational database;sql;sticky bit	Andrea Calì;Georg Gottlob;Andreas Pieris	2011			natural language processing;computer science;data mining;database	AI	-24.435058490772516	8.841986530621966	6830
a2275392b3ef50369b4af74e77f8f8fd753e8180	partial evaluation of queries in deductive databases	optimisation;optimizacion;query processing;query optimization;partial evaluation;evaluation;optimization;evaluacion;deductive databases	This paper presents some applications of partial evaluation method to a query optimization in deductive database. A Horn clause transformation is used for the partial evaluation of a query in an intensional database, and its application to multiple query processing is discussed. Three strategies are presented for the compatible case, ordered case and crossed case. In each case, partial evaluation is used to preprocess the intensional database in order to obtain subqueries which direct access to an extensional database.	deductive database;horn clause;intensional logic;mathematical optimization;partial evaluation;preprocessor;random access;sql	Chiaki Sakama;Hidenori Itoh	1988	New Generation Computing	10.1007/BF03037140	sargable;query optimization;database theory;query expansion;boolean conjunctive query;computer science;query by example;theoretical computer science;evaluation;database;programming language;view;partial evaluation;query language	DB	-28.882662460007104	8.791357389069036	6839
f243135f582bfaf7965aa42be71c43807780b42d	safety functions on commodity hardware with diversified encoding.		Currently, hardware designed and certified for safety-critical systems is one important building block for any safety-critical application. Such hardware provides the detection of execution errors. However, many modern safety-critical applications, like autonomous driving, require features and performance levels that are not available from safety-certified hardware. One solution to this problem is to use hardware that is not certified for safety-critical systems, for example consumergraded hardware, but that fulfills the feature and performance requirements. Additionally, a software solution provides the detection of execution errors. This paper introduces such a software solution called “Diversified Encoding with Coded Processing”. Due to its hardware-independence, this solution provides the flexibility to build safety-critical systems from non-safety-critical hardware components. This solution can be automated with a code transformation tool to further increase the flexibility.	autonomous car;commodity computing;requirement	Martin Süßkraut;André Schmitt;Jörg Kaienburg	2015			encoding (memory);real-time computing;commodity;computer science	Arch	-46.16222091712841	36.86687667618754	6844
d1c6c5fac7c5d2f90157cba2cd8ad078cafa1817	mathematical theory exploration	problem solving mathematical theory exploration automated reasoning lemmas theorems algorithms;inference mechanisms;automated reasoning;theorem proving;theorem proving inference mechanisms;mathematics reduced instruction set computing automation power generation humans failure analysis data mining scientific computing computational geometry algorithm design and analysis	Summary form only given. Mathematics is characterized by its method of gaining knowledge, namely reasoning. The automation of reasoning has seen significant advances over the past decades and, thus, the expectation was that these advances would also have significant impact on the practice of doing mathematics. However, so far, this impact is small. We think that the reason for this is the fact that automated reasoning so far concentrated on the automated proof of individual theorems whereas, in the practice of mathematics, one proceeds by building up entire theories in a step-by-step process. This process of exploring mathematical theories consists of the invention of notions, the invention and proof of propositions (lemmas, theorems), the invention of problems, and the invention and verification of methods (algorithms) that solve problems	algorithm;automated reasoning;theory	Bruno Buchberger	2006	2006 Eighth International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2006.50	computer science;artificial intelligence;theoretical computer science;automated proof checking;machine learning;data mining;mathematics;mathematical proof;automated theorem proving;automated reasoning;algorithm;algebra	PL	-9.778443830383507	4.99865867103794	6846
7a95c73cfde82947563f01c10ee809341e55f11b	a framework for the development of personalized, distributed web-based configuration systems	personalized user interaction;configuration system;web-based configuration application;mass-customization business model;open xml-based protocol;heterogeneous user group;web-based configuration system;cawicoms workbench;ai technique;configuration service;real-world business scenario	For the last two decades, configuration systems relying on AI techniques have successfully been applied in industrial environments. These systems support the configuration of complex products and services in shorter time with fewer errors and, therefore, reduce the costs of a mass-customization business model. The European Union-funded project entitled CUSTOMER-ADAPTIVE WEB INTERFACE FOR THE CONFIGURATION OF PRODUCTS AND SERVICES WITH MULTIPLE SUPPLIERS (CAWICOMS) aims at the next generation of web-based configuration applications that cope with two challenges of today's open, networked economy: (1) the support for heterogeneous user groups in an open-market environment and (2) the integration of configurable subproducts provided by specialized suppliers.This article describes the CAWICOMS WORKBENCH for the development of configuration services, offering personalized user interaction as well as distributed configuration of products and services in a supply chain. The developed tools and techniques rely on a harmonized knowledge representation and knowledge-acquisition mechanism, open XML-based protocols, and advanced personalization and distributed reasoning techniques. We exploited the workbench based on the real-world business scenario of distributed configuration of services in the domain of information processing-based virtual private networks.		Liliana Ardissono;Alexander Felfernig;Gerhard Friedrich;Anna Goy;Dietmar Jannach;Giovanna Petrone;Ralph Schäfer;Markus Zanker	2003	AI Magazine		configuration management;configuration management database;computer science;systems engineering;database;world wide web	AI	-47.61199926378257	11.593421339936356	6852
997bb576ff9ccb27acf6ca51f67843967b880ffa	efficient fully secure (hierarchical) predicate encryption for conjunctions, disjunctions and k-cnf/dnf formulae			conjunctive normal form;encryption	Angelo De Caro;Vincenzo Iovino;Giuseppe Persiano	2010	IACR Cryptology ePrint Archive		encryption;predicate (grammar);algorithm;mathematics	Crypto	-12.807980503472306	14.885857395377116	6855
8c8a942eeae9975b746cf202fe48dcc4e295e2dd	software architecture for the cloud - a roadmap towards control-theoretic, model-based cloud architecture	control theory;uncertainty;microservice;software engineering;software architecture;model based controller;adaptive system;cloud computing	The cloud is a distributed architecture providing resources as tiered services. Through the principles of service-orientation and generally provided using virtualisation, the deployment and provisioning of applications can be managed dynamically, resulting in cloud platforms and applications as interdependent adaptive systems. Dynamically adaptive systems require a representation of requirements as dynamically manageable models, enacted through a controller implementing a feedback look based on a control-theoretic framework. We argue that a controltheoretic, model-based architectural framework for the cloud is needed. While some critical aspects such as uncertainty have already been taken into account, what has not been accounted for are challenges resulting from the cloud architecture as a multi-tiered, distributed environment. We identify challenges and define a framework that aims at a better understanding and a roadmap towards control-theoretic, model-based cloud architecture – driven by software architecture concerns.	adaptive system;cloud computing;distributed computing;enterprise architecture framework;interdependence;microservices;model-driven integration;modular programming;multitier architecture;provisioning;requirement;robust control;service-orientation;software architecture;software deployment;theory	Claus Pahl;Pooyan Jamshidi	2015		10.1007/978-3-319-23727-5_17	multilayered architecture;enterprise architecture framework;reference architecture;software architecture;space-based architecture;real-time computing;simulation;database-centric architecture;uncertainty;cloud computing;computer science;engineering;applications architecture;adaptive system;service-oriented modeling;software engineering;cloud testing;solution architecture;distributed computing;software architecture description;view model;systems architecture	HPC	-43.05595666164752	39.46578347417294	6858
7deb8b40b295d24cfadcc9bfd997b0b609f88663	thesb-tree an index-sequential structure for high-performance sequential access	disque;pagination memoire;memoria acceso secuencial;memory layout;evaluation performance;gestion memoire;disk;paginacion memoria;storage access;performance evaluation;storage management;evaluacion prestacion;sistema informatico;sequential access storage;computer system;disco;organizacion memoria;gestion memoria;arbol binario;indexation;tree structure;arbre binaire;acces memoire;performance analysis;organisation memoire;acceso memoria;systeme informatique;long range;memoire acces sequentiel;high performance;storage pagination;binary tree	A variant of aB-tree known as anSB-tree is introduced, with the object of offering high-performance sequential disk access for long range retrievals. The key to this efficiency is a structure that supports multi-page reads (or writes) during sequential access to any node level below the root, even following significant node splitting. In addition, theSB-tree will support a policy to ‘stripe’ successive multi-page blocks on multiple disks to achieve maximum parallelism. Compared to traditionalB-tree structures,SB-tree performance characteristics are less subject to degradation resulting from modifications entailed in growing and shrinking;SB-trees are therefore more appropriate for use in situations where frequent reorganization is not possible. A performance analysis reveals the strengths of theSB-tree by comparing its performance under various circumstances to theB +-tree and the bounded disorder (BD) file of [11]. The performance analysis formulates a new useful concept, the ‘effective depth’ of anSB- orB +-tree, defined as the expected number of pages read from disk to perform a random retrieval search given standard buffering behavior. A graph of effective depth against tree size is shown to have a scalloped appearance, reflecting the changing effectiveness of incremental additions to buffer space.	(a,b)-tree;b-tree;blu-ray;elegant degradation;parallel computing;profiling (computer programming);sandy bridge;sequential access;toby turner	Patrick E. O'Neil	1992	Acta Informatica	10.1007/BF01185680	binary tree;telecommunications;computer science;artificial intelligence;tree structure;programming language;algorithm	DB	-16.91350016935802	45.32100052363612	6860
3622690ff66109eb48f0f2cc5070320f7f0ec922	synchronous collaboration in ubiquitous computing environments: conceptual model and software infrastructure for roomware components		iv the concept of sharing as known from CSCW can be extended to function as a guiding principle for UbiComp application design. This novel design approach helps ensuring the extensibility and flexibility that is required in ubiquitous computing. (2) A flexible software architecture identifies essential abstractions that support the development of synchronous applications in “roomware” environments. Roomware refers to the integration of room elements with information technology, such as interactive tables, walls, or chairs. Roomware environments represent one form of ubiquitous computing environment. They are used in this thesis as an application context for the conceptual model. The software architecture refines the conceptual model to meet the needs of roomware environments. (3) An object-oriented application framework that has been designed and implemented provides a reusable design and reusable software components. Furthermore, extensibility is supported by explicit mechanisms that are provided to allow adaptability for variable aspects of applications. Thus, the application framework helps developers with the design and implementation. (4) To show how model, architecture, and framework can be applied, the design of sample roomware applications is explained. To demonstrate the extensibility, several new forms of interaction that are required for roomware environments are implemented. The developed applications and interaction forms are used in i-LAND, the roomware environment at Fraunhofer IPSI. Besides being a contribution on their own, the developed applications and new forms of interaction provide evidence that the conceptual model effectively supports developers in meeting the requirements of roomware environments. They show that the model helps reduce the implementation effort when accompanied by appropriate software development tools such as the application framework. The conceptual model, software architecture, and application framework presented in this thesis relieve software developers from the burden of handling all details of multiple interaction forms, and of many critical issues when dealing with synchronous collaboration. By these means, the developer can concentrate on the task at hand designing software at an appropriately high abstraction level, and thus create applications with a higher quality that are flexibly extensible.	abstraction layer;application framework;component-based software engineering;computer-supported cooperative work;context (computing);extensibility;land;norm (social);programming tool;requirement;software architecture;software developer;software development;ubiquitous computing	Peter Tandler	2004			conceptual model;human–computer interaction;ubiquitous computing;software;systems engineering;computer science	SE	-49.03496472622539	22.888936558641554	6874
c6eb26eb0742999d5529f8254b47ee54584e8a39	a framework for automated and certified refinement steps	certification;refinement;software engineering	The refinement calculus provides a methodology for transforming an abstract specification into a concrete implementation, by following a succession of refinement rules. These rules have been mechanized in theorem provers, thus providing a formal and rigorous way to prove that a given program refines another one. In a previous work, we have extended this mechanization for object-oriented programs, where the memory is represented as a graph, and we have integrated our approach within the rCOS tool, a model-driven software development tool providing a refinement language. Hence, for any refinement step, the tool automatically generates the corresponding proof obligations and the user can manually discharge them, using a provided library of refinement lemmas. In this work, we propose an approach to automate the search of possible refinement rules from a program to another, using the rewriting tool Maude. Each refinement rule in Maude is associated with the corresponding lemma in Isabelle, thus allowing the tool to automatically generate the Isabelle proof when a refinement rule can be automatically found. The user can add a new refinement rule by providing the corresponding Maude rule and Isabelle lemma.	aliasing;class implementation file;discharger;entity;foobar;isabelle;late binding;loop invariant;maude system;model-driven architecture;model-driven engineering;out-of-order execution;programming tool;rcos (computer sciences);recursion;refinement (computing);refinement calculus;rewrite (programming);rewriting;scalability;software development;software engineering;structured programming;succession;the witness;type safety	Andreas Griesmayer;Zhiming Liu;Charles Morisset;Shuling Wang	2012	Innovations in Systems and Software Engineering	10.1007/s11334-012-0183-6	refinement calculus;computer science;database;refinement;programming language;certification;algorithm	Logic	-18.872594693345437	26.646192666712448	6876
2b7af4c770d7bcd5fbf683a7c932c05634d62ea3	introducing high performance computing in digital library processing workflows	high performance computing;digital library;digital libraries;large scale;high performance computer;digitization	As larger collections need to be processed for digital library projects, libraries have to adopt technologies of scale. We present a case that involved creating image derivatives using High Performance Computing (HPC) resources. This experience opens up possibilities to conduct various processing tasks effectively and in reasonable time frames. Most importantly, it enables library IT staff access to cyberinfrastructure that can address the computing challenges of large-scale digital library projects.	cyberinfrastructure;digital library;image derivatives;library (computing);supercomputer	Bill Barth;Maria Esteva;Jon Gibson;Ladd Hanson;Christopher Jordan	2012		10.1145/2232817.2232905	digital library;computer science;world wide web	HPC	-29.873349758233115	52.10592771943606	6880
a991802c9bf924be955869a028930ee64da92d5c	on the computation of stubborn sets of colored petri nets	developpement logiciel;modelizacion;distributed system;unfolding;red petri colorado;verificacion modelo;systeme reparti;relation ordre partiel;traduccion automatica;algoritmo busqueda;deploiement;red petri;algorithme recherche;search algorithm;analisis intervalo;despliegue;verification modele;simultaneidad informatica;calcul ensembliste;specification programme;systeme ordre reduit;modelisation;concurrency;calculo conjunto;sistema repartido;traduction automatique;model checking;state space method;methode espace etat;causalite;partial ordering;desarrollo logicial;state space;software development;colored petri net;relacion orden parcial;set computation;software specification;petri net;program specification;modeling;simultaneite informatique;analyse intervalle;reduced order systems;especificacion programa;reseau petri;metodo espacio estado;reseau petri colore;interval analysis;causality;causalidad;partial order;automatic translation	Valmari’s Stubborn Sets method is a member of the so-called partial order methods. These techniques are usually based on a selective search algorithm: at each state processed during the search, a stubborn set is calculated and only the enabled transitions of this set are used to generate the successors of the state. The computation of stubborn sets requires to detect dependencies between transitions in terms of conflict and causality. In colored Petri nets these dependencies are difficult to detect because of the color mappings present on the arcs: conflicts and causality connections depend on the structure of the net but also on these mappings. Thus, tools that implement this technique usually unfold the net before exploring the state space, an operation that is often untractable in practice. We present in this work an alternative method which avoids the cost of unfolding the net. To allow this, we use a syntactically restricted class of colored nets. Note that this class still enables wide modeling facilities since it is the one used in our model checker Helena which has been designed to support the verification of software specifications. The algorithm presented has been implemented and several experiments which show the benefits of our approach are reported. For several models we obtain a reduction close or even equal to the one obtained after an unfolding of the net. We were also able to efficiently reduce the state spaces of several models obtained by an automatic translation of concurrent software.	approximation;causality;coloured petri net;computation;computer aided verification;concurrent computing;denotational semantics;enumerated type;experiment;explicit substitution;formal verification;high- and low-level;lecture notes in computer science;machine translation;model checking;mutual exclusion;precondition;reachability;search algorithm;springer (tank);state space;unfolding (dsp implementation)	Sami Evangelista;Jean-François Pradat-Peyre	2006		10.1007/11767589_9	partially ordered set;computer science;artificial intelligence;mathematics;programming language;algorithm	SE	-16.800421851127464	27.240889800196936	6883
ed29a464b218fa6501f23fea96e0233389073240	a survey on data-flow testing	coverage criteria;coverage tracking;test data generation;data flow testing;data flow analysis	Data-flow testing (DFT) is a family of testing strategies designed to verify the interactions between each program variable’s definition and its uses. Such a test objective of interest is referred to as a def-use pair. DFT selects test data with respect to various test adequacy criteria (i.e., data-flow coverage criteria) to exercise each pair. The original conception of DFT was introduced by Herman in 1976. Since then, a number of studies have been conducted, both theoretically and empirically, to analyze DFT’s complexity and effectiveness. In the past four decades, DFT has been continuously concerned, and various approaches from different aspects are proposed to pursue automatic and efficient data-flow testing. This survey presents a detailed overview of data-flow testing, including challenges and approaches in enforcing and automating it: (1) it introduces the data-flow analysis techniques that are used to identify def-use pairs; (2) it classifies and discusses techniques for data-flow-based test data generation, such as search-based testing, random testing, collateral-coverage-based testing, symbolic-execution-based testing, and model-checking-based testing; (3) it discusses techniques for tracking data-flow coverage; (4) it presents several DFT applications, including software fault localization, web security testing, and specification consistency checking; and (5) it summarizes recent advances and discusses future research directions toward more practical data-flow testing.	data-flow analysis;dataflow;dynamic testing;gabor herman;interaction;internet security;model checking;random testing;security testing;test data generation;variable (computer science)	Ting Su;Ke Wu;Weikai Miao;Geguang Pu;Jifeng He;Yuting Chen;Zhendong Su	2017	ACM Comput. Surv.	10.1145/3020266	test data generation;simulation;orthogonal array testing;software performance testing;white-box testing;computer science;data-flow analysis;functional testing;data mining;risk-based testing;software testing;programming language;test management approach	SE	-60.31128050770269	31.589702008968352	6887
d61723e9cac1443531b8a4e68336a7f13591bd16	rest based service composition: exemplified in a care network scenario	user interaction service composition rest care network;service composition;senior citizens;medical administrative data processing;senior citizens pervasive computing computer architecture testing calendars service oriented architecture;pervasive computing;testing;rest;calendars;care network;service oriented architecture health care medical administrative data processing;computer architecture;representational state transfer rest based service composition health care network scenario secrest architecture;service oriented architecture;user interaction;health care	This paper presents an ongoing work developing and testing a Service Composition framework based upon the REST architecture named SECREST. A minimalistic approach have been favored instead of a creating a complete infrastructure. One focus has been on the system's interaction model. Indeed, an aim is to allow users in different healthcare scenarios to experiment with service composition to support highly individual and changing needs.	minimalism (computing);representational state transfer;service composability principle	Erik Grönvall;Mads Ingstrup;Morten Pløger;Morten Rasmussen	2011	2011 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)	10.1109/VLHCC.2011.6070417	simulation;engineering;knowledge management;world wide web	SE	-42.22164357495851	43.89126631815219	6898
31b18e41d8d0effef47d31074098e60772cd16c9	software verification and system assurance	probabilistic property;software;reliability;system level assurance;probability;software systems formal verification software safety airplanes hazards aircraft failure analysis software engineering computer science laboratories;uncertainty;software verification;hazards;software reliability probability program verification;testing;program verification;formal verification;assurance;possible perfection formal verification assurance reliability probabilistic assessment;probabilistic logic;probabilistic property software verification system level assurance formal verification;software reliability;possible perfection;probabilistic assessment;aircraft	Littlewood introduced the idea that software may be possibly perfect and that we can contemplate its probability of (im)perfection. We review this idea and show how it provides a bridge between correctness, which is the goal of software verification (and especially formal verification), and the probabilistic properties such as reliability that are the targets for system-level assurance. We enumerate the hazards to formal verification, consider how each of these may be countered, and propose relative weightings that an assessor may employ in assigning a probability of perfection.	correctness (computer science);enumerated type;formal verification;software verification	John M. Rushby	2009	2009 Seventh IEEE International Conference on Software Engineering and Formal Methods	10.1109/SEFM.2009.39	uncertainty;formal verification;software verification;hazard;computer science;software engineering;probability;reliability;software testing;runtime verification;probabilistic logic;functional verification;software quality	SE	-60.53953011235724	32.44428468104471	6900
0f73e74af8fdcc62e4d1d8c058bf8c9594f12fca	parsing in a broad sense	model synchronisation;universiteitsbibliotheek;parsing;technical space bridging;bidirectional model transformation;pretty printing;unparsing;chapter	Having multiple representations of the same instance is common in software language engineering: models can be visualised as graphs, edited as text, serialised as XML. When mappings between such representations are considered, terms “parsing” and “unparsing” are often used with incompatible meanings and varying sets of underlying assumptions. We investigate 12 classes of artefacts found in software language processing, present a case study demonstrating their implementations and state-of-the-art mappings among them, and systematically explore the technical research space of bidirectional mappings to build on top of the existing body of work and discover as of yet unused relationships.	parsing;unparser;xml	Vadim Zaytsev;Anya Helene Bagge	2014		10.1007/978-3-319-11653-2_4	computer science;artificial intelligence;theoretical computer science;machine learning;parsing;database;mathematics;distributed computing;programming language;algorithm	SE	-27.498701687997904	18.440604556302052	6916
33121fbef1141c60dd309e0e8aa841ffb6a8e169	copying garbage collection for the wam: to mark or not to mark?	methode recursive;gestion memoire;overflow computer arithmetics;prolog;storage management;ramasse miettes;metodo recursivo;recursive method;garbage collection;recogemigas;gestion memoria;garbage collector;rebasamiento capacidad;wam;depassement capacite	Garbage collection by copying is becoming more and more popular for Prolog. Copying requires a marking phase in order to be safe: safeness means that the to-space is guaranteed not to overflow. However, some systems use a copying garbage collector without marking prior to copying, and instead postpone the copying of potentially unsafe cells. Such systems only collect small portions of the heap and it is not clear whether postponing works while collecting the whole heap. Moreover, it is shown here that postponing does not solve the problem in a fundamental way. Since marking takes time, it is worth studying the tradeoffs involved. These observations have prompted the experimentation with a series of garbage collectors based on copying without marking and without postponing. In particular, variants were implemented that are named dangerous, optimistic and cautious copying which exhibit various degrees of unsafeness. Versions of each have been implemented based on recursive copying as in most implementations of copy term/2 and on the Cheney algorithm. Performance on benchmarks suggests that large performance gains can be obtained by skipping the marking phase, that dangerous copying is still relatively safe but can be costly, and that the additional effort of cautious copying over optimistic copying is not worth it. The optimistic collectors based on recursive copying perform best and slightly better than the ones based on Cheney. Cache performance measurements back up the benchmark results.	garbage collection (computer science);warren abstract machine	Bart Demoen;Phuong-Lan Nguyen;Ruben Vandeginste	2002		10.1007/3-540-45619-8_14	parallel computing;real-time computing;computer science;artificial intelligence;garbage collection;programming language;mark-compact algorithm;algorithm	PL	-18.795216585643445	33.655117611459424	6919
444523126f1ce7799af2de12ac94702b9a4f4e45	computer aided motion: move3d within molog	design automation;dedicated software architecture;european project computer aided motion move3d molog probabilistic path planning techniques logistics industrial installations domain constraints dedicated software architecture;molog;computer aided motion;path planning;cad;logistics data processing;industrial robots cad path planning user interfaces logistics data processing mobile robots;path planning logistics design automation costs power generation software architecture software tools visualization power engineering computing power system modeling;mobile robots;move3d;probabilistic approach;industrial installations;software architecture;visualization;power engineering computing;logistics;industrial robots;domain constraints;power generation;software tools;power system modeling;user interfaces;european project;probabilistic path planning techniques	This paper reports on our current eeort for applying probabilistic path planning techniques to logistics and operation in huge industrial installations (e.g., power plants). We show how the speciic domain constraints impose a dedicated software architecture to take advantage of the generality of probabilistic approaches. In addition, such an architecture should be compatible with existing CAD systems making critical the interface issues. We conclude on three study cases currently under development within the European project MOLOG.	algorithm;computer-aided design;local area augmentation system;logistics;motion planning;software architecture	Thierry Siméon;Jean-Paul Laumond;Carl Van Geem;Juan Cortés	2001		10.1109/ROBOT.2001.932822	mobile robot;logistics;electricity generation;software architecture;simulation;visualization;electronic design automation;computer science;systems engineering;engineering;artificial intelligence;cad;motion planning;user interface;computer engineering	EDA	-56.061827925597186	9.799779243591127	6920
f7cbac472dd7b6350eba820293a8c766c44fb251	modeling multi-rate dsp specification semantics for formal transformational design in hol	formal semantics;specification language;chip;theorem proving	The CATHEDRAL Silicon Compilers synthesize hardware for DSP algorithms specified in Silage, a high level applicative language. In order to optimize the results of the silicon compilation in terms of chip-area and/or throughput, the user often massages the specification applying transformations to the Silage code. To guarantee that the transformations preserve the behavior of the specified algorithm, the formal semantics of the specification language had to be defined. The semantics has been used to prove in HOL the correctness of the transformations and to prove properties of the specification. We are currently building a system where a menu of useful andcorrectness preserving transformations will be available to the user. In this system the user could choose appropriate transformations from the menu taking advantage of his creativity and expertise to interactively guide the silicon compiler, without the risk of introducing inconsistencies. This article describes the formalmulti-rate semantics of a substantial subset of Silage and illustrates some formally verified transformations.	hol (proof assistant)	Catia M. Angelo;Luc J. M. Claesen;Hugo De Man	1994	Formal Methods in System Design	10.1007/BF01384234	chip;action semantics;specification language;computer science;theoretical computer science;formal semantics;formal specification;automated theorem proving;programming language;operational semantics;algorithm	EDA	-20.254886791520196	26.051458804501188	6923
3dd367e17075e8eea2c6abcd72edfdaf4bf9d45c	a software infrastructure for rss deployment and linking on the web	digital television;transport layer;fpga implementation;recommender system;hdtv;ts multiplexer	Everyday users browse the Web seeking relevant subsets of information. Without knowing, users access irrelevant or old content available as hyperlinks. The Rich Site Summary (RSS) is an XML-based format that allows the syndication of lists of hyperlinks, that helps viewers decide whether they want to follow the link. Linking services help users to find related information. Aiming to improve the definition of hyperlinks, we propose a software infrastructure that incorporates the manipulation of RSS format for defining updated hyperlinks on the Web. We present the use of our proposal with an extension of a previous recommender system.	browsing;hyperlink;rss;recommender system;relevance;software deployment;web syndication;world wide web;xml	José Antonio Camacho Guerrero;Alessandra Alaniz Macedo	2005		10.1145/1114223.1114236	digital television;telecommunications;computer science;operating system;database;internet privacy;world wide web;transport layer;recommender system;computer network	Web+IR	-39.38412378783596	10.451152925703644	6925
fbd5da4fd2177b9451a848d0be5de9331e9fa500	non-termination analysis of logic programs using types	application of specialization;types;non failure analysis;application of non failure analysis;program specialization;termination analysis;failure analysis;logic programs;non termination analysis	In recent years techniques and systems have been developed to prove non-termination of logic programs for certain classes of queries. In previous work, we developed such a system based on mode-information and a form of loop checking performed at compile time. In the current paper we improve this technique by integrating type information in the analysis and by applying non-failure analysis and program specialization. It turns out that there are several classes of programs for which existing non-termination analyzers fail and for which our extended technique succeeds in proving non-termination.	abstract syntax tree;approximation;compile time;compiler;divergence (computer science);failure analysis;international conference on logic programming;lecture notes in computer science;parse tree;partial template specialization;springer (tank);termination analysis;warren abstract machine	Dean Voets;Danny De Schreye	2010		10.1007/978-3-642-20551-4_9	failure analysis;real-time computing;computer science;termination analysis;programming language;algorithm	PL	-17.941694179701898	24.739816138311355	6944
0bfb8c24a78e168947c28f134b94f62d67606fca	reflection without remorse: revealing a hidden sequence to speed up monadic reflection	monads;performance;data structures;reflection	A series of list appends or monadic binds for many monads performs algorithmically worse when left-associated. Continuation-passing style (CPS) is well-known to cure this severe dependence of performance on the association pattern. The advantage of CPS dwindles or disappears if we have to examine or modify the intermediate result of a series of appends or binds, before continuing the series. Such examination is frequently needed, for example, to control search in non-determinism monads.  We present an alternative approach that is just as general as CPS but more robust: it makes series of binds and other such operations efficient regardless of the association pattern-- and also provides efficient access to intermediate results. The key is to represent such a conceptual sequence as an efficient sequence data structure. Efficient sequence data structures from the literature are homogeneous and cannot be applied as they are in a type-safe way to series of monadic binds. We generalize them to type aligned sequences and show how to construct their (assuredly order-preserving) implementations. We demonstrate that our solution solves previously undocumented, severe performance problems in iteratees, LogicT transformers, free monads and extensible effects.	algorithm;cp system;computation;continuation;continuation-passing style;data structure;delimiter;free variables and bound variables;iteratee;monad (functional programming);monadic predicate calculus;overhead (computing);reification (computer science);software bug;traverse;transformers;type safety;undocumented feature	Atze van der Ploeg;Oleg Kiselyov	2014		10.1145/2633357.2633360	reflection;data structure;performance;computer science;programming language;monad;algorithm	PL	-18.721407466927282	32.18207025351143	6989
18ac924713c740cbfd51549cf1b45a4df767d796	"""the semantics of """"semantic patches"""" in coccinelle: program transformation for the working programmer"""	temporal logic;program transformation;denotational semantic;model checking;partial evaluation;source code;higher order functions	We rationally reconstruct the core of the Coccinelle system, used for automating and documenting collateral evolutions in Linux device drivers. A denotational semantics of the system’s underlying semantic patch language (SmPL) is developed, and extended to include variables. The semantics is in essence a higher-order functional program and so executable; but is inefficient and limited to straight-line source programs. A richer and more efficient SmPL version is defined, implemented by compiling to the temporal logic CTL-V (CTL with existentially quantified variables ranging over source code parameters and program points; defined using the staging concept from partial evaluation). The compilation is formally proven correct and a model check algorithm is outlined.	algorithm;coccinelle;compiler;denotational semantics;device driver;disk staging;executable;existential quantification;functional programming;line source;linux;partial evaluation;program transformation;programmer;software documentation;temporal logic	Neil D. Jones;René Rydhof Hansen	2007		10.1007/978-3-540-76637-7_21	model checking;temporal logic;computer science;theoretical computer science;programming language;higher-order function;partial evaluation;algorithm;source code	PL	-20.284157473726825	26.877145109215732	6990
75a9f7ee40c71c54df475c5353a053635c8c07b5	term models for weak set theories with a universal set		We shall be concerned here with weak axiomatic systems of set theory with a universal set. The language in which they are expressed is that of set theory—two primitive predicates, = and ϵ, and no function symbols (though some function symbols will be introduced by definitional abbreviation). All the theories will have stratified axioms only, and they will all have Ext (extensionality: (∀ x )(∀ y )( x = y · ↔ ·(∀ z )( z ϵ x ↔ z ϵ y ))). In fact, in addition to extensionality, they have only axioms saying that the universe is closed under certain set-theoretic operations, viz. all of the form and these will always include singleton, i.e., ι ′ x exists if x does (the iota notation for singleton, due to Russell and Whitehead, is used here to avoid confusion with { x : Φ }, set abstraction), and also x ∪ y , x ∩ y and − x (the complement of x ). The system with these axioms is called NF 2 in the literature (see [F]). The other axioms we consider will be those giving ⋃ x , ⋂ x , { y : y ⊆ x } and { y : x ⊆ y }. We will frequently have occasion to bear in mind that 〈 V , ⊆ 〉 is a Boolean algebra in any theory extending NF 2 . There is no use of the axiom of choice at any point in this paper. Since the systems with which we will be concerned exhibit this feature of having, in addition to extensionality, only axioms stating that V is closed under certain operations, we will be very interested in terms of the theories in question. A T -term, for T such a theory, is a thing (with no free variables) built up from V or ∧ by means of the T -operations, which are of course the operations that the axioms of T say the universe is closed under.		Thomas E. Forster	1987	J. Symb. Log.		mathematical logic;calculus;mathematics;axiom;algorithm;set theory	Logic	-6.644510689977856	13.41242380296924	6997
7f6bd3d63eeae3001ae232ef10b2ddc8c0d068a8	modeling cyber effects in cyber-physical systems with devs		Increased interconnectivity of Cyber-Physical Systems, by design or otherwise, increases the cyber attack surface and attack vectors. It is not always desirable to do cyber risk assessment of such interconnected systems by cyber attacks on the entire system. This paper presents a formal approach for simulating cyber effects for cyber risk assessment of subcomponents of such systems using a control component based on Discrete EVent System (DEVS) models. Our approach first separates components of a system that may be directly attacked, from components of the system whose behavior need to be studied for the impact originating from those attacks. The cyber attack effects are simulated through interception of communications among the separated components. We define the types of cyber effects, and show an example of how an attack may be simulated with a control component through a case study.	algorithm;attack surface;cyber-physical system;devs;programmer;risk assessment;semantics (computer science);side effect (computer science);simulation	Suresh Damodaran;Saurabh Mittal	2017			devs;model-based design;cyber-attack;cyber-physical system;risk assessment;distributed computing;interconnectivity;computer science	SE	-55.21629146746647	51.04051226272397	7006
8b19b9322da7429f169231a97df8e936598abe4b	analysis and verification of smil documents	verification;authoring multimedia presentations;consistency checking;smil	In this paper, we consider the problem of automatic verification of SMIL documents and present a tool which can assist the user in the complex task of authoring a multimedia presentation. The tool is based on a formal semantics defining the temporal aspects of SMIL elements by means of a set of inference rules. The rules, in the spirit of Hoare’s semantics, describe how the execution of a piece of code changes the state of the computation of a player. If any temporal conflict is found, the system returns a message to the user pointing out the element which contains the conflict and its motivation. This helps the user to develop robust and clear code.	bottom-up proteomics;computation;constraint logic programming;formal system;hoare logic;interaction;operational semantics;petri net;precondition;predicate transformer semantics;prolog;requirement;scheduling (computing);semantics (computer science);software propagation;streaming media;synchronized multimedia integration language;test suite;timeline;top-down and bottom-up design;user interface	Ombretta Gaggi;Annalisa Bossi	2011	Multimedia Systems	10.1007/s00530-011-0233-1	verification;synchronized multimedia integration language;computer science;database;multimedia;world wide web	Logic	-24.947224363895376	16.59470632876583	7017
d06f603667dddc6882acc5a32278d0354df37d84	quantifier-elimination for the first-order theory of boolean algebras with linear cardinality constraints	busqueda informacion;base relacional dato;quantifier elimination;programmation logique avec contrainte;logica booleana;base donnee;algebra relacional;constrenimiento igualdad;variable elimination;information retrieval;interrogation base donnee;database;programacion logica con restriccion;interrogacion base datos;base dato;relational database;boolean algebra;base donnee avec contrainte;constrained database;equality constraint;eliminacion cuentificador;first order;recherche information;algebre boole;base donnee relationnelle;logique booleenne;base dato forzada;constraint logic programming;algebre relationnelle;information system;elimination quantificateur;boolean logic;database query;relational algebra;systeme information;algebra boole;contrainte egalite;sistema informacion	We present for the first-order theory of atomic Boolean algebras of sets with linear cardinality constraints a quantifier elimination algorithm. In the case of atomic Boolean algebras of sets, this is a new generalization of Boole’s well-known variable elimination method for conjunctions of Boolean equality constraints. We also explain the connection of this new logical result with the evaluation of relational calculus queries on constraint databases that contain Boolean linear cardinality constraints.	algebraic equation;algorithm;boolean algebra;boolean satisfiability problem;cardinality (data modeling);computation;concatenation;data structure;database;decision problem;existential quantification;expressive power (computer science);first-order logic;first-order predicate;linear algebra;mathematical optimization;polynomial ring;presburger arithmetic;quantifier (logic);query language;query optimization;relational calculus;variable elimination	Peter Z. Revesz	2004		10.1007/978-3-540-30204-9_1	boolean algebra;boolean circuit;and-inverter graph;boolean model;circuit minimization for boolean functions;boolean network;boolean domain;boolean expression;product term;standard boolean model;computer science;maximum satisfiability problem;stone's representation theorem for boolean algebras;boolean algebras canonically defined;database;boolean function;complete boolean algebra;algorithm;two-element boolean algebra;free boolean algebra;parity function	Logic	-16.67084228524593	14.703595698925893	7025
520b38c51b6f80003d14f3817fe3bd6f10bc51ad	power and performance study of hpc applications on qct developer cloud		We present direct performance measurement for eight popular HPC applications on the Knights Landing (KNL) platform. Performance numbers for Haswell processors are provided for contrast. The applications (DGEMM. SGEMM, STREAM, IOR, HPCG, Quantum Espresso, WRF and HPL) were selected from among the ten most used in the QCT developer cloud as well as good representative of workloads used by large number of users and, given their diversity, should be representative of typical HPC workloads. All runs were performed with publicly available codes without modification and so results should be expected to improve as developers gain access to KNL. Current results are promising, with execution on a single KNL processor showing speedups up to 1.7x with respect to a dual socket Haswell.	blas;central processing unit;code;hpcg benchmark;haswell (microarchitecture);interoperable object reference;knights;linpack benchmarks;quantitative computed tomography;quantum espresso;weather research and forecasting model;xeon phi	Paul Young;Probha Madhavan;Gong-Do Hwang	2017	2017 IEEE 4th International Conference on Cyber Security and Cloud Computing (CSCloud)	10.1109/CSCloud.2017.50	computer science;parallel computing;weather research and forecasting model;quantum espresso;basic linear algebra subprograms;cloud computing;performance measurement;server;benchmark (computing)	HPC	-5.277791785441595	44.50529800521838	7027
b1f9b2140c97e60099d50ff6aed646fc4f11bab5	scenario-based validation of embedded systems	libraries;analytical models;model driven design;abstract state machine;unified modeling language biological system modeling modeling embedded system analytical models libraries design methodology;biological system modeling;object oriented programming;embedded system;formal method;systemc uml profile;embedded systems;c language;design environment;levels of abstraction;system level design validation;unified modeling language;system level design;finite automata;scenario based validation;system components;tool integration;abstract state machines formal method;program compilers;modeling;system components scenario based validation embedded systems system level design validation abstract state machines formal method model driven design systemc uml profile;unified modeling language c language embedded systems finite automata object oriented programming program compilers;design methodology	This paper describes a scenario-based methodology for system-level design validation based on the Abstract State Machines formal method. This scenario-based approach complements an existing model-driven design methodology for embedded systems based on the SystemC UML profile. It allows the designer to functionally validate system components from SystemC UML designs early at high levels of abstraction and without requiring strong skills and expertise on formal methods. A validation tool integrated into an existing model-driven co-design environment to support the proposed scenario-based validation flow is also presented.	abstract state machines;abstraction layer;conformance testing;electronic system-level design and verification;embedded system;fault detection and isolation;formal methods;formal verification;level design;model checking;model-driven architecture;model-driven engineering;model-driven integration;principle of abstraction;profile (uml);spin;systemc;test case;unified modeling language	Angelo Gargantini;Elvinia Riccobene;Patrizia Scandurra;Alessandro Carioni	2008	2008 Forum on Specification, Verification and Design Languages	10.1109/FDL.2008.4641444	unified modeling language;embedded system;computer architecture;formal methods;systems modeling;design methods;computer science;electronic system-level design and verification;finite-state machine;programming language;object-oriented programming;abstract state machines	EDA	-41.79488868031013	32.31989470016495	7029
eaa6ccc2aefc43cf80bc5bf520987584fc6e0ad4	the logicist manifesto: at long last let logic-based artificial intelligence become a field unto itself	continuous system;artificial intelligent	2 Background 1 2.1 Logic-Based AI Encapsulated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2.1.1 LAI is Ambitious . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.1.2 LAI is Based on Logical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.1.3 LAI is a Top-Down Enterprise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Ignoring the “Strong” vs. “Weak” Distinction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 A Slice in the Day of a Life of a LAI Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3.1 Knowledge Representation in Elementary Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.2 Deductive Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.3 A Note on Nonmonotonic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.4 Beyond Elementary Logical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.4 Examples of Logic-Based Cognitive Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15	artificial intelligence;comparison of relational database management systems;knowledge representation and reasoning;non-monotonic logic	Selmer Bringsjord	2008	J. Applied Logic	10.1016/j.jal.2008.09.001	philosophy;progress in artificial intelligence;epistemology;computer science;artificial intelligence;mathematics;algorithm	AI	-16.233796922090054	9.725130679633436	7030
bafdd219b3d7424fc739123acb2b27b5a1b7bf79	a verified compiler for handel-c		The recent popularity of Field Programmable Gate Array (FPGA) technology has made the synthesis of Hardware Description Language (HDL) programs into FPGAs a very attractive topic for research. In particular, the correctness in the synthesis of an FPGA programming file from a source HDL program has gained significant relevance in the context of safety or mission-critical systems. The results presented here are part of a research project aiming at producing a verified compiler for the Handel-C language. Handel-C is a high level HDL based on the syntax of the C language extended with constructs to deal with parallel behaviour and process communications based on CSP. Given the complexity of designing a provably correct compiler for a language like HandelC, we have adopted the algebraic approach to compilation as it offers an elegant solution to this problem. The idea behind algebraic compilation is to create a sound reasoning framework in which the a formal model of the source Handel-C program can be embedded and refined into a formal abstraction of the target hardware. As the algebraic rules used to compile the program are proven to preserve the semantics, the correctness of the entire compilation process (i.e., semantic equivalence between source and target programs) can be argued by construction, considering each programming construct in isolation, rather than trying to assert the correctness of the compilation in a single step. Regarding hardware synthesis, the algebraic approach has already been applied to subsets of Occam and Verilog. Our work builds on some ideas from these works but focuses on the more complex timing model imposed by Handel-C. Moreover, our work covers features like shared variables, multi-way communications and priorities which, to our knowledge, have never been addressed within the framework of algebraic compilation. Finally, one characteristic of the algebraic approach is that the basic reduction laws in the reasoning framework are postulated as axioms. As an invalid axiom would allow us to prove invalid results (up to the extent of being able to prove a false theorem) we are also concerned about the consistency of the basic postulates in our theory. We addressed this by providing denotational semantics for Handel-C and its reasoning extensions in the context of the Unifying Theories of Programming (UTP). Our UTP denotational semantics not only provided a model for our theory (hence, proving its consistency) but also allowed us to prove all the axioms in the compilation framework.	algebraic equation;automated theorem proving;compiler;correctness (computer science);denotational semantics;embedded system;field-programmable gate array;handel;handel-c;hardware description language;high-level programming language;linear algebra;mathematical model;mission critical;parallel computing;relevance;shared variables;turing completeness;unifying theories of programming;verilog;occam	Juan Ignacio Perna	2010			dynamic compilation;computer science;theoretical computer science;programming language;algorithm	PL	-20.675846902332413	24.81031558356661	7043
6a72d0097977fbdb4b38088ba929ae78279d7d83	a dynamic ecc scheme for lengthening the lifetime of flash memory.	flash memory		flash memory	Shen-Ming Chung;Shun-Chieh Lin;Yi-Chen Chung	2010			parallel computing	Embedded	-10.873196960042788	54.97409923741366	7045
2a2237326e7543decf12c493769c4573a6c7be60	csmagic improves program generation with interactive cobol	cobol;program generation	"""The ability to have error-free, documented, high-leve l source code in seconds is the dream of every data processin g manager. Toward this end, Data General Systems Divisio n has developed a COBOL program generator calle d CSMAGIC . Previous efforts at program generators stress the capability of multiple languages and complex structures . As a result they often require learning a nonprocedure-oriented language and much experience to operate . CSMAGIC has taken a new and different approach . Structured entirely t o ease implementation, CSMAGIC is a part of the """"ne w wave"""" of conversational software that delivers computin g power to more people for less cost than ever before possible . The normal output from an application generator i s source code . The user compiles the source and begins execution within minutes . Changes or additions to the generated code are simplified by the use of standard logic b y CSMAGIC . After a few uses, the programmer-user know s exactly how the program will look before requesting that i t be generated . Earlier attempts at generators missed thi s key point . These other systems execute your request directly, and no source code is generated which is available fo r modifications or additional enhancements . The user is required to do it their way or not at all . CSMAGIC produces standardized code automatically, but permits any customization allowed within the COBOL structure . CSMAGIC is for the interactive business application that revolves around one or more data files . It generates structured COBOL source code that is error-free and documented . It eliminates coding, keying the code to disk, de bugging and testing . It maintains its own library of system specifications . CSMAGIC is a series of interactive COBOL programs . These programs operate in a tutorial mode. The state of th e art is to lead the user through his or her request . To quote Zemanek, the new systems will """" . . . let the computer re strict the human user in the practical situation to point a t yes or no, or some more equally simple choices . . ."""" The primary assumption is that the user wants an interactiv e COBOL program accessing one or more file structures . Toward this end, the system requests simple answers fro m the user, verifies the correctness of the response and proceeds to the next logical question . An example of this tech -"""	business software;cobol;correctness (computer science);fo (complexity);fastest;fourth-generation programming language;key (cryptography);programmer;simpl;software testing	Thomas C. Bishop;Edward J. Grace	1980	DATA BASE	10.1145/1017517.1017525	computer science;software engineering;cobol;programming language;computer engineering	OS	-51.665276349018264	35.37825811628152	7051
94026c7c8138522b3721caed9b1439701e556ef9	checking multi-view consistency of discrete systems with respect to periodic sampling abstractions		Abstract In multi-view modeling (MVM) the system under development is described by distinct models, called views, which capture different perspectives of the system. Possible overlaps of the views may give rise to inconsistencies. Following the formal MVM framework of [33] , the view consistency problem asks to check the consistency of a given set of views with respect to a given set of abstraction functions. Existing work checks view consistency of discrete systems (transition systems or finite automata) with respect to two types of abstraction functions: (1) projections of state variables and (2) projections of an alphabet of events onto a subalphabet. In this paper, we study view consistency with respect to timing abstractions, specifically, periodic sampling , for automata and transition systems.	discrete system;sampling (signal processing)	Maria Pittou;Panagiotis Manolios;Jan Reineke;Stavros Tripakis	2018	Sci. Comput. Program.	10.1016/j.scico.2018.07.003	periodic graph (geometry);finite-state machine;computer science;theoretical computer science;sampling (statistics);formal methods;abstraction;state variable;alphabet	Logic	-9.690899424227052	23.244866204662035	7057
6811dfc2a7f3e7969bd8ecbfc2a8821ea8075c6e	soft component adaptation	service provider;ministry of science and technology;component based software engineering;formal method	Component adaptation is widely recognised to be one of the crucial problems in Component-Based Software Engineering (CBSE). We present here a formal methodology for the soft adaptation of components presenting mismatching interaction behaviours. The notions of access rights (associating components with the services they are allowed to use), and sub-servicing (providing alternative services in place of those requested by components lacking the required access rights), are exploited to feature a secure and flexible adaptation of third-party components.	access control;component-based software engineering	Antonio Brogi;Carlos Canal;Ernesto Pimentel	2003	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80681-9	service provider;formal methods;computer science;component-based software engineering;programming language;computer security	SE	-46.78283540218058	53.92614916732127	7059
3dbc2b0469ce80e755227a82abe6d84003ff20e0	hardware and petri nets: application to asynchronous circuit design	synthese circuit;concepcion circuito;red petri;circuit design;state machine;asynchronous circuit;formal verification;circuit asynchrone;design framework;circuito asincrono;sintesis circuito;hardware design;verification formelle;conception circuit;computer hardware;model of computation;petri net;materiel informatique;material informatica;circuit synthesis;reseau petri	Asynchronous circuits is a discipline in which the theory of concurrency is applied to hardware design. This paper presents an overview of a design framework in which Petri nets are used as the main behavioral model for speciication. Techniques for synthesis, analysis and formal veriication of asynchronous circuits are reviewed and discussed.	asynchronous circuit;behavioral modeling;circuit design;concurrency (computer science);petri net	Jordi Cortadella;Michael Kishinevsky;Alex Kondratyev;Luciano Lavagno;Alexandre Yakovlev	2000		10.1007/3-540-44988-4_1	model of computation;embedded system;real-time computing;asynchronous circuit;formal verification;computer science;theoretical computer science;circuit design;process architecture;finite-state machine;programming language;petri net	EDA	-34.18799346875454	31.343285033199816	7077
551c147041c9c86cc779da623b39ed0d60c34bc2	context-aware communication support system with pictographic cards	context awareness;context aware;mobile device;support system;pictographic card;communication;speechlanguage difficulties	We present a context-aware pictographic display system that facilitates the search for communication cards that bear some relation to the location and goal of the user. The system consists of a server and a mobile device: the server searches for relevant cards on the basis of the context, and then prioritizes them in terms of their relatedness; the mobile device displays pictographic cards according to the priority. This system can help people who have speech-language difficulties by reducing the searching time and difficulties, when the user wants to find the pictographic cards.	communications satellite;mobile device;pictogram;requirement prioritization;server (computing)	Gunhee Kim;Jukyung Park;Manchul Han;Se Hyung Park;Sungdo Ha	2009		10.1145/1613858.1613958	computer science;operating system;mobile device;internet privacy;world wide web;computer security	HCI	-40.132904554513665	54.23252315085968	7079
25978261fee7606b34bec194a46a9ae1ee617440	a jini-enabled active badge system	mobile device;middleware;infrared	We present an active badge system to test the efficacy of Jini as middleware for a system of small, mobile devices. Our system consists of badges that transmit infrared signals, detectors attached to fixed PC's serving as Jini clients, and a Jini server to keep track of the locations of all badges. A second type of Jini client can query the server and display location information.	middleware;mobile device;sensor;server (computing);jini	Ryan Brooks;Homer Carlisle;Richard Chapman;Amos Confer;Kaushik Lahoti;Missam Momin;Neal Rogers;Ruslana Svidzinksa;Bindu Vrishabhendra	2000		10.1145/1127716.1127740	embedded system;real-time computing;engineering;operating system	Mobile	-36.28899159954987	52.791509060819465	7092
03ffedd497da6e3ab04a457852671b5a4a5b8eed	syndesi: a framework for creating personalized smart environments using wireless sensor networks	energy conservation;protocols;personal awareness;wireless sensor networks actuators protocols logic gates monitoring ip networks wireless communication;distributed systems wireless sensor networks smart environments internet of things personal awareness;actuators;serveur institutionnel;wireless communication;internet of things;embedded systems;electrical devices syndesi personalized smart environments wireless sensor networks embedded devices intelligent services urban ecosystems user friendliness quality of life energy efficiency sustainability embedded technologies distributed technologies smart technologies system architecture;archive institutionnelle;logic gates;monitoring;open access;ubiquitous computing;ip networks;archive ouverte unige;distributed systems;cybertheses;smart environments;wireless sensor networks;institutional repository;wireless sensor networks embedded systems energy conservation ubiquitous computing	Smart environments are places where different kind of embedded devices are interconnected in order to provide their occupants intelligent services improving their comfort and convenience. These smart environments are seen to be important for the future urban ecosystems in terms of user friendliness, quality of life, energy efficiency and sustainability. Lately such environments have become economically and technologically feasible due to the advancements in embedded and distributed technologies. Most of the novel infrastructures adopt smart technologies, while old infrastructures need a transition towards smart environments. Even though different technologies and devices are available, there is a need for an appropriate methodology and a system architecture for a smooth and profitable transition towards smart environments. In this paper we present a framework for creating personalized smart environments using wireless sensor networks. This framework, among other services that it provides, is able to identify people and take personalized actions such as control electrical devices based on their preferences and needs. We present, as a proof of concept, a real world deployment where two scenarios are implemented in two office premises.	accessibility;crowdsensing;embedded system;interaction;location-based service;near field communication;overhead (computing);personalization;personally identifiable information;requirement;scalability;sensor;service discovery;smart environment;social network;software deployment;systems architecture;urban ecosystem;usability;user experience;vii;web service;world wide web	Orestis Evangelatos;Kasun Samarasinghe;José D. P. Rolim	2013	2013 IEEE International Conference on Distributed Computing in Sensor Systems	10.1109/DCOSS.2013.35	embedded system;communications protocol;wireless sensor network;energy conservation;logic gate;computer science;computer security;internet of things;ubiquitous computing;wireless;computer network;actuator	Robotics	-42.52913026088806	50.56148569158673	7093
23b3a2f6067dcc2859faebab7cd5889b9119a640	memory arbiter synthesis and verification for a radar memory interface card	data stream;hardware synthesis;model checking;hash table;bit state hashing;guided model checking;point of view;formal analysis	In this paper synthesis and verification are carried out by specifying two different problems of logic model checking which are solved by applying a concrete model checker — Uppaal and its extended counterpart Uppaal CORA. A great deal of care is taken in constructing the corresponding models as abstractions of the behaviour of the system from the point of view of memory communication. This results in a rather simple abstract model of the system which is one of the key factors to the success of the synthesis task. The other key factor is the application of a sound but incomplete method for space saving — bit-state hashing, where each visited state is represented by one bit in the hash table, in a location determined by the hash of the state. We analyse a radar memory interface case study of the IST AMETIST project. The reasoning in this paper presents a way to synthesise a memory arbiter for the system, to minimise memory used for buffering the data streams and to verify that the resultant arbiter does not deadlock and never starves nor overflows any of the intermediate buffers. ACM CCS	abstraction layer;arbiter (electronics);cora;deadlock;guard (information security);hash table;model checking;radar;resultant;starvation (computer science);state space;static random-access memory;synchronicity;uppaal	Juhan P. Ernits	2005	Nord. J. Comput.		model checking;hash table;parallel computing;real-time computing;computer science;theoretical computer science;programming language;algorithm	Logic	-17.628126649786296	30.432736321792284	7116
f47bffabb020402d0f6f1754ca7ff152d491a639	multiple fault localization with data mining		We propose an interactive fault localization method based on two data mining techniques, formal concept analysis and association rules. A lattice formalizes the partial ordering and the dependencies between the sets of program elements (e.g., lines) that are most likely to lead to program execution failures. The paper provides an algorithm to traverse that lattice starting from the most suspect places. The main contribution is that the algorithm is able to deal with any number of faults within a single execution of a test suite. In addition, a stopping criterion independent of the number of faults is provided.	algorithm;association rule learning;data mining;data structure;debugger;debugging;formal concept analysis;traverse;test suite;tracing (software)	Peggy Cellier;Mireille Ducassé;Sébastien Ferré;Olivier Ridoux	2011			debugging;data mining;traverse;partially ordered set;computer science;formal concept analysis;test suite;lattice (order);theoretical computer science;association rule learning	SE	-59.457995765298534	37.43856160962117	7127
0959bb59b82ed3d7930cd759b3c8d4b562ab0827	the inscape environment	symbiosis;change management;formal specifications;distributed computing;software systems;specification language;formal method;permission;system evolution;specification languages;software development;version management;writing;specification languages writing permission programming buildings software systems symbiosis user interfaces formal specifications distributed computing;interconnect modeling;integrated software development environment;programming;user interfaces;buildings;semantic analysis	The Inscape Environment is an integrated software development enviroment for building large software systems by large groups of developers. It provides tools that are knowledgeable about the process of system construction and evolution and that work in symbiosis with the system builders and evolvers. These tools are integrated around the constructive use of formal module interface specifications. We first discuss the problems that Inscape addresses, outline our research strategies and approaches to solving these problems, and summarize the contributions of the Inscape Environment. We then discuss the major aspects of the Inscape Environment: the specification language, system construction, system evolution, use and reuse, and validation. We illustrate these various components with examples and discussions.	integrated software;software development;software system;specification language	Dewayne E. Perry	1989		10.1145/74587.74588	programming;specification language;computer science;systems engineering;software development;operating system;software engineering;change management;programming language;user interface;management;writing;software system;symbiosis	SE	-48.802644975755186	28.952234091366115	7136
fc52a48a13ae3b479d80840cc7d4c63e1c21c515	eclone: detect semantic clones in ethereum via symbolic transaction sketch		The Ethereum ecosystem has created a prosperity of smart contract applications in public blockchains, with transparent, traceable and programmable transactions. However, the flexibility that everybody can write and deploy smart contracts on Ethereum causes a large collection of similar contracts, i.e., clones. In practice, smart contract clones may amplify severe threats like security attacks, resource waste etc.   In this paper, we have developed EClone, a semantic clone detector for Ethereum. The key insight of our clone detection is Symbolic Transaction Sketch, i.e., a set of critical semantic properties generated from symbolic transaction. Sketches of two smart contracts will be normalized into numeric vectors with a same length. Then, the clone detection problem is modeled as a similarity computation process where sketches and other syntactic information are combined. We have applied EClone in identifying semantic clones of deployed Ethereum smart contracts and achieved an accuracy of 93.27%.   A demo video of EClone is at https://youtu.be/IRasOVv6vyc.	computation;duplicate code;ecosystem;ethereum;sketch;smart contract;traceability	Hao Liu;Zhiqiang Yang;Chao Liu;Yu Jiang;Wenqi Zhao;Jia-Guang Sun	2018		10.1145/3236024.3264596	theoretical computer science;semantic property;computer science;computation;clone (java method);smart contract;syntax;sketch;database transaction	Security	-58.05439763906775	58.3726652593933	7145
4c47fdc082df87bc69277717bc0d05b5b9fc197c	a hierarchical blackboard architecture for distributed ai systems	distributed cad cae distributed ai systems distributed problem solving incremental development stepwise refinement functional decomposition organizational structure communication structure multi blackboard architecture hierarchical blackboard system integration architecture heterogeneous expert systems configuration management;incremental development;stepwise refinement;communication system;blackboard architecture;telecommunications computing blackboard architecture cad configuration management distributed processing expert systems hierarchical systems problem solving;expert systems;communication structure;distributed ai systems;cad;prototypes;distributed processing;hierarchical systems;contracts;testing;integration architecture;satisfiability;functional programming;distributed cad cae;hierarchical blackboard system;telecommunications computing;functional decomposition;artificial intelligence problem solving expert systems prototypes computer aided engineering;community structure;data structures;heterogeneous expert systems;computer aided engineering;organizational aspects;artificial intelligence;distributed problem solving;distributed ai;multi blackboard architecture;configuration management;organizational structure;problem solving;expert system	A framework f o r distributed problem solving should support the incremental development and integration of problem solvers into the system in agreement with the principles of stepwise refinement and functional decomposition. The organizational structure of the framework should guide the mapping of each of the functional areas onto agents and its communication structure should reflect this decomposition. W e describe a hierarchical multiblackboard architecture, called Hierarchical Blackboard System (H BBS) which satisfies these requirements. It serves as an integration architecture for heterogeneous expert systems. Important features of the system, such as the hierarchy concept, communication strategy and configuration management of the system at run-time are discussed. A working prototype H BBS for distributed CAD/CAE of communication systems is described.	blackboard system;computer-aided design;configuration management;distributed artificial intelligence;expert system;iterative and incremental development;problem solving;prototype;refinement (computing);requirement;stepwise regression;top-down and bottom-up design	Michael Weiss;Franz Stetter	1992		10.1109/SEKE.1992.227969	organizational structure;blackboard system;functional decomposition;computer science;systems engineering;knowledge management;artificial intelligence;theoretical computer science;iterative and incremental development;top-down and bottom-up design;cad;prototype;software testing;configuration management;expert system;community structure;communications system;satisfiability	AI	-40.58449367285062	18.27636753499315	7158
7f1bb34f4ce179614856f022a6496a704857aef5	database design and query reformulation with an inference machine			database design	P. M. da Silveira	1985				EDA	-31.44940736904522	8.664109323916582	7167
e35a409705a97d43bafd60d13b3012a9d80d00f2	gaining the profits of cloud computing in a public authority environment	service models;administrative data processing;government;military;cloud computing	In this paper, we discuss cloud computing in a public authority context. We define the key features and characteristics of the cloud in security centric environments such as the military environment. We address the most essential problems and obstacles to be considered before the benefits of cloud computing can be fully enabled therein. As a solution to problems with the adoption of cloud computing in public authority environments, we propose a new service model called knowledge management as a service, which also improves usability. The discussion and views presented in this paper can be adopted in any organisation with doubts concerning the sensitive and classified contents of current ICT systems in cloud computing.	cloud computing	Klaus Zaerens	2012	IJCSE	10.1504/IJCSE.2012.049748	cloud computing security;simulation;cloud computing;computer science;operating system;database;utility computing;internet privacy;law;world wide web;computer security;government	HPC	-48.79445903929697	57.0693964731383	7185
6a86caf1dd6f9fdfbc7923f9840ba92aa228b836	a protocol compiler for secure sessions in ml	distributed application;management system;specification language;remote procedure call	Distributed applications can be structured using sessions that specify flows of messages between roles. We design a small specific language to declare sessions. We then build a compiler, called s2ml, that transforms these declarations down to ML modules securely implementing the sessions. Every run of a well-typed program executing a session through its generated module is guaranteed to follow the session specification, despite any low-level attempt by coalitions of remote peers to deviate from their roles. We detail the inner workings of our compiler, along with our design choices, and illustrate the usage of s2ml with two examples: a simple remote procedure call session, and a complex session for a conference management system. 1 Sessions for distributed programming Programming networked, independent systems is complex: when systems communicate through an untrusted network, and do not trust each other, enforcing security properties is hard. As a first step to simplify this task, programming languages and system libraries offer abstractions for common communication patterns (such as private channels or RPCs). Beyond simple abstractions for communications, distributed applications can often be structured as parties that exchange messages according to some fixed, prearranged patterns, called sessions (also named contracts, or workflows, or protocols). Sessions simplify distributed programming by specifying the behaviour of each network entity, or role: the parties can then resolve most of the programming complexity upfront. Language-based support for sessions is the subject of active research [2,3,4,7,10,14,15]. Several of these works focus on developing type systems which statically ensure compliance to session specifications. There, type safety implies that user code that instantiates a session role always behaves as prescribed in the session. Thus, assuming that every distributed program participating in a session is well-typed, any run of the session follows its specification. There, being well-typed implies that every session participant is benign, and therefore complies with the session specification. Moreover, the network is also assumed to behave as expected, (e.g., delivering messages correctly). However, in an adversarial setting, remote parties may not be trusted to play their role. Moreover, they may collude to attack compliant participants, and may also control the network, being able to eavesdrop, intercept, and modify en route messages. Hence, defensive implementations also have to monitor one another, in order to prevent any confusion between parallel sessions, to ensure authentication, correlation, and causal dependencies between messages, and to detect any deviation from the assigned roles of a session. Left to the programmer, this task involves delicate low-level coding below session abstractions, which defeats their purpose. In order to keep sessions being useful and safe abstractions, we consider their secure implementation in terms of cryptographic communication protocols, by developing s2ml. To our knowledge, our compiler s2ml is the first to systematically compile session specifications to tailored cryptographic protocols, providing strong security guarantees beyond simple functional properties. In ongoing work [5], we explore language-based support for sessions. We design a small language for specifying sessions, and identify a secure implementability condition. We present a formal language extending ML [11,12] with distributed communication and sessions, designed in a way so that type safety yields functional guarantees: any sent message is expected by its receiver, with matching payload types. Then, we develop the s2ml compiler that translates sessions to cryptographic communication protocols, and formally show, as main result, that programs are shielded from any lowlevel attempt by coalitions of remote peers to deviate from their roles. In that work, we are most concerned about establishing the correctness of the code generation, and illustrate the approach with a small, simple toy example. In this paper, we turn to present the details of our implementation. We focus on presenting our compiler s2ml, along with its usage and inner workings. Furthermore, we investigate the applicability and scalability of our approach to more realistic and complex settings through the study of a RPC session and a conference management system (CMS) session example. Architecture. The basis of our work is a language for sessions with a CCS-like syntax to describe the different roles in a session. The s2ml compiler reads the session declarations, and works as follows: First, it checks correctness and security conditions on every session declaration, using an internal graph-based, global representation of the message flow. Then, it generates an ML module (along with its interface) for each specified session. The interface provides the programmer with the functions and types needed to execute every session role. We rely on the ML language for several reasons. First, we take advantage of ML’s typechecking to ensure functional correctness (i.e., that user code follows the session as prescribed), as opposed to having a dedicated type system as in other session types approaches. Second, our generated session role functions have (usually mutually recursive) types which are driven by user code using a continuation passing style (CPS) which allows for compact session programming. Finally, our generated types and cryptographic protocols heavily use algebraic types and pattern matching to specify and check the different allowed session paths. Our generated code uses the Ocaml syntax3 and can be run in both Ocaml and F# [13]. Programs using the generated session interfaces can be linked against networking and cryptographic libraries, obtaining executable code. We provide three alternative implementations for these libraries: two concrete implementations using either Ocaml/OpenSSL and F#/Microsoft .NET produce executable code supporting distributed runs; a third, symbolic library implements cryptography using algebraic datatypes and communication via a Pi calculus library, useful for correctness checks and debugging. 3 Although we use Ocaml syntax, our work can easily be adapted to other ML dialects.	adversary (cryptography);algebraic data type;authentication;calculus of communicating systems;causal filter;code generation (compiler);compiler;content management system;continuation-passing style;correctness (computer science);cryptographic protocol;cryptography;debugging;declaration (computer programming);distributed computing;executable;formal language;high- and low-level;library (computing);mutual recursion;ocaml;openssl;pattern matching;programmer;programming complexity;programming language;remote procedure call;scalability;subroutine;type safety;type system;visual intercept;π-calculus	Ricardo Corin;Pierre-Malo Deniélou	2007		10.1007/978-3-540-78663-4_19	specification language;computer science;management system;database;distributed computing;programming language;remote procedure call;computer security	PL	-53.85324161457807	53.0974231553246	7188
93b58f721de046dacada133902e6d07c6f46501f	ijournaling: fine-grained journaling for improving the latency of fsync system call		For data durability, many applications rely on synchronous operations such as an fsync() system call. However, latency-sensitive synchronous operations can be delayed under the compound transaction scheme of the current journaling technique. Because a compound transaction includes irrelevant data and metadata, as well as the data and metadata of fsynced file, the latency of an fsync call can be unexpectedly long. In this paper, we first analyze various factors that may delay an fsync operation, and propose a novel hybrid journaling technique, called ijournaling, which journals only the corresponding file-level transaction for an fsync call, while recording a normal journal transaction during periodic journaling. The file-level transaction journal has only the related metadata updates of the fsynced file. By removing several factors detrimental to fsync latency, the proposed technique can reduce the fsync latency, mitigate the interference between fsync-intensive threads, and provide high manycore scalability. Experiments using a smartphone and a desktop computer showed significant improvements in fsync latency through the use of ijournaling.	desktop computer;durability (database systems);interference (communication);interrupt latency;manycore processor;multi-core processor;relevance;scalability;smartphone;sync (unix);system call;transaction log;usenix annual technical conference	Daejun Park;Dongkun Shin	2017			system call;journaling file system;real-time computing;latency (engineering);parallel computing;computer science	OS	-12.62211376826978	54.6134446305828	7205
0e1ea6a46c3d2ce3c077f10d0f6b824afcb91b6a	cone-based clustering heuristic for list-scheduling algorithms	directed graphs;resource constraint;high level synthesis;scheduling;cone based list scheduling algorithm clustering heuristic latency minimization resource constraint priority function dependency graph;list scheduling;clustering algorithms heuristic algorithms optimal scheduling scheduling algorithm processor scheduling delay algorithm design and analysis high level synthesis clocks contracts;high level synthesis scheduling directed graphs	List scheduling algorithms attempt to minimize latency under resource constraints using a priority list. We propose a new heuristic that can be used in conjunction with any priority function. At each time-step, the proposed clustering heuristic tries to find a best match between ready operations and the resource set. The heuristic arbitrates among equal priority operations based on operation-clusters formed from the dependency graph. Based on this heuristic we have presented a new Cone-Based List Scheduling (\CBLS\@) algorithm. Results presented in this paper compare \CBLS\ with the well-known Force Directed List Scheduling (\FDLS\@) algorithm, for several synthesis benchmarks. In cases where \FDLS\ produces sub-optimal schedules, \CBLS\ produces better schedules and in other cases \CBLS\ performs as good as \FDLS\@. Moreover, in conjunction with a simple priority function (namely the self-force of an operator), \CBLS\ results in considerable improvement in latency when compared to \FDLS\ that has the same priority function. Finally, we show that \CBLS\ with the simple priority function performs better in execution time as well as latency when compared to the original \FDLS\ that has a relatively complex priority function.	algorithm;cluster analysis;cone;heuristic;least squares;list scheduling;overhead (computing);run time (program lifecycle phase);scheduling (computing);whole earth 'lectronic link	Sriram Govindarajan;Ranga Vemuri	1997		10.1109/EDTC.1997.582400	fair-share scheduling;priority inheritance;fixed-priority pre-emptive scheduling;mathematical optimization;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;deadline-monotonic scheduling;distributed computing	EDA	-11.460212764860534	59.24217508580432	7243
5e555bff9d93420b110809fc4cbd3b1591809739	typecasting actors: from akka to takka	actor programming;type checking;fault tolerance	"""Scala supports actors and message passing with the Akka library. Though Scala is statically typed, messages in Akka are dynamically typed (that is, of type Any). The Akka designers argue that using static types is """"impossible"""" because """"actor behaviour is dynamic"""", and, indeed, it is not clear that important actor support, such as supervision or name servers, can be implemented if messages are statically typed. Here we present TAkka, a variant of Akka where messages are statically typed, and show that it is possible to implement supervisors and name servers in such a framework. We show it is possible to smoothly migrate from Akka to TAkka, porting one module at a time. We show that TAkka can support behavioural upgrades where the new message type of an actor is a supertype of the old type. We demonstrate the expressiveness of TAkka by rewriting approximately 20 Akka applications; the percentage of lines that need to be changed varies from 44% (in a 25-line program) to 0.05% (in a 27,000-line program), with a geometric mean of 8.5%. We show that the execution speed, scalability, and throughput of TAkka programs are comparable to those of Akka programs."""	akka;benchmark (computing);code refactoring;compile time;compiler;emoticon;erlang (programming language);generic programming;message passing;rewriting;scala;scalability;smoothing;throughput;type system;typeparameter;typecasting (blogging)	Jiansen He;Philip Wadler;Philip W. Trinder	2014		10.1145/2637647.2637651	computer science;software engineering;programming language;algorithm	PL	-25.71373913496535	29.8286936859745	7260
31c8f208289cac64b2d3e221f1eb8fba71f69ffc	log-based middleware server recovery with transaction support	tolerancia falta;distributed system;crash failure;web based applications;entreprise;tratamiento transaccion;systeme reparti;red www;fault tolerant;client server architecture;averia franca;architecture client serveur;perforation;empresa;reseau web;logicial personalizado;database;base dato;indice aptitud;optimistic logging;recovery;intergiciel;indice aptitude;fichier log;sistema repartido;fichero actividad;internet;capability index;fault tolerance;firm;application fault tolerance;base de donnees;arquitectura cliente servidor;world wide web;middleware;panne franche;distributed systems;transaction processing;tolerance faute;log file;traitement transaction;exactly once execution	Providing enterprises with reliable and available Web-based application programs is a challenge. Applications are traditionally spread over multiple nodes, from user (client), to middle tier servers, to back end transaction systems, e.g. databases. It has proven very difficult to ensure that these applications persist across system crashes so that “exactly once” execution is produced, always important and sometimes essential, e.g., in the financial area. Our system provides a framework for exactly once execution of multi-tier Web applications, built on a commercially available Web infrastructure. Its capabilities include low logging overhead, recovery isolation (independence), and consistency between mid-tier and transactional back end. Good application performance is enabled via persistent shared state in the middle tier while providing for private session state as well. Our extensive experiments confirm both the desired properties and the good performance.	asp.net;database;experiment;middleware;multitier architecture;overhead (computing);server (computing);transaction processing;web application	Rui Wang;Betty Salzberg;David B. Lomet	2010	The VLDB Journal	10.1007/s00778-010-0199-1	embedded system;fault tolerance;real-time computing;computer science;operating system;database	DB	-26.93751940986209	42.92773300281041	7267
41ad450675d32f5a86f6a37cd65cf52c9939d958	the disjunction and related properties for constructive zermelo-fraenkel set theory		This paper proves that the disjunction property, the numerical existence property, Church’s rule, and several other metamathematical properties hold true for Constructive Zermelo-Fraenkel Set Theory, CZF, and also for the theory CZF augmented by the Regular Extension Axiom. As regards the proof technique, it features a self-validating semantics for CZF that combines realizability for extensional set theory and truth. The technique applies to full intuitionistic Zermelo-Fraenkel set theory, IZF, as well. MSC:03F50, 03F35	numerical analysis;zermelo–fraenkel set theory	Michael Rathjen	2005	J. Symb. Log.		zermelo–fraenkel set theory;universal set;combinatorics;discrete mathematics;topology;zermelo set theory;mathematics;constructive set theory;set theory	Logic	-9.806914673798298	12.42850594020351	7275
4110eb94ac40d0f4263847838a66b2265fc562a0	multi-level conceptual modeling and owl	tool support;conceptual model;levels of abstraction;ontology engineering;integrity constraints;multilevel model	Ontological metamodeling or multilevel-modeling refers to describing complex domains at multiple levels of abstraction, especially in domains where the borderline between individuals and classes is not clear cut. Punning in OWL2 provides decideable metamodeling support by allowing to use one symbol both as identifier of a class as well as of an individual. In conceptual modeling more powerful approaches to ontological metamodeling exist: materialization, potency-based deep instantiation, and m-objects/m-relationships. These approaches not only support to treat classes as individuals but also to describe domain concepts with members at multiple levels of abstraction. Based on a mapping from m-objects/m-relationships to OWL we show how to transfer these ideas from conceptual modeling to ontology engineering. Therefore we have to combine closed world and open world reasoning. We provide semantic-preserving mappings from m-objects and m-relationships to the decideable fragment of OWL, extended by integrity constraints, and sketch basic tool support for applying this approach.	data integrity;identifier;metamodeling;multilevel model;ontology (information science);ontology engineering;open world;plug-in (computing);principle of abstraction;unified modeling language;universal instantiation;web ontology language	Bernd Neumayr;Michael Schrefl	2009		10.1007/978-3-642-04947-7_23	computer science;conceptual model;multilevel model;data integrity;data mining;database	AI	-31.594333152437088	12.492439710319262	7280
818922aa08699a313beb6f157fb77b8eef6689ca	automated verification of communication protocols using ccs and bdds	verification;developpement logiciel;distributed system;diagrama binaria decision;systeme reparti;diagramme binaire decision;protocol verification;simultaneidad informatica;formal method;automated verification;concurrency;sistema repartido;desarrollo logicial;software development;safety critical system;communication protocol;systeme parallele;parallel system;verificacion;simultaneite informatique;sistema paralelo;binary decision diagram	The application of formal methods in protocol veri cation is of great importance, especially in the area of safety critical systems. Formal methods, however, are scarcely used in industrial practice today because they are hardly to integrate into the conventional system design and require a high e ort in computing. We describe the implementation and application of a tool that handles formal speci cations written in the process calculus CCS. The automatic veri cation process is based on binary decision diagrams to e ciently cope with state explosion problems. As an veri cation example we use a model of the CSMA/CD protocol including propagation delay e ects on the transmission medium.	binary decision diagram;bisimulation;calculus of communicating systems;communications protocol;complex systems;computation;formal methods;formal verification;machine translation;model checking;open road tolling;process calculus;propagation delay;scheduling (computing);software propagation;state space;systems design;transition system;turing completeness	Reiner Lichtenecker;Klaus Gotthardt;Janusz Zalewski	1998		10.1007/3-540-64359-1_771	communications protocol;real-time computing;verification;formal methods;concurrency;computer science;software development;programming language;binary decision diagram;algorithm	Logic	-23.970868231768392	31.445399273777692	7290
644dff46e54f55795fd40094a7370e92ba9f35ef	reasoning about actions with abductive logic programming	computational mechanics;representacion conocimientos;reasoning about action;adquisicion del conocimiento;circonscription;intelligence artificielle;logical programming;acquisition connaissance;circumscription;programmation logique;knowledge acquisition;artificial intelligence;inteligencia artificial;knowledge representation;representation connaissances;programacion logica;abductive logic programming;circonscripcion	In order to construct a computer-based system which can reason and act intelligently in the real world, we have to develop a computational but provably correct methodology and its related software system which can reason about actions and changes in dynamic domain. For this purpose we propose to use abductive logic programming paradigm as the computational mechanism. Technically, we make use of a simple, but extensible if needed, action description language to describe the domain in question. Then we use a sound and complete translation algorithm to transform domain descriptions into abductive logic programs. And thus reasoning about actions is reduced to abductive queries against abduc-tive logic programs. In this paper we will only address three important issues: knowledge assimilation, reenement of action theories, and concurrent actions. For the task of knowledge assimilation we will introduce a formal and computational methodology, called the possible causes approach , in contrast to Ginsberg's possible worlds approach and Winslett's possible models approach. For the reenement of a possibly incomplete action theory, we use tests on the domain, and then abductively reene the original domain description to a new one which is closer to the domain in reality. For concurrent actions, we introduce a new semantics by using three-valued uents to resolve connicts among atomic actions.	abductive logic programming;abductive reasoning;action description language;action theory (philosophy);algorithm;correctness (computer science);data assimilation;linearizability;marianne winslett;model of computation;possible world;programming paradigm;software system	Luís Moniz Pereira;Renwei Li	1997		10.1007/BFb0023940	knowledge representation and reasoning;abductive reasoning;computer science;artificial intelligence;computational mechanics;non-monotonic logic;machine learning;database;mathematics;programming language;circumscription;algorithm;abductive logic programming;temporal logic of actions	AI	-19.886671393388404	9.294034342637383	7293
04411bf945f721f5e375d478b0a2b2014d9fe4d3	an optimal evaluation of boolean expressions in an online query system	file organization;information retrieval;boolean expression;query	"""In this paper we consider the problem of retrieving reeords from a very large file, containing hundreds of thousands of records. Each record consists of a list of attributes and values. The selection of the subset of reeords from the file is in response to a Boolean expression which specifies a combination of desired values of certain attributes and values. The Boolean expression is communicated to the system by the user as a query, typed in on a low speed remote terminal. As a response to the query, the entire file has to be searched, and the Boolean expression is evaluated for each record. The method presented here improves the performance of such a system by speeding up the process of computing the truth value of a Boolean expression for an individual record. This is done by evaluating the expression based on the values of a subset of its components rather than the entire expression. The idea of evaluating a Boolean expression based on the values of a selected subset of its components is in itself not new. An early approach is presented in [1]. However, our approach to the problem differs in that we do not consider the individual components of the expression comparable in the effort it takes to compute their individual Boolean values. Rather we take into account a measure of that effort, and select the subset of components to be evaluated Copyright © 1977, Association for Computing Machinery, Inc. General permission to republish, but not for profit, all or part of this material is granted provided that ACM's copyright notice is given and that reference is made to the publication, to its date of issue, and to the fact that reprinting privileges were granted by permission of the Association for Computing Machinery. Author's address: Department of Mathematics, Ben Gurion University of the Negev, Beersheva 84 120, Israel. so that the """"expected effort"""" of evaluating the entire expression is minimal. The problem of easy and fast access to large data banks is one of today's most important topics in data processing. A new technology for handling large data bases is currently evolving (see, for example, [7]). In particular, the notion of organizing the data in formats that enable easy access, such as tree formats [2, 10, 12], as well as more general approaches such as Codd's [4--6] are now under development. However, the problem that we have been confronted with, which is not atypical, is that of making a very large data bank stored as a sequential file available to a quick response reporting system, flexible enough to respond to very general queries. Changes in the structure of the data bank would be very expensive to implement, and the requirement regarding the generality of the queries have made the desired structure of the data bank unclear. Consequently, we have directed our efforts towards minimizing the time of negotiating the existing file rather than change its structure. In the presentation that follows, we first define the set of Boolean expressions that are being dealt with in this paper as a context-free language. We then describe a particular tree representation of an expression. The terminal nodes 1~(i = 1, . . . , n) of the tree represent the attributes and values of the expression, and with each of them we associate two numbers t(i) and p(i) that are properties of the file and of the attributes. We then associate two numbers t and p, which are, in a way, generalizations of t(i) and p(i), with every node of the tree. Two theorems are then given which enable computat ion of the number t for a node in a straightforward fashion (p is easy enough to compute from its definition). Finally, an algorithm is given that computes the value of the Boolean expression so that the expected time of evaluation is minimal."""	accessibility;algorithm;average-case complexity;boolean expression;context-free language;database;information retrieval;organizing (structure);privilege (computing)	Michael Z. Hanani	1977	Commun. ACM	10.1145/359581.359600	and-inverter graph;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;boolean expression;standard boolean model;computer science;theoretical computer science;database;programming language;web search query;information retrieval;query language	PL	-28.749142636782736	4.2093452740236375	7300
5241df91775db0eb5b15b06cde9e179b96d81339	a meta-level architecture for strategic reasoning in naval planning	modelizacion;organization management;maximization;surveillance;intelligence artificielle;strategic planning;modelisation;vigilancia;monitoring;planification strategique;artificial intelligence;inteligencia artificial;monitorage;gestion organizacion;monitoreo;modeling;maximizacion;planificacion estrategica;gestion organisation;maximisation	The management of naval organizations aims at the maximization of mission success by means of monitoring, planning, and strategic reasoning. In resource-bounded (naval) situations strategic reasoning helps in determining if a go or no go should be given, or if attention should be shifted, to a certain evaluation of possible plans after an incident. An incident is an unexpected event which results in an unmeant chain of events, if left alone. In a planning context strategic reasoning can occur both in plan generation strategies (cf. [4]) and plan selection strategies. The above context gives rise to two important questions. Firstly, what possible plans are first to be considered? And secondly, what criteria are important for selecting a certain plan for execution? In resource-bounded situations first generated plans should have a high probability to result in a mission success, and the criteria to determine this should be as sound as possible. In this paper a generic meta-level architecture (cf. [1, 2, 3]) is presented for planning, extended with a strategic reasoning level. Expert knowledge is used to formally specify executable properties for each of the components of the architecture. These properties are used for simulation and facilitate formal verification of the simulation results.	automated planning and scheduling;entropy maximization;executable;formal verification;simulation	Mark Hoogendoorn;Catholijn M. Jonker;Peter-Paul van Maanen;Jan Treur	2005		10.1007/11504894_117	systems modeling;strategic planning;profit motive;artificial intelligence;operations research	AI	-39.862074777248445	23.578318908505405	7302
510c3f82759bb44300c25d52edbbcf8c369e343e	toga—a customizable service for data-centric collaboration	system engineering;gestion informacion;layered architecture;systeme intelligent;architecture systeme;computacion informatica;systeme cooperatif;sistema inteligente;collaboration;cooperative information system;information space;corba;groups;concurrent simultaneous engineering;cooperative systems;group awareness;ciencias basicas y experimentales;events;information management;intelligent system;communication protocol;workflow;cscw;arquitectura sistema;information system;gestion information;grupo a;system architecture;change propagation;systeme information;sistema informacion	Collaboration in cooperative information systems, like concurrent design and engineering, exploits common work and information spaces. In this paper we introduce the TOGA service (Transaction-Oriented Group and Coordination Service for Data-Centric Applications), which offers group management facilities and a push model for change propagation w.r.t. shared data, thus allowing for group awareness. Through TOGA's customizability and its layered architecture the service can be adapted to a variety of different collaboration scenarios. Multiple communication protocols (CORBA, UDP/IP, TCP/IP) are supported as well as basic transaction properties. Our approach enables the evolution of a set of separate applications to form a cooperative information system, i.e., it provides a technique towards component-oriented system engineering. In this paper we report on design issues, implementation aspects, and first experiences gained with the TOGA prototype.		Aiko Frank;Jürgen Sellentin;Bernhard Mitschang	1999	Inf. Syst.	10.1016/S0306-4379(00)00014-4	communications protocol;workflow;real-time computing;simulation;computer science;multitier architecture;operating system;computer-supported cooperative work;common object request broker architecture;database;information management;management;information system;collaboration	DB	-30.259600169653826	42.649066118501665	7311
e6962489980f344fb7a356f578f4e7eb92b082ae	a home-automation platform towards ubiquitous spaces based on a decentralized p2p architecture	p2p;ubiquitous computing;self organization;interactive space;distributed architecture;home automation	The vision of Home Environment is changing towards living interaction space populated of interconnected devices, services that encapsulate the functionality, and multiple interfaces through which the user can interact with these devices, in accordance with vision of ubiquitous computing. This paper presents a pervasive services platform for ubiquitous spaces based on distributed architecture P2P using JXTA technology, with an innovative approach, that favors the building of collaborative services from proactive entities, the peers. These ones are able to establish dynamic intercommunications synchronizing with others, form coalitions to cooperate with others for a common purpose, and are self-organized into groups.	centralized computing;client–server model;distributed computing;embedded system;entity;home automation;jxta;middleware;mobile information device profile;peer-to-peer;population;scalability;self-organization;server (computing);software deployment;spaces;ubiquitous computing	Sandra S. Rodríguez;Juan Antonio Holgado Terriza	2008		10.1007/978-3-540-85863-8_36	embedded system;home automation;space-based architecture;real-time computing;self-organization;computer science;applications architecture;artificial intelligence;peer-to-peer;solution architecture;distributed computing;ubiquitous robot;ubiquitous computing	HCI	-39.101201500832296	44.842303186196794	7321
e7203d6783725e7c7d3fbc9cbf97e1c6e4e5716c	roledisco: a middleware architecture and implementation for coordinated on-demand composition of smart service systems in decentralized environments		Future smart computing environments will heavily rely on the collaboration of services, which are dynamically interconnected to Smart Service Systems in order to develop their full potential. Such computing environments are coined by a high degree of distribution and decentralization rendering a centralized control of service composition and system adaptation infeasible. Self-organizing Software Systems tackle this issue to automatically compose service systems at run time. Due to missing holistic system specifications, they constitute a mainly linear service chain, which contradicts the collaborative nature of Smart Service Systems. Thus, we proposed a Two-Phase Development Methodology, which includes a role-based collaboration specification, for engineering Smart Service Systems. In this paper, we present RoleDiSCo, a middleware for coordinated on-demand composition of Smart Service Systems in decentralized environments in order to bridge the gap between design and run time.	centralized computing;holism;middleware;organizing (structure);role-based collaboration;run time (program lifecycle phase);service composability principle;software system;two-phase locking	Markus Wutzler;Thomas Springer;Alexander Schill	2017	2017 IEEE 2nd International Workshops on Foundations and Applications of Self* Systems (FAS*W)	10.1109/FAS-W.2017.118	architecture;rendering (computer graphics);software system;middleware;composition (visual arts);service design;distributed computing;internet of things;engineering	SE	-42.98287276670616	39.92911884586678	7328
dc2d0bc7812c7e40627a7772399eee9eddcb1ed6	on pushout consistency, modularity and interpolation for logical specifications	computability and decidability;interpolation;formal specification;extensions;pushout;formal specifications;internalization;interpretations;tipo dato;data type;calculabilite decidabilite;software engineering;specification formelle;especificacion formal;theory of computing;theory of computation;genie logiciel;modularity;coequalizer;type donnee;consistency;data types	We generalize three known results concerning (conservative) extensions to (faithful) interpretations. These results are Extension Modularity (a special case of the Modularization Theorem for logical specifications) and two familiar logical theorems, namely Robinson’s Joint Consistency and Craig-Robinson Interpolation. Their generalizations involve a pushout construction, in lieu of union, and their proofs rely on internalization techniques, including a novel one, which reduce to a large extent interpretations to extensions.	interpolation	Paulo A. S. Veloso	1996	Inf. Process. Lett.	10.1016/S0020-0190(96)00146-9	combinatorics;discrete mathematics;theory of computation;data type;computer science;formal specification;mathematics;programming language;algorithm	AI	-8.754802059606272	13.544474439282215	7337
2013971baf2130ca16e71b8fc8376ea6faf3b9ff	integrated environments	debuggers;debugging aids;integrated environments;performance;verification	Wikis are online collaboration tools to share information amongst users. Today’s wiki engines typically lack features to integrate structured data from backend databases. We have created a solution for integrating wiki technology with SAP structured data. Our solution will help provide a lightweight end-user programming environment for users to access complex structured data.	database;end-user development;integrated development environment;wiki	Insup Lee	1983	SIGPLAN Notices	10.1145/1006142.1006161		DB	-40.084792192987585	10.605732728490233	7339
2c5f46d4fffabd4fd3d9f8db9ceff2c5db828265	gracefully degradable algorithm for byzantine agreement			algorithm;byzantine fault tolerance	Felicita Di Giandomenico;M. L. Guidotti;Fabrizio Grandoni;Luca Simoncini	1988	Comput. Syst. Sci. Eng.			Theory	-22.92149541709955	44.86180014166174	7343
6a8de4fd0b3ae5d083a03651516419b1a079c0db	using developers' experience in cooperative design processes	design process;environmental impact;raw materials;process integration;integrated design;mechanical engineering;conceptual design;lessons learned;batch process;product quality;cooperative design	The process industries are characterized by continuous or batch processes of material transformation with the aim of converting raw materials or chemicals into more useful and valuable forms. The design of such processes is a complex process itself that determines the competitiveness of these industries, as well as their environmental impact. Especially the early phases of such design processes, the so-called conceptual design and basic engineering, reveal an inherent creative character that is less visible in other engineering domains, such as in mechanical engineering. This special character constitutes a key problem largely impacting final product quality and cost.#R##N##R##N#As a remedy to this problem, in cooperation with researchers and industrial partners from chemical and plastics engineering, we have developed an approach to capture and reuse experiences captured during the design process. Then, fine-grained method guidance based on these experiences can be offered to the developer through his process-integrated tools. In this section, we describe the application of our approach on the case study of the IMPROVE project. We first report on experiments made with a prototypical implementation of an integrated design support environment in the early project phases, and successively describe how it has been reengineered and extended based on additional requirements and lessons learned.		Michalis Miatidis;Matthias Jarke;Klaus Weidenhaupt	2008		10.1007/978-3-540-70552-9_10	iterative design;process design;systems engineering;engineering;computer-automated design;operations management;industrial engineering;design review;conceptual design;design education;product design;engineering design process;product engineering	SE	-61.12362722681437	11.404788531172992	7344
9517919056fb6877bc496542cd7d3f4bb61c4b99	space-aware coordination in respect		Spatial issues are essential in new classes of complex software systems, such as pervasive, multi-agent, and selforganising ones. Understanding the basic mechanisms of spatial coordination is a fundamental issue for coordination models and languages in order to deal with such systems, governing situated interaction in the spatio-temporal fabric. Along this line, in this paper we make space-aware coordination media out of ReSpecT tuple centres, by introducing the few basic mechanisms and constructs that enable the ReSpecT language to face most of the main challenges of spatial coordination in complex software systems.	android;complex systems;middleware;multi-agent system;pervasive informatics;situated;software system	Stefano Mariani;Andrea Omicini	2013			software system;situated;tuple;event-driven programming;distributed computing;computer science	AI	-39.85001106296713	41.84194626653686	7345
7168095cc741da5709549539e96e8ae88932dd48	the network ramdisk: using remote memory on heterogeneous nows	policy making;atm networks;data storage;low latency;operating system;device driver;unix file system	Efficient data storage, a major concern in the modern computer industry, is mostly provided today by traditional magnetic disks. However, the cost of a disk transfer (measured in processor cycles) continues to increase with time, making disk accesses increasingly expensive. In this paper we describe the design, implementation and evaluation of a Network RamDisk device that uses main memory of remote workstations as a faster‐than‐disk storage device. In our study we propose various reliability policies, making the device tolerant to single workstation crashes. We show that the Network RamDisk is portable, flexible, and can operate under any of the existing Unix file systems. The Network RamDisk has been implemented both on the Linux and the Digital Unix operating systems, as a block device driver without any modifications to the kernel code. Using several real applications and benchmarks, we measure the performance of the Network RamDisk over an Ethernet and an ATM network, and find it to be usually four to eight times better than the magnetic disk. In one benchmark, our system was two orders of magnitude faster than the disk. We believe that a Network RamDisk can be efficiently used to provide reliable low‐latency access to files that would otherwise be stored on magnetic disks.	atm turbo;benchmark (computing);computer data storage;device driver;disk storage;linux;operating system;tru64 unix;workstation	Michail Flouris;Evangelos P. Markatos	1999	Cluster Computing	10.1023/A:1019051330479	embedded system;parallel computing;real-time computing;computer science;operating system;computer data storage;computer network;low latency	OS	-15.18004527523997	51.84678976829701	7349
259ff151c4f79cdc5ec593bec29650c9643c6043	deterministic replay of system's execution with multi-target qemu simulator for dynamic analysis and reverse debugging	quick emulator;software;debugging;virtual machine;virtual machining;quick emulator full system logging mechanism deterministic replay mechanism multitarget qemu simulator dynamic analysis reverse debugging;virtual machine deterministic replay reverse debugging qemu;loading;multitarget qemu simulator;reverse debugging;full system logging mechanism;design and implementation;deterministic replay;debugging virtual machining computer bugs software loading programming linux;linux;program debugging digital simulation;program debugging;computer bugs;programming;qemu;deterministic replay mechanism;digital simulation;dynamic analysis	Deterministic replay is a technology that allows capturing the execution of a running virtual machine for later replay. It is useful for debugging failures that are not easily reproduced, capturing execution traces, dynamic analysis of the software, and so on. This paper describes the design and implementation of full-system logging and deterministic replay mechanisms in QEMU simulator for several target platforms.	debugger;debugging	Pavel Dovgalyuk	2012	2012 16th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2012.74	programming;parallel computing;real-time computing;software bug;computer science;virtual machine;operating system;dynamic program analysis;debugging;linux kernel	SE	-21.012833323423315	37.83441940114632	7354
73af2dbd6ded8c9fdbf9aa48a3a0c9d9b76010e3	statistical databases: their model, query language and security	query language	In this paper we deal with the problem of security of statistical databases, i.e. file systems. We propose a model of a statistical database in which to investigate the properties of statistical databases and we describe a query language connected with such a database. We discuss the problem of dependencies between attributes and we consider the case when database contains incomplete information. In the case of incomolete information the problems arrive with the interpretation of the query language, mainly for statistical terms. The need for a orecise semantic is here evident.	query language;statistical database	Zbigniew Michalewicz	1983			language identification;online aggregation;sargable;query optimization;database theory;query expansion;web query classification;boolean conjunctive query;data control language;computer science;query by example;data mining;database;rdf query language;web search query;view;information retrieval;query language;object query language	DB	-29.782643196281033	8.439296005788835	7356
3f44d3331bfdbfa3ec3d7cda41178a41051cf8ee	distributed causal model-based diagnosis based on interacting behavioral petri nets	silicon;analytical models;distributed system;petri nets reachability analysis computer science chaos fault diagnosis predictive models distributed computing safety protocols;behavior modeling;behavioral petri net;distributed processing;distributed causal model based diagnosis reasoning;data mining;forward reachability analysis;petroleum;adaptation model;petri nets distributed processing fault diagnosis;cognition;forward reachability analysis distributed causal model based diagnosis reasoning behavioral petri net backward reachability analysis;backward reachability analysis;petri nets;petri net;causal models;reachability analysis;fault diagnosis	This paper deals with the problem of causal model-based diagnosis of distributed systems. The setting we consider is a collection of interacting behavioral Petri nets (BPNs). Each BPN model represents the causal behavioral model of one subsystem and its interactions with neighboring subsystems. Interactions among subsystems are modeled by tokens that pass from one model to another via common places. Diagnosis reasoning scheme exploits, in a first step a backward reachability analysis on each net model to obtain local diagnoses; and in a second step, it exploits a forward reachability analysis for ensuring that local diagnoses are consistent and form global ones.	behavioral modeling;business process network;causal filter;causal model;distributed computing;graph (discrete mathematics);interaction;item unique identification;linear algebra;petri net;reachability;state space;telecommunications network;unsharp masking	Hammadi Bennoui;Allaoua Chaoui;Kamel Barkaoui	2009	2009 Eighth International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2009.11	real-time computing;computer science;theoretical computer science;distributed computing;petri net	Logic	-35.99057623078854	30.312573405100437	7370
8fcf8e3cd14d5b524d8a141ca6395151a767c977	application servers: born-again tp monitors for the web? (panel abstract)	intellectual property;application server;security	Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. While transaction processing monitors (TPMs) have been providing similar functionality for over three decades, ASs are their modern cousins. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard. ASs integrate developments in a number of areas of computer science: software engineering, distributed computing, transaction processing, database management, workflow management, ... Of course, the traditional TPM-style requirements for industrial strength features such as scalability, availability, reliability and high performance are equally important for ASs also. Security and authentication issues are additional important requirements in the web context. ASs support DBMSs not only as storage engines for user data but also as repositories for tracking their own state.	authentication;computer science;database engine;distributed computing;e-commerce;emergence;java;requirement;scalability;server (computing);server-side;software engineering;software repository;transaction processing;trusted platform module;world wide web;xml	C. Mohan	2001		10.1145/375663.375798	computer science;information security;operating system;database;world wide web;intellectual property;application server;server	DB	-33.403026800344755	43.046654659784274	7374
b39d585e6ca74507390172ff9f446c9d2456c2b2	semi-intelligible isar proofs from machine-generated proofs	proof assistants;automatic theorem provers;natural deduction	Sledgehammer is a component of the Isabelle/HOL proof assistant that integrates external automatic theorem provers (ATPs) to discharge interactive proof obligations. As a safeguard against bugs, the proofs found by the external provers are reconstructed in Isabelle. Reconstructing complex arguments involves translating them to Isabelle’s Isar format, supplying suitable justifications for each step. Sledgehammer transforms the proofs by contradiction into direct proofs; it iteratively tests and compresses the output, resulting in simpler and faster proofs; and it supports a wide range of ATPs, including E, LEO-II, Satallax, SPASS, Vampire, veriT, Waldmeister, and Z3.	algorithm;automated theorem proving;black box;code refactoring;correctness (computer science);discharger;hol (proof assistant);interactive proof system;interoperability;isabelle;logical framework;natural deduction;parsing;proof assistant;spass;satisfiability modulo theories;semiconductor industry;software bug;solver;z3 (computer)	Jasmin Christian Blanchette;Sascha Böhme;Mathias Fleury;Steffen Juilf Smolka;Albert Steckermeier	2015	Journal of Automated Reasoning	10.1007/s10817-015-9335-3	discrete mathematics;computer science;mathematics;proof assistant;programming language;natural deduction;algorithm	PL	-19.285000880493524	25.594178210603676	7383
34e12760a009d1f9662ba762e9de82e0322315fa	a survey of approaches for the visual model-driven development of next generation software-intensive systems	mda;adaptable;control engineering;software engineering;dynamic environment;visual modeling;information processing;software intensive systems;next generation;business process engineering;survey;mdd;hard real time	Software-intensive systems of the future are expected to be highly distributed and to exhibit adaptive and anticipatory behavior when operating in highly dynamic environments and interfacing with the physical world. Therefore, visual modeling techniques to address these software-intensive systems require a mix of models from a multitude of disciplines such as software engineering, control engineering, and business process engineering. As in this concert of techniques software provides the most flexible element, the integration of these different views can be expected to happen in the software. The software thus includes complex information processing capabilities as well as hard real-time coordination between distributed technical systems and computers. In this article, we identify a number of general requirements for the visual model-driven specification of next generation software-intensive systems. As business process engineering and software engineering are well integrated areas and in order to keep this survey focused, we restrict our attention here to approaches for the visual model-driven development of adaptable softwareintensive systems where the integration of software engineering with control engineering concepts and safety issues are important. In this survey, we identify requirements and use them to classify and characterize a number of approaches that can be employed for the development of the considered class of software-intensive systems. r 2006 Elsevier Ltd. All rights reserved.	business process;computer;control engineering;information processing;model-driven architecture;model-driven engineering;next-generation network;real-time clock;real-time computing;requirement;software engineering;visual modeling	Holger Giese;Stefan Henkler	2006	J. Vis. Lang. Comput.	10.1016/j.jvlc.2006.10.002	domain analysis;computing;software engineering process group;information engineering;information processing;system of systems engineering;business process reengineering;search-based software engineering;computer science;systems engineering;function model;social software engineering;software development;requirement;operating system;software engineering;domain engineering;software construction;database;requirements engineering;systems development life cycle;programming language;software development process;software requirements;software system;computer engineering;systems design	SE	-61.530049743402316	20.58211404803243	7406
7f772bc69a67bfb602b9596c95f2a179e11653e6	exploring the performance implications of memory safety primitives in many-core processors executing multi-threaded workloads	randomized mapping;cache side channel;svm	Security is a vital consideration for today's processor architectures, both at the software and hardware layers. However, security schemes are known to incur significant performance overheads. For example, buffer overflow protection schemes perform software checks for bounds on program data structures, and incur performance overheads that are up to several orders of magnitude. To mitigate these overheads, prior works focus on either changing the security scheme itself, or selectively apply the security scheme to minimize program vulnerabilities. Most of these works also focus primarily on single core processors, with no prior work done in the context of multicore processors. In this paper, we show how increasing thread counts can help hide the latency overheads of security schemes. We also analyze the architectural implications in the context of multucores, and the insights and challenges associated with applying these security schemes on mutithreaded workloads.	buffer overflow protection;central processing unit;data structure;list of cpu architectures;manycore processor;memory safety;multi-core processor;thread (computing)	Masab Ahmad;Syed Kamran Haider;Farrukh Hijaz;Marten van Dijk;Omer Khan	2015		10.1145/2768566.2768572	parallel computing;real-time computing;computer science;distributed computing	Arch	-10.074996125377336	51.24572952781804	7411
785c3caf691274186ae8efac68e985bdef921434	formal derivation of greedy algorithms from relational specifications: a tutorial	galois connection;program derivation;algebra of programming;dependent type;machine aided theorem proving	Abstract Many programming tasks can be specified as optimisation problems in which a relation is used to generate all possible solutions, from which we wish to choose an optimal one. A relational operator “shrink”, developed by Jose N. Oliveira, is particularly suitable for constructing greedy algorithms from such specifications. Meanwhile, it has become standard in many sub-fields in programming language that proofs must be machine-verified. This tutorial leads the readers through the development of algebraic derivations of three greedy algorithms, one fold-based and two unfold-based, using AoPA , a library designed for machine-verified relational program calculation.	algorithm;ontology components	Yu-Hsi Chiang;Shin-Cheng Mu	2016	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.12.003	discrete mathematics;dependent type;computer science;theoretical computer science;mathematics;programming language;program derivation;algorithm	AI	-15.850886219615711	18.626953827209153	7417
b18127b60d65db9c7bee284e62f04f75325690f8	scooter: a compact and scalable dynamic labeling scheme for xml updates	information retrieval;xml;algorithms;information storage and retrieval systems;dynamic labelling	Although dynamic labeling schemes for XML have been the focus of recent research activity, there are significant challenges still to be overcome. In particular, though there are labeling schemes that ensure a compact label representation when creating an XML document, when the document is subject to repeated and arbitrary deletions and insertions, the labels grow rapidly and consequently have a significant impact on query and update performance. We review the outstanding issues todate and in this paper we propose SCOOTER a new dynamic labeling scheme for XML. The new labeling scheme can completely avoid relabeling existing labels. In particular, SCOOTER can handle frequently skewed insertions gracefully. Theoretical analysis and experimental results confirm the scalability, compact representation, efficient growth rate and performance of SCOOTER in comparison to existing dynamic labeling schemes.	algorithm;binary file;cpu cache;experiment;fixed point (mathematics);graceful exit;graph labeling;retrievability;scalability;sequence labeling;xml;xml database	Martin F. O'Connor;Mark Roantree	2012		10.1007/978-3-642-32600-4_4	xml;computer science;data mining;database;world wide web	DB	-30.717053019391404	4.288185875781553	7423
2f4fcf094ca27d3de156459e648490fab917a4cb	using theory morphisms for implementing formal methods tools	lenguaje programacion;morphisme;morfismo;langage contrainte objet;representacion conocimientos;embedding theorem;programming language;semantica formal;object constrained language;metodo formal;methode formelle;tipo dato;existence theorem;formal methods;structuration theory;data type;formal semantics;specification language;formal method;theorem proving;semantique formelle;demonstration theoreme;theorem prover;ocl;langage programmation;plongement peu profond;morphism;lenguaje especificacion;demostracion teorema;knowledge representation;type donnee;representation connaissances;langage specification;shallow embeddings	Tools for a specification language can be implemented directly (by building a special purpose theorem prover) or by a conservative embedding into a typed meta-logic, which allows their safe and logically consistent implementation and the reuse of existing theorem prover engines. For being useful, the conservative extension approach must provide derivations for several thousand “folklore” theorems. In this paper, we present an approach for deriving the mass of these theorems mechanically from an existing library of the meta-logic. The approach presupposes a structured theory morphism mapping library datatypes and library functions to new functions of the specification language while uniformly modifying some semantic properties; for example, new functions may have a different treatment of undefinedness compared to old ones.	automated theorem proving;combinatory logic;coq (software);correctness (computer science);first-order reduction;formal methods;haskell;lambda lifting;mike lesser;navier–stokes equations;organizing (structure);specification language;theory;type system	Achim D. Brucker;Burkhart Wolff	2002		10.1007/3-540-39185-1_4	discrete mathematics;formal methods;computer science;mathematics;automated theorem proving;programming language;algorithm	PL	-14.15576464878293	17.326102253150722	7440
60405bbfc656c72144d3e7672150e939075907bd	the conic communication system for distributed process control	communication system;distributed processing	The physical distribution of equipment and machinery on an industrial site makes it particularly suitable for implementing distributed computer control systems based on the use of microcomputers. A communication system is needed to allow cooperation and coordination between the microcomputer stations forming the control system. This paper identifies the communication requirements for this class of applications and then gives an overview of the CONIC Architecture, developed at Imperial College for Distributed Computer Control Systems (DCCS). The paper concentrates on the CONIC Communication System which was developed to support this distributed computer architecture, and uses the framework of the ISO Open Systems Reference model to describe the communication system.		Morris Sloman	1983		10.1007/978-3-642-68829-4_35	distributed algorithm;real-time computing;computer science;distributed computing;distributed design patterns;communications system	HPC	-29.262549037359676	38.89303648096289	7444
29627f98b263ddc2f85155b4b195493a6bbdd1f9	an adaptive bloom filter cache partitioning scheme for multicore architectures	multicore architectures cache partitioning bloom filters;cache storage;cache storage adaptive filters;multicore architectures;adaptive filters multicore processing resource management computer architecture computer science application software degradation silicon delay system performance;bloom filter;last level cache;uniprocessor caches;least recently used policy;indexing terms;uniprocessor caches adaptive bloom filter cache partitioning scheme multicore architectures exacerbating contention last level cache cache replacement policies uniprocessor systems least recently used policy;chip;bloom filters;cache partitioning;adaptive filters;cache replacement;resource sharing;memory systems;uniprocessor systems;multicore processors;cache replacement policies;adaptive bloom filter cache partitioning scheme;least recently used;exacerbating contention;replacement policy	This paper investigates the problem of partitioning the last-level shared cache of multicore architectures. Contention for such a shared resource has been shown to severely degrade performance when running multiple applications. As architectures incorporate more cores, multiple application workloads become increasingly attractive, further exacerbating contention at the last-level cache. Today, cache replacement policies, extensively studied for uniprocessor systems, are being employed within new multicore architectures with little, if any, adaptation. However the parameters in these new systems are likely to be different. The least recently used (LRU) policy, for example, which is widely accepted as the best replacement policy in uniprocessor caches, often results in poor resource sharing in a multicore system, signalling the importance of reevaluating the effectiveness of these policies in the new architectures. This paper proposes adaptive bloom filter cache partitioning (ABFCP), a low-cost, dynamic cache partitioning mechanism capable of better resource sharing at the last-level cache than LRU, improving the performance of an eight-core system on average by 5.92% over the LRU policy. Moreover, the proposed scheme provides the equivalent performance benefits that could be gained from almost a 50% increase in the last-level cache and shows increasing benefit as the number of cores rises.	approximation;bloom filter;cpu cache;cache (computing);computer architecture;emi (protocol);multi-core processor;overhead (computing);requirement;simultaneous multithreading;symmetric multiprocessing;thread (computing);uniprocessor system	Konstantinos Nikas;Matthew Horsnell;Jim D. Garside	2008	2008 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation	10.1109/ICSAMOS.2008.4664843	bus sniffing;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;cache;computer science;cache invalidation;distributed computing;smart cache;cache algorithms;cache pollution	HPC	-9.291956862497821	51.90424274976173	7448
20b260c18ad6fdb4552e311fa490d0656d66b96a	relating default logic and circumscription	default logic	Default logic and the various forms of circumscript ion were developed to deal w i th similar problems. In this paper, we consider what is known about the relationships between the two approaches and present several new results extending this knowledge. We show that there are interesting cases in which the two formalisms do not correspond, as wel l as cases where default logic subsumes circumscript ion. We also consider positive and negative results on translating between defaults and circumscript ion, and develop a context in which they can be evaluated.	circumscription (logic);default logic	David W. Etherington	1987			discrete mathematics;computer science;artificial intelligence;mathematics;default logic;algorithm	AI	-15.36342592216885	9.161012152310363	7449
6933cecf5135f03502dc2539e69f682e0782a618	a dynamic virtual organization solution for web-services based grid middleware	service oriented architecture dynamic virtual organization solution web services grid computing middleware pseudo code message sequence authentication single sign on identity federation authorization;single sign on;message sequence;authorisation;middleware authorisation grid computing internet message authentication message passing;authentication;cost reduction;middleware authorization content addressable storage grid computing authentication data security telecommunication traffic system performance decision support systems computer science;grid middleware;web service;satisfiability;system performance;it security;internet;identity federation;network traffic;virtual organization;pseudo code;dynamic virtual organization solution;web services;message passing;middleware;authorization;message authentication;service oriented architecture;grid computing	"""Many solutions of virtual organization have been proposed in recent years. Most of them have one or more centralized component(s). Here, we show a system, which is fully distributed and independent. This system is designed using the Web-services based technologies and uses the idea """"node"""" to indicate the server. It provides several simple behaviours to satisfy the basic requirements of grid computing. To easily describe the solution, we use pseudo code to model the message sequence in the system. The virtual organization solution builds an environment of authentication, single-sign-on, identity federation, long-lived authorization, short-lived delegation and transferable configuration. It is composed of several components. This system is designed using service-oriented architecture and has shown its security and benefit in administrative cost reduction, network traffic decrease and system performance improvement"""	authorization;basic access authentication;centralized computing;computation;data integrity;grid computing;hoc (programming language);middleware;network traffic control;pseudocode;requirement;scalability;security token;server (computing);service-oriented architecture;service-oriented device architecture;single sign-on;user space;virtual organization (grid computing);web service;world wide web	Yih-Jiun Lee	2005	16th International Workshop on Database and Expert Systems Applications (DEXA'05)	10.1109/DEXA.2005.12	web service;computer science;database;distributed computing;computer performance;authorization;law;world wide web;computer security	HPC	-45.92956931638647	55.98154752277519	7450
fa22f5505f45df81033f512869d5a76434db41e7	developing secure products in the age of advanced persistent threats	apt advanced persistent threats secure software attack resistant software secure software development lifecycle emc risk governance bsimm safecode computer security;electronic mail;bsimm;software engineering product development security of data;product life cycle management;safecode;electromagnetic compatibility;secure software development lifecycle;secure software;software engineering;computer security;software security;apt;advanced persistent threats;software development;computer security software development electronic mail programming electromagnetic compatibility product life cycle management;attack resistant software;industry roadmap advanced persistent threats security assumptions secure product development apt product security emc;risk governance;programming;security of data;emc;product development	Advanced persistent threats (APTs) are making technology providers reconsider their security assumptions for secure product development. This article suggests an industry roadmap for rethinking product security in the face of APTs. It also describes steps EMC has taken to implement this roadmap and strengthen its product development practices.	new product development	Eric Baize	2012	IEEE Security & Privacy	10.1109/MSP.2012.65	software security assurance;programming;computer science;software development;computer security;product life-cycle management;new product development;electromagnetic compatibility	Security	-57.339779346649706	47.579932612777675	7456
0677fe9b6d1fd9684ab548bb2cd5e505ae74c7f9	an implementation model of rendezvous communication	higher order	Introduction This paper describes the low-level primitives necessary to implement a particular flavor or inter-process communication. It is motivated by the design of a communication subsystem for a higher-order functional language [Cardelli 84]. Although we try to abstract somewhat from the special characteristics of that language, the model does not accommodate a wide range of communication schemes. This communication model is intended to be used on (uniprocessor) personal computers. In this model, processes running on the same processor can share the same address space. If the underlying language is safe, a process can affect other processes only by communication, or by affecting data structures which have been explicitly transmitted. This ensures privacy and data protection even in a shared address space. Processes running in the same address space can exchange arbitrarily complex objects very cheaply, just by passing pointers. Processes running on different processors communicate through restricted “flat” channels, e.g. character channels. In this case, complex objects have to be encoded to fit into flat channels, and decoded on the other side; the encoding activity may or may not be automatic. In any case there is a semantic difference between exchange of objects in the same address space, where objects are shared, or in different address spaces, where objects are copied. The basic communication mechanism is rendezvous [Milner 80]: both the sender and the receiver may have to wait until the other side is ready to exchange a message. Both the sender and the receiver may offe communications simultaneously on different channels: when a pair of complementary offers is selected for a rendezvous, all the other simultaneous offers, on both sides, are retracted. The scheduling is non-preemptive: a running process will run until it explicitly gives up control (e.g. by attempting a communication); at that point other processes will get a chance to run. We assume a cooperative universe, where no process will try to take unfair advantage of other processes, unless it has some reason for doing so. Channels and processes can be dynamically created. Channels can be manipulated as objects and even passed through other channels [Abramsky 83, Inmos 84, Milner 82]. Processes are not denotable values; they can only be accessed through channels. In the following sections, we use the term “pool” rather than “queue” because the latter term implies a particular scheduling policy to which we do not wish to commit ourselves. Note that the relation “being in the same pool” is non-transitive: an object can appear in different pools without implying the equivalence of all such pools.	address space;central processing unit;code;concurrency (computer science);data structure;functional programming;high- and low-level;information privacy;inter-process communication;personal computer;plotkin bound;pointer (computer programming);programming language;scheduling (computing);semantics (computer science);state transition table;turing completeness;uniprocessor system	Luca Cardelli	1984		10.1007/3-540-15670-4_20	simulation;higher-order logic;computer science;control theory;distributed computing;programming language	PL	-26.922405479766123	32.60642096461846	7464
dab848a622756cd288c6a2f82ab9d35070b821b1	characterizing sat problems with the row convexity property	probleme satisfiabilite;regle inference;logique propositionnelle;satisfiability;constraint satisfaction;inference rule;satisfaction contrainte;codificacion;propositional logic;problema satisfactibilidad;coding;characterization;constraint satisfaction problem;satisfaccion restriccion;logica proposicional;caracterisation;satisfiability problem;caracterizacion;codage;regla inferencia	Using the literal encoding of the satisfiability problem (SAT) as a binary constraint satisfaction problem (CSP), we relate the path consistency concept and the row convexity of CSPs with the inference rules in the propositional logic field. Then, we use this result to propose a measure characterizing satisfiable and unsatisfiable 3-SAT instances. The correlation between the computational results allows us to validate this measure.	hereditary property;local consistency	Hachemi Bennaceur;Chu Min Li	2002		10.1007/3-540-46135-3_52	mathematical optimization;combinatorics;discrete mathematics;constraint satisfaction;computer science;mathematics;propositional calculus;coding;boolean satisfiability problem;constraint satisfaction problem;algorithm;rule of inference;satisfiability	AI	-7.18043342347202	19.063465159965155	7468
5fa908e59588c4a2ba81f8f8f4245ddaca53c87d	secured information flow for asynchronous sequential processes	distribution;security model;secure information flow;information flow;distributed objects;asynchronous communication;active objects;futures;objects;access control;data flow;data confidentiality	We present in this article a precise security model for data confidentiality in the framework of ASP (Asynchronous Sequential Processes). ASP is based on active objects, asynchronous communications, and data-flow synchronizations. We extend it with security levels attached to activities (active objects) and transmitted data. We design a security model that guarantees data confidentiality within an application; this security model takes advantages of both mandatory and discretionary access models. We extend the semantics of ASP with predicate conditions that provide a formal security framework, dynamically checking for unauthorized information flows. As a final result, all authorized communication paths are secure: no disclosure of information can happen. This theoretically-founded contribution may have a strong impact on distributed object-based applications, that are more and more present and confidentiality-demanding on the Internet, it also arises a new issue in data confidentiality: authorization of secured information flow transiting (by the mean of futures) through an unsecured component.	actor model;authorization;computer security model;confidentiality;dataflow;distributed object;download;futures and promises;information flow;internet;object-based language	Isabelle Attali;Denis Caromel;Ludovic Henrio;Felipe Luna Del Aguila	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.05.045	computer security model;distribution;futures contract;data flow diagram;information flow;confidentiality;asset;computer science;access control;object;asynchronous communication;security service;distributed computing;distributed object;internet privacy;computer security	Security	-50.804065147376775	53.81145534787666	7469
4aa603ff2f436a7fd655970b741932b53ad10343	coalgebraic weak bisimulation from recursive equations over monads	conference paper	Strong bisimulation for labelled transition systems is one of the most fundamental equivalences in process algebra, and has been generalised to numerous classes of systems that exhibit richer transition behaviour. Nearly all of the ensuing notions are instances of the more general notion of coalgebraic bisimulation. Weak bisimulation, however, has so far been much less amenable to a coalgebraic treatment. Here we attempt to close this gap by giving a coalgebraic treatment of (parametrized) weak equivalences, including weak bisimulation. Our analysis requires that the functor defining the transition type of the system is based on a suitable order-enriched monad, which allows us to capture weak equivalences by least fixpoints of recursive equations. Our notion is in agreement with existing notions of weak bisimulations for labelled transition systems, probabilistic and weighted systems, and simple Segala systems.	bisimulation;monad (functional programming);process calculus;recursion	Sergey Goncharov;Dirk Pattinson	2014		10.1007/978-3-662-43951-7_17	discrete mathematics;computer science;bisimulation;mathematics;algorithm	PL	-10.73767238812712	20.666182607815	7474
aceda07b05f7cdc7dcc754a663aadfcb40f792eb	an active functional intensional database	document structure;base donnee;informatique mobile;estructura documental;activation function;structure document;pervasive computing;database;base dato;informatica difusa;informatique diffuse;base donnee active;base donnee intentionnelle fonctionnelle active afid;version management;architecture editeur souscripteur;active databases;programmation intentionnelle;modele donnee;arquitectura publicacion suscripcion;mobile computing;publish subscriber architecture;gestion version;data models	We introduce a new kind of functional database that unifies concepts from the realms of publish-subscribe middleware, pervasive computing, and intensional programming. The AFID (Active Functional Intensional Database) Project allows the distribution of both pervasive context and related, versioned content, and offers the means of effecting a client’s interaction with both. The AFID data model builds on existing infrastructure from the Intense project for the efficient manipulation and networked distribution of intensional context, adding the ability to encode multiple versions of any complex entity, where each version may vary in both content and structure, at any level of granularity. Further, the system ensures that clients may listen to structured, minimal changes in specific logical versions of encoded entities, as they occur, whenever any change is made to the total entity encoded in the database.	data model;encode;entity;intensional logic;lucid;middleware;pervasive informatics;publish–subscribe pattern;realms;software versioning;ubiquitous computing	Paul Swoboda;John Plaice	2004		10.1007/978-3-540-30075-5_6	data modeling;computer science;artificial intelligence;document structure description;data mining;database;activation function;programming language;mobile computing;world wide web	DB	-31.01014816979114	12.813514685766657	7477
5194ef8924d17395810e0540c006b41673f3d536	a modular approach to on-stack replacement in llvm	llvm;mcjit;on stack replacement;matlab;jit compilation;dynamic optimization	On-stack replacement (OSR) is a technique that allows a virtual machine to interrupt running code during the execution of a function/method, to re-optimize the function on-the-fly using an optimizing JIT compiler, and then to resume the interrupted function at the point and state at which it was interrupted. OSR is particularly useful for programs with potentially long-running loops, as it allows dynamic optimization of those loops as soon as they become hot.  This paper presents a modular approach to implementing OSR for the LLVM compiler infrastructure. This is an important step forward because LLVM is gaining popular support, and adding the OSR capability allows compiler developers to develop new dynamic techniques. In particular, it will enable more sophisticated LLVM-based JIT compiler approaches. Indeed, other compiler/VM developers can use our approach because it is a clean modular addition to the standard LLVM distribution. Further, our approach is defined completely at the LLVM-IR level and thus does not require any modifications to the target code generation.  The OSR implementation can be used by different compilers to support a variety of dynamic optimizations. As a demonstration of our OSR approach, we have used it to support dynamic inlining in McVM. McVM is a virtual machine for MATLAB which uses a LLVM-based JIT compiler. MATLAB is a popular dynamic language for scientific and engineering applications that typically manipulate large matrices and often contain long-running loops, and is thus an ideal target for dynamic JIT compilation and OSRs. Using our McVM example, we demonstrate reasonable overheads for our benchmark set, and performance improvements when using it to perform dynamic inlining.	benchmark (computing);code generation (compiler);compiler;dynamic programming;inline expansion;interrupt;just-in-time compilation;llvm;matlab;mathematical optimization;open-source religion;virtual machine	Nurudeen Lameed;Laurie J. Hendren	2013		10.1145/2451512.2451541	computer architecture;parallel computing;real-time computing;computer science;just-in-time compilation;programming language	PL	-17.800130440510504	36.05220091619302	7492
d7f78d5f3511dcb262d3bd798f06193ddc7cda95	declarative multithreaded programming	declarative programming;active port;privileged dimension;multi threading;synchronous programming;multidimensional programming;yarn;clocks;set theory multi threading programming languages;distributed computing;set theory multithreaded programming translucid declarative programming language reactive system privileged dimension active port;set theory;yarn equations libraries distributed computing mobile computing computer applications application software computer science australia multidimensional systems;data mining;translucid declarative programming language;cartesian programming;multidimensional programming synchronous programming distributed computing declarative programming cartesian programming;reactive system;mathematical model;programming;context;programming languages;multithreaded programming	We demonstrate how TransLucid can be used as a reactive system by introducing sets and privileged dimensions for time and port in the language. At each instant, there is a set of active ports, where sets of equations, demands and threads are all registered. Each thread defines a sequence of (state,demand) pairs, and threads may interact through the overall set of equations. The entire system remains fully declarative.	declarative programming;real-time computing;software system;state (computer science);thread (computing);ubiquitous computing	Blanca Mancilla;John Plaice	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.140	programming;thread;parallel computing;declarative programming;multithreading;reactive system;computer science;theoretical computer science;operating system;mathematical model;database;programming language;set theory	Visualization	-25.53126886535551	33.87303761946537	7493
c0edfe44ac3eac876c0f6c521ba48667183286b1	pressing for parallelism: a prolog program made concurrent	swinburne	Abstract   We describe the translation of a nontrivial program for solving equations from PROLOG to Concurrent PROLOG, and further to Flat Concurrent PROLOG. The translation from PROLOG to Concurrent PROLOG required understanding of the program but was straightforward. The translation from Concurrent PROLOG to Flat Concurrent PROLOG was more suitable to be the basis for automatic procedures. The different styles of translation used are illustrated with examples of code from the three programs. The gain in speed by performing computations in parallel is discussed.	prolog	Leon Sterling;Michael Codish	1986	J. Log. Program.	10.1016/0743-1066(86)90006-3	computer science;theoretical computer science;programming language;algorithm	PL	-24.15744617238749	23.239865294399806	7502
0a513a0bed4ab56f297821e9ef0de2f4c0259e4d	integrated business-process driven design for service-oriented enterprise applications	modeling technique;information systems;service orientation;indexing terms;business process model;business model;business environment;use case diagram;linear time;3d vision;process engineering;business process re engineering;service oriented architecture;modeling;sequence diagram;business planning;supply chain management;business process	This paper proposed a new approach for integrated business-process-driven modeling and implementation for service-oriented enterprise applications. There are three phases in this approach: business environment modeling, business process modeling, and compiling. Business environment modeling employs a new modeling technique combining both the advantages of use case diagram and sequence diagram in UML. Business process modeling is an essential part of the development, this paper contributed a new BPEL-enabled foldable flowchart for it. A BPEL-enabled foldable flowchart is capable of showing any levels of the model details in a same plane, while a normal flowchart demands a 3d-vision indeed. The mapping algorithms are also given and proved to be of linear time complexity. At compiling phase, the business process model is compiled into several deployable files. These files can be deployed on a process engine to create a new process that meets the business requirement. Furthermore, this approach is implemented on the open platform Eclipse V3.1 so that it can be integrated with other SOA tools to provide a total solution for building enterprise applications. Finally there is a demonstration showing how to develop a supply chain management system for the retail industry using this approach.	algorithm;backup-file format;business process execution language;business requirements;compiler;eclipse;enterprise software;flowchart;nvidia 3d vision;open platform;process modeling;requirement;semantic web service;sequence diagram;service-oriented architecture;service-oriented device architecture;service-oriented modeling;time complexity;unified modeling language;use case diagram	Xingdong Shi;Weili Han;Yinsheng Li;Ying Huang	2007	Int. J. Pervasive Computing and Communications	10.1108/17427370710847309	sequence diagram;business model;time complexity;use case diagram;supply chain management;systems modeling;index term;business domain;computer science;artifact-centric business process model;business process management;function model;integrated enterprise modeling;service-oriented modeling;service-oriented architecture;process driven development;process modeling;business process model and notation;process management;business process;enterprise integration;business process discovery;business rule;business process modeling;information system;business activity monitoring;business architecture	DB	-56.214124984407064	16.97315406600907	7522
d02005b7e3081d0797c0fbb56845379693ba3f32	nymph: a multiprocessor for manipulation applications	control systems;programming environments;application software robot kinematics intelligent robots computer graphics laboratories programming environments robot control control systems artificial intelligence computer interfaces;application software;intelligent robots;programming environment;computer graphics;processor sharing;artificial intelligent;robot control;rest of the world;artificial intelligence;computer interfaces;robot kinematics	The robotics group of the Stanford Artificial Intelligence Laboratorg is currently developing a new computational system for robotics applications. Stanford’s NYMPH system uses multiple NSC 32016 processors and one MC68010 based processor, sharing a common Intel hfultbbus. The 32K processors provide the raw computational power needed for advanced robotics applications, and the 68K provides a pleasant interface with the rest of the world. Software has been developed to provide useful communications and synchronization primitives, without consuming excessive processor resources or bus bandwidth. NYMPH provides both large amounts of computing power and a good programming environment, making it an effective research tool. Introduction The real time requirements of modern applications in robotics control require large amounts of computing power. Multiprocessor machines are well suited for these applications because they can economically and flexibly provide the large amounts of computing power equired. Single processors of a multiprocessor system can manage single time critical tasks. By coordinating these processors, real time systems can be constructed. NYMPH (Not Your average Multiprocessor Hack) is a system being developed by the robotics group of the Stanford Artificial Intelligence Laboratory to meet the computational requirements of present and future robotics control applications. The computational requirements of creating a pleasant and functional software environment for a robotics control processor are in direct opposition to the primary goal of keeping the computing power available to the servo loop calculations. In previous control systems, programmers have suffered immeasurable grief searching for clever ways to prevent servo calculations from being interrupted by 1 / 0 requests and other operating system tasks. Such interrupt mechanisms are good for systems in which the first concern is to communicate with people, but in real time robotics applications they often prevent the computers from keeping pace with the machines. IJsing a multiprocessor a chitecture further complicates the problem of creating a pleasant user environment. It would be convenient t o be able to control arbitrary applications running on all the processors from a single program or a single terminal, but the communications and synchronization problems involved in creating such an environment mandate large and complex structures to coordinate them, making the pleasant programming environment inefficient and difficult to build. Because of the inherent complcxif,y of multiprocessor environments, and the need to preserve available computing resources for the application programs, manipulation multiprocessors tend to have somewhat uncivilized user interfaces. In KY-MPH, multiprocessor c ntrol structures have been integrated into an existing system, adding the processing power necessary for robotics applications to an lready cxistmg programming environment. While NYMPH’S 32K computers provide real time computing power and high speed floating point computation, the 68K manages user interaction. The 68K is part of a Sun 120 computer, with an 800 by 1024 bitmapped display and high speed graphics capabilities. The 68K runs the V-System with the Virtual Graphics Terminal Server (VGTS) window system, developed by the Distributed Systems Group of Stanford LJniversity (Cheriton 1982). Using the V-Syskm with t.he VGTS, the NYMPH programmer can have interaction windows for each processor, plus editors, t,erminal emulators, graphics capabilities, network access, and other useful facilities provided by the V-System. The researcher can edit files, test software on the multiprocessor, and analyze output with the aid of graphics, quickly and conveniently, all from the same console. Previous Work Computers using multiple microprocessors are becoming important as a cost effective solut.ion to the computational demands of real time robotics applications. The 32K one board computers used in NYMPH provide 40% of t.he floating point speed of a Vas 11/780 (with a floating point accelerator) at 1% of the cost. Much work has been done in the area of the feasibility and efficiency of manipulation multiprocessors (Nigam and Lee 1985, Zhcng and Chen 1985). Research has also been directed towards the development of special system software to run on such machines (Schwan et . al. 1985, Siegel et. al. 1986). A group of researchers a t MIT (Siegel e t a1 1985) used five MC68010s with a DMA link to a DEC Vax 11/750 to control the UtahlhlIT Hand. In this system, the Vax is used for user and file 1 / 0 and program development. This system also utilized a messagestyle communication system and synchronization by means of a servo-loop-scheduler routine. Ozguner and Kao (1985) have d signed a reconfigurable multiprocessor to control the Ohio State University hexapod walking machine. This multiprocessor uses 4 Intel 86/30 single board computers with fault detectlon and correction hardware, communicating on four busses. Error recovery using redundancy was investigated with this machine. Architecture NYMPH uses seven 32K single board computers to provide the bulk of its computing power. The 32K boards run at 10MHz and each board includes 32K bytes of ROhl, 5i2k bytes of RAM, a floating point co-processor, an 825512 Programmable Peripheral Interface, and two serial ports. The 32081 floating point co-processor greatly enhances the performance of the system, enabling the 32K to do a floating point multiply in 6 microseconds. CH2282-2/86/0000/1731$01.0	access network;artificial intelligence;bitmap;byte;central processing unit;computation;computer;control system;coprocessor;direct memory access;distributed computing;emulator;entity–relationship model;fallout;integrated development environment;interrupt;microprocessor;microsoft windows;motorola 68000;multiprocessing;ns320xx;national supercomputer centre in sweden;negative feedback;operating system;peripheral;programmer;random-access memory;raster graphics;real-time computing;reconfigurable computing;redundancy (engineering);requirement;robotics;scheduling (computing);serial port;servo;siegel disc;stanford university centers and institutes;terminal server;user interface;vax	J. Bradley Chen;Ronald S. Fearing;Brian S. R. Armstrong;Joel W. Burdick	1986		10.1109/ROBOT.1986.1087485	embedded system;application software;simulation;computer science;control system;artificial intelligence;robot control;computer graphics;robot kinematics;computer engineering	Robotics	-15.43051199359743	50.75062654166136	7529
70b8520dc4ca9cc2ea718ad3c856099be336d27d	a lightweight vulnerability mitigation framework for iot devices		Many of today's Internet of Things (IoT) devices are vulnerable due to the large amount of overhead incurred when their operating systems are patched against emerging vulnerabilities. In addition, legacy IoT devices are no longer supported by their manufacturers, leaving customers with unpatched devices that can be easily exploited by attackers. Thus, there is an urgent need for a solution that provides a lightweight and low-cost mechanism for preventing exploitation of vulnerable IoT devices. In this paper, we propose an innovative cloud-based framework for protecting IoT devices. The proposed framework consists of a cloud service and a designated IoT security appliance. The security appliance controls the network traffic flowing to and from the vulnerable device and verifies that it does not violate a set of rules, represented by a vulnerability mitigation policy, that have been derived and synthesized by the cloud service from public corpora of Common Vulnerabilities and Exposures (CVE). We demonstrate how the proposed solution can be applied as a cost-effective solution capable of preventing exploitation of vulnerable IP cameras as part of a prominent botnet attack called Mirai.	botnet;cloud computing;common vulnerabilities and exposures;internet of things;mirai;network traffic control;operating system;overhead (computing);text corpus;vulnerability management	Noy Hadar;Shachar Siboni;Yuval Elovici	2017		10.1145/3139937.3139944	common vulnerabilities and exposures;computer security;computer network;botnet;computer science;cloud computing;internet of things;vulnerability	Security	-50.76886272775013	58.554341426158516	7552
044bc67479bce5237041334235862abeaa21e7e0	cyrus: towards client-defined cloud storage	distributed algorithms;extensibility;depspace;zookeeper;coordination services	Public cloud storage has recently surged in popularity. However, cloud storage providers (CSPs) today offer fairly rigid services, which cannot be customized to meet individual users' needs. We propose a distributed, client-defined architecture that integrates multiple autonomous CSPs into one unified cloud and allows individual clients to specify their desired performance levels and share files. We design, implement, and deploy CYRUS (Client-defined privacY-protected Reliable cloUd Service), a practical system that realizes this architecture. CYRUS ensures user privacy and reliability by scattering files into smaller pieces across multiple CSPs, so that no one CSP can read users' data. We develop an algorithm that sets reliability and privacy parameters according to user needs and selects CSPs from which to download user data so as to minimize latency. To accommodate multiple autonomous clients, we allow clients to upload simultaneous file updates and detect conflicts after the fact from the client. We finally evaluate the performance of a CYRUS prototype that connects to four popular commercial CSPs in both lab testbeds and user trials, and discuss CYRUS's implications for the cloud storage market.	algorithm;autonomous robot;cloud storage;download;prototype;upload	Jae Yoon Chung;Carlee Joe-Wong;Sangtae Ha;James Won-Ki Hong;Mung Chiang	2015		10.1145/2741948.2741951	distributed algorithm;extensibility;computer science;operating system;database;world wide web;computer security	OS	-40.448112513853665	56.39725901426226	7558
2e7b17004881b622b1a3aae286c09dc2e0739cc0	data stream processing infrastructure for intelligent transport systems	intelligent systems costs application software intelligent transportation systems power system management large scale systems intelligent sensors energy management data analysis software reusability;intelligent transport system;system s;data stream;real time;automated highways;traffic management;satisfiability;data communication;traffic engineering computing automated highways data communication;modern transportation;software component;user requirements;traffic engineering computing;traffic management data stream processing infrastructure intelligent transport systems modern transportation system s fleet management center;stream processing;intelligent transport systems;data stream processing infrastructure;fleet management center	Intelligence Transportation Systems are critical to improve the efficiency of modern transportation. A system that is flexible and powerful enough to handle diverse demands from a large user base, is still elusive. Studies have shown that developing and integrating the various components constitute a significant portion of the capital cost and complexity of such systems. In this paper, we present a stream processing infrastructure we call System S. System S enables the deployment of large scale applications. It supports a mechanism for sharing data sources, software components, and even intermediate results allowing a reduction in the cost of software integration, and ownership. We experiment the stream processing infrastructure with a Fleet Management Center, and demonstrate how the infrastructure can be used to address unique issues in traffic management.	complexity;component-based software engineering;modular programming;programming paradigm;software deployment;stream processing;system integration	Eric Bouillet;Mark Feblowitz;Zhen Liu;Anand Ranganathan;Anton Riabov;Fan Ye;Schuman Shao;Don A. Schlosnagle	2007	2007 IEEE 66th Vehicular Technology Conference	10.1109/VETECF.2007.303	embedded system;intelligent transportation system;active traffic management;simulation;stream processing;computer science;engineering;user requirements document;component-based software engineering;operating system;transport engineering;satisfiability	HPC	-28.891489437260212	48.548332434132796	7573
48c8537d9c8bf117dee9317e32c929fc76fe22c6	an intelligent system dealing with complex nuanced information within a statistical context	linguistique;statistical data;systeme intelligent;habla;sistema inteligente;lenguaje;speech;langage;negation;informacion;linguistica;intelligent system;donnee statistique;negacion;parole;language;dato estadistico;information;linguistics	The main object of this paper is to propose an intelligent system dealing with affirmative or negative information. We do not refer to a logical negation but to a linguistic one. Moreover, not only atomic but also complex nuances can be denied. Among the inteded meanings of a linguistic negation, the choice is made by using the strength of the user negation and a preference principle which takes into account the answer simplicity.	artificial intelligence	Daniel Pacholczyk;Florence Bannay	2000		10.1007/3-540-39963-1_57	information;computer science;speech;artificial intelligence;negation;language;algorithm	HCI	-13.735364185009638	5.49159307676217	7591
5e28986709ee1176d5656362eb0421a47f4bed4d	eq-algebra-based fuzzy type theory and its extensions	type theory	In this paper, we introduce a new algebra called ‘EQ-algebra’, which is an alternative algebra of truth values for formal fuzzy logics. It is specified by replacing implication as the main operation with a fuzzy equality. Namely, EQ-algebra is a semilattice endowed with a binary operation of fuzzy equality and a binary operation of multiplication. Implication is derived from the fuzzy equality and it is not a residuation with respect to multiplication. Consequently, EQ-algebras overlap with residuated lattices but are not identical with them. We choose one class of suitable EQ-algebras (good EQ-algebras) and develop a formal theory of higher-order fuzzy logic called ‘basic fuzzy type theory’ (FTT). We develop in detail its syntax and semantics, and we prove some basic properties, including the completeness theorem with respect to generalized models. The paper also provides an overview of the present state of the art of FTT.	bl (logic);boolean algebra;commonsense reasoning;fuzzy logic;fuzzy set;linear algebra;logical connective;mv-algebra;natural language;particle filter;relational operator;remote manipulator;residuated lattice;turing completeness;type theory;vagueness	Vilém Novák	2011	Logic Journal of the IGPL	10.1093/jigpal/jzp087	possibility theory;computer science;fuzzy subalgebra;programming language;type theory	PL	-14.310335153555219	10.853471434670073	7595
0cb4ca02a90bd59fdd5f07d1d9c51a172c475e90	transformations of timed cooperating automata	timed automata;cooperating automata;retiming;stepwise development	The paper pursues the investigation of Timed Cooperating Automata (TCAs) by studying transformations which are suggested as means for stepwise TCA construction.	advanced telecommunications computing architecture;automaton;stepwise regression	Ruggero Lanotte;Andrea Maggiolo-Schettini;Simone Tini;Adriano Peron	2001	Fundam. Inform.		real-time computing;computer science;artificial intelligence;retiming;theoretical computer science;timed automaton	Logic	-35.287908718562576	31.67662206481322	7596
450bb27c3ba0b01cb26b22bd928de667eca3ccab	collaborative development environment in peer-to-peer networks	software;collaboration;maintenance engineering;graphical user interfaces;peer to peer computing;programming;real time systems	Collaboration is a huge part of modern software development. Yet most tools used in software development are aimed for single user instances. To support collaborative software practices, researchers and practitioners have presented several tools. The existing tools support collaboration either through distributed version control systems or through client/server-based concurrent text editors. Distributed version control systems do not support real-time collaboration; while the server-based concurrent editors do not support offline work. In this paper, we propose the design of a replica-based collaborative development environment (CDE) within a peer to peer network of users. The CDE supports both real-time collaborative editing and offline work. In addition, the CDE is scalable, resilient to the dynamic joining/leaving of collaborating users, and can be augmented into existing development environments as a plugin. We evaluate the usability of the CDE with respect to operation propagation time, and the correctness with respect to the causality, convergence and intention preservation (CCI) criteria.	causality;client–server model;collaborative development environment;collaborative real-time editor;collaborative software;control system;correctness (computer science);distributed version control;online and offline;peer-to-peer;plug-in (computing);propagation time;real-time clock;real-time computing;real-time transcription;scalability;server (computing);software development;software propagation;text editor;usability	Martin Thodi;Satoshi Fujita	2016	2016 Fourth International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2016.0106	computer science;database;distributed computing;world wide web	SE	-49.75975295140243	21.019341446277956	7606
30b759edc80afbd124f13df3523aaa4c1e63a192	a repository of semantic open ehr archetypes	tecnologias generalidades;tecnologias	— This paper describes a repository of openEHR archetypes that have been translated to OWL. In the work presented here, five different CKMs (Clinical Knowledge Managers) have been downloaded and the archetypes have been translated to OWL. This translation is based on an existing translator that has been improved to solve programming problems with certain structures. As part of the repository a tool has been developed to keep it always up-to-date. So, any change in one of the CKMs (addition, elimination or even change of an archetype) will involve translating the changed archetypes once more. The repository is accessible through a Web interface (http://www.openehr.es/).	web ontology language	Samuel Benavides;Fernando Moreno;Guillermo Garzón;María del Mar Roldán García;Ismael Navas Delgado;José Francisco Aldana Montes	2015	IJIMAI	10.9781/ijimai.2015.327	computer science;data science;data mining;world wide web	SE	-41.52263129942539	4.355956035783015	7617
63ccbdeeea222629a2f20a9c764405fe4f7974e9	object-oriented design of preconditioned iterative methods in diffpack	object oriented design;object oriented programming;iterative methods;preconditionting;iteration method;preconditioned iterative method	As modern programming methodologies migrate from computer science to scientific computing, developers of numerical software are faced with new possibilities and challenges. Based on experiences from an ongoing project that develops C11 software for the solution of partial differential equations, this article has its focus on object-oriented design of iterative solvers for linear systems of equations. Special attention is paid to possible conflicts that have to be resolved in order to achieve a very flexible, yet efficient, code.	c11 (c standard revision);computational science;computer science;iterative method;linear system;list of numerical analysis software;preconditioner	Are Magnus Bruaset;Hans Petter Langtangen	1997	ACM Trans. Math. Softw.	10.1145/244768.244776	mathematics;iterative method;programming language;algebra	PL	-9.565932895974305	36.0956751148122	7627
6bc564d3f337076cb9a5fdd46f749aa494482b90	benchmarking the resilience of self-adaptive software systems: perspectives and challenges	benchmarking;autonomic database management systems;self adaptive;software systems;resilience;adaptive system;database management system	Self-adaptive systems are widely recognized as the future of computer systems. Due to their dynamic and evolving nature, the characterization of self-adaptation and resilience attributes is of utmost importance, but also presents itself as a huge challenge. In fact, currently there is no practical way to characterize self-daptation capabilities, especially when comparing alternative systems concerning resilience. In this position paper we discuss the problem of resilience benchmarking of self-adaptive software systems. We identify a set of key challenges and propose a roadmap to tackle those challenges. At the same time, we present some perspectives on the development of such a benchmark, taking Autonomic Database Management Systems (ADBMS) as an illustrative case.	adaptive system;autonomic computing;benchmark (computing);computer;software system	Raquel Almeida;Marco Vieira	2011		10.1145/1988008.1988035	systems engineering;engineering;knowledge management;socio-ecological system;management science	DB	-61.46898495380473	19.657052298844604	7629
027642432f8725f4f2ab1e59c72b87c6e7b02968	verification of general markov decision processes by approximate similarity relations and policy refinement		In this work we introduce new approximate similarity relations that are shown to be key for policy (or control) synthesis over general Markov decision processes. The models of interest are discretetime Markov decision processes, endowed with uncountably-infinite state spaces and metric output (or observation) spaces. The new relations, underpinned by the use of metrics, allow in particular for a useful tradeoff between deviations over probability distributions on states, and distances between model outputs. We show that the new probabilistic similarity relations can be effectively employed over general Markov decision processes for verification purposes, and specifically for control refinement from abstract models.	approximation algorithm;markov chain;markov decision process;markov property;refinement (computing);temporal logic	Sofie Haesaert;Sadegh Esmaeil Zadeh Soudjani;Alessandro Abate	2016	SIAM J. Control and Optimization	10.1137/16M1079397	markov decision process;markov chain;mathematical optimization;markov kernel;discrete mathematics;partially observable markov decision process;machine learning;mathematics;markov model;statistics;variable-order markov model	Logic	-6.055536958401308	4.929987846166529	7639
3dab868b9952fa5077e8c105e8ca0103cec7ce8e	olap for xml data	query language;xml relational databases data analysis data models atomic measurements helium computer science database languages information representation information technology;xml data analysis data mining;gxaggregation;gxaggregation operator;complex structure;olap;relational database;data mining;xcube operator;data model;data analysis;internet;aggregation operator;information exchange;data mining olap xml information exchange internet data analysis gxaggregation operator xcube operator query language;xml;data analysis techniques;xcube;xcube xml olap gxaggregation	XML is an important standard of information exchange and representation on the Web. Analysis of data on the Web requires data analysis techniques of XML data. With tags representing semantics, XML data has more complex structure than relational database. Therefore, analysis of XML data can be more flexible. In this paper, concepts of XOLAP, OLAP of XML data, are presented. In order to supporting XOLAP, a general aggregation operator of XML, GXaggregation, is presented. Based on GXaggregation, XCube, an extension of tradition cube operator, is defined on XML data model. These operators can be embedded into XML query languages such as XQuery. In this paper, definitions and expressions operators related to XOLAP are presented, as well as a primary implementation of GXaggregation and XCube	data model;embedded system;information exchange;online analytical processing;query language;relational database;tree traversal;world wide web;xml tree;xquery	Hongzhi Wang;Jianzhong Li;Zhenying He;Hong Gao	2005	The Fifth International Conference on Computer and Information Technology (CIT'05)	10.1109/CIT.2005.146	xml catalog;xml validation;xml encryption;xml namespace;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document structure description;xml framework;soap;data mining;xml database;xml schema;database;xml signature;data analysis;xml schema editor;information retrieval;efficient xml interchange	DB	-33.269500503119446	7.109470000991149	7641
2baf1be469ee5fdc07606e6c67eccf758ac1edc1	scalcore: designing a core for voltage scalability	power aware computing multiprocessing systems pipeline processing;hpmode voltage scalability multicore system energy efficient execution modes dvfs low voltage core design high performance mode energy efficient mode scalcore core eemode logic structures storage structures storage intensive pipeline stage average program execution time reduction;pipelines multicore processing scalability transistors delays program processors registers	Upcoming multicores need to provide increasingly stringent energy-efficient execution modes. Currently, energy efficiency is attained by lowering the voltage (Vdd) through DVFS. However, the effectiveness of DVFS is limited: designing cores for low Vdd results in energy inefficiency at nominal Vdd. Our goal is to design a core for Voltage Scalability, i.e., one that can work in high-performance mode (HPMode) at nominal Vdd, and in a very energy-efficient mode (EEMode) at low Vdd. We call this core ScalCore. To operate energy-efficiently in EEMode, ScalCore introduces two ideas. First, since logic and storage structures scale differently with Vdd, ScalCore applies two low Vdds to the pipeline: one to the logic stages (Vlogic) and a higher one to storage-intensive stages. Secondly, ScalCore further increases the low Vdd of the storage-intensive stages (Vop), so that they are substantially faster than the logic ones. Then, it exploits the speed differential by either fusing storage-intensive pipeline stages or increasing the size of storage structures in the pipeline. Our simulations of 16 cores show that a design with ScalCores in EEMode is much more energy-efficient than one with conventional cores and aggressive DVFS: for approximately the same power, ScalCores reduce the average execution time of programs by 31%, the energy (E) consumed by 48%, and the ED product by 60%. In addition, dynamically switching between EEMode and HPMode based on program phases is very effective: it reduces the average execution time and ED product by a further 28% and 15%, respectively.	dynamic voltage scaling;run time (program lifecycle phase);scalability;simulation	Bhargava Gopireddy;Choungki Song;Josep Torrellas;Nam Sung Kim;Aditya Agrawal;Asit K. Mishra	2016	2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2016.7446104	computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-5.110104977154495	55.04695779485636	7645
5d98b372fd15f9b7b9d9633ae35fca06ac7d505a	an operational semantics and type safety prooffor multiple inheritance in c++	artefacto;developpement logiciel;modelizacion;teoria demonstracion;herencia;high order logic;semantica operacional;theorie preuve;compilateur;securite;proof theory;heritage;operational semantics;semantics;program transformation;program verification;transformation programme;compiler;modele multiple;artefact;logica orden superior;modelisation;semantic model;verificacion programa;transformacion programa;semantique operationnelle;object oriented;desarrollo logicial;multimodel;theory;software development;safety;multiple inheritance;oriente objet;logique ordre superieur;modelo multiple;c;inheritance;verification programme;seguridad;modeling;orientado objeto;languages;correctness proof;type safety;compilador	We present an operational semantics and type safety proof for multiple inheritance in C++. The semantics models the behaviour of method calls, field accesses, and two forms of casts in C++ class hierarchies exactly, and the type safety proof was formalized and machine-checked in Isabelle/HOL. Our semantics enables one, for the first time, to understand the behaviour of operations on C++ class hierarchies without referring to implementation-level artifacts such as virtual function tables. Moreover, it can - as the semantics is executable - act as a reference for compilers, and it can form the basis for more advanced correctness proofs of, e.g., automated program transformations. The paper presents the semantics and type safety proof, and a discussion of the many subtleties that we encountered in modeling the intricate multiple inheritance model of C++.	c++;class hierarchy;compiler;correctness (computer science);executable;hol (proof assistant);isabelle;multiple inheritance;operational semantics;program transformation;type safety	Daniel Wasserrab;Tobias Nipkow;Gregor Snelting;Frank Tip	2006		10.1145/1167473.1167503	semantic data model;multiple inheritance;compiler;run-time type information;c++;systems modeling;action semantics;type safety;computer science;artificial intelligence;software development;proof theory;semantics;programming language;object-oriented programming;operational semantics;theory;algorithm	PL	-23.665620166068752	29.324359864753404	7662
2f20027fec0df0056a8195465fd309013f65b689	symbolische bdd-basierte modellprüfung asynchroner nebenläufiger systeme		Today, information and communication systems are ubiquitous and consist very often of several interacting and communicating components. One reason is the widespread use of multi-core processors and the increasing amount of concurrent software for the efficient usage of multi-core processors. Also, the dissemination of distributed emergent technologies like sensor networks or the internet of things is growing. Additionally, a lot of internet protocols are client-server architectures with clients which execute computations in parallel and servers that can handle requests of several clients in parallel. Systems which consist of several interacting and communicating components are often very complex and due to their complexity also prone to errors. Errors in systems can have dramatic consequenses, especially in safety-critical areas where human life can be endangered by incorrect system behavior. Hence, it is inevitable to have methods that ensure the proper functioning of such systems. This thesis aims on improving the verifiability of asynchronous concurrent systems using symbolic model checking based on Binary Decision Diagrams (BDDs). An asynchronous concurrent system is a system that consists of several components, from which only one component can execute a transition at a time. Model checking is a formal verification technique. For a given system description and a set of desired properties, the validity of the properties for the system is decided in model checking automatically by software tools called model checkers. The main problem of model checking is the state-space explosion problem. One approach to reduce this problem is the use of symbolic model checking. There, system states and transitions are not stored explicitely as in explicit model checking. Instead, in symbolic model checking sets of states and sets of transitions are stored and also manipulated together. The data structure which is used in this thesis to store those sets are BDDs. BDD-based symbolic model checking has already been used successful in industry for several times. Nevertheless, BDD-based symbolic model checking still suffers from the state-space explosion problem and further improvements are necessary to improve its applicability. Central operations in BDD-based symbolic model checking are the computation of successor and predecessor states of a given set of states. Those computations are called image computations. They are applied repeatedly in BDD-based symbolic model checking to decide the validity of properties for a given system description. Hence, their efficient execution is crucial for the memory and runtime requirements of a model checker. In an image computation a BDD for a set of transitions and a BDD for a set of states are combined to compute a set of successor or predecessor states. Often, also the size of the BDDs to represent the transition relation ist critical for the successful use of model checking. To further improve the applicability of symbolic model checking, we present in this thesis new data structures to store the transition relation of asynchronous concurrent systems. Additionally, we present new image computation algorithms. Both can lead to large runtime and memory reductions for BDD-based symbolic model checking. Asynchronous concurrent systems often contain symmetries. A technique to exploit those symmetries to diminish the state-space explosion problem is symmetry reduction. In this thesis we also present a new efficient algorithm for symmetry reduction in BDD-based symbolic model checking.	algorithm;binary decision diagram;central processing unit;client–server model;computation;concurrency (computer science);data structure;emergence;formal verification;interaction;internet of things;internet protocol suite;model checking;multi-core processor;requirement;run time (program lifecycle phase);server (computing);state space	Christian Appold	2015				Logic	-12.78665283144749	27.819323233810017	7670
da88c86cb8c1c07a1b98f6a081616fd543742044	circuit complexity meets ontology-based data access		Ontology-based data access is an approach to organizing access to a database augmented with a logical theory. In this approach query answering proceeds through a reformulation of a given query into a new one which can be answered without any use of theory. Thus the problem reduces to the standard database setting. However, the size of the query may increase substantially during the reformulation. In this survey we review a recently developed framework on proving lower and upper bounds on the size of this reformulation by employing methods and results from Boolean circuit complexity.	boolean circuit;circuit complexity;data access;organizing (structure)	Vladimir V. Podolskii	2015		10.1007/978-3-319-20297-6_2	query optimization;computer science;theoretical computer science;data mining;database;mathematics;view;algorithm	AI	-25.248440579638036	7.680577248065704	7673
28307cf1225742a3e0eb5bf17b4352084b51383f	building media-rich cloud services from network-attached i/o devices	computer applications application virtualization;software engineering software architecture client server systems;distributed computing internet internet of things;software software systems;software system software operating systems	"""We present a system, called Cygnet, that makes it easy both to use and to implement media-rich interactive cloud services. Usage simplicity is achieved by providing a simple but powerful mechanism that makes an I/O device a first-class network-accessible resource that any user can dynamically """"bind"""" to any service at any time. Implementation simplicity is achieved by providing a development and runtime environment that is identical to that of a typical Linux distribution running on a local machine. In Cygnet the cloud is thus transparent to both developers and users, the latter of whom concern themselves simply with choosing the I/O devices via which they wish to interact with their services. The system supports all classes of services and is optimized for the difficult class that requires a GPU to render content in real time. Designing and implementing a cloud-transparent system for that class presents a number of significant technical challenges. We describe how we overcame those challenges and then evaluate the performance of the system's cloud-based GPU rendering."""	cloud computing;desktop computer;graphical user interface;graphics processing unit;input/output;linux;mathematical optimization;real-time transcription;remote desktop software;runtime system;sandbox (computer security);user space;virtual machine	Ilija Hadzic;Martin D. Carroll;Michael J. Coss;Hans C. Woithe	2016	2016 IEEE 4th International Conference on Future Internet of Things and Cloud (FiCloud)	10.1109/FiCloud.2016.14	embedded system;real-time computing;computer science;operating system;middleware;software as a service;distributed computing;software system	OS	-29.336291554594148	55.14211476586401	7678
36cddeb6d1e1daa2e1a50d7dcddad54eacc30808	re-trust: trustworthy execution of sw on remote untrusted platforms	software integration;cosic;software security;network connectivity	A major challenge in software security is preserving software integrity. Traditionally, this problem is addressed through the development of software (self-) checking techniques that verify the integrity of its code and execution. Unfortunately, no satisfactory solutions for run-time verification of software integrity have been presented. In this paper, we approach the problem of run-time software integrity verification in a networked context. That is, we present techniques to enable remote verification of the execution of software, given the availability of a continuous network connection between the verification entity and the untrusted execution platform.	adversary (cryptography);application security;authentication;bcrypt;continuous availability;entity;field-programmable gate array;microtransaction;shattered world;smart card;trusted platform module;trustworthy computing;yoram moses;yoram ofek	Brecht Wyseur	2009		10.1007/978-3-8348-9363-5_33	software security assurance;computer science;engineering;operating system;database;distributed computing;law;computer security;system integration	Security	-52.033702661622726	56.046847678584136	7685
45a2f4527ce97033d94282e3f652ae459a041c72	how to specify security services: a practical approach	commerce electronique;marco;comercio electronico;multimedia;standards;securite;asynchrone;implementation;specification;standard;service web;langage evolue;multi user;web service;secure communication;service utilisateur;message sequence chart;especificacion;security requirements;safety;norma;system development;lenguaje evolucionado;servicio usuario;user service;implementacion;etalon;seguridad;high level language;diagramme sequence message;norme;electronic trade;asincrono;asynchronous;security protocol	Security services are essential for ensuring secure communications. Typically no consideration is given to security requirements during the initial stages of system development. Security is only added latter as an afterthought in function of other factors such as the environment into which the system is to be inserted, legal requirements, and other kinds of constraints. In this work we introduce a methodology for the specification of security requirements intended to assist developers in the design, analysis, and implementation phases of protocol development. The methodology consists of an extension of the ITU-T standard requirements language MSC and HMSC, called SRSL, defined as a high level language for the specification of security protocols. In order to illustrate it and evaluate its power, we apply the new methodology to a real world example, the integration of an electronic notary system into a web-based multiusers service platform.	constraint (mathematics);cryptographic protocol;cryptography;high-level programming language;requirement;secure communication;software deployment;software system;unified framework;web application	Javier López;Juan J. Ortega;José Luis Vivas;José M. Troya	2003		10.1007/978-3-540-45184-6_13	computer security model;web service;cloud computing security;fabry–pérot interferometer;embedded system;secure communication;security through obscurity;security information and event management;security engineering;computer science;operating system;asynchronous communication;system requirements specification;database;security service;distributed computing;programming language;security testing;implementation;world wide web;high-level programming language;computer security;specification;message sequence chart	Security	-40.734638846894505	25.708904864200722	7686
0d98a29a709ec085daa849695d01bb38b8cfe73f	an unfolding-based loop optimization technique	unfolding;haute performance;behavioral analysis;deploiement;boucle programme;despliegue;optimizacion compiladora;bucle programa;program optimization;performance programme;indexation;analyse comportementale;compiler optimization;alto rendimiento;multithread;invariante;program loop;eficacia programa;analisis conductual;optimisation programme;code motion;multitâche;program performance;loop optimization;high performance;multitarea;optimisation compilateur;invariant;optimizacion programa	Loops in programs are the source of many optimizat ions for improving program performance, particularly on modern hig h-performance architectures as well as vector and multithreaded systems. Techniques such as loop invariant code motion, loop unrolling and loop peelin g have demonstrated their utility in compiler optimizations. However, many of these techniques can only be used in very limited cases when the loops are “w ell-structured” and easy to analyze. For instance, loop invariant code motion w orks only when invariant code is inside loops; loop unrolling and loop peeli ng work effectively when the array references are either constants or affine fun ctio s of index variable. It is our contention that there are many opportunities ov erl oked by limiting the optimizations to well structured loops. In many cases , ven “badly-structured” loops may be transformed into well structured loops . A a case in point, we show how some loop-dependent code can be transforme d into loop-invariant code by transforming the loops. Our technique descr ibed in this paper relies on unfolding the loop for several initial iterations s uch that more opportunities may be exposed for many other existing compiler opt imization techniques such as loop invariant code motion, loop peeling, loop u nrolling and so on.	algorithm;dependence analysis;erlang (programming language);instruction-level parallelism;iteration;linear function;loop invariant;loop optimization;loop splitting;loop unrolling;loop-invariant code motion;mathematical optimization;optimizing compiler;parallel computing;program optimization;program transformation;static program analysis;static single assignment form;thread (computing);transformers;unfolding (dsp implementation);vector graphics;ven (currency)	Litong Song;Krishna M. Kavi;Ron Cytron	2003		10.1007/978-3-540-39920-9_9	loop tiling;loop fusion;manifest expression;loop inversion;while loop;parallel computing;real-time computing;loop-invariant code motion;simulation;loop fission;conditional loop;loop interchange;loop dependence analysis;computer science;infinite loop;loop nest optimization;loop optimization;operating system;invariant;program optimization;loop unrolling;do while loop;optimizing compiler;loop counter;programming language;inner loop;loop splitting	PL	-18.263115488922168	34.831990908170376	7687
a65a4c8478939c5f062d32349223637abb3528e1	studying on aadl-based architecture abstraction of embedded software	software metrics;software complexity aadl based architecture abstraction embedded software system development component based development mda software maintenance software evolution large scale software cost reduction legacy system c source code reuse aadl specification model driven architecture architecture analysis design language;program diagnostics;aadl;software cost estimation;formal specification;software maintenance;component;component based development;object oriented programming;embedded system;embedded systems;large scale;software architecture;computer architecture embedded software software architecture software maintenance object oriented modeling large scale systems aerospace electronics embedded computing embedded system costs;vxworks;software reusability;source code;vxworks software architecture component reengineering aadl;legacy system;reengineering;object oriented languages;software reusability embedded systems formal specification object oriented languages object oriented programming program diagnostics software architecture software cost estimation software maintenance software metrics;embedded software;software evaluation	Since embedded software reaches a large scale and complexity, MDA and component-based development have become popular in embedded system development. Architecture-based software evaluation, maintenance and evolution can reduce cost of large-scale software, and raise the efficiency of maintenance and evolution. Especially, for legacy system, source code is regarded as the only dependable module to be reused in new systems. Thereby architecture abstraction from source code can describe the software at the level of architecture to support evaluation, maintenance and evolution. This paper studies on abstracting AADL model from C source code and introduces a set of mapping rules between the two languages. The mapping rules focus on identifying components and their relationships defined in AADL specification. An example is given at the end of the paper to demonstrate the algorithm.	algorithm;architecture analysis & design language;c++;complexity;component-based software engineering;diversification (finance);embedded software;embedded system;legacy system;operating system;software architecture;software development;software release life cycle;unified modeling language;vxworks	Geng Wang;Xingshe Zhou;Yunwei Dong;Hong-bing Zhao	2009	2009 International Conference on Scalable Computing and Communications; Eighth International Conference on Embedded Computing	10.1109/EmbeddedCom-ScalCom.2009.13	reference architecture;software visualization;software architecture;computer architecture;real-time computing;architecture analysis & design language;computer science;backporting;software framework;software development;software construction;software architecture description;programming language;resource-oriented architecture;software quality;systems architecture;avionics software	SE	-53.0063967360735	29.00602664628925	7694
b89a0c99cddb65e94b7772910184ca9fba575134	garantir la complétude des cartes de patrons à l'aide de méta-patrons		The pattern notion defines techniques allowing the existing knowledge reuse. A pattern solves a specific problem of a software system life cycle. The knowledge encapsulated in these patterns is generally stored in classic library repositories that quickly become overcrowded. As a result, [DEN 01B] proposes the use of process maps in order to help the method engineer to sort and select them. But the completeness of the maps are a very important problem that has to be solved in order to offer a useful guidance to the method engineer. This paper deals with this problem with a pattern construction technique guiding engineers when creating the maps. MOTS CLEFS : Ingénierie des méthodes, méthode situationnelle, extension de méthode, construction de patron, carte de processus, méta-patron.	linear algebra;map;software engineer;software system;system lifecycle	Rébecca Deneckère	2002				Logic	-43.3513873469973	25.016083714096133	7697
153e3cc9b95a3afe3c0185d5b6b5f12ddce24aa7	storing and managing data in a distributed hash table	electrical engineering and computer science;thesis	Distributed hash tables (DHTs) have been proposed as a generic, robust storage infrastructure for simplifying the construction of large-scale, wide-area applications. For example, UsenetDHT is a new design for Usenet News developed in this thesis that uses a DHT to cooperatively deliver Usenet articles: the DHT allows a set of N hosts to share storage of Usenet articles, reducing their combined storage requirements by a factor of O(N). Usenet generates a continuous stream of writes that exceeds 1 Tbyte/day in volume, comprising over ten million writes. Supporting this and the associated read workload requires a DHT engineered for durability and efficiency. Recovering from network and machine failures efficiently poses a challenge for DHT replication maintenance algorithms that provide durability. To avoid losing the last replica, replica maintenance must create additional replicas when failures are detected. However, creating replicas after every failure stresses network and storage resources unnecessarily. Tracking the location of every replica of every object would allow a replica maintenance algorithm to create replicas only when necessary, but when storing terabytes of data, such tracking is difficult to perform accurately and efficiently. This thesis describes a new algorithm, Passing Tone, that maintains durability efficiently, in a completely decentralized manner, despite transient and permanent failures. Passing Tone nodes make replication decisions with just basic DHT routing state, without maintaining state about the number or location of extant replicas and without responding to every transient failure with a new replica. Passing Tone is implemented in a revised version of DHash, optimized for both disk and network performance. A sample 12 node deployment of Passing Tone and UsenetDHT supports a partial Usenet feed of 2.5 Mbyte/s (processing over 80 Tbyte of data per year), while providing 30 Mbyte/s of read throughput, limited currently by disk seeks. This deployment is the first public DHT to store terabytes of data. These results indicate that DHT-based designs can successfully simplify the construction of large-scale, wide-area systems. Thesis Supervisor: M. Frans Kaashoek Title: Professor Thesis Supervisor: Robert T. Morris Title: Associate Professor Previously Published Material Portions of this thesis are versions of material that were originally published in the following publications: SIT, E., DABEK, F., AND ROBERTSON, J. UsenetDHT: A low overhead Usenet server. In Proc. of the 3rd International Workshop on Peer-to-Peer Systems (Feb. 2004). CHUN, B.-G., DABEK, F., HAEBERLEN, A., SIT, E., WEATHERSPOON, H., KAASHOEK, F., KUBIATOWICZ, J., AND MORRIS, R. Efficient replica maintenance for distributed storage systems. In Proc. of the 3rd Symposium on Networked Systems Design and Implementation (May 2006). SIT, E., MORRIS, R., AND KAASHOEK, M. F. UsenetDHT: A low overhead design for Usenet. In Proc. of the 5th Symposium on Networked Systems Design and Implementation (Apr. 2008).	algorithm;clustered file system;distributed hash table;durability (database systems);megabyte;network performance;overhead (computing);peer-to-peer;r language;requirement;routing;server (computing);software deployment;terabyte;throughput;usenet	Emil Sit	2008			real-time computing;computer science;database;distributed computing	Networks	-20.40139863601071	49.50306398730957	7707
a3ba4850e98c1e6ac3576bc3795e82b65976a029	identifying poorly documented object oriented software components	kohonen self organizing mapping;software maintenance;object oriented software;software engineering;cluster analysis;data envelopment analysis	Recent research suggests that software testing consumes substantial resources and documenting source code is one way to reduce software maintenance cost. The software engineering literature provides little information on how to establish whether a software component is adequately documented. It appears that benchmarking can be used to identify a software component with little or no documentation. In this paper, we propose and illustrate how a hybrid Data Envelopment Analysis (DEA), Kohonen self-organizing mapping, and Euclidean distance based algorithm can be used to identify poorly documented software components.	component-based software engineering	Parag C. Pendharkar	2009	Int. J. Hybrid Intell. Syst.	10.3233/HIS-2009-0083	verification and validation;software sizing;computer science;package development process;backporting;software framework;component-based software engineering;software development;software construction;data mining;data envelopment analysis;database;cluster analysis;software walkthrough;software analytics;resource-oriented architecture;software maintenance;software deployment;software quality;software metric;software system	SE	-59.4282407927757	29.302628542203344	7708
60f4ca095a5c576682e3c516132f9d66bd3b2f3e	research in attacks, intrusions, and defenses		A cloud customer’s inability to verifiably trust an infrastructure provider with the security of its data inhibits adoption of cloud computing. Customers could establish trust with secure runtime integrity measurements of their virtual machines (VMs). The runtime state of a VM, captured via a snapshot, is used for integrity measurement, migration, malware detection, correctness validation, and other purposes. However, commodity virtualized environments operate the snapshot service from a privileged VM. In public cloud environments, a compromised privileged VM or its potentially malicious administrators can easily subvert the integrity of a customer VMs snapshot. To this end, we present HyperShot, a hypervisor-based system that captures VM snapshots whose integrity cannot be compromised by a rogue privileged VM or its administrators. HyperShot additionally generates trusted snapshots of the privileged VM itself, thus contributing to the increased security and trustworthiness of the entire cloud infrastructure.	cloud computing;correctness (computer science);hypervisor;malware;rogue;snapshot (computer storage);trust (emotion);virtual machine;z/vm	Josef Kittler	2012		10.1007/978-3-642-33338-5		Security	-51.96284248251762	57.66348551179412	7710
686273e9c83aa7fff207f16b060a35727078d47d	composing models	action model update;agent observational power;dynamic epistemic logic;epistemic model checking;interpreted systems;model composition;multi-agent models;reduction techniques	We study a new composition operation on (epistemic) multiagent models and update actions that takes vocabulary extensions into account. • This operation allows to represent partial observational information about a large model in a small model, where the small models can be viewed as representations of the observational power of agents, and about their powers for changing the facts of the world. • Our investigation provides ways to check relevant epistemic properties on small components of large models, and our approach generalizes the use of ‘locally generated models’. Overview: Three Simple Messages • Models can be made small by vocabulary restriction • Composing restricted models is easy • Compositions of restricted models are useful Note: an expanded version of this LOFT paper can be found in Chapter 5 of the PhD Thesis of Yanjing Wang, Epistemic Modelling and Protocol Dynamics, to be defended in September 2010 (available upon request from the author). Multi-agent Models with Different Vocabularies Fix a set of proposition letters P. Call a subset of P a vocabulary. Consider multi-agent models with vocabularies Q taken from P. Call such models restricted models. This allows us to refine ‘knowledge about the world’ to ‘knowledge about Q’. Knowing Nothing About Anything The restricted model E for knowing nothing about anything:	agent-based model;multi-agent system;vocabulary	Jan van Eijck;Floor Sietsma;Yanjing Wang	2011	Journal of Applied Non-Classical Logics	10.3166/jancl.21.397-425		AI	-23.444642203499246	14.653967750853203	7718
22ce1e8f308c8aff2c78ae1c124132bbc1ce5002	symbolic model checking with fewer fixpoint computations	state transition system;diagrama binaria decision;systeme intelligent;diagramme binaire decision;formal specification;sistema informatico;sistema inteligente;computer system;specification formelle;especificacion formal;computation tree logic;formal verification;decision procedure;intelligent system;verification formelle;systeme informatique;symbolic model checking;binary decision diagram	Symbolic model checking, smc, is a decision procedure that verifies that some finite-state structure is a model for a formula of Computation Tree Logic (CTL). smc is based on fixpoint computations. Unfortunately, as the size of a structure grows exponentially with the number of state components, smc is not always powerful enough to handle realistic problems.#R##N##R##N#We first show that a subset of CTL formulas can be checked by testing simple sufficient conditions, that do not require any fixpoint computation. Based on these observations, we identify a second, larger, subset of CTL that can by verified with fewer fixpoint computations than smc.#R##N##R##N#We propose a model checking algorithm for CTL that tests the identified sufficient conditions whenever possible and falls back to smc otherwise. In the best (resp. worst) case, the complexity of this algorithm is exponentially better (resp. the same) in terms of state components than that of smc.	computation;fixed point (mathematics);model checking	David Déharbe;Anamaria Martins Moreira	1999		10.1007/3-540-48119-2_17	discrete mathematics;formal verification;computation tree logic;computer science;artificial intelligence;theoretical computer science;machine learning;formal specification;database;mathematics;distributed computing;programming language;binary decision diagram;algorithm	Logic	-13.214992102415502	27.24920726177337	7734
70c871786612fdea5684c053f9f3c2938ecd7e85	planning the execution of task groups in real-time systems	unifi;verification;reliability;fault tolerant;real time;real time transactions;resource management;satisfiability;strategic planning;firenze;dependable systems;bonding;fault tolerant computing;time factors;redundancy;affidabili;distributed environment;ricerca;resilient computing lab;dependability requirements;fault tolerance;dependability;task groups;end to end constraints;validation;rcl;affidabilita;florence;sistemi;real time application;assessment;fault tolerance task groups real time systems real time transactions distributed environment end to end constraints dependability requirements fault tolerant real time applications;real time systems fault tolerance redundancy bonding strategic planning time factors resource management;fault tolerant real time applications;real time systems	Many real time applications are designed such that not simply individual tasks but also groups thereof are specified and must be executed satisfying a common real time requirement. Examples are real time transactions executed in a distributed environment, applications with end-to-end constraints and real-time applications with some dependability requirements in which redundancy is introduced for fault tolerance purposes. This paper investigates on the problem of planning groups of tasks in real-time environments. It first discusses the issues related to the design choices and their implications on planning strategies for tasks groups. Then an algorithm for planning groups of tasks is proposed for the specific case of fault tolerant real-time applications where fault tolerance is realised by groups of tasks forming together fault tolerant structures.	algorithm;dependability;end-to-end principle;fault tolerance;real-time clock;real-time computing;real-time locating system;real-time operating system;real-time transcription;requirement	Paolo Bizzarri;Andrea Bondavalli;Felicita Di Giandomenico;Fabio Tarini	1996		10.1109/EMWRTS.1996.557850	fault tolerance;real-time computing;strategic planning;computer science;resource management;operating system;distributed computing	Embedded	-36.38722698704152	38.514526241144374	7748
f484797e86d9dd1fffaa162fe1d7910276668d19	adaptive service composition based on runtime verification of formal properties	service composition;runtime verification;conference paper;adaptation;formalisms	Service-Oriented Computing (SOC) has been used in business environments in order to integrate heterogeneous systems. The dynamic nature of these environments causes changes in the application requirements. As a result, service composition must be flexible, dynamic and adaptive, which motivate the need to ensure the service composition behavior at runtime. The development of adaptive service compositions is still an opportunity due to the complexity of dealing with adaptation issues, for example, how to provide runtime verification and automatic adaptation. Formal description techniques can be used to detect runtime undesirable behaviors that help in adaptation process. However, formal techniques have been used only at design-time. In this paper, we propose an adaptive service composition approach based on the lightweight use of formal methods. The aim is detecting undesirable behaviors in the execution trace. Once an undesirable behavior is detected during the execution of a service composition, our approach triggers an adequate reconfiguration plan for the problem at runtime. In order to evaluate the effectiveness of the proposal, we illustrate it with a running example. Keywords-service composition; formalisms; runtime verification; adaptation	computer performance;formal methods;functional requirement;requirement;run time (program lifecycle phase);runtime verification;sensor;service-oriented software engineering	Glaucia Melissa Medeiros Campos;Nelson Souto Rosa;Luís Ferreira Pires	2017			real-time computing;computer science;distributed computing;runtime verification;programming language;adaptation	SE	-41.83777875316152	37.88646261046491	7753
d8734fcecd5f99187d8529081fd89034a1dffc6e	a graphics-based programming-support system	computer program;program representation;computer graphic;support system;structured programming;system development;nassi sheiderman diagrams;interactive computer graphics	A programming support system using extended Nassi-Shneiderman diagrams (NSD) is described. The aim of the work is to develop techniques for improving the quality and cost of specifying, documenting and producing computer programs. NSD's can be executed interpretively or compiled to produce running code. The system implementation has begun and charts can be drawn on a variety of display devices. The system is being developed using the Picture Building System developed earlier.	chart;compiler;computer program;display device;graphics;nassi–shneiderman diagram;software documentation	H. P. Frei;Daniel L. Weller;Robin Williams	1978		10.1145/800248.807368	simulation;computer science;programming language;structured programming;computer graphics (images)	Graphics	-32.46663813273471	24.73348019133233	7756
628c580d5442aea9d22767b1273df181fd7ce583	control and optimization of the srpt service policy by frequency scaling		In this paper, we study a system where the speed of a processor depends on the current number of jobs. We propose a queueing model in which jobs consist of a variable number of tasks, and priority is given to the job with the fewest remaining tasks. The number of processor frequency levels determines the dimensionality of the queueing process. The objective is to evaluate the trade-offs between holding cost and energy cost when setting the processor frequency. We obtain exact results for two and three frequency levels, and accurate approximations that can be further generalized. Numerical and simulation results show the high accuracy of the approximate solutions that we propose. Our experiments suggest that a parsimonius model with only two frequency levels is sufficient, since more elaborate models provide negligible improvements when optimizing the system.	approximation algorithm;average-case complexity;clock rate;complex systems;experiment;frequency scaling;image scaling;job stream;loss function;numerical method;quality of service;queueing theory;response time (technology);scheduling (computing);shortest remaining time;simulation	Andrea Marin;Isi Mitrani;B. Maryam Elahi;Carey L. Williamson	2018		10.1007/978-3-319-99154-2_16	theoretical computer science;computer science;mathematical optimization;frequency scaling;holding cost;curse of dimensionality;queueing theory	Metrics	-5.09666034315276	59.66935890960755	7763
737caffc29de5b54afd6dc7c838a7e24b3a140a9	a reliable, safe, and secure run-time platform for cyber physical systems	virtualization;safety critical networked applications reliable run time platform safe run time platform secure run time platform global research collaboration project korean usa research group reliable cyber physical systems safe cyber physical systems secure cyber physical systems cps soc design virtualized software architecture upgradable systems middleware architecture;cyber physical systems;software architecture;system on chip;system on chip cyber physical systems virtualization run time platform;safety critical software;middleware;system on chip middleware safety critical software software architecture;reliability system on chip real time systems virtual machine monitors monitoring educational institutions virtualization;run time platform	This paper introduces a global research collaboration project performed by a Korean-USA research group. The project aims at designing and implementing a run-time platform for reliable, safe, and secure cyber physical systems (CPS). The project consists of layered sub-projects including SoC design for reliable systems, virtualized software architecture for dynamically upgradable systems, and middleware architecture for safety critical networked applications. This paper describes the objectives of each sub-project and the current accomplishments.	cyber-physical system;middleware;software architecture;system on a chip	Sung-Soo Lim;Eun-Jin Im;Nikil D. Dutt;Kyung-Woo Lee;Insik Shin;Chang-Gun Lee;Insup Lee	2013	2013 IEEE 6th International Conference on Service-Oriented Computing and Applications	10.1109/SOCA.2013.65	system on a chip;embedded system;software architecture;real-time computing;virtualization;computer science;operating system;middleware;cyber-physical system	Embedded	-33.33598475563453	37.78679860963116	7765
3a7f671238a01ba5210db835e5e92400b8753ba9	software cartography: thematic software visualization with consistent layout	latent semantic analysis;latent semantic indexing;software systems;software evolution;software visualization;vector space;two dimensions;multidimensional scaling	Software visualizations can provide a concise overview of a complex software system. Unfortunately, as software has no physical shape, there is no ‘natural’ mapping of software to a two-dimensional space. As a consequence most visualizations tend to use a layout in which position and distance have no meaning, and consequently layout typically diverges from one visualization to another. We propose an approach to consistent layout for software visualization, called Software Cartography, in which the position of a software artifact reflects its vocabulary, and distance corresponds to similarity of vocabulary. We use Latent Semantic Indexing (LSI) to map software artifacts to a vector space, and then use Multidimensional Scaling (MDS) to map this vector space down to two dimensions. The resulting consistent layout allows us to develop a variety of thematic software maps that express very different aspects of software while making it easy to compare them. The approach is especially suitable for comparing views of evolving software, as the vocabulary of software artifacts tends to be stable over time. We present a prototype implementation of Software Cartography, and illustrate its use with practical examples from numerous open-source case studies. Copyright © 2010 John Wiley & Sons, Ltd.	algorithm;artifact (software development);awesome;cartography;contour line;entity;john d. wiley;multi-user;multidimensional scaling;open-source software;prototype;reverse engineering;shading;software map;software system;software visualization;thematic map;usability testing;vasa (ship);vocabulary	Adrian Kuhn;David Erni;Peter Loretan;Oscar Nierstrasz	2010	Journal of Software Maintenance	10.1002/smr.414	software visualization;two-dimensional space;latent semantic indexing;software sizing;latent semantic analysis;multidimensional scaling;vector space;computer science;engineering;software evolution;software design;theoretical computer science;software framework;component-based software engineering;software development;software design description;software engineering;software construction;database;programming language;software analytics;software quality;software metric;software system	SE	-55.410495832398645	34.553312892357546	7767
3ad9091c55314da25819e0d128f797dacb1ccb45	gryphon: an information flow based approach to message brokering	cluster computing;distributed computing;information flow;communication technology	"""Gryphon is a distributed computing paradigm for message brokering , which is the transferring of information in the form of streams of events from information providers to information consumers. This abstract outlines the major problems in message brokering and Gryphon's approach to solving them. In Gryphon, the flow of streams of events is described via an information flow graph. The information flow graph specifies the selective delivery of events, the transformation of events, and the generation of derived events as a function of states computed from event histories. For this, Gryphon derives from and integrates the best features of distributed communications technology and database technology. Message brokering is motivated by the need for efficient delivery of information across a large number of users and applications, in an environment characterized by heterogeneity of computing platforms, anonymity between information producers and consumers, and dynamic change due to system evolution. Within a single business, such as a stock exchange or a weather forecasting agency, there is a dynamically varying number of sub-applications supplying events, and a varying number consuming events. The suppliers and consumers may not necessarily be aware of one another; instead the suppliers may simply be supplying information of a certain type to any interested consumer and each consumer may be interested in subsets of this information having particular properties. For example, in a stock exchange, one consumer may be interested in all stock trades greater than 1000 shares, and another in specific market trends, such as all stock trades representing a drop of more than 10 points from the previous day's high. There is also a growing need to """"glue"""" together applications within multiple businesses, to support interbusiness network commerce or maybe as a result of mergers and acquisitions. For example, a retailer may need to connect to its suppliers and customers, or a customer to various retailers and financial organizations. This may require transforming events from different sources into a compatible form, merging them, and selecting from these events. Message brokering is an extension of publish-subscribe technology [Powell96]. The Gryphon approach augments the publish-subscribe paradigm with the following features: • Content-based subscription, in which events are selected by predicates on their content rather than by pre-assigned subject categories; • Event transformations , which convert events by projecting and applying functions to data in events; • Event stream interpretation , which allows sequences of events to be collapsed to a state and/or expanded back to a new sequence of events; and • Reflection, which allows system management through meta-events . Gryphon technology includes a collection of efficient implementations to support this paradigm and still provide scalability, high throughput and low latency."""	distributed computing;gryphon software morph;information flow;programming paradigm;publish–subscribe pattern;scalability;systems management;throughput	Robert E. Strom;Guruduth Banavar;Tushar Deepak Chandra;Marc A. Kaplan;Kevan Miller;Bodhi Mukherjee;Daniel C. Sturman;Michael Ward	1998	CoRR		information and communications technology;information flow;computer cluster;computer science;theoretical computer science;database;distributed computing	DB	-34.94613901772034	15.604121885829104	7771
16aee629f93020dc996ade66d5fe455a3b813396	teaching inheritance concepts with java	java programming;pedagogy;object oriented programming;experience report;dynamic binding;code reuse;polymorphism;software development;design rationale	In teaching object-oriented programming, teaching inheritance is the most challenging and at the same time the most crucial aspect. The interplay of dynamic binding, late-bound self-reference, subtype polymorphism and method redefinition is so complex that it is difficult for instructors to design a gentle, step-by-step introduction. Should polymorphism be introduced first? Or is code reuse better suited as an introductory motivation? The Java Programming Language adds a further aspect to this discussion: when should named interfaces be introduced? Most textbooks follow the historical development of the mechanism and cover interfaces after the discussion of abstract classes. In this paper a different approach is described: interfaces are introduced long before and in isolation from inheritance; and the discussion of inheritance is explicitly split into its two major constituents, namely subtype polymorphism and implementation inheritance. We applied this novel approach in the two introductory courses on software development (SD1 and SD2) in the newly created Bachelor of Science in Informatics curriculum at the University of Hamburg, Germany. This experience report reflects on the design rationale behind this new approach.	code reuse;design rationale;dynamic dispatch;informatics;java;late binding;multiple inheritance;name binding;programming language;self-reference;software development	Axel Schmolitzky	2006		10.1145/1168054.1168084	polymorphism;computer science;software development;programming language;object-oriented programming;design rationale	PL	-29.371999878431254	24.226792298019824	7779
575e1b8366cc1e94094906a1a9f8994d14ea93ba	reconsidering complex branch predictors	performance evaluation;delay hardware clocks microarchitecture pipelines arithmetic computer science throughput microprocessors random access memory;chip;parallel architectures;pipeline processing parallel architectures performance evaluation delays;predictor latency complex branch predictors instruction throughput rates aggressively clocked microarchitectures performance latency hiding ipc pipelines;pipeline processing;delays	To sustain instruction throughput rates in more aggressively clocked microarchitectures, microarchitects have incorporated larger and more complex branch predictors into their designs, taking advantage of the increasing numbers of transistors available on a chip. Unfortunately, because of penalties associated with their implementations, the extra accuracy provided by many branch predictors does not produce a proportionate increase in performance. Specifically, we show that the techniques used to hide the latency of a large and complex branch predictor do not scale well and will be unable to sustain IPC for deeper pipelines. We investigate a different way to build large branch predictors. We propose an alternative predictor design that completely hides predictor latency so that accuracy and hardware budget are the only factors that affect the efficiency of the predictor. Our simple design allows the predictor to be pipelined efficiently by avoiding difficulties introduced by complex predictors. Because this predictor eliminates the penalties associated with complex predictors, overall performance exceeds that of even the most accurate known branch predictors in the literature at large hardware budgets. We conclude that as chip densities increase in the next several years, the accuracy of complex branch predictors must be weighed against the performance benefits of simple branch predictors.	branch predictor;clock rate;kerrison predictor;pipeline (computing);throughput;transistor	Daniel A. Jiménez	2003		10.1109/HPCA.2003.1183523	chip;parallel computing;real-time computing;computer hardware;telecommunications;computer science;operating system	Arch	-7.592586616614936	52.05406990948069	7789
26850b3658253b0cf491c2bc49f17da4f2e6437f	reduction strategies for declarative programming	declarative programming;efficient implementation;declarative languages;functional logic programming	This paper surveys reduction or evaluation strategies for functional and functional logic programs. Reasonable reduction strategies for declarative languages must be efficiently implementable to be useful in practice. On the other hand, they should also support the programmers to write programs in a declarative way ignoring the influence of the evaluation strategy to the success of a computation as good as possible. We review existing reduction strategies along these lines and discuss some aspects for further investigation.	computation;constructor (object-oriented programming);declarative programming;programmer;programming style;reduction strategy (lambda calculus);rewrite (programming)	Michael Hanus	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)00273-7	constraint programming;declarative programming;reactive programming;computer science;theoretical computer science;functional logic programming;programming paradigm;procedural programming;ontology language;inductive programming;datalog;fifth-generation programming language;programming language;logic programming;second-generation programming language;comparison of multi-paradigm programming languages;algorithm	PL	-20.384861401771712	23.08332908010186	7797
35839bc0b6eceb5be8c8ac153e09f76b17ed479d	processor—time tradeoffs under bounded-speed message propagation: part ii, lower bounds		Lower bounds are developed for the processor—time tradeoffs of machines such as linear arrays and two-dimensional meshes, which are compatible with the physical limitation on propagation speed of messages. The machines we consider are characterized by the property that identical numbers of simultaneous memory accesses, operations, and message communications are expressed by the number of processors. For this class of machines, a class of computations is analyzed for which parallelism and locality combined yield speedups superlinear in the number of processors. The results are obtained by means of a novel technique, called the ``closed-dichotomy-size technique,'' designed to obtain lower bounds to the computation time for networks of processors, each of which is equipped with a local hierarchical memory.	central processing unit;computation;locality of reference;parallel computing;software propagation;time complexity	Gianfranco Bilardi;Franco P. Preparata	1997	Theory of Computing Systems	10.1007/s002240000131		Theory	-9.7516925792408	44.85681361256359	7802
d2d4cc7b1771260cddd6ddb193de94215b38607d	specification and testing of abstract data types	software testing;axiomatic specifications;abstract data types;completeness of axioms;abstract data type	-Specifications are means to formally define the behavior of a software system or a module, and form the basis for testing an implementation. Axiomatic specification technique is one of the methods to formally specify an abstract data type (ADT). In this paper we describe a system called SITE, that is capable of automatically testing the completeness of an ADT specification. In addition, an implementation of the ADT, can also be tested, with SITE providing the test oracle and a number of testcases. For achieving these two goals, the system generates an implementation of the specifications and a set of testcases from the given specifications. For testing the completeness of specifications, the generated implementation is tested with the generated testcases. For testing a given implementation, the generated and given implementations are executed with the generated testcases and the results compared. One way to utilize the system effectively is to first test the specifications and modify them until they are complete, and then use the specifications as a basis for prototyping and testing. The system has been implemented on a Sun workstation. Abstract data types Axiomatic specifications Completeness of axioms Software testingdata types Axiomatic specifications Completeness of axioms Software testing 1. I N T R O D U C T I O N Software specifications form a critical component in developing reliable software. Specifications provide the means to properly communicate the functionality of the system to be developed, and form the basis for testing the software. Correct, complete and unambiguous specifications are essential for developing error-free software. Formal specifications have been proposed as a means for making the communicat ion about the functionality of the system precise, and less prone to errors due to mis-interpretations [1]. An important use of specifications is during the testing of software developed to implement the specifications. During testing, the consistency between the behavior of actual software and the specification is checked for the given testcases. With formal specifications, a desirable goal is to automate, as much as possible, the testing of software against its specifications. However, formal specifications need not always be complete. That is, the specifications, though formally stated, may not completely specify the behavior of the software. Incompleteness provides room for misinterpretations, and is likely to cause problems when the software is tested against the specifications. Hence, another desirable goal with formal specifications is to test the completeness of specifications themselves. It has been argued that testing specifications early is quite important of reducing the cost of software construction [2], as detecting problems in the specifications later may result in considerable amount of wasted effort. In this paper, we focus on abstract data types (ADTs). An A D T supports data abstraction, and comprises of a group of related operations that act upon a particular class of objects, with the constraint that the behavior of the objects can only be observed by applications of the operations [1]. Da ta abstraction is extremely useful for information hiding and supporting high level abstraction. Many languages like Simula [3], CLU [4], Euclid [5], and Ada [6] support data abstraction.ion. Many languages like Simula [3], CLU [4], Euclid [5], and Ada [6] support data abstraction. Abstract data types have frequently been used for formal specifications, and various specification techniques have evolved for specifying them [1]. One of the methods for specifying ADTs is thedata types have frequently been used for formal specifications, and various specification techniques have evolved for specifying them [1]. One of the methods for specifying ADTs is the axiomatic specification technique [7-11]. Axiomatic specification techniques employ axioms to specify the behavior of the operations of the ADT. In this paper we describe the Specification and Implementat ion TEsting (SITE) system. There are two major goals of the system. First, to automatically test the completeness of a given set of	abstract data type;abstraction (software engineering);ada;clu;euclid;formal specification;high-level programming language;oracle (software testing);sun workstation;sensor;simula;software construction;software prototyping;software system;software testing	Pankaj Jalote	1992	Comput. Lang.	10.1016/0096-0551(92)90024-H	computer science;functional testing;programming language;abstract data type;algorithm	SE	-46.42141218135674	29.778585215373976	7803
183f663ffbb328e2ccacdf3f1fd9e5b83e056ef9	tql: a query language to support traceability	software;query language;program diagnostics;kernel;trace query language;xml program diagnostics query languages;multiple traceability link type;database languages xml safety testing linux kernel joining processes conferences;software artifact xml multiple traceability link type trace query language;testing;data mining;query languages;software artifact;evolution biology;safety;unified modeling language;xml;joining processes;linux;database languages;conferences	A query language for traceability is proposed and presented. The language, TQL, is based in XML and supports queries across multiple artifacts and multiple traceability link types. A number of primitives are defined to allow complex queries to be constructed and executed. Example queries are presented in the context of traceability questions. The technical details of the language and issues of implementation are discussed.	query language;traceability;xml	Jonathan I. Maletic;Michael L. Collard	2009	2009 ICSE Workshop on Traceability in Emerging Forms of Software Engineering	10.1109/TEFSE.2009.5069577	computer science;data mining;database;programming language	SE	-54.05647804535463	34.371040697051185	7805
07e3c551aa9a1ea294106d248a6709efbceb5c69	visual formalisms revisited	directed graphs;formal specification;interactive application development;application software;model system;statecharts;distributed computing;room;object oriented programming;software engineering;computational linguistics interactive systems formal specification visual programming directed graphs object oriented programming real time systems;telephony;visual specifications;visual programming;computer architecture;compositional semantics;interactive application;real time object oriented modeling;directed graph;unified modeling language;statecharts visual formalisms interactive application development software expert software engineering methods visual specifications directed graphs denotational model hierarchical graphs behavior aspects real time object oriented modeling room compositional semantics or states;denotational model;or states;informatics;computational linguistics;hierarchical graphs;power system modeling;switches;behavior aspects;interactive systems;software expert;visual formalisms;switches application software computer architecture telephony software engineering unified modeling language power system modeling informatics hardware distributed computing;software engineering methods;hardware;real time systems	The development of an interactive application is a complex task that has to consider data behavior inter communication architecture and distribution aspects of the modeled system In particular it presupposes the successful communication between the customer and the software expert To enhance this communica tion most modern software engineering methods rec ommend to specify the di erent aspects of a system by visual formalisms In essence visual speci cations are directed graphs that are interpreted in a particular way for each as pect of the system They are also intended to be com positional This means that each node can itself be a graph with a separate meaning However the lack of a denotational model for hierarchical graphs often leads to the loss of compositionality This has severe negative consequences in the development of realistic applications In this paper we present a simple denotational model which is by de nition compositional for the architecture and behavior aspects of a system This model is then used to give a semantics to almost all the concepts occurring in ROOM Our model also provides a compositional semantics for or states in statecharts Introduction Recent advances in telecommunication and hardware technology made distributed interactive applications into an important domain of concern of software con struction The development of an interactive applica tion is however a complex task that has to consider data behavior intercommunication architecture and distribution aspects of the modeled system In par ticular it presupposes the successful communication between the customer and the software expert To enhance this communication most modern soft ware engineering methods such as Rhapsody This research was partially supported by the ESPRIT basic research action NADA ROOM SDL and UML recommend to specify the di erent aspects of a system by vi sual formalisms In essence all visual speci cations are directed graphs that are interpreted in a partic ular context In the data context the nodes de ne data entities and the arcs de ne data relationships e g entity relationships diagrams In the behav ior context the nodes de ne states and the arcs de ne state transitions e g statecharts ROOM charts In the intercommunication context the nodes de ne processes and the arcs de ne events e g mes sage sequence charts In the architecture con text the nodes de ne components and the arcs de ne data ow paths e g data ow diagrams Finally in the distribution context the nodes de ne compo nents and computation resources and the arcs de ne the placement of components on resources e g UML deployment diagrams All these visual speci cations are intended to be compositional i e each node can be a graph with a separate meaning However the lack of a denotational model for hierarchical graphs often leads to the loss of compositionality This has severe negative conse quences in the development of realistic applications In this paper we present a simple denotational model which is by de nition compositional for the architecture and behavior aspects of a system This model is then used to give a semantics to almost all the concepts occurring in ROOM Our model also provides a compositional semantics for or states in statecharts In comparison with the compositional semantics for or states given in this semantics retains the full power of the higraph semantics To better appreciate the importance of a compo sitional model let us give a small example showing the problems arising when using the hierarchical state chart notation A telephone switch which resides in a telephone exchange is supposed to control the function of an associated telephone Its simpli ed overall behavior is given in Figure The state onHook consists of two sub states idle and ring Idle is responsible for call initiation	apple rhapsody;chart;computation;demoscene compo;diagram;directed graph;entity;naruto shippuden: clash of ninja revolution 3;software deployment;software engineering;telephone exchange;unified modeling language;warez	Radu Grosu;Gheorghe Stefanescu;Manfred Broy	1998		10.1109/CSD.1998.657538	directed graph;computer science;theoretical computer science;computational linguistics;programming language;algorithm	SE	-37.95576775959677	38.185244889741256	7807
2e56c4b7427585f272657f36b9f1ff8ab2067286	subtyping and constructive specification	wide spectrum language;algebraic specification;generator induction;specification language;partial functions;subtypes;syntactic control;definedness	The use of subtyping is explored in the context of a style of constructive specification. The intention is to enhance expressiveness and strengthen syntactic controls. The resulting specification style is part of a wide spectrum language, ABEL, developed at the Oslo University. A concept of signature completeness is presented, as well as a completion algorithm. Some results relating to weak, strong, and optimal typing are proved. CR Classification: D.1.1, D.2.1, D.2.4, D.3.3, F.3.1, F.3.3.	acm computing classification system;algorithm;np-completeness;wide-spectrum language	Ole-Johan Dahl;Olaf Owe;Tore J. Bastiansen	1998	Nord. J. Comput.		specification language;partial function;computer science;theoretical computer science;programming language;algorithm;language of temporal ordering specification	Logic	-14.362020852708728	15.81491261744455	7808
b0c2b05aa8767b4ac57e2d8a3e8914318ab8668b	note to semantical interpretation of non-trivial syllogisms with intermediate quantifiers	pragmatics;yttrium bismuth context standards pragmatics fuzzy sets semantics;standards;bismuth;semantics;fuzzy sets;yttrium;context	This paper is a contribution to the study of a special kind of syllogisms with intermediate quantifiers. We stem from our previous papers where a formal theory of the intermediate quantifiers was introduced. Besides other results, we syntactically proved validity of 105 basic syllogisms with them. We also demonstrated how our theory works in the semantic interpretation. In this paper, we will address some special kinds of syllogisms that are non-trivial in the sense that both premises as well as conclusion contain general intermediate quantifiers.	fuzzy logic;quantifier (logic);semantic interpretation	Petra Murinová;Vilém Novák	2015	2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2015.7338046	artificial intelligence;yttrium;bismuth;mathematics;semantics;fuzzy set;algorithm;pragmatics	DB	-12.431590730260407	7.997458845587523	7809
2d7ace80e6b605b07b5a282c9347e56ce7192a2f	integrated framework for classification of malwares	classification;machine learning;malware;static analysis;dynamic analysis	Malware is one of the most terrible and major security threats facing the Internet today. It is evolving, becoming more sophisticated and using new ways to target computers and mobile devices. The traditional defences like antivirus softwares typically rely on signature based methods and are unable to detect previously unseen malwares. Machine learning approaches have been adopted to classify malwares based on the features extracted using static or dynamic analysis. Both type of malware analysis have their pros and cons. In this paper, we propose a classification framework which uses integration of both static and dynamic features for distinguishing malwares from clean files. A real world corpus of recent malwares is used to validate the proposed approach. The experimental results, based on a dataset of 998 malwares and 428 cleanware files provide an accuracy of 99.58% indicating that the hybrid approach enhances the accuracy rate of malware detection and classification over the results obtained when these features are considered separately.	antivirus software;computer;internet;machine learning;malware analysis;mobile device	Ekta Gandotra;Divya Bansal;Sanjeev Sofat	2014		10.1145/2659651.2659738	biological classification;computer science;data mining;dynamic program analysis;malware;world wide web;computer security;static analysis	ML	-58.16707464548286	59.86762732389637	7816
0055c74454f649d023f6f62675759d26b44d2fad	a petri nets based approach to specify individual and collaborative interaction in 3d virtual environments		This work describes a methodology that supports the design and implementation of software modules, which represent the individual and collaborative three-dimensional interaction process phases. The presented methodology integrates three modeling approaches: Petri Nets, a collaborative manipulation model based on the combination of single user interaction techniques taxonomy, and object-oriented programming concepts. The combination of these elements allows for the description of interaction tasks, the sequence of interaction processes being controlled by Petri Nets with the codes generated automatically. By the integration of these approaches, the present work addresses not only the entire development cycle of both individual and collaborative three-dimensional interaction, but also the reuse of developed interaction blocks in new virtual environment projects.	code;interaction technique;modular programming;petri net;taxonomy (general);virtual reality	Rafael Rieder;Márcio Sarroglia Pinho;Alberto Barbosa Raposo	2011	J. UCS	10.3217/jucs-017-02-0243	simulation;computer science;knowledge management;process architecture;petri net	HCI	-47.42108608629848	23.82955412471056	7821
a0d360f7eabd3b54ba3346516e086f890ae3e2ff	closing the loop: the influence of code analysis on design	analyse statique;retroingenierie;program verification;analisis programa;software architecture;verificacion programa;source code;program analysis;error detection;analyse programme;static analysis;verification programme;ingeniera inversa;reverse engineering	Static code analysis originally concerned the extraction from source code of various properties of a program. Although this kind of reverse engineering approach can uncover errors that are hard to detect in other ways, it is not a very efficient use of resources because of its retrospective nature and the late error detection that results. The SPARK language and its associated Examiner tool took a different approach which emphasises error prevention (“correctness by construction”) rather than error detection. Recent work with SPARK has shown that very early application of static analysis can have a beneficial influence on software architectures and designs. The paper describes the use of SPARK to produce designs with demonstrably low coupling and high cohesion.	altran praxis;closing (morphology);cohesion (computer science);correctness (computer science);dynamic testing;error detection and correction;information flow (information theory);loose coupling;reverse engineering;spark;static program analysis;systems development life cycle	Peter Amey	2002		10.1007/3-540-48046-3_12	program analysis;kpi-driven code analysis;software architecture;real-time computing;simulation;error detection and correction;computer science;theoretical computer science;operating system;software engineering;programming language;static analysis;algorithm;reverse engineering;static program analysis;source code	SE	-56.65241064006877	38.87780674790331	7831
3312312774844e768a584683cde1813b1581f5ce	review of state-of-the-art wireless technologies and applications in smart cities		There are increasing preferences to employ wireless communication technologies for high mobility, high scalability and low-cost applications in smart city development. This paper gives a brief synopsis of typical wireless technologies in smart city applications and the comparison analysis between them. The trend for smart city wireless technology is also presented. Examples, for several key applications within smart city development (healthcare, smart grid, localization) are studied and current advanced solutions supporting these applications are summarized with futuristic trends and demands are presented.		Hongxu Zhu;Anna S. F. Chang;R. S. Kalawsky;Kim Fung Tsang;Gerhard Petrus Hancke;Lucia Lo Bello;Wing Kuen Ling	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8217074	smart city;computer engineering;control engineering;scalability;wireless sensor network;engineering;smart grid;wireless	EDA	-43.803813715258045	50.738389566961985	7837
934c3b2ce289f1599304073b979a7fa5b20d7ebf	medfdtd: a parallel and open-source cross-platform framework for bioelectromagnetics field simulation		MedFDTD, a new parallel and open-source cross-platform framework for bioelectromagnetics research, has been developed by solving Maxwell’s equations using the finite-difference time-domain method. This framework implements the complex-frequency-shifted perfectly matched layer, supports the import of antenna and bioelectromagnetic models through media with non-dispersive/dispersive properties, and can calculate the antenna output power and specific absorption rate of biological tissues, thereby making it favorable for bioelectromagnetics studies. In addition, MedFDTD can be implemented on multiple CPUs or a variety of chip-based GPUs to accelerate all parallel computing tasks.	central processing unit;dispersive partial differential equation;finite-difference time-domain method;graphics processing unit;maxwell (microarchitecture);open-source software;parallel computing;perfectly matched layer;simulation	Wang Meng;Liu Qian	2018	IEEE Access	10.1109/ACCESS.2017.2784838	chip;computer science;distributed computing;computational science;perfectly matched layer;bioelectromagnetics;finite difference method;cross-platform;dipole antenna;specific absorption rate	HPC	-6.202589032432493	38.03714784216903	7840
c680a36ee28cfb7f2dab49a5663b2fcaca992442	unification and passive inference rules for modal logics	superintuitionistic logics;inference rule;unification;modal logic;modal logics;inference rules;unifier	ABSTRACT We1 study unification of formulas in modal logics and consider logics which are equivalent w.r.t. unification of formulas. A criteria is given for equivalence w.r.t. unification via existence or persistent formulas. A complete syntactic description of all formulas which are non-unifiable in wide classes of modal logics is given. Passive inference rules are considered, it is shown that in any modal logic over D4 there is a finite basis for passive rules.	han unification;modal logic	Vladimir V. Rybakov;Mehmet Terziler;Çigdem Gencer	2000	Journal of Applied Non-Classical Logics	10.1080/11663081.2000.10511004	t-norm fuzzy logics;normal modal logic;structural rule;discrete mathematics;epistemology;computer science;artificial intelligence;axiom s5;mathematics;programming language;accessibility relation;algorithm;rule of inference	Logic	-12.993156894321451	12.86607993485969	7846
76eaebe59658c128fae45b3a330ab98cff44d82f	an overview of reflective memory systems	distributed shared memory systems reflective memory systems message passing multicomputer environments future trends;distributed system;gestion memoire;architecture systeme;systeme reparti;systeme multiprocesseur memoire repartie;storage management;reseau ordinateur;transmission message;message transmission;interconnection network;computer network;gestion memoria;sistema repartido;distributed shared memory systems;sistema multiprocesador memoria distribuida;red ordenador;message passing;memory systems;arquitectura sistema;distributed memory multiprocessor system;system architecture;red interconexion;transmision mensaje;delay application software protocols software systems mirrors network interfaces read write memory research and development multiprocessor interconnection networks problem solving;reseau interconnexion;technological forecasting;technological forecasting distributed shared memory systems message passing	Reflective memory systems are an effective solution to problems raised by message passing in multicomputer environments. The authors provide an overview of existing and emerging RM systems and offer a forecast of future trends.		Milan M. Jovanovic;Veljko M. Milutinovic	1999	IEEE Concurrency	10.1109/4434.766965	embedded system;technology forecasting;parallel computing;message passing;computer science;operating system;distributed computing;programming language;computer network;systems architecture	Embedded	-19.472258308618404	43.09857020563188	7861
24ae3b24fce66dab37e0a7dde455527e8c27b7e7	the strong completeness of a system based on kleene's strong three-valued logic			three-valued logic	Hiroshi Aoyama	1994	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1040511343	algorithm	Logic	-12.752273439311194	12.611159180753203	7863
ded032032f9ec63a6430970ba0ce63328a21c36a	information agents cooperating with heterogenous data sources for customer-order management	application development;multi agent system;information agents;erp system;customer order management;enterprise resource planning;information agent;heterogeneous data sources	As multi-agent systems and information agents obtain an increasing acceptance by application developers, existing legacy Enterprise Resource Planning (ERP) systems still provide the main source of data used in customer, supplier and inventory resource management. In this paper we present a multi-agent system, comprised of information agents, which cooperates with a legacy ERP in order to carry out orders posted by customers in an enterprise environment. Our system is enriched by the capability of producing recommendations to the interested customer through agent cooperation. At first, we address the problem of information workload in an enterprise environment and explore the opportunity of a plausible solution. Secondly we present the architecture of our system and the types of agents involved in it. Finally, we show how it manipulates retrieved information for efficient and facile customer-order management and illustrate results derived from real-data.	erp;enterprise resource planning;multi-agent system;order management system	Dionisis D. Kehagias;Andreas L. Symeonidis;Kyriakos C. Chatzidimitriou;Pericles A. Mitkas	2004		10.1145/967900.967915	enterprise software;computer science;knowledge management;artificial intelligence;multi-agent system;digital firm;database;rapid application development;management;computer security;human resource management system;enterprise information system;enterprise information integration;enterprise life cycle	AI	-52.476202530374465	14.103231652812743	7871
604bc1030a41f1dbeb0b9302776b2a7ab0533700	a workflow mining method through model rewriting	grammar;modelizacion;distributed system;groupware;systeme reparti;data mining;process mining;modelisation;sistema repartido;fouille donnee;grammaire;reecriture;workflow;rewriting;collecticiel;modeling;busca dato;gramatica;reescritura	This work presents a workflow process mining method that is, at least, as powerful as many others presented in the literature, as measured by the examples presented in the literature. The method is based on a grammar of rewriting expressions, by which a model is adapted to include a new execution trace. We also discuss the intrinsic limits of the mining process, which we believe has not been a topic clearly stated and discussed in the published research.	algorithm;heuristic (computer science);rewrite (programming);rewriting;tracing (software)	Jacques Wainer;Kwang-Hoon Kim;Clarence A. Ellis	2005		10.1007/11560296_14	workflow;systems modeling;rewriting;computer science;data mining;grammar;database;process mining;programming language;algorithm	DB	-38.4315979086698	23.705884854327646	7875
d492453aa10e81e39ef430a8d887e54d4b124676	merging procedural and declarative proof	proof assistant;pv system;rewrite systems;natural deduction	There are two different styles for writing natural deduction proofs: the ‘Gentzen’ style in which a proof is a tree with the conclusion at the root and the assumptions at the leaves, and the ‘Fitch’ style (also called ‘flag’ style) in which a proof consists of lines that are grouped together in nested boxes. In the world of proof assistants these two kinds of natural deduction correspond to procedural proofs (tactic scripts that work on one or more subgoals, like those of the Coq, HOL and PVS systems), and declarative proofs (like those of the Mizar and Isabelle/Isar languages). In this paper we give an algorithm for converting tree style proofs to flag style proofs. We then present a rewrite system that simplifies the results. This algorithm can be used to convert arbitrary procedural proofs to declarative proofs. It does not work on the level of the proof terms (the basic inferences of the system), but on the level of the statements that the user sees in the goals when constructing the proof. The algorithm from this paper has been implemented in the ProofWeb interface to Coq. In ProofWeb a proof that is given as a Coq proof script (even with arbitrary Coq tactics) can be displayed both as a tree style and as a flag style proof.	algorithm;coq (software);declarative programming;hol (proof assistant);isabelle;mizar;natural deduction;probabilistically checkable proof;procedural programming;proof assistant;prototype verification system;rewriting	Cezary Kaliszyk;Freek Wiedijk	2008		10.1007/978-3-642-02444-3_13	direct proof;computer-assisted proof;probabilistically checkable proof;computer science;theoretical computer science;photovoltaic system;analytic proof;mathematical proof;mizar system;proof assistant;programming language;natural deduction;structural proof theory;proof complexity;algorithm	PL	-19.760238460586958	20.735281776225168	7876
1452894c1ecab25cae195fb2c93f020ee1954145	communication-efficient outlier detection for scale-out systems		Modern scale-out services are built on top of large datacenters composed of thousands of individual machines. These must be continuously monitored because unexpected failures can overload fail-over mechanism and cause large-scale outages. Such monitoring can be accomplished by periodically measuring hundreds of performance metrics and looking for outliers, often caused by misconfigurations, hardware failures or even software bugs. Previous work has shown that many failures are indeed preceded by such performance outliers, known as performance problems or latent faults. In this work we adapt an existing unsupervised statistical framework for latent fault detection to provide an online, communicationand computation-reduced version. The existing framework is effective in predicting machine failures days before they happen, but requires each monitored machine to send all its periodic metric measurements, which is prohibitive in some settings and requires that the datacenter provide parallel storage and processing. Our adapted framework is able to reduce the amount of data sent and the processing cost at the central coordinator by processing the data in situ, making it usable in wider settings. We utilize techniques from the domain of stream processing, specifically sketching and safe zones, to trade-off accuracy for communication and computation, without compromising its advantages. Like the original framework, our adapted framework is unsupervised, does not require domain knowledge, and provides statistical guarantees on the rate of false positives. Initial experiments show that scores yielded by the adapted framework match the original scores very well, while reducing communications by over 90%.	anomaly detection;computation;data center;experiment;failover;fault detection and isolation;performance tuning;scalability;software bug;stream processing;unsupervised learning	Moshe Gabel;Daniel Keren;Assaf Schuster	2013			anomaly detection;data mining;domain knowledge;software bug;real-time computing;stream processing;scalability;outlier;computation;computer science;fault detection and isolation	OS	-22.919017864539835	55.8203822574453	7883
00e90d7acd9c0a8b0efdb68c38af1632d5e60beb	who wrote this code? identifying the authors of program binaries	programmer style;binary code;authorship attribution;stylistic feature;program binary;extent programmer style;attribution problem;stylistic characteristic;program authorship attribution;novel program representation	Program authorship attribution—identifying a programmer based on stylistic characteristics of code—has practical implications for detecting software theft, digital forensics, and malware analysis. Authorship attribution is challenging in these domains where usually only binary code is available; existing source code-based approaches to attribution have left unclear whether and to what extent programmer style survives the compilation process. Casting authorship attribution as a machine learning problem, we present a novel program representation and techniques that automatically detect the stylistic features of binary code. We apply these techniques to two attribution problems: identifying the precise author of a program, and finding stylistic similarities between programs by unknown authors. Our experiments provide strong evidence that programmer style is preserved in program binaries.	binary code;binary file;cluster analysis;compiler;experiment;framing (social sciences);information retrieval;machine learning;malware analysis;person of interest;programmer;sensor;software forensics;stylometry;toolchain	Nathan E. Rosenblum;Xiaojin Zhu;Barton P. Miller	2011		10.1007/978-3-642-23822-2_10	computer science;theoretical computer science;data mining;computer security;algorithm	Security	-60.19939423937242	41.0469301457661	7890
42823bdad5dc60f6d73abebbecd0733b5d137a13	utilizing role-based models for on-demand composition of smart service systems		Future smart computing environments will heavily rely on the collaboration of autonomous services interconnected dynamically to smart service systems to develop their full potential. Other than existing approaches, e.g., from the domain of Self-organizing Software Systems, where the application structure emerges from dependencies or internal constraints, smart service systems require more complex application models and a coordinating instance for their setup, since they have a rather collaborative than dependent nature. In this paper, we utilize role-based application models, by means of a collaboration specification, to achieve automated, on-demand service composition in infrastructures without a predefined central managing system. We propose a two-phase development methodology in which a collaboration designer specifies the overall system structure including its abstract functionality in the first phase and several (other) developers complement it with its concrete performance in a second phase. We validate our approach using a case study which rigorously follows the proposed development methodology and demonstrate that role-based application models provide sufficient information to guide the discovery and composition process for smart service systems.	autonomous robot;organizing (structure);pervasive informatics;role-based collaboration;run time (program lifecycle phase);service composability principle;software system;two-phase commit protocol	Markus Wutzler;Thomas Springer;Alexander Schill	2017		10.1145/3079368.3079390	software system;composition (visual arts);systems engineering;engineering;internet of things	SE	-42.968759814415364	39.830352598440044	7899
837a76b72d77c184ae2deee06405e7278701cdad	exploitation of multicore systems in a java virtual machine	java virtual machine;java multicore processing computer languages programming virtual machining	The JavaA programming language and the Java virtual machine (JVMA) are intended to provide a level of abstraction from the underlying hardware and operating system (OS). This abstraction poses challenges from a performance perspective, because developers are often unable to make use of best-practice approaches during software development for their deployed OSs and hardware platforms. The rise of multicore processor systems has been swift and has changed many software developers’ underlying assumptions with respect to the hardware over the last ten years. The role of the JVM is to hide such platform complexity by adapting appropriately through runtime analysis and reaction to application behavior. The JVM is an essential component for exploiting the full potential of multicore processor systems through effective management of the memory subsystem, removing impediments to application and system scalability with respect to the number of logical processors, producing efficient and highly optimized code, and providing user tools for monitoring and analysis. This paper reviews the key techniques and tools available in the IBM Developer Kit for the Java 6 release for managing and optimizing Java for multicore processor environments and describes performance results to demonstrate the effectiveness of such tools and techniques.	analysis of algorithms;benchmark (computing);bottleneck (software);central processing unit;computer hardware;debugging;enterprise software;high- and low-level;instruction path length;java version history;java virtual machine;just-in-time compilation;memory hierarchy;multi-core processor;norm (social);operating system;optimizing compiler;programming language;scalability;software development;swift (programming language);synergy;technical standard;thread (computing)	R. A. Sciampacone;Vijay Sundaresan;Daryl Maier;T. Gray-Donald	2010	IBM Journal of Research and Development	10.1147/JRD.2010.2057911	parallel computing;real-time computing;jsr 94;java concurrency;computer science;operating system;cross-platform;strictfp;embedded java;real time java;programming language;java;java annotation	OS	-23.271838244817413	38.57286959896011	7902
8646740a86c65a531c571fe312f8e3189032672c	self-adaptive systems: a survey of current approaches, research challenges and applications	feedback loops;goal based model;model driven development;software reflection;self adaptive software	Self-adaptive software is capable of evaluating and changing its own behavior, whenever the evaluation shows that the software is not accomplishing what it was intended to do, or when better functionality or performance may be possible. The topic of system adaptivity has been widely studied since the mid-60s and, over the past decade, several application areas and technologies relating to self-adaptivity have assumed greater importance. In all these initiatives, software has become the common element that introduces self-adaptability. Thus, the investigation of systematic software engineering approaches is necessary, in order to develop self-adaptive systems that may ideally be applied across multiple domains. The main goal of this study is to review recent progress on self-adaptivity from the standpoint of computer sciences and cybernetics, based on the analysis of state-of-the-art approaches reported in the literature. This review provides an over-arching, integrated view of computer science and software engineering foundations. Moreover, various methods and techniques currently applied in the design of self-adaptive systems are analyzed, as well as some European research initiatives and projects. Finally, the main bottlenecks for the effective application of self-adaptive technology, as well as a set of key research issues on this topic, are precisely identified, in order to overcome current constraints on the effective application of self-adaptivity in its emerging areas of application. 2013 Elsevier Ltd. All rights reserved.	adaptive system;assistive technology;computer science;cybernetics;software engineering	Frank D. Macías-Escrivá;Rodolfo E. Haber;Raúl M. del Toro;Vicente Hernández	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.07.033	simulation;software engineering process group;computer science;artificial intelligence;software design;software framework;software development;software design description;feedback;management science;software technical review;software walkthrough;software analytics;software development process;software requirements;software metric	SE	-61.73179103254475	20.187961470133246	7906
174a4d34319fa515118aa0bcf0de8c6ea042d822	utilizing hoare logic to strengthen testing for error detection in programs		Hoare logic (also known as Floyd-Hoare logic) can be used to formally verify the correctness of programs while testing provides a practical way to detect errors in programs. Unfortunately, the former is rarely applied in practice and the later is difficult to detect all existing errors. In this paper, we propose a novel technique that makes good use of Hoare logic to strengthen testing. The essential idea is first to use specification-based testing to discover all traversed program paths and then to use Hoare logic to prove their correctness. During the proof process, all errors on the paths can be detected. A case study is conducted to show its feasibility; an example taken from the case study is used to illustrate how the proposed method is applied; and discussion on the potential challenges to the method is presented.	code coverage;correctness (computer science);error detection and correction;formal verification;hoare logic;test case	Shaoying Liu	2012			error detection and correction;algorithm;theoretical computer science;correctness;hoare logic;computer science	SE	-59.343121570876534	37.22272932762911	7913
4f8c14701958a64913a4020e72c85a8ec0d9cebb	linear logics with communication-merge	mix;communication principle;mingle;process algebra with communication merge;linear logic	Cut-elimination property, relevance principle, interpolation property and a new property named communication principle are proved for a number of modal intuitionistic linear logics with communication-merge rules. A concurrent-computational interpretation for these logics is obtained based on a process algebra with communication-merge.		Norihiro Kamide	2005	J. Log. Comput.	10.1093/logcom/exh029	t-norm fuzzy logics;linear logic;principle of explosion;discrete mathematics;principle of bivalence;computer science;absorption law;mathematics;programming language;algorithm	Logic	-12.735822343374632	12.850836354636566	7927
a327a1483dd60988fcc011b7061223404b8a9891	analyse statique par interprétation abstraite		Abstract interpretation was introduced as a generic framework, which allows to formalize, derive and prove static analyses, which are based on conservative approximation, that is, which are able to establish a fixed property for a subset of the programs which actually satisfy it. First, we provide a step by step description of the main steps in the design of such analyses, from the choice of a model for the programs to study, to the formalization of the analysis algorithm, including the choice of a set of predicates to use for the analysis. Then, we present a panel of recent applications of these techniques to various areas of computer science. MOTS-CLÉS : Interprétation Abstraite, Analyse Statique, Méthodes Formelles	abstract interpretation;algorithm;approximation;computer science;static program analysis	Xavier Rival	2011	Technique et Science Informatiques	10.3166/tsi.30.371-380	predicate (grammar);distributed computing;theoretical computer science;computer science;abstract interpretation	Logic	-15.972030739573498	22.294663688624457	7948
bfe79347fe36be575f3bb79396f8e0d24d327c7f	browsing in a loosely structured database	query language;exploratory search;independent mappings;inference rule;adverse effect;view update;integrity constraints;complementary mappings;relational databases;database theory;structured data	"""Current database architectures emphasize structure and are inappropriate for applications which model environments that are subject to constant evolution, or environments which do not lend themselves to massive classifications In this paper we describe an architecture which promotes databases that are only loosely structured heaps of facts instead of highly structured data This architecture avoids the traditional dichotomy between """"schema"""" and """"data"""", and it incorporates a single mechanism for defining both inference rules and integrity constraints As lack of organization will usually have adverse effect on retrieval, the principal retrieval method for loosely structured databases is browsing exploratory searching which does not assume any knowledge of the organization (or even the very existence of organization) Two styles of browsing, called navigation and probing, are defined Both are derived from a standard query language based on predicate logic"""	browsing;data integrity;data model;data segment;database model;database theory;hector levesque;knowledge representation and reasoning;query language;rick adams (internet pioneer);smear campaign;windows me	Amihai Motro	1984		10.1145/602259.602286	database theory;adverse effect;data model;relational database;computer science;data integrity;data mining;database;information retrieval;query language;rule of inference	DB	-28.126461947688494	9.774339322226037	7951
a0fdfd3bdaa025ed54d5be24a91ba04743e34ba6	stability of long-lived consensus (extended abstract)	component substitutability;bounded space;stabilization;distributed computing;vector clocks;synchronous system;reset events;fault tolerance;lower bound	This paper introduces the notion stability for a long-lived consensus system. This notion reflects how sensitive to changes the decisions of the system are, from one invocation of the consensus algorithm to the next, with respect to input changes. Stable long-lived consensus systems are proposed, and tight lower bounds on the achievable stability are proved, for several different scenarios. The scenarios include systems that keep memory from one invocation of consensus to the next versus memoryless systems; systems that take their decisions based on the number of different inputs but not on the source identities of those inputs versus non-symmetric systems. These results intend to study essential aspects of stability, and hence are independent of specific models of distributed computing. Applications to particular asynchronous and synchronous system are described.	chandra–toueg consensus algorithm;distributed computing;numerical stability;synchronous circuit	Shlomi Dolev;Sergio Rajsbaum	2000		10.1145/343477.343633	fault tolerance;real-time computing;vector clock;computer science;control theory;mathematics;distributed computing;upper and lower bounds;algorithm	Embedded	-22.377448252719937	44.587404874831826	7965
6adeb3c6bb2bbb9ecdc2c44a89a1bca7c6e986e5	context modeling reflecting the perspectives of constituent agents in distributed reasoning	distributed reasoning;context aware;performance evaluation;ubiquitous computing distributed processing software agents software performance evaluation;agent based;distributed processing;software performance evaluation;ubiquitous computing agent context modeling distributed reasoning merge operator;context model;software agents;agent;computational modeling;context aware service;context context modeling cognition computational modeling reflection context aware services object oriented modeling;cognition;agent systems;ubiquitous computing;smart classroom context modeling distributed reasoning rational reasoning agent based context aware system performance evaluation computer simulation;context aware systems;merge operator;context modeling;computer simulation;use case;context;reflection;object oriented modeling;context aware services	Effective manipulation of context is very important in the research of context-awareness. In recent years, thus, a variety of context models have been proposed to properly handle the key aspects of the context, while focusing on scenario-based acquisition, management, and representation of context. However, they are difficult to be employed for agent system requiring distributed reasoning. In this paper we propose a context modeling approach for distributed reasoning and merge operator reflecting the perspective of constituent agents for rational reasoning. In addition, an agent-based context-aware system is developed implementing the proposed scheme. Performance evaluation by computer simulation on a use case of smart classroom shows that the proposed approach enables the agents to rationally reason and thereby provide intelligent context-aware services to the users.	agent-based model;computer simulation;context awareness;context-aware network;performance evaluation;seamless3d;smart tv	Seong Woo Lee;Chang Hoon Lyu;Kyu Sung Ahn;Seung Wok Han;Hee Yong Youn	2010	2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing	10.1109/GreenCom-CPSCom.2010.50	real-time computing;computer science;knowledge management;theoretical computer science;context model	AI	-42.59771346719772	16.789747837129177	7981
8457815aa710832d075cba537b814ee2bb5b6e64	functional big-step semantics	publikationer;qa 76 software;konferensbidrag;computer programming;artiklar;rapporter	When doing an interactive proof about a piece of software, it is important that the underlying programming language’s semantics does not make the proof unnecessarily difficult or unwieldy. Both smallstep and big-step semantics are commonly used, and the latter is typically given by an inductively defined relation. In this paper, we consider an alternative: using a recursive function akin to an interpreter for the language. The advantages include a better induction theorem, less duplication, accessibility to ordinary functional programmers, and the ease of doing symbolic simulation in proofs via rewriting. We believe that this style of semantics is well suited for compiler verification, including proofs of divergence preservation. We do not claim the invention of this style of semantics: our contribution here is to clarify its value, and to explain how it supports several language features that might appear to require a relational or small-step approach. We illustrate the technique on a simple imperative language with C-like for-loops and a break statement, and compare it to a variety of other approaches. We also provide ML and lambda-calculus based examples to illustrate its generality.	accessibility;automated theorem proving;compiler;control flow;coq (software);expectation propagation;functional programming;imperative programming;input/output;interactive proof system;interpreter (computing);lambda calculus;mathematical induction;operational semantics;oracle machine;programmer;programming language;proof assistant;recursion (computer science);relational model;rewriting;semantics (computer science);symbolic simulation;the australian;vergence	Scott Owens;Magnus O. Myreen;Ramana Kumar;Yong Kiam Tan	2016		10.1007/978-3-662-49498-1_23	computer science;theoretical computer science;computer programming;programming language;well-founded semantics;operational semantics;algorithm;computational semantics	PL	-21.3238495733717	24.533629617885456	7986
ff1eeeebce0ea5fa6c7a5884c7eedcf2b43dfba6	using plans to automate software applications	ai planning;satisfiability	Many software applications consist of a number of interdependent steps and are executed under the supervision of a human administrator or operator. The administrator responds to error conditions and checks to see that the dependencies between steps has been satisfied before allowing the application to continue. AI planning systems, because they have the capability of responding to a dynamically changing environment by replanning, are an excellent candidate for automating such applications. We report on a project that uses AI planning technology as part of the solution in automating a corporate data mining application.	automated planning and scheduling;data mining;interdependence	Jon R. Wright	2007			operator (computer programming);automated planning and scheduling;data mining;software;computer science	AI	-52.987598922807415	42.48615222753567	8016
2bf616a790fa0490fc6b8b2864c5316e482b4927	the navigational power of web browsers	navigational problem;click complexity;expressive power;computational complexity;web browser;finite automaton;computational completeness	We investigate the computational capabilities of Web browsers, when equipped with a standard finite automaton. We observe that Web browsers are Turing-complete. We introduce the notion of a navigational problem, and investigate the complexity of solving Web queries and navigational problems by Web browsers, where complexity is measured by the number of clicks.	automaton;finite-state machine;formal language;human–computer interaction;turing completeness;web search engine;world wide web	Michal Bielecki;Jan Hidders;Jan Paredaens;Marc Spielmann;Jerzy Tyszkiewicz;Jan Van den Bussche	2010	Theory of Computing Systems	10.1007/s00224-010-9294-3	navigation bar;web modeling;simulation;computer science;theoretical computer science;finite-state machine;programming language;computational complexity theory;world wide web;expressive power;algorithm	Logic	-8.347777058597723	17.723009662787774	8022
bf082dc8e1490a13f7cdf9b058beb18d25f3786a	improved multi-core nested depth-first search	model checking;depth first search;correctness proof	This paper presents CNDFS, a tight integration of two earlier multicore nested depth-first search (NDFS) algorithms for LTL model checking. CNDFS combines the different strengths and avoids some weaknesses of its predecessors. We compare CNDFS to an earlier ad-hoc combination of those two algorithms and show several benefits: It has shorter and simpler code and a simpler correctness proof. It exhibits more robust performance with similar scalability, while at the same time reducing memory requirements. The algorithm has been implemented in the multi-core backend of the LTSMIN model checker, which is now benchmarked for the first time on a 48 core machine (previously 16). The experiments demonstrate better scalability than other parallel LTL model checking algorithms, but we also investigate apparent bottlenecks. Finally, we noticed that the multi-core NDFS algorithms produce shorter counterexamples, surprisingly often shorter than their BFS-based counterparts.	algorithm;backtracking;best, worst and average case;blocking (computing);correctness (computer science);depth-first search;experiment;fastest;http 404;hoc (programming language);ltsmin;model checking;multi-core processor;parallel computing;requirement;run time (program lifecycle phase);scalability;speedup;time complexity;work stealing;worst-case execution time	Sami Evangelista;Alfons Laarman;Laure Petrucci;Jaco van de Pol	2012		10.1007/978-3-642-33386-6_22	model checking;breadth-first search;computer science;theoretical computer science;mathematics;distributed computing;algorithm	PL	-14.484802415516931	25.733029580669065	8030
ecaec45d7755c5c742d3f816666c42eeedce90d5	manip-2 : a multicomputer architecture for evaluating logic programs.	logic programs		logic programming;parallel computing	Guo-Jie Li;Benjamin W. Wah	1985			logic synthesis;computer science;theoretical computer science	Arch	-10.402387288735481	34.0297006878375	8039
b5f6507cf05b9c68e36c510b358afa332f356c8a	use case modeling and refinement: a quality-based approach	developpement logiciel;modelizacion;quality metric;raisonnement base sur cas;razonamiento fundado sobre caso;metodo paso a paso;step by step method;behavioral analysis;conceptual analysis;semantics;metric;use case modelling;complexity;semantica;semantique;analisis conceptual;modularization;modelisation;refinement method;desarrollo logicial;analyse comportementale;software development;methode pas a pas;metrico;quality criteria;analisis conductual;information system;analyse conceptuelle;case based reasoning;methode raffinement;software design;modeling;metodo afinamiento;use case;systeme information;metrique;sistema informacion	In this paper, we propose a quality-based use case refinement approach. It consists of a step by step refinement process that combines quality metrics with use case transformation rules. We propose several quality metrics, based on complexity concepts, aimed at measuring the complexity of use cases. Starting from an initial use case, we apply successively a set of transformation rules and measure the resulting use case based on the quality metrics. Our approach is embedded in a general framework allowing us to guide software designers by the mean of quality metrics.	algorithm;computer-aided software engineering;correctness (computer science);embedded system;metamodeling;prototype;refinement (computing);semiconductor industry;software quality;use case diagram	Samira Si-Said Cherfi;Jacky Akoka;Isabelle Comyn-Wattiau	2006		10.1007/11901181_8	use case;case-based reasoning;complexity;systems modeling;metric;computer science;artificial intelligence;software design;software development;software engineering;modular programming;semantics;information system;algorithm	Embedded	-42.366316158553175	25.63199946674077	8041
2010a3d1846459d12916690058a8c033e309df6d	towards a practical formal method for object oriented modelling	object oriented methods;specification languages object oriented methods formal specification;formal specification;analysis and design;formal method;specification languages;object z object oriented modelling formal method real life systems implementation phase object oriented analysis object oriented design omt method object modelling technology system development process formal specification;system development methodologies;system development;object oriented analysis and design;object oriented modelling;object oriented modeling;object oriented modeling data models software engineering	Complexity of real-life systems requires the implementation phase to be preceded by analysis and design. A model employed by a method, to be useful, should be complete, abstract, unambiguous and coherent. Unfortunately, most object oriented analysis and design methods, being informal, fall far short of the ideal. We examine one such method, the OMT method of Rumbaugh et al. (1991). We point out the shortcomings of the method which would adversely affect the system development process. We then outline a practical procedure which shows how to proceed from the initial informal description to a final model formally specified in Object-Z. We also point out the advantages of the Object-Z model for subsequent phases of system development.	formal methods	Kinh Nguyen	1996		10.1109/APSEC.1996.566757	object-oriented analysis and design;method;formal methods;object language;computer science;systems engineering;object-oriented design;software engineering;formal specification;programming language;object-modeling technique;object definition language	Robotics	-46.84873865170778	28.20324623738348	8047
d5d2669bf5deaa223902e6cf76558226c4b61884	garbage collecting the internet: a survey of distributed garbage collection	distributed memory;lenguaje programacion;distributed system;reference counting;gestion memoire;base donnee;systeme reparti;memory management;programming language;memoria compartida;storage management;database;base dato;gestion fichier;network communication;spectrum;reseau;file management;garbage collection;red;gestion memoria;distributed objects;sistema repartido;internet;object oriented;recuperacion automatica memoria;distributed object oriented management;garbage collector;manejo archivos;langage programmation;oriente objet;distributed memories;automatic storage reclamation;distributed file system;object oriented databases;object oriented database;distributed;memoire repartie;recuperation automatique memoire;communication;orientado objeto;comunicacion;distributed file systems;network;automatic storage garbage collection	Internet programming languages such as Java present new challenges to garbage-collection design. The spectrum of garbage-collection schema for linked structures distributed over a network are reviewed here. Distributed garbage collectors are classified first because they evolved from single-address-space collectors. This taxonomy is used as a framework to explore distribution issues: locality of action, communication overhead and indeterministic communication latency.	address space;distributed garbage collection;garbage collection (computer science);internet;java;locality of reference;overhead (computing);programming language	Saleh E. Abdullahi;Graem A. Ringwood	1998	ACM Comput. Surv.	10.1145/292469.292471	manual memory management;parallel computing;computer science;operating system;garbage in, garbage out;database;distributed object;garbage collection;programming language	PL	-18.90961719555258	42.171890473358545	8054
80c24b59ecbb86fdb385be3f88fa4ebd4a557279	automatic synthesis of communication and concurrency for exploring component-based system implementations considering uml channel semantics	communication semantics;uml;embedded systems;automatic synthesis	Nowadays, multi-processor systems play a critical role in embedded system engineering. As a result, the generation of optimal concurrent implementations is an unavoidable but difficult task. Correct concurrent codes achieving maximum performance on the target platform are hard to obtain. On the one hand, dependencies on concurrent computations, such as shared variables or synchronizations, are extremely difficult to analyze from source code. On the other hand, it is completely unfeasible for designers to manually generate multiple implementations in order to evaluate and compare all the possible design alternatives. To overcome these limitations, this paper presents an automatic code generation approach focusing on communication channel semantics. The approach proposes the use of UML/MARTE models to enable designers to graphically handle dependencies and concurrency of the models. As a result, the automatic generation process enables multiple design alternatives to be easily obtained and evaluated without adding manual effort to the design process. To demonstrate these capabilities, the methodology is tested with two large examples.	component-based software engineering;concurrency (computer science);unified modeling language	Héctor Posadas;Pablo Peñil;Alejandro Nicolás;Eugenio Villar	2015	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2015.07.002	unified modeling language;embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;applications of uml;distributed computing;programming language;algorithm	Embedded	-42.13594067552738	33.576085396628436	8063
f80862ecdb0832eb4bb3fb93439943a495540cef	fully abstract semantics for concurrent lambda-calculus	abstract semantics;concurrent lambda-calculus;lambda calculus	Type free lazy -calculus is enriched with angelic parallelism and demonic nondeterminism. Call-by-name and call-by-value abstractions are considered and the operational semantics is stated in terms of a must convergency predicate. We introduce a type assignment system with intersection and union types and we prove the induced logical semantics to be fully abstract.	denotational semantics;lambda calculus	Mariangiola Dezani-Ciancaglini;Ugo de'Liguoro;Adolfo Piperno	1994		10.1007/3-540-57887-0_88	formal semantics;operational semantics;denotational semantics	Logic	-13.371412380466635	18.98371526917828	8083
79461a1a83fca3c1e54f491d05e7aab58f752588	numerical discrepancies between 'some' and 'a few' a basis for dutch scalar implicature research		Horn scales are a popular vehicle in the investigation of implicatures. Yet even this most user-friendly of implicature research categories is plagued by methodological and extrapolating difficulties. One of these difficulties is the possible existence of pungent semantic discrepancies that get lost in translation. To form a basis for past and future Dutch scalar implicature research, we investigated the popular quantifier ‘some’. In an experiment we registered different elements that make up its numerical description (i.e. minimal, most likely and maximal value) and compared them to those of other quantifiers from its Horn scale. The experiment showed that the parameter values for some are overall higher than those for a few. A scaling effect on some, however, appears to blur some’s discrepancies with a few for lower population sizes.	extrapolation;gaussian blur;horn clause;image scaling;list of code lyoko episodes;mass effect trilogy;maximal set;numerical analysis;numerical method;quantifier (logic);usability	Rien Debrouwer;Walter Schaeken	2015			scalar implicature;pure mathematics;mathematics	HCI	-8.942065802537353	10.519384178052425	8086
725e4f06a53c889a201457a887c05d6d3caaeb07	performance estimation for a parallel system with a hierarchical switch network	distributed memory systems;performance evaluation;performance estimation;parallel systems;mapping strategy;message passing;switching network;path contention;static workload analysis	The performance investigations of this paper are based on the performance evaluation system Lapas [3]. The method of static workload analysis is applied to parallel distributed memory systems. The elements of the method are estimations of the computational work and of transfer cost for the most important system components. These basic time costs which are approximated by linear functions are combined in an appropriate model for estimating the overall run time of an algorithm. Numerical grid oriented algorithms are considered as basic applications. Such problems can be parallelized by grid partitioning. The execution of the resulting algorithm consists of a sequence of computation and communication steps for each of the processes running in parallel. The parallel processor system is a hierarchical switch network as it was considered as a basic system in the Genesis project. In the first part of the paper this system is investigated. Path contention is discussed in connection with routing and mapping strategies. Performance estimations and workload studies give an impression of the performance which can be obtained in case of the considered grid problem.		Otto Kolp	1994	Parallel Computing	10.1016/0167-8191(94)90060-4	parallel computing;message passing;real-time computing;computer science;operating system;distributed computing;programming language	HPC	-9.737751431368542	47.70759317865886	8087
378d2eec203d66282b342ebcc3de1bed505a6db3	data parallel programming in the parallel object-oriented language ocore	data parallel;object oriented language;optimization technique;communication model;garbage collection;massively parallel computer;object oriented	A b s t r a c t . The parallel object-oriented language OCore is designed to generate efficient code especially for multi computers. To support massively parallel computation models, advanced communication models, and optimization techniques, OCore introduces the notion of communities, a meta-level architecture, and a distributed garbage collection mechanism on top of a fundamental concurrent object-oriented layer. A community structures a set of objects and provides an abstraction of parallelism. Communities support data parallel computation as well as multi-access data. In this paper we show data parallel programming in OCore. After giving an overview of OCore, we will introduce the notion of communities. Then we will present a molecular dynamics simulation program and a wave equation solver as examples of data parallel programming using communities.	computation;computer;data parallelism;distributed garbage collection;garbage collection (computer science);mathematical optimization;molecular dynamics;parallel computing;simulation;solver	Hiroki Konaka;Takashi Tomokiyo;Munenori Maeda;Yutaka Ishikawa;Atsushi Hori	1995		10.1007/3-540-61487-7_19	parallel computing;computer science;distributed computing;programming language;parallel programming model	PL	-11.140868101206685	39.11503103550107	8094
585067e9a64536eb8787120e3d755bd26c582903	integrating data structure diagrams into source level debuggers (abstract)	first order predicate logic;prolog;logic data base;data base view;query by example;data structure;group	"""As a part of CaseDE, our software design environment project, we have developed techniques to automatically draw data structure diagrams similar to those in textbooks. Diagrams are mechanically obtained from type and variable declarations. Hints to control their layout and composition can be given in a hint language. Figures are composed of diagrams of variables perceived by the user to be related. This paper addresses the use of such figures and their integration into source-level debuggers. Debuggers that can graphically display program states must address the following issues: (1) Displaying a less perfect figure in real-time compared to a better looking one that takes too long. (2) Figures that do not fit on the screen. Operations such as clipping, panning and zooming therefore become necessary. (3) Continually tracing the changing values of some variables, preferably in place and flicker-free. This results in the animation of the diagram, and we need to control the speed of animation. (4) Composition of figures of variables from different procedures or recursive activations of the same procedure. This is best done by an adaptation of windows. (5) Taking snapshots of figures (even during animation) for later use. (6) Changing the value of a data structure by editing the figure. (7) Making a compact transcript of a debugging session for later replay. An abstract representation of the figure is computed by our algorithm assuming that the drawing surface is infinite. A portion of the image given by this representation is then clipped and displayed in a window on the screen. The standard graphics techniques of panning and zooming can be used to see various portions of a figure. In addition, one can (1) """"follow a pointer,"""" i.e., center on the screen the component at the arrow head of the pointer even if it is not near the currently displayed window, and (2) display more abstract views of the figure. Our animation algorithm compares abstract representations of the figures (rather than graphics primitives} to find which areas of the screen to update. Those figure components that are present in successive frames remain where"""	algorithm;clipping (computer graphics);computer graphics;data structure;debugger;debugging;declaration (computer programming);diagram;flicker (screen);flicker-free;jumbo frame;microsoft windows;pointer (computer programming);real-time clock;recursion;software design	Prabhaker Mateti;Gerald M. Radack	1986		10.1145/324634.324990	data structure;computer science;query by example;theoretical computer science;first-order logic;database;group;programming language;prolog	Graphics	-29.266524294338936	25.979625796995478	8102
61f9675013fc48acf4049491eef0afca500af406	obtaining resource controllability in service cooperation environments	resource control;access control;service cooperation	In a ubiquitous computing environment, devices host services that are accessed by anonymous users. This causes threats of resource abuse through malicious attacks or resource scarcity through unintentional excessive service invocations. In this paper, the secure resource control for service sharing is presented which depends on two basic functionalities: access control and soft-state management. Services are grouped into virtual communities for secure access. Service cooperation through external orchestration can only be done within a virtual community. Services can be activated/deactivated by a device management service running on their host devices. Real-time resource utilization of services for virtual community applications is monitored. Therefore, resources on a shared device can be protected from malicious use and become controllable for special quality of service provisioning.	access control;malware;provisioning;quality of service;real-time transcription;soft state;state management;ubiquitous computing;virtual community	Shudong Chen;Johan J. Lukkien	2008		10.1145/1543137.1543160	service product management;computer science;access control;service delivery framework;world wide web;computer security;computer network	OS	-47.77994343347135	57.138532616536494	8105
3953cb1c09558e8f1e544f0e8662e493bfa214d2	oaws: memory occlusion aware warp scheduling	memory occlusion;memory management;mshr;scheduling;graphics processing units;pipelines;gpgpus;parallel processing;benchmark testing;dynamic scheduling	We have closely examined GPU resource utilization when executing memory-intensive benchmarks. Our detailed analysis of GPU global memory accesses reveals that divergent loads can lead to the occlusion of Load-Store units, resulting in quick consumption of MSHR entries. Such memory occlusion prevents other ready memory instructions from accessing L1 data cache, eventually stalling warp schedulers and degrading the overall performance. We have designed memory Occlusion Aware Warp Scheduling (OAWS) that can dynamically predict the demand of MSHR entries of divergent memory instructions, and maximize the number of concurrent warps such that their aggregate MSHR consumptions are within the MSHR capacity. Our dynamic OAWS policy can prevent memory occlusions and effectively leverage more MSHR entries for better IPC performance for GPU. Experimental results show that the static and dynamic versions of OAWS achieve 36.7% and 73.1% performance improvement, compared to the baseline GTO scheduling. Particularly, dynamic OAWS outperforms MASCAR, CCWS, and SWL-Best by 70.1%, 57.8%, and 11.4%, respectively.	aggregate data;baseline (configuration management);benchmark (computing);cpu cache;gaussian orbital;graphics processing unit;schedule (project management);scheduling (computing)	Bin Wang;Yue Zhu;Weikuan Yu	2016	2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)	10.1145/2967938.2967947	parallel processing;benchmark;parallel computing;real-time computing;computer hardware;dynamic priority scheduling;computer science;operating system;pipeline transport;scheduling;memory management	HPC	-9.777795524164416	52.281520525506004	8112
5140d8d239455446198d97955f9926a838bd0e7a	strong normalisation in the pi -calculus	logique lineaire;teoria demonstracion;embedding;distributed system;no determinismo;systeme reparti;proceso lineal;theorie preuve;fonctionnelle;proof theory;interaction;product;raisonnement;logica lineal;processus lineaire;state;funcional;sistema repartido;03fxx;non determinism;functional;strong normalisation;non determinisme;linear process;producto;rewriting systems;informatique theorique;polymorphism;reecriture;type theory;the π calculus;plongement;etat;razonamiento;produit;polymorphisme;interaccion;polimorfismo;inmersion;reasoning;rewriting;term rewriting;linear logic;systeme reecriture;reescritura;computer theory;concurrency theory;informatica teorica	We introduce a typed π-calculus where strong normalisation is ensured by typability. Strong normalisation is a useful property in many computational contexts, including distributed systems. In spite of its simplicity, our type discipline captures a wide class of converging name-passing interactive behaviour. The proof of strong normalisability combines methods from typed λ-calculi and linear logic with process-theoretic reasoning. It is adaptable to systems involving state, non-determinism, polymorphism, control and other extensions. Strong normalisation is shown to have significant consequences, including finite axiomatisation of weak bisimilarity, a fully abstract embedding of the simply typed λ-calculus with products and sums and basic liveness in interaction. Strong normalisability has been extensively studied as a fundamental property in functional calculi, term rewriting and logical systems. This work is one of the first steps to extend theories and proof methods for strong normalisability to the context of name-passing processes.	normalization property (abstract rewriting);π-calculus	Nobuko Yoshida;Martin Berger;Kohei Honda	2004	Inf. Comput.	10.1016/j.ic.2003.08.004	product;polymorphism;linear logic;discrete mathematics;interaction;state;rewriting;computer science;proof theory;embedding;mathematics;programming language;type theory;reason;algorithm	Logic	-8.685055893668313	21.950879633655006	8118
9502a910554c6697e8dd8b679e48b222264e37fd	efficient sleep/wake-up protocols for user-level ipc	sleep wake up protocols;protocols;kernel;sleep protocols operating systems kernel electronic switching systems throughput linux proposals context packaging;sgi multiprocessor;sleep wake up protocols ipc operating system mechanisms blocking semantics sgi multiprocessor operating system hand off scheduling linux implementation;packaging;multiprocessing systems protocols operating systems computers;ipc;sleep;operating system;blocking semantics;electronic switching systems;linux;linux implementation;hand off scheduling;multiprocessing systems;operating system mechanisms;proposals;operating systems computers;context;operating systems;throughput	We present a new facility for cross-address space IPC that exploits queues in memory shared between the client and server address space. The facility employs only widely available operating system mechanisms, and is hence easily portable to different commercial operating systems. It incorporates blocking semantics to avoid wasting processor cycles, and still achieves almost twice the throughput of the native kernelmediated IPC facilities on SGI and IBM uniprocessors. In addition, we demonstrate significantly higher performance gains on an SGI multiprocessor. We argue that co-operating tasks will be better served if the operating system is aware of the co-operation, and propose an interface for a hand-off scheduling mechanism. Finally, we report initial performance results from a Linux implementation of our proposal.	address space;blocking (computing);database;inter-process communication;linux;multiprocessing;operating system;scheduling (computing);server (computing);throughput;uniprocessor system;user space	Ronald C. Unrau;Orran Krieger	1998		10.1109/ICPP.1998.708530	embedded system;communications protocol;packaging and labeling;throughput;parallel computing;kernel;real-time computing;computer science;operating system;sleep;linux kernel;computer network;inter-process communication	OS	-11.879667737286708	46.495943089327994	8125
7c0858374a39043f9bc30548dddb7a0cc8c5f8f4	semantic assistants - user-centric natural language processing services for desktop clients	web service;knowledge worker;natural language processing;domain specificity;semantic analysis	Today’s knowledge workers have to spend a large amount of time and manual effort on creating, analyzing, and modifying textual content. While more advanced semantically-oriented analysis techniques have been developed in recent years, they have not yet found their way into commonly used desktop clients, be they generic (e.g., word processors, email clients) or domain-specific (e.g., software IDEs, biological tools). Instead of forcing the user to leave his current context and use an external application, we propose a “Semantic Assistants” approach, where semantic analysis services relevant for the user’s current task are offered directly within a desktop application. Our approach relies on an OWL ontology model for context and service information and integrates external natural language processing (NLP) pipelines through W3C Web services.	as-easy-as;central processing unit;context switch;desktop computer;documentation;email;information overload;integrated development environment;internet;natural language processing;open-source license;open-source software;pipeline (computing);plug-in (computing);programming paradigm;question answering;semantic desktop;server (computing);server-side;software development;software engineering;text mining;usability testing;web ontology language;web service	René Witte;Thomas Gitzinger	2008		10.1007/978-3-540-89704-0_25	web service;semantic computing;computer science;knowledge management;semantic web stack;database;law;world wide web	Web+IR	-40.606313315492635	9.776896206452983	8138
4613de54347e9a2f5d2e942cd87acb45b78cefb9	the road from stochastic automata to the simulation of rare events		We report in the advances on stochastic automata and its use on rare event simulation. We review and introduce an extension of IOSA, an input/output variant of stochastic automata that under mild constraints can be ensured to contain non-determinism only in a spurious manner. That is, the model can be regarded as fully probabilistic and hence amenable for simulation. We also report on our latest work on fully automatizing the technique of rare event simulation. Using the structure of the model given in terms a network of IOSAs allows us to automatically derive the importance function, which is crucial for the importance splitting technique of rare event simulation. We conclude with experimental results that show how promising our technique is.	automaton;rare events;simulation	Pedro R. D'Argenio;Carlos E. Budde;Matias David Lee;Raúl E. Monti;Leonardo Rodríguez;Nicolás Wolovick	2017		10.1007/978-3-319-68270-9_14	theoretical computer science;spurious relationship;machine learning;computer science;rare events;probabilistic logic;artificial intelligence	Logic	-10.908147884080918	28.508253054738045	8141
b387dedb29a0019c9802229437a087920facb41a	soar: an extended model-based reasoning for diagnosing faults in service-oriented architecture	model based reasoning;soar extended model based reasoning;circuit faults;web services business data processing fault diagnosis inference mechanisms;service orientation;inference capability soar extended model based reasoning fault diagnosis service oriented architecture enterprise application service oriented abductive reasoning qos based reasoning;feature modeling;service management;inference mechanisms;data mining;inference mechanisms service oriented architecture fault diagnosis computer science costs runtime concrete engines buildings application software;monitoring;qos based reasoning;inference capability;business data processing;service oriented abductive reasoning;model based reasoning service oriented architecture diagnosis;cognition;unified modeling language;web services;abductive reasoning;cost effectiveness;quality of service;fault model;service oriented architecture;diagnosis;fault diagnosis;enterprise application	Service-Oriented Architecture (SOA) is a cost effective approach to building enterprise applications. SOA reveals non-conventional characteristics of heterogeneity, grid-like distribution, evolvability, and limited visibility. Hence, services management presents non-conventional challenges. Especially, fault diagnosis at runtime is challenging due to the SOA features. Model-Based Reasoning (MBR) is a formal approach to diagnosing faults, which is based on predicate calculus and term resolution. In this paper, we present SOAR (Service-Oriented Abduvtive Reasoning) which extends the basic MBR to diagnose faults in various SOA components. SOAR provides an enhanced inference capability with state-based and QoS-based reasoning in addition to the basic setting/observation-based reasoning. We propose concrete schemes to formally represent system description, normal behavior, fault model and observations, and reasoning methods to diagnose faults and to determine their causes. In addition, we present a case study of applying SOAR to show how it is applied in practice and how the diagnosis can be conducted in autonomous way.	autonomous robot;enterprise software;fault model;first-order logic;model-based reasoning;quality of service;run time (program lifecycle phase);service-oriented architecture;service-oriented device architecture;soar (cognitive architecture)	Soo Dong Kim;Soo-Ho Chang	2009	2009 Congress on Services - I	10.1109/SERVICES-I.2009.57	reliability engineering;computer science;artificial intelligence;data mining	AI	-43.5109334836411	40.915597141246046	8163
e9296bf79c3e2d09c586740a0826709e209bc7de	speculative tcp connection admission using connection migration in cluster-based servers	operating systems;tcp networking;cluster computing.	This paper presents speculative TCP connection admission, a mechanism for improving sub-optimal request distribution decisions in cluster-based servers. Overloaded server nodes in the cluster speculatively accept incoming requests only to offload them to less-loaded nodes. Speculative admission uses connection endpoint migration, a technique that allows server-side connection endpoints to be arbitrarily assigned to server nodes in the cluster. Speculative connection admission targets distributed load balancing policies at the back-end level that leverage request routing decisions taken outside the cluster. The mechanism has been implemented in the Linux kernel as part of a policy-based software architecture for request distribution. We have been able to show that speculative connection admission adds little overhead to the normal TCP processing, offsets load imbalances and accommodates higher request rates. That makes it suitable for an active role in request distribution in cluster-based servers.	algorithm;communication endpoint;linux;load balancing (computing);overhead (computing);routing;server (computing);server-side;software architecture;speculative execution;testbed	Vlad Olaru;Walter F. Tichy	2004			leverage (finance);computer network;software architecture;network operating system;computer cluster;load balancing (computing);linux kernel;computer science;server	OS	-23.220880438761235	58.71272577510686	8185
a9568ba43bd241415d27b65a6cdea7cf46a5e2ed	teapot: language support for writing memory coherence protocols	semantics directed compiler generation;shared memory;specialized application languages;context specific languages;software notations and tools;programming model;model checking;global address space;partial evaluation;parallel computer;message passing;domain specific language;static analysis;compilation of higher order functional languages;high level language;software and its engineering	Recent shared-memory parallel computer systems offer the exciting possibility of customizing memory coherence protocols to fit an application's semantics and sharing patterns. Custom protocols have been used to achieve message-passing performance---while retaining the convenient programming model of a global address space---and to implement high-level language constructs. Unfortunately, coherence protocols written in a conventional language such as C are difficult to write, debug, understand, or modify. This paper describes Teapot, a small, domain-specific language for writing coherence protocols. Teapot uses continuations to help reduce the complexity of writing protocols. Simple static analysis in the Teapot compiler eliminates much of the overhead of continuations and results in protocols that run nearly as fast as hand-written C code. A Teapot specification can be compiled both to an executable coherence protocol and to input for a model checking system, which permits the specification to be verified. We report our experiences coding and verifying several protocols written in Teapot, along with measurements of the overhead incurred by writing a protocol in a higher-level language.	cache coherence;compiler;continuation;domain-specific language;executable;high- and low-level;high-level programming language;memory coherence;message passing;model checking;overhead (computing);parallel computing;partitioned global address space;programming model;shared memory;static program analysis;verification and validation	Satish Chandra;Brad Richards;James R. Larus	1996		10.1145/231379.231430	model checking;shared memory;parallel computing;message passing;computer science;domain-specific language;theoretical computer science;programming paradigm;programming language;partial evaluation;high-level programming language;static analysis	PL	-17.54544992459051	32.70403973771234	8207
cab34948dd1d22d4b02cca712c44f348cdf2b46d	a remote i/o solution for the cloud	keyboards;kernel;mice;device drivers cloud computing;navigation;servers;streaming media;device drivers;cloud computing remote i o solution i o architecture device driver transformation modules i o stream system architecture;cryptography;kernel streaming media servers mice cryptography keyboards navigation;cloud computing	With applications increasingly moving to the cloud, it is becoming common for an application to be separated by the network from the I/O devices with which the user is interacting. Currently this requires modifying the application to receive user input from the network rather than the device. We present a new I/O architecture in which the device driver is split into two parts, with the network between them. This architecture makes the network invisible to both device and application, allowing both of them to work unmodified. Our architecture also supports transformation modules, each of which comes in a pair that operates on each side of the network. Via these module pairs, the resulting system is capable of supporting the modification of the I/O stream in a variety of ways to compensate for the network, while remaining transparent to the application.	bespoke;cloud computing;device driver;experiment;input/output;interaction;overhead (computing);thin client;video card;virtual machine	Cynthia Taylor;Joseph Pasquale	2012	2012 IEEE Fifth International Conference on Cloud Computing	10.1109/CLOUD.2012.116	input/output;embedded system;navigation;kernel;real-time computing;cloud computing;computer hardware;computer science;cryptography;operating system	HPC	-34.53566329245204	53.74970557986678	8208
b6ede95c72ca8d99ec955aed0608cad2be56b4d5	on keys, foreign keys and nullable attributes in relational mapping systems	anonymization bias;performance evaluation;generic algorithm;data privacy;integrity constraints;schema mapping	We consider the following scenario for a mapping system: given a source schema, a target schema, and a set of value correspondences between these two schemas, generate an executable transformation (i.e., a set of queries) to compute target instances from source instances. We base this computation on two main components: (i) a schema mapping generation algorithm, to compute a declarative schema mapping from the correspondences, and (ii) a query generation algorithm, to compute a transformation from the schema mapping. In this paper, we introduce novel schema mapping and query generation algorithms for mappings between relational schemas with keys, foreign keys and nullable attributes. We extend current relational mapping algorithms (e.g., those proposed in the Clio framework), which are able to deal only in a more limited way with such integrity constraints. As a further contribution, we propose referenced-attribute correspondences, which permit to specify more precise mappings than traditional attribute correspondences, while retaining a simple and intuitive semantics.	algorithm;computation;data integrity;database schema;executable;foreign key;nullable type;object-relational database;object-relational mapping;operational semantics;xml schema	Luca Cabibbo	2009		10.1145/1516360.1516392	information schema;genetic algorithm;information privacy;computer science;theoretical computer science;star schema;data integrity;data mining;database;superkey;database schema	DB	-28.39016115034469	11.065289677375405	8212
43c6289f7283c8397342637f9dbb15823a410735	exploiting psl standard assertions in a theorem-proving-based verification environment	verification;formal semantics;higher order;theorem proving;psl;system design;assertion based design;modeling;automated theorem proving	Assertion-based design is becoming more widely used in industry. However, little has been done to take advantage of existing design assertions in the theorem-proving verification environments. In this paper, we present our work on development of the semi-automated theorem-proving based verification system ROVERIFIC that makes use of existing design assertions. We have defined generic predicate templates that capture the semantics of PSL, and a subset of Verilog. ROVERIFIC uses these templates, and automatically compiles a design under verification (Verilog) and its assertions (PSL) into the higher-order predicates of the PVS [11] theorem proving system. Design verification can be subsequently conducted by proving the correctness properties.	automated theorem proving;correctness (computer science);formal verification;semiconductor industry;theorem proving system;verilog	Youngsik Kim;Parija Sule;Nazanin Mansouri	2004		10.1145/1057661.1057756	computer science;theoretical computer science;mathematics;automated theorem proving;programming language;algorithm;functional verification	Logic	-19.722496558796482	27.11300903850725	8224
a38d9723e4f2c4edf5534180281082552c0c5c7b	on the acceptability of meta-arguments	toulmin scheme;meta argumentation argumentation theory toulmin scheme;meta argumentation;filters;intelligent agent labeling logic programming jacobian matrices conferences voting filtering theory filters;logic programming;voting;intelligent agent;argumentation theory;jacobian matrices;filtering theory;labeling;conferences	In this paper we introduce a theory of meta-argumentation, by using Dung’s theory of abstract argumentation to reason about itself. Meta-arguments are generated from atomic arguments, and extensions of acceptable meta-arguments are based on Dung’s argumentation semantics. To illustrate our theory, we show how to represent Toulmin schemes in this theory by introducing meta-arguments using the Caminada labeling, and meta-arguments for support.	semantic role labeling;theory	Guido Boella;Leon van der Torre;Serena Villata	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.159	voting;computer science;artificial intelligence;theoretical computer science;machine learning;argumentation theory;data mining;probabilistic argumentation;logic programming;law;intelligent agent;algorithm	AI	-17.246737981765875	5.187754012828081	8234
8f6ef618fbc08d88435b3737c8a0a50da7dba089	partial-order reduction in model checking object-oriented petri nets	reduccion sistema;gestion memoire;relation ordre partiel;explosion;metodo reduccion;red petri;maquina estado finito;storage management;system reduction;ramasse miettes;espace etat;verification modele;program verification;systeme ordre reduit;recogemigas;software architecture;gestion memoria;verificacion programa;model checking;state space method;reduction systeme;methode espace etat;object oriented;partial ordering;state space;garbage collector;partial order reduction;oriente objet;explosions;methode reduction;relacion orden parcial;espacio estado;machine etat fini;verification programme;petri net;reduction method;orientado objeto;finite state machine;reduced order systems;architecture logiciel;reseau petri;metodo espacio estado	The main problem being faced in finite-state model checking is the state space explosion problem. For coping with it, many advanced methods for reducing state spaces have been proposed. One of the most successful methods (especially when dealing with software systems) is the so-called partial-order reduction. In the paper, we examine how this method can be used in the context of object-oriented Petri nets, which bring in features like dynamic instantiation, late binding, garbage collec- tion, etc.	model checking;partial order reduction;petri net	Milan Ceska;Ludek Hasa;Tomás Vojnar	2003		10.1007/978-3-540-45210-2_25	partially ordered set;model checking;partial order reduction;software architecture;computer science;state space;artificial intelligence;mathematics;finite-state machine;garbage collection;programming language;object-oriented programming;petri net;algorithm	Logic	-16.842002778191222	27.129968139796286	8247
f1f7a3a15c264ec6750d1424b8ab1f03fd46a0f8	qrygraph: a graphical tool for big data analytics	debugging;cybernetics;automobiles;visualization;big data;ecosystems;conferences	The advent of Big Data has created a rich set of diverse languages and tools for data manipulation and analytics within the Hadoop ecosystem. Pig has a prominent role within this ecosystem as a scripting layer—a convenient way to create analytics jobs that are issued for batch processing in a Hadoop cluster. In order to leverage the benefits of graphical domain specific languages, namely intuitive visual design and inspection, we implemented a web-based graphical tool called QryGraph that complements Pig in various ways. First, it allows a user to create Pig queries in a graphical editor and check their syntax. Second, it provides an administrative interface for managing the execution and overall lifecycle of Pig queries. Finally, it will allow for debugging by running queries on test data sets and for creating user-defined query sub-graphs that can be reused across different Pig queries.	apache hadoop;batch processing;big data;compiler;debugging;domain-specific language;ecosystem;graphical user interface;rapid prototyping;sql;scripting language;server (computing);test data;web application;websocket	Sanny Schmid;Ilias Gerostathopoulos;Christian Prehofer	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844863	analytics;ecosystem;big data;visualization;cybernetics;computer science;artificial intelligence;data mining;database;debugging;world wide web	Visualization	-52.71748862758223	34.67956752088193	8250
0b3a038e24154ca3a800ce7d482aa8acae23811d	functional adaptivity for digital library services in e-infrastructures: the gcube approach	info eu repo semantics conferenceobject πρακτικά συνeδρίου;digital library;digital libraries;functional adaptivity;virtual research environment;electronic computers computer science	We consider the problem of e-Infrastructures that wish to reconcile the generality of their services with the bespoke requirements of diverse user communities. We motivate the requirement of functional adaptivity in the context of gCube, a service-based system that integrates Grid and Digital Library technologies to deploy, operate, and monitor Virtual Research Environments defined over infrastructural resources. We argue that adaptivity requires mapping service interfaces onto multiple implementations, truly alternative interpretations of the same functionality. We then analyse two design solutions in which the alternative implementations are, respectively, full-fledged services and local components of a single service. We associate the latter with lower development costs and increased binding flexibility, and outline a strategy to deploy them dynamically as the payload of service plugins. The result is an infrastructure in which services exhibit multiple behaviours, know how to select the most appropriate behaviour, and can seamlessly learn new behaviours.	bespoke;coexist (image);digital library;extensibility;functional requirement;management system;plug-in (computing);requirement;run time (program lifecycle phase);sensitivity and specificity;transparency (projection);web mapping	Fabio Simeoni;Leonardo Candela;David Lievens;Pasquale Pagano;Manuele Simi	2009		10.1007/978-3-642-04346-8_7	digital library;simulation;computer science;world wide web	HPC	-32.41704358820688	52.326635452632274	8252
77cc10ca7622ce7d5fd125a5b3bf288d94d9d9aa	a survivable critical infrastructure control application	certification;dynamic monitoring;design for survivability;contingency management;real time profiling;critical infrastructure	A microcontroller-based system is described that utilizes real-time distributed sensor data to adapt a critical infrastructure control application to reflect current environment conditions. The system monitors the behavior of its software execution based on real-time analysis of certified frequency spectra and call-graph transition validations in order to detect and react to uncertified behavior and state transitions. The real-time data used to adapt the application is geographically distributed and redundant. The overall system is outlined with focus on the contingency management of the system. Finally, a basic reliability analysis is given as a tool to evaluate the impact of fail rate assumptions.	call graph;microcontroller;real-time clock;real-time computing;real-time data	Ahmed Serageldin;Axel W. Krings;Ahmed Abdel-Rahim	2013		10.1145/2459976.2460015	reliability engineering;real-time computing;engineering;computer security	Embedded	-36.683169709188775	36.86799156966908	8258
df2295edcf23b17419427adcaa27ddb1d4cd0f11	a first attempt at ordinal projective measurement		To our knowledge, all applications of the quantum framework in social sciences are used to model measurements done on a discrete nominal scale. However, especially in cognition, experiments often produce data on an ordinal scale, which implies some internal structure between the possible outcomes. Since there are no ordinal scales in physics, orthodox projection-valued measurement (PVM) lacks the tools and methods to deal with these ordinal scales. Here, we sketch out an attempt to incorporate the ordinal structure of outcomes into the subspaces representing these outcomes. This will also allow us to reduce the dimensionality of the resulting Hilbert spaces, as these often become too high in more complex quantum-like models. To do so, we loosen restrictions placed upon the PVM (and even POVM) framework. We discuss the two major consequences of this generalization: scaling and the loss of repeatability. We also present two applications of this approach, one in game theory and one concerning Likert scales.	measurement in quantum mechanics;ordinal data	Jacob Denolf	2016		10.1007/978-3-319-52289-0_18	welfare economics	Vision	-14.435556651702948	4.349916658011771	8263
fdbf53905c3ccb1c7a0f8ea354b064f1d56622b2	multi-core performance studies of a monte carlo neutron transport code	shared memory;openmp;monte carlo;openmc;reactor analysis;multi core	Performance results are presented for a multi-threaded version of the OpenMC Monte Carlo neutronics code using OpenMP in the context of nuclear reactor criticality calculations. Our main interest is production computing, and thus we limit our approach to threading strategies that both require reasonable levels of development effort and preserve the code features necessary for robust application to real-world reactor problems. Several approaches are developed and the results compared on several multicore platforms using a popular reactor physics benchmark. Our main focus is distilling a broad range of performance studies into a simple, consistent picture of the performance characteristics of reactor Monte Carlo algorithms on current multi-core architectures. Additionally, we speculate on the source of the observed scaling bottlenecks in terms of the exhaustion of shared hardware resources, and suggest programming approaches and strategies to help overcome them.	algorithm;benchmark (computing);criticality matrix;elegant degradation;image scaling;manycore processor;memory hierarchy;monte carlo method;multi-core processor;openmp;overhead (computing);parallel computing;reactor (software);shared memory;speedup;supercomputer;thread (computing)	Andrew R. Siegel;Kord Smith;Paul K. Romano;Benoit Forget;Kyle G. Felker	2014	IJHPCA	10.1177/1094342013492179	multi-core processor;shared memory;parallel computing;simulation;computer science;theoretical computer science;operating system;monte carlo molecular modeling;monte carlo method	HPC	-7.091930774189401	46.06866582425537	8270
ab575aa25c0dbc289ad0d87f16938a53a89ec5a5	u-kitchen: application scenario	pervasive computing;intelligent sensors;mobile computing;convergence;ubiquitous computing;home automation;rfid tags	The research topic of U-spaces is drifting towards the practical implementation. We have presented application scenario of a ubiquitous kitchen environment. The scenario is made using some smart devices communicating to each other and sharing the context via a kitchen server. By addition of limited level of smartness in the appliances, ubiquitous services are developed that makes the job for an autonomous ubiquitous system far more easier and technical issues, less complex. One of our objectives in this paper is to increase the quality and reduce the link cost so that the convergence of large number of devices can be made possible.	autonomous robot;garry kitchen's gamemaker;server (computing);smart device	Junaid Ahsenali Chaudhry;Won-Sik Yoon;Jai-Hoon Kim;We-Duke Cho	2004	Second IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems, 2004. Proceedings.	10.1109/WSTFEUS.2004.10025	embedded system;real-time computing;context-aware pervasive systems;ubiquitous commerce;engineering;ubiquitous robot;computer security	Robotics	-40.73874764986883	49.73706300586637	8274
8e80fada3ca226fbb622c7bbf1080032a46322b6	ontology-based exception handling for semantic business process execution	semantic programming language;agent;semantic web services;exception handling;ontology	Along with the widespread acceptance of business process management (BPM) and Semantic Web services composition technologies, Semantic Web service oriented programming is becoming an efficient way to develop modern business applications. As Semantic Web services are inherently unreliable, how to develop reliable service oriented applications is a significant and challenging problem, especially in complex, untamed and dynamic services environment. However, current business process programming languages for Semantic Web services provide almost no support for exception handling, and runtime environments are weak in providing reliability and adaptability. In this paper, we propose an ontology based exception handling method for semantic business processes using Semantic Programming Language (SPL). We identity a new exception taxonomy described as exception ontology and devise an event-driven exception events detection mechanism. We also devise an exception handling framework based on Multi agent system for SPL program. Especially, the framework provides forward recovery support with dynamically substituting failed services when an exception arises during execution by semantically equivalent or semantically similar Web services.	business process;event-driven architecture;exception handling;multi-agent system;ontology (information science);programming language;runtime system;semantic web service;service-oriented architecture;service-oriented infrastructure;service-oriented programming	Kai Zhao;Linlin Zhang;Shi Ying	2012	JSW	10.4304/jsw.7.8.1791-1798	exception handling;semantic computing;business process execution language;semantic web rule language;semantic grid;computer science;semantic web;ontology;social semantic web;semantic web stack;database;programming language;world wide web;owl-s;semantic analytics	Web+IR	-43.675150857526965	14.953000900066288	8283
dd218a50bef6b51ca5fb869bc4c91b7f90da01b1	social and location-based collaboration mechanism to manage wireless connectivity context data	mobility management mobile radio;context aware;social networking online mobile computing mobility management mobile radio quality of service;high density;complex network;context mobile communication wireless communication collaboration mobile handsets media mobile computing;wireless network;collaboration;mobile computer;modern operational system location based collaboration mechanism social based collaboration mechanism wireless connectivity context data management wireless mobile connectivity management on line social networks wireless connectivity experiences qos metrics location based social media high density handover area identification mobile users mobility predictor;collaboration in online social networks context aware wireless connectivity management wireless network context data;wireless communication;media;social network;collaboration in online social networks;operating system;context aware wireless connectivity management;social networking online;mobile communication;mobile handsets;mobility pattern;wireless network context data;online social network;quality of service;mobile computing;social media;context;mobile user	This paper address the challenge of design a feasible social-based mechanism to manage wireless mobile connectivity. In a previous work, we proposed a methodology to share connectivity experiences among mobile users inside on-line social networks [11]. The aim was explore peoples social circles to enhance their wireless connectivity experiences e.g., QoS metrics such as: throughput, latency and signal quality. In this paper, details of the mashups, between wireless connectivity context data and location-based social media, are provided. We report how this data is handled using complex networks metrics e.g., vertex's strength and centrality degree, to identify high density handover areas, define the mobile users' reputation and to reveal the networks' coverage. Real experiments showed that collaboration can improve QoS metrics from ~18 to ~30% if compared to just use a mobility predictor or a modern operational system, respectively. The discussion unfolds with focus on the collaboration's efficiency as function of time, number of users, discovered area size and mobility patterns.	centrality;complex network;experiment;kerrison predictor;location-based service;mashup (web application hybrid);online and offline;operational system;quality of service;social media;social network;socialization;throughput;wireless access point	Roberto Rigolin Ferreira Lopes;Azzedine Boukerche;Bert-Jan van Beijnum;Edson dos Santos Moreira	2012	2012 IEEE Wireless Communications and Networking Conference (WCNC)	10.1109/WCNC.2012.6214175	media;quality of service;social media;mobile telephony;computer science;operating system;wireless network;distributed computing;mobile computing;world wide web;complex network;wireless;computer network;social network;collaboration	Mobile	-38.20090000287646	57.816272176184825	8311
27a9d73379d8d5015acacc32e1f41ff41918dbfd	model checking with sat-based characterization of actl formulas	satisfiability;kripke model;bounded model checking;first order;model checking;transition systems;error detection	Bounded semantics of LTL with existential interpretation and that of ECTL (the existential fragment of CTL), and the characterization of these existentially interpreted properties have been studied and used as the theoretical basis for SAT-based bounded model checking [2, 18]. This has led to a lot of successful work with respect to error detection in the checking of LTL and ACTL (the universal fragment of CTL) properties by satisfiability testing. Bounded semantics of LTL with the universal interpretation and that of ACTL, and the characterization of such properties by propositional formulas have not been successfully established and this hinders practical verification of valid universal properties by satisfiability checking. This paper studies this problem and the contribution is a bounded semantics for ACTL and a characterization of ACTL properties by propositional formulas. Firstly, we provide a simple bounded semantics for ACTL without considering the practical aspect of the semantics, based on converting a Kripke model to a model (called a k-model) in which the transition relation is captured by a set of k-paths (each path with k transitions). This bounded semantics is not practically useful for the evaluation of a formula, since it involves too many paths in the k-model. Then the technique is to divide the k-model into submodels with a limited number of k-paths (which depends on k and the ACTL property to be verified) such that if an ACTL property is true in every such model, then it is true in the k-model as well. This characterization can then be used as the basis for practical verification of valid ACTL properties by satisfiability checking. A simple case study is provided to show the use of this approach for both verification and error detection of an abstract two-process program written as a first order transition system.	boolean satisfiability problem;error detection and correction;kripke semantics;linear temporal logic;model checking;transition system	Wenhui Zhang	2007		10.1007/978-3-540-76650-6_12	model checking;error detection and correction;computer science;first-order logic;programming language;algorithm;satisfiability	Logic	-13.845954112385613	25.93634655526283	8316
6491d2b5e361946a9365249cc1ddbac47408b233	data-driven operation of building systems: present challenges and future prospects		In this paper we review the current landscape of data-driven decision making in the context of operating residential and commercial building systems with energy management objectives. First, we present results from a literature review focused on identifying new sources of data that have become available (e.g., smart-phone sensors, utility smart meters) and their potential to impact the decision making processes involved in operating these facilities. Existing obstacles to realizing the full potential of these novel data sources are discussed and later explored more in depth through case studies. These include limited interoperability and standardization practices, high labor and/or maintenance costs for installing and maintaining the instrumentation and computationally expensive inference procedures for extracting useful information out of the measurements. Finally, two specific research projects that address some of these challenges are presented in detail: one on disaggregating the total electricity consumption of a building into its constituent loads for informing predictive maintenance practices; and another on standardizing meta-data about sensors and actuators in existing Building Automation Systems (BAS) so that software applications targeting building systems can be deployed in different buildings without the need for manual configuration. Our case studies reveal that the rapid proliferation of sensing/control devices, alone, will not improve the building systems being monitored or significantly alter the way these systems are managed or controlled. When data about the physical world is a commodity, it is the ability to extract actionable information from this resource what generates value and, more often than not, this process requires significant domain expertise.		Mario Bergés;Henning Lange;Jingkun Gao	2018		10.1007/978-3-319-91638-5_2	subject-matter expert;building automation;predictive maintenance;interoperability;standardization;systems engineering;software;energy management;instrumentation;engineering	HCI	-31.853829625299728	20.27716781155352	8317
79f895e56923b7681e063c0cde0a3bd888f91833	to a man with an ordbms everything looks like a row in a table	relational databases;database systems;relational data;data analysis;query languages;software engineering;query language;software architecture;reference architecture;unified modeling language;application software;software quality;persistent data;information systems	In large software projects it is required to manage all project-related artefacts in a shared database in order to support cooperation of developers and reuse of design. Unfortunately, such projects have to be supported by various development tools using proprietary strategies for storing their persistent data. Since we need a strong query language to analyse the project-related data we choose an object-relational database system (ORDBMS) as integration platform. In this paper we will discuss possibilities of how to integrate external data in an ORDBMS. Further, we introduce a reference architecture for discussing the architectural options of an ORDBMS-based integration environment. Finally, we present our own system.	integration platform;object-relational database;programming tool;query language;reference architecture;relational database management system	Wolfgang Mahnke;Hans-Peter Steiert	2001			reference architecture;software architecture;sargable;query optimization;entity–relationship model;relational database;computer science;query by example;component-based software engineering;software development;software design description;database model;software construction;data mining;database;software architecture description;programming language;resource-oriented architecture;view;database design;query language;software system	DB	-33.05967626176694	12.235271105364882	8321
e5860e5da81109cce0e32d6de8f7de6cd461fa43	on building location aware applications using an open platform based on the nexus augmented world model	object oriented data model;world wide web;location awareness	How should the World Wide Web look like if it were for location-based information? And how would mobile, spatially aware applications deal with such a platform? In this paper we present the neXus Augmented World Model, an object oriented data model which plays a major role in an open framework for both providers of location-based information and new kinds of applications: the neXus platform. We illustrate the usability of the model with several sample applications and show the extensibility of this framework. At last we present a stepwise approach for building spatially aware applications in this environment.	data model;extensibility;global variable;location awareness;location-based service;microsoft outlook for mac;open architecture;open platform;sensor;software development process;software engineering;stepwise regression;usability;www;web service;world wide web	Daniela Nicklas;Bernhard Mitschang	2004	Software and Systems Modeling	10.1007/s10270-004-0055-0	simulation;human–computer interaction;computer science;engineering;world wide web	Web+IR	-47.62375987011195	21.22817419731708	8324
5d06be5513d175e0e356ec31b51d038fb4942309	system modeling using the parallel devs formalism and the modelica language	system modeling;modelica;continuous time models;hybrid model;parallel devs;devs formalism;multi domain;simulacion por ordenador;doctoral thesis;sistemas de control;hybrid system;tecnologias;modelica lenguaje de ordenador;ingenieria informatica;hybrid systems modeling;discrete event;simulation environment;ingenieria de sistemas y automatica	1569-190X/$ see front matter 2010 Elsevier B.V doi:10.1016/j.simpat.2010.03.004 * Corresponding author. Tel.: +34 91 3089469. E-mail addresses: vsanz@dia.uned.es (V. Sanz) (S. Dormido). The analysis and identification of the requirements needed to describe P-DEVS models using the Modelica language are discussed in this manuscript. A new free Modelica package, named DEVSLib, is presented. It facilitates the description of discrete-event models according to the Parallel DEVS formalism and provides components to interface with continuous-time models, which can be composed using other Modelica libraries. In addition, DEVSLib contains models implementing Quantized State System (QSS) integration methods. The model definition capabilities provided by DEVSLib are similar to the ones in the simulation environments specifically designed for supporting the DEVS formalism. The main additional advantage of DEVSLib is that it can be used together with other Modelica libraries in order to compose multi-domain and multi-formalism hybrid models. DEVSLib is included in the DESLib Modelica library, which is freely available for download at http:// www.euclides.dia.uned.es. 2010 Elsevier B.V. All rights reserved.	devs;download;library (computing);requirement;semantics (computer science);simulation	Victorino Sanz;Alfonso Urquia;François E. Cellier;Sebastián Dormido	2010	Simulation Modelling Practice and Theory	10.1016/j.simpat.2010.03.004	embedded system;real-time computing;simulation;systems modeling;functional mock-up interface;computer science;control theory;modelica;hybrid system	AI	-36.797448443543	26.84370415483019	8333
4a70a1c32622dc218cd266980ad47f36eb3cf4c9	twitcident: fighting fire with information from social web streams	social web streams;filtering;information filtering;semantics;social web;faceted search;continuous improvement;web based system	In this paper, we present Twitcident, a framework and Web-based system for filtering, searching and analyzing information about real-world incidents or crises. Twitcident connects to emergency broadcasting services and automatically starts tracking and filtering information from Social Web streams (Twitter) when a new incident occurs. It enriches the semantics of streamed Twitter messages to profile incidents and to continuously improve and adapt the information filtering to the current temporal context. Faceted search and analytical tools allow users to retrieve particular information fragments and overview and analyze the current situation as reported on the Social Web. Demo: http://wis.ewi.tudelft.nl/twitcident/	direct-broadcast satellite;faceted classification;gene ontology term enrichment;information filtering system;streaming media	Fabian Abel;Claudia Hauff;Geert-Jan Houben;Richard Stronkman;Ke Tao	2012		10.1145/2187980.2188035	filter;social web;web development;web modeling;computer science;information filtering system;social semantic web;data mining;semantics;internet privacy;world wide web	Web+IR	-39.45604179212431	7.551963157390483	8334
c92c5df77ca0b4e6d491ad414083531bf0792c6a	object-oriented simulation animation builder	analytical models;queueing network;computer graphics;object oriented simulation;packaging;software engineering;transient analysis;model validation;computational modeling;workstations;animation;software component;object oriented modeling animation computational modeling transient analysis packaging computer graphics computer simulation analytical models workstations explosions;explosions;object oriented modelling;petri net;system simulation;manufacturing system;computer simulation;object oriented modeling	This paper presents GIGA (Gknkrateur d'Interfaces Graphiques Animkes) a toolbox designed to help in constructing animation builders for discrete flow system simulation results. Each builder is dedicated to a specific domain and allows the production of consistent and realistic animations with high level model validation and verification facilities. GIGA consists of a set of reusable software components and is independent from any simulation packages. The software components are written in C + + on PCIPS micro-computers and HPJAPOLLO workstations and Objective-C under NeXTStep W. The object-oriented modelling process proposed attempts to reduce the gap between the software engineering and simulation communities. Different domains of discrete flow systems are studied including manufacturing systems, administrative systems, queueing network systems and Petri Nets.	component-based software engineering;computer;high-level programming language;microcomputer;nextstep;objective-c;petri net;queueing theory;simulation;verification and validation;workstation	David R. C. Hill;Michel Gourgand;Patrick Kellert	1993		10.1109/SIMSYM.1993.639140	simulation;computer science;computer engineering;computer graphics (images)	SE	-35.383768595252604	27.546823753529182	8337
cf28ee70611f29c8f3147b9dca9312c1876f28ca	mobicloud: building secure cloud framework for mobile computing and communication	context awareness;mobicloud;risk management mobile computing mobile communication cloud services mobile devices mobile users mobile cloud framework mobicloud ad hoc network trust management secure routing;context aware;computer network security;mobile device;security mobile ad hoc network cloud computing context awareness;mobile cloud framework;risk management;trust management;mobile computer;ad hoc network;data mining;mobile users;secure routing;internet;telecommunication network routing;clouds;mobile communication;telecommunication network routing ad hoc networks computer network security internet mobile computing risk management;mobile handsets;ad hoc networks;mobile ad hoc network;cloud services;mobile computing;security;mobile devices;mobile computing ad hoc networks clouds security mobile communication mobile handsets cloud computing;cloud computing;mobile user	Cloud services can greatly enhance the computing capability of mobile devices. Mobile users can rely on the cloud to perform computationally intensive operations such as searching, data mining, and multimedia processing. In this paper, we propose a new mobile cloud framework called MobiCloud. In addition to providing traditional computation services, MobiCloud also enhances the operation of the ad hoc network itself by treating mobile devices as service nodes. The MobiCloud framework will enhance communication by addressing trust management, secure routing, and risk management issues in the network. A new class of applications can be developed using the enhanced processing power and connectivity provided by MobiCloud. Open research issues for MobiCloud are also discussed to outline future research directions.	computation;data mining;hoc (programming language);mobile cloud computing;mobile computing;mobile device;open research;platform as a service;risk management;routing;trust management (information system)	Dijiang Huang;Xinwen Zhang;Myong H. Kang;Jim Luo	2010	2010 Fifth IEEE International Symposium on Service Oriented System Engineering	10.1109/SOSE.2010.20	wireless ad hoc network;mobile search;cloud computing;computer science;operating system;mobile device;distributed computing;internet privacy;mobile computing;computer network	Mobile	-41.815170819762116	59.59801070521278	8339
3efbcb011eaa39021be75ea182b25cb8749801bd	tagged procedure calls (tpc): efficient runtime support for task-based parallelism on the cell processor	cell processor;system performance;chip;design and implementation;data access;runtime system;data layout;task management;parallel programs;high performance;shared memory multiprocessor	Increasing the number of cores in modern CPUs is the main trend for improving system performance. A central challenge is the runtime support that multi-core systems ought to use for sustaining high performance and scalability without increasing disproportionally the effort required by the programmer. In this work we present Tagged Procedure Calls (TPC ), a runtime system for supporting task-based programming models on architectures that require explicit data access specification by the programmer. We present the design and implementation of TPC for the Cell processor and examine how the runtime system can support task management functions with on-chip communication only. Through minimizing off-chip transactions in the runtime, we achieve sub-microsecond task initiation latency and minimum null task initiation/completion latency of 385 ns. We evaluate TPC with several kernels and applications, demonstrating that TPC achieves scalable on-chip execution of codes previously parallelized and optimized for shared-memory multiprocessors, can exploit additional fine-grain parallelism in codes previously parallelized at coarse levels of granularity, and performs competitively to existing task-based parallel programming frameworks that statically optimize data layout and task placement.	cell (microprocessor);central processing unit;code;data access;direct memory access;ibm sequoia;ibm tivoli storage productivity center;image scaling;multi-core processor;multiprocessing;parallel computing;programmer;programming language;programming model;runtime system;scalability;scheduling (computing);shared memory;speedup	George Tzenakis;Konstantinos Kapelonis;Michail Alvanos;Konstantinos Koukos;Dimitrios S. Nikolopoulos;Angelos Bilas	2010		10.1007/978-3-642-11515-8_23	chip;data access;embedded system;computer architecture;parallel computing;real-time computing;telecommunications;computer science;operating system;computer performance;programming language	HPC	-7.574815706989173	49.20927054224935	8355
5f03c3e80b32118194cfe887c63e53827958dcdc	comparing semantic registries: owljesskb and instancestore	semantic annotation;mathematical services;query formulation;distributed computing;semantics;semantic interoperability;qa75 electronic computers computer science;service discovery;ontology	Service discovery is a critical task in distributed computing architectures for finding a particular service instance. Semantic annotations of services help to enrich the service discovery process. Semantic registries are an important component for the discovery of services and they allow for semantic interoperability through ontology-based query formulation and dynamic mapping of terminologies between system domains. This paper evaluates two semantic registries -- OWLJessKB implementation and instanceStore -- to determine the suitability of these with regards to the performance of loading ontologies, the query response time and the overall scalability for use in mathematical services.	distributed computing;ontology (information science);response time (technology);scalability;semantic interoperability;service discovery	Simone A. Ludwig;Omer F. Rana	2007		10.1145/1272457.1272463	semantic interoperability;semantic computing;semantic grid;computer science;data mining;semantic web stack;database;semantic technology;information retrieval;semantic analytics	HPC	-42.94401929147336	10.000285037854862	8365
96c2477b21edeb160f705ce45f828740bd2814c2	purity in microcanonical thermodynamics: a tale of three resource theories		Resource theories provide a framework for thermodynamics beyond the thermodynamic limit. For systems with definite energy, the key resource is purity. In this paper we formulate three alternative resource theories of purity in the framework of general probabilistic theories (GPTs). Each resource theory corresponds to a different choice of free operations, including i) random reversible operations, ii) noisy operations, generated by reversible interactions with ancillas in the microcanonical state, and iii) unital operations, which preserve the microcanonical state. We determine the requirements the underlying GPT must satisfy in order to support well-behaved resource theories with the above sets as free operations. Then, we focus on a special class of physical theories, called sharp theories with purification, which are appealing for an information-theoretic foundation of thermodynamics. For every sharp theory with purification we show that the convertibility of states implies the validity of a majorisation condition. Moreover, we characterise exactly when majorisation is sufficient for convertibility via random reversible operations: precisely, the sufficiency of majorization is equivalent to the requirement that every maximal set of perfectly distiinguishable pure states can be reversibly converted in any other such set. Under this condition, we prove that i) random reversible, noisy, and unital operations obey inclusion relations like in quantum theory, ii) despite being different, the three sets of operations are equivalent in terms of convertibility, iii) there exists a one-to-one correspondence between measures of purity and Schur-convex functions, and iv) purity resources are dual to entanglement resources.	convex function;guid partition table;information theory;interaction;maximal set;one-to-one (data model);pure function;purification of quantum state;purity (quantum mechanics);quantum entanglement;quantum mechanics;requirement	Giulio Chiribella;Carlo Maria Scandolo	2016	CoRR		discrete mathematics;mathematics;mathematical economics;quantum mechanics	NLP	-7.625446597734775	9.39333279912954	8367
102a5e6381d99b458dd788b1e163a71edaa481a0	query transformation method by dalaunay triangulation for multi-source distributed spatial database systems	delaunay triangulation;query processing;spatial relationship;spatial database;ground control point;spatial relationship resolution;map generalization	One of the difficulties in building a distributed GIS comes from the heterogeneity of spatial databases. In particular, positional mismatches between spatial databases arise due to several reasons, such as different scales, or different ground control points. They result in unreliable outputs of query processing. One simple solution is to correct positional data in spatial databases at each site, according to the most accurate one. This solution is however not practical in most of the cases where the autonomy of each database should be respected. In this paper, we propose a spatial query processing method without correcting positional data in each spatial database. Instead of correcting positional data, we dynamically transform a given query region or position onto each space where spatial objects of each site are located. Our method is based on an elastic transformation method by delaunay triangulation.	autonomy;delaunay triangulation;distributed gis;geographic information system;householder transformation;multi-source;spatial database;spatial query	Jung-Rae Hwang;Ji-Hyeon Oh;Ki-Joune Li	2001		10.1145/512161.512171	spatial relation;query optimization;object-based spatial database;delaunay triangulation;computer science;theoretical computer science;data mining;database;mathematics;geometry;view;spatial database;cartographic generalization;remote sensing;spatial query	DB	-27.80836523668383	7.619586905012264	8370
a400acc6db3a78371bdb56c826bf6a984bd15a37	context-aware agents for vehicular networks: an aspect-oriented approach	context aware;vehicular network;software agent;adaptive dynamics;software engineering;software architecture;interactive system;agent technology;context dependent;aspect oriented;agent architecture	* Abstract. Agent technology plays an important role in the construction of context- aware AmI environments, providing the means to develop intelligent, proactive, autonomous, and interacting systems, able to adapt their behaviour according to some contextual conditions. However, the development of modular context-aware agents has been a challenge to software engineers, since context-awareness use to have in- fluence in several components of the agent architecture, increasing their complexity and making them difficult to evolve and reuse. In order to improve the modulariza- tion and development of context-aware agents for AmI applications, this work proposes the application of aspect-orientation to separate all the issues related to context-awareness inside the agent software architecture. Specifically, our approach focuses on the modularization of context-awareness as aspects for Malaca software agents. The application of aspect-orientation leads to separate in aspects all the agent functions related to context-awareness, such as context acquisition and context- dependent behaviour, improving the internal modularization of context-awareness in the agent architecture and making it able to adapt dynamically to different contexts.	aspect-oriented programming;aspect-oriented software development	Mercedes Amor;Lidia Fuentes	2010		10.1007/978-3-642-12433-4_17	agent architecture;reference architecture;real-time computing;simulation;systems engineering;engineering;autonomous agent;resource-oriented architecture;intelligent agent	AI	-41.18716589667375	42.00369308313505	8382
c7a7ba8445646574a2ea5e14ee92bae504230c3a	backtracking intrusions	computer forensics;intrusion analysis;information flow	Analyzing intrusions today is an arduous, largely manual task because system administrators lack the information and tools needed to understand easily the sequence of steps that occurred in an attack. The goal of BackTracker is to identify automatically potential sequences of steps that occurred in an intrusion. Starting with a single detection point (e.g., a suspicious file), BackTracker identifies files and processes that could have affected that detection point and displays chains of events in a dependency graph. We use BackTracker to analyze several real attacks against computers that we set up as honeypots. In each case, BackTracker is able to highlight effectively the entry point used to gain access to the system and the sequence of steps from that entry point to the point at which we noticed the intrusion. The logging required to support BackTracker added 9&percent; overhead in running time and generated 1.2 GB per day of log data for an operating-system intensive workload.		Samuel T. King;Peter M. Chen	2005	ACM Trans. Comput. Syst.	10.1145/1047915.1047918	real-time computing;information flow;computer science;operating system;distributed computing;computer security;computer forensics	OS	-58.88348045449033	57.91903099978155	8384
9d3c893ec061b0edf3bb334a89d8e7d7907871a5	symbolic techniques in satisfiability solving	quantifier elimination;optimization technique;satisfiability;constraint satisfaction;decision procedure;symbolic decision procedure;propositional satisfiability;binary decision diagram	Recent work has shown how to use binary decision diagrams for satisfiability solving. The idea of this approach, which we call symbolic quantifier elimination, is to view an instance of propositional satisfiability as an existentially quantified proposition formula. Satisfiability solving then amounts to quantifier elimination; once all quantifiers have been eliminated, we are left with either 1 or 0. Our goal in this work is to study the effectiveness of symbolic quantifier elimination as an approach to satisfiability solving. To that end, we conduct a direct comparison with the DPLL-based ZChaff, as well as evaluate a variety of optimization techniques for the symbolic approach. In comparing the symbolic approach to ZChaff, we evaluate scalability across a variety of classes of formulas. We find that no approach dominates across all classes. While ZChaff dominates for many classes of formulas, the symbolic approach is superior for other classes of formulas. Once we have demonstrated the viability of the symbolic approach, we focus on optimization techniques for this approach. We study techniques from constraint satisfaction for finding a good plan for performing the symbolic operations of conjunction and of existential quantification. We also study various variable-ordering heuristics, finding that while no heuristic seems to dominate across all classes of formulas, the maximum-cardinality search heuristic seems to offer the best overall performance.	binary decision diagram;boolean satisfiability problem;chaff algorithm;constraint satisfaction;dpll algorithm;existential quantification;heuristic (computer science);mathematical optimization;quantifier (logic);scalability	Guoqiang Pan;Moshe Y. Vardi	2005	Journal of Automated Reasoning	10.1007/s10817-005-9009-7	combinatorics;discrete mathematics;quantifier elimination;constraint satisfaction;computer science;mathematics;binary decision diagram;algorithm;satisfiability	AI	-15.033620286815587	24.003190866475148	8390
45feeb668c21df320d4419ae655d5ff0616a27bc	making the java memory model safe	operational semantics;java memory model;data race freedom;type safety	This work presents a machine-checked formalisation of the Java memory model and connects it to an operational semantics for Java and Java bytecode. For the whole model, I prove the data race freedom guarantee and type safety. The model extends previous formalisations by dynamic memory allocation, thread spawns and joins, infinite executions, the wait-notify mechanism, and thread interruption, all of which interact in subtle ways with the memory model. The formalisation resulted in numerous clarifications of and fixes to the existing JMM specification.	interrupt;java bytecode;java memory model;memory management;memory model (programming);operational semantics;race condition;type safety	Andreas Lochbihler	2013	ACM Trans. Program. Lang. Syst.	10.1145/2518191	memory model;parallel computing;real-time computing;jsr 94;java concurrency;type safety;computer science;java modeling language;strictfp;real time java;programming language;java memory model;java;operational semantics;generics in java;scala;java annotation	PL	-22.39731647537541	32.391516985245595	8391
cc2f822a25a40d8f97ee5de74d59c89660c71a72	supplementing a uml development process with b	stereotypie;language class;lenguaje uml;stereotyping;langage modelisation unifie;b method;development process;desarrollo verbal;uml class diagram;estereotipia;stereotypy;uml class diagrams;unified modelling language;classe langage;language development;developpement verbal;methode b;use case;clase lenguaje	This paper discusses our experiences of using UML and B together through an illustrative case study. Our approach to using UML and B centers around stereotyping UML classes in order to identify which classes should be modelled in B. We discuss the tensions between the notations, and the compromises that need to be reached in order for B to supplement a UML development. The case study begins from the initial conception of a library system and its use case view in order to demonstrate how the classes were identified.	library classification;stereotype (uml);unified modeling language	Helen Treharne	2002		10.1007/3-540-45614-7_32	use case;b-method;unified modeling language;uml tool;computer science;software engineering;applications of uml;class diagram;stereotypy;software development process	SE	-43.69766560890825	25.42072871143627	8426
4aecdecf7d1b3698953c570925cd1881d6a0dcfe	branching bisimilarity of normed bpa processes is in nexptime	verification;process calculi	"""Branching bisimilarity of nor med Basic Process Algebra (BPA) processes was shown to be decidable by Yuxi Fu (ICALP 2013) but his proof has not provided any upper complexity bound. We present a simpler approach based on relative prime decompositions that leads to a nondeterministic exponential-time algorithm, this is """"close"""" to the known exponential-time lower bound. We also derive that semantic finiteness (the question if a given nor med BPA process is branching bisimilar with some finite-state process) belongs to NExpTime as well."""	algorithm;bisimulation;execution unit;icalp;nexptime;oracle bpa suite;process calculus;time complexity	Wojciech Czerwinski;Petr Jancar	2015	2015 30th Annual ACM/IEEE Symposium on Logic in Computer Science		combinatorics;discrete mathematics;verification;computer science;mathematics;programming language;algorithm	Logic	-5.296036915222725	22.978418844817256	8430
01a562fc5ff0881731a6005d370cef91162ed998	towards a smart city based on cloud of things	cloud of things;smart cities;internet of things	Smart City represents one of the most promising and prominent Internet of Things (IoT) applications. In the last few years, smart city concept has played an important role in academic and industry fields, with the development and deployment of various middleware platforms. However, this expansion has followed distinct approaches creating a fragmented scenario, in which different IoT ecosystems are not able to communicate between them. To fill this gap, there is a need to revisit the smart city IoT semantic and offer a global common approach. To this purpose, this paper browses the semantic annotation of the sensors in the cloud, and innovative services can be implemented and considered by bridging Clouds and IoT. Things-like semantic will be considered to perform the aggregation of heterogeneous resources by defining the Clouds of Things (CoT) paradigm. We survey the smart city vision, providing information on the main requirements and highlighting the benefits of integrating different IoT ecosystems within the cloud under this new CoT vision and discuss relevant challenges in this research area.	bridging (networking);cloud computing;ecosystem;fragmented object;internet of things;middleware;programming paradigm;requirement;sensor;smart city;software deployment	Riccardo Petrolo;Valeria Loscrì;Nathalie Mitton	2014		10.1145/2633661.2633667	computer science;internet privacy;world wide web;computer security;internet of things	AI	-42.786894572168386	48.574082814514256	8458
1f70da031d7ec80f1258884369d009d0310058b4	memory hierarchy characterization of nosql applications through full-system simulation		In this work, we conduct a detailed memory characterization of a representative set of modern data-management software (Cassandra, MongoDB, OrientDB and Redis) running an illustrative NoSQL benchmark suite (YCSB). These applications are widely popular NoSQL databases with different data models and features such as in-memory storage. We compare how these data-serving applications behave with respect to other well-known benchmarks, such as SPEC CPU2006, PARSEC and NAS Parallel Benchmark. The methodology employed for evaluation relies on state-of-the-art full-system simulation tools, such as gem5. This allows us to explore configurations unattainable using performance monitoring units in actual hardware, being able to characterize memory properties. The results obtained suggest that NoSQL application behavior is not dissimilar to conventional workloads. Therefore, some of the optimizations present in state-of-the-art hardware might have a direct benefit. Nevertheless, there are some common aspects that are distinctive of conventional benchmarks that might be sufficiently relevant to be considered in architectural design. Strikingly, we also found that most database engines, independently of aspects such as workload or database size, exhibit highly uniform behavior. Finally, we show that different data-base engines make highly distinctive demands on the memory hierarchy, some being more stringent than others.		Adrian Colaso;Pablo Prieto;Jose Angel Herrero;Pablo Abad;Lucia G. Menezo;Valentin Puente;Jose Angel Gregorio	2018	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2017.2787150	computer science;nosql;real-time computing;workload;database;big data;distributed database;data modeling;memory hierarchy;spec#;benchmark (computing)	Metrics	-5.5895976463752195	45.538595422520835	8460
f7789e20d61e3c5b9de8da40f2264ee323eecb27	eureka definitions for free! or disagreement points for fold/unford transformations	disagreement point;eureka definitions;disagreement points;unford transformations;eureka definition	The fold/unfold framework of Burstall and Darlington is a very powerful framework for transforming function definitions in the form of recursion equation schemes. This may be used to transform a function so as to improve the efficiency of its implementation. However, for this to work the user must supply so-called Eureka definitions and it may require some ingenuity to construct these. This paper shows that a class of these Eureka definitions can be derived in a rather systematic way.	eureka (opac)	Hanne Riis Nielson;Flemming Nielson	1990		10.1007/3-540-52592-0_70	artificial intelligence;algorithm	Crypto	-12.448071234617311	16.318352260188185	8461
db9b265f498294c7a31813b14872cad4a252b6f2	distributed geographic information querying approach based on mobile agent	disconnected operation;geographic information;query processing;mobile agents;real time;distributed processing;distributed computing;internet;mobile agents geographic information systems internet query processing distributed processing;geographic information systems;mobile agent development platform distributed geographic information querying geographic information access geographic information sharing geographic information dissemination internet gis architecture mobile internet distributed computing asynchronous task execution communication bandwidth real time abilities offline processing disconnected operation query distributed geographic information connection based access method aglets;internet gis;distributed computing environment;mobile agents internet geographic information systems distributed computing mobile computing computer architecture mobile communication bandwidth robustness prototypes;mobile agent;access method;mobile internet	GIS based on Internet as a promising research field brings opportunities and challenges to geographic information accessing, sharing and disseminating. However, the inherent limitation of current Internet GIS architecture based on connection makes it incompetent to fulfill various requirements of GIS applications under dynamic, complicated and distributed computing environment such as the mobile Internet. As a developed paradigm in distributed computing, mobile agent can especially benefit (mobile) Internet applications on providing asynchronous task execution and more dynamics, supporting flexible and extensible cooperation, reducing communication bandwidth, enhancing real time abilities and higher degree of robustness, enabling offline processing and disconnected operation. This paper presents a mobile agent based approach to query distributed geographic information, which can overcome the drawbacks of the traditional connection-based access method. A prototype was implemented to access distributed geographic information transparently by using Aglets, which is a free mobile agent development platform developed by IBM.	agent-based model;aglets;client (computing);distributed computing environment;distributed gis;gis applications;geographic information system;international conference on computer and information technology;internet;mobile agent;online and offline;programming paradigm;prototype;requirement;server (computing);sourceforge	Yang An;Jihong Guan;Bo Zhao	2004	The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.	10.1109/CIT.2004.1357290	mobile search;the internet;mobile web;mobile database;computer science;mobile agent;database;distributed computing;geographic information system;mobile computing;access method;world wide web;distributed computing environment;computer network	Robotics	-34.26211787505883	48.52569051651468	8472
50181e662458d067ecda146d069f54ddd9f1506e	cooperative concurrency control on the web	data sharing;groupware;concurrent computing;collaborative work;data integrity;information retrieval system;information retrieval;information technology;collaboration;distributed computing;file formats;software fault tolerance;concurrency control collaborative work concurrent computing collaboration web sites information retrieval information technology vehicles distributed computing message passing;internet world wide web cooperative concurrency control data sharing collaborative environments data consistency system failures transactions information storage information retrieval system file formats file uploading file downloading system recovery cooperative work;system failures;collaborative environment;information storage;system recovery;internet;cooperative concurrency control;web sites;concurrency control;message passing;file uploading;cooperative applications;world wide web;system recovery internet groupware concurrency control data integrity software fault tolerance transaction processing;vehicles;transaction processing;collaborative environments;data consistency;transactions;file downloading;cooperative work	Sharing of data in collaborative environments requires mechanisms that ensure the consistency of data in spite of concurrency and failures. This is traditionally handled by transactions or extended transaction mechanisms. The World Wide Web (hereinafter the Web), although originally designed as information storage and retrieval system, is currently being extended to serve as vehicle for collaboration. However, the proposed extensions mainly address the basic issues such as file formats and upand downloading of files. We are developing a configurable set of services for handling concurrency, recovery and collaboration that can be integrated into different system environments. In this paper, we show an approach to use them for supporting cooperative work on the Web. In particular, the design and implementation of a cooperative concurrency control mechanism for the Web is presented.	computer data storage;concurrency (computer science);concurrency control;data recovery;download;world wide web	Michael Mock;Martin Gergeleit;Edgar Nett	1997		10.1109/FTDCS.1997.644713	optimistic concurrency control;isolation;computer science;database;distributed computing;multiversion concurrency control;non-lock concurrency control;world wide web;distributed concurrency control	DB	-25.554982693607258	47.402465907621945	8475
4da0d6db29b36afa8a6b2b8216fc4975f6184cda	daper joint learning from partially structured graph databases		In this paper, we are interested in learning specific probabilistic relational models, named Directed Acyclic Probabilistic Entity Relationship (DAPER) models, from partially structured databases. Algorithms for such a learning task already exist for structured data coming from a relational database. They have been also extended to partially structured data stored in a graph database where the Entity Relationship (ER) schema is first identified from data, and then the DAPER dependency structure is learnt for this specific ER schema. We propose in this work a joint learning from partially structured graph databases where we want to learn at the same time the ER schema and the probabilistic dependencies. The Markov Logic Network (MLN) formalism is an efficient solution for this task. We show with an illustrative example that MLN structure learning can effectively learn both parts of the DAPER model in one single task, with a comparative precision, but with a very high complexity.		Marwa El Abri;Philippe Leray;Nadia Essoussi	2018		10.1007/978-3-319-97749-2_10	markov logic network;machine learning;probabilistic logic;data model;graph database;entity–relationship model;schema (psychology);formalism (philosophy);relational database;computer science;artificial intelligence	ML	-21.50037719084793	7.070757605666525	8480
cdcf6e57fedadc7b0ccf4091f2390f40653a2fee	toward an elastic service based framework for enterprise application integration	software architecture business data processing;bottom up;education service domain;service based architecture;application software service oriented architecture web services computer architecture conference management software development management technology management engineering management buildings companies;education service domain elastic service based framework enterprise application integration service based architecture;software architecture;real world application;elastic service based framework;business data processing;enterprise application integration	Enterprise application integration (EAI) is a very complicated process of integrating multiple applications, which were independently developed, use incompatible technology, and remain independently managed. In this paper we are going to discuss about the evolution of EAI concepts and a new elastic framework for enterprise application integration (EAI). The new framework provides a service based architecture for seamless integration of enterprise applications which can develop from the bottom up, building on existing technology. The approach is demonstrated by building a real world application for EAI in the education service domain. Business drivers for and approaches to EAI are present first. We then present our implementation of the architecture and practical challenges encountered in EAI.	enterprise application integration;enterprise software;seamless3d;top-down and bottom-up design	O. R. Bagheri;R. Nasiri;Majid Peyravi;P. Khosraviyan Dehkordi	2007	5th ACIS International Conference on Software Engineering Research, Management & Applications (SERA 2007)	10.1109/SERA.2007.128	enterprise architecture framework;functional software architecture;reference architecture;software architecture;the open group architecture framework;enterprise application integration;enterprise software;sherwood applied business security architecture;nist enterprise architecture model;computer science;systems engineering;engineering;knowledge management;architecture domain;applications architecture;service-oriented modeling;software engineering;top-down and bottom-up design;enterprise architecture management;service;solution architecture;enterprise architecture;enterprise integration;view model;data architecture;business architecture;enterprise life cycle	SE	-57.45092336880275	16.766691948016824	8487
5032cdcb8178e65a6ff46d588788634041f97086	higher-order contingentism, part 3: expressive limitations		Two expressive limitations of an infinitary higher-order modal language interpreted on models for higher-order contingentism – the thesis that it is contingent what propositions, properties and relations there are – are established: First, the inexpressibility of certain relations, which leads to the fact that certain model-theoretic existence conditions for relations cannot equivalently be reformulated in terms of being expressible in such a language. Second, the inexpressibility of certain modalized cardinality claims, which shows that in such a language, higher-order contingentists cannot express what is communicated using various instances of talk of ‘possible things’, such as ‘there are uncountably many possible stars’.		Peter Fritz	2018	J. Philosophical Logic	10.1007/s10992-017-9443-0	mathematics;algorithm;discrete mathematics;cardinality;expressivity;stars	Theory	-12.989063723655255	8.660232503216209	8501
d80619f402a50542f92d76277a6d20d667657d31	debugging applications created by a domain specific language: the ipac case	software testing;debugger;domain specific language	Nowadays, software developers have created a large number of applications in various research domains of Computer Science. However, not all of them are familiar with the majority of the research domains. Hence, Domain Specific Languages (DSLs) can provide an abstract, concrete description of a domain in terms that can easily be managed by developers. The most important in such cases is the provision of a debugger for debugging the generated software based on a specific DSL. In this paper, we propose and present a simple but efficient debugger created for the needs of the IPAC system. The debugger is able to provide debugging facilities to developers that define applications for autonomous mobile nodes. The debugger can map code lines between the initial application workflow and the final code defined in a known programming language. Finally, we propose a logging server responsible to provide debugging facilities for the IPAC framework. The IPAC system is consisted of a number of middleware services for mobile nodes acting in a network. In this system a number of mobile nodes exchanged messages that are visualized for more efficient manipulation.	debugging;domain-specific language	Kostas Kolomvatsos;George Valkanas;Stathes Hadjiefthymiades	2012	Journal of Systems and Software	10.1016/j.jss.2011.11.1009	real-time computing;computer science;domain-specific language;operating system;software engineering;database;software testing;programming language	OS	-51.947229987594575	34.797258789256155	8527
2e1d7b8a921720856a779223e22cbbca123dbae7	preserving the context of interrupted business process activities	data analysis;business processes;b2b and b2c applications.;information systems;flexibility;information system;business process	The capability to safely interrupt business process activities is an important requirement for advanced processaware information systems. Indeed, exceptions stemming from the application environment often appear while one or more application-related process activities are running. Safely interrupting an activity consists of preserving its context, i.e., saving the data associated with this activity. This is important since possible solutions for an exceptional situation are often based on the current data context of the interrupted activity. In this paper, a data classification scheme based on data relevance and on data update frequency is proposed and discussed with respect to two different real-world applications. Taking into account this classification, a correctness criterion for interrupting running activities while preserving their context is proposed and analyzed.	business process;comparison and contrast of classification schemes in linguistics and metadata;correctness (computer science);information system;interrupt;relevance;stemming	Sarita Bassil;Stefanie Rinderle-Ma;Rudolf K. Keller;Peter G. Kropf;Manfred Reichert	2005			real-time computing;engineering;data mining;database	HCI	-54.108107641486534	18.1619204408215	8531
26466aff60515320a50c16dc7f9e6cdde6ada192	which xml storage for knowledge and ontology systems?	management system;semantic network;xml database;lexical database;data storage;data structure	New research concerning knowledge and ontology management systems in many cases need the versatility of native XML storage for manipulations with diverse and changing data structures. Within the DEB (Dictionary Editor and Browser) development platform, the efficiency of the background data storage for all kinds of structures and services including dictionaries, wordnet semantic networks, classical ontologies or lexical databases, tends to be a crucial property of the system. In this paper, we describe a large set of tests that were run on four selected (out of twenty) available XML database systems, where the tests were run with the aim to recommend the best XML database for knowledge and ontology storage.	benchmark (computing);computer data storage;data structure;dictionary;mathematical optimization;monetdb;ontology (information science);sedna xml dbms;semantic network;wordnet;xml database;xquery;xmark93	Martin Bukatovic;Aleš Horák;Adam Rambousek	2010		10.1007/978-3-642-15387-7_47	xml catalog;xml validation;xml base;xml;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-35.8455965995166	7.213054537648827	8534
2b24294b2c098575cf030363d726fabacaf78a91	energy efficient collaborative sensing-based design: soft keyboard case study	keyboards;power aware computing;concept prototype evaluation energy efficient collaborative sensing based design pressure sensor based soft keyboard energy optimization sensor system architecture sensing scheduling sensor data processing semantic information energy reduction customization techniques user typing speed typing pulse duration;soft keyboard e textiles sensing;scheduling;scheduling keyboards power aware computing pressure sensors;sensors keyboards arrays semantics event detection prototypes energy consumption;pressure sensors	With our approach, for the synthesis and energy-efficient operation of a pressure sensor-based soft keyboard, energy is optimized at five levels of abstraction: 1) architecture of sensor systems, 2) sensing schedule, 3) sensor data processing, 4) use of semantic information, and 5) customization. We have been able to achieve up to a factor of 63 energy reduction in a keyboard case study over the standard using various techniques. For example, when each sensor senses several keyboard events and each event is sensed by three sensors, sensing and data processing results in a 12- to 42-fold energy savings depending on typing speeds. Customization techniques accounting for user typing speed and duration of typing pulses support another 1.5-fold savings. We demonstrate using a proof of concept prototype evaluation with five participants.	e-textiles;principle of abstraction;prototype;sensor;shoes;wearable computer;words per minute	Mahsan Rofouei;Miodrag Potkonjak;Majid Sarrafzadeh	2014	IEEE Transactions on Human-Machine Systems	10.1109/TSMC.2013.2290503	embedded system;real-time computing;computer hardware;computer science;operating system;pressure sensor;scheduling	Mobile	-40.46353974744819	48.27309455943583	8551
99d6450274a73e0a3b4524c2b5cf0480c1f38dd0	energy-aware joint scheduling of tasks and messages in wireless sensor networks	wireless sensor networks graph colouring scheduling task analysis;mixed tree coloring;processor scheduling;color;graph coloring energy aware joint scheduling wireless sensor networks embedded systems mixed tree coloring dynamic modulation scaling dynamic voltage scaling;real time;wireless network;resource manager;resource management;dynamic voltage scaling;graph coloring;testing;energy aware joint scheduling;joints;polynomials;sensor network;embedded system;wireless sensor network;tree graphs;embedded systems;scheduling algorithm;computational modeling;sensor networks;energy consumption;scheduling;task analysis;graphcoloring;schedules;dynamic modulation scaling;sensor networks resource management scheduling graphcoloring;wireless sensor networks;wireless sensor networks processor scheduling energy management energy consumption polynomials real time systems embedded system tree graphs scheduling algorithm testing;energy management;graph colouring;real time systems	We consider the problem of energy-aware joint scheduling of tasks and messages with real-time constraints in wireless networked embedded systems specifically wireless sensor networks. We use the mixed tree coloring approach in order to model the constraints and show that k-coloring of a mixed tree can be mapped to a non-conflicting schedule consisting of k time slots. Also, we propose to conduct testbed evaluation to quantify the performance of combined implementation of energy management techniques such as Dynamic Modulation Scaling (DMS) along with Dynamic Voltage Scaling (DVS).	dynamic voltage scaling;embedded system;graph coloring;greedy algorithm;mixed graph;modulation;real-time clock;scheduling (computing);testbed;tree network	Benazir Fateh;G. Manimaran	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470808	real-time computing;computer science;distributed computing;key distribution in wireless sensor networks;computer network	Embedded	-5.702385918878666	60.295150759244514	8572
319761421a705ccb4ead401b6278abf104308909	reification without evaluation	programming language;towers;processing equipment;computer programming;serial processors;common sense;programming languages	Constructing self-referential systems, such as Brian Smith’s 3Lisp language, is actually more straightforward than you think. Anyone can build an infinite tower of processors (where each processor implements the processor at the next level below) by employing some common sense and one simple trick. In particular, it is not necessary to re-design quotation, take a stand on the relative merits of evaluation vs. normalization, or treat continuations as meta-level objects. This paper presents a simple programming language interpreter that illustrates how this can be done. By keeping its expression evaluator entirely separate from the mechanisms that implement its inlinite tower, this interpreter avoids many troublesome aspects of previous self-referential programming languages. Given these basically straightforward techniques, processor towers might be easily constructed for a wide variety of systems to enable them to manipulate and reason about themselves.	brian;central processing unit;continuation;interpreter (computing);programming language;reification (computer science);self-reference	Alan Bawden	1988		10.1145/62678.62726	fourth-generation programming language;first-generation programming language;very high-level programming language;language primitive;programming domain;computer science;computer programming;programming paradigm;symbolic programming;fifth-generation programming language;programming language;algorithm	Arch	-23.62317764630318	24.29048284937902	8584
c72203b74f25dff15a29173df2c2753915da71dc	concept and pragmatics of an intuitive visualization-oriented metamodeling tool	case tools;tool support;business process modeling;business process model;visual modeling;case tool;design rationale;visual modeling languages;model driven architecture	In this article we present a metamodeling tool that is strictly oriented towards the needs of the working domain expert. The working domain expert looks for intuitive metamodeling features. In particular, these features include rich capabilities for specifying the visual appearance of models. Our research has identified an important design rationale for metamodeling tools that we call visual reification, which is the notion that metamodels are visualized the same way as their instances. Our tool supports both standard and innovative metamodeling features oriented towards the principle of visual reification. In this paper we present an unbiased discussion of the pragmatics of metamodeling tools against the background of this design rationale. & 2010 Elsevier Ltd. All rights reserved.	design rationale;metamodeling;reification (knowledge representation);subject-matter expert	Dirk Draheim;Melanie Himsl;Daniel Jabornig;Josef Küng;Werner Leithner;Peter Regner;Thomas Wiesinger	2010	J. Vis. Lang. Comput.	10.1016/j.jvlc.2010.03.002	metamodeling;metadata modeling;simulation;computer science;software engineering;management science;business process modeling	HCI	-53.35881907577545	22.36471189277572	8586
5bb3e8a0ada9c207e5ea1973700dd04c06352cc2	an optimal testing technique for finite state machines	clocks;testing;embedded system;automata;embedded systems;finite state machines;conformance testing;automata testing real time systems algorithm design and analysis clocks timing embedded systems;test sequences optimal testing technique finite state machines embedded systems real time systems;automata theory protocol engineering validation conformance testing timed automata;automata theory;validation;timed automata;experience base;algorithm design;protocol engineering;algorithm design and analysis;finite state machine;finite state machines conformance testing embedded systems;real time systems;real time and embedded systems;timing	This paper presents an algorithm able to generate test sequences for real-time and embedded systems. This algorithm is divided into 3 steps. The first step is less complex than the second one which has lower complexity than the third. The first step is able to generate test sequences for some states in the system. The second step generates test sequences for some of the remaining states. The last step derives test sequences for the states which have not been covered by the two first steps. Then, we analyse a large number of experiments based on this algorithm with different automata, and we deduce that in most of the cases the first step generates test sequences for almost 85 % of all states while the second derives approximatively 10 % and finally the third one derives for 5 % of the states. That proves the efficiency of our methodology since we have no mean to prove formally these results.	algorithm;automata theory;complexity;embedded system;experiment;finite-state machine;procedural generation;randomness;real-time clock;real-time computing;real-time transcription;test automation;timed automaton	Hacène Fouchal	2011	2011 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2011.5984036	algorithm design;real-time computing;computer science;theoretical computer science;finite-state machine;algorithm	Embedded	-11.297183392183396	29.42785109066763	8589
d576e76b7294f954f678aa0455481bb85bae55a9	towards a simple ontology definition language (sontodl) for a semantic web of evidence-based medical information	outil logiciel;informatica biomedical;software tool;ontologie;base donnee;biomedical data processing;glosario;aplicacion medical;red www;decision aid;genie biomedical;langage manipulation donnee;informatique biomedicale;lenguaje manipulacion dato;reseau web;database;base dato;hombre;ayuda decision;glossaire;biomedical engineering;herramienta controlada por logicial;human;aide decision;semantic web;world wide web;evaluation;ingenieria biomedica;medical application;information service;information system;evaluacion;analisis semantico;analyse semantique;glossary;systeme information;data manipulation language;semantic analysis;homme;application medicale;sistema informacion	This paper documents the development of a Simple Ontology Definition Language (SOntoDL). The development is part of a project aimed at the implementation of an ontology-based semantic navigation through the glossary of an evidence-based medical information service on the WWW. The latest version of SOntoDL is integrated with the RDF/RDFS framework thereby providing for the foundation of a Semantic Web of evidence-based medical information.	semantic web	Rolf Grütter;Claus Eikemeier	2001		10.1007/3-540-48229-6_45	upper ontology;open biomedical ontologies;semantic computing;semantic web rule language;bibliographic ontology;data manipulation language;ontology inference layer;computer science;ontology;evaluation;semantic web;social semantic web;data mining;semantic web stack;database;ontology-based data integration;world wide web;owl-s;information system;semantic analytics;suggested upper merged ontology	AI	-37.26216713807599	11.929708958453956	8590
aa91d1962812476e7afd0d1bb7f92fb9c70eef80	dynsgx: a privacy preserving toolset for dinamically loading functions into intel(r) sgx enclaves		Intel(R) Software Guard eXtensions (SGX) is a hardware-based technology for ensuring security of sensitive data from disclosure or modification that enables user-level applications to allocate protected areas of memory called enclaves. Such memory areas are cryptographically protected even from code running with higher privilege levels. This memory protection can be used to develop secure and dependable applications, but the technology has some limitations: (i) the code of an enclave is visible at load time, (ii) libraries used by the code must be statically linked, and (iii) the protected memory size is limited, demanding page swapping to be done when this limit is exceeded. We present DynSGX, a privacy preserving tool that enables users and developers to dynamically load and unload code to be executed inside SGX enclaves. Such a technology makes possible that developers use public cloud infrastructures to run applications based on sensitive code and data. Moreover, we present a series of experiments that assess how applications dynamically loaded by DynSGX perform in comparison to statically linked applications that disregard privacy of the enclave code at load time.	cloud computing;dependability;experiment;library (computing);loader (computing);memory protection;network enclave;overhead (computing);paging;privacy;programming language;programming model;protection ring;recursion (computer science);static build;static library;tee (command);user space	Rodolfo Silva;Pedro Barbosa;Andrey Brito	2017	2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)	10.1109/CloudCom.2017.42	guard (information security);real-time computing;operating system;data security;cloud computing;information privacy;software;swap (computer programming);computer science;memory protection	SE	-54.82768782325324	56.25999530857392	8592
44f3185fdd70da7893baffcde2dce0a0250ae371	sharing actions and attributes in modal action logic	distributed system	"""Distributed systems may be speciied in Structured Modal Action Logic by decomposing them into agents which interact by sharing attributes (memory) as well as actions. In the formalism we describe, speciication texts denote theories, and theories denote the set of semantic structures which satisfy them. The semantic structures are Kripke models, as is usual for modal logic. The \possible worlds"""" in a Kripke model are the states of the agent, and there is a separate relation on the set of states for each action term. Agents potentially share actions as well as attributes in a way controlled by locality annotations in the speciication texts. These become locality axioms in the logical theories the texts denote. These locality axioms provide a reened way of circumscribing the eeects of actions. Safety and liveness conditions are expressed (implicitly) by deontic axioms, which impose obligations and deny permissions on actions. We show that \deontic defaults"""" exist so that the speciier need not explicitly grant permissions or avoid obligations in situations where normative behaviour is not an issue."""	action algebra;formal system;kripke semantics;liveness;locality of reference;many-worlds interpretation;modal logic;peano axioms;principle of locality	Mark Ryan;José Luiz Fiadeiro;T. S. E. Maibaum	1991		10.1007/3-540-54415-1_65	theoretical computer science;distributed computing;communication;multimodal logic	AI	-16.755184666090372	6.333841939276863	8595
6e75f82e99ddff6e36ac50f90c6ba9a8895aeddc	five axioms of alpha-conversion	programming language;theorem prover;higher order logic	We present five axioms of name-carrying lambda-terms identified up to alpha-conversion—that is, up to renaming of bound variables. We assume constructors for constants, variables, application and lambdaabstraction. Other constants represent a function Fv that returns the set of free variables in a term and a function that substitutes a term for a variable free in another term. Our axioms are (1) equations relating Fv and each constructor, (2) equations relating substitution and each constructor, (3) alpha-conversion itself, (4) unique existence of functions on lambda-terms defined by structural iteration, and (5) construction of lambda-abstractions given certain functions from variables to terms. By building a model from de Bruijn’s nameless lambda-terms, we show that our five axioms are a conservative extension of HOL. Theorems provable from the axioms include distinctness, injectivity and an exhaustion principle for the constructors, principles of structural induction and primitive recursion on lambda-terms, Hindley and Seldin’s substitution lemmas and the existence of their length function. These theorems and the model have been mechanically checked in the Cambridge HOL system. The axioms presented in this paper are intended to give a simple, abstract characterisation of untyped lambda-terms, with constants, identified up to alphaconversion, that is, renaming of bound variables. We were led to develop these axioms because we are interested in representing the syntax of programming languages with binding operators within a theorem prover. The difficulty of correctly defining substitution on lambda-terms is notorious. Previous experience with the pi-calculus (Milner, Parrow, and Walker 1992) in HOL (Melham 1994) suggests that developing substitution and binding operators directly is a tedious and error-prone business. Instead, to avoid error and repetition, we advocate first developing a metatheory of untyped lambda-terms, and secondly deriving syntax for a particular programming language as abbreviations for untyped lambda-terms. We will show in section 4 how to do this for a finitary pi-calculus. Given higher-order logic, as implemented in the Cambridge HOL system (Gordon and Melham 1993), what we are after is a logical type (α)term that stands for the set of lambda-terms, where α is the type of constants. Terms are generated by the four constructors: Con : α → (α)term (constants) Var : string → (α)term (variables) App : (α)term → (α)term → (α)term (applications) Lam : string → (α)term → (α)term (lambda-abstractions) Consider the concrete recursive type—the free algebra—generated by these constructors. Concrete recursive types are implemented in HOL using Melham’s type definition package (Gordon and Melham 1993, Chapter 20). Given these constructors, the package proves the existence of a type characterized by the single axiom: ` ∀con : α → β. ∀var : string → β. ∀app : β → β → (α)term → (α)term → β. ∀lam : β → string → (α)term → β. ∃!hom : (α)term → β. ∀k. hom(Con k) = con k ∧ ∀x. hom(Var x) = var x ∧ ∀t u. hom(App t u) = app (hom t) (hom u) t u ∧ ∀xu. hom(Lam x u) = lam (hom u) x u The axiom allows for the definition of functions by primitive recursion, where functions con, var, app and lam determine the outcome of the function when applied to each constructor, given access to the outcome of recursive calls and to the arguments of the constructor. In fact Melham’s tool derives this axiom from a simpler iteration axiom. Iteration also allows for the definition of functions by recursion, but the functions con, var, app and lam have no direct access to the constructor arguments, only to the outcomes of recursive calls. Here the type (α)term is a free algebra; all the constructors are injective. Two lambda-abstractions are equal just if their bound variables and their bodies are equal. Instead we are after a type in which terms are identified up to alphaconversion, that is, in which two lambda-abstractions are equal just if their bodies are equal when the bound variables are renamed to a fresh variable.	amiga walker;automated theorem proving;cognitive dimensions of notations;de bruijn graph;emoticon;free variables and bound variables;hol (proof assistant);iteration;lam/mpi;lambda calculus;naruto shippuden: clash of ninja revolution 3;primitive recursive function;programming language;provable security;random access;recursion;recursive data type;sequent calculus;structural induction;type theory;π-calculus	Andrew D. Gordon;Thomas F. Melham	1996		10.1007/BFb0105404	higher-order logic;object language;computer science;automated theorem proving;programming language;logic programming;peano axioms;theory;second-order logic;algorithm	PL	-15.259847938881872	19.674449518987334	8600
045214e63e1681582f713f66dff0cdc6377f84e7	regulated multi-party communications and context awareness through the environment	article accepte pour publication ou publie;context aware;heredite et milieu;intellect philosophie;multi party communication;awareness;environment infrastructure;environnement;intelligence;communication;prise de conscience;information;intelligence periodique;multiagent systems	Recent research in the multi-agent systems field has highlighted the relevance of complex interaction models such as multi-party communication and context awareness. Nevertheless, there are no generic interaction model and infrastructure that enable to apply them in a standardized way. Emerging as a first-order abstraction, the environment, in the sense of a common medium for the agents, is a suitable paradigm to support these new interaction models. We present an operational model called Environment as Active Support of Interaction, that enables each agent to actively modify the environment according to its interaction needs. This model provides a suitable framework for the regulation of MAS interactions, and priority policies are given to manage the rules. An algorithm is proposed and assessed with an example stemming from the ambient intelligence domain.		Julien Saunier;Flavien Balbo	2009	Multiagent and Grid Systems	10.3233/MGS-2009-0120	intelligence;awareness;information;computer science;knowledge management;artificial intelligence;multi-agent system	HPC	-42.95055289536336	19.193160176079815	8603
ab64ac082af1ded129006fdc67fa40f254a41f7a	strategies to hide communication for a classical molecular dynamics proxy application	classical molecular dynamics;communication computation overlap;openmp;mpi;hybrid programming model	Issakar Ngatang and Masha Sosonkina Summary Co-designing applications and computer architectures has become of major importance due to the growing complexity of both applications and architectures and the need to better match application characteristics to the available hardware. Thus, “miniapplications”, which serve as proxies of large-scale ones by highlighting their most intensive parts and major workflow components, have appeared to the co-design, tuning, and adaptation purposes. This paper presents a work on optimizing the communication subsystem of a classical MD proxy (CoMD) application executed on multi-core computing clusters. The research focuses on hiding communication with certain buffer handling operations. In particular, two strategies are presented: one that uses two parallel threads for communication and buffer handling and another that introduces more parallelism by allowing all the available threads to unload the buffers while using two thread to communicate, thereby improving load balancing. The first proposed strategy yields performance gains up to 61% in the communication routines, corresponding to 6% gains in the overall time, while the second strategy achieves, respectively, about 73% and 6.3% improvement.Sy mposium Shared-Memory Parallelization of the Semi-Ordered Fast Iterative Method Josef Weinbub, Florian Dang, Tor Gillberg and Siegfried Selberherr Summary The semi-ordered fast iterative method allows to compute monotone front propagation of anisotropic nature by solving the eikonal equation. Compared to established iterative methods, such as the fast iterative method, the semi-ordered fast iterative method offers increased stability for variations in the front velocity. So far, the method has only been investigated in a serial, twodimensional context; therefore, we investigate in this work a parallelization approach via OpenMP and evaluate it for threedimensional problems, being especially of interest to real-world applications. We discuss the parallel algorithm and compare the performance as well as the computed solutions with an OpenMP-powered fast iterative method. We use different speed functions as well as varying problem sizes to investigate the impact of the computational load. We show that although the semi-ordered fast iterative method is inferior to the fast iterative method with respect to parallel efficiency, execution performance is significantly faster.	automatic parallelization;computation;computational complexity theory;computer architecture;iterative method;load balancing (computing);molecular dynamics;multi-core processor;openmp;parallel algorithm;parallel computing;performance tuning;semiconductor industry;shared memory;siegfried selberherr;software propagation;speedup;tor messenger;velocity (software development);monotone	Issakar Ngatang;Masha Sosonkina	2015			parallel computing;real-time computing;computer science;message passing interface;operating system;distributed computing	HPC	-4.734427011318923	39.218317677985745	8611
b3d9e5d01b7a6504b12aa8436bd3127d1e456862	hiptnt+: a termination and non-termination analyzer by second-order abduction - (competition contribution)		HipTNT+ is a modular termination and non-termination analyzer for imperative programs. For each given method, the analyzer first annotates it with an initial specification with second-order unknown predicates and then incrementally derives richer known specifications with case analysis. Subsequently, the final inference result indicates either conditional termination, non-termination, or unknown. During the proving process, new conditions for the case analysis are abductively inferred from the failure of both termination and non-termination proof, which aim to separate the terminating and non-terminating behaviors for each method. This paper introduces the verification approach and the structure of HipTNT+, and instructs how to set up and use the system.	non-functional requirement	Ton Chanh Le;Quang-Trung Ta;Wei-Ngan Chin	2017		10.1007/978-3-662-54580-5_25		NLP	-17.083361399739353	23.542077542462195	8612
2cd2118002831cccd37bc7f0a352214e75d3dfb2	improved wpm encoding for coalition structure generation under mc-nets		The Coalition Structure Generation (CSG) problem plays an important role in the domain of coalition games. Its goal is to create coalitions of agents so that the global welfare is maximized. To date, Weighted Partial MaxSAT (WPM) encoding has shown high efficiency in solving the CSG problem, which encodes a set of constraints into Boolean propositional logic and employs an off-the-shelf WPM solver to find out the optimal solution. However, in existing WPM encodings, a number of redundant encodings are asserted. This results in additional calculations and correspondingly incurs performance penalty. Against this background, this paper presents an Improved Rule Relation-based WPM (I-RWPM) encoding for the CSG problem, which is expressed by a set of weighted rules in a concise representation scheme called Marginal Contribution net (MC-net). In order to effectively reduce the constraints imposed on encodings, we first identify a subset of rules in an MC-net, referred as a set of freelance rules. We prove that solving the problem made up of all freelance rules can be achieved with a straightforward means without any extra encodings. Thus the set of rules requiring to be encoded is downsized. Next, we improve the encoding of transitive relations among rules. To be specific, compared with the existing rule relation-based encoding that generates transitive relations universally among all rules, I-RWPM only considers the transitivity among rules with particular relationship. In this way, the number of constraints to be encoded can be further decreased. Experiments suggest that I-RWPM significantly outperforms other WPM encodings for solving the same set of problem instances.		Xiaojuan Liao;Miyuki Koshimura;Kazuki Nomoto;Suguru Ueda;Yuko Sakurai;Makoto Yokoo	2018	Constraints	10.1007/s10601-018-9295-4	discrete mathematics;mathematical optimization;maximum satisfiability problem;encoding (memory);mathematics;transitive relation;propositional calculus;solver	AI	-19.006808687584552	14.363655517072973	8613
8ccaac59889e667f007d035f2aca4d6ef8ec93d0	fixing pages in database buffer	sorting;page pair graphs;mutual clustering;clustering;natural join;merging;relational algebra	Most of today's general purpose database management systems (DBMSs) are implemented on top of a standard multi-user operating system. In order to avoid a duplication of implementation efforts, they use many of the features of the operating system, such as file management, process scheduling, interprocess communication, etc. Since the operating system is not tailored to the needs of a DBMS, many practical problems and inefficiencies result from this approach. A comprehensive overview over these issues can be found in [St81].	database;inter-process communication;multi-user;operating system;scheduling (computing)	Wolfgang Effelsberg	1983	SIGMOD Record	10.1145/984523.984527	relational algebra;computer science;sorting;theoretical computer science;data mining;database;cluster analysis;sort-merge join	DB	-27.982254761315186	5.194563324096992	8614
55badaa81b1b4a87cfc718fd5a40e75b2a584d60	abstract flow learning for web application test generation		Achieving high software quality today involves manual analysis, test planning, documentation of testing strategy and test cases, and the development of scripts to support automated regression testing. To keep pace with software evolution, test artifacts must also be frequently updated. Although test automation practices help mitigate the cost of regression testing, a large gap exists between the current paradigm and fully automated software testing. Researchers and practitioners are realizing the potential for artificial intelligence and machine learning (ML) to help bridge the gap between the testing capabilities of humans and those of machines. This paper presents an ML approach that combines a language specification that includes a grammar that can be used to describe test flows, and a trainable test flow generation model, in order to generate tests in a way that is trainable, reusable across different applications, and generalizable to new applications.		Dionny Santiago;Peter J. Clarke;Patrick Alt;Xihua Lian	2018		10.1145/3278186.3278194	regression testing;software engineering;test case;software quality;test plan;programming language specification;software;software evolution;computer science;test strategy	SE	-56.77242773532405	32.372775612956325	8615
b66ec5774a89e7b7bc817ae490f10cac729e2c84	a scalable approach for the description of dependencies in hard real-time systems	real time;satisfiability;schedulability analysis;embedded system;hard real time system	During the design iterations of embedded systems, the schedulability analysis is an important method to verify whether the real-time constraints are satisfied. In order to achieve a wide acceptance in industrial companies, the analysis must be as accurate as possible and as fast as possible. The system context of the tasks has to be considered in order to accomplish an exact analysis. As this leads to longer analysis times, there is a tradeoff between accuracy and runtime. This paper introduces a general approach for the description of dependencies between tasks in distributed hard real-time systems that is able to scale the exactness and the runtime of the analysis. We will show how this concept can be applied to a real automotive application.	approximation algorithm;embedded system;holism;iteration;real-time clock;real-time computing;real-time locating system;real-time transcription;runtime system;scheduling analysis real-time systems	Steffen Kollmann;Victor Pollex;Kilian Kempf;Frank Slomka	2010		10.1007/978-3-642-16561-0_37	real-time computing;computer science;theoretical computer science;distributed computing;satisfiability	Embedded	-8.140266885332725	60.18638006250592	8654
3bb1056f9b887cacb3043e3d53a78693fe7b651e	type inference and extensionality	shape inference algorithms remuneration proposals;type checking;polymorphism;type theory;typability polymorphic type assignment system type inference extensionality type assignment counterpart type checking f decidable restrictions bounded type inference bounded type checking algorithm;formal logic;type inference;decidability;decidability type theory formal logic	The polymorphic type assignment system F2 is the type assignment counterpart of Girani’s and Reynolds ’ system F. Though introduced in the early seventies, both the type inference and the type checking problems for F2 remained open for a long time. Recently, an undecidabiliry result was announced. Consequently, it is considerably interesting to find decidable restrictions of the system. We show a bounded type inference and a bounded type checking algorithm, both based on the study of the telationship between the typability of a term and the typability of terms that “properly” q-reduce to it.	algorithm;system f;type inference;type system	Adolfo Piperno;Simona Ronchi Della Rocca	1994		10.1109/LICS.1994.316072	decidability;type class;polymorphism;parametric polymorphism;combinatorics;discrete mathematics;unit type;computer science;recursive data type;type inference;mathematics;hindley–milner type system;programming language;kind;logic;type theory;algorithm	PL	-11.788083926114247	18.281153426391867	8657
5a2fe81d7c40f43eaacded56444a721b0eaec2a3	observability of transportation systems - a methodology for reliability analysis in logistics and manufacturing		Real world events are observed by sensors since decades, for instance in the logistics where packages are identified and tracked. This information result in an information flow. This information flow is used to control the physical material flow. Hence, the information flow is a digital representation of the physical material flow. However, to guarantee that the digital representation is in alignment to the physical world is a challenging task. Especially for scenarios with manual operations, the representation is vulnerable for errors. This paper proposes a generic approach to assure consistency between digital and physical world. The paper presents a methodology to model the monitoring of physical entities and to analyse the model to evaluate the risk of unreliable digital representation.	entity;logistics;material flow;reliability engineering;sensor	Jan Pinkowski;Axel Hahn	2012			engineering;observability;reliability engineering;systems engineering	AI	-54.12431254127956	45.83999655059552	8669
06584675f04c58156e2608ca39277b2ba815cc8a	the slgad procedure for inference on logic programs with annotated disjunctions	perforation;well founded semantics;logic programs;probabilistic logic;conditional probability	Logic Programs with Annotated Disjunctions (LPADs) allow to express probabilistic information in logic programming. The semantics of an LPAD is given in terms of well founded models of the normal logic programs obtained by selecting one disjunct from each ground LPAD clause. The paper presents SLGAD resolution that computes the (conditional) probability of a ground query from an LPAD and is based on SLG resolution for normal logic programs. The performances of SLGAD are evaluated on classical benchmarks for normal logic programs under the well founded semantics, namely the stalemate game and the ancestor relation. SLGAD is compared with Cilog2 and SLDNFAD, an algorithm based on SLDNF, on the programs that are modularly acyclic. The results show that SLGAD deals correctly with cyclic programs and, even if it is more expensive than SLDNFAD on problems where SLDNFAD succeeds, is faster than Cilog2 when the query is true in an exponential number of instances. Topics: Probabilistic Logic Programming, Well Founded Semantics, Logic Programs with Annotated Disjunctions, SLG resolution.	approximation algorithm;directed acyclic graph;experiment;logic programming;performance;time complexity;top-down and bottom-up design;whole earth 'lectronic link	Fabrizio Riguzzi	2008			predicate logic;dynamic logic;discrete mathematics;description logic;higher-order logic;horn clause;stable model semantics;many-valued logic;intermediate logic;theoretical computer science;bunched logic;predicate functor logic;mathematics;probabilistic logic;axiomatic semantics;well-founded semantics;logic programming;probabilistic logic network;multimodal logic;algorithm;autoepistemic logic	AI	-16.838504658427283	12.999063884393529	8686
b5bffccfe7733a118b3de7d00896e4298adc8444	virtual memory management for interactive continuous media applications	libraries;storage allocation;virtual memory management system;page allocation;timing constraints;virtual memory;paged storage;secondary disks;memory management;motion pictures;videoconference;secondary storage;continuous media;storage management;multimedia systems;technology management;stack segments;wire;multimedia computing;data storage;operating system;interactive continuous media applications;virtual address spaces;memory management wiring timing operating systems wire videoconference libraries technology management multimedia systems motion pictures;wired memory virtual memory management system interactive continuous media applications data storage stack segments operating systems demand paging secondary disks timing constraints secondary storage memory wiring primitives virtual address spaces page allocation;memory wiring primitives;wiring;storage allocation paged storage storage management interactive systems multimedia computing operating systems computers timing;wired memory;interactive systems;operating systems computers;demand paging;operating systems;timing;time constraint	This paper proposes a virtual memory management system suitable for interactive continuous media applications. Interactive continuous media applications usually require a large amount of memory for storing their code, data, and stack segments. In traditional operating systems, demand paging makes it possible to execute such large applications by storing most pages in secondary disks. However, continuous media applications should avoid page faults for ensuring timing constraints of continuous media since it takes a long time to swap pages between physical memory and secondary storages. Thus, it is di cult to satisfy timing constraints of continuous media. Therefore, some operating systems provide memory wiring primitives that enable applications to wire pages in physical memory by specifying the range of virtual address spaces explicitly. On the other hand, our virtual memory management system enables continuous media applications to reserve physical memory for allocating pages as soon as possible when the applications require the pages. The system implicitly and incrementally allocates and wires pages used for processing timing critical media data. Also, our system supports applications that adapt the amount of wired memory to the memory usages of other continuous media applications.	computer data storage;memory management;operating system;paging;wiring	Tatsuo Nakajima;Hiroshi Tezuka	1997		10.1109/MMCS.1997.609752	auxiliary memory;uniform memory access;demand paging;embedded system;interleaved memory;real-time computing;computer hardware;computer science;physical address;virtual memory;technology management;operating system;computer data storage;memory protection;overlay;extended memory;flat memory model;videoconferencing;world wide web;data diffusion machine;memory map;computer network;memory management	OS	-19.25973809484643	50.585542482882325	8697
5f3066829e20358838b307513130129170efe84f	high fidelity data reduction for big data security dependency analyses	dependency analysis;intrusion detection;data reduction;forensics	"""Intrusive multi-step attacks, such as Advanced Persistent Threat (APT) attacks, have plagued enterprises with significant financial losses and are the top reason for enterprises to increase their security budgets. Since these attacks are sophisticated and stealthy, they can remain undetected for years if individual steps are buried in background """"noise."""" Thus, enterprises are seeking solutions to """"connect the suspicious dots"""" across multiple activities. This requires ubiquitous system auditing for long periods of time, which in turn causes overwhelmingly large amount of system audit events. Given a limited system budget, how to efficiently handle ever-increasing system audit logs is a great challenge. This paper proposes a new approach that exploits the dependency among system events to reduce the number of log entries while still supporting high-quality forensic analysis. In particular, we first propose an aggregation algorithm that preserves the dependency of events during data reduction to ensure the high quality of forensic analysis. Then we propose an aggressive reduction algorithm and exploit domain knowledge for further data reduction. To validate the efficacy of our proposed approach, we conduct a comprehensive evaluation on real-world auditing systems using log traces of more than one month. Our evaluation results demonstrate that our approach can significantly reduce the size of system logs and improve the efficiency of forensic analysis without losing accuracy."""	algorithm;big data;data security;display resolution;list of code lyoko episodes;tracing (software)	Zhang Xu;Zhenyu Wu;Zhichun Li;Kangkook Jee;Junghwan Rhee;Xusheng Xiao;Fengyuan Xu;Haining Wang;Guofei Jiang	2016		10.1145/2976749.2978378	intrusion detection system;data reduction;computer science;data mining;forensic science;world wide web;computer security;dependence analysis	Security	-61.131581316250276	53.85623590700733	8699
554bacf178315237f880a4f0381506df35588c4d	a low-cost green it design and application of vhsp based on virtualization technology	resource utilization;honeypot virtual honeynet security platform vhsp virtualization technology vt honeynet;xen paravirtualization;virtualization technology vt;honeynet;virtualization technology;virtual research platform;computer architecture;application virtualization space technology costs information security hardware operating systems application software computer science technology management physics computing;honeypot;monitoring;green it design;vhsp;virtual research platform green it design vhsp virtualization technology virtual honeynet security platform xen paravirtualization;driver circuits;fires;security;security of data;virtual honeynet security platform;operating systems;virtual honeynet security platform vhsp	Honeynet includes various Honeypot, thus it has managerial limitations in the flexibility, time limits, technological integrations and dynamic deployment. In addition, the S/W and H/W resource utilization is comparatively low, and meanwhile it also lacks for the effective application of strategic integration; therefore, there are still a lot of space should be improved. In this work, we proposed a Virtual Honeynet Security Platform (VHSP), we utilize the integration of the Xen paravirtualization and Honeynet technologies to design a virtual research platform and implement such platform to verify the effectiveness of the virtual architecture and method that designed in this paper. The main contribution of this work is not only reduces the complexity for the great deployment in traditional Honeynet system, but also decrease the limitation in spatial region for the physical Honeynet environment, and this conform to the concept of Green IT.	honeynet project;honeypot (computing);software deployment;x86 virtualization	Chih-Hung Chang	2009	2009 International Conference on Computational Science and Engineering	10.1109/CSE.2009.322	embedded system;computer science;information security;operating system;honeypot;computer security;computer network	DB	-49.285492312236926	57.464184054755364	8708
dbe7401677f90c518b4d5df410304eabd0200716	alternative characterizations for program equivalence under answer-set semantics based on unfounded sets	answer sets;program optimization;answer set semantics;propositional logic;logic programs;equivalence checking;problem solving;model theory	Logic programs under answer-set semantics constitute an important tool for declarative problem solving. In recent years, two research issues received growing attention. On the one hand, concepts like loops and elementary sets have been proposed in order to extend Clark’s completion for computing answer sets of logic programs by means of propositional logic. On the other hand, different concepts of program equivalence, like strong and uniform equivalence, have been studied in the context of program optimization and modular programming. In this paper, we bring these two lines of research together and provide alternative characterizations for different conceptions of equivalence in terms of unfounded sets, along with the related concepts of loops and elementary sets. Our results yield new insights into the model theory of equivalence checking. We further exploit these characterizations to develop novel encodings of program equivalence in terms of standard and quantified propositional logic, respectively.	control flow;formal equivalence checking;logic programming;mathematical optimization;modular programming;problem solving;program optimization;propositional calculus;stable model semantics;turing completeness	Martin Gebser;Torsten Schaub;Hans Tompits;Stefan Woltran	2008		10.1007/978-3-540-77684-0_5	zeroth-order logic;logical equivalence;stable model semantics;computer science;intermediate logic;program optimization;formal equivalence checking;propositional variable;propositional calculus;programming language;algorithm;model theory;autoepistemic logic	AI	-15.563658455183983	17.48153069292522	8724
cb2575c18a411c5994675b5d9ef17e900f1be574	on checkpointing and heavy-tails in unreliable computing environments	heavy tail	In this paper, we discuss checkpointing issues that should be considered whenever jobs execute in unreliable computing environments. Specifically, we show that if proper check-pointing procedures are not properly implemented, then under certain conditions, job completion time distributions exhibit properties of heavy-tail or power-tail distributions (hereafter referred to as power-tail distributions (PT), which can lead to highly-variable and long completion times.	application checkpointing;job stream;tails	Craig Bossie;Pierre M. Fiorini	2006	SIGMETRICS Performance Evaluation Review	10.1145/1168134.1168142	real-time computing;heavy-tailed distribution;computer science;distributed computing;statistics	HPC	-15.903669891126151	60.294361539925596	8728
2c343e924cb3b8bbd871f880d3b04f295763b0cd	dynamic file migration in distributed computer systems	file transfer;distributed system;sistema operativo;hospital information system;systeme reparti;articulo sintesis;protocole transmission;article synthese;estudio comparativo;sistema informatico;transferencia fichero;gestion fichier;computer system;file management;etude comparative;protocolo transmision;sistema repartido;dynamic allocation;operating system;transfert fichier;distributed computing system;manejo archivos;comparative study;systeme exploitation;systeme informatique;asignacion dinamica;allocation dynamique;review;migration policy;transmission protocol	The importance of file migration is increasing because of its potential to improve the performance of distributed office, manufacturing and hospital information systems. To encourage research in the file migration problem, the authors summarize accomplishments of researchers of the problem, provide a detailed comparison of file migration and dynamic file allocation problems, and identify important areas of research to support the development of effective file migration policies.	information system	Bezalel Gavish;Olivia R. Liu Sheng	1986		10.1145/75577.75583	embedded system;simulation;telecommunications;computer science;operating system;comparative research;global namespace	HPC	-19.054356969432583	43.62491586386123	8733
4fd9e46ffa6479c01ac736d85902cb8c19e76a5e	modeling anatomical spatial relations with description logics	computer simulation;anatomy;logic	Although spatial relations are essential for the anatomy domain, spatial reasoning is only weakly supported by medical knowledge representation systems. To remedy this shortcoming we express spatial relations that can intuitively be applied to anatomical objects (such as 'disconnected', 'externally connected', 'partial overlap' and 'proper part') within the formal framework of description logics. A special encoding of concept descriptions (in terms of SEP triplets) allows us to emulate spatial reasoning by classification-based reasoning.	anatomic structures;description logic;knowledge representation and reasoning;mereology;physical object;spatial–temporal reasoning;symantec endpoint protection	Stefan Schulz;Udo Hahn;Martin Romacker	2000	Proceedings. AMIA Symposium		spatial intelligence;encoding (memory);theoretical computer science;machine learning;knowledge representation and reasoning;description logic;artificial intelligence;spatial relation;computer science	AI	-20.48539177296193	6.5579018194826775	8745
fe81f2b4214756407338439a1e6e1d5e85c3a014	prioritization of test scenarios derived from uml activity diagram using path complexity	activity diagram;software testing;control flow graph;path complexity;test scenario prioritization	This paper presents a novel approach for prioritizing test scenarios generated from UML 2.0 activity graph using path complexity. Activity Diagram is used as it is available at an early stage of the software development life cycle allowing us to detect faults at early stages, hence reducing the overall time and effort required for testing. In the proposed approach, activity diagram is converted into control flow graph and then test scenarios are derived from it using basis path method. The methodology adopted for prioritizing test scenarios is based on path complexity using the concept of path length, information flow metric, predicate node and multiple condition coverage.		Preeti Kaur;Priti Bansal;Ritu Sibal	2012		10.1145/2381716.2381783	reliability engineering;basis path testing;real-time computing;computer science;systems engineering	SE	-57.63521130971355	31.020347196761207	8749
25dc6b0a19d1dcc90fa24bbef365a3e3cb3ac559	a compositional approach to superimposition	distributed programs;concurrent programs	A general definition of the notion of superimposition is presented. We show that previous constructions under the same name can be seen as special cases of our definition. We consider several properties of superimposition definable in our terms, notably the nonfreezing property. We also consider a syntactic representation of our construct in CSP		Luc Bougé;Nissim Francez	1988		10.1145/73560.73581	computer science;programming language;algorithm	Logic	-10.197542713810199	20.015334316607262	8761
5dd3a44c96bcef48c913340b082b3d24812eeebe	combining behavioural types with security analysis	security properties;session types;communication centred computing;behavioural types;qa75 electronic computers computer science;distributed systems	Today’s software systems are highly distributed and interconnected, and they increasingly rely on communication to achieve their goals; due to their societal importance, security and trustworthiness are crucial aspects for the correctness of these systems. Behavioural types, which extend data types by describing also the structured behaviour of programs, are a widely studied approach to the enforcement of correctness properties in communicating systems. This paper offers a unified overview of proposals based on behavioural types which are aimed at the analysis of security properties.	abstraction layer;access control;cobham's thesis;column (database);compiler;conformance testing;correctness (computer science);cryptography;data integrity;data security;deadlock;distributed computing;essence;gradual typing;high- and low-level;lambda lifting;legacy code;linear logic;liveness;ocean observatories initiative;process (computing);proof-carrying code;public key certificate;requirement;run time (program lifecycle phase);scalability;semiconductor consolidation;software system;source input format;strand (programming language);theory;top-down and bottom-up design;trust (emotion);type system;whole earth 'lectronic link	Massimo Bartoletti;Ilaria Castellani;Pierre-Malo Deniélou;Mariangiola Dezani-Ciancaglini;Silvia Ghilezan;Jovanka Pantovic;Jorge A. Pérez;Peter Thiemann;Bernardo Toninho;Hugo Torres Vieira	2015	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.09.003	computer science;theoretical computer science;distributed computing;management science	PL	-46.57887981019294	53.3675564587943	8762
630fabe85e7a62c48b5241a8f44b903232019447	bluetooth security - an overview			bluetooth	Joakim Persson;Ben J. M. Smeets	2000	Inf. Sec. Techn. Report	10.1016/S1363-4127(00)03005-3	computer network	DB	-45.98426034720953	49.61467328631618	8766
06137d979fbc494c298d21259ae42d28bf283458	grammar design with multi-tape automata and composition		In this paper we show how traditional composition-based finite-state grammars can be augmented to preserve intermediate results in a chain of compositions. These intermediate strings can be very helpful for various tasks: enriching information while parsing or generating, providing accurate information for debugging purposes as well as offering explicit alignment information between morphemes and tags in morphological grammars. The implementation strategies discussed in the paper hinge on a representation of multi-tape automata as a single-tape automaton. A simple composition algorithm for such multitape automata is provided.	algorithm;automaton;debugging;parsing	Mans Hulden	2015			deterministic context-free grammar;programming language;affix grammar;operator-precedence grammar;computer science;regular tree grammar;grammar systems theory;adaptive grammar;automata theory;mildly context-sensitive grammar formalism	NLP	-23.054759666950797	17.96067762875844	8768
f0c463c53b0ad217e2a630318c80d90d6dbdda94	term models of horn clauses over rational pavelka predicate logic		This paper is a contribution to the study of the universal Horn fragment of predicate fuzzy logics, focusing on the proof of the existence of free models of theories of Horn clauses over Rational Pavelka predicate logic. We define the notion of a term structure associated to every consistent theory T over Rational Pavelka predicate logic and we prove that the term models of T are free on the class of all models of T. Finally, it is shown that if T is a set of Horn clauses, the term structure associated to T is a model of T.	fuzzy logic;horn clause;theory	Vicent Costa;Pilar Dellunde	2017	2017 IEEE 47th International Symposium on Multiple-Valued Logic (ISMVL)	10.1109/ISMVL.2017.26	arithmetic;discrete mathematics;horn-satisfiability;horn clause;mathematics;predicate variable;algorithm	Logic	-11.937651244630253	13.685145199949078	8771
022586536a752480a4abc9072b7f3396d75c7d1d	problem decomposition and sub-model reconciliation of control systems in event-b	software;control systems;problem decomposition shared event reconciliation step automotive cruise control system functional requirement document shared variable reconciliation step event b formal language four variable model decomposition problem frame approach control system submodel reconciliation submodel reconciliation;complexity theory;formal languages;monitoring;abstracts;mathematical model;control engineering computing;vehicles;control systems monitoring abstracts software mathematical model vehicles complexity theory;formal languages control engineering computing	To break the complexity of the formalisation process, we propose to model a functional requirement document of a control system as composeable monitored, controlled, mode and commanded sub-models. Influenced by the problem frame approach and the decomposition of the four-variable model, we suggest decomposing requirements of a control system into monitored, controlled, mode and commanded sub-problems. Each sub-problem can be formalised in a step-wise manner as a separate sub-model. To introduce the phenomena shared amongst the subproblems, the sub-models are reconciled. We propose a reconciliation process in the Event-B formal language based on the shared-variable and the shared-event styles which were originally developed for a model decomposition. The advantages and disadvantages of shared-variable and the shared-event reconciliation steps are also discussed. The requirements of an automotive cruise control system are decomposed and formalised as sub-models. These sub-models are also reconciled to introduce shared phenomena.	b-method;composability;control system;event (computing);file system permissions;formal language;functional requirement;overhead (computing);read-only memory;read-write memory;refinement (computing);ruby document format;sensor;shared variables;user interface	Sanaz Yeganefard;Michael J. Butler	2013	2013 IEEE 14th International Conference on Information Reuse & Integration (IRI)	10.1109/IRI.2013.6642515	formal language;simulation;computer science;control system;artificial intelligence;theoretical computer science;operating system;machine learning;mathematical model;data mining;database;distributed computing;programming language;statistics	Robotics	-32.88703297153105	34.157202198542215	8774
10d3e0f0648d0a5cfaebb3044ea7b14a52e54466	predicting unroll factors using supervised classification	optimising compilers;instruction level parallel;pipeline processing machine learning humans optimizing compilers support vector machines support vector machine classification history computer science artificial intelligence laboratories;learning algorithm;high dimensionality;supervised learning;heuristic programming;program control structures;supervised classification;heuristic programming program compilers machine learning loop unrolling branch prediction instruction level parallelism open research compiler supervised learning spec 2000 benchmark suite optimising compilers;heuristic programming program control structures optimising compilers learning artificial intelligence parallelising compilers;model complexity;machine learning;parallelising compilers;floating point;learning artificial intelligence;open research compiler	Compilers base many critical decisions on abstracted architectural models. While recent research has shown that modeling is effective for some compiler problems, building accurate models requires a great deal of human time and effort. This paper describes how machine learning techniques can be leveraged to help compiler writers model complex systems. Because learning techniques can effectively make sense of high dimensional spaces, they can be a valuable tool for clarifying and discerning complex decision boundaries. In this work we focus on loop unrolling, a well-known optimization for exposing instruction level parallelism. Using the Open Research Compiler as a testbed, we demonstrate how one can use supervised learning techniques to determine the appropriateness of loop unrolling. We use more than 2,500 loops - drawn from 72 benchmarks - to train two different learning algorithms to predict unroll factors (i.e., the amount by which to unroll a loop) for any novel loop. The technique correctly predicts the unroll factor for 65% of the loops in our dataset, which leads to a 5% overall improvement for the SPEC 2000 benchmark suite (9% for the SPEC 2000 floating point benchmarks).	algorithm;benchmark (computing);compiler;complex systems;instruction-level parallelism;loop unrolling;machine learning;mathematical optimization;norm (social);open research;parallel computing;supervised learning;testbed	Mark Stephenson;Saman P. Amarasinghe	2005	International Symposium on Code Generation and Optimization	10.1109/CGO.2005.29	parallel computing;computer science;floating point;theoretical computer science;operating system;machine learning;loop unrolling;supervised learning;programming language	Arch	-16.901693971819867	37.664328476072235	8777
3c9ab34388fd9226c7a7b243a00c64bd1146e763	fast portable orthogonally persistent java	portable orthogonally persistent java	A powerful feature of the Java programming languageis its user-definableclassloading policy, which when combined with the namespaceindependencebetween class loaders, allows portable implementation of semi-dynamic program transformations. Suchtransformations canbeusedfor a rangeof purposes,including optimization and semanticextension. In this paper wepresenta framework for semanticextensionsin Java. This framework consistsof a number of simplebut powerful transformations that, amongother things, allow us to semanticallyextendJava to provide orthogonal persistence. The use of semi-dynamic program transformations lends our orthogonally persistent Java a number of important qualities, including simplicity, portability and a cleanmodel of persistence.Significantly, our implementationsare efficient and can outperform in somecasesPJama , a well-known orthogonally persistent Java, which is basedon a modified virtual machine. In addition to describing the application of thesetransformations to orthogonally persistentJava, we foreshadow their usein a number of other contexts,including dynamic instanceversioning and instrumentation. Copyright c 1999John Wiley & Sons,Ltd.	application programming interface;byte;c++;certificate authority;gemstone/s object server;java;john d. wiley;lazy evaluation;lecture notes in computer science;mathematical optimization;optimizing compiler;primer;persistence (computer science);pointer swizzling;program transformation;semiconductor industry;software development kit;stanley (vehicle);swizzling (computer graphics);type system;vldb;virtual machine	Alonso Marquez;John N. Zigman;Stephen M. Blackburn	2000	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(20000410)30:4%3C449::AID-SPE306%3E3.0.CO;2-Y	persistence;java api for xml-based rpc;real-time computing;plug-in;jsr 94;java concurrency;separation of concerns;computer science;engineering;virtual machine;theoretical computer science;operating system;java modeling language;interface;strictfp;embedded java;real time java;virtual reality;programming language;java;transformer;generics in java;scala;java applet;java annotation	PL	-29.105492399144314	28.593901623263598	8785
d53506855db800e4a6b27b7d067519df9246befc	demo: medusa: a programming framework for crowd-sensing applications	capabilities motivates crowd-sensing;crowd-sourcing system;medusa task description;crowd-sensing application;high-level abstraction;mobile device;novel programming framework;runtime system;mobile phone user;crowd-sensing task;novel requirement;programming framework;process capability;satisfiability	The ubiquity of smartphones and their on-board sensing capabilities motivates crowd-sensing, a capability that harnesses the power of crowds to collect sensor data from a large number of mobile phone users. Unlike previous work on wireless sensing, crowd-sensing poses several novel requirements: support for humans-in-the-loop to trigger sensing actions or review results, the need for incentives, as well as privacy and security. Beyond existing crowd-sourcing systems, crowd-sensing exploits sensing and processing capabilities of mobile devices. In this paper, we design and implement Medusa, a novel programming framework for crowd-sensing that satisfies these requirements. Medusa provides high-level abstractions for specifying the steps required to complete a crowd-sensing task, and employs a distributed runtime system that coordinates the execution of these tasks between smartphones and a cluster on the cloud. We have implemented ten crowd-sensing tasks on a prototype of Medusa. We find that Medusa task descriptions are two orders of magnitude smaller than standalone systems required to implement those crowd-sensing tasks, and the runtime has low overhead and is robust to dynamics and resource attacks.	crowdsourcing;high- and low-level;medusa4;mobile device;mobile phone;on-board data handling;overhead (computing);prototype;requirement;runtime system;smartphone	Moo-Ryong Ra;Bin Liu;Thomas F. La Porta;Ramesh Govindan	2012		10.1145/2307636.2307693	embedded system;real-time computing;process capability;computer science;software framework;operating system;mobile device;computer security;satisfiability	Mobile	-40.866317611171596	49.349836713938075	8791
520e35df3444a187771b7fec9b22c703bbcaa708	handling sensing failures with partial causal models	causal models		causal filter;failure	Robin R. Murphy;David Hershberger	1996			reliability engineering;causal model;computer science	AI	-24.136070918505737	14.992894867351078	8806
8bc5d06d2a6033ecfb39d373a663bee2b28fee52	crowdbuy: privacy-friendly image dataset purchasing via crowdsourcing		In recent years, advanced machine learning techniques have demonstrated remarkable achievements in many areas. Despite the great success, one of the bottlenecks in applying machine learning techniques in real world applications lies in the lack of a large amount of high-quality training data from diverse domains. Meanwhile, massive personal data is being generated by mobile devices and is often underutilized. To bridge the gap, we propose a general dataset purchasing framework, named CrowdBuy and CrowdBuy++, based on crowdsourcing, with which a buyer can efficiently buy desired data from available mobile users with quality guarantee in a way respecting users' data ownership and privacy. We present a complete set of tools including privacy-preserving image dataset quality measurements and image selection mechanisms, which are budget feasible, truthful and highly efficient for mobile users. We conducted extensive evaluations of our framework on large-scale images and demonstrate that the system is capable of crowdsourcing high quality datasets while preserving image privacy with little computation and communication overhead.	bottleneck (software);computation;crowdsourcing;display resolution;machine learning;mobile device;overhead (computing);personally identifiable information;privacy;purchasing	Lan Zhang;Yannan Li;Xiang Xiao;Xiang-Yang Li;Junjun Wang;Sara Campinoti;Qiang Li	2018	IEEE INFOCOM 2018 - IEEE Conference on Computer Communications	10.1109/INFOCOM.2018.8485902	data mining;distributed computing;computation;feature extraction;computer science;information privacy;mobile device;crowdsourcing;purchasing;training set	Vision	-33.07993073706223	20.570155579603888	8811
2112a1d481e40b9d28d1dd148ca688632f21c47d	byzantine fault tolerance for agent systems	protocols;software fault tolerance;client server systems;telegraphy;autonomous agent behavior byzantine fault tolerance agent system server client model communicating agents;communicating agents;telephony;software agents;protection;fault tolerant systems;byzantine fault tolerance;agent system;fault tolerance;web services;byzantine fault tolerant;agent systems;ip networks;fault tolerant systems ip networks fault tolerance timing protection protocols telegraphy telephony laboratories web services;autonomous agent behavior;server client model;software fault tolerance client server systems software agents;timing	This paper presents a Byzantine fault tolerance method for agent systems. We extend Castro and Liskov's well-known practical Byzantine fault tolerance method for the server-client model to a method for the agent system model. There are two main differences between the methods. First, in agent systems we have to create replicas on both sides of the communicating agents, while in the server-client model of Castro and Liskov's method, replicas are created only on the server side, and the client is assumed to be non-faulty or is treated differently from a replica model. Second, due to the autonomous behavior of agents, we have to synchronize the timing of the receiving of messages among replicas. Agents decide their actions based on their current state of knowledge and do not wait indefinitely for messages that may not reach them	autonomous robot;byzantine fault tolerance;client–server model;server (computing);server-side	Tadashi Araragi	2006	2006 International Conference on Dependability of Computer Systems	10.1109/DEPCOS-RELCOMEX.2006.11	real-time computing;engineering;quantum byzantine agreement;distributed computing;computer security	Robotics	-42.042349304128045	55.91327916953489	8814
6f740456350c351dfeebb715ee76205e1b5e65ce	blasting linux code	important error;computer program;kernel developer;memory safety;device driver;linux kernel code;refinement technique;case study;blasting linux code;popular software model checker;analysing pointer	Computer programs can only run reliably if the underlying operating system is free of errors. In this paper we evaluate, from a practitioner’s point of view, the utility of the popular software model checker Blast for revealing errors in Linux kernel code. The emphasis is on important errors related to memory safety in and locking behaviour of device drivers. Our conducted case studies show that, while Blast’s abstraction and refinement techniques are efficient and powerful, the tool has deficiencies regarding usability and support for analysing pointers, which are likely to prevent kernel developers from using it.	blast;device driver;documentation;kernel (operating system);linux;lock (computer science);memory debugger;memory safety;model checking;operating system;parsing;pointer (computer programming);refinement (computing);software bug;software developer;software development;structured programming;usability	Jan Tobias Mühlberg;Gerald Lüttgen	2006		10.1007/978-3-540-70952-7_14	real-time computing;computer hardware;computer science;operating system;programming language	OS	-56.53287873663666	38.92746762548803	8815
0cf0f3095b7140e57bce2c11848b3529ded543c7	equivalence operators in nilpotent systems	t transitivity;equivalence;associativity;łukasiewicz operator system;bounded system;nilpotent operator	A consistent connective system generated by nilpotent operators is not necessarily isomorphic to the Łukasiewicz system. Using more than one generator function, consistent nilpotent connective systems (so-called bounded systems) can be obtained with the advantage of three naturally derived negation operators and thresholds. In this paper, equivalences in bounded systems are examined. Here, three different types of operators are studied, and a paradox of the equivalence (i.e. there is no equivalence relation in a non-Boolean setting which fulfils źx e ( x , x ) = 1 and e ( x , n ( x ) ) = 0 ) is resolved by aggregating the implication-based equivalence and its dual operator. We will also show that the aggregated equivalence has nice properties like associativity, threshold transitivity and T-transitiviy.	turing completeness	József Dombi;O. Csiszár	2016	Fuzzy Sets and Systems	10.1016/j.fss.2015.08.012	logical equivalence;equivalence;mathematical analysis;discrete mathematics;topology;associative property;mathematics	Logic	-8.303441518960515	10.654303235236439	8820
5dd784682d331dbd2df9cda3c227528d7ca42228	the mathworks distributed and parallel computing tools for signal processing applications	distributed application;distributed algorithms;distributed computing distributed algorithms parallel processing computation time;signal processing parallel processing;distributed computing;parallel processing signal processing distributed computing application software matlab equalizers concurrent computing computer applications signal processing algorithms maximum likelihood estimation;signal processing;parallel computer;computer application;computation time;coarse grained;distributed algorithm;parallel applications;parallel processing;embarrassingly parallel applications mathworks distributed computing tool parallel computing tool signal processing applications technical computing applications computational intensity coarse grained parallel application	As requirements for technical computing applications become more complex, engineers and scientists must solve problems of increasing computational intensity that frequently outstrip the capability of their own computers. Some are distributed applications (also called coarse-grained or embarrassingly parallel applications), where the same algorithm is independently executed over and over on different input parameters. Others consist of parallel (or fine-grained) applications, which contain interdependent tasks that exchange data during the execution of the application. This article introduces the distributed and parallel computing capabilities in The MathWorks distributed computing tools and provides examples of how these capabilities are applied to signal processing applications.	algorithm;client (computing);computation;computer;distributed computing;embarrassingly parallel;interactivity;interdependence;matlab;parallel computing;prototype;requirement;signal processing;simulink	Ali Behboodian;Silvina Grad-Freilich;Grant Martin	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367287	distributed algorithm;parallel computing;embarrassingly parallel;computer science;theoretical computer science;massively parallel;data-intensive computing;distributed computing;parallel algorithm;distributed design patterns;unconventional computing;cost efficiency;distributed concurrency control	HPC	-10.035839143657132	39.414653526214636	8821
48a106bed3cf75b46c87d5e52f14b85260533e07	internet of things (iot): operating system, applications and protocols design, and validation techniques		Abstract By combining energy efficient micro-controllers, low-power radio transceivers, and sensors as well as actuators in so called smart objects , we are able to connect the digital cyber world with the physical world as in cyber physical systems. In the vision of the Internet of Things, these smart objects should be seamlessly integrated into the traditional Internet. Typically, smart objects are heavily constrained in terms of computation, memory and energy resources. Furthermore, the commonly used wireless links among smart objects or towards the Internet are typically slow and subject to high packet loss. Such characteristics pose challenges, on one hand in terms of software running on smart objects, and on the other hand in terms of network protocols which smart objects use to communicate. New operating systems, application programming interfaces, frameworks, and middleware have to be designed with consideration of such constraints. In consequence, novel validation methods and experimental tools are needed to study smart object networks in vivo, new software platforms are needed to efficiently operate smart objects, and innovative networking paradigms and protocols are required to interconnect smart objects.	internet of things;operating system	Yousaf Bin Zikria;Heejung Yu;Muhammad Khalil Afzal;Mubashir Husain Rehmani;Oliver Hahm	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.07.058	application programming interface;communications protocol;software;efficient energy use;the internet;operating system;cyber-physical system;smart objects;distributed computing;computer science;middleware	Arch	-38.14261842710406	46.80799865398882	8830
836a5dac9016e1af74248123359655264eb511c0	on a method of axiomatization of some propositional calculi			axiomatic system;propositional calculus	Zdzislaw Dywan	1987	Math. Log. Q.	10.1002/malq.19870330108	propositional variable	Theory	-12.405112082874497	12.696096769049603	8834
a646a6a3640908930d1c56895348444be191a419	collaborative technique for concurrency bug detection	concurrency bug detection;static program slicing;期刊论文;dynamic active tester;collaborative	Concurrency bugs hidden in deployed software can cause severe failures and real-world disasters. They are notoriously difficult to detect during in-house testing due to huge and non-deterministic interleaving space. Unfortunately, the multicore technology trend worsens this problem. Unlike previous work that detects particular concurrency bugs (e.g., data races and atomicity violations), we target harmful concurrency bugs that cause program failures. In order to detect harmful concurrency bugs effectively and efficiently, we propose an innovative, collaborative approach called ColFinder. First, ColFinder statically analyzes the program to identify potential concurrency bugs. ColFinder then uses static program slicing to get smaller programs with respect to potential concurrency bugs. Finally, ColFinder dynamically controls thread scheduler to force multiple threads access the same memory location, verifying whether the potential concurrency bug will cause program failure. If a failure occurs, a harmful concurrency bug is detected. We have implemented ColFinder as a prototype tool and have experimented on a number of real-world concurrent programs. The probability of bug manifestation in these programs is only 0.64 % averagely during native execution. It is significantly raised to 90 % with ColFinder. The runtime overhead imposed by many previous approaches is always more than 10 $$\times $$ × . The overhead of ColFinder is more acceptable, with an average of 79 %. Additionally, to our knowledge, this is the first technique that introduces program slicing to reduce the time of bug manifestation, with an average of 33 %.	atomicity (database systems);concurrency (computer science);concurrent computing;deadlock;fast fourier transform;forward error correction;machine code;memory address;multi-core processor;multiversion concurrency control;nondeterministic algorithm;overhead (computing);parallel computing;program slicing;prototype;relevance;scalability;scheduling (computing);sensor;software bug;static program analysis;thread (computing)	Zhendong Wu;Kai Lu;Xiaoping Wang;Xu Zhou	2014	International Journal of Parallel Programming	10.1007/s10766-014-0304-y	timestamp-based concurrency control;optimistic concurrency control;parallel computing;real-time computing;isolation;security bug;computer science;operating system;distributed computing;collaboration	Arch	-21.225979385298302	39.369414616286555	8835
1a930013b5ecbf8ba81080eb0871b776e511200e	detecting conflicts among declarative ui extensions	conflicts;extensions;web browsers;overlays	We examine overlays, a flexible aspect-like mechanism for third-party declarative extensions of declarative UIs. Overlays can be defined for any markup language and permit extensions to define new content that is dynamically woven into a base UI document. While powerful, overlays are inherently non-modular and may conflict with each other, by defining duplicate or contradictory UI components. We construct an abstract language to capture core overlay semantics, and design an automatic analysis to detect inter-extension conflicts. We apply the analysis to a case study of Firefox extensions, finding several real-world bugs. Our analysis provides low-level feedback to extension developers and high-level reports to end users. Finally, we show how variants of overlays more expressive than those of Firefox complicate conflict detection.	declarative programming;file synchronization;firefox;high- and low-level;markup language;software bug;user interface	Benjamin S. Lerner;Dan Grossman	2012		10.1145/2384577.2384590	computer science;theoretical computer science;database;overlay;programming language;world wide web	PL	-27.155556319608067	28.417415278487102	8842
12482cdd8e45d87a4f23fb5ef542461de8d595dd	information exchange network for the liberalised electricity market with object-oriented and internet-based technologies			information exchange;internet	Joseph Olefumi Dada	2002				AI	-49.51289990787266	8.81352382376892	8844
1a3ce2cd6d80e863468925505cdd8f227e8cebad	an integrated distance for atoms	distance function;programming language;data representation;first order;similarity;logic programs;knowledge representation;distance functions;first order logic	In this work, we introduce a new distance function for data representations based on first-order logic (atoms, to be more precise) which integrates the main advantages of the distances that have been previously presented in the literature. Basically, our distance simultaneously takes into account some relevant aspects, concerning atom-based presentations, such as the position where the differences between two atoms occur (context sensitivity), their complexity (size of these differences) and how many times each difference occur (the number of repetitions). Although the distance is defined for first-order atoms, it is valid for any programming language with the underlying notion of unification. Consequently, many functional and logic programming languages can also use this distance.	atom;complexity;first-order logic;first-order predicate;logic programming;programming language;unification (computer science)	Vicent Estruch;César Ferri;José Hernández-Orallo;M. José Ramírez-Quintana	2010		10.1007/978-3-642-12251-4_12	knowledge representation and reasoning;edit distance;computer science;theoretical computer science;first-order logic;programming language;distance;algorithm	PL	-19.712603515373065	7.149249652431751	8846
dc188067977352fe198d341980351ae0fc919697	fixpoint extensions of temporal description logics	first order logic;description logic	In this paper we introduce a decidable fixpoint extension of temporal Description Logics. We exploit the decidability results obtained for various monodic extensions of Description Logics to obtain decidability and tight complexity results for temporal fixpoint extensions of these Description Logics and more generally for the decidable monodic fragments of first order logic.	description logic;first-order logic;fixed point (mathematics)	Enrico Franconi;David Toman	2003			t-norm fuzzy logics;monoidal t-norm logic;modal μ-calculus;Łukasiewicz logic;description logic;computation tree logic;classical logic;decidability;mathematics;algorithm	AI	-12.992923633942457	14.092909092682932	8852
99e5c15de22de150b25376c580b42d8c43f8bed3	a reputation-based game for tasks allocation	theoretical model;task allocation	One of the most difficult tasks in the design of information systems is how to control the behaviour of the back-end storage engine, usually a relational database. As the load on the database increases, the longer issued transactions will take to execute, mainly because the presence of a high number of locks required to provide isolation and concurrency. In this paper we present MIDAS, a middleware designed to manage the behaviour of database servers, focusing primarily on guaranteeing transaction execution within an specified amount of time (deadline). MIDAS was developed for Java applications that connects to storage engines through JDBC. It provides a transparent QoS layer and can be adopted with very few code modifications. All transactions issued by the application are captured, forcing them to pass through an Admission Control (AC) mechanism. To accomplish such QoS constraints, we propose a novel AC strategy, called 2-Phase Admission Control (2PAC), that minimizes the amount of transactions that exceed the established maximum time by accepting only those transactions that are not expected to miss their deadlines. We also implemented an enhancement over 2PAC, called diffserv – which gives priority to small transactions and can adopted when their occurrences are not often.	concurrency (computer science);database engine;database server;differentiated services;information system;jdbc;java;lock (computer science);middleware;quality of service;relational database	Hamdi Yahyaoui	2009		10.1007/978-3-642-01347-8_60	computer science;static memory allocation	Embedded	-24.576733540747902	48.87478948459586	8861
e222a603a14ec630792e02c3e75d35da6ab2ad89	bridging mocs in systemc specifications of heterogeneous systems	signal image and speech processing;circuits and systems;control structures and microprogramming;heterogeneous systems;electronic circuits and devices	In order to get an efficient specification and simulation of a heterogeneous system, the choice of an appropriate model of computation (MoC) for each system part is essential. The choice depends on the design domain (e.g., analogue or digital), and the suitable abstraction level used to specify and analyse the aspects considered to be important in each system part. In practice, MoC choice is implicitly made by selecting a suitable language and a simulation tool for each system part. This approach requires the connection of different languages and simulation tools when the specification and simulation of the system are considered as a whole. SystemC is able to support a more unified specification methodology and simulation environment for heterogeneous system, since it is extensible by libraries that support additional MoCs. A major requisite of these libraries is to provide means to connect system parts which are specified using different MoCs. However, these connection means usually do not provide enough flexibility to select and tune the right conversion semantic in a mixed-level specification, simulation, and refinement process. In this article, converter channels, a flexible approach for MoC connection within a SystemC environment consisting of three extensions, namely, SystemC-AMS, HetSC, and OSSS+R, are presented.	abstraction layer;bridging (networking);digital data;level of detail;library (computing);low-pass filter;model of computation;refinement (computing);run time (program lifecycle phase);systemc;systems design;vhdl-ams	Markus Damm;Jan Haase;Christoph Grimm;Fernando Herrera;Eugenio Villar	2008	EURASIP J. Emb. Sys.	10.1155/2008/738136	embedded system;real-time computing;computer science;theoretical computer science;operating system	EDA	-40.049012719826074	34.76985769569309	8867
f63f821ee0c41a38d8a6947e65833a124a6849de	automatic generation of predictive monitors from scenario-based specifications		Abstract Context Unpredictability and uncertainty about future evolutions of both the system and its environment may easily compromise the behavior of the system. The subsequent software failures can have serious consequences. When dealing with open environments, run-time monitoring is one of the most promising techniques to detect software failures. Several monitoring approaches have been proposed in the last years; however, they suffer from two main limitations. First, they provide limited information to be exploited at run-time for early detecting and managing situations that most probably will lead to failures. Second, they mainly rely on logic-based specifications, whose intrinsic complexity may hamper the use of these monitoring approaches in industrial contexts. Objective In order to address these two limitations, this paper proposes a novel approach, called PREDIMO ( PREDI ctive MO nitoring). The approach starts from scenario-based specifications, automatically generates predictive monitors called MA s (Multi-valued Automata), which take into account the actual status and also the possible evolution of both system and environment in the near future, and enables the definition of precise strategies to prevent failures. More specifically, the generated monitors evaluate the specified properties and return one of the seven different values representing the degree of controllability of the system and the distance of the potential incoming failure. The translation from scenario-based specifications to MA s preserves the semantics of the starting specification. Method We use the design and creation research methodology to design an innovative approach that fills highlighted gaps of state-of-the-art approaches. The validation of the approach is performed through a large experimentation with OSGi (Open Service Gateway Initiative) applications. Results We present a novel language to specify the properties to be monitored. Then, we present a novel approach to automatically generate predictive monitors from the specified properties. Conclusion The overall approach is tool supported and a large experimentation demonstrates its feasibility and usability.		Pengcheng Zhang;Patrizio Pelliccione;Hareton K. N. Leung;Xuandong Li	2018	Information & Software Technology	10.1016/j.infsof.2018.01.014	controllability;systems engineering;computer science;real-time computing;semantics;software;default gateway;usability;compromise	SE	-55.68622843475056	28.093074259024252	8868
99d60c41f9dd4364ab481204bac231eed7b09484	bugs and breaches			software bug	Elizabeth MacDonald	2005	I. J. Law and Information Technology	10.1093/ijlit/eai005	computer security;computer science	HPC	-58.052485305353386	57.995163095911934	8877
67f961f98d34fea3ab15f473429a5156b62b5c65	vigilare: toward snoop-based kernel integrity monitor	hardware based integrity monitor;kernel integrity monitor;transient attack	In this paper, we present Vigilare system, a kernel integrity monitor that is architected to snoop the bus traffic of the host system from a separate independent hardware. This snoop-based monitoring enabled by the Vigilare system, overcomes the limitations of the snapshot-based monitoring employed in previous kernel integrity monitoring solutions. Being based on inspecting snapshots collected over a certain interval, the previous hardware-based monitoring solutions cannot detect transient attacks that can occur in between snapshots. We implemented a prototype of the Vigilare system on Gaisler's grlib-based system-on-a-chip (SoC) by adding Snooper hardware connections module to the host system for bus snooping. To evaluate the benefit of snoop-based monitoring, we also implemented similar SoC with a snapshot-based monitor to be compared with. The Vigilare system detected all the transient attacks without performance degradation while the snapshot-based monitor could not detect all the attacks and induced considerable performance degradation as much as 10% in our tuned STREAM benchmark test.	benchmark (computing);bus snooping;data dredging;elegant degradation;kernel (operating system);prototype;snapshot (computer storage);system on a chip;snoop	Hyungon Moon;Hojoon Lee;Jihoon Lee;Kihwan Kim;Yunheung Paek;Brent ByungHoon Kang	2012		10.1145/2382196.2382202	embedded system;real-time computing;operating system;computer security	Security	-54.506292014831445	56.92573017906678	8887
0e9f056cce7beb09d9c5d852f3ea23d88d870fd7	amva techniques for high service time variability	residence time;shared memory;tcp;mean value analysis;congestion avoidance;memory systems;mean residence time;shared memory system;rtt measurement	Motivated by experience gained during the validation of a recent Approximate Mean Value Analysis (AMVA) model of modern shared memory architectures, this paper re-examines the “standard” AMVA approximation for non-exponential FCFS queues. We find that this approximation is often inaccurate for FCFS queues with high service time variability. For such queues, we propose and evaluate: (1) AMVA estimates of the mean residual service time at an arrival instant that are much more accurate than the standard AMVA estimate, (2) a new AMVA technique that provides a much more accurate estimate of mean center residence time than the standard AMVA estimate, and (3) a new AMVA technique for computing the mean residence time at a “downstream” queue which has a more bursty arrival process than is assumed in the standard AMVA equations. Together, these new techniques increase the range of applications to which AMVA may be fruitfully applied, so that for example, the memory system architecture of shared memory systems with complex modern processors can be analyzed with these computationally efficient methods.	algorithmic efficiency;approximation;central processing unit;downstream (software development);heart rate variability;shared memory;systems architecture;time complexity	Derek L. Eager;Daniel J. Sorin;Mary K. Vernon	2000		10.1145/339331.339418	parallel computing;real-time computing;computer science;operating system;distributed computing;residence time;computer network	Metrics	-21.86690487354327	56.68341481173624	8895
3875bdd852055475617e6341ee74938ab546204d	register file write data gating techniques and break-even analysis model	switching activity;data gating;data distribution;low power;leakage power;sram;register file	"""Register Files account for 30% of 32nm Intel WSM Core dynamic power of which 25% is due to write data distribution. We analyze Register File data gating strategies used to reduce write bitline dynamic power by as much as 96%. We explore the tradeoff of various data gating topologies (Global, Midway, Local), logic implementations (NAND, NOR, Tri-State), and techniques (Stack-Forcing, State-Forcing) to reduce both dynamic and leakage power. We then present a simple and accurate data gating break-even analysis model. The model comprehends """"Data"""" and """"Enable"""" switching activity, signal probability, logic implementation overhead, demonstrating an average error range of ±5%."""	battle of midway;nand gate;overhead (computing);register file;spectral leakage;three-state logic	Eric Donkoh;Teck Siong Ong;Yan Nee Too;Patrick Chiang	2012		10.1145/2333660.2333700	parallel computing;real-time computing;static random-access memory;computer hardware;computer science;operating system;clock gating;register file	Arch	-7.0207794945450654	53.82902450875431	8896
d4f5825d3485e386f4004747ad6eeec6dc938f29	a distributed spatial architecture for bush fire simulation	spatial information system;distributed networks;fire behavior;emergency management;world wide web;system architecture	This paper describes a spatial systems architecture allowing clients to modify parameters and run their own bush fire simulations via map data provided by a http server. The simulations conform to the model of bush fire behaviors as well as operational requirements. A wavelet model and fire simulation over a distributed network are considered in this paper and we demonstrate how the architecture is general enough to accommodate other spatial simulations for emergency management using the Web.	hypertext transfer protocol;requirement;server (computing);simulation;systems architecture;wavelet;web server;world wide web	Peter W. Eklund	2001	International Journal of Geographical Information Science	10.1080/13658810010017883	simulation;computer science;world wide web;computer security;emergency management;systems architecture	Networks	-31.729706670355263	46.907266989843265	8901
f27fbf620d6ad22b5f76d6acc9cabe7b3d478f77	a framework to compute inference rules valid in agents' temporal logics	temporal logic;inference rule;transit time	Our paper suggests a computational framework for verification valid inference in agents' temporal logics. As a tool, describing human reasoning procedure, we suggest valid inference rules (valid semantically - in Kripke-like frames generating logic). We investigate valid inference rules in agents' temporal logics with linear and branching intransitive time. Main results of our paper are suggested algorithms which allow to compute valid inference rules in agents' liner time logics LTLK and LTLK(Z), agents' logic with branching intransitive time LTAi, and the logic with branching transitive time LTAt.		Sergey Babenyshev;Vladimir V. Rybakov	2010		10.1007/978-3-642-15387-7_27	discrete mathematics;disjunction introduction;non-monotonic logic;data mining;mathematics;proof calculus;algorithm;universal generalization	Logic	-16.34108660023934	9.802033860817854	8902
b54001b4629e6165fec5f2811968e29864a621d6	fork diagrams for teaching selection in cs i	concurrent programming;object oriented programming;boolean operation;visual representation;high level language	We propose a fork diagram as a visual representation of the algorithm for binary selection. Among other things, fork diagrams can be used to teach students how to write correctly nested if-else statements, analyze nested selection code, appreciate the problem of dangling else, and understand short circuit evaluation of conditions with boolean operators (and and or). We have used C code to illustrate concepts, although fork diagrams can be used for any high level language.	algorithm;dangling else;diagram;fork (software development);high-level programming language;logical connective;short circuit;short-circuit evaluation	Amruth N. Kumar	1996		10.1145/236452.236575	concurrent computing;computer science;theoretical computer science;programming language;object-oriented programming;high-level programming language;algorithm	EDA	-27.21137156314952	24.269061309928514	8903
760d27674260466f5244972044a95469a8e5049c	a semantic approach to prolog program analysis	program analysis		program analysis;prolog	Brian J. Ross	1991			semantic computing;theoretical computer science;database;programming language	SE	-23.764438899920314	20.904996605221992	8915
05a48e3b2f59dbb2417e0ec5fd993dfd0aab7c40	a new approach to the functional design of a digital computer	automatic control;computer languages;programming profession;storage automation;hardware;programming profession automatic control storage automation computer languages hardware	The present methods of determining the functional design of computers are critically reviewed and a new approach proposed. This is illustrated by explaining, in abstracted form, part of the control organization of a new and different machine based, in part, on the ALGOL 60 language. The concepts of expression and procedure lead directly to use of a Polish string program. A new arrangement of control registers results, which provides for automatic allocation of temporary storage within expressions and procedures, and a generalized subroutine linkage. The simplicity and power of these notions suggests that there is much room for improvement in present machines and that more attention should be given to control functions in new designs.(1961).	algol 60;computer;control function (econometrics);control register;functional design;linkage (software);operand;subroutine;terabyte	Robert S. Barton	1987	Annals of the History of Computing	10.1109/MAHC.1987.10002	computer science;engineering;electrical engineering;artificial intelligence;theoretical computer science;operating system;automatic control;database;programming language;algorithm;control flow analysis	Graphics	-28.053400401486435	25.471127464926123	8933
44c8bffb7b914a27a7160c22fa88f161e9ca9ec5	the ksr1: experimentation and modeling of poststore	distributed data;general and miscellaneous mathematics computing and information science;memory management;selected works;performance;data processing;mathematical models 990200;computer architecture;mathematical models;array processors;processing 990200 mathematics computers;mathematical model;bepress;distributed data processing;distributed shared memory;cache only memory architecture;mathematics and computers;analytical model	Kendall Square Research introduced the KSR1 system in 1991. The architecture is based on a ring of rings of 64-bit microprocessora. It is a distributed, shared memory system and is scalable. The memory structure is unique and is the key to understanding the system. Different levels of caching eliminates physical memory addressing and leads to the ALLCACHE&trade; scheme. Since requested data may be found in any of several caches, the initial access time is variable. Once pulled into the local (sub) cache, subsequent access times are fixed and minimal. Thus, the KSR1 is a Cache-Only Memory Architecture (COMA) system.This paper describes experimentation and an analytic model of the KSR1. The focus is on the poststore programmer option. With the poststore option, the programm er can elect to broadcast the updated value of a variable to all processors that might have a copy. This may save time for threads on other processors, but delays the broadcasting thread and places additional traffic on the ring. The specific issue addressed is to determine under what conditions poststore is beneficial. The analytic model and the experimental observations are in good agreement. They indicate that the decision to use poststore depends both on the application and the current system load.	64-bit computing;access time;cache-only memory architecture;central processing unit;code;computer data storage;data access;distributed shared memory;experiment;glossary of computer graphics;kendall square research;load (computing);memory address;operating system;operational amplifier;overhead (computing);prefetch input queue;programmer;random-access memory;scalability;scheduling (computing);subpage;web search engine	Emilia Rosti;Evgenia Smirni;Thomas D. Wagner;Amy W. Apon;Lawrence W. Dowdy	1993		10.1145/166955.166985	uniform memory access;parallel computing;distributed memory;data processing;computer science;theoretical computer science;operating system;mathematical model;mathematics;distributed computing;statistics;computer network	Metrics	-12.6948417850706	51.612865520200586	8951
92b052d441f22dd073cbe235b58a96dc78bb48ff	towards transactional memory semantics for c++	sequential consistency;programming language;atomicity;synchronization;serializability;point of view;transactional memory;c;hardware implementation	Transactional memory (TM) eliminates many problems associated with lock-based synchronization. Over recent years, much progress has been made in software and hardware implementation techniques for TM. However, before transactional memory can be integrated into mainstream programming languages, we must precisely define its meaning in the context of these languages. In particular, TM semantics should address the advanced features present in the existing software TM implementations, such as interactions between transactions and locks, explicit user-level abort and support for legacy code.  In this paper, we address these topics from both theoretical and practical points of view. We give precise formulations of several popular TM semantics for the domain of sequentially consistent executions and show that some of these semantics are equivalent for C++ programs that do not contain other forms of synchronization. We show that lock-based semantics, such as Single Global Lock Atomicity (SLA) or Disjoint Lock Atomicity (DLA), do not actually guarantee atomicity for race-free programs and propose a new semantics, Race-Free Atomicity (RFA) that gives such a guarantee. We compare these semantics from the programmer and implementation points of view and explain why supporting non-atomic transactions is useful. Finally, we propose a new set of language constructs that let programmers explicitly specify whether transactions should be atomic and describe how these constructs interact with user-level abort and legacy code.	atomicity (database systems);c++;drive letter assignment;interaction;legacy code;lock (computer science);memory semantics (computing);programmer;programming language;race condition;service-level agreement;transactional memory;user space	Tatiana Shpeisman;Ali-Reza Adl-Tabatabai;Robert Geva;Yang Ni;Adam Welc	2009		10.1145/1583991.1584012	synchronization;transactional memory;parallel computing;computer science;operating system;database;distributed computing;programming language;serializability;sequential consistency;atomicity;algorithm	PL	-15.297408026935557	39.564611328150484	8956
5113d00befde6b26bf4a1e067b292f4deea4d8a2	language-based rapid prototyping methods for legacy system re-engineering and re-use	computer maintenance;prototypes design engineering military computing costs electronic design automation and methodology computer science identity based encryption microelectronics production systems assembly systems;service system;top down;hardware description languages;rapid prototyping;system upgrades top down language based rapid prototyping methodology legacy system reengineering legacy system reuse electronic technology evolution commercial systems military systems waste inefficiency serviceable systems obsolescence spare assemblies replacement parts legacy design information component replacement obsolete legacy system components design reuse path;legacy systems;vhdl;hardware description languages systems re engineering computer maintenance recycling;re engineering;legacy system;recycling;reverse engineering;systems re engineering	Electronic technologies are evolving more rapidly than any other aspect of commercial and military systems. This pace can lead to waste and inefficiency when perfectly serviceable systems must be scrapped because spare assemblies and replacement parts are no longer available. While rapid prototyping methods offer the means to quickly go from concept to prototype, they lack the ability to easily include good concepts from old designs. This paper describes preliminary work to develop methods that allow legacy design information to be incorporated into a top-down language-based rapid prototyping methodology. This will allow rapid re-engineering and replacement of obsolete legacy system components, as well as offering a new design re-use path for system upgrades.	legacy system;rapid prototyping	David L. Landis;Praveen Guddeti;Paul T. Hulina;Lee D. Coraor	1999		10.1109/IWRSP.1999.779031	embedded system;software modernization;computer science;systems engineering;engineering;operating system;software engineering;legacy system;computer engineering	HCI	-61.71483204974049	23.05867433042842	8958
537869dd88eb6a400ed3e5a812ca791d4e1b3f38	a decentralized and service-based solution for data mediation: the case for data providing service compositions	composition;mediation;web services;mediation as a service maas	The last few years have seen an increase in the use of e-services as a natural consequence of the service-based economy growth. However, the distributed nature of services can raise several heterogeneity problems that can hamper the widespread adoption of service-oriented architectures (SOA). In this paper, we are interested in data heterogeneity problems that can occur when using composite services. We propose a service-based approach for automatically inserting appropriate mediation services in service compositions to resolve structural heterogeneities in their data flow. This mediation is ensured through a special kind of Web services called data mapping Web services. In addition, in order to be in agreement with the distributed nature of the Web, we propose a decentralized solution to publish/discover these services. We demonstrate how our approach applies to the data structural heterogeneity problem in data providing service compositions as a particular data heterogeneity case. Provided experimental results show that our approach is efficient in realistic situations. Copyright © 2013 John Wiley & Sons, Ltd.	ambiguous name resolution;business process execution language;centralized computing;dataflow;discovery system;distributed hash table;e-services;executable;functional requirement;iterative method;john d. wiley;mobile device;non-functional requirement;peer-to-peer;response time (technology);scalability;seamless3d;service discovery;service-oriented architecture;service-oriented device architecture;web service;world wide web	Mohamed Sellami;Walid Gaaloul;Bruno Defude	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3048	composition;differentiated service;computer science;service delivery framework;ws-policy;data mining;database;distributed computing;mediation;services computing;world wide web	DB	-44.46015370788685	12.444998268267646	8962
3847a3ee90b0dbdc530a9b4c8b5efd9942c308f4	hiding more of hidden algebra	algebraic specification;program verification;satisfiability;specification language;verificacion programa;formal verification;verification formelle;lenguaje especificacion;verification programme;langage specification	Behavioral speci cation is a rapidly advancing area of algebraic semantics that supports practical applications by allowing models (implementations) that only behaviorally satisfy speci cations, in nitary data structures (such as streams), behavioral re nements, and coinduction proof methods. This paper generalizes the hidden algebra approach to allow: (P1) operations with multiple hidden arguments, and (P2) de ning behavioral equivalence with a subset of operations, in addition to the already present (P3) built-in data types, (P4) nondeterminism, (P5) concurrency, and (P6) non-congruent operations. All important results generalize, but more elegant formulations use the new institution in Section 5. Behavioral satisfaction appeared 1981 in [20], hidden algebra 1989 in [9], multiple hidden arguments 1992 in [1], congruent and behavioral operations in [1, 18], behavioral equivalence de ned by a subset of operations in [1], and non-congruent operations in [6]; all this was previously integrated in [21], but this paper gives new examples, institutions, and results relating hidden algebra to information hiding. We assume familiarity with basics of algebraic speci cation, e.g., [11, 13].	algebraic semantics (computer science);coinduction;concurrency (computer science);data structure;hidden algebra;linear algebra;p6 (microarchitecture);streams;turing completeness	Joseph A. Goguen;Grigore Rosu	1999		10.1007/3-540-48118-4_40	formal methods;specification language;formal verification;computer science;theoretical computer science;programming language;language of temporal ordering specification;satisfiability	Logic	-11.9802346576715	21.229748905561312	8978
19171d4bf92e7cb82e3e48da603a4f7480f032e9	context aware service discovery and service enabled workflow		We provide a conceptual model for context aware Semantic Web Service (SWS) discovery, which can utilize realtime legacy data from external systems and support user contextbased service discovery and selection. This model offers advantages over current SWS technology which cannot be easily applied to different domains or be integrated with legacy systems. Using this conceptualization we propose an intelligent decision support system, which offers Service Enabled Workflow. Keywords—Semantic Web Service, Context Aware Service Discovery, Service Enabled Workflow, Service Metadata, Ontology	conceptualization (information science);intelligent decision support system;legacy system;semantic web service;service discovery;sinewave synthesis	Altaf Hussain;Wendy MacCaull	2013			web service;service delivery framework;knowledge management;workflow engine;world wide web;workflow management system;workflow technology;service discovery;workflow;intelligent decision support system;computer science	HPC	-43.70727102425909	11.470286942401003	8992
00d4d4dd07a34599718f453eec481608170891b8	efficient alias set analysis using ssa form	total order;dominance;alias analysis;pointer analysis;shape analysis;dataflow analysis;live variables;transfer function;hash table;static single assignment;data structure;static single assignment form	Precise, flow-sensitive analyses of pointer relationships often represent each object using the set of local variables that point to it (the alias set), possibly augmented with additional predicates. Many such analyses are difficult to scale due to the size of the abstraction and due to flow sensitivity. The focus of this paper is on efficient representation and manipulation of the alias set. Taking advantage of certain properties of static single assignment (SSA) form, we propose an efficient data structure that allows much of the representations of sets at different points in the program to be shared. The transfer function for each statement, instead of creating an updated set, makes only local changes to the existing data structure representing the set. The key enabling properties of SSA form are that every point at which a variable is live is dominated by its definition, and that the definitions of any set of simultaneously live variables are totally ordered according to the dominance relation. We represent the variables pointing to an object using a list ordered consistently with the dominance relation. Thus, when a variable is newly defined to point to the object, it need only be added to the head of the list. A back edge at which some variables cease to be live requires only dropping variables from the head of the list. We prove that the analysis using the proposed data structure computes the same result as a set-based analysis. We empirically show that the proposed data structure is more efficient in both time and memory requirements than set implementations using hash tables and balanced trees.	assignment (computer science);data structure;depth-first search;hash table;local variable;pointer (computer programming);requirement;self-balancing binary search tree;static single assignment form;transfer function	Nomair A. Naeem;Ondrej Lhoták	2009		10.1145/1542431.1542443	data structure;computer science;theoretical computer science;operating system;distributed computing;live variable analysis;programming language;static single assignment form;algorithm	PL	-19.085936343618336	31.846513229538687	8997
38717a7ead3127bc527621c20c7c75c0a5cdacf2	defining variants of default logic: a modal approach	default logic	Recently some variants of Reiter's default logic have been proposed. These variants have been defined by altering the definition of default extension and, sometimes, also the definition of default theory. Recently a uniform semantic framework has been introduced by Besnard and Schaub, in which the semantics of the various default logics is given in terms of Kripke structures.	default logic;modal logic	Laura Giordano	1993		10.1007/3-540-56804-2_6	linear temporal logic;computer science;artificial intelligence;default logic;multimodal logic;autoepistemic logic	Logic	-15.42605583417813	9.432671451968105	9000
45750e1a0ad05c98d4e3888467bca0f493209860	execution support to long running workflows	long running workflows;dynamic reconfiguration;monitoring security robustness proposals prototypes organizations;performance evaluation execution support long running workflows enterprise environments business processes formalization business processes automation computational services business rules resource provisioning architecture design;dynamic reconfiguration long running workflows osgi;osgi;workflow management software business data processing software architecture software performance evaluation	Workflows have been widely adopted in enterprise environments to allow the formalization and automation of business processes through the execution of activities implemented by computational services. Some of these workflows, namely long running workflows, take a long time (e.g., Hours or even days and weeks) to complete their work and produce the expected results. The long execution time is inherent to the nature of some business processes, e.g., To buy a book in a bookstore using the Internet. These workflows are usually deployed at operational environments that explicitly deal with the particularities of their executions, e.g., The long time running, possibility of changes in the business rules, need of resource provisioning, security and so on. To build this kind of environment is a complex task due to the aforementioned characteristics of long running workflows. In this context, this paper presents the architecture, design and implementation of an environment specially conceived to support long running workflows. In order to evaluate the proposed environment, we carried out a performance evaluation.	business process;cloud computing;equinox;internet;java;osgi;performance evaluation;prototype;provisioning;requirement;run time (program lifecycle phase);state management	Milton S. S. Junior;Nelson Souto Rosa;Fernando Antônio Aires Lins	2014	2014 IEEE International Conference on Computer and Information Technology	10.1109/CIT.2014.94	real-time computing;artifact-centric business process model;operating system;database;distributed computing;event-driven process chain	DB	-44.87525914839522	44.10833292046055	9011
23e320254bdca4be709027f09e54bd7746b33490	towards a real-world oriented workflow system-based on practical experiments for three years	document handling;design principle;collaborative work;financial management;information science;prototypes;contracts;budgeting data processing;office automation workflow management software business data processing budgeting data processing;case study workflow system business office automation document handling research budget management national facility;workflow system;business data processing;business;workflow management software;business information science prototypes financial management contracts collaborative software collaborative work environmental management humans design methodology;humans;environmental management;national facility;human activity;office automation;research budget management;collaborative software;design methodology	"""Current commercial workflow systems handle only information that is in digital format. However, when they are applied to actual business in an office, we sometimes want to manage not only digital information but also human activities and documents, under some constraints such as the work context and computer environment. In this study, a workflow that handles information found in the real world as well as digital information in the system is called a """"real-world oriented workflow"""". To study the workflow, we implemented a prototype workflow system for research budget management in a national facility as a case study. Based on this case study, this paper describes the design principles and methods of supporting the real-world oriented workflow. The effectiveness of the prototype workflow system, which was used for actual business for three years, is shown."""	experiment	M. Shikida;C. Kadowaki;Susumu Kunifuji	1999		10.1109/KES.1999.820116	workflow;xpdl;systems engineering;knowledge management;workflow management coalition;management science;business;windows workflow foundation;workflow management system;workflow engine;workflow technology	Security	-53.44389175460305	8.624805358816308	9018
b0ceddc7e37cfc3138e439db0f77747a3ceea414	using bsp to optimize data distribution in skeleton programs	programacion paralela;program transformation;parallel programming;transformation programme;data distribution;program optimization;performance programme;transformacion programa;eficacia programa;optimisation programme;program performance;parallel architecture;parallel programs;programmation parallele;optimizacion programa	"""Parallel programming can be made easier by means of a skeleton based methodology, such as P 3 L, which helps programmers to compose their applications by using a set of xed parallel patterns. Such kind of approach is also useful to obtain portability because the \struc-tured"""" nature of the language can be used to devise a composable support for each parallel pattern so that the complexity of nding an \optimal"""" implementation on diierent parallel architectures can be reduced. In this work, we show how we can conjugate the BSP abstract model and its related cost analisys to provide an implementation strategy \abstract enough"""" for being also machine independent. We hope this can be a rst step towards the idea of a portable set of optimization rules. The rst results show how an implementation template for the Map constructor, able of to be tuned automatically, can be designed. A validation of the technique is given for a Cray T3E and a cluster of PC-Linux."""	abstract machine;analysis of algorithms;central processing unit;cray t3e;iterative method;linux;mathematical optimization;parallel computing;programmer;skeleton (computer programming)	Andrea Zavanella;Susanna Pelagatti	1999		10.1007/BFb0100622	parallel computing;real-time computing;computer science;operating system;program optimization;distributed computing;programming language;algorithm	DB	-14.287124361040377	37.10513148838082	9019
6a8f2db85b5880d3de378ed24ffded78baf716a0	fault tolerant control in manufacturing processes			fault tolerance;fault-tolerant computer system	J. L. Fernández de Arroyabe	1989			reliability engineering;fault tolerance;fault (power engineering);computer science	Robotics	-36.224417062880434	36.75559792557712	9026
d5e85e19c03f86bfacbb86a2ea3d3d30d1a06d23	formal verification of backward compatibility of microcode	new technology;legacy software;systeme interaction;automatic proving;microprogrammation;microprogramacion;software systems;compatibilidad;demostracion automatica;program verification;sistema interaccion;demonstration automatique;software architecture;verificacion programa;logiciel patrimonial;formal verification;logicial herencia;compatibility;verification formelle;compatibilite;microprogramming;verification programme;interaction system;architecture logiciel	Microcode is used to facilitate new technologies in Intel CP U designs. A critical requirement is that new designs be backwar dly compatible with legacy code when new functionalities are disabled. Several f atures distinguish microcode from other software systems, such as: interactio n with the external environment, sensitivity to exceptions, and the complexit y of instructions. This work describes the ideas behind M ICROFORMAL, a technology for fully automated formal verification of functional backward compatibi lity of microcode.	backward compatibility;formal verification;legacy code;microcode;software system	Tamarah Arons;Elad Elster;Limor Fix;Sela Mador-Haim;Michael Mishaeli;Jonathan Shalev;Eli Singerman;Andreas Tiemeyer;Moshe Y. Vardi;Lenore D. Zuck	2005		10.1007/11513988_20	software architecture;computer architecture;parallel computing;formal verification;computer science;operating system;microcode;programming language;compatibility;legacy system;algorithm;software system	Logic	-23.83835488853682	30.769786607081166	9049
f68e4758ca099a61b24fd5a712d039816e25197c	isemserv: towards the engineering of intelligent semantic-based services	semantic web service;value added services;service provider;intelligent semantic based services;intelligent agents;service creation;model building;service creation framework and platform;intelligent agent;laboratory experiment;ontology	The emergence of Semantic Web Services is stimulating the need for modern enterprises to efficiently and rapidly develop and deliver machineprocessable and machine-interpretable value-added services in order to automate a variety of tasks on the Web. However, semantic-based services are scarcely adopted and utilised as there are few real-life examples that demonstrate the possibilities and benefits of such services. Furthermore, there is a lack of service creation frameworks and technical platforms that purport to guide and promote simple, flexible, rapid, and unified engineering of semantic-based services. In addition, current semantic service platforms do not support the construction of semantic services that are intelligent beyond the application of ontologies. In this position paper, preliminary efforts that seek to address the challenges of simplifying and speeding-up the engineering process of intelligent semantic services are presented. The goal of the work presented in this paper is about providing service providers, designers, and consumers with simple, unified, and yet simple tools that can aid in the technical implementation of intelligent semantic-based services. The main contributions envisioned from this research is a conceptual service creation framework called iSemServ and a technological service creation platform, which is intended to simplify and support the phases of building intelligent semantic services in an integrated manner. The proposed research adopts a quantitative approach with the main focus on model-building, prototypes, laboratory experiments, and computer-based simulations.	emergence;experiment;ontology (information science);provisioning;real life;semantic web service;simulation;software prototyping;usability;world wide web	Jabu Mtsweni;Elmarie Biermann;Laurette Pretorius	2010		10.1007/978-3-642-16985-4_54	service provider;semantic computing;model building;semantic grid;computer science;knowledge management;artificial intelligence;service delivery framework;social semantic web;services computing;world wide web;intelligent agent;semantic analytics	Web+IR	-46.43239504295879	11.0218811505035	9053
4c1cd7263f48b5d5b6992bd25594c62a5f70a292	c++ memory check tool based on dynamic binary instrumentation platform		In software development, to detect the presence of defects in the software as soon as possible, would greatly reduce the extent of losses arising. In this paper, focus on the memory-use error in C++ program, designed and implemented a memory check tools named ShadowCheck, based on dynamic binary instrumentation platform, which is platform-cross, efficiency and accuracy. In this paper, introduced dynamic binary instrumentation platform and the memory layout of Linux first, then explained how the ShadowCheck works, at last, summarized the efficiency and accuracy of ShadowCheck.	c++	Jingling Zhao;Lei He;Bing He	2016		10.1007/978-3-319-49106-6_1	embedded system;computer hardware;computer science;operating system	Embedded	-21.776242138352988	38.65453104662113	9058
52c00f4953c15bd067804d58c74302200c481619	evaluation of design alternatives for a multiprocessor microprocessor	scalable shared memory multiprocessors;backward error recovery;coherence protocol;fault-tolerance;application specific integrated circuits;integrated circuit packaging;computer architecture;fault tolerance;operating systems;integrated circuit;fault tolerant;operating system;shared memory	In the future, advanced integrated circuit processing and packaging technology will allow for several design options for multiprocessor microprocessors. In this paper we consider three architectures: shared-primary cache, shared-secondary cache, and shared-memory. We evaluate these three architectures using a complete system simulation environment which models the CPU, memory hierarchy and I/O devices in sufficient detail to boot and run a commercial operating system. Within our simulation environment, we measure performance using representative hand and compiler generated parallel applications, and a multiprogramming workload. Our results show that when applications exhibit fine-grained sharing, both shared-primary and shared-secondary architectures perform similarly when the full costs of sharing the primary cache are included.	microprocessor;multiprocessing	Basem A. Nayfeh;Lance Hammond;Kunle Olukotun	1996		10.1109/ISCA.1996.10017	fault tolerance;computer architecture;parallel computing;real-time computing;cache coloring;cache;computer science;operating system;cache algorithms;cache pollution	Arch	-10.94102889714979	49.03532301495695	9062
b6cfdae581903ce257a811641b7b399529097a69	extension du formalisme des flux opérationnels par une algèbre temporelle		Workflows constitute an important language to represent knowledge about processes, but also increasingly to reason on such knowledge. On the other hand, there is a limit to which time constraints between activities can be expressed. Qualitative interval algebras can model processes using finer temporal relations, but they cannot reproduce all workflow patterns. This paper defines a common ground modeltheoretical semantics for both workflows and interval algebras, making it possible for reasoning systems working with either to interoperate. Thanks to this, interesting properties and inferences can be defined, both on workflows and on an extended formalism combining workflows with interval algebras. Finally, similar formalisms proposing a sound formal basis for workflows and extending them are discussed.	interoperability;interval arithmetic;semantics (computer science);workflow pattern	Valmi Dufour-Lussier;Florence Le Ber;Jean Lieber	2012	CoRR		machine learning;artificial intelligence;calculus;computer science	AI	-16.409863205550142	11.537162228269205	9065
3f43fb70dc5cd8b5f00a59ec5a481b33796e4791	cooperative objects: principles, use and implementation	modelizacion;lenguaje programacion;distributed system;systeme reparti;programming language;red petri;cooperation;simultaneidad informatica;cooperacion;conceptual framework;expressive power;modelisation;concurrency;sistema repartido;object oriented;estructura datos;object oriented approach;active objects;langage programmation;oriente objet;structure donnee;petri net;modeling;simultaneite informatique;orientado objeto;data structure;reseau petri;open distributed system	It is no longer useful to speak in praise of the Object-Oriented Approach and the Petri Net Theory. Each of them has proved to be a worthwhile framework in its scope of use. Yet it is a challenge to associate them into a conceptual framework which combines the expressive power of both approaches and maintains all their respective merits. Moreover, it has to be established that such a formalism may be implemented in a sound and efficient way. This paper is a comprehensive presentation of the CoOperative Objects formalism. This formalism extends the theoretical and pragmatic features of both the Petri net and the ObjectOriented approaches by thoroughly integrating their concepts. It is appropriate as well for the specification and the validation of open distributed systems as for their implementation. The basic idea is that the tokens of a Petri net are passive objects while the behavior of an active object is defined by a Petri net. This paper also proposes a CoOperative Object solution to the dynamic dining philosophers problem, and tackles implementation issues through the presentation of SYROCO, a CoOperative Objects compiler.	active object;autonomous robot;autonomy;compiler;conceptual schema;concurrency (computer science);data structure;dining philosophers problem;distributed artificial intelligence;distributed computing;formal language;information system;multi-agent system;petri net;reference model;semantics (computer science);separation of concerns;software development process;system analysis	Christophe Sibertin-Blanc	2001		10.1007/3-540-45397-0_7	systems modeling;concurrency;data structure;computer science;artificial intelligence;operating system;machine learning;conceptual framework;database;distributed computing;programming language;object-oriented programming;computer security;petri net;cooperation;expressive power;algorithm	PL	-38.61250169734948	24.892548733515973	9070
1fd93e870902f82c40ec7e7f0115d438a5282a83	an initial investigation into querying an untrustworthy and inconsistent web	semantic web	The Semantic Web is bound to be untrustworthy and inconsistent. In this paper, we present an initial approach for obtaining useful information in such an environment. In particular, we replace the question of whether an assertion is entailed by the entire Semantic Web with two other queries. The first asks if a specific statement is entailed given an identification of the trusted documents. The second asks for the document sets that entail a specific statement. We propose a mechanism for efficiently computing and representing the contexts of the statements and managing inconsistency. This system could be seen as a component in an overall trust system.	assertion (software development);document;semantic web	Yuanbo Guo;Jeff Heflin	2004			computer science;social semantic web;data mining;database;world wide web	Web+IR	-39.878831893442246	8.405067636342546	9072
c5a4c75f7160d193ebaccba1256721a8be6aa447	light-weight kernel instrumentation framework using dynamic binary translation	instrumentation;light weight framework;dynamic binary translation	Mobile platforms such as Android and iOS, which are based on typical operating systems, have been widely adopted in various computing devices from smart phones even to smart TVs. Along with this, the necessity of kernel instrumentation framework has also grown up for efficient development and debugging of a kernel itself and its components. Although the existing approaches are providing some information about the kernel state including physical register value and primitive memory map, it is hard for the developers to understand and exploit the information. Moreover, the excessive analysis overhead in the existing approach makes them impractical to be used in real systems. Meanwhile, there have been a few studies on analyzing the user-level applications using dynamic binary translation and they are now widely used. In this paper, by extending this idea of dynamic binary translation for user-level applications to the kernel, we propose a new dynamic kernel instrumentation framework. Our framework focuses on the modules such as device drivers, rather than the kernel itself, since the modules comprise a large portion of OS development. Because of the frequent execution of kernel modules, the dynamic kernel instrumentation framework should guarantee the quality of the translated target code. However, costly optimizations to achieve high execution performance are rather harmful to the overall performance. Therefore, in order to improve performance of both translations, we suggest light-weight translator based on pseudo-machine instruction representation and tabular-base translation instead of typical intermediate representation. We implement our framework on Linux system, and our experimental evaluations show that it could quite effectively instrument the target with nominal overhead.	android;basic block;binary translation;debugging;device driver;experiment;instrumentation (computer programming);intermediate representation;kernel (operating system);linux;live variable analysis;machine code;mathematical optimization;memory map;operating system;overhead (computing);smart tv;smartphone;table (information);tracing (software);user space;x86;ios	Dongwoo Lee;Inhyuk Kim;Jee-hong Kim;Hyung Kook Jun;Won Tae Kim;Sangwon Lee;Young Ik Eom	2013	The Journal of Supercomputing	10.1007/s11227-013-0954-3	sysfs;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;kernel preemption;programming language;computer security;instrumentation	OS	-21.777064983350385	38.523050647560204	9074
a81c73e2e277f290bdf4dc2b0e34a61a2920afc8	picking statistically valid and early simulation points	program diagnostics;architecture configuration pipeline simulation hardware metrics statistically driven algorithm;probability;parallel architectures pipeline processing sampling methods probability program diagnostics system recovery;load imbalance;clustered processors;system recovery;inter pe communication;instruction replication;parallel architectures;sampling methods;instruction distribution;computational modeling computer architecture clustering algorithms vectors frequency computer science pipelines computer simulation hardware application software;pipeline processing;instructions per cycle	Modern architecture research relies heavily on detailed pipeline simulation. Simulating the full execution of an industry standard benchmark can take weeks to months to complete. To address this issue we have recently proposed using Simulation Points (found by only examining basic block execution frequency profiles) to increase the efficiency and accuracy of simulation. Simulation points are a small set of execution samples that when combined represent the complete execution of the program.In this paper we present a statistically driven algorithm for forming clusters from which simulation points are chosen, and examine algorithms for picking simulation points earlier in a program's execution - in order to significantly reduce fast-forwarding time during simulation. In addition, we show that simulation points can be used independent of the underlying architecture. The points are generated once for a program/input pair by only examining the code executed. We show the points accurately track hardware metrics (e.g., performance and cache miss rates) between different architecture configurations. They can therefore be used across different architecture configurations to allow a designer to make accurate trade-off decisions between different configurations.	algorithm;approximation error;basic block;benchmark (computing);cpu cache;cluster analysis;fast fourier transform;fast forward;loose coupling;mean squared error;monte carlo method;pipeline (computing);profiling (computer programming);relative change and difference;sampling (signal processing);simulation;technical standard;variance reduction	Erez Perelman;Greg Hamerly;Brad Calder	2003		10.1109/PACT.2003.1238020	sampling;computer architecture;parallel computing;real-time computing;computer science;operating system;probability;instructions per cycle	Arch	-6.6058307926955075	50.87851988549262	9078
cecb0790973143779fbdc983b5ea569487cd1b25	imgdw generator: a tool for generating data for medical image data warehouses		In this paper, we introduce ImgDW Generator, a tool that generates synthetic data for populating medical image data warehouses designed according to the relational technology. The tool supports different star schemas for the image data warehouse and offers a graphical interface that assists users to manipulate these schemas. ImgDW Generator can be used to generate data aiming at different scenarios of performance evaluation.	graphical user interface;performance evaluation;population;synthetic data;xerox star	Guilherme Muzzi da Rocha;Cristina Dutra de Aguiar Ciferri	2018			database;data warehouse;computer science	DB	-36.26189179643844	8.021079498206344	9084
42107363a593925740f79ea9c019b578b1679970	a security framework.		There is an assumption in the design and implementation of many distributed batch computing systems that once a task enters the system, the system can be fully trusted by all participants, even when the system spans administrative boundaries. As a result, execution hosts and other intermediaries have no way of independently confirming the origin of tasks, attackers have an incentive to attack the intermediaries who handle the tasks, and when results are returned to users, they have no way of determining where and how those results were computed. Users need to be able to specify policies that limit the actions their tasks can perform and the uses to which their delegated credentials can be put, and ways to link these policies to their jobs and credentials. In this thesis, I address these shortcomings by introducing and analyzing a framework of mechanisms that can be used to reduce the trustworthiness requirements of components in the system. The framework protects execution hosts by making the association between a particular task and a particular user explicit rather than implicit. It protects end users by permitting them to specify a policy regarding task confidentiality and integrity to accompany their tasks. Finally, it protects intermediaries by making them less attractive to attackers. With relaxed trustworthiness requirements on intermediaries, the benefits of sharing tasks and resources between different administrative domains may be realized without relaxing security policies.		Jeremy L. Jacob	1988			computer security model;cloud computing security;sherwood applied business security architecture;security information and event management;security engineering;security convergence;security service;security analysis;network security policy	OS	-46.9852948110824	58.6471291391851	9086
0846c28cf3bc30c3286b03e0a504e4fd7aa4d81a	designing interactive narrative systems: is object-orientation useful?	new technology;empirical study;object oriented model;object oriented methods;system analysis and design;interactive narrative;object oriented;object oriented analysis and design;experimental evaluation;design methodology	New technologies challenge our methods for system analysis and design. This article reports from an experimental evaluation of a typical object-oriented modeling method that was used to design an interactive narrative system that simulates a certain real world environment in order to train and assess the decision-making capabilities of persons operating in that specific setting. Based on this empirical study, we emphasize strengths and weaknesses of the objectoriented method. To complement this evaluation, the article also reports from a related empirical study of a process where general knowledge about storytelling and filmmaking was used as the methodological basis for designing a comparable system. From this second study, we elicit ideas for increasing the extent to which the object-oriented modeling method can support the design of a training and assessment system. r 2002 Elsevier Science Ltd. All rights reserved.	system analysis	Mikael B. Skov;Jan Stage	2002	Computers & Graphics	10.1016/S0097-8493(01)00178-9	object-oriented analysis and design;simulation;design methods;computer science;multimedia;object-oriented programming;empirical research;structured systems analysis and design method;computer graphics (images)	HCI	-52.295378227643745	23.09853284132487	9097
c43335ac76535d7ac446af367979db8ced6655ea	integrating requirements & specifications in the telecommunications service creation process	service creation;requirement specification	Existing telecommunications systems are gradually converging into a ubiquitous information infrastructure inside an open deregulated multi-provider telecommunications market place. Additionally, the demand for new value-generating telecommunications services is increasing and will increase rapidly in the years to come. Therefore, in order to derive a viable service paradigm, a service creation methodology is essential. After a brief presentation of such a proposed methodology, this paper focuses on its service analysis phase. More specifically, it determines the activities that take part in the service analysis phase and the artifacts that are produced, and examines important matters related to the role of use cases and the definition of conceptual models, interaction diagrams, operation contracts and state diagrams in the framework of telecommunications service engineering, exploiting the use of UML. Finally, alternative and complementary approaches for service analysis are highlighted and a validation attempt is briefly outlined.	interaction design;programming paradigm;requirement;state diagram;unified modeling language	Dionisis X. Adamopoulos	2004			service level requirement;computer science;basic service	SE	-58.99403471306108	18.64935305621326	9107
968a54340fd229b09df5872ca68bee942ab9ea8f	energy efficient scheduling for real-time systems with mixed workload	energy efficient;earliest deadline first;dynamic voltage scaling;worst case execution time;mixed workload real time system;energy consumption;periodic tasks;proceedings paper;inter task dynamic voltage scaling;actual workload;energy saving;slack time;real time systems	In spite of numerous inter-task dynamic voltage scaling (DVS) algorithms of real-time systems with either periodic tasks or aperiodic tasks, few of them were aimed at the mixed workload of both kind of tasks. A DVS algorithm for mixed workload real-time systems should not only focus on energy saving, but also should consider low response time of aperiodic tasks. In this paper, we develop an on-line energy efficient scheduling, called Slack Stealing for DVS (SS-DVS), to reduce CPU energy consumption for mixed workload real-time systems under the earliest deadline first (EDF) scheduling policy. The SS-DVS is based on the concept of slack stealing to serve aperiodic tasks and to save energy by using the dynamic reclaiming algorithm (DRA). Unlike other existing approaches, the SS-DVS does not need to know the workload and the worst case execution time of aperiodic tasks in advance. Experimental results show that the proposed SS-DVS obtains better energy reduction (17% ~ 22%) while maintaining the same response time compared to existing approaches.	algorithm;best, worst and average case;central processing unit;dynamic resolution adaptation;dynamic voltage scaling;earliest deadline first scheduling;image scaling;minimal recursion semantics;need to know;online and offline;real-time clock;real-time computing;real-time operating system;real-time transcription;response time (technology);run time (program lifecycle phase);scheduling (computing);simulation;slack variable;slow-scan television;web services enhancements;worst-case execution time	Jheng-Ming Chen;Kuochen Wang;Ming-Ham Lin	2007		10.1007/978-3-540-77092-3_4	parallel computing;real-time computing;earliest deadline first scheduling;computer science;distributed computing;efficient energy use;least slack time scheduling;worst-case execution time	Embedded	-5.457185211711154	58.81937896590494	9111
1b1450b53d404326f0da581c35d2ac1f03d6dc3b	morphing the hugin and shenoy-shafer architectures	redundancia;interrogation base donnee;interrogacion base datos;redundancy;query answering;database query;redondance	The Hugin and Shenoy–Shafer architectures are two variations on the jointree algorithm, which exhibit different tradeoffs with respect to efficiency and query answering power. The Hugin architecture is more time–efficient on arbitrary jointrees, avoiding some redundant computations performed by the Shenoy–Shafer architecture. This efficiency, however, comes at the price of limiting the number of queries the Hugin architecture is capable of answering. In this paper, we present a simple algorithm which retains the efficiency of the Hugin architecture and enjoys the query answering power of the Shenoy–Shafer architecture.	algorithm;computation;hugin;morphing;von neumann architecture	James D. Park;Adnan Darwiche	2003		10.1007/978-3-540-45062-7_12	computer science;theoretical computer science;data mining;database;redundancy	Arch	-25.41809287848033	6.007911090085591	9112
5285ce10f70e1b60e7d6b821b2879573a89903a3	in search of elegance in the theory and practice of computation		We present two models for data-centric workflows: the first based on business artifacts and the second on Active XML. We then compare the two models and argue that Active XML is strictly more expressive, based on a natural semantics and choice of observables. Finally, we mention several verification results for the two models.	computation;observable;operational semantics;xml	Val Tannen;Limsoon Wong;Leonid Libkin;Wenfei Fan;W. Tan;Michael P. Fourman	2013		10.1007/978-3-642-41660-6		Web+IR	-21.42737300814437	13.626647343559469	9125
61d88ca36ecf27f764bc07281d3c77cfd4a8d100	program speedup in a heterogeneous computing network	algorithm performance;multiprocessor;heterogeneous computing;efficient algorithm;reseau ordinateur;computer network;programming model;resultado algoritmo;scheduling;borne inferieure;performance algorithme;red ordenador;parallel machines;ordonamiento;systeme parallele;parallel system;multiprocesador;ordonnancement;lower bound;sistema paralelo;heterogeneous network;cota inferior;multiprocesseur	Program speedup is an important measure of the performance of an algorithm on a parallel machine. Of particular importance is the near linear or superlinear speedup exhibited by the most performance-eecient algorithms for a given system. We describe network and program models for heterogeneous networks, deene notions of speedup and superlinear speedup, and observe that speedup consists of both heterogeneous and parallel components. We also consider the case of linear tasks, give a lower bound for the speedup, and show that there is no theoretical upper limit on heterogeneous speedup.	algorithm;heterogeneous computing;parallel computing;speedup	Val Donaldson;Francine Berman;Ramamohan Paturi	1994	J. Parallel Distrib. Comput.	10.1006/jpdc.1994.1062	linear speedup theorem;mathematical optimization;parallel computing;multiprocessing;heterogeneous network;speedup;computer science;theoretical computer science;operating system;distributed computing;programming paradigm;upper and lower bounds;karp–flatt metric;scheduling;algorithm;symmetric multiprocessor system	HPC	-14.78086184196252	42.45163127344655	9128
5649bbc4cc8392918e19fbdb846872130d6fec67	implementing first-class polymorphic delimited continuations by a type-directed selective cps-transform	lenguaje programacion;anotacion;control effects;delimited continuations;theorie type;programming language;delimited continuation;program transformation;langage java;annotation;transformation programme;transformacion programa;tipificacion;control structure;typing;selective cps transform;polymorphism;type theory;typage;langage programmation;direction selectivity;lenguaje java;polymorphisme;polimorfismo;theory delimited continuations;languages;java language	We describe the implementation of first-class polymorphic delimited continuations in the programming language Scala. We use Scala's pluggable typing architecture to implement a simple type and effect system, which discriminates expressions with control effects from those without and accurately tracks answer type modification incurred by control effects. To tackle the problem of implementing first-class continuations under the adverse conditions brought upon by the Java VM, we employ a selective CPS transform, which is driven entirely by effect-annotated types and leaves pure code in direct style. Benchmarks indicate that this high-level approach performs competitively.	benchmark (computing);delimited continuation;delimiter;direct style;effect system;high- and low-level;java virtual machine;programming language;scala	Tiark Rompf;Ingo Maier;Martin Odersky	2009		10.1145/1596550.1596596	delimited continuation;computer science;theoretical computer science;programming language;algorithm	PL	-22.820161672198036	28.249294901201807	9139
5201c118bdcb4d650e8f859a4d7a95fd7ad1aba7	domain theoretic models of polymorphism	lenguaje programacion;theoretical model;programming language;language theory;lambda calculus;teoria lenguaje;modelo;category theory;informatique theorique;theorie categorie;polymorphism;langage programmation;lambda calculo;modele;λ calcul polymorphique;teoria categoria;lambda calcul;models;theorie langage;computer theory;informatica teorica	We give an illustration of a construction useful in producing and describing models of Girard and Reynolds’ polymorphic I-calculus. The key unifying ideas are that of a Grothendieck tibration and the category of continuous sections associated with it, constructions used in indexed category theory; the universal types of the calculus are interpreted as the category of continuous sections of the fibration. As a major example a new model for the polymorphic I-calculus is presented. In it a type is interpreted as a Scott domain. In fact, understanding universal types of the polymorphic I-calculus as categories of continuous sections appears to be useful generally. For example, the technique also applies to the tinitary projection model of Bruce and Longo, and a recent model of Girard. (Indeed the work here was inspired by Girard’s and arose through trying to extend the construction of his model to Scott domains.) It is hoped that by pin-pointing a key construction this paper will help towards a deeper understanding of models for the polymorphic I-calculus and the relations between them. d 1989 Academic Press. Inc.	application domain;category theory;domain theory;scott domain;system f	Thierry Coquand;Carl A. Gunter;Glynn Winskel	1989	Inf. Comput.	10.1016/0890-5401(89)90068-0	polymorphism;computer science;philosophy of language;calculus;lambda calculus;mathematics;programming language;algorithm;category theory	Logic	-12.55875607351917	17.559375586094482	9142
dc42484509e039d5c524f09ebf30d12413fe39a2	load distribution services in hla	long period;resource management;least squares approximation;computer architecture;large scale;computational modeling;distributed environment;geographic information systems;workstations;load management;run time infrastructure;load distribution;grid computing;multithreading;real time systems;computational modeling load management workstations least squares approximation resource management real time systems grid computing multithreading computer architecture geographic information systems	Running a large-scale HLA-based simulation in a distributed environment may require large amounts of computing resources at geographically different locations. A simulation running for a long period of time may result in imbalance of load levels at different computing hosts, hence leading to degradation of performance. Performance typically improves if the workload can be equally distributed among the different computing hosts. However, the Run Time Infrastructure (RTI) does not have any facilities to perform load distribution to alleviate this problem. This paper describes an architecture that provides load distribution services over a multi-threaded RTI to improve the performance of HLA simulations. A new metric for measuring the workload for both optimistic and conservative simulation federates is proposed. Load distribution is achieved through federate migration by introducing mobility and reactivity to each simulation federate.	anomaly detection;authentication;callback (computer programming);elegant degradation;experiment;federated identity;federation (information technology);load balancing (computing);programming paradigm;run-time infrastructure (simulation);server (computing);simulation;synchronous programming language;thread (computing)	Gary S. H. Tan;K. C. Lim	2004	Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications	10.1109/DS-RT.2004.27	parallel computing;real-time computing;computer science;distributed computing	Arch	-21.148109757261533	53.841941448585956	9147
10922128d76b6420e6f2c3435924184b65e3167e	transparent microprogramming in support of abstract type oriented dynamic vertical migration	increased security;dynamic vertical migration;abstract data type;paper discusses language aspect;current model;type visibility;data type;abstract type;migration decision;working prototype implementation;transparent microprogramming;source language;vlsi;microarchitecture;microcode;rom;vertical migration;control store	Migration of functionality into microcode for the purposes of performance improvement and increased security may be oriented toward the migration of abstract data types. This provides a model of migration consistent with current models of machine architecture. An advantage is that information in making migration decisions can be drawn from the programmer's model of his problem as encapsulated in the data types in the program. In addition, the migration can be sensitive to changes in type visibility in various execution environments of the program. The paper discusses language aspects of abstract type oriented migration (ATOM) and details a working prototype implementation. Ada is used as the source language in the description of ATOM but C is used in the prototype.	abstract data type;abstract type;ada;atom;microcode;programmer;prototype	Edward M. Carter;Robert I. Winner	1984		10.1145/800016.808227	computer architecture;parallel computing;data migration;data type;microarchitecture;computer science;operating system;microcode;very-large-scale integration;programming language;control store;abstract data type;read-only memory;diel vertical migration	PL	-22.557306119639943	35.142598275090606	9169
351670cdae6c826352c827831bd2f546995f95a5	algorithms for selecting energy-efficient storage servers in storage and computation oriented applications	energy conservation;green it technology;information systems;digital ecosystems;storage drives;power consumption models;energy efficient;computer model;storage media energy conservation information systems power aware computing;drives;storage media;power aware computing;servers power demand computational modeling power measurement continuous wavelet transforms drives;servers;computational modeling;electric power;continuous wavelet transform;storage drive energy efficient storage server selection computation oriented application electric power consumption information system;information system;power consumption;digital ecosystems green it technology power consumption models storage drives;power demand;power measurement;continuous wavelet transforms	The electric power consumption of servers has to be reduced in information systems in order to realize green societies. In information systems, servers mainly consume the electric power to process regrets from clients. We have to reduce the electric power consumed by servers. There are computation (CP), communication (CM), and storage (ST) types of applications to be performed on servers. In CP and CM applications, CPU and communication resources are mainly consumed, respectively. In this paper, we consider ST applications where storage drives are manipulated on a server in addition to CP applications. First, we measure the power consumption of a server to perform CP and ST types of application processes, C, R, and W processes which just compute, read files, and write files, respectively. Then, we discuss a power consumption model of a server by abstracting most essential parameters dominating the power consumption of the server from the experimental results. Here, the power consumption rate of a server is max um if at least one process is performed. However, the maximum power consumption rate of a server depends on types of processes concurrently performed on the server. In addition, we obtain an access rate model to show how long it takes to perform each type of process concurrently with other processes. By using both the power consumption model and the access rate model, we discuss algorithms from selecting a server so that not only the execution time but also the power consumption can be reduced.	algorithm;central processing unit;computation;information system;maximum power transfer theorem;reduction (complexity);run time (program lifecycle phase);server (computing);unified model	Takuro Inoue;Ailixier Aikebaier;Tomoya Enokido;Makoto Takizawa	2012	2012 IEEE 26th International Conference on Advanced Information Networking and Applications	10.1109/AINA.2012.136	computer simulation;embedded system;real-time computing;computer hardware;telecommunications;computer science;operating system;database;computer security;information system;client–server model;server;computer network;server farm	HPC	-18.09183057960997	59.1947659252445	9179
7f1611f6138972540f02c1640e18ad095c0363db	six strategies for building high performance soa applications.	service oriented architecture;strategies	The service-oriented architecture (SOA) concepts such as loose coupling may have negative impact on the overall execution performance of a single request. There are ways to facilitate high performance applications which benefit from this kind of architectural style compensating the caused overhead significantly. This paper gives an overview on six high level strategies to improve the performance of SOAs with a central service bus and presents how to apply them to build high performance service-oriented applications without corrupting the SOA paradigm and concepts on the technical level.	high-level programming language;loose coupling;mathematical optimization;microsoft outlook for mac;overhead (computing);performance tuning;programming paradigm;service-oriented architecture;service-oriented device architecture	Uwe Breitenbücher;Oliver Kopp;Frank Leymann;Michael Reiter;Dieter Roller;Tobias Unger	2012			real-time computing;architecture;architectural style;loose coupling;computer science	HPC	-25.35679909758378	54.77398219589795	9190
a7b29a27cf49a4246c816e5595c442d3d782cf4b	a deductive approach for resource interoperability and well-defined workflows	service composition;web service;integration;mediation;web services;semantic web;workflow;scientific protocol;domain ontology;ontology;bioinformatics	We present a model that supports Web service description, composition, and workflow design. The model is composed of an ontological layer that represents domain concepts and their relationships, and a resource layer that describes the resources in terms of a domain ontology. With our approach the design of a workflow is expressed as a conceptual query network while its implementation is computed deductively from the selected resource descriptions. We illustrate our approach with an application case from the domain of bioinformatics.	interoperability	Nadia Yaacoubi Ayadi;Zoé Lacroix;Maria-Esther Vidal	2008		10.1007/978-3-540-88875-8_127	web service;web modeling;ontology inference layer;web standards;computer science;ontology;data mining;database;world wide web;owl-s;workflow management system;workflow engine;workflow technology	HPC	-43.21116774524573	9.763689692882672	9192
5e6cee4d13c3110f857b3aec5b57d21f1eb89b83	parallel quasi-newton optimization using graphics processing units			graphics processing unit;newton	Kreshna Gopal;Liuxia Wang;Steven Washington	2009			texture mapping unit;general-purpose computing on graphics processing units;computational science;graphics;computer science	Graphics	-6.577682002751189	38.06067839023308	9225
2c28cf704ea2adc959cdc899d706316a2df2d7d1	radiator: context propagation based on delayed aggregation	context propagation;scalability;privacy	Context-aware systems take into account the user's current context (such as location, time and activity) to enrich the user interaction with the application. However, these systems may produce huge amounts of information that must be efficiently propagated to a group of people or even large communities while still protecting the privacy of the participants.  We argue that both scalability and privacy can be ensured by delaying context propagation until certain conditions are met and then aggregating such messages both at the syntactic and semantic level. Since such conditions vary from application to application, we propose Radiator, a systematic way to model the propagation characteristics of a distributed context-aware system.  Our qualitative evaluation shows that Radiator is generic enough to model the needs of different context propagation scenarios. To assess the impact of the model on the scalability of an application, we developed twiRadiator, an adaptation of Twitter to the Radiator model which, while preserving user expectations, reduces bandwidth consumption to approx. one third.	approximation;scalability;software propagation	Pedro Alves;Paulo José Azevedo Vianna Ferreira	2013		10.1145/2441776.2441806	scalability;simulation;computer science;communication;privacy;world wide web;computer security	HCI	-43.55452189881095	43.58982044831354	9227
bd0662c6172fba261f6d40f558f99f39fe86f11d	on-line monitoring of computer systems	online operation performance evaluation;performance evaluation;resource usage measurements on line monitoring computer systems on line diagnosis system event logging performance measurements;computerized monitoring conferences electronic equipment testing system testing application software;on line monitoring;online operation	The paper deals with the problem of on-line diagnosis of computer systems. It is based on three monitoring techniques related to system event logging, performance and resource usage measurements. This approach is illustrated with practical results.	online and offline;tracing (software)	Janusz Sosnowski;Marek Poleszak	2006	Third IEEE International Workshop on Electronic Design, Test and Applications (DELTA'06)	10.1109/DELTA.2006.71	real-time computing;simulation;engineering;computer engineering	Embedded	-25.38851677660811	50.01390531438334	9229
12694521b99d9766b504eb02c45624ab442eeb25	developing a keystroke biometric system for continual authentication of computer users	computers;keyboards;standards;biometrics access control;authorisation;training;keystroke biometrics;authentication;biometrics;system performance;machine learning;authentication computers training standards biometrics access control system performance keyboards;feature extraction;pattern classification authorisation biometrics access control feature extraction;pattern classification;pattern recognition;closed system performance keystroke biometric system continual authentication data windows keyboard input computer users authentication intruder detection training process data capturing feature extraction authentication classification receiver operating characteristic curve generation keystroke data system performance user population size;user authentication;user authentication pattern recognition machine learning biometrics keystroke biometrics	Data windows of keyboard input are analyzed to continually authenticate computer users and verify that they are the authorized ones. Because the focus is on fast intruder detection, the authentication process operates on short bursts of roughly a minute of keystroke input, while the training process can be extensive and use hours of input. The biometric system consists of components for data capture, feature extraction, authentication classification, and receiver-operating-characteristic curve generation. Using keystroke data from 120 users, system performance was obtained as a function of two independent variables: the user population size and the number of keystrokes per sample. For each population size, the performance increased (and the equal error rate decreased) roughly logarithmically as the number of keystrokes per sample was increased. The best closed-system performance results of 99 percent on 14 participants and 96 percent on 30 participants indicate the potential of this approach.	authentication;authorization;biometrics;event (computing);feature extraction;intruder detection;microsoft windows;plover;user (computing)	John V. Monaco;Ned Bakelman;Sung-Hyuk Cha;Charles C. Tappert	2012	2012 European Intelligence and Security Informatics Conference	10.1109/EISIC.2012.58	feature extraction;computer science;machine learning;authentication;computer performance;authorization;internet privacy;world wide web;computer security;biometrics	DB	-50.86681047518803	6.348299521313918	9236
74368a25577ac23b7a5abca7209b830e06934c40	logical aspects of learning concepts	modal logic;information system;incomplete information;rough sets;acquisition of concepts;logical aspect	This paper is concerned with semantic analysis of learning concepts within the framework of rough set theory. A logic is introduced to express and prove facts about relationships between extensions and intensions of concepts in an incompletely specified universe.		Ewa Orlowska	1988	Int. J. Approx. Reasoning	10.1016/0888-613X(88)90109-0	modal logic;complete information;theoretical computer science;machine learning;artificial intelligence;mathematics;information system;universe;rough set	AI	-19.10802722728814	7.187915596607979	9238
5bc3a2917fca44d3a25179906aa4b7cd77bc7b0b	privacy implications of room climate data		Smart heating applications promise to increase energy efficiency and comfort by collecting and processing room climate data. While it has been suspected that the sensed data may leak crucial personal information about the occupants, this belief has up until now not been supported by evidence.		Frederik Armknecht;Zinaida Benenson;Philipp Morgner;Christian Müller;Christian Riess	2019	Journal of Computer Security	10.3233/JCS-181133	data science;computer science;distributed computing	Security	-32.722407555072806	18.73525028263204	9259
68e8c14ede92e4747dd5d21118bac1859f08e5f3	heterogeneous computing for epidemiological model fitting and simulation	asynchronous;epidemiology;gpu;heterogeneous computing;infectious diseases;ode;pde;parallel;particle swarm optimization;sir model	Over the last years, substantial effort has been put into enhancing our arsenal in fighting epidemics from both technological and theoretical perspectives with scientists from different fields teaming up for rapid assessment of potentially urgent situations. This paper focusses on the computational aspects of infectious disease models and applies commonly available graphics processing units (GPUs) for the simulation of these models. However, fully utilizing the resources of both CPUs and GPUs requires a carefully balanced heterogeneous approach. The contribution of this paper is twofold. First, an efficient GPU implementation for evaluating a small-scale ODE model; here, the basic S(usceptible)-I(nfected)-R(ecovered) model, is discussed. Second, an asynchronous particle swarm optimization (PSO) implementation is proposed where batches of particles are sent asynchronously from the host (CPU) to the GPU for evaluation. The ultimate goal is to infer model parameters that enable the model to correctly describe observed data. The particles of the PSO algorithm are candidate parameters of the model; finding the right one is a matter of optimizing the likelihood function which quantifies how well the model describes the observed data. By employing a heterogeneous approach, in which both CPU and GPU are kept busy with useful work, speedups of 10 to 12 times can be achieved on a moderate machine with a high-end consumer GPU as compared to a high-end system with 32 CPU cores. Utilizing GPUs for parameter inference can bring considerable increases in performance using average host systems with high-end consumer GPUs. Future studies should evaluate the benefit of using newer CPU and GPU architectures as well as applying this method to more complex epidemiological scenarios.	algorithm;architecture as topic;cpu (central processing unit of computer system);central processing unit;communicable diseases;computation (action);computer graphics;curve fitting;end system;epidemiology;genetic heterogeneity;graphics processing unit;heterogeneous computing;inference;likelihood functions;mathematical optimization;open dynamics engine;particle swarm optimization;population parameter;simulation	Thomas Kovac;Tom Haber;Frank Van Reeth;Niel Hens	2018		10.1186/s12859-018-2108-3	symmetric multiprocessor system;genetics;ode;asynchronous communication;graphics;epidemic model;biology;computer graphics;particle swarm optimization;distributed computing	HPC	-6.439386100540088	34.62095202294615	9282
83afadf78da85b7109136a0fd2b4dbff96e5e6fd	complete removal of redundant expressions (with retrospective)	code size explosion;speculative code motion;pre implementation;code growth;control flow restructuring;additional code growth reduction;code motion;partial redundancy elimination;redundant expression;profile-guided optimization;resulting code duplication;speculative execution;complete removal;demand-driven frequency data-flow analysis;complete pre;acceptable code growth	Partial redundancy elimination (PRE), the most important component of global optimizers, generalizes the removal of common subexpressions and loop-invariant computations. Because existing PRE implementations are based on code motion, they fail to completely remove the redundancies. In fact, we observed that 73% of loop-invariant statements cannot be eliminated from loops by code motion alone. In dynamic terms, traditional PRE eliminates only half of redundancies that are strictly partial. To achieve a complete PRE, control flow restructuring must be applied. However, the resulting code duplication may cause code size explosion.This paper focuses on achieving a complete PRE while incurring an acceptable code growth. First, we present an algorithm for complete removal of partial redundancies, based on the integration of code motion and control flow restructuring. In contrast to existing complete techniques, we resort to restructuring merely to remove obstacles to code motion, rather than to carry out the actual optimization.Guiding the optimization with a profile enables additional code growth reduction through selecting those duplications whose cost is justified by sufficient execution-time gains. The paper develops two methods for determining the optimization benefit of restructuring a program region, one based on path-profiles and the other on data-flow frequency analysis. Furthermore, the abstraction underlying the new PRE algorithm enables a simple formulation of speculative code motion guaranteed to have positive dynamic improvements. Finally, we show how to balance the three transformations (code motion, restructuring, and speculation) to achieve a near-complete PRE with very little code growth.We also present algorithms for efficiently computing dynamic benefits. In particular, using an elimination-style data-flow framework, we derive a demand-driven frequency analyzer whose cost can be controlled by permitting a bounded degree of conservative imprecision in the solution.	algorithm;computation;control flow;dataflow;frequency analysis;loop-invariant code motion;mathematical optimization;partial redundancy elimination;speculative execution	Rastislav Bodík;Rajiv Gupta;Mary Lou Soffa	1998		10.1145/989393.989453	computer science;theoretical computer science;expression (mathematics)	PL	-18.61301938078591	32.123301679828494	9286
b64d5d8af3998807cb5f91a7311538514b7f2779	experimental comparison of call string and functional approaches to interprocedural analysis	interprocedural analysis;flot donnee;flujo datos;optimizacion compiladora;functional programming;analisis programa;transfer function;compiler optimization;programmation fonctionnelle;program analysis;analyse programme;data flow;programacion funcional;optimisation compilateur;reachability analysis;analyse atteignabilite	The techniques which are used to implement (non-trivial) interprocedural data ow analyzers can be generally divided into two subsets: the call string and the functional approach as presented in Sharir and Pnueli, 1981]. Both diier in their time and space complexity as well as in the precision due to properties of the abstract domains and transfer functions. We have developed a data ow analyzer generator PAG which is able to produce interprocedural analyzers for both techniques. We speciied two variants of constant propagation working in an ANSI-C compiler; a copy constant propagation that uses distribu-tive transfer function and can be solved precisely, even interprocedurally Sagiv et al., 1995], and a full constant propagator which includes an interpreter for expressions of the language. We present the practical relevant results applying both analyzers to a fair set of real-world programs and compare the space/time consumption of the analyzers versus their precision.	abstract interpretation;compiler;constant folding;dspace;functional approach;interprocedural optimization;propagator;software propagation;transfer function	Florian Martin	1999		10.1007/978-3-540-49051-7_5	program analysis;data flow diagram;computer science;theoretical computer science;optimizing compiler;transfer function;programming language;functional programming;algorithm	PL	-19.59753484327692	29.820153933590984	9290
5c89fe4936e1638bf2ed154bfe849388e9b1c12d	quest, an owl 2 ql reasoner for ontology-based data access		Ontology Based Data Access (OBDA) has drawn considerable attention from the OWL and RDF communities. In OBDA, instance data is accessed by means of mappings, which state the relationship between the data in a data source (e.g., an RDBMSs) and the vocabulary of an ontology. In this paper we present Quest, a new system for OBDA focused on fast and efficient reasoning with large ontologies and large volumes of data. Quest provides SPARQL query answering with OWL 2 QL/RDFS entailments and can function as a traditional OWL reasoner/triple store, or as a mediator, located on-top of a legacy data source linked to the ontology by means of mappings. In such configuration all data remains in the data source and is only accessed at run-time. Quest uses query rewriting techniques as the inference mechanism in both modes. In this paper we describe the architecture of Quest, and the optimization techniques it currently implements.	conjunctive query;data access;field (computer science);interoperation;mathematical optimization;ontology (information science);question answering;rdf schema;relational database management system;resource description framework;rewriting;run time (program lifecycle phase);sparql;semantic web rule language;semantic reasoner;triplestore;vocabulary;web ontology language	Mariano Rodriguez-Muro;Diego Calvanese	2012			architecture;data mining;rdf;database;computer science;owl-s;ontology (information science);web ontology language;rdf schema;sparql;semantic reasoner	AI	-35.54911143019692	5.147527279917262	9292
438d640dd35ad943be4c54f2db6e84f60854f908	on the opportunities of scalable modeling technologies: an experience report on wind turbines control applications development		Scalability in modeling has many facets, including the ability to build larger models and domain specific languages (DSLs) efficiently. With the aim of tackling some of the most prominent scalability challenges in Model-based Engineering (MBE), the MONDO EU project developed the theoretical foundations and open-source implementation of a platform for scalable modeling and model management. The platform includes facilities for building large DSLs, for splitting large models into sets of smaller interrelated fragments, and enables modelers to construct and refine complex models collaboratively, among other features. This paper reports on the improvements provided by the MONDO technologies in a software development division of IK4-IKERLAN, a Medium-sized Enterprise which in recent years has embraced the MBE paradigm. The evaluation, conducted in the Wind Turbine Control Applications development domain, shows that scalable MBE technologies give new growth opportunities to Small and Medium-sized Enterprises.		Abel Gómez;Xabier Mendialdua;Gábor Bergmann;Jordi Cabot;Csaba Debreceni;Antonio Garmendia;Dimitrios S. Kolovos;Juan de Lara;Salvador Trujillo	2017		10.1007/978-3-319-61482-3_18	computer science;systems engineering;wind power;scalability;domain-specific language	SE	-60.8871017580609	20.430251437153064	9297
c822c125fdb674c53f6e7f0a8b3273accfe0acc6	gamma graph calculi for modal logics	gamma graphs;peirce;existential graphs;broken-cut operator;modal logic;epistemic logic	We describe Peirce’s 1903 system of modal gamma graphs, its transformation rules of inference, and the interpretation of the broken-cut modal operator. We show that Peirce proposed the normality rule in his gamma system. We then show how various normal modal logics arise from Peirce’s assumptions concerning the broken-cut notation. By developing an algebraic semantics we establish the completeness of fifteen modal logics of gamma graphs. We show that, besides logical necessity and possibility, Peirce proposed an epistemic interpretation of the broken-cut modality, and that he was led to analyze constructions of knowledge in the style of epistemic logic.	algebraic semantics (computer science);conceptualization (information science);emoticon;epistemic modal logic;existential graph;graphical user interface;linear algebra;modal operator;modality (human–computer interaction);putnam model;reverse polish notation;secondary source	Minghui Ma;Ahti-Veikko Pietarinen	2017	Synthese	10.1007/s11229-017-1390-3	normal modal logic;discrete mathematics;mathematics;accessibility relation;algorithm;modal operator	AI	-11.36306829321442	10.708201132387234	9301
ea5c73487940e9711eacbe154f5cd80a5e46b805	discrimination network for rule condition matching in object-oriented database rule systems	discrimination network;production rules;object oriented database systems;pattern matching;object oriented database;production rule;object oriented database rule system	In this paper, we propose a design for the integration of a production rule system into an object-oriented database system. We put an emphasis on the rule system, and propose a pattern matching algorithm based on a discrimination network. A detail methodology for constructing a discrimination network for different types of rule conditions is presented.	algorithm;data model;database;operand;pattern matching;production (computer science);relational model	Moez Chaabouni;Soon Myoung Chung	1995		10.1145/315891.315903	intelligent database;computer science;pattern matching;pattern recognition;data mining;production rule representation;database;programming language;database schema;database design	DB	-30.63243729001897	10.54641716334976	9311
13dce00322e7e4fea5de0ac0e1f2ebc50c5e513d	distributed concurrent development of software systems: an object-oriented process model	description languages software systems object oriented process model development process model distributed concurrent development large scale switching software systems process programming concept software development processes process description languages behavior description languages process design process;software systems;software development process;object oriented programming;development process;software engineering;process design;large scale;software engineering object oriented programming;object oriented;software systems object oriented modeling programming production standards development large scale systems hardware process design productivity electronic mail;software development environment;object oriented approach;object oriented software development;process model;parallel processing;dynamic behavior	This papa proposes a new development process model distributed concurrent development. The model is bom from various experiences in the development of largescale switching software systems. First, the background and motivation where the distributed concurrent development has emerged are explained. Then, critical issues in the distributed concurrent development are discussed based on a process programming concepc observations on analogies between the modeling of distributed c o n c m t development and the disciplines of distributed concurrent software systems. Since an appmprhe process model is necessary to describe the development process and to analyze it, we propose a formalization of software development processes with an object-oriented approach. Two process description languages and two behavior description languages are propmed. which enable us to represent the structure and dynamic behavior of development processes, respectively. Besides developnent process itself, we propose a processdesign process which is unique in that we can design and evaluate a development process before we initiate development activities. The entire processes for developing a family of switching software systems have been defined with the description languages. Various propexties of distributed concurrent development are also discussed.	distributed computing;process modeling;software development;software system	Mikio Aoyama	1990		10.1109/CMPSAC.1990.139377	parallel processing;design process;computer science;systems engineering;software development;software design description;software engineering;iterative and incremental development;systems development life cycle;programming language;object-oriented programming;empirical process;goal-driven software development process;software development process	SE	-50.07767824508518	29.59594734846562	9316
18a855b33fbdf8f0a36bf55c543b1285aeb5a978	fail-aware untrusted storage	weak fork linearizability fail aware untrusted storage online service provider failure detection storage protocol;protocols;groupware;history;online service provider;service provider;probability density function;failure detection;data mining;fail aware untrusted storage;protocols stability laboratories online communities technical collaboration collaborative tools collaborative work cloud computing contracts resists timing;servers;system recovery;registers;weak fork linearizability;semantic web;system recovery groupware protocols semantic web;real time systems;storage protocol	We consider a set of clients collaborating through an online service provider that is subject to attacks, and hence not fully trusted by the clients. We introduce the abstraction of a fail-aware untrusted service, with meaningful semantics even when the provider is faulty. In the common case, when the provider is correct, such a service guarantees consistency (linearizability) and liveness (wait-freedom) of all operations. In addition, the service always provides accurate and complete consistency and failure detection. We illustrate our new abstraction by presenting a Fail-Aware Untrusted STorage service (FAUST). Existing storage protocols in this model guarantee so-called forking semantics. We observe, however, that none of the previously suggested protocols suffice for implementing fail-aware untrusted storage with the desired liveness and consistency properties (at least wait-freedom and linearizability when the server is correct). We present a new storage protocol, which does not suffer from this limitation, and implements a new consistency notion, called weak fork-linearizability. We show how to extend this protocol to provide eventual consistency and failure awareness in FAUST.	eventual consistency;faust;linearizability;liveness;online service provider;server (computing)	Christian Cachin;Idit Keidar;Alexander Shraer	2009	2009 IEEE/IFIP International Conference on Dependable Systems & Networks	10.1109/DSN.2009.5270299	service provider;communications protocol;probability density function;computer science;consistency model;operating system;semantic web;database;distributed computing;processor register;eventual consistency;world wide web;computer security;server;computer network	OS	-44.50714437689918	56.85346438596155	9321
42d08c79b0753ba9bc60be8a114e1c1320e3c9a6	a method to enhance service survivability based on autonomous configuration	mass customization;software production;product customisation;feature modeling;feature selection feature configuration modeling software product line software production software product mass customization;problem solving software maintenance mass customization application software programming mass production software reusability computer architecture software quality costs;software product mass customization;software reusability;feature selection;software reusability configuration management product customisation;theoretical foundation;software product line;configuration management;problem solving;feature configuration modeling	This paper puts forward the conception of service survivability and proposes an algorithm to enhance essential service survivability by autonomous configuration. This algorithm considers that survivability is the most important element of atomic module fulfilling essential service instead of failure rate. The contrast experiment validated the survivability computing method and the simulation confirmed that the autonomous configuration algorithm can efficiently enhance the survivability of essential service.	algorithm;autonomous robot;failure rate;simulation	Lejun Zhang;Wu Yang;Zhongyong Wang;Lin Guo;Yongtian Yang	2007	Second International Multi-Symposiums on Computer and Computational Sciences (IMSCCS 2007)	10.1109/IMSCCS.2007.44	reliability engineering;reusability;verification and validation;software sizing;software configuration management;computer science;systems engineering;package development process;backporting;software design;software framework;component-based software engineering;software development;software design description;feature-oriented domain analysis;software engineering;software construction;resource-oriented architecture;software measurement;software deployment;feature model;software system	EDA	-57.1533598312732	27.51683944640405	9327
26df1279667e096dcd043a8935ad0f845bf7b761	finitary higher inductive types in the groupoid model		A higher inductive type of level 1 (a 1-hit) has constructors for points and paths only, whereas a higher inductive type of level 2 (a 2-hit) has constructors for surfaces too. We restrict attention to finitary higher inductive types and present general schemata for the types of their point, path, and surface constructors. We also derive the elimination and equality rules from the types of constructors for 1-hits and 2-hits. Moreover, we construct a groupoid model for dependent type theory with 2-hits and point out that we obtain a setoid model for dependent type theory with 1-hits by truncating the groupoid model.		Peter Dybjer;Hugo Moeneclaey	2018	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2018.03.019	discrete mathematics;dependent type;computer science;inductive type;intuitionistic type theory;setoid;schema (psychology);homotopy type theory;finitary;restrict	PL	-9.43793566545682	15.556478671953098	9332
4f005bee7580db8e0fec0bebb76ad098c53c5b93	toward situational secure web services design methods	software systems;situational secure web services design secure web service design method;web service;process design;navigation;design method;web design;web services design methodology guidelines security laboratories computer science navigation software systems process design proposals;guidelines;web services;computer science;secure web service design method;situational secure web services design;web services security of data web design;security;proposals;security of data;design methodology	SWSD practices cannot be defined independently of the situation in which they are applied. To address this challenge, we propose an approach for guiding the construction and execution of situational SWSD methods by reusing and integrating different existing SWSD method components suited to the specific situation on hand.	web service	Dhafer Thabet;Lamia Hassine;Henda Hajjami Ben Ghézala	2007	IEEE International Conference on Web Services (ICWS 2007)	10.1109/ICWS.2007.175	web service;design methods;computer science;knowledge management;information security;database;law;world wide web	Robotics	-49.13009453753319	19.718127121537186	9344
805367e3e51080c275c7a6156bb90b320833bc48	a ramsey theorem in boyer-moore logic	peano arithmetic	We use the Boyer-Moore Prover, Nqthm, to verify the Paris-Harrington version of Ramsey's theorem. The proof we verify is a modification of the one given by Ketonen and Solovay. The theorem is not provable in Peano Arithmetic, and one key step in the proof requires ε0 induction.	boyer–moore string search algorithm;nqthm;peano axioms;provable prime;ramsey's theorem	Kenneth Kunen	1995	Journal of Automated Reasoning	10.1007/BF00881917	gödel's incompleteness theorems;discrete mathematics;computer-assisted proof;robinson arithmetic;provability logic;gentzen's consistency proof;computer science;ramsey theory;true arithmetic;mathematics;compactness theorem;programming language;non-standard model of arithmetic;peano axioms;algorithm;gödel's completeness theorem	Theory	-10.003769469019565	16.123122567283854	9349
a6252e336bbcb68058d3a0f6c20faa790ac88c8b	instrumentation-driven validation of dataflow applications	models of computation;design validation;signal processing systems;dataflow graphs	Dataflow modeling offers a myriad of tools for designing and optimizing signal processing systems. A designer is able to take advantage of dataflow properties to effectively tune the system in connection with functionality and different performance metrics. However, a disparity in the specification of dataflow properties and the final implementation can lead to incorrect behavior that is difficult to detect. This motivates the problem of ensuring consistency between dataflow properties that are declared or otherwise assumed as part of dataflow-based application models, and the dataflow behavior that is exhibited by implementations that are derived from the models. In this paper, we address this problem by introducing a novel dataflow validation framework (DVF) that is able to identify dispariIlya Chukhman ilya@umd.edu Yang Jiao yjiao1@umd.edu Haifa Ben Salem haifa.ben-salem@tum.de Shuvra S. Bhattacharyya ssb@umd.edu 1 Department of Electrical, Computer Engineering, University of Maryland, College Park, MD, USA 2 Electronics, Electrical, Computer Engineering, Technische Universitat Munchen, Munich, Germany 3 Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USA 4 Department of Pervasive Computing, Tampere University of Technology, Tampere, Finland ties between an application’s formal dataflow representation and its implementation. DVF works by instrumenting the implementation of an application and monitoring the instrumentation data as the application executes. This monitoring process is streamlined so that DVF achieves validation without major overhead. We demonstrate the utility of our DVF through design and implementation case studies involving an automatic speech recognition application, a JPEG encoder, and an acoustic tracking application.	acoustic cryptanalysis;binocular disparity;computer engineering;dataflow;encoder;fault detection and isolation;instrumentation (computer programming);jpeg;molecular dynamics;overhead (computing);run time (program lifecycle phase);salem;signal processing;speech recognition;ubiquitous computing;yang	Ilya Chukhman;Yang Jiao;Haifa Ben Salem;Shuvra S. Bhattacharyya	2016	Signal Processing Systems	10.1007/s11265-015-1073-6	dataflow architecture;model of computation;embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language;algorithm	Mobile	-23.800011626682487	37.11845844451636	9350
2d313b5211681a1d7a95bdbbca4c0e3aedd4b99f	bayesian synthesis of probabilistic programs for automatic data modeling		We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.		Feras A. Saad;Massachusetts;Marco F. Cusumano-Towner;Ulrich Schaechtle;Martin C. Rinard;Vikash K. Mansinghka	2019	PACMPL	10.1145/3290350	modeling language;theoretical computer science;computer science;probabilistic logic;machine learning;data modeling;data analysis;artificial intelligence;bayesian inference;data set;bayesian probability;rule-based machine translation	PL	-23.49087424871781	14.06443200265229	9380
2f73845dd08e299061332f0b562ce54ca08a18c9	on characteristic formulae for event-recording automata	event clock automata;timed mu calculus;timed logic;bisimulation;model checking;timed bisimulation;timed automata	A standard bridge between automata theory and logic is provided by the notion of characteristic formula. This paper investigates this problem for the class of event-recording automata (ERA), a subclass of timed automata in which clocks are associated with actions and that enjoys very good closure properties (complementation, determinization...). We first study the problem of expressing characteristic formulae for ERA in Event-Recording Logic (ERL), a logic introduced by Sorea to express event-based timed specifications. We prove that the construction proposed by Sorea for ERA without invariants is false. More generally, we prove that bisimulation can not be expressed in ERL for the class of ERA, even without invariants. Then, we introduce the logic WTμ, a new logic for event-based timed specifications, closer to the timed logic Lν . We prove that it is strictly more expressive than ERL, and that its model-checking problem against ERA is EXPTIME-complete. Finally, we provide constructions for characterizing ERA up to timed (bi)similarity and study the complexity issues.	algorithm;automata theory;bisimulation;boolean satisfiability problem;exptime;erlang (programming language);model checking;real-time clock;real-time computing;simulation;time complexity;timed automaton;unary operation	Omer Nguena-Timo;Pierre-Alain Reynier	2009		10.1051/ita/2012029	model checking;discrete mathematics;computer science;bisimulation;programming language;timed automaton	Logic	-10.769221053065566	24.3856836803444	9382
52f0189205bb47fe96f795cff856652bbc43008f	a framework for distributed reasoning on the semantic web based on open answer set programming	network topology;semantic web	The Semantic Web envisions a Web where information is represented by means of formal vocabularies called ontologies for enabling automatic processing and retrieval of information. It is expected that ontologies will be interconnected by mappings forming a network topology. Reasoning with such a network of ontologies is a big challenge due to scalability issues. The local model semantics seems to be a promising approach in this direction that has served as the basis of several frameworks/distributed languages. The intent of the work described in this paper is to define a new framework for representing and reasoning with interconnected ontologies that will be based on a new language for representing ontologies and mappings called Distributed Open Answer Set Programming (DOASP). DOASP is a syntactical extension of OASP in the direction of distributedness and its semantics is a combination between the local model semantics and the OASP semantics. The reason for choosing OASP is the emerging interest in hybrid formalisms for the Semantic Web. 1 Research Context and Problem Statement While the current Web is only usable by humans, the Semantic Web [1] envisions to make information processable and services on the Web usable by machines as well. The main technology for establishing the Semantic Web is the creation of so-called ontologies, which can be used to represent information and services on the Web. Ontologies are commonly defined as formal specifications of shared conceptualizations [2, 3] where the sharing aspect is important: ontologies have to be reusable. This reusability does not mean that on the Web there will be a single ontology for capturing all the knowledge (not even all the knowledge corresponding to a given domain); there will be different ontologies that describe the same domain at different levels of granularity or from different perspectives, or that describe partially overlapping domains. In order to effectively use such overlapping ontologies one needs a means to describe the way they are interconnected. In particular, one uses logical axioms called mappings to relate elements of one ontology to elements of others. In order to represent such ontologies and mappings, logical languages such as Description Logics (DLs) [4] or Logic Programming(LP) [5] can be used. The usage of formal languages allows in general for well-defined formal reasoning and thus the necessary automation of tasks such as checking consistency of ontologies. Some popular languages for representing ontologies anchored in one or both of the mentioned formalisms are WSML [6], OWL [7], SWRL [8].	answer set programming;description logic;formal language;network topology;ontology (information science);scalability;semantic web rule language;vocabulary;web ontology language;world wide web	Cristina Feier	2007			social semantic web;natural language processing;formal specification;description logic;ontology (information science);answer set programming;data mining;semantic web;semantics;formal language;artificial intelligence;computer science	AI	-43.193205445190564	11.93297727658756	9383
13875088254a585cd0b050f3bc27c1af9ada690f	design principles for scaling multi-core oltp under high contention	multicore;concurrency control;scalability;database architecture;transaction processing	Although significant recent progress has been made in improving the multi-core scalability of high throughput transactional database systems, modern systems still fail to achieve scalable throughput for workloads involving frequent access to highly contended data. Most of this inability to achieve high throughput is explained by the fundamental constraints involved in guaranteeing ACID --- the addition of cores results in more concurrent transactions accessing the same contended data for which access must be serialized in order to guarantee isolation. Thus, linear scalability for contended workloads is impossible. However, there exist flaws in many modern architectures that exacerbate their poor scalability, and result in throughput that is much worse than fundamentally required by the workload. In this paper we identify two prevalent design principles that limit the multi-core scalability of many (but not all) transactional database systems on contended workloads: the multi-purpose nature of execution threads in these systems, and the lack of advanced planning of data access. We demonstrate the deleterious results of these design principles by implementing a prototype system, Orthrus, that is motivated by the principles of separation of database component functionality and advanced planning of transactions. We find that these two principles alone result in significantly improved scalability on high-contention workloads, and an order of magnitude increase in throughput for a non-trivial subset of these contended workloads.	acid;data access;database transaction;existential quantification;multi-purpose viewer;multi-core processor;online transaction processing;prototype;scalability;serialization;throughput	Kun Ren;Jose M. Faleiro;Daniel J. Abadi	2016		10.1145/2882903.2882958	multi-core processor;parallel computing;real-time computing;scalability;transaction processing;computer science;concurrency control;database;distributed computing;data architecture	DB	-18.124581358764853	53.43936619909368	9389
1c1e94f6b3c776f192e3ece337176ef87e9d654d	a language for specifying the composition of reliable distributed applications	distributed application;microwave integrated circuits;dataflow dependencies;formal specification;fault tolerant;dr stuart wheater;dynamic reconfiguration;open systems distributed processing software reliability specification languages formal specification authoring languages;distributed processing;heterogeneous environment;distributed computing;software fault tolerance;reliable distributed applications;authoring languages;specification language;software fault tolerance reliable distributed applications specification language scripting language task composition inter task dependencies heterogeneous environment dataflow dependencies modularity interoperability dynamic reconfigurability;dynamic reconfigurability;eprints newcastle university;specification languages;emeritus professor santosh shrivastava;open access;fault tolerance;modularity;interoperability;frederic ranno;fault tolerance microwave integrated circuits distributed computing buildings;open systems;software reliability;buildings;scripting language;task composition;inter task dependencies	This paper describes the design of a scripting language aimed at expressing task (unit of computation) composition and inter-task dependencies of distributed applications whose execution could span arbitrary large durations. This work is motivated by the observation that an increasingly large number of distributed applications are constructed by composing them out of existing applications, and are executed in an heterogeneous environment. The resulting applications can be very complex in structure, containing many notification and dataflow dependencies between their constituent applications. The language enables applications to be structured with the properties of modularity, interoperability, dynamic reconfigurability and faulttolerance.	computation;dataflow;distributed computing;interoperability;modularity (networks);reconfigurability;scripting language	Frédéric Ranno;Santosh K. Shrivastava;Stuart M. Wheater	1998		10.1109/ICDCS.1998.679806	fault tolerance;real-time computing;computer science;operating system;database;distributed computing;programming language	OS	-38.173124817772226	39.198036076929476	9392
700aa9e5bd3dd716ed73f428f10a0a5aba554d64	know-who/know-how navigation using development project-related taxonomies	electronic mail;semantics based knowledge management;know who know how extraction;information retrieval;navigation taxonomy data mining graphical user interfaces research and development telegraphy telephony laboratories knowledge management visualization;e mail analysis;resource management;knowledge management;text analysis;data mining;development project related ontology semantics based knowledge management know who know how extraction e mail analysis;know how navigation;knowledge visualization;navigation;text analysis electronic mail information retrieval knowledge management;development project related ontology;corporate knowledge management system;taxonomy;e mails;semantic web;know who navigation development project specific taxonomies corporate knowledge management system e mails knowledge visualization business keywords know how navigation;business keywords;know who navigation;development project specific taxonomies	In this demonstration, we introduce a corporate knowledge management system. Our system uses develeoment project-specific taxonomies to extract know-who/know-how information from e-mails automatically. After that, it visualizes knowledge on taxonomies in an understandable way. Accordingly, the system lets us easily learn about business keywords and knowledge that each developer has.	email;knowledge management;taxonomy (general)	Takahiro Madokoro;Makoto Nakatsuji;Kenichiro Okamoto;Sumio Miyazaki;Tsuyoshi Harada	2009	2009 IEEE International Conference on Semantic Computing	10.1109/ICSC.2009.57	navigation;computer science;knowledge management;resource management;semantic web;data mining;database;world wide web;taxonomy	Robotics	-46.144347010322754	9.090394809419038	9394
70c2962259124e37c28848581c031739c3e8db2f	a fault-tolerance strategy for pyramid architecture	fault tolerant		fault tolerance	Kuo-Liang Chung;Ferng-Ching Lin;Richard C. T. Lee	1989	J. Inf. Sci. Eng.		distributed computing;computer science;pyramid;fault tolerance;architecture	DB	-29.204727074289867	46.52593761694922	9395
269af3835ff79812ba4f5cf406d104ab95e5c0fa	automatic test data generation from embedded c code	programmation logique avec contrainte;software testing;fiabilidad;reliability;constraint logic programs;calculateur embarque;analyse statique;validacion;defecto;code coverage;automatic testing;securite informatique;programacion logica con restriccion;test data generation;logical programming;program verification;essai dynamique;software engineering;automatic generation;analisis estatica;generacion automatica prueba;analisis programa;installation essai;computer security;verificacion programa;symbolic execution;marcador;couverture;pointer;programmation logique;generation test;fiabilite;seguridad informatica;test facility;defect;software development;software industry;boarded computer;genie logiciel;defaut;generation automatique test;instalacion ensayo;automatic test generation;pointeur;test generation;probleme complementarite;problema complementariedad;validation;complementarity problem;source code;coverage;constraint logic programming;program analysis;logic programs;static analysis tools;analyse programme;ensayo dinamico;dynamic test;static analysis;verification and validation;verification programme;programacion logica;ingenieria informatica;generacion prueba;calculador embarque;cobertura	A fundamental area of software engineering that remains a challenge for software developers is the delivery of software with the minimum of remaining defects. While progress is constantly being made in the provision of static analysis tools to partly address this problem, the complementary dynamic testing approach, which remains an essential technique in the software industry for the verification and validation of software, has received less attention. Within the software testing activity, the actual generation of test data for the purpose of automated software testing is still mainly a manual task. We present CSET (C Symbolic Execution Tool) which automatically generates test data from C source code to fulfil code coverage criteria. CSET implements the symbolic execution technique with an intermediate path traversal conditions checker and a test data generation facility. We examine how the traditional problems associated with the symbolic execution technique have been overcome using Logic Programming and Constraint Logic Programming (CLP). The approach used to handle pointer manipulations is detailed. Interprocedural results on previously published sample code and industrial embedded C code with pointers are presented.	embedded c;test data generation	E. Dillon;Christophe Meudec	2004		10.1007/978-3-540-30138-7_16	program analysis;constraint logic programming;kpi-driven code analysis;development testing;verification and validation;regression testing;test data generation;real-time computing;verification and validation;pointer;fuzz testing;software sizing;software verification;computer science;software reliability testing;software framework;software development;software construction;reliability;dynamic testing;linear code sequence and jump;software testing;code coverage;programming language;concolic testing;static analysis;test management approach;algorithm;software quality;static program analysis;source code	EDA	-58.993750994021795	32.774841053243236	9414
06176c666ca01f98b86ef2a5201a6c89b0e50d6d	computer supported query formulation in an evolving context	query language;conceptual schema	Even if high-level query languages are used, query formulation may cause problems. This is notably so in case of large and complex application domains. Typical examples of these kinds of application domains are evolving application domains. In an evolving application domain not only populations may change, but also the conceptual schema. Even more, the history of the application domain should be recorded, and be retrievable. This paper focuses on support for query formulation in the context of large conceptual schemata. The solution presented uses the idea of query-by-navigation in conjunction with query-by-construction. First this idea is illustrated by means of some examples, then it is formally defined.	application domain;conceptual schema;high- and low-level;hypermedia;information system;path expression;population;query language	Arthur H. M. ter Hofstede;Henderik Alex Proper;Theo P. van der Weide	1995			web search query;computer science;database;query expansion;web query classification;rdf query language;query language;query by example;sargable;query optimization	DB	-35.15657083027039	8.840461221554486	9416
4538fcaf5778090f39d4db30127f71a5940e2eb8	bounded verification of voting software	programming language;java modeling language	We present a case-study in which vote-tallying software is analyzed using a bounded verification technique, whereby all executions of a procedure are exhaustively examined within a finite space given by a bound on the size of the heap and the number of loop unrollings. The technique involves an encoding of the procedure in an intermediate relational programming language, a translation of that language to relational logic, and an analysis of the logic that exploits recent advances in finite model-finding. Our technique yields concrete counterexamples – traces of the procedure that violate the specification. The vote-tallying software, used for public elections in the Netherlands, had previously been annotated with specifications in the Java Modeling Language and analyzed with ESC/Java2. Our analysis found counterexamples to the JML contracts, indicating bugs in the code and errors in the specifications that evaded prior analysis.	esc/java;java modeling language;logic programming;programming language;relational algebra;software bug;tracing (software)	Greg Dennis;Kuat Yessenov;Daniel O Jackson	2008		10.1007/978-3-540-87873-5_13	computer science;theoretical computer science;java modeling language;programming language;algorithm	PL	-17.747793862982437	26.73750325453378	9422
8e0a39497d4867ccf8a742183725af4d75082657	raps: a rule-based language for specifying resource allocation and time-tabling	allocation rule;allocation rules;resource management expert systems educational institutions humans knowledge acquisition engines job shop scheduling specification languages performance analysis production;formal specification;expert systems;raps;esra;compiled resource allocation problem specification;job shop scheduling;resource allocation;prolog;knowledge acquisition tool;rule based;resource management;inference mechanisms;indexing terms;time tabling problems;specification language;expert system paradigm;generalized inference engine;engines;control structure;specification languages;resource allocation specification;rule specification;scheduling;knowledge acquisition;performance analysis;backtracking;production;rule based language;prolog raps rule based language resource allocation specification time tabling problems expert system paradigm resource allocation problems heuristics specification language allocation rules knowledge acquisition tool language syntax rule specification backtracking generalized inference engine compiled resource allocation problem specification esra;humans;heuristics;inference mechanisms resource allocation knowledge acquisition specification languages expert systems formal specification scheduling;language syntax;resource allocation problems;expert system	A general language for specifying resource allocation and time-tabling problems is presented. The language is based on an expert system paradigm that was developed previously by the authors and that enables the solution of resource allocation problems by using experts' knowledge and heuristics. The language enables the specification of a problem in terms of resources, activities, allocation rules, and constraints, and thus provides a convenient knowledge acquisition tool. The language syntax is powerful and allows the specification of rules and constraints that are very difficult to formulate with traditional approaches, and it also supports the specification of various control and backtracking strategies. We constructed a generalized inference engine that runs compiled resource allocation problem specification language (RAPS) programs and provides all necessary control structures. This engine acts as an expert system shell and is called expert system for resource allocation (ESRA). The performance of RAPS combined with ESRA is demonstrated by analyzing its solution of a typical resource allocation problem. >	memoization;rule-based system	Gadi Solotorevsky;Ehud Gudes;Amnon Meisels	1994	IEEE Trans. Knowl. Data Eng.	10.1109/69.317700	index term;specification language;resource allocation;computer science;heuristics;machine learning;data mining;formal specification;database;programming language;control flow;prolog;scheduling;expert system;backtracking	DB	-26.888092109344157	20.412478268068256	9432
a5a728677240f80fcc7ab8fef1673d06b85c0892	software cost analysis of gpu-accelerated aeroacoustics simulations in c++ with openacc		Aeroacoustics simulations leverage the tremendous computational power of today’s supercomputers, e.g., to predict the noise emissions of airplanes. The emergence of GPUs that are usable through directive-based programming models like OpenACC promises a cost-efficient solution for flow-induced noise simulations with respect to hardware expenditure and development time. However, OpenACC’s capabilities for real-world C++ codes have been scarcely investigated so far and software costs are rarely evaluated and modeled for this kind of high-performance projects. In this paper, we present our OpenACC parallelization of ZFS, an aeroacoustics simulation framework written in C++, and its early performance results. From our implementation work, we derive common pitfalls and lessons-learned for real-world C++ codes using OpenACC. Furthermore, we borrow software cost estimation techniques from software engineering to evaluate the development efforts needed in a directive-based HPC environment. We discuss applicability and challenges of the popular COCOMO II model applied to the parallelization of ZFS.	c++;graphics processing unit;openacc	Marco Nicolini;Julian Miller;Sandra Wienke;Michael Schlottke-Lakemper;Matthias Meinke;Matthias S. Müller	2016		10.1007/978-3-319-46079-6_36	computational science	Arch	-6.244929817311785	42.925168884075966	9466
56c178714698059c8edab6258474743d29805daa	a relative interpolation theorem for infinitary universal horn logic and its applications	algebraic logic;or phrases;propositional logic	In this paper we deal with infinitary universal Horn logic both with and without equality. First, we obtain a relative Lyndon-style interpolation theorem. Using this result, we prove a non-standard preservation theorem which contains, as a particular case, a Lyndon-style theorem on surjective homomorphisms in its Makkai-style formulation. Another consequence of the preservation theorem is a theorem on bimorphisms, which, in particular, provides a tool for immediate obtaining characterizations of infinitary universal Horn classes without equality from those with equality. From the theorem on surjective homomorphisms we also derive a non-standard Beth-style preservation theorem that yields a non-standard Beth-style definability theorem, according to which implicit definability of a relation symbol in an infinitary universal Horn theory implies its explicit definability by a conjunction of atomic formulas. We also apply our theorem on surjective homomorphisms, theorem on bimorphisms and definability theorem to algebraic logic for general propositional logic.	horn clause;interpolation	Alexej P. Pynko	2006	Arch. Math. Log.	10.1007/s00153-005-0302-2	algebraic logic;carlson's theorem;mathematical logic;mathematical analysis;discrete mathematics;brouwer fixed-point theorem;theorem;factor theorem;intersection theorem;danskin's theorem;no-go theorem;first-order logic;fundamental theorem;mathematics;bruck–ryser–chowla theorem;utm theorem;picard–lindelöf theorem;open mapping theorem;fixed-point theorem;compactness theorem;propositional calculus;second-order logic;gödel's completeness theorem;model theory;algebra;gap theorem	Theory	-10.752551910806417	13.67731036996094	9473
718039299a76f42ab3cd7e2c8f5f8fd13f68c52f	a top down approach to the formal specification of sci cache coherence	working group;formal specification;top down;scalable coherent interface;cache coherence;memory systems;c programming language;shared memory multiprocessor	1 I n t r o d u c t i o n SCI Scalable Coherent Interface is a new standard defined by IEEE working group P1596 for the interconnect in shared memory multiprocessors [9, 10]. The interconnect is scalable, meaning that up to (a theoretical limit of) 64 000 processor-, memoryor I/O-nodes can interface to. In order for a processor to operate at high speed, it needs a cache that contains copies of the memory entities in use at the moment. Because different processor nodes share memory, SCI defines a cache coherence protocol that ensures that all caches are coherent, i.e. at any time all processor nodes agree upon the state of the memory. In present buses coherence is achieved by eavesdropping or snooping: all processor nodes listen to the bus and invalidate or update their caches when they hear that memory is written into [6]. This snooping scheme does not work well when there are many (e.g. more than 10) processor~ connected to the same bus. SCI therefore defines a directory based cache coherency scheme that is intended to work for a large number of processors with shared memory. In a directory based cache coherence protocol cache updating is based on information sent to a directory instead of observations on a bus. This directory then keeps track of which nodes cache copies of a given memory entity, and hence the directory knows which caches are affected by a modification of this entity. Directory based cache coherence protocols were suggested quite a long time ago [2, 15]. Only recently, however, is communication speed so fast and components so cheap that further development of such cache protocols is interesting. In addition to the protocol that is the topic of this paper, there are currently several directory based protocols under development [1, 10, 12, 16]. The SCI cache coherence protocol designates a directory to each line of memory (a line is 64 bytes). Each directory is implemented as a doubly linked list that starts at the memory and contains all nodes	applicative programming language;cpu cache;cache (computing);cache coherence;coherent;concurrency (computer science);data structure;defense in depth (computing);directory (computing);distributed cache;doubly linked list;formal specification;high-level programming language;interleaved memory;mind;multiprocessing;shared memory;stepwise regression;the c programming language	Stein Gjessing;Stein Krogdahl;Ellen Munthe-Kaas	1991		10.1007/3-540-55179-4_9	bus sniffing;shared memory;cache coherence;computer architecture;parallel computing;cache coloring;working group;computer science;write-once;cache invalidation;top-down and bottom-up design;formal specification;programming language;mesi protocol;mesif protocol;cache-only memory architecture;non-uniform memory access	HPC	-14.398141532726212	46.00643019330058	9476
0d1aa35eb70ef129023e8f04e9cc56d7fe02d569	lynx: an open architecture for catalyzing the deployment of interactive digital government workflow-based systems	xml schema;instant messaging;web service;collaborative tools;open architecture;workflow system;web services;xforms;xml;workflow;bpel;user interaction;e mail;business process;digital government;dynamic loading	We introduce Lynx, a new email extension for workflow systems based on Web Services. Web service based workflows provide support for aggregating web services into new higher-level web services by means of process composition. This approach does not usually support direct interaction with people. On the other hand, traditional collaboration tools like email or instant messaging do not provide the necessary support for structured business processes. Lynx provides a web service through which a workflow application can interact with human partners via an email based forms interface without requiring a specialized client. We constructed a Lynx prototype and tested it with the ActiveBPEL engine. User interaction is achieved by means of XForms dynamically generated by Java classes dynamically loaded based on the XML schema of the documents exchanged. We illustrate the usefulness of our approach in a Digital Government scenario.	apache struts;business process execution language;business process;e-government;email;experiment;graphical user interface;instant messaging;java;load (computing);lynx (web browser);open architecture;prototype;rico;software deployment;throughput;web application;web service;xml schema	Iván P. Vélez;Bienvenido Vélez	2006		10.1145/1146598.1146683	web service;computer science;database;internet privacy;world wide web	Web+IR	-41.161670259211746	11.082819118991344	9489
a6c5d51f90240f7d05e359ed693ac87ab01a5198	revisiting snapshot algorithms by refinement-based techniques	verification;distributed algorithms;formal specification;distributed algorithms computational modeling abstracts concrete system recovery electronic mail context;formal verification;snapshot;formalization process snapshot algorithm refinement based technique distributed computation distributed program distributed algorithm correct by construction process distributed system event b modeling language refinement based incremental development rodin platform;formal verification distributed algorithms formal specification;correctness by construction;verification distributed algorithms correctness by construction snapshot	The snapshot problem addresses a collection of important algorithmic issues related to the distributed computations, which are used for debugging or recovering the distributed programs. Among the existing solutions, Chandy and Lamport propose a simple distributed algorithm. In this paper, we explore the correct-by-construction process to formalize the snapshot algorithms in distributed system. The formalization process is based on a modeling language Event B, which supports a refinement-based incremental development using RODIN platform. These refinement-based techniques help to derive a correct distributed algorithm. Moreover, we demonstrate how this class of other distributed algorithms can be revisited. A consequence is to provide a fully mechanized proof of the distributed algorithms.	algorithm;snapshot isolation	Manamiary Bruno Andriamiarina;Dominique Méry;Neeraj Kumar Singh	2012		10.1109/PDCAT.2012.119	snapshot;distributed algorithm;parallel computing;verification;formal verification;computer science;theoretical computer science;formal specification;database;distributed computing;distributed design patterns;programming language	Theory	-27.35642251904371	35.189190179001216	9490
93a0cbfec662a381a9721840b9ae912a4f0cbbc2	a study on intelligent user-centric logistics service model using ontology		Much research has been undergone in the smart logistics environment for the prompt delivery of the product in the right place at the right time.Most of the services were based on timemanagement, routing technique, and location based services.The services in the recent logistics environment aim for situation based logistics service centered around the user by utilizing various information technologies such as mobile devices, computer systems, and GPS.This paper proposes a smart logistics service model for providing user-centric intelligent logistics service by utilizing smartphones in a smart environment. We also develop an OWL based ontology model for the smart logistics for the better understanding among the context information. In addition to basic delivery information, the proposed service model makes use of the location and situation information of the delivery vehicle and user, to draw the route information according to the user’s requirement. With the increase of internet usage, the real-time situations are received which helps to create a more reliable relationship, owing to the Internet of Things. Through this service model, it is possible to engage in the development of various IT and logistics convergence services based on situation information between the deliverer and user which occurs in real time.		Sivamani Saraswathi;KyungHun Kwak;Yongyun Cho	2014	J. Applied Mathematics	10.1155/2014/162838	service delivery framework;integrated logistics support	HCI	-40.62853467743317	50.21543950201181	9493
022ef27c32743fca26f58ca222a55b68cc490cb2	maximal synthesis for hennessy-milner logic	recursive functions bisimulation equivalence;deterministic simulant maximal synthesis hennessy milner logic kripke structure labelled transition transition system hml formula bisimulation equivalence recursive structure operational rules;recursive functions;modeling supervisory control context system recovery model checking semantics concurrent computing;bisimulation equivalence	We present a solution for the synthesis on Kripke structures with labelled transitions, with respect to Hennessy-Milner Logic. This encompasses the definition of a theoretical framework that is able to express how such a transition system should be modified in order to satisfy a given HML-formula. The transition system is mapped under bisimulation equivalence onto a recursive structure, thereby unfolding up to the applicable reach of a given HML-formula. Operational rules define the required adaptations to ensure validity upon this structure. Synthesis might result in multiple valid adaptations which are all related to the original transition system via simulation. The set of synthesized products contains an outcome which is maximal with respect to all deterministic simulants which satisfy the HML-formula.	bisimulation;boolean satisfiability problem;hennessy–milner logic;kripke semantics;maximal set;recursion;requirement;simulation;transition system;turing completeness;unfolding (dsp implementation)	Allan van Hulst;Michel A. Reniers;Wan Fokkink	2013	2013 13th International Conference on Application of Concurrency to System Design	10.1109/ACSD.2013.4	bisimulation;algorithm	Logic	-11.6608797975469	23.69269281778864	9502
db9ab9e07cff9c233067be523f67a95dc9e59973	towards a framework for specifying software robustness requirements based on patterns	structure methods;publikationer;functional properties;konferensbidrag;specification and verification;levels of abstraction;requirement engineering;artiklar;rapporter;software quality;real time systems	[Context and motivation] With increasing use of software, quality attributes grow in relative importance. Robustness is a software quality attribute that has not received enough attention in requirements engineering even though it is essential, in particular for embedded and real-time systems. [Question/Problem] A lack of structured methods on how to specify robustness requirements generally has resulted in incomplete specification and verification of this attribute and thus potentially a lower quality. Currently, the quality of robustness specification is mainly dependent on stakeholder experience and varies wildly between companies and projects. [Principal idea/results] Methods targeting other non-functional properties such as safety and performance suggest that certain patterns occur in specification of requirements, regardless of project and company context. Our initial analysis with industrial partners suggests robustness requirements from different projects and contexts, if specified at all, follow the same rule. [Contribution] By identifying and gathering these commonalities into patterns we present a framework, ROAST, for specification of robustness requirements. ROAST gives clear guidelines on how to elicit and benchmark robustness requirements for software on different levels of abstraction.	benchmark (computing);embedded system;list of system quality attributes;principle of abstraction;real-time clock;real-time computing;requirement;requirements engineering;robustness (computer science);software quality	Ali Shahrokni;Robert Feldt	2010		10.1007/978-3-642-14192-8_9	reliability engineering;software requirements specification;systems engineering;engineering;software engineering;data mining;requirements engineering;software quality	SE	-56.75534875959476	26.53604818599915	9521
22e513737510eeed611832de231612b9ebdf381f	nonmonotonic probabilistic logics between model-theoretic probabilistic logic and probabilistic logic under coherence	default reasoning;artificial intelligent;belief revision;probabilistic logic	Recently, it has been shown that probabilistic entailmentunder coherenceis weaker than model-theoreticprobabilisticentailment.Moreover, probabilistic entailmentunder coherence is a generalizationof default entailmentin System . In this paper , we continuethis line of researchby presentingprobabilisticgeneralizations of more sophisticatednotions of classical default entailmentthat lie betweenmodeltheoreticprobabilisticentailmentandprobabilistic entailment under coherence.That is, the new formalismsproperlygeneralizetheir counterpartsin classicaldefault reasoning,they are weaker thanmodel-theoreticprobabilisticentailment,andthey arestrongerthanprobabilisticentailmentundercoherence.The new formalisms are useful especiallyfor handling probabilistic inconsistenciesrelatedto conditioning on zero events.They canalsobeappliedfor probabilistic beliefrevision. Moregenerally, in thesamespirit asasimilarpreviouspaper , thispapershedslight onexciting new formalismsfor probabilisticreasoningbeyondthewell-known standardones.	lexicographical order;microsoft outlook for mac;theory	Thomas Lukasiewicz	2002			natural language processing;discrete mathematics;probabilistic ctl;probabilistic relevance model;computer science;artificial intelligence;mathematics;probabilistic logic;preferential entailment;probabilistic argumentation;belief revision;algorithm;divergence-from-randomness model	AI	-15.75843641276758	8.36772977456075	9524
a2b510882624b500b8dc59f386fe687bb252463d	natural language updates to databases through dialogue	lenguaje natural;base relacional dato;linguistique;base donnee;mise a jour;langage naturel;database;base dato;relational database;actualizacion;linguistica;natural language;base donnee relationnelle;analizador sintaxico;langage base donnee;parser;analyseur syntaxique;database languages;updating;linguistics	This paper reopens the long dormant topic of natural language updates to databases. A protocol to handle database updates of the IDM (Insert-Delete-Modify) class is proposed and implemented. This protocol exploits modern relational update facilities and constraints and structures update dialogues using DAMSL dialogue acts. The protocol may be used with any natural language parser that maps to relational queries.	constraint (mathematics);interactive media;map;natural language;parsing;relational database	Michael Minock	2006		10.1007/11765448_19	natural language processing;relational database;computer science;database;linguistics;natural language;programming language;query language	NLP	-31.946615096481796	8.838794038322138	9541
4704fb2047286a15b7359c0f503bc6280dba4292	federated access control and workflow enforcement in systems configuration	system configuration;access control	syntax tree Config file Com pila tion Tre e ma tch ing Edit script generation Algorithm based on: • Meaningful change detection in structured data. CHAWATHE AND GARCIA-MOLINE. 1997 • Change Distilling: Tree Differencing for Fine-Grained Source Code Change Extraction. FLURI, WUERSCH, PINZGER AND GALL. 2007 28 / 40	access control;algorithm;federated identity;parse tree	Bart Van Brabant;Thomas Delaet;Wouter Joosen	2009			configuration management;configuration management database;software configuration management;computer science;access control;database;distributed computing;sociology;configuration item	SE	-35.96443321126028	58.45891613235283	9544
0d0f39c45427bfeb9fa45b601011f6dd0a084221	a study of the concurrency control and recovery algorithms in nested transaction environment	modelizacion;tratamiento transaccion;nested transaction;systeme base donnee;modelisation;object oriented;database systems;concurrency control;oriente objet;workflow;controle concurrence;transaction processing;application;modeling;orientado objeto;applications;traitement transaction	In this paper, we present a study on the concurrency control and recovery algorithms in nested transaction environment. We have reviewed the work done in the area of nested transaction modelling, its applications in object-oriented and mobile databases, and in workflow models. We have contrasted various nested transaction models by discussing their advantages and disadvantages. We have outlined some important future research directions in the area of nested transaction processing.	algorithm;concurrency control;database;nested transaction;transaction processing	Sanjay Kumar Madria	1997	Comput. J.	10.1093/comjnl/40.10.630	workflow;real-time computing;systems modeling;transaction processing;distributed transaction;computer science;concurrency control;database;distributed computing;online transaction processing;object-oriented programming;serializability;information technology;nested transaction	DB	-20.51089680424146	46.272203265025254	9570
7af8859a0c7d81cefd43b234b3c5675688ec130a	fault tolerance on large scale systems using adaptive process replication	libraries;message passing checkpointing fault tolerant computing;supercomputer education research centre;exascale systems large scale systems adaptive process replication mean time between failures mtbf periodic checkpointing application failures proactive fault tolerance mechanisms partial replication fault tolerance framework mpi prototype implementation parep mpi;checkpointing;receivers;redundancy;fault tolerant systems;fault tolerance;exascale systems fault tolerance process replication;checkpointing fault tolerant systems redundancy receivers large scale systems libraries;process replication;exascale systems;large scale systems	Exascale systems of the future are predicted to have mean time between failures (MTBF) of less than one hour. At such low MTBFs, employing periodic checkpointing alone will result in low efficiency because of the high number of application failures resulting in large amount of lost work due to rollbacks. In such scenarios, it is highly necessary to have proactive fault tolerance mechanisms that can help avoid significant number of failures. In this work, we have developed a mechanism for proactive fault tolerance using partial replication of a set of application processes. Our fault tolerance framework adaptively changes the set of replicated processes periodically based on failure predictions to avoid failures. We have developed an MPI prototype implementation, PAREP-MPI that allows changing the replica set. We have shown that our strategy involving adaptive process replication significantly outperforms existing mechanisms providing up to 20 percent improvement in application efficiency even for exascale systems.	application checkpointing;fault tolerance;mean time between failures;message passing interface;prototype;rollback (data management)	Cijo George;Sathish S. Vadhiyar	2015	IEEE Transactions on Computers	10.1109/TC.2014.2360536	fault tolerance;parallel computing;real-time computing;computer science;distributed computing;redundancy;software fault tolerance	HPC	-18.908170919356603	49.55973500810133	9571
252c007ce576c057e40e167e9c5ea2b244fdd52b	global control for partial deduction through characteristic atoms and global trees	functional programming;partial deduction;logic programs;on line control;control strategy	Recently, considerable advances have been made in the (online) control of logic program specialisation. A clear conceptual distinction has been established between local and global control and on both levels concrete strategies as well as general frameworks have been proposed. For global control in particular, recent work has developed concrete techniques based on the preservation of characteristic trees (limited, however, by a given, arbitrary depth bound) to obtain a very precise control of polyvariance. On the other hand, the concept of an m-tree has been introduced as a refined way to trace “relationships” of partially deduced atoms, thus serving as the basis for a general framework within which global termination of partial deduction can be ensured in a non ad hoc way. Blending both, formerly separate, contributions, in this paper, we present an elegant and sophisticated technique to globally control partial deduction of normal logic programs. Leaving unspecified the specific local control one may wish to plug in, we develop a concrete global control strategy combining the use of characteristic atoms and trees with global (m-)trees. We thus obtain partial deduction that always terminates in an elegant, non ad hoc way, while providing excellent specialisation as well as fine-grained (but reasonable) polyvariance. We conjecture that a similar approach may contribute to improve upon current (on-line) control strategies for functional program transformation methods such as (positive) supercompilation.	alpha compositing;bim;control theory;experiment;failure;feedback;functional programming;hoc (programming language);jones calculus;large eddy simulation;logic programming;m-tree;metacompilation;mogensen–scott encoding;natural deduction;online and offline;polyvariance;program transformation;prolog	Michael Leuschel;Bern Martens	1996		10.1007/3-540-61580-6_13	discrete mathematics;mathematics;algorithm	PL	-14.46542387596907	18.706466841395688	9582
41c6cbf0fc1a96f625d59519a5dcd617c7548c86	diligent: towards a fine-grained methodology for distributed, loosely-controlled and evolving engineering of ontologies	peer to peer system;rhetorical structure theory;ontology engineering;semantic web	Ontology engineering processes in truly distributed settings like the Semantic Web or global peer-to-peer systems may not be adequately supported by conventional, centralized ontology engineering methodologies. In this paper, we present our work towards the DILIGENT methodology, which is intended to support domain experts in a distributed setting to engineer and evolve ontologies with the help of a fine-grained methodological approach based on Rhetorical Structure Theory, viz. the DILIGENT model of ontology engineering by argumentation.	argumentation framework;centralized computing;experiment;intel matrix raid;ontology (information science);ontology engineering;peer-to-peer;process modeling;ring counter;semantic web;video-in video-out;viz: the computer game	Helena Sofia Pinto;Steffen Staab;Christoph Tempich	2004			upper ontology;bibliographic ontology;ontology inference layer;computer science;knowledge management;ontology;artificial intelligence;semantic web;data mining;database;ontology-based data integration;process ontology	Web+IR	-44.264664610084104	8.88419775842609	9603
e6d2fb81bae5c14d0e086d2f7e1eb04544ca37b5	elasticity economics of cloud-based applications	elasticity;cost performance trade off analysis;iaas;web transactional applications;generators;electronic commerce;e business;measurement;electronic commerce cloud computing economics;cost performance trade off analysis elasticity economics cloud based applications cloud infrastructure services elastic computing web transactional applications e business applications;cloud infrastructure services;elastic computing;web applications;cloud based applications;elasticity economics;servers;e business iaas elasticity economics cloud computing web applications;monitoring;elasticity economics servers measurement monitoring generators business;business;economics;cloud computing;e business applications	Cloud infrastructure services offer elastic computing resources that particularly match the requirements of web transactional applications. Such applications, e.g., e-business applications, have high business value and variable workload patterns and therefore utilizing such elastic cloud resources in a cost-effective way is crucial need for cloud consumers. Defining economic elasticity rules, however, is not trivial as it requires (i) running considerable number of experiments with different parameters/metrics and workloads (ii) collecting appropriate cloud and application cost/performance measurements and (iii) performing cost/performance trade-off analysis. In this paper we introduce our on-going work on elasticity economics platform that supports cloud consumers to achieve the above three requirements. We also discuss the key elements of the elasticity economics platform and our early prototyping experience.	cloud computing;elasticity (cloud computing);elasticity (data store);electronic business;experiment;requirement	Basem Suleiman	2012	2012 IEEE Ninth International Conference on Services Computing	10.1109/SCC.2012.65	simulation;marketing;business;commerce	DB	-26.508405169489983	60.42596504898336	9610
0b02ed7f00ce3bf813f07e6a8aad57f2b959ce2a	structural refinement of systems specified in object-z and csp	integrated formal methods;renemen t;life cycle;structure refinement;specification;simulation;object z;semantics;simultaneidad informatica;qa 76 software;simulacion;theorie structurelle;refinement;sistema complejo;structural theory;semantica;semantique;integration;specification language;formal method;computer programming;refinement method;modele semantique;afinamiento;semantic model;modelo;concurrency;systeme complexe;especificacion;complex system;integracion;affinement;csp;modele;lenguaje especificacion;teoria estructural;methode raffinement;simultaneite informatique;metodo afinamiento;langage specification;models;ensp	This paper is concerned with methods for refinement of specifications written using a combination of Object-Z and CSP. Such a combination has proved to be a suitable vehicle for specifying complex systems which involve state and behaviour, and several proposals exist for integrating these two languages. The basis of the integration in this paper is a semantics of Object-Z classes identical to CSP processes. This allows classes specified in Object-Z to be combined using CSP operators. It has been shown that this semantic model allows state-based refinement relations to be used on the Object-Z components in an integrated Object-Z/CSP specification. However, the current refinement methodology does not allow the structure of a specification to be changed in a refinement, whereas a full methodology would, for example, allow concurrency to be introduced during the development life-cycle. In this paper, we tackle these concerns and discuss refinements of specifications written using Object-Z and CSP where we change the structure of the specification when performing the refinement. In particular, we develop a set of structural simulation rules which allow single components to be refined to more complex specifications involving CSP operators. The soundness of these rules is verified against the common semantic model and they are illustrated via a number of examples.	authentication;communicating sequential processes;complex systems;concurrency (computer science);embedded system;michael j. fischer;model checking;morgan;object-z;precondition;refinement (computing);simulation;specification language;taguchi methods;verification and validation	John Derrick;Graeme Smith	2003	Formal Aspects of Computing	10.1007/s00165-003-0002-9	concurrency;specification language;computer science;computer programming;semantics;refinement;programming language;specification;algorithm	SE	-39.90276492222479	25.87927515456856	9612
80eedaccc5689f97935c3ee278d5b437d9550222	optimizing flash-based key-value cache systems	conference paper	Flash-based key-value cache systems, such as Facebook’s McDipper [1] and Twitter’s Fatcache [2], provide a cost-efficient solution for high-speed key-value caching. These cache solutions typically take commercial SSDs and adopt a Memcached-like scheme to store and manage key-value pairs in flash. Such a practice, though simple, is inefficient. We advocate to reconsider the hardware/software architecture design by directly opening device-level details to key-value cache systems. This co-design approach can effectively bridge the semantic gap and closely connect the two layers together. Leveraging the domain knowledge of key-value caches and the unique device-level properties, we can maximize the efficiency of a key-value cache system on flash devices while minimizing its weakness. We are implementing a prototype based on the Open-channel SSD hardware platform. Our preliminary experiments show very promising results.	attribute–value pair;cpu cache;cache (computing);cost efficiency;experiment;memcached;open-channel ssd;optimizing compiler;prototype;software architecture;solid-state drive	Zhaoyan Shen;Feng Chen;Yichen Jia;Zili Shao	2016			bus sniffing;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;computer hardware;cache;computer science;write-once;cache invalidation;smart cache;cache algorithms;cache pollution	OS	-12.016268411389522	52.615610268066455	9619
aa0c721976c86ece4c5288bb77924a78d22e090d	a fuzzy logic based approach to expressing and reasoning with uncertain knowledge on the semantic web		The necessity of dealing with uncertain knowledge has arisen from Semantic Web applications in different areas. Handling uncertainty thus becomes one of the key research directions in the Semantic Web community. This paper introduces a fuzzy logic based approach which extends the classic Description Logic with Zadeh semantics to deal with uncertain knowledge about concepts and roles as well as instances of concepts and roles. Uncertain knowledge representation and the reasoning algorithm for consistency checking of a fuzzy knowledge base are addressed in detail. This paper also discusses complexity issues of the reasoning problem.	fuzzy logic;semantic web	Jidi Zhao;Harold Boley;Weichang Du	2010		10.1007/978-3-642-27534-0_11	fuzzy logic;knowledge representation and reasoning;description logic;semantic web rule language;knowledge management;model-based reasoning;machine learning;data mining;reasoning system;deductive reasoning;probabilistic logic network	AI	-20.774947919121868	7.361399673761537	9633
fb94e8c0975ff2b1e442644caac6886d4b2cee28	extending datalog to express functional queries: a language and its implementation.				Stefano Basta;Sergio Flesca;Sergio Greco	1998			computer science;theoretical computer science;database;datalog;programming language	DB	-28.33668993609706	11.338497454875712	9640
346eaae4234c9ebf91bd5d0ca65025ca08ab51b6	duality in knowledge compilation techniques	compilacion;disjunctive normal form;forma normal;forme disjonctive;intelligence artificielle;lengua blanco;target language;logique ordre 1;langue cible;disjunctive form;knowledge compilation;normal form;compilation;artificial intelligence;forme normale;classical logic;forma disyuntiva;inteligencia artificial;first order logic;logica orden 1	Several classes of propositional formulas have been used as target languages for knowledge compilation. Some are based primarily on c-paths (essentially, the clauses in disjunctive normal form); others are based primarily on dpaths. Such duality is not surprising in light of the duality fundamental to classical logic. There is also duality among target languages in terms of how they treat links (complementary pairs of literals): Some are link-free; others are pairwise-linked (essentially, each pair of clauses is linked). In this paper, both types of duality are explored, first, by investigating the structure of existing forms, and secondly, by developing new forms for target languages.	compiler;disjunctive normal form;knowledge compilation	Neil V. Murray;Erik Rosenthal	2005		10.1007/11425274_19	classical logic;computer science;artificial intelligence;calculus;first-order logic;mathematics;disjunctive normal form;algorithm	AI	-15.533716963726523	15.989554868149543	9649
9ec80fb7c089c96efe0d7bc270bbaa3d32facae9	configuring e-government services using ontologies	semantic technologies;consistency model;system management	The increasing complexity of e-Government services demands a correspondingly larger effort for management. Today, many system management tasks, such as service verification and re-configuration due to changes in the law, are often performed manually. This can be time consuming and error-prone. The main objective of the OntoGov (IST-2002-507237) project is to overcome the above mentioned problems by developing a semantically-enriched platform that will facilitate the consistent configuration and re-configuration of e-Government services. This paper outlines the overall OntoGov platform and demonstrates how the Service Modeller can be used to consistently model e-Government Services.	cognitive dimensions of notations;e-government;ontology (information science);systems management	Dimitris Apostolou;Ljiljana Stojanovic;Tomás Pariente Lobo;Jofre Casas Miró;Andreas Papadakis	2005		10.1007/0-387-29773-1_10	knowledge management;data mining;database;semantic analytics	OS	-44.79611931657528	11.313391037722782	9675
1663be49af29572161b750b4135124ce76e652ce	asserts: a toolkit for real-time software design, development and evaluation	multiprocessor scheduling;scheduling computer aided software engineering software tools real time systems digital simulation;real time;software engineering;computer aided software engineering;levels of abstraction;scheduling;real time scheduling;software development;software tools;software design;software design real time systems programming timing analytical models hardware software systems delay independent component analysis time factors;macroinstructions real time software design toolkit real time software engineering toolkit asserts simulation capabilities real time system designer configuration independent hardware platform task system simulation component built in models practical real time schedulers resource access protocols;digital simulation;real time systems	"""We describe a real time software engineering toolkit called ASSERTS that provides complementary analysis and simulation capabilities to assist the real time system designer. ASSERTS is configuration independent and allows users to describe the parameters of the hardware platform, interconnection and the kernel and the task system for the purpose of simulation and analysis. The simulation component of ASSERTS is quite detailed and features a number of built-in models for practical real time schedulers, interconnections and resource access protocols. Users can define the behavior of the tasks at various levels of abstraction using a fairly small set of macroinstructions. ASSERTS allows users to quickly gain an insight into the design of the software as it evolves and also allows """"what if"""" studies to be done very easily."""	real-time transcription;software design	Kanad Ghose;Sudhir Aggarwal;Pavel Vasek;S. Chandra;Amritansh Raghav;Abrahijt Ghosh;David R. Vogel	1997		10.1109/EMWRTS.1997.613789	computer architecture;verification and validation;computing;real-time computing;software sizing;software verification;computer science;package development process;backporting;software design;social software engineering;software reliability testing;software framework;component-based software engineering;software development;software design description;operating system;software construction;software walkthrough;resource-oriented architecture;software deployment;computer-aided software engineering;scheduling;multiprocessor scheduling;software system	Embedded	-41.649298441227685	34.390841304931044	9677
6067c3c52a1b6eb8a7723ce584364e2ddfc9ff5a	mapping petri nets with inhibitor arcs onto basic lotos behavior expressions	concurrent systems design;distributed system;concurrent;protocols;systeme reparti;formal specification;protocole transmission;tarea concurrente;semantica formal;data description;formal semantics;formal description technique;semantique formelle;expressive power;protocolo transmision;computer operating procedures;sistema repartido;formal description techniques;concurrent systems;specification languages;simultaneo;concurrent systems petri nets mapping inhibitor arcs basic lotos behavior expressions formal description techniques communication protocols;communication protocol;simultane;tâche concurrente;petri nets;specification languages petri nets protocols formal specification;petri net;concurrent task;protocol engineering;reseau petri;description donnee;lotos;procedure exploitation informatique;petri nets inhibitors protocols design engineering power engineering and energy communication standards specification languages design methodology writing;transmission protocol	The integration of different formal description techniques is an important feature in the design of communication protocols and concurrent systems. In this paper we address the problem of translating Petri nets with inhibitor arcs into basic LOTOS specifications, which is an important step in the direction of integrating these two commonly used formalisms. A mapping which preserves strong bisimulation equivalence is formally defined and illustrated by means of an example. The definition of the mapping enables us also to state a new result about the expressive power of the basic LOTOS subset which constitutes the mapping range.		Riccardo Sisto;Adriano Valenzano	1995	IEEE Trans. Computers	10.1109/12.477242	communications protocol;computer science;theoretical computer science;programming language;petri net;algorithm	Robotics	-32.57858141739786	31.327699338976057	9696
569241dc349386cd1d27e65e1142e8d7f34daaa8	studies based on the motion environment sensation aspect's dynamic service combination	dynamic programming;quality assurance;software;dynamic change;functional integration;application software;information technology;technological development;construction industry;software systems;software construction;object oriented programming;software architecture;visualization;computational modeling;enterprise function integration;dynamic service combination;dynamics;motion environment sensation;dynamic autoadapted operational mechanism;service auto adapted;application software software quality space technology reflection dynamic programming information analysis software design visualization information technology software systems;software development;demand analysis;system development;middleware;aspect oriented;space technology;information system;flow design;software design;programming;information analysis;reflection;software quality middleware object oriented programming software architecture;software quality;enhancement system service middleware motion environment sensation aspect dynamic service combination flow design software construction system development quality assurance information system software development dynamic autoadapted operational mechanism enterprise function integration;enhancement system service middleware;service auto adapted motion environment sensation aspect oriented dynamic service combination;motion environment sensation aspect	The distributional net constructs software's network environment to be day by day complex, system's demand analysis, the flow design, the software construction, the system development and the quality assurance came under the distributional external environment influence, information system's dynamic change have led alternately the software development entire process the uncertainty attribute category. Distributional to external environment's visualization, the motion environment sensation software main body's substantialization, the dynamic auto-adapted operational mechanism's predictability set the high request alternately. Under the hierarchical motion environment sensation network construction theory influence, how to carry on the visualization the dynamic service combination, realizes the cross domain level, the enterprise function integration, enhancement system service middleware auto-adapted ability and the reusability becomes the software development technological development the important advancement.	information system;middleware;software construction;software development	Yanhong Zhang	2009	2009 International Forum on Information Technology and Applications	10.1109/IFITA.2009.303	simulation;computer science;systems engineering;software engineering	SE	-56.3280379256275	27.43611040765227	9701
5fc2248a75f473500c7e07f1e9332a6f26166cbf	towards a generalized map algebra: principles and data types	algebraic function;spatial data;data type	Map Algebra is a collection of functions for handling continuous spatial data, which allows modeling of different problems and getting new information from the existing data. There is an established set of map algebra functions in the GIS literature, originally proposed by Dana Tomlin. However, the question whether his proposal is complete is still an open problem in GIScience. This paper describes the design of a map algebra that generalizes Tomlin’s map algebra by incorporating topological and directional spatial predicates. Our proposal enables operations that are not directly expressible by Tomlin’s proposal. One of the important results of our paper is to show that Tomlin’s Map Algebra can be defined as an application of topological predicates to coverages. This paper points to a convergence between these two approaches and shows that it is possible to develop a foundational theory for GIScience where topological predicates are the heart of both object-based algebras and field-based algebras.	coverage data;generalized map;geographic information science;geographic information system;map algebra;object-based language;spatial database	Gilberto Câmara;Danilo Palomo;Ricardo Cartaxo Modesto de Souza;Olga Regina Fradico de Oliveira Bittencourt	2005			algebraic data type;map algebra;computer science;open problem;data type;combinatorics;discrete mathematics;spatial analysis;recursive data type;type family;algebra;dimension of an algebraic variety	DB	-24.06050746254082	9.990913137727244	9705
cc7f56ce88c43961260ece0c409b0a325b5b997d	how to multiply dynamic programming algorithms	linear grammar;context free grammar;product structure;haskell;multiple alignment	We develop a theory of algebraic operations over linear grammars that makes it possible to combine simple “atomic” grammars operating on single sequences into complex, multi-dimensional grammars. We demonstrate the utility of this framework by constructing the search spaces of complex alignment problems on multiple input sequences explicitly as algebraic expressions of very simple 1-dimensional grammars. The compiler accompanying our theory makes it easy to experiment with the combination of multiple grammars and different operations. Composite grammars can be written out in LTEX for documentation and as a guide to implementation of dynamic programming algorithms. An embedding in Haskell as a domain-specific language makes the theory directly accessible to writing and using grammar products without the detour of an external compiler. http://www.bioinf.uni-leipzig.de/Software/gramprod/	algorithm;compiler;documentation;domain-specific language;dynamic programming;haskell;linear algebra;linear grammar;theory	Christian Höner zu Siederdissen;Ivo L. Hofacker;Peter F. Stadler	2013		10.1007/978-3-319-02624-4_8	grammar systems theory;context-sensitive grammar;tree-adjoining grammar;synchronous context-free grammar;l-attributed grammar;deterministic context-free grammar;link grammar;parsing expression grammar;multiple sequence alignment;computer science;bioinformatics;affix grammar;theoretical computer science;machine learning;extended affix grammar;mathematics;context-free grammar;programming language;ambiguous grammar;stochastic context-free grammar;embedded pushdown automaton;algorithm	PL	-23.412461601628856	24.58897860268052	9711
2003d941de02063fef1501d094e27cbd5e9c750b	recursion versus iteration with the list as a data structure	iterative method;organigramme;funcion discreta;flowchart;mathematiques discretes;loop;matematicas discretas;discrete mathematics;list data structure;recurrence;metodo iterativo;recursive function;logo;discrete function;fonction discrete;methode iterative;recurrencia;recursion;estructura datos;funcion recursiva;iteration;fonction recursive;structure donnee;enseignement;data structure;organigrama;tail end recursion;teaching;ensenanza	A reversible sequence of steps from the specification of the algorithm and the mathematical definition of the recurrent solution through the recursive procedure, the tail recursive procedure and finally to the iteration procedure, is shown. The notation for analysing recursive function execution as well as modified flow charts of an algorithm to identify the differences between the iteration and the tail recursion are proposed. All the procedures are written in Logo, so the lists are used as the data structure. Transformation from the recursive procedure to the iterative procedure and vice versa can be shown in such a way in every language in which the recursion is allowed. All examples are one-recursion-call examples and all except one are the functions of discrete mathematics.	algorithm;chart;data structure;discrete mathematics;flowchart;iteration;iterative method;logo;recursion (computer science);tail call	Izabella Foltynowicz	2007	Informatics in Education		left recursion;combinatorics;discrete mathematics;iteration;recursive definition;double recursion;primitive recursive function;recursive language;mathematics;mutual recursion;algorithm	PL	-15.02308476533096	31.649202460269567	9715
b6ef461e8c883f8fee772ad1fd1f6ff2147e9b4f	do we need new strategies for testing systems-of-systems?		This paper overviews the main Systems-of-Systems (SoS) characteristics that can impact on their verification, validation and testing (VV&T). Furthermore, it addresses technical, conceptual, social and organizational challenges, discusses which existing approaches of VV&T can be used for SoS, and points out future research in the field.	apple sos;system of systems	Vânia de Oliveira Neves;Antonia Bertolino;Guglielmo De Angelis;Luis Garces	2018	2018 IEEE/ACM 6th International Workshop on Software Engineering for Systems-of-Systems (SESoS)	10.1145/3194754.3194758	systems engineering;system of systems;software system;computer science	SE	-61.555883624938694	20.955601253765238	9723
2753d70c61d104ea467910469a6b3f72a6eded18	improving semantic query answering	knowledge based system;system performance;query answering;knowledge base	The retrieval problem is one of the main reasoning tasks for knowledge base systems. Given a knowledge base K and a concept C, the retrieval problem consists of finding all individuals a for which K logically entails C(a). We present an approach to answer retrieval queries over (a restriction of) OWL ontologies. Our solution is based on reducing the retrieval problem to a problem of evaluating an SQL query over a database constructed from the original knowledge base. We provide complete answers to retrieval problems. Still, our system performs very well as is shown by a standard benchmark.	benchmark (computing);knowledge base;ontology (information science);sql;select (sql);semantic query;web ontology language	Norbert Kottmann;Thomas Studer	2007		10.1007/978-3-540-74469-6_65	knowledge base;query expansion;question answering;computer science;artificial intelligence;knowledge-based systems;data mining;database;information retrieval	AI	-35.411971920422296	7.999217915487166	9725
33697e1a9b77fdf96cce0a955367ad2fa8e00b2a	a novel solution of distributed file storage for cloud service	cluster storage scalability data storage;indexes;data storage;servers;fault tolerant computing;distributed object management;concurrency control;distributed databases;fault tolerant function distributed file storage cloud service internet application technology web2 0 cloud computing data content geometric progression technical architecture file system mass small files frequency creating operations writing operations high concurrency requests high disk io backup difficulty poor scalability berrystore distributed object storage system massive small files storing distributed coordinate controller;cluster storage;scalability;servers cloud computing file systems distributed databases throughput memory indexes;file organisation cloud computing concurrency control distributed databases distributed object management fault tolerant computing;memory;file systems;cloud computing;throughput;file organisation	With the rapid development of the Internet application technology, especially Web2.0 and cloud computing, the data content people produced on the network grew in the geometric progression. And one of the fastest growing and the most likely to challenge the technical architecture things is the large number of small files. Traditional file system processes these mass small files is very toughly, In addition, high frequency creating and writing operations on small files may bring many problems under high concurrency requests, such as high disk IO, backup difficulty and poor scalability. This paper describes BerryStore, a distributed object storage system designed for cloud service especially for the massive small files storing, It provides a simple solution to handle a large number of small files. With a distributed coordinate controller introduced, our system is able to provide scalability, concurrency, fault tolerant function.	backup;cloud computing;clustered file system;color gradient;computer data storage;concurrency (computer science);distributed object;fastest;fault tolerance;information technology architecture;object storage;rich internet application;scalability;web 2.0	Yu Zhang;Weidong Liu;Jiaxing Song	2012	2012 IEEE 36th Annual Computer Software and Applications Conference Workshops	10.1109/COMPSACW.2012.15	database index;throughput;real-time computing;scalability;cloud computing;computer science;operating system;concurrency control;computer data storage;database;distributed computing;data file;memory;distributed database;server	HPC	-18.45626311081585	52.07076332689624	9731
04b72c028ca75adb2dd03b454816ced56c198944	finding concurrency-related bugs using random isolation	java programming;data races	This paper concerns automatically verifying safety properties of concurrent programs. In our work, the safety property of interest is to check for multi-location data races in concurrent Java programs, where a multi-location data race arises when a program is supposed to maintain an invariant over multiple data locations, but accesses/updates are not protected correctly by locks. The main technical challenge that we address is how to generate a program model that retains (at least some of) the synchronization operations of the concrete program, when the concrete program uses dynamic memory allocation. In the presence of dynamic memory allocation, the finite number of abstract objects of the abstract program must represent the unbounded number of concrete objects that the concrete program may allocate, and thus by the pigeon-hole principle some of the abstract objects must be summary objects—they represent more than one concrete object. Because abstract summary objects represent multiple concrete objects, the program analyzer typically must perform weak updates on the abstract state of a summary object, where a weak update accumulates information. Because weak updates accumulate rather than overwrite, the analyzer is only able to determine weak judgements on the abstract state, i.e., that some property possibly holds, and not that it definitely holds. The problem with weak judgements is that determining whether an interleaved execution respects program synchronization requires the ability to determine strong judgements, i.e., that some lock is definitely held, and thus the analyzer needs to be able to perform strong updates—an overwrite of the abstract state—to enable strong	computer multitasking;java;lock (computer science);memory management;program analysis;race condition;software bug;verification and validation	Nicholas Kidd;Thomas W. Reps;Julian Dolby;Mandana Vaziri	2009		10.1007/978-3-540-93900-9_18	real-time computing;computer science;programming language;computer security;algorithm	PL	-21.225244416677292	30.69268128473903	9733
a5652d3ba731c60321bf5ee799bdeee5b4e70cbc	extending the curry-howard interpretation to linear, relevant and other resource logics		Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/about/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.	archive;curry;curry–howard correspondence;download	Dov M. Gabbay;Ruy J. G. B. de Queiroz	1992	J. Symb. Log.		linear logic;combinatory logic;intuitionistic logic;pure mathematics;lambda calculus;mathematics;abstraction;programming language;type theory;algorithm	Comp.	-9.33374685373754	9.610449443415572	9735
18be141d3ccd7559c6ddf11f46511f127276fd27	a new high-performance, energy-efficient replication storage system with reliability guarantee	front end;reliability time factors energy consumption servers mirrors energy efficiency arrays;system reliability;storage system;energy efficient;storage management;storage management raid;raid;eraid energy efficient replication storage system system reliability peraid high performance replication storage system parity software raid virtual write buffer disk parity redundancy small write problem graid;energy consumption;time factor;flush disk reliability power consumption performance;power consumption;high performance	In modern replication storage systems where data carries two or more multiple copies, a primary group of disks is always up to service incoming requests while other disks are often spun down to sleep states to save energy during slack periods. However, since new writes cannot be immediately synchronized onto all disks, system reliability is degraded. This paper develops PERAID, a new high-performance, energy-efficient replication storage system, which aims to improve both performance and energy efficiency without compromising reliability. It employs a parity software RAID as a virtual write buffer disk at the front end to absorb new writes. Since extra parity redundancy supplies two or more copies, PERAID guarantees comparable reliability with that of a replication storage system. In addition, PERAID offers better write performance compared to the replication system by avoiding the classical small-write problem in traditional parity RAID: buffering many small random writes into few large writes and writing to storage in a parallel fashion. By evaluating our PERAID prototype using two benchmarks and two real-life traces, we found that PERAID significantly improves write performance and saves more energy than existing solutions such as GRAID, eRAID.	allocate-on-flush;computer data storage;computer performance;disk array;disk storage;experiment;multiplexing;nested raid levels;prototype;real life;requirement;slack variable;standard raid levels;tracing (software);write buffer	Jiguang Wan;Chao Yin;Jun Wang;Changsheng Xie	2012	012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2012.6232373	parallel computing;real-time computing;computer hardware;engineering;disk mirroring;non-standard raid levels;parity drive;standard raid levels;raid;raid processing unit	OS	-11.60970243793307	54.51651613515042	9748
c6fcc7e9cca4649b6641a419278b4e2a72b4c07f	helpercoredb: exploiting multicore technology to improve database performance	microprocessors;performance evaluation;multicore processing throughput microprocessors prefetching sun spatial databases database systems computer science proposals performance evaluation;database management systems;sql;helpercoredb;storage management;prefetching;dual core processor system helpercoredb multicore technology database performance microprocessor design database workload data prefetching postgresql dbms;chip;performance improvement;multicore technology;dual core processor system;multicore processing;spatial databases;database systems;database workload;data prefetching;sun;microprocessor design;computer science;proposals;microcomputers;storage management database management systems microcomputers sql;postgresql dbms;database performance;throughput	Due to limitations in the traditional microprocessor design, such as high complexity and power, all current commercial high-end processors contain multiple cores on the same chip (multicore). This trend is expected to continue resulting in increasing number of cores on the chip. While these cores may be used to achieve higher throughput, improving the execution of a single application requires careful coding and usually results in smaller benefit as the scale of the system is increased. In this work we propose an alternative way to exploit the multicore technology where certain cores execute code which indirectly improves the performance of the application. We call this approach the Helper Core approach. The main contribution of this work is the proposal and evaluation of HelperCoreDB, a Helper Core designed to improve the performance of database workloads by performing efficient data prefetching. We validate the proposed approach using a HelperCoreDB implementation for the PostgreSQL DBMS. The approach is evaluated with native execution on a dual core processor system using the standard TPC-H benchmark. The experimental results show a significant performance improvement specially considering that the baseline system is a modern optimized processor that includes advanced features such as hardware prefetching. In particular, for the 22 queries of TPC-H, our approach achieves a large reduction of the secondary cache misses, 75% on average, and an improvement in the execution time of up to 21.5%.	baseline (configuration management);benchmark (computing);cpu cache;central processing unit;database;helper class;ibm tivoli storage productivity center;machine code;microprocessor;multi-core processor;postgresql;processor design;run time (program lifecycle phase);throughput	Kostas Papadopoulos;Kyriakos Stavrou;Pedro Trancoso	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536288	chip;multi-core processor;throughput;sql;parallel computing;real-time computing;database tuning;computer science;operating system;database;microcomputer;distributed computing	Arch	-10.874937506673763	52.292643920024695	9756
b272d26a0e6caf27f864712ae6d0a5df45469310	towards inductive generalization in higher order logic	induction generator;higher order logic		higher-order function	Cao Feng;Stephen Muggleton	1992			induction generator;higher-order logic;computer science	AI	-13.397961209237302	13.074505818183809	9760
3b8bc8bab8df74a9066a74df5f417277047c086d	a case-based reasoning approach for norm adaptation	multi agent system;case base reasoning;p2p;environmental change;empirical evaluation;use case	Existing organisational centred multi-agent systems regulate agents' activities. However, population/environmental changes may lead to a poor ful lment of system's goals, and therefore, adapting the whole organisation becomes key. In this paper, we propose to use Case-Based Reasoning learning to adapt norms that regulate agents' behaviour. Moreover, we empirically evaluate this approach in a P2P scenario.	bittorrent;case-based reasoning;international ergonomics association;multi-agent system	Jordi Campos Miralles;Maite López-Sánchez;Marc Esteva	2010		10.1007/978-3-642-13803-4_21	use case;environmental change;computer science;knowledge management;artificial intelligence;multi-agent system;peer-to-peer;management science	AI	-43.69346076481307	20.361875532208757	9774
94d981d195ec5fe7144d416d68562a1f6d9d4f37	homomorphism preserving algebraic specifications require hidden sorts	algebraic specification;analisis datos;tipo dato;data type;universiteitsbibliotheek;homomorphism;data analysis;specification donnee;wijsbegeerte;homomorphisme;especificacion datos;analyse donnee;homomorfismo;type donnee;data specification	Although every computable data type has an initial algebra speci-cation with hidden functions, it may happen that some of the homo-morphic images of the data type are not models of the speciication. The latter are reducts of algebras that would be models of the specii-cation if all its functions were visible, whereas the homomorphic images of the data type are independent of the speciication and need not be compatible with the hidden functions used in it. A hidden function speciication that does not exclude any of the homomorphic images of its initial model from its model class will be called homomorphism preserving. It turns out that, unlike unrestricted initial algebra specii-cation, homomorphism preserving initial algebra speciication of computable data types requires both hidden sorts and hidden functions. 1991 Mathematics Subject Classiication: 08A70 Algebraic struc-tures]: Applications of universal algebra in computer science; 68Q65 Theory of computing]: Abstract data types; algebraic speciica-tion.	abstract data type;algebraic riccati equation;computable function;computer science;initial algebra;linear algebra;morphic (software);theory of computing	Jan A. Bergstra;Jan Heering	1995	Inf. Comput.	10.1006/inco.1995.1079	homomorphism;combinatorics;discrete mathematics;data type;computer science;mathematics;data analysis;programming language;algorithm;algebra	Logic	-10.322019341432288	17.57748152987721	9776
dee8b2684eb26efda8a1939f394b30ed8404eaa9	a formal model for service-based behavior specification using stream-based i/o tables		The increasing complexity of embedded systems makes the formal specification of requirements both more important and more dif- ficult. Services can help provide a foundation for model-driven require- ments engineering for multi-functional embedded systems. This paper provides a conceptual framework that applies a novel modeling approach to the development of embedded systems. We suggest tables as pragmatic specification formalism for a both precise and readable specification of systems, their interfaces, and their functional properties. By translating tables into logical formulas, which define precise semantics for them, the structure specification and refinement of system can be contained. The approach is illustrated by a case study - a tabular specification of a SwStore system.	input/output	Xiuna Zhu	2013		10.1007/978-3-319-07602-7_22	specification language;computer science;systems engineering;system requirements specification;formal specification;functional specification;algorithm;language of temporal ordering specification	DB	-45.9937205355193	28.54459880083318	9777
a7fb4ef7dd922565ccfcf5d6c027904cd416d0c5	rpc in the x-kernel: evaluating new design techniques	remote procedure call;design technique;object oriented;experience design	This paper reports our experiences implementing remote procedure call (RPC) protocols in the x-kernel. This exercise is interesting because the RPC protocols exploit two novel design techniques: virtual protocols and layered protocols. These techniques are made possible because the x-kernel provides an object-oriented infrastructure that supports three significant features: a uniform interface to all protocols, a late binding between protocol layers, and a small overhead for invoking any given protocol layer. For each design technique, the paper motivates the technique with a concrete example, describes how it is applied to the implementation of RPC protocols, and presents the results of experiments designed to evaluate the technique.	experiment;kernel (operating system);late binding;overhead (computing);protocol stack;remote procedure call;subroutine	Norman C. Hutchinson;Larry L. Peterson;Mark B. Abbott;Sean W. O'Malley	1989		10.1145/74850.74860	dce/rpc;real-time computing;experience design;computer science;operating system;database;distributed computing;programming language;object-oriented programming;remote procedure call	OS	-30.72214659276143	39.34559924813012	9779
6e6cfee579c6635e0b6ed2c4c107e9468645ba6c	inter-class test technique between black-box-class and white-box-class for component customization failures	system recovery subroutines program testing;component based development;testing computer science object oriented modeling pattern analysis gold;component based fault injection;system recovery;program testing;component customization testing inter class test technique software component customization failures black box class implementation white box class component interface fault injection targets component customization patterns fault injection operators;software component;fault injection;component customization testing;subroutines	We propose component customization failures by proposing the inter-class test technique between the black­ box class, which represents the 'implementation', and the white-box class, which represents the 'interface' for component customizaiion failures. Our proposed test technique is based on a fault injection technique where a fault is injected into the 'interface' of the component. For our technique, we first extract the component customization patterns and fault injection targets. We then define the fault injection operators, which are applied to the fault injection targets. Since thc fault injection oper;ltors can cover all possible failures that can occur within component customization, the proposed testing technique is suitable for component customization testing.	fault injection	Hoijin Yoon;Byoungju Choi	1999		10.1109/APSEC.1999.809597	reliability engineering;embedded system;real-time computing;computer science;engineering;component-based software engineering;programming language	SE	-54.739709411949946	32.13576871000249	9791
e4264a953838e3750882244f1ada2f6f802ba528	evaluation of a method for reliable message transfer communication in corba	object oriented methods;information systems;electronic messaging information systems software standards business communication object oriented programming object oriented methods open systems software reliability;object oriented programming;business communication;enterprise information system;message service laboratories middleware quality of service information systems application software communication standards timing contracts computer crashes;electronic messaging;software standards;qos property reliable message transfer communication corba 2 0 specification communication infrastructure enterprise information systems interoperability operation time administrative policies nonblocking communication interface standards messaging service;open systems;software reliability	A communication infrastructure for enterprise information sysrems requires not only interoperability but also high reliability. An effective way to allow for differences in operation time and administrative policies is to provide nonblocking communication interface. In the CORBA 2.0 spec$cation, however; non-blocking communication is not defined as a delivery reliability aspect. The OMG is working on standards for Messaging Service, and has taken up reliability as one QoS properly. This does not mean, however; that reliability will always be defined as a function when ORB is implemented. It is therefore important to realize reliability on CORBA at the application level. This paper proposes and implements a communication method whereby the client is not blocked while communicating, and a method for realizing this as an application on CORBA. It then evaluates the implemented system and show the effectiveness of the proposed method for enterprise information systems.	blocking (computing);common object request broker architecture;enterprise information system;interoperability;non-blocking algorithm;operation time;quality of service;sms language	Kouji Iida;Junichi Kikuchi	1997		10.1109/EDOC.1997.628351	real-time computing;computer science;systems engineering;operating system;software engineering;database;distributed computing;open system;programming language;business communication;object-oriented programming;computer security;information system;software quality;enterprise information system	HPC	-37.33999041717586	40.12639493289581	9809
ddbcfa803651425cd57df402a45a26bd42e8d308	special track on coordination models, languages and architectures: editorial message	distributed application;simulation;dynamic environment;intelligent agents;coordination model;negotiation	For the tenth edition of the Special Track on Coordination Models, Languages and Architectures, interaction and coordination are among the main dimensions characterizing any modern notion of computing, deeply affecting the engineering of concurrent and distributed applications. There, the issue about how to put several heterogeneous components (processes, objects, services, agents, humans) together within a working system to integrate them fruitfully in an open and dynamic environments can be considered today among the most important and challenging issues.	distributed computing;intelligent agent;process (computing)	Michael Ignaz Schumacher;Alan Wood	2008		10.1145/1363686.1363710	real-time computing;simulation;computer science;artificial intelligence;operating system;software engineering;database;distributed computing;computer security;intelligent agent;negotiation	SE	-50.52142359557068	20.049083021700216	9818
4334ea9a348efd1c02ad97beb1af2d97e4b01ca7	"""""""jumping through hoops"""": why do java developers struggle with cryptography apis?"""	libraries;complexity theory;encryption;empirical software engineering;public key;cryptography;face;api misuse;java	To protect sensitive data processed by current applications, developers, whether security experts or not, have to rely on cryptography. While cryptography algorithms have become increasingly advanced, many data breaches occur because developers do not correctly use the corresponding APIs. To guide future research into practical solutions to this problem, we perform an empirical investigation into the obstacles developers face while using the Java cryptography APIs, the tasks they use the APIs for, and the kind of (tool) support they desire. We triangulate data from four separate studies that include the analysis of 100 StackOverflow posts, 100 GitHub repositories, and survey input from 48 developers. We find that while developers find it difficult to use certain cryptographic algorithms correctly, they feel surprisingly confident in selecting the right cryptography concepts (e.g., encryption vs. signatures). We also find that the APIs are generally perceived to be too low-level and that developers prefer more task-based solutions.	algorithm;cryptography;data breach;electronic signature;encryption;high- and low-level;java;stack overflow	Sarah Nadi;Stefan Krüger;Mira Mezini;Eric Bodden	2016	2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)	10.1145/2884781.2884790	face;computer science;cryptography;theoretical computer science;operating system;software engineering;cryptography law;java;world wide web;computer security;encryption	SE	-58.56520065732469	56.613189517043914	9825
c825c41f787b08a176d95f512420d2c229fd8b58	wahrscheinlichkeitsbasierte modellverifikation netzbasierter automatisierungssysteme (probabilistic model checking of networked automation systems)	probabilistic model checking		automation;model checking;statistical model	Jürgen Greifeneder;Georg Frey	2007	Automatisierungstechnik	10.1524/auto.2007.55.12.624	reliability engineering;engineering;database	EDA	-46.84365614850007	32.13138019735253	9828
110c359a9b201de06bc78aa85ebb2acc471e7c4f	an iot-based gamified approach for reducing occupants’ energy wastage in public buildings	behavioral economics;employee;energy disaggregation;energy efficiency;gamification;sustainability	Conserving energy amenable to the activities of occupants in public buildings is a particularly challenging objective that includes associating energy consumption to particular individuals and providing them with incentives to alter their behavior. This paper describes a gamification framework that aims to facilitate achieving greater energy conservation in public buildings. The framework leverages IoT-enabled low-cost devices, to improve energy disaggregation mechanisms that provide energy use and-consequently-wastage information at the device, area and end-user level. The identified wastages are concurrently targeted by a gamified application that motivates respective behavioral changes combining team competition, virtual rewards and life simulation. Our solution is being developed iteratively with the end-users' engagement during the analysis, design, development and validation phases in public buildings located in three different countries: Luxembourg (Musée National d'Histoire et d'Art), Spain (EcoUrbanBuilding, Institut Català d'Energia headquarters, Barcelona) and Greece (General Secretariat of the Municipality of Athens).	abnormal behavior;gamification;rewards;simulation	Thanasis G. Papaioannou;Nikos Dimitriou;Kostas Vasilakis;Anthony Schoofs;Manolis Nikiforakis;Fabian Pursche;Nikolay Deliyski;Amr Taha;Dimosthenis Kotsopoulos;Cleopatra Bardaki;Sarantis Kotsilitis;Anastasia Garbi	2018		10.3390/s18020537	electronic engineering;efficient energy use;environmental resource management;energy consumption;behavioral economics;engineering;internet of things;sustainability;energy conservation;incentive	HCI	-32.67809166062784	19.84368816458132	9842
7f9da4ec61e9de979cdc12bfb00963e42e6e6931	bridging semantic gaps between stakeholders in the production automation domain with ontology areas	semantic gap;data collection;data model;use case	Stakeholders from several domains with local terminologies have to work together to develop and operate softwareintensive systems, like production automation systems. Ontologies support the translation between local terminologies via common domain concepts. Unfortunately, the ontology models can become large and complex if they include several aspects on a domain and some parts of the data model are volatile. In this paper, we propose a data modeling approach to support ontology users based on ontology building blocks, so-called “Ontology Areas” (OAs), which allow solving tasks with smaller parts of the overall ontology. We evaluate the proposed approach with use cases from the production automation domain: translation between stakeholder roles to support design-time and run-time decision making. Major result in the study context is that OAs improved the efficiency of data collection for decision making.	automation;bridging (networking);cognitive complexity;data model;data modeling;database;ontology (information science);ontology engineering;run time (program lifecycle phase);volatile memory	Stefan Biffl;Wikan Danar Sunindyo;Thomas Moser	2009			data mining;semantic gap;semantic data model;ontology-based data integration;ontology (information science);data modeling;computer science;ontology;data model;use case	AI	-55.14694696804036	17.84074020601732	9843
1b944436298e7866018b441f3716b8116f38aa97	scalable resilience: the resist network of excellence	ubiquitous computing systems;mobile device;ambient intelligence;pervasive computing;resists;physics computing;computer networks;computer security;security of data computer networks mobile computing;human factors;resilience;malicious attacks resist ubiquitous computing systems computer networks mobile devices ambient intelligence;ubiquitous computing;europe;mobile computing;malicious attacks;security of data;mobile devices;resilience resists computer security human factors europe ubiquitous computing computer networks pervasive computing physics computing mobile computing;resist;data security;network of excellence	Summary form only given. ReSIST is a network of excellence that integrates leading researchers active in the multidisciplinary domains of dependability, security, and human factors, in order that Europe will have a well-focused coherent set of research activities aimed at ensuring that future ubiquitous computing systems (the immense systems of ever-evolving networks of computers and mobile devices which are needed to support and provide ambient intelligence), have the necessary resilience and survivability, despite any residual development and physical faults, interaction mistakes, or malicious attacks and disruptions	ambient intelligence;coherence (physics);computer;cyber resilience;dependability;evolving networks;human factors and ergonomics;malware;mobile device;scalability;ubiquitous computing	J. Laprie	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639662	computer science;operating system;mobile device;resist;distributed computing;internet privacy;computer security;ubiquitous computing;computer network	Arch	-50.167513554197754	55.70361736335475	9850
51c6e03c8bdd9e54887218f336949bc1d062c237	invariants synthesis over a combined domain for automated program verification	book chapter	Program invariants such as loop invariants and method specifications ( a.k.a. procedural summaries) are key components in program verification. Such invariants are usually manually specified by users before passed as inputs to a program verifier. The process of manually annotating programs with such invariants is tedious and error-prone and can significantly hinder the level of automation in program verification. Although invariant synthesis techniques have made noticeable progress in reducing the burden of user annotations; when it comes to automated verification of memory safety and functional correctness for heap-manipulating programs, it remains a rather challenging task to discover program specifications and invariants automatically, due to the complexity of aliasing and mutability of data structures.#R##N##R##N#In this paper, we present invariant synthesis algorithms for the following scenarios: i) to synthesise a missing loop invariant, ii) to refine given pre/post shape templates to complete pre/post-conditions, iii) to infer a missing precondition, iv) to calculate a missing postcondition, given a precondition. The proposed analyses are based on abstract interpretation and are built over an abstract domain combining separation, numerical and multi-set (bag) information. Our inference mechanisms are equipped with newly designed abstraction, join, widening and abduction operations. Initial prototypical experiments have shown that they are viable and powerful enough to discover interesting useful invariants for non-trivial programs.	formal verification	Shengchao Qin;Guanhua He;Wei-Ngan Chin;Hongli Yang	2013		10.1007/978-3-642-39698-4_19	computer science;theoretical computer science;data mining;programming language;algorithm	Logic	-19.40675907661017	27.105883210284716	9861
39a7be10d317f1d9b791d01251b9016863c77807	toward resilient networks with fog computing		Cloud computing is a solution to reduce the cost of IT by providing elastic access to shared resources. It also provides solutions for on-demand computing power and storage for devices at the edge networks with limited resources. However, increasing the number of connected devices caused by IoT architecture leads to higher network traffic and delay for cloud computing. The centralised architecture of cloud computing also makes the edge networks more susceptible to challenges in the core network. Fog computing is a solution to decrease the network traffic, delay, and increase network resilience. In this paper, we study how fog computing may improve network resilience. We also conduct a simulation to study the effect of fog computing on network traffic and delay. We conclude that using fog computing prepares the network for better response time in case of interactive requests and makes the edge networks more resilient to challenges in the core network.	algorithm;centralisation;cloud computing;fog computing;load balancing (computing);network architecture;network packet;network traffic control;response time (technology);simulation	Amir Modarresi;James P. G. Sterbenz	2017	2017 9th International Workshop on Resilient Networks Design and Modeling (RNDM)	10.1109/RNDM.2017.8093032	grid computing;utility computing;computer network;cloud computing;architecture;response time;core network;edge computing;computer science;internet of things;distributed computing	HPC	-20.84283465500074	59.71696013202873	9863
92a8dba7bbb7cfb7f348d23a5aeef3febce09503	a methodological approach for assessing amplified reflection distributed denial of service on the internet of things	vulnerability assessment;risk management;amplified reflection;distributed denial of service;pentest	Concerns about security on Internet of Things (IoT) cover data privacy and integrity, access control, and availability. IoT abuse in distributed denial of service attacks is a major issue, as typical IoT devices' limited computing, communications, and power resources are prioritized in implementing functionality rather than security features. Incidents involving attacks have been reported, but without clear characterization and evaluation of threats and impacts. The main purpose of this work is to methodically assess the possible impacts of a specific class-amplified reflection distributed denial of service attacks (AR-DDoS)-against IoT. The novel approach used to empirically examine the threat represented by running the attack over a controlled environment, with IoT devices, considered the perspective of an attacker. The methodology used in tests includes that perspective, and actively prospects vulnerabilities in computer systems. This methodology defines standardized procedures for tool-independent vulnerability assessment based on strategy, and the decision flows during execution of penetration tests (pentests). After validation in different scenarios, the methodology was applied in amplified reflection distributed denial of service (AR-DDoS) attack threat assessment. Results show that, according to attack intensity, AR-DDoS saturates reflector infrastructure. Therefore, concerns about AR-DDoS are founded, but expected impact on abused IoT infrastructure and devices will be possibly as hard as on final victims.	access control;attack surface;best practice;brute-force search;computer systems;constrained application protocol;denial (psychology);denial-of-service attack;experiment;flow;formal verification;information privacy;internet of things;mathematical optimization;penetration test;protocols documentation;ssbp3 gene;simple service discovery protocol;substance abuse problem;threat (computer);thrombocytopenia;vulnerability (computing);cyclophosphamide/cytarabine/prednisone/vincristine protocol	João José Costa Gondim;Robson de Oliveira Albuquerque;Anderson Clayton Alves Nascimento;L. Javier García-Villalba;Tai-Hoon Kim	2016		10.3390/s16111855	penetration test;risk management;computer science;engineering;vulnerability assessment;distributed computing;internet privacy;computer security;denial-of-service attack	Security	-51.69290587970504	55.408674924451596	9877
2e74cf4c4d8ae4dbfa8f4a7a0d3dab6d558d9754	an algorithm for keyword search on an execution path	directed graphs;software;java source code execution path code search method keyword code search algorithm query search result oo program object oriented program and or call graph data structure;query processing;prototypes;object oriented programming;receivers;indexing;data structures;java prototypes receivers indexing data structures software couplings;query processing directed graphs object oriented programming;couplings;java	This paper presents a code-search method, which includes an algorithm of keyword code-search and a prototype implementation. In this paper, a query is a set of keywords and a search result is a set of execution paths fulfilling the query, that is, each of the execution paths includes all of the keywords. Here, an execution path represents one of all levels of method calls of all possible dynamic dispatches in an OO program; thus, many execution paths can be generated even from a small program. The algorithm works on a data structure named an And/Or/Call graph, which is a compact representation of execution paths. The prototype implementation searches names of methods or types, or words in string literals from Java source code.	call graph;comment (computer programming);data structure;dynamic dispatch;entity;entry point;java;negative base;numerical analysis;open-source software;prototype;pseudocode;return statement;search algorithm;unreachable memory;jedit	Toshihiro Kamiya	2014	2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)	10.1109/CSMR-WCRE.2014.6747187	search engine indexing;directed graph;data structure;computer science;theoretical computer science;database;prototype;coupling;programming language;object-oriented programming;web search query;java	SE	-56.41849677083672	37.12028405517307	9887
54db7de37bc2675587259036d4fa63193cbd219d	agent-based communities of web services: an argumentation-driven approach	agent based;software agent;web service;agents;community of web services;community of web services agents argumentation theory;argumentation theory	The objective of this paper is to discuss how to sustain the growth of Web services through the use of communities. A community aims at gathering Web services with the same functionality independently of their origins, locations, and functioning. To make Web services more responsive to the environment in which they run and to be more flexible when managing communities, Web services are associated with software agents enhanced with argumentation capacities. This type of agents persuade and negotiate with other peers for the sake of letting their respective Web services reach their goals in an efficient way. Associating Web services with this type of agents allows them to select good communities and allow the communities to host the good Web services and to select the best ones for composite scenarios. Furthermore, this provides satisfactory solutions for three open problems: starvation (Web services refuse all the possibilities of joining communities), competition-free (Web services accept joining any community without being selective), and unfairness (always the same Web services members of a community are selected out of many others to participate in composite scenarios). In addition, the paper presents a formal and computational persuasive and negotiation protocol to manage the attraction and retainment of Web services in the communities and their identification for composite services.	computation;design rationale;experiment;prototype;simulation;software agent;web service;world wide web	Jamal Bentahar;Zakaria Maamar;Wei Wan;Djamal Benslimane;Philippe Thiran;Sattanathan Subramanian	2008	Service Oriented Computing and Applications	10.1007/s11761-008-0033-4	web service;web application security;web development;web accessibility initiative;web standards;computer science;knowledge management;artificial intelligence;software agent;ws-policy;service-oriented architecture;social semantic web;ws-addressing;multimedia;services computing;web intelligence;web engineering;world wide web	Web+IR	-45.53258906546955	14.535708878783812	9897
a590d333489ffc908b94e1f853cd98169f2e3a8e	general recursion on second order term algebras	second order;methode recursive;canonical form;forme canonique;metodo recursivo;orden 2;recursive method;lambda calculus;functional programming;algebre;higher order;theorem proving;demonstration theoreme;first order;algebra;rewriting systems;indexation;metaobjet;forma canonica;normal form;lambda calculo;programmation fonctionnelle;ordre 2;demostracion teorema;typed lambda calculus;lambda calcul;programacion funcional;systeme reecriture;intersection types;type system;metaobject	"""Extensions of the simply typed lambda calculus have been used as a metalanguage to represent """"higher order term algebras"""", such as, for instance, formulas of the predicate calculus. In this representation bound variables of the object language are represented by bound variables of the metalanguage. This choice has various advantages but makes the notion of """"recursive definition"""" on higher order term algebras more subtle than the corresponding notion on first order term algebras. Despeyroux, Pfenning and Schurmann pointed out the problems that arise in the proof of a canonical form theorem when one combines higher order representations with primitive recursion.#R##N##R##N#In this paper we consider a stronger scheme of recursion and we prove that it captures all partial recursive functions on second order term algebras. We illustrate the system by considering typed programs to reduce to normal form terms of the untyped lambda calculus, encoded as elements of a second order term algebra. First order encodings based on de Bruijn indexes are also considered. The examples also show that a version of the intersection type disciplines can be helpful in some cases to prove the existence of a canonical form. Finally we consider interpretations of our typed systems in the pure lambda calculus and a new godelization of the pure lambda calculus."""	recursion	Alessandro Berarducci;Corrado Böhm	2001		10.1007/3-540-45127-7_4	system f;deductive lambda calculus;canonical form;fixed-point combinator;typed lambda calculus;discrete mathematics;dependent type;binary lambda calculus;normalisation by evaluation;higher-order logic;type system;pure type system;computer science;normalization property;lambda calculus;first-order logic;simply typed lambda calculus;mathematics;automated theorem proving;programming language;functional programming;church encoding;lambda cube;second-order logic;type inhabitation;algorithm;director string;algebra	Logic	-12.904498120645211	17.624943141737614	9900
866abfd8cbd747cb50dc48c0f1e0b7c04d6f8b15	improved multilevel security with latent semantic indexing	systems;decision support;retrieval;information security;text mining;policies;classification;access control;business and economics;data protection;operating characteristic curves;multilevel security	Multilevel security (MLS) is specifically created t o protect information from unauthorized access. In MLS, documents are assigned to a security label by a tru sted subject e.g. an authorized user and based on t his assignment; the access to documents is allowed or d enie . Using a large number of security labels lead to a complex administration in MLS based operating syste m . This is because the manual assignment of docume nts to a large number of security labels by an authoriz ed user is time-consuming and error-prone. Thus in practice, most MLS based operating systems use a small number of s curity labels. However, information that is n ormally processed in an organization consists of different s sitivities and belongs to different compartments . To depict this information in MLS, a large number of security labels is necessary. The aim of this paper is to show that the use of la tent semantic indexing is successful in assigning t extual information to security labels. This supports the a uthorized user by his manual assignment. It reduces complexity by the administration of a MLS based operating syst em and it enables the use of a large number of secu rity labels. In future, the findings probably will lead to an increased usage of these MLS based operating systems in organizations.	authorization;cognitive dimensions of notations;data logger;decision support system;document classification;information exchange;information privacy;information security;latent semantic analysis;linear algebra;logistic regression;mandatory access control;multilevel security;operating system;tru-vue	Dirk Thorleuchter;Dirk Van den Poel	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.06.002	computer security model;cloud computing security;text mining;security through obscurity;security information and event management;asset;biological classification;computer science;information security;access control;logical security;data mining;database;security service;system;data protection act 1998;computer security	Security	-60.28557114432824	51.13877643561506	9919
78ed84bd91c001be71d6ac37c6971cd65a2282df	analysis of permission-based security in android through policy expert, developer, and end user perspectives		Being one of the major operating system in smartphone industry, security in Android is paramount importance to end users. Android applications are published through Google Play Store which is an official marketplace for Android. If we have to define the current security policy implemented by Google Play Store for publishing Android applications in one sentence then we can write it as “all are suspect but innocent until proven guilty.” It means an application does not have to go through rigorous security review to be accepted for publication. It is assumed that all the applications are benign which does not mean it will remain so in future. If any application is found doing suspicious activities then the application will be categorized as malicious and it will be removed from the Play Store. Though filtering of malicious applications is performed at Play Store, some malicious applications escape the filtering process. Thus, it becomes necessary to take strong security measures at other levels. Security in Android can be enforced at system and application levels. At system level Android uses sandboxing technique while at application level it uses permission. In this paper, we analyze the permission-based security implemented in Android through three different perspectives – policy expert, developer, and end user.	android;categorization;experiment;information security;malware;operating system;play store;run time (program lifecycle phase);sandbox (computer security);sensor;smartphone;transport layer security	Ajay Kumar Jha;Woo Jin Lee	2016	J. UCS		end user;world wide web;android (operating system);computer science;permission;internet privacy	Security	-56.176780700951205	60.11506603062592	9931
3bc38250f949ce509828bdbb55f3e3626180229e	supporting collaboration in the development of tools and dies in manufacturing networks	manufacturing systems;collaboration in manufacturing networks;groupware;pulp manufacturing;computer networks groupware concurrent engineering industrial control manufacturing systems;collaborative work;dies;collaboration;mass production;collaborative system;manufacturing industries;collaborative tools;computer networks;mobile phone;collaborative environment;manufacturing networks;special needs;collaborative systems;industrial implementation;industrial control;collaborative tools dies manufacturing industries collaborative work mobile handsets product development mass production collaboration concurrent engineering pulp manufacturing;mobile handsets;die manufacturing industry;manufacturing industry;tool manufacturing industry;global production operations;supporting collaboration;off the shelf;collaborative support in manufacturingnetworks;industrial implementation supporting collaboration manufacturing networks tool manufacturing industry die manufacturing industry global production operations collaborative systems;concurrent engineering;product development	Parallel to the development of a main product such as a car body or a mobile phone, numerous tools and dies need to be specified, developed and produced. The process chain of the tools and dies industry represents the functional link between the main product development and the mass production of that product. The evolving market as well as the changing production and supplier structures of main product vendors is forcing the producers of tools and dies to change their traditional working methods and to focus on global production operations. As a result, there is an increasing need for collaborative systems in the tool and die manufacturing industry. Off-the-shelf collaborative systems do not meet the special needs of tools and die producers and such systems often have to be customized. This paper reports on the ongoing design and industrial implementation of a collaborative environment to support concurrent engineering in a manufacturing network.		Zorlu Yalniz;Engin Kirda	2003		10.1109/ENABL.2003.1231388	manufacturing;management;product engineering;collaboration	Robotics	-59.27798537138663	10.222618181349858	9936
bbf0ff8b6a11e371fca1779e61d0e336a86aaf77	workflow services: a petri net-based approach to web services	web service;petri net	This application relates to phenyl-substituted 1,2,3,4-tetrahydrocarbazoles and to their use as anti-depressant agents useful in the treatment of mental depression of either endogenous or reactive nature.	petri net;web service	Kirsten Lenz;Andreas Oberweis	2004			workflow management system;software engineering;web service;petri net;database;workflow;ws-i basic profile;computer science	Robotics	-54.62286648000043	15.473305384001227	9938
9df7b44a8ce5b9d6b44202211e6aa928820c60a9	using abstraction in mda-based reverse engineering for creative evolution	software engineering reverse engineering;creativity;mda based reverse engineering software evolution methodology e learning system abstraction techniques model driven architecture software community legacy systems software engineering creative computing creative software application software development creativity creative evolution;abstraction;evolution reverse engineering abstraction model driven architecture creativity;software reverse engineering computer integrated manufacturing object oriented modeling aging computer architecture computational modeling;model driven architecture;reverse engineering;evolution	Creativity becomes an ever important feature in software development nowadays as software application in various domains is on an ever rapid growth. It leads to changes on system requirements, hoping to improve systems with attractive creativity features. Consequently, creative computing is aroused as an emerging research field in software engineering. Currently, there are researches working on approaches and processes to develop creative software from scratch. However, evolving existing systems can achieve the same purpose, though it is hard to add creativity features to legacy systems by traditional software evolution approaches. Obviously, based on experiences over decades from software community, development from scratch might cause a waste in terms of resources. To solve this problem, this paper proposes a Model Driven Architecture based reverse engineering method for creative evolution. Abstraction techniques are explored, classified, designed and applied in this proposed approach. Meanwhile, details and roles of abstraction techniques were developed for each step. Also, an e-learning system is chosen as a case study to illustrate, validate and evaluate the proposed method. Overall, our work aims to contribute to software evolution methodology and its application creatively.	legacy system;model-driven architecture;requirement;reverse engineering;software development;software engineering;software evolution;system requirements	Delin Jing;Hongji Yang;Hossam Hakeem	2014	2014 20th International Conference on Automation and Computing	10.1109/IConAC.2014.6935462	personal software process;architecture tradeoff analysis method;verification and validation;computing;software engineering process group;software sizing;search-based software engineering;software evolution;software design;social software engineering;software framework;component-based software engineering;software development;software construction;evolution;abstraction;software analytics;resource-oriented architecture;creativity;software development process;software requirements;reverse engineering;software system	SE	-60.94420332201962	24.272196743167854	9952
0de1f206f899101e213ed9e25a4055d23300b0af	monitoring and control for energy efficiency in the smart house	energy efficient;software agent;web service;monitoring and control	The high heterogeneity in smart house infrastructures as well as in the smart grid poses several challenges when it comes into developing approaches for energy efficiency. Consequently, several monitoring and control approaches are underway, and although they share the common goal of optimizing energy usage, they are fundamentally different at design and operational level. Therefore, we consider of high importance to investigate if they can be integrated and, more importantly, we provide common services to emerging enterprise applications that seek to hide the existing heterogeneity. We present here our motivation and efforts in bringing together the PowerMatcher, BEMI and the Magic system.	algorithm;computer-mediated communication;duplex (telecommunications);enterprise integration;enterprise software;fits;gateway (telecommunications);imperative programming;mathematical optimization;smart house;web service	Stamatis Karnouskos;Anke Weidlich;Jan Ringelstein;Aris L. Dimeas;Koen Kok;Cor Warmer;Patrick Selzam;Stefan Drenkard;Nikos D. Hatziargyriou;Vally Lioliou	2010		10.1007/978-3-642-19322-4_21	simulation;engineering;operations management;computer security	PL	-43.79696732653501	46.16320029078746	9956
8844bc92e015a45f918c1b168767b94bba297b65	a three tier architecture applied to lidar processing and monitoring	tier architecture;scientific problem;grid technology;lidar processing;high performance computational task;scientific workflow engine;scientific work;high performance processing;emerging grid technology;scientific workflow approach;accessing grid cluster;grid resource;lidar	Emerging Grid technologies enable solving scientific problems that involve large datasets and complex analyses, which in the past were often considered difficult to solve. Coordinating distributed Grid resources and computational processes requires adaptable interfaces and tools that provide modularized and configurable environments for accessing Grid clusters and executing high performance computational tasks. Computationally intensive processes are also subject to a high risk of component failures and thus require close monitoring. In this paper we describe a scientific workflow approach to coordinate various resources via data analysis pipelines. We present a three tier architecture for LiDAR interpolation and analysis, a high performance processing of point intensive datasets, utilizing a portal, a scientific workflow engine and Grid technologies. Our proposed solution is available to the community in a unified framework through a shared cyberinfrastructure, the GEON portal, enabling scientists to focus on their scientific work and not be concerned with the implementation of the underlying infrastructure.	multitier architecture	Efrat Jaeger-Frank;Christopher J. Crosby;Ashraf Memon;Viswanath Nandigam;Jeffery Conner;J. Ramon Arrowsmith;Ilkay Altintas;Chaitanya K. Baru	2006	Scientific Programming		lidar;computer science;data science;operating system;data mining;database	HPC	-30.268761551077958	51.36576512572614	10018
f48fd18a39bc1eee2ddfedabd82f679b038d83fb	starots: an effcient distributed transaction recovery mechanism in the corba component runtime environment	distributed transactions	Two Phase Commit (2PC) protocol can be used to guarantee atomicity and durability of global transactions in distributed environment. In this paper, we adopt optimized 2PC protocol (O2PC), which reduces the number of messages between transaction participants and the coordinator. Based on the protocol, an object-oriented transaction recovery manager, StarOTS is implemented as a CORBA service running on top of a given ORB. We discuss how StarOTS is designed and implemented to ensure atomicity and durability of distributed transactions and how it is integrated with the CORBA component environment to meet the requirements of interoperability, efficiency and reliability. Further, we have constructed a novel dynamic management tool offering flexible control and management to the running transactions without modifying StarOTS. The CORBA component model prototype we implemented and StarOTS integrated together help developers quickly design and implement mission critical distributed transactional applications.	atomicity (database systems);common object request broker architecture;component-based software engineering;customer information control system (cics);distributed computing;distributed transaction;durability (database systems);integrated development environment;interoperability;loose coupling;mission critical;programming paradigm;prototype;requirement;runtime system;software architecture;teleprocessing monitor;transaction processing;two-phase commit protocol	Yi Ming Ren;Jianbo Guan;Yan Jia;Weihong Han;Quanyuan Wu	2003		10.1007/978-3-540-24679-4_155	real-time computing;database transaction;distributed transaction;computer science;database;distributed computing;compensating transaction;serializability;acid	SE	-31.869562705278607	44.65524336082082	10023
fa7892481990fba487d2dc55f4d65dde2e48a12b	distributed systems: from models to components	modelizacion;design model;distributed system;description systeme;system description;systeme reparti;telecommunication network;object oriented programming;specification language;modelisation;sistema repartido;design method;red telecomunicacion;reseau telecommunication;middleware;descripcion sistema;lenguaje especificacion;programmation orientee objet;modeling;langage specification;lenguaje formal;formal language;langage formel	Advanced design methods are needed to fulfill the increasing requirements of telecommunication service development. For a design method the relevant concepts for the application domain have to be defined, a supporting notation has to be declared and finally rules have to be developed to map design models to supporting runtime environments. The ITU-T has followed this route by defining concepts for the design of distributed telecommunication applications and supporting notations for these concepts. In the past, the ITU-T has defined several languages and notations to support structural and behavioral descriptions of distributed telecommunication systems, namely ODL, SDL-2000 and MSC2000. With the rise of the component age, an additional technique (DCL) is under development that enables component based manufacturing of distributed systems. Beside these languages, the ITU-T recognized the common need for open, component aware object middleware platform standards as the runtime environment for these systems. This contribution is about integration. 1 Motivation and Introduction A dedicated and efficient design methodology contributes significantly to a reduction of the time to market distributed applications and telecommunication services. An appropriate treatment of all kinds of communication aspects lies in the very nature of the targeted application domain. These aspects span from functional requirements on object interactions over quality-of-service issues to security properties. Taking into account the broad acceptance of object middleware technology, middleware platforms provide an ideal implementation environment for such designs. Conceptually, an appropriate design method may be split into three separate parts: R. Reed and J. Reed (Eds.): SDL 2001, LNCS 2078, pp. 250–267, 2001. c © Springer-Verlag Berlin Heidelberg 2001 Distributed Systems: From Models to Components 251 – A concept space, that contains all relevant entities that conceptually reflect the elements of the problem domain and the information for the description of these elements and their relations; – a concrete notation to visually specify models using the elements of the concept space; – and a set of rules to specify the mapping of models onto middleware technologies. Following this approach, a concept space is independent of a specific design notation. Design models can be developed in different notations but are based on the same concepts. Design information can then be exchanged on the basis of the common concept space. Second, both the notation and the concept space are independent of a specific runtime-environment. The same design can be mapped onto different environments. This enables a high flexibility and is also important for the aspect of re-use of component design models. In the past, the ITU-languages ODL [1] and SDL [3] as well as the proposed language DCL [19] represented different aspects within the design of distributed systems. No common concept space for them has been defined, an integration is only possible by pair-wise direct mappings based on notational concepts ([12,13]). This is also valid for the relation of ITU languages to languages of other communities, for instance between ODL and OMG-Component IDL or SDL/MSC and UML [5]. This paper discusses the concepts of and the relations between selected ITUlanguages and the current trends for their future development and refinement taking into account corresponding OMG activities. In order to apply different techniques with their specific advantages in a combined approach for the design of distributed systems, the underlying concepts have to be integrated within a common concept space. This can be achieved in 3 main steps: – the identification of those concepts of each technique, that are relevant to a combined approach; – the definition of the semantics for a combined concept space; – the realization of the integration within a common meta-model framework by applying a suitable technology. Consequently, major parts of the components that form a distributed system will be generated from design models that are based on the common meta-model and target specific object middleware platforms. Therefore, an example of such a platform will be presented here as well. 2 Modelling Distributed Systems 2.1 ODL for Modelling of Software Components Starting from the basic reference model of Open Distributed Processing (ODP), the TINA community has taken up this general approach to define dedicated computational modelling concepts and a supporting notation Object Definition 252 F. Dubois et al. Language (ODL) that was standardized later by ITU. As a result ITU-ODL offers necessary key concepts necessary to design distributed telecommunication applications as Computational Objects (COs) communicating via multiple well defined interfaces. Interfaces are described in terms of signatures defining the elements for potential interactions a CO instance may participate in. ODL distinguishes between operational and stream interactions. Because ODL was designed to be technology independent, different language mappings to technology dependent modelling or implementation languages have to be defined for a complete model based generation of software components. The design process should also include the distribution, installation, and configuration of produced binary components, taking into account concrete distributed processing environments. Some of the mappings (Fig. 1) will be discussed in the following.	application domain;bi-directional text;cartography;common object request broker architecture;component-based software engineering;computation;digital command language;distributed computing;entity;executable;functional requirement;gadget (computer science);interaction;lecture notes in computer science;mathematics subject classification;meta-object facility;metamodeling;middleware;object language;problem domain;quality of service;rm-odp;reference model;refinement (computing);runtime system;sms language;simple directmedia layer;software design;springer (tank);type signature;unified modeling language	Fabrice Dubois;Marc Born;Harald Böhme;Joachim Fischer;Eckhardt Holz;Olaf Kath;Bertram Neubauer;Frank Stoinski	2001		10.1007/3-540-48213-X_16	formal language;systems modeling;design methods;specification language;computer science;operating system;middleware;programming language;object-oriented programming;algorithm;telecommunications network	SE	-38.55589928786758	27.687721921041756	10026
d94f66470749bff76358b777e4b96d1972c932bf	virtual shared memory programming on workstation clusters	parallel computing;application development;modelizacion;agregacion;virtual memory;estacion trabajo;dynamic load balancing;shared memory;structure programme;programacion paralela;memoria compartida;station travail;parallel programming;transmission message;supercomputer;aggregation;message transmission;parallel computation;price level;virtual shared memory;programming model;supercomputador;modelisation;workstation;calculo paralelo;complex data;estructura programa;estructura datos;memoire virtuelle;parallel computer;message passing;agregation;structure donnee;parallel programs;modeling;program structure;calcul parallele;data structure;memoria virtual;data transfer;superordinateur;memoire partagee;programmation parallele;transmision mensaje;workstation cluster	Workstation clusters have recently attracted high interest as a technology providing supercomputer class performance at much lower price levels. Today the message passing programming model dominates the application development, despite the overhead and the complexity introduced by the explicitly coded data transfers. We give an introduction to the virtual shared memory programming model and report on the experiences with the MNFS system. We show that shared memory with weak coherency can perform competitive to message passing and provide an excellent tool for parallelizing programs using complex data structures with dynamic load balancing.	shared memory;workstation	Jörg-Thomas Pfenning;Achim Bachem;Ronald Minnich	1995	Future Generation Comp. Syst.	10.1016/0167-739X(95)00006-E	distributed shared memory;shared memory;supercomputer;parallel computing;message passing;systems modeling;workstation;data structure;computer science;virtual memory;operating system;database;distributed computing;programming paradigm;programming language;rapid application development;data diffusion machine;price level;complex data type	Arch	-16.733382267841368	42.6100120682284	10031
2e391bb7615efc02983ff6f08f3a923b4f6858eb	software scheduling in the co-synthesis of reactive real-time systems	real time systems software systems timing delay scheduling algorithm software algorithms control systems embedded system hardware embedded software;control systems;software systems;embedded system;scheduling algorithm;software algorithms;coarse grained;hard real time;embedded software;hardware;real time systems;timing;time constraint	Existing software scheduling techniques limit the functions that can be implemented in software to those with a restricted class of timing constraints, in particular those with a coarse-grained, uniform, periodic behavior. In practice, however, many systems change their I/O behavior in response to the inputs from the environment. This paper considers one such class of systems, called reactive real-time systems, where timing requirements can include sequencing, rate, and response time constraints. We present a static, non-preemptive, fine-grained software scheduling algorithm to meet these constraints. This algorithm is suitable for control-dominated embedded systems with hard real-time constraints, and is part of the core of a hardware/software co-synthesis system.	algorithm;embedded system;input/output;real-time clock;real-time computing;real-time transcription;requirement;response time (technology);scheduling (computing)	Pai H. Chou;Gaetano Borriello	1994	31st Design Automation Conference	10.1145/196244.196247	fair-share scheduling;embedded system;computer architecture;real-time computing;software sizing;embedded software;computer science;control system;software design;software framework;component-based software engineering;software development;software design description;operating system;software construction;distributed computing;systems development life cycle;scheduling;software metric;software system;avionics software	Embedded	-8.410549632526537	60.201336920176225	10034
a21bb0135948300d25438d860ce33c5d1d0ec507	a hybrid approach to detecting security defects in programs	analytical models;software;cluster algorithm;program diagnostics;fuzzy reasoning;ontology based static analysis;feature extraction security defects static analysis model checking ontology model fuzzy inference;security defects;program verification;logic reasoning;ontologies artificial intelligence;hybrid approach;model checking;source code security defect ontology based static analysis model checking fuzzy inference system cluster algorithm logic reasoning;feature extraction;security defect;cognition;security of data fuzzy reasoning ontologies artificial intelligence program diagnostics program verification;fuzzy inference;fuzzy inference system;ontologies;source code;state explosion;static analysis;security face detection system recovery routing explosions fuzzy systems clustering algorithms inference algorithms ontologies logic;security;ontology model;security of data;java	Static analysis works well at checking defects that clearly map to source code constructs. Model checking can find defects of deadlocks and routing loops that are not easily detected by static analysis, but faces the problem of state explosion. This paper proposes a hybrid approach to detecting security defects in programs. Fuzzy Inference System is used to infer selection among the two detection approaches. A cluster algorithm is developed to divide a large system into several clusters in order to apply model checking. Ontology based static analysis employs logic reasoning to intelligently detect the defects. We also put forwards strategies to improve performance of the static analysis. At last, we perform experiments to evaluate the accuracy and performance of the hybrid approach.	algorithm;deadlock;experiment;malware;model checking;routing;sensor;static program analysis	Lian Yu;Jun Zhou;Yue Yi;Jianchu Fan;Qianxiang Wang	2009	2009 Ninth International Conference on Quality Software	10.1109/QSIC.2009.10	model checking;logical reasoning;cognition;feature extraction;computer science;ontology;information security;theoretical computer science;machine learning;data mining;programming language;java;static analysis;source code	SE	-61.33892980336296	58.51163921495865	10040
171268c76e683c5b34aad22188d276bb6ae7139c	synthesis of operation-centric hardware descriptions	finite state machines;logic design;concurrency;cooperating finite state machines;cycle-by-cycle interactions;hardware synthesis;multiple concurrent state machines;operation-centric descriptions;operation-centric hardware descriptions;scheduling	"""Most hardware description frameworks, whether schematic or textual, use cooperating finite state machines (CFSM) as the underlying abstraction. In the CFSM framework, a designer explicitly manages the concurrency by scheduling the exact cycle-by-cycle interactions between multiple concurrent state machines. Design mistakes are common in coordinating interactions between two state machines because transitions in different state machines are not semantically coupled. It is also difficult to modify one state machine without considering its interaction with the rest of the system.This paper presents a method for hardware synthesis from an """"operation centric"""" description, where the behavior of a system is described as a collection of """"atomic"""" operations in the form of rules. Typically, a rule is defined by a predicate condition and an effect on the state of the system. The atomicity requirement simplifies the task of hardware description by permitting the designer to formulate each rule as if the rest of the system is static.An implementation can execute several rules concurrently in a clock cycle, provided some sequential execution of those rules can reproduce the behavior of the concurrent execution. In fact, detecting and scheduling valid concurrent execution of rules is the central issue in hardware synthesis from operation-centric descriptions. The result of this paper shows that an operation-centric framework offers significant reduction in design time, without loss in implementation quality."""	atomicity (database systems);clock signal;concurrency (computer science);finite-state machine;hardware description language;interaction;linearizability;scheduling (computing);schematic;sensor	James C. Hoe;Arvind	2000			embedded system;electronic engineering;logic synthesis;real-time computing;inductance;computer science;theoretical computer science;operating system;distributed computing;very-large-scale integration;finite-state machine;programming language;hardware register;algorithm	EDA	-33.259093841191614	33.24485470638585	10043
823a479b581b8e144114adb79dd328c93f6c7172	cyber-physical system integration for industry 4.0: modelling and simulation of an induction heating process for aluminium-steel molds in footwear soles manufacturing		In recent years, the Cyber-Physical Systems (CPSs), have become a new trend to increase and to enrich the interactions between physical and virtual systems with the goal to create a truly connected world in which smart objects interact and exchange data with each other. The CPS is the core of the new industrial revolution called “Industry 4.0”, which promotes the computerization of manufacturing to make decentralized decisions. Within the modular structured smart factories, Cyber-Physical Systems monitor physical processes, create a virtual copy of the physical world, simulate parts of process and implement sophisticated control policies in order to take optimized decisions. This research proposes the modelling and simulation of an induction heating process for aluminium-steel mold, which is used in the production of footwear soles. The modelling supports the simulation of a CPS model related to the use of a multi-use LGV (Laser Guided Vehicle) which transports aluminum-steel molds from a mechanized warehouse to the final rotary production line, used for the soles foaming. In detail, a thermal model and an induction heating electronic circuit model have been studied to describe the whole mold heating system and they have been simulated using Simulink/MATLAB. In addition, two types of controllers, an induction preheating control technique based on a Model Predictive Controller (MPC), and another one based on PID, have been developed in order to analyse the different behaviour of the system.	3d modeling;cyber-physical system;electronic circuit;industry 4.0;interaction;matlab;pid;rotary woofer;simulation;simulink;smart objects;switched-mode power supply;system integration	Paolo Cicconi;Anna Costanza Russo;Michele Germani;Mariorosario Prist;Emanuele Pallotta;Andrea Monteriù	2017	2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI)	10.1109/RTSI.2017.8065972	heating system;cyber-physical system;pid controller;industry 4.0;induction heating;control theory;smart objects;manufacturing engineering;modular design;engineering	Robotics	-60.22416662723728	8.708859526495255	10065
1b80b3d3a8b69c99ef8887cb0664be10d62ac308	research on opc ua based on fdt/dtm and eddl	opc ua;software;control systems;server address space;client server architecture;eddl;client server systems;device integration opc ua fdt dtm eddl server address space;opc ua address space node fdt dtm eddl fieldbus device integration domain client server architecture;field buses;transport protocols;computer architecture;control system;servers;iec standards;monitoring;subscriptions;servers communication channels software control systems monitoring subscriptions computer architecture;communication channels;fdt dtm;device integration;industrial engineering;transport protocols client server systems field buses iec standards industrial engineering	Currently, there are two major technologies: FDT/ DTM and EDDL in the field bus device integration domain. But they are not consistent with each other, and lack of integration between two kinds of techniques. This paper describes that integrates these two technologies into OPC UA cLient/server architecture, and gives mapping of the DTM to the OPC UA address space node and EDD to OPC UA address space node. The method will not only help device manufacturers but also to users.	address space;client–server model;data access;fieldbus;opc unified architecture;open platform communications;powerflasher fdt;server (computing);user agent	Hongli Yuan;Feng Liu	2011	2011 Second International Conference on Digital Manufacturing & Automation	10.1109/ICDMA.2011.246	embedded system;real-time computing;engineering;operating system;opc data access	EDA	-35.45732163916835	46.45920161349991	10066
37f31915b345ed652242361f7c1189ff99f5da2b	proving termination of unfolding graph rewriting for general safe recursion		In this paper we present a new termination proof and complexity analysis of unfolding graph rewriting which is a specific kind of infinite graph rewriting expressing the general form of safe recursion. We introduce a termination order over sequences of terms together with an interpretation of term graphs into sequences of terms. Unfolding graph rewrite rules expressing general safe recursion can be successfully embedded into the termination order by the interpretation, yielding the polynomial runtime complexity. Moreover, generalising the definition of unfolding graph rewrite rules for general safe recursion, we propose a new criterion for the polynomial runtime complexity of infinite GRSs and for the polynomial size of normal forms in infinite GRSs.	analysis of algorithms;database normalization;embedded system;graph rewriting;net (polyhedron);newman's lemma;order by;polynomial;recursion;rewrite (programming);termination analysis;time complexity;unfolding (dsp implementation)	Naohi Eguchi	2014	CoRR		combinatorics;discrete mathematics;mathematics;algorithm;graph rewriting	Logic	-11.12493022180753	23.495119452857296	10069
7eedca0b49698d1b45ea5a06e7529ebd500e8638	finitely many-valued logics and natural deduction			natural deduction	Cécilia Englander;Edward Hermann Haeusler;Luiz Carlos Pereira	2014	Logic Journal of the IGPL	10.1093/jigpal/jzt032	monoidal t-norm logic;t-norm fuzzy logics	Logic	-12.2452342383099	12.600042422184528	10073
1789ae68ad34d429c9543d7d125e3770130a7d3c	conceptual simulation modeling of warehousing operations	conceptual simulation modeling;simulation modeling;simulation model;effective construction;key process;robust representation;general merchandize distribution center;warehousing operation;behavioral characteristic;conceptual modeling;conceptual model;warehousing	This study focuses on the simulation modeling of warehousing operations commonly seen at general merchandize distribution centers. The key processes and their structural and behavioral characteristics were identified and analyzed. Robust representations and patterns were developed to represent the elements and logic of the simulation models for these processes to facilitate efficient and effective construction of the models. The issues of conceptual modeling such as synthesis, abstraction and specialization were also discussed.	partial template specialization;simulation	Ming Zhou;Kitti Setavoraphan;Zhimin Chen	2005	Proceedings of the Winter Simulation Conference, 2005.		simulation;systems engineering;engineering;conceptual model;simulation modeling;data mining;warehouse	EDA	-59.978260923287856	12.92857168226202	10077
30b90342a549e767f3599d55dd49b88757fb72c2	recovery mechanisms for semantic web services	semantic web service;semantic annotation;owl s;recovery;web service;web services;semantic web	Web service-based applications are widely used, which has inevitably led to the need for proper mechanisms for the web service paradigm that can provide sustainable and reliable execution flows. In this paper we revise recovery techniques in OWL-S and show how semantic annotations may ensure seamless web service provision in a sophisticated way, such as, exploiting the ontology-based description of processes in order to dynamically find alternative services as substitutes for failed services. We also discuss the consequences of these semanticenabled approaches and point out required changes for integration in	owl-s;process specification;programming paradigm;seamless3d;semantic web service;sinewave synthesis;virtual machine	Kevin Wiesner;Roman Vaculín;Martin J. Kollingbaum;Katia P. Sycara	2008		10.1007/978-3-540-68642-2_8	web service;web application security;web development;web modeling;semantic web rule language;data web;web mapping;semantic grid;web standards;computer science;ws-policy;semantic web;web navigation;social semantic web;semantic web stack;database;web intelligence;web 2.0;law;world wide web;owl-s;information retrieval;semantic analytics	Web+IR	-44.25680132894515	12.472427216016973	10080
c736c883aba4c5c554f0c6c85a7fcdb77254b14a	embedding of quantified higher-order nominal modal logic into classical higher-order logic		In this paper, we present an embedding of higher-order nominal modal logic into classical higher-order logic and study its automation. There exists no automated theorem prover for first-order or higherorder nominal logic at the moment, hence, this is the first automation for this kind of logic. In our work, we focus on nominal tense logic and have successfully proven some first theorems.	automated theorem proving;experiment;first-order predicate;higher-order function;modal logic;temporal logic	Max Wisniewski;Alexander Steen	2014			modal logic;discrete mathematics;modal μ-calculus;embedding;mathematics;higher-order logic	Logic	-13.200152309638272	13.172928947206552	10091
4a871e5771a9a84d557cf1b8248fc88fc02f4137	a new effective approach for modelling and verification of security protocols		This paper presents a new mathematical model, and based on it a new automatic approach for veri cation of security protocols. This model contains a representation of the actions executed during the realization of the protocol, and the representation of changes of knowledge states of honest users and an Intruder. These possibilities are given to us by the formal de nition (also shown in the article) of the runs representing the execution of protocols in real, open computer networks. Our model uses a simple and intuitive idea of chains of states for which the concept of protocol's properties veri cation has been given. This idea has been implemented, and the results presented in this article are compared with the results obtained by the use of the best veri cation tools.	communications protocol;mathematical model;mathematical optimization	Olga Siedlecka-Lamch;Miroslaw Kurkowski;Henryk Piech	2012			intelligent verification;systems engineering;cryptographic protocol;computer science	Theory	-33.54993908683503	30.74733216190643	10125
cbbe246aa6b480f6b1fe00e7024d1cbb6a357c4b	avoiding non-termination when learning logical programs: a case study with foil and focl	learning process;search space;logic programs	Many systems that learn logic programs from examples adopt θ-subsumption as model of generalization and refer to Plotkin's framework in order to define their search space. However, they seldom take into account the fact that the lattice defined by Plotkin is a set of equivalence classes rather than simple clauses. This may lead to non-terminating learning processes, since the search gets stuck within an equivalence class, which contains an infinite number of clauses.	non-repudiation	Giovanni Semeraro;Floriana Esposito;Donato Malerba;Clifford Brunk;Michael J. Pazzani	1994		10.1007/3-540-58792-6_12	searching the conformational space for docking;computer science;theoretical computer science;machine learning;algorithm	NLP	-17.69415511922462	14.816981411710884	10128
1cd009b154ef184207c029c8c77c1ee4bce5a4f9	mobile cloud computing on delay tolerant network protocol	disaster information system;delay tolerant networking;delay tolerant networking disaster information system cloud computing system virtualization;disaster server resources mobile cloud computing delay tolerant network protocol geological condition japan island resilient oriented network infrastructure disaster information system;servers cloud computing monitoring information systems mobile communication protocols;mobile computing cloud computing delay tolerant networks disasters emergency management geology information systems;system virtualization;cloud computing	From the geological condition of Japan Island, many serious disasters such as earthquake, tsunami and typhoon occurred in history. A huge numbers of people, buildings and communication infrastructure are completely damaged. More resilient oriented network infrastructure that does not stop under challenged communication environment is strongly required. In this paper, we propose a mobile cloud computing system which can flexibly deal with failures of the network communication capability and information servers and respond to rapid change of information network condition. We construct the disaster information system based on the cloud computing technology. In addition, we develop a dynamic allocation of disaster server resources in accordance with the load change on the system in the disaster areas. Also, by introducing a mobile clouding with DTN protocol, our system can realize rapidly sharing disaster information even though the communication breakdown area occurred.	delay-tolerant networking;information system;memory management;mobile cloud computing;server (computing);typhoon	Yosuke Kikuchi;Yoshitaka Shibata	2015	2015 18th International Conference on Network-Based Information Systems	10.1109/NBiS.2015.35	cloud computing security;cloud computing;computer science;operating system;delay-tolerant networking;distributed computing;utility computing;computer security;computer network	HPC	-32.75078416230374	59.810030897146916	10149
81fd47c7501f0ae0dfaec0084eff5e5f99f5e118	supporting rescheduling using csp, rms and pob - an example application	dependence analysis;manufacturing industry;partial order	This paper introduces an innovative approach to the problem of rescheduling within manufacturing industry. An example of a manufacturing context that requires rescheduling capability is given (tyre production). The meaning of rescheduling, possible metrics for assessment of rescheduling and the advantages of applying the new techniques are reviewed. Of particular importance is the notion that the technology for providing rescheduling and explanation capabilities is to a large degree problem and context insensitive. The manner in which an original schedule has been created is irrelevant to the use of the technology described, allowing the advantages of the approach to be realized as an add-on facility to any existing scheduling system that fulfills a minimal set of requirements. These advantages are due to the use of a constraint based approach to new schedule creation used in tandem with dependency analysis techniques based on reason maintenance systems (de Kleer, 1986) and partial order backtracking (Ginsberg and McAllister, 1995; Spragg and Kelleher, 1996).	communicating sequential processes;record management services	Gerry Kelleher;Paolo Cavichiollo	2001	J. Intelligent Manufacturing	10.1023/A:1011267400804	partially ordered set;simulation;computer science;engineering;manufacturing;engineering drawing;dependence analysis	Robotics	-60.18716814064604	11.980382959652777	10160
b3f9a0160729b168e0d5d8c652b77a02f4fd62a8	a model-driven framework for domain specific process design and governance		Current BPM approaches and standards have not su ciently reduced the Business-IT gap. Indeed, today’s solutions are mostly domainindependent and platform-dependent, which limits the ability of business matter experts to express business intent and enact process change. In contrast, the tool presented in this paper supports an approach that focuses on BPM and SOA environments in a domain-dependent and platform-independent way. We propose to add a domain specific-layer on top of current solutions so that business stakeholders can design and understand their processes in a more intuitive way. This significantly improves the agility and governance of processes. The demo shows the appropriateness and the feasibility of the approach.	beam propagation method;model-driven integration;norm (social);service-oriented architecture	Adrian Mos;Mario Cortes Cornax;José Miguel Pérez-Álvarez;María Teresa Gómez López	2017			process design;business;systems engineering;corporate governance	SE	-57.66891160040843	19.075344390407828	10163
6427ac2629cc8480d7474c21027b342e23bb28b7	microfuge: a middleware approach to providing performance isolation in cloud storage systems	scheduling caching middleware performance isolation storage;caching;servers cloud computing load modeling admission control time factors resource management;scheduling;middleware;performance isolation;memcached microfuge cloud storage system cloud based services distributed caching scheduling middleware performance isolation adaptive deadline aware cache eviction load balancing policy distributed storage system;storage;middleware cache storage cloud computing	Most cloud providers improve resource utilization by having multiple tenants share the same resources. However, this comes at the cost of reduced isolation between tenants, which can lead to inconsistent and unpredictable performance. This performance variability is a significant impediment for tenants running services with strict latency deadlines. Providing predictable performance is particularly important for cloud storage systems. The storage system is the performance bottleneck for many cloud-based services and therefore often determines their overall performance characteristics. In this paper, we introduce MicroFuge, a new distributed caching and scheduling middleware that provides performance isolation for cloud storage systems. MicroFuge addresses the performance isolation problem by building an empirically-driven performance model of the underlying storage system based on measured data. Using this model, MicroFuge reduces deadline misses through adaptive deadline-aware cache eviction, scheduling and load-balancing policies. MicroFuge can also perform early rejection of requests that are unlikely to make their deadlines. Using workloads from the YCSB benchmark on an EC2 deployment, we show that adding MicroFuge to the storage stack substantially reduces the deadline miss rate of a distributed storage system compared to using a deadline oblivious distributed caching middleware such as Memcached.	application programming interface;benchmark (computing);cpu cache;cache (computing);cloud computing;cloud storage;clustered file system;computer data storage;dls format;distributed cache;heart rate variability;load balancing (computing);memcached;middleware;multitenancy;rejection sampling;requirement;scheduling (computing);software deployment;software performance testing;technical standard;ycsb	Akshay K. Singh;Xu Cui;Benjamin Cassell;Bernard Wong;Khuzaima Daudjee	2014	2014 IEEE 34th International Conference on Distributed Computing Systems	10.1109/ICDCS.2014.58	real-time computing;computer science;operating system;middleware;distributed computing;scheduling;computer network	HPC	-22.359297579301778	60.09023974162586	10172
f974efcbf7d8673f334d1da23c13131bdbddbbac	vectorized higher order finite difference kernels	paper;nvidia geforce gtx 680;ati radeon hd 6990;tesla c2050;ati radeon hd 7970;performance;ati;finite difference;nvidia geforce gtx 590;nvidia;algorithms;tesla k20;computer science;opencl	Several highly optimized implementations of Finite Difference schemes are discussed. The combination of vectorization and an interleaved data layout, spatial and temporal loop tiling algorithms, loop unrolling, and parameter tuning lead to efficient computational kernels in one to three spatial dimensions, truncation errors of order two to twelve, and isotropic and compact anisotropic stencils. The kernels are implemented on and tuned for several processor architectures like recent Intel Sandy Bridge, Ivy Bridge and AMD Bulldozer CPU cores, all with AVX vector instructions as well as Nvidia Kepler and Fermi and AMD Southern and Northern Islands GPU architectures, as well as some older architectures for comparison. The kernels are either based on a cache aware spatial loop or on time-slicing to compute several time steps at once. Furthermore, vector components can either be independent, grouped in short vectors of SSE, AVX or GPU warp size or in larger virtual vectors with explicit synchronization. The optimal choice of the algorithm and its parameters depend both on the Finite Difference stencil and on the processor architecture.	adaptive mesh refinement;advanced vector extensions;algorithm;automatic vectorization;bulldozer (microarchitecture);cas latency;cpu cache;cuda;central processing unit;coefficient;computer data storage;data access;fermi (microarchitecture);finite difference;graphics processing unit;ivy bridge (microarchitecture);kepler (microarchitecture);loop unrolling;mathematical optimization;multigrid method;opencl api;preemption (computing);radeon hd 6000 series;refinement (computing);sandy bridge;streaming simd extensions;tiling window manager;time slicing (digital broadcasting);truncation;x86	Gerhard W. Zumbusch	2012		10.1007/978-3-642-36803-5_25	finite difference;parallel computing;computer hardware;performance;computer science;operating system;computer graphics (images)	HPC	-5.336291291430949	39.71622485474792	10176
5aed922638f6b9e758cfa80fae023903c5e7700c	sql markup language for enterprise integration	electronic commerce;software integration;sql;distributed computing;database;markup languages web services xml distributed computing information analysis distributed databases relational databases cities and towns educational institutions computer science;web service;integrated software sql electronic commerce internet xml middleware relational databases;service model;internet;enterprise integration;web services;integrated software;xml;middleware;eai;relational databases;xml sql enterprise integration e business distributed computing universal web services model software integration markup language;markup language;open standard;business process	With the advent of dynamic e-business and its open standards-based supporting technologies, valuable legacy applications that support essential business processes in enterprises can join this new area of distributed computing. We introduce a framework for enterprise integration with universal Web services model. We propose an SQL markup language based on the Software Integration Markup Language (SIML), which is an XML based markup language designed to EAI.	business process;distributed computing;electronic business;enterprise application integration;enterprise integration;markup language;sql;xml	Yu-Jie Xu;Min-Hua Shi	2004	IEEE International Conference onServices Computing, 2004. (SCC 2004). Proceedings. 2004	10.1109/SCC.2004.1358032	ruleml;data exchange;xhtml;business intelligence markup language;semantic web rule language;synchronized multimedia integration language;html;collaborative application markup language;computer science;document definition markup language;data mining;pcdata;database;wireless markup language;world wide web;universal description discovery and integration;enterprise information integration;sgml	DB	-48.26208058253124	13.494779643027623	10182
51e5bd52b8be64a759dc4c39e2de0d044e764f47	design and implementation of an inter-process communication model for an embedded distributed processing network	model design;conference;distributed processing;communicating sequential process;communication model;embedded system;artificial intelligent;operating system;design and implementation;communication protocol;source code;institutional repository research archive oaister;embedded processor;inter process communication	A network of embedded processors has been used as the platform for a distributed blackboard system, which is an artificial intelligence (AI) technique. A distributed blackboard system called DARBS had been created for a PC-based Linux operating system using the TCP/IP communication protocol. The Nottingham Trent University had developed an embedded distributed processing network called SARNets that can be used to run an embedded version of DARBS (emDARBS). SARNet uses the SARNUX operating system that uses the communicating sequential process (CSP) communication model. Because this communication model is different from that of the Linux operating system, a new InterProcess Communication model was required that would emulate Linuxs communication model on SARNUX. This paper focuses on the development of an Inter-Process Communication model designed to allow DARBS to run on the SARNet with minimum changes to the original DARBS source code.	artificial intelligence;blackboard system;central processing unit;communications protocol;concurrent computing;distributed computing;embedded system;inter-process communication;internet protocol suite;linux;operating system;parallel computing;server (computing);windows legacy audio components	Kum Wah Choy;Adrian A. Hopgood;Lars Nolle;Brian C. O'Neill	2003			embedded system;embedded operating system;real-time computing;computer science;distributed computing	Embedded	-30.313457579820398	41.31826542898945	10185
44f76e73c332dc343ecb92fabeb883deb203af8d	understanding natural language requirement descriptions for telecommunication services	natural language interfaces;expression meanings natural language requirement descriptions telecommunication services system development requirements analysis description viewpoints terminal operations requirement description method orthogonal viewpoints interface viewpoint telecommunication service concepts;description viewpoint;telecommunication computing;natural language interfaces telecommunication services telecommunication computing;natural language;telecommunication services;system development;natural languages telecommunication services laboratories formal languages intelligent networks productivity formal specifications target recognition pressing;telecommunication service;concept recognition;requirement description;quick response	Demands on telecommunication services will increase as today's society evolves. A quick response to these demands will require clarifying what is truly needed for services at an early stage of system development. It is therefore important to allow users to define these demands. Natural language is suitable for describing requirements. However, diverse expressions are allowed in natural language, and users have many description viewpoints. The main issue is discerning the meanings behind the various expressions, and the paper aims to resolve that point. The paper creates a model for telecommunication services from the viewpoint of their functions and terminal operations and prescribes a requirement description method. The paper also reveals that the diversity in expressions is caused by a set of three orthogonal viewpoints: an interface viewpoint from among three entities, i.e., the terminal, network and user; a viewpoint from both sides of each interface; and a logical and physical viewpoint. It then systematizes the telecommunication service concepts based on the model. Finally, it proposes a method for understanding expression meanings by managing the correspondence between concepts and expressions.		Yoshizumi Kobayashi;Hiroshi Enoki;Tadashi Ohta	1995		10.1109/TAI.1995.479617	computer science;knowledge management;artificial intelligence;telecommunications service	AI	-49.22761813186424	14.767493280998861	10193
acdf44a8696e6ba06ed7caa744c5f990865fc50c	sps: a middleware for multi-user sensor systems	sensor system;distributed processing;states;dynamic system;multi user;sensor network;sensor networks;publish subscribe;middleware	With the increased realisation of the benefits of studying environmental data, sensor networks are rapidly scaling in size, heterogeneity of data, and applications. In this paper, we present a State-based Publish/Subscribe (SPS) framework for sensor systems with many distributed and independent application clients. SPS provides a state-based information deduction model that is suited to many classes of sensor network applications. State Maintenance Components (SMCs) are introduced that are simple in operation, flexible in placement, and decomposable for distributed processing. Publish/Subscribe communication forms the core messaging component of the framework. SPS uses the decoupling feature of Pub/Sub and extends this across the SMCs to support a more flexible and dynamic system structure. Our evaluation, using real sensor data, shows that SPS is expressive in capturing conditions, and scalable in performance.	coupling (computer programming);distributed computing;dynamical system;ibm 1401 symbolic programming system;image scaling;middleware;multi-user;natural deduction;programming paradigm;publish–subscribe pattern;scalability;sensor	Salman Taherian;Jean Bacon	2007		10.1145/1376866.1376870	sensor web;real-time computing;computer science;database;distributed computing;key distribution in wireless sensor networks;visual sensor network	Mobile	-31.039115419576547	45.817443627974825	10195
a98954b9593118dcb299aad241b71b52898a7833	consearch: using hypertext contexts as web search boundaries	support construction;extensible markup language;recherche internet;hipertexto;standards;red www;resource description framework;appui;prototipo;internet search;internet;norma;allgemeine werke;000 informatik;world wide web;web search;reseau www;apoyo;prototype;hypertexte;norme;hypertext;informationswissenschaft	This paper describes a schema for searching on the Web by making use of hypertext contexts that can be represented with new Web standards as search boundaries and argues that this would help to provide users more accurate results in their searches concerning specific topics or subject domains. It also proposes several issues to be addressed in making the schema applicable and presents a prototype system that addresses these issues and supports hypertext context-based search. This work reveals a great potential to make use of the structural and semantic information that can be represented with the new Web standards efficiently for search purposes.	database schema;hypertext;prototype;web search engine;web standards;world wide web	Zhanzi Qiu;Matthias Hemmje;Erich J. Neuhold	2000		10.1007/10722620_4	the internet;xml;hypertext;computer science;artificial intelligence;web navigation;rdf;database;prototype;world wide web;information retrieval	Web+IR	-37.750030303905106	10.989859593798172	10200
b9955cadb421d90a4f8100466479531e662a7524	dependency-preserving normalization of relational and xml data	dependency preservation;relational data;redundancy elimination;information content;hierarchical representation;normalization;xml;normal form;xml document;database design;information theoretic	(⇒) Suppose (R, Σ) is in 3NF and I ∈ inst(R, Σ). Having INFI(p|Σ) < 1 for some p = (R, t, A) in Pos(I) means that there is redundant information in position p. Since we assume Σ only contains FDs, there must be an FD X → A ∈ Σ and a different tuple t in I, such that t[X] = t[X] and therefore t[A] = t[A]. This can only happen when X is not a key. Thus, A is a prime attribute. (⇐) Assume that there is an FD X → A ∈ Σ, such that X is not a key for R and A is not prime. We show that there is an instance I ∈ inst(R, Σ) and position p ∈ Pos(I) such that INFI(p|Σ) < 1. Let I be an instance of (R, Σ) containing two tuples t1, t2 defined as follows. For every B ∈ sort(R), t1[B] = 1. If B ∈ X, t2[B] = 1, otherwise t2[B] = 2. It is easy to see that I satisfies Σ, and for position p = (R, t1, A) we have INFI(p|Σ) < 1. This contradicts the assumption that for every non-prime attribute A and position p = (R, t, A), we have INFI(p|Σ) = 1.	candidate key;key (cryptography);third normal form;xml	Solmaz Kolahi	2007	J. Comput. Syst. Sci.	10.1016/j.jcss.2006.10.014	xml validation;xml;computer science;data mining;xml schema;database;information retrieval;multivalued dependency	Theory	-26.33348235120367	9.879317663007786	10215
901c03dca79133684bbb5f359d68fc776faa701a	asserting the utility of co2p3s using the cowichan problem set	distributed memory;arbre recherche;patron conception;cowichan problems;programming environments;shared memory;programming environment;programacion paralela;structure arborescente;systeme programmation parallele co2p3s;patron concepcion;parallel programming;medio ambiente programacion;search trees;performance programme;metamodel;search tree pattern;metamodele;arbol investigacion;estructura arborescente;metamodelo;parallel programming environment;tree structure;fifteen puzzle;design pattern;co2p3s;eficacia programa;design patterns;program performance;probleme cowichan;search tree;parallel programs;parallel applications;environnement programmation;programmation parallele	Parallel programming environments provide a way for programmers to reap the benefits of parallelism, while reducing the effort required to create parallel applications. The CO2P3S parallel programming system is one such tool that uses a pattern-based approach to express concurrency. Using the Cowichan Problems, we demonstrate that CO2P3S contains a rich set of parallel patterns for implementing a wide variety of applications running on shared-memory or distributed-memory hardware. An example of these parallel patterns, the Search-Tree pattern, is described and it is shown how the pattern was used to solve the Fifteen Puzzle problem. Code metrics and performance results are presented for the Cowichan applications to show the usability of the CO2P3S system and its ability to reduce programming effort, while producing programs with reasonable performance. © 2005 Elsevier Inc. All rights reserved.	15 puzzle;acoustic lobing;admissible numbering;automatic parallelization;bro;chart parser;dec alpha;design pattern;dini derivative;distributed computing;doug lea;duane's hypothesis;extensibility;h tree;han unification;hierarchical editing language for macromolecules;hyperbolic absolute risk aversion;immutable object;international federation for information processing;ishikawa diagram;iterative deepening depth-first search;iterative method;java;john vlissides;metaprogramming;naruto shippuden: clash of ninja revolution 3;os-tan;overhead (computing);pc bruno;parallel computing;pattern language;programming tool;self-balancing binary search tree;simulation;solver;sorting;thinning;together;tree-depth;turing;usability;wavefront .obj file	John Anvik;Jonathan Schaeffer;Duane Szafron;Kai Tan	2005	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2005.05.029	metamodeling;shared memory;software design pattern;parallel computing;simulation;distributed memory;computer science;theoretical computer science;operating system;distributed computing;design pattern;tree structure;search tree;programming language;algorithm	HPC	-14.738389428845526	35.219004010664065	10237
5fdd76a45bea33cb1c41f29d75963b53d215cc21	task-parallel global optimization with application to protein folding	biology computing;optimisation;programming environments;approximate algorithm;optimization proteins parallel processing newton method approximation algorithms approximation methods multicore processing;software system;approximation algorithms;programming environment;approximation method;multicore cluster task parallel global optimization software framework high performance numerical global optimization run time library programming environment irregular task based parallelism adaptive task based parallelism multilevel parallelism global optimization application numerical differentiation newton based local optimizations parallelization protein folding problem experimental evaluation software system;software systems;parallel programming;newton based local optimizations;protein folding task parallelism cluster programming numerical differentiation global optimization;irregular task based parallelism;numerical differentiation;task parallel global optimization;differentiation;proteins;high performance numerical global optimization;multicore processing;parallelization;software framework;protein folding;global optimization;optimization;newton method;approximation methods;multiprocessing systems;experimental evaluation;task parallelism;cluster programming;multilevel parallelism;high performance;global optimization application;proteins biology computing differentiation multiprocessing systems optimisation parallel programming programming environments;adaptive task based parallelism;parallel processing;run time library;multicore cluster;protein folding problem	This paper presents a software framework for high performance numerical global optimization. At the core, a runtime library implements a programming environment for irregular and adaptive task-based parallelism. Building on this, we extract and exploit the multilevel parallelism of a global optimization application that is based on numerical differentiation and Newton-based local optimizations. Our framework is used in the efficient parallelization of a real application case that concerns the protein folding problem. The experimental evaluation presents performance results of our software system on a multicore cluster.	dhrystone;general-purpose computing on graphics processing units;global optimization;gradient;hessian;integrated development environment;linear programming;mathematical optimization;multi-core processor;newton;nonlinear programming;nonlinear system;numerical analysis;numerical differentiation;parallel computing;protein structure prediction;runtime library;scalability;software framework;software system	Constantinos Voglis;Panagiotis E. Hadjidoukas;Vassilios V. Dimakopoulos;Isaac E. Lagaris;Dimitris G. Papageorgiou	2011	2011 International Conference on High Performance Computing & Simulation	10.1109/HPCSim.2011.5999823	protein folding;parallel processing;computer architecture;parallel computing;computer science;theoretical computer science;task parallelism;global optimization;software system	HPC	-9.311485624225615	38.28189650343265	10246
6a8b0726f03807779603ac8ed666fece2b564906	building self-configuring data centers with cross layer coevolution	evolvable network systems;index terms—autonomic self-configuring network systems;biologically-inspired networking;coevolution;middleware;environmental change;data center;indexing terms	This paper describes a biologically-inspired architecture, called SymbioticSphere, which allows data centers to autonomously adapt to dynamic environmental changes. SymbioticSphere follows biological principles such as decentralization, evolution and symbiosis to design application services and middleware platforms in a data center. Each service and platform is designed as a biological entity, and implements biological behaviors such as energy exchange, migration, reproduction and death. Each service/platform also possesses behavior policies, as genes, each of which defines when to and how to invoke a particular behavior. This paper presents a set of behaviors for services and platforms, and describes how services and platforms act and interact with each other. Simulation results show that services and platforms autonomously adapt to dynamic network conditions (e.g., user location, network traffic and resource availability) by evolving their behavior policies across generations. Simulation results also show that services and platforms coevolve to improve their adaptability by adjusting their behavior policies cooperatively.	data center;middleware;network traffic control;simulation	Paskorn Champrasert;Junichi Suzuki	2007	JSW		data center;real-time computing;simulation;index term;environmental change;coevolution;computer science;artificial intelligence;operating system;middleware;database;distributed computing;computer security;computer network	Metrics	-38.565989766951574	43.79795722742298	10253
c178b7ebd842adff649477706ca8cb79c68e4ab3	single-pass testing automata for ltl model checking		Testing Automaton (TA) is a new kind of ω-automaton introduced by Hansen et al. [6] as an alternative to the standard Büchi Automata (BA) for the verification of stutter-invariant LTL properties. Geldenhuys and Hansen [5] shown later how to use TA in the automata-theoretic approach to LTL model checking. They propose a TA-based approach using a verification algorithm that requires two searches (two passes) and compare its performance against the BA approach. This paper improves their work by proposing a transformation of TA into a normal form (STA) that only requires a single one-pass verification algorithm. The resulting automaton is called Single-pass Testing Automaton (STA). We have implemented the STA approach in Spot model checking library. We are thus able to compare it with the “traditional” BA and TA approaches. These experiments show that STA compete well on our examples.	a-normal form;algorithm;benchmark (computing);business architecture;deadlock;experiment;formal verification;generalized büchi automaton;model checking;ω-automaton	Ala-Eddine Ben Salem	2015		10.1007/978-3-319-15579-1_44	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-12.709294506664259	24.91894810572874	10256
01ae10e4e7b6afb914847c34e2875a47a79245b8	service level aware - contract management	topology;measurement;contracts monitoring topology engines quality of service measurement;rule based system;contracts;engines;monitoring;sla design;trees mathematics cloud computing contracts directed graphs knowledge based systems resource allocation specification languages;directed tree graph;rule based engine service level aware contract management cloud computing service ccs computational resource allocation service level agreement wsla sla description language directed tree graph;quality of service;rule based system sla management sla measurement sla composition sla design directed tree graph;sla measurement;sla composition;sla management	The success achieved by Cloud Computing Services (CCSs) in modern IT scenarios is nowadays a matter of fact and service offerings between providers and customers continuously grow up and widen their scope. Similarly, service offerings are more and more based upon dynamic reservation and allocation of network, storage and computational resources, the hiding of visibility of internal IT components, as well as the pay-per-use paradigm. The main drawback of such a situation is represented by the complexity in composing services to satisfy users' requests, as well as in performance monitoring and service level comparisons. Effective strategies are then needed to model IT service contracts and corresponding Service Level Agreements (SLAs), as well as their composition. However, the lack of expressivity in current SLA specifications and the inadequacy of tools for managing SLA and contract compositions is relevant. Therefore, we present a possible extension of WSLA, a widely known SLA description language, for modeling contracts and SLAs suitable to support contract owners during service composition and monitoring phases. An ad-hoc developed tool based on the usage of directed tree-graphs and of a rule-based engine is examined to assess the feasibility of the proposed model and to simplify SLA and contract composition.	cloud computing;complexity;computational resource;contract management;directed graph;hoc (programming language);human-readable medium;logic programming;mean time between failures;mean time to repair;nosql;performance engineering;programming paradigm;responsiveness;service composability principle;service-level agreement;xml	Antonella Longo;Marco Zappatore;Mario A. Bochicchio	2015	2015 IEEE International Conference on Services Computing	10.1109/SCC.2015.74	service level objective;contract management;computer science;database;distributed computing;world wide web	DB	-47.4464990377588	42.504845057209835	10257
09dda7581beb7d4e1df68e3d756ae8bcaf319895	clone detection in uml class models using class metrics	clone detection	This paper presents a technique to detect clones in UML class models. Class metrics (number of attributes, number of operations) of a class, class attribute names, root nodes, child nodes and class method names are compared with corresponding metrics, attribute names, root node, child nodes and method names of another class. Based on the number of matched attributes, operations and metrics, a percentage of cloning is calculated. We declare two classes as clones of each other if the matching percentage of the class metrics, class attribute names and class method names is greater than a specific threshold.	duplicate code;html attribute;method (computer programming);tree (data structure);unified modeling language	Satwinder Singh;Raminder Kaur	2014	ACM SIGSOFT Software Engineering Notes	10.1145/2597716.2597726	class variable;real-time computing;computer science;data mining;database	SE	-57.1431404935993	35.96137097309267	10260
ef016e78a6b9721bb5247358f17eee12282dbb58	european law databases: an experiment in retrieval	databases;database storage format;qa 76 software computer programming;database indexing engine;document retriever;document handling;retrieved documents;document selector;information retrieval;novel retrieval methods;web;hypertext markup language;graphical user interface;law;single instruction multiple data;hypermedia;qa 75 electronic computers computer science;indexes;legal factors;law administration;graphical user interfaces;engines;legal documents;indexing;dap parallel processor;indexation;graphical user interfaces law administration information retrieval document handling bibliographic systems hypermedia indexing parallel machines full text databases;massively parallel single instruction multiple data;retrieval engine;graphic user interface;user queries;parallel machines;full text databases;bibliographic systems;object server;document retrieval;european law databases;dap 610c;web european law databases novel retrieval methods legal documents dap parallel processor document selector daptext index server retrieval engine database indexing engine dap 610c massively parallel single instruction multiple data object server document retriever full text graphical user interface user queries retrieved documents database storage format hypertext markup language;full text;digital audio players;databases information retrieval engines law digital audio players graphical user interfaces legal factors indexes indexing parallel processing;markup language;parallel processing;daptext index server	The project presented has looked at novel retrieval methods for a variety of legal documents using the DAP parallel processor. The system has three main components: the document selector-this may be either the DAPText index server or any other suitable retrieval engine (DAPText is a database indexing engine from Cambridge Parallel Processing which runs on the DAP 610C, a massively parallel Single Instruction Multiple Data (SIMD) processor; the object server or document retriever that returns the full text of selected documents; and the graphical user interface (GUI) that manages the interactions-for example, it translates the user queries into a form appropriate for the retrieval engine being used and converts the retrieved documents from the database storage format into Hypertext Markup Language (HTML) for the Web.	database;icl distributed array processor;parallel computing	Philip J. O'Shea;Eve Wilson	1997		10.1109/DEXA.1997.617275	document retrieval;parallel processing;computer science;data mining;graphical user interface;database;programming language;world wide web;information retrieval	DB	-34.70099063074877	7.424581133462125	10265
07d47884a231fa8bf4c8b2e44f20584a7912227a	computer aided tolerancing using positioning features	manufacturing;design;dimensions		global positioning system	Bernard Anselmetti;Kwamiwi Mawussi	2003	J. Comput. Inf. Sci. Eng.		design;mechanical engineering technology;systems engineering;engineering;marketing;manufacturing;management;engineering drawing;manufacturing engineering;mechanical engineering	DB	-59.15979537090429	8.017480188012971	10276
1d3d75d62fc5af1364fcfabaca390721bc1a210e	novel game theoretic frameworks for security risk assessment in cloud environments				Louai Maghrabi	2017			risk analysis (engineering);cloud computing;risk assessment;business	Security	-56.59379827573123	49.51437622400662	10300
1e4dcd43927e54d712c1fadcc3f775586920604d	peer-to-peer collaboration over xml documents	collaborative editing;peer to peer collaboration;xml;xml document;operational transformation;peer to peer	Existing solutions for the collaboration over XML documents are limited to a centralised architecture. In this paper we propose an approach for peer-to-peer collaboration over XML documents where users can work off-line on their document replica and synchronise in an ad-hoc manner with other users. Our algorithm for maintaining consistency over XML documents recursively applies the tombstone operational transformation approach over the document levels.	acm transactions on computer-human interaction;algorithm;centralisation;concurrency (computer science);entity–relationship model;hoc (programming language);human–computer interaction;multi-user;online and offline;operational transformation;peer-to-peer;quantum state;real-time transcription;recursion;sensor;uml state machine;xml	Claudia-Lavinia Ignat;Gérald Oster	2008		10.1007/978-3-540-88011-0_9	well-formed document;xml catalog;xml validation;binary xml;xml base;simple api for xml;xml;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;xml database;database;xml signature;programming language;world wide web;xml schema editor;cxml;information retrieval;efficient xml interchange	DB	-26.142424867111558	47.68365012806652	10323
693626c191dc4362ef04d9a4f9b6cc90a651bf9f	adopting a software component model in real-time systems development	component based software engineering;network operating systems;real time;real time systems communication system control software engineering industrial control predictive models application software java embedded system automation automatic control;object oriented programming;software engineering;net model software component model real time systems development component based software engineering desktop software server side software microsoft com model dcom model;engineering and technology;teknik och teknologier;network operating systems software engineering object oriented programming real time systems;industrial control;software component;component model;real time systems	Component-based software engineering (CBSE) and the use of (de-facto) standard component models have gained popularity in recent years, particularly in the development of desktop and server-side software. This paper presents a motivation for applying CBSE to realtime systems and discusses the consequences of adopting a software component model in the development of such systems. Specifically, the consequences of adopting Microsoft’s COM, DCOM, and .NET models are analyzed. The most important aspects of these models are discussed in an incremental fashion. The analysis considers both real-time systems in general, and a reallife industrial control system where some aspects the COM model have been adopted. It is concluded that adopting these models makes it possible to meet real-time requirements, but that some overhead must be expected and that special precautions may have to be taken to prevent loss of real-time predictability.	component-based software engineering;control system;desktop computer;distributed component object model;overhead (computing);real-time clock;real-time computing;real-time transcription;requirement;server (computing);server-side	Frank Lüders	2003		10.1109/SEW.2003.1270733	personal software process;verification and validation;real-time computing;software sizing;software verification;computer science;systems engineering;package development process;backporting;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;real-time control system software;systems development life cycle;programming language;software analytics;resource-oriented architecture;software measurement;software deployment;software development process;software requirements;software system	Embedded	-49.13741012017319	30.788727822357703	10326
6508597e7794d70d834b4fa7566008adf00d0536	visualizing the java heap demonstration proposal	pediatrics;memory management;history;information extraction;data mining;data visualisation;visualization;image color analysis;java data visualisation digital storage;java system heap model information extraction memory visualization memory related problems;memory related problems;memory visualization;digital storage;system heap model;visualization java proposals programming profession data mining computer science displays data structures least squares approximation quadratic programming;java	Many of the problems that occur in long-running systems involve the way that the system uses memory. We have developed a framework for extracting and building a model of the heap from a running Java system. Such a model is only useful if the programmer can extract from it the information they need to understand, find, and eventually fix memory-related problems in their system. We propose to demonstrate the tool in action, showing how it works dynamically on running processes and how it is designed to address a variety of specific memory issues.	java;programmer	Steven P. Reiss	2009	2009 IEEE International Conference on Software Maintenance	10.1109/ICSM.2009.5306287	visualization;computer science;theoretical computer science;database;programming language;java;information extraction;memory management	SE	-30.974453836131893	24.758616118049744	10331
77dfea58a38ee407842ffb82ba921070aa829f38	a correspondence between martin-löf type theory, the ramified theory of types and pure type systems	nuprl;types;pure type systems;orders;type theory	In Russell’s Ramified Theory of Types RTT, two hierarchical concepts dominate: orders and types. The use of orders has as a consequence that the logic part of RTT is predicative. The concept of order however, is almost dead since Ramsey eliminated it from RTT. This is why we find Church’s simple theory of types (which uses the type concept without the order one) at the bottom of the Barendregt Cube rather than RTT. Despite the disappearance of orders which have a strong correlation with predicativity, predicative logic still plays an influential role in Computer Science. An important example is the proof checker Nuprl, which is based on Martin-Löf’s Type Theory which uses type universes. Those type universes, and also degrees of expressions in A UTOMATH, are closely related to orders. In this paper, we show that orders have not disappeared from modern logic and computer science, rather, orders play a crucial role in understanding the hierarchy of modern systems. In order to achieve our goal, we concentrate on a subsystem of Nuprl. The novelty of our paper lies in: (1) a modest revival of Russell’s orders, ?? (2) the placing of the historical systemRTT underlying the famous Principia Mathematica in a context with a modern system of computer mathematics (Nuprl) and modern type theories (Martin-Löf’s type theory and PTSs), and (3) the presentation of a complex type system (Nuprl) as a simple and compact PTS.	automated proof checking;computational science;computer science;history of type theory;impredicativity;klee–minty cube;nuprl;pure type system;wolfram mathematica	Fairouz Kamareddine;Twan Laan	2001	Journal of Logic, Language and Information	10.1023/A:1011286100450	philosophy;epistemology;computer science;artificial intelligence;pure mathematics;mathematics;order;programming language;type theory;algorithm	Logic	-8.986158795730033	10.802296910344124	10351
66b2a056d3ff78ba13ac4cc33313ded744ada46e	propagating epistemic coordination through mutual defaults i	speech acts;common ground;dynamic environment;indexation	A mutual default is a rule, capable of tolerating exceptions, that is mutually supposed by a group G: i.e., the rule is supposed by all members of the group, is supposed by all members of the group to be supposed by all members of the group, etc. A family of propositional attitudes Bi indexed for i C G (and representing, say, supposition) is coordinated for G if Bi applies to the same propositions for all members i of G, and is commonly supposed by the members of G to do so. This paper is a preliminary exploration of formal postulates that ensure maintenance of coordination of a propositional attitude, representing the common ground of a conversation, in dynamic environments that allow for assertional speech acts. I present results showing that mutually supposed rules of conversation provide a mechanism for preserving coordination. If coordination can be assumed, reasoning about propositional attitudes can be greatly simplified, through collapse of iterated operators. I also show how coordination maintenance can be secured, at least in unexceptional cases, when rules of conversation are defeasible; this relaxation of the theory is needed because plausible conversational rules are subject to exceptions. The project of formalizing coordination maintenance using Circumscription Theory raises some interesting technical problems; to provide a finitely axiomatized theory of coordination, it is apparently necessary to quantify over intensional types of at least third order. For this purpose, Richard Montague's Intensional Logic seems to be an appropriate vehicle.	circumscription (logic);default logic;defeasible reasoning;indexed language;intensional logic;iteration;linear programming relaxation;montague grammar	Richmond H. Thomason	1990			epistemology;computer science;artificial intelligence;mathematics;algorithm	AI	-16.898952982109336	4.449083685440532	10366
af818ec7b6f9a65252163095e9d23a022b419e1e	dynamic logics of the region-based theory of discrete spaces	dynamic model;dynamic logic;modal logic;propositional dynamic logic;spatial logic	The aim of this paper is to give new kinds of modal logics suitable for reasoning about regions in discrete spaces. We call them dynamic logics of the region-based theory of discrete spaces. These modal logics are linguistic restrictions of propositional dynamic logic with the global diamond E. Their formulas are equivalent to Boolean combinations of modal formulas like E(A ∧ 〈α〉B) where A and B are Boolean terms and α is a relational term. Examining what we can say about dynamic models when we use formulas to describe them, we successively address the axiomatization/completeness issue and the decidability/complexity issue of our dynamic logics of the region-based theory of discrete spaces.	axiomatic system;boolean algebra;dynamic logic (modal logic);modal logic	Philippe Balbiani;Tinko Tinchev;Dimiter Vakarelov	2007	Journal of Applied Non-Classical Logics	10.3166/jancl.17.39-61	modal logic;dynamic logic;zeroth-order logic;t-norm fuzzy logics;normal modal logic;modal μ-calculus;discrete mathematics;classical logic;linear temporal logic;description logic;higher-order logic;philosophy;epistemology;computer science;intermediate logic;theoretical computer science;mathematics;accessibility relation;substructural logic;multimodal logic;algorithm;autoepistemic logic	Logic	-14.010825816734279	12.646875435092387	10370
ec20f7957afc3be46def23b45d7cb1e068f003f1	quality of service aggregation in e-business applications	quality of service time factors business computational modeling availability probabilistic logic;availability;computational modeling;time factors;business;the choquet integral qos aggregation e business applications satisfaction degree measurement;probabilistic logic;quality of service	In e-business applications, enterprises build their processes to achieve their business goals. One of the architectural models of e-business applications is a service-based approach. This approach consists in orchestrating the e-services offered by one or several enterprises partners in order to build the desired business processes. It is important for the enterprises to ensure client satisfaction in order to be more attractive and more competitive. Quality of Service has a significant impact on client satisfaction. Therefore, clients need e-business applications with high Quality of Service to be satisfied. In this context, we propose in this paper an approach that allows clients to measure the satisfaction degree of the services orchestration. This approach takes into account client's preferences on QoS attributes and their related dependencies in the measurement of the satisfaction degree. We treat two examples of services orchestration and show how does the measured satisfaction allow the client to choose the best one.	business process;business software;e-services;electronic business;manufacturing execution system;olami–feder–christensen model;quality of service;software quality;workflow pattern	Nabil Fakhfakh;Frédéric Pourraz;Hervé Verjus	2011	Proceedings of the International Conference on e-Business		availability;quality of service;computer science;knowledge management;marketing;operations management;database;probabilistic logic;computational model;computer security	SE	-48.301874325611195	43.08908191498218	10379
1a6c49e79d778f022b9b745c60ca442cbd7caa49	multiprocessing of the time domain analysis of thin-wire antennas and scatterers	sparse linear system time domain analysis thin wire antennas scatterer numerical method electric field integral equation distributed memory multiprocessor parallel programming mpi;distributed memory;electric field integral equation;distributed memory systems;numerical method;parallel programming time domain analysis message passing numerical analysis distributed memory systems electric field integral equations;parallel programming;time domain analysis integral equations electromagnetic scattering electromagnetic analysis signal analysis electromagnetic fields parallel programming libraries concurrent computing distributed computing;time domain analysis;data partitioning;numerical analysis;electric field integral equations;relative efficiency;message passing;parallel implementation;memory hierarchy;portable extensible toolkit for scientific computation;parallel programs;sparse linear system	We deal with the computational aspects of a numerical method for solving the electric field integral equation (EFIE) for the analysis of the interaction of electromagnetic signals with thin-wires structures. Our interest is mainly to device an efficient parallel implementation of this numerical method which helps physicist to solve the electric field integral equation for very complex and large thin-wires structures The development of this parallel implementation has been carried out on distributed memory multiprocessors, with the use of the parallel programming library MPI and routines of PETSc (portable, extensible toolkit for scientific computation). These routines can solve sparse linear systems in parallel. Appropriate data partitions have been designed in order to optimize the performance of the parallel implementation. A parameter named relative efficiency has been defined to compare two parallel executions with different number of processors. This parameter allows us to better describe the superlinear performance behavior of our parallel implementation. Evaluation of the parallel implementation is given in terms of the values of the speed-up and the relative efficiency. Moreover, a discussion about the requirements of memory versus the number of processors is included. It will be shown that memory hierarchy management improves substantially as the number of processors increases and that this is the reason why superlinear speed-up is obtained.	access time;algorithm;central processing unit;computation;computational science;data access;distributed memory;domain analysis;flash memory;high-level programming language;library (computing);linear system;memory hierarchy;message passing interface;multiprocessing;numerical method;petsc;parallel computing;requirement;run time (program lifecycle phase);sparse matrix;speedup	Ester M. Garzón;Siham Tabik;Inmaculada García;Amelia Rubio Bretones	2004	12th Euromicro Conference on Parallel, Distributed and Network-Based Processing, 2004. Proceedings.	10.1109/EMPDP.2004.1271431	parallel computing;numerical analysis;computer science;theoretical computer science;operating system;distributed computing;programming language	HPC	-5.590698729284824	38.315650312327165	10410
5821dc484f96b5fdf49b82407d2c548fc0b1ff45	testing database transaction concurrency	testing transaction databases concurrent computing application software relational databases database systems programming profession information science data analysis fault diagnosis;relational database;relational database application database transaction concurrency testing database application programs database queries dbms database management system acid properties atomicity consistency isolation durability offline concurrency dataflow analysis concurrency faults transaction sequences agenda;satisfiability;software engineering concurrency control transaction processing relational databases program testing;software engineering;dataflow analysis;program testing;concurrency control;relational databases;transaction processing;empirical evaluation	Database application programs are often designed to be executed concurrently by many clients. By grouping related database queries into transactions, DBMS systems can guarantee that each transaction satisfies the well-known ACID properties: Atomicity, Consistency, Isolation, and Durability. However, if a database application is decomposed into transactions in an incorrect manner, the application may fail when executed concurrently due to potential offline concurrency problems. This paper presents a dataflow analysis technique for identifying schedules of transaction execution aimed at revealing concurrency faults of this nature, along with techniques for controlling the DBMS or the application to force execution to follow these transaction sequences. The techniques have been integrated into AGENDA, a tool set for testing relational database application programs. Preliminary empirical evaluation is presented.	acid;algorithm;atomicity (database systems);benchmark (computing);concurrency (computer science);data-flow analysis;database transaction;dataflow;deadlock;durability (database systems);ibm tivoli storage productivity center;integration testing;isolation (database systems);online and offline;phantom reference;relational database;schedule (computer science);system testing;turing test;unit testing;whetstone (benchmark);whole earth 'lectronic link;write–read conflict	Yuetang Deng;Phyllis G. Frankl;Zhongqiang Chen	2003		10.1109/ASE.2003.1240306	database theory;optimistic concurrency control;isolation;database transaction;rollback;database tuning;distributed transaction;relational database;computer science;transaction log;database;distributed computing;online transaction processing;multiversion concurrency control;non-lock concurrency control;programming language;serializability;acid;consistency;database testing;database design;nested transaction;component-oriented database;distributed concurrency control	SE	-23.28537702551516	47.970969226634345	10413
c8d70a16209f9fd421d8fb9eb5c7fb314c1d77d6	plug-and-play integration of dual-model based knowledge artefacts into an open source ehr system		In this paper we present our experiences with extending an existing approach for an archetype-compliant collection and export of data according to the openEHR specifications within the open source EHR system OpenMRS. It allows an automatic generation of forms from templates, which were introduced by openEHR as an extension of the dual-model approach. Data entered in these forms can be exported in form of standardized EHR extracts. The use of templates allowed us to solve problems reported for the original archetype-based version of the approach, which were caused by the high optionality within archetypes.		Rabea Krexner;Georg Duftschmid	2014	Studies in health technology and informatics	10.3233/978-1-61499-432-9-101	knowledge management;data mining;database;openehr;plug and play;computer science	SE	-41.616956182315604	4.427550311838982	10419
8c32a8f1167b67103e0ab5aa937942e3a1320cbd	body area network for first responders: a case study	distributed computing;multi hop network;wireless sensor node;wireless sensor network;operating system;first responder;middleware;data handling;distributed shared memory;wireless sensor networks;body area network	In this paper we present a case study for a design of a reliable body area network (BAN) for monitoring fire fighter rescue teams according to the requirements defined by the Berlin fire brigades. This case study considers all layers of the system, starting from the hardware, going through the operating system and data handling middleware, ending at the application layer. The main parts of the proposed solution are the tinyDSM middleware and a new prototyping hardware platform for wireless sensor nodes--IHPNode. The resulting BAN shall be a part of a larger system, where the BANs are connected via an additional multi-hop network to the control centre. Even if this connection fails, the BAN is able to take autonomous decisions. This system is developed within the FeuerWhere project.	autonomous robot;middleware;operating system;requirement	Krzysztof Piotrowski;Anna Sojka;Peter Langendörfer	2010		10.1145/2221924.2221933	embedded system;real-time computing;wireless wan;wireless sensor network;engineering;wireless network;body area network;key distribution in wireless sensor networks;computer network	Embedded	-39.051535616477075	46.042710631140714	10431
56fd6038ce6adcb25ac33f038ec7dbe72aa3b65a	enabling a quantum monte carlo application for the deep architecture	quantum computing monte carlo methods parallel machines;monte carlo methods computer architecture lattices software programming microwave integrated circuits prototypes;modeling and simulation using hpc systems;barcelona supercomputing center deep architecture exascale system intel xeon cluster intel xeon phi nodes high speed network cineca project turborvb quantum monte carlo simulation program ompss offload task model marenostrum supercomputer;parallelization of simulation exascale systems modeling and simulation using hpc systems;parallelization of simulation;exascale systems	In the DEEP project a prototype Exascale system consisting of a standard Intel Xeon cluster linked to a “Booster” part containing Intel Xeon Phi nodes connected in a high-speed network, is being designed and constructed. In order to evaluate this novel architecture, expected to be available in the second half of 2015, a number of grand challenge applications in computational science and engineering are being modified and optimised. In this study we report on the efforts made by the Cineca project partner and DEEP support staff to enable one of these applications, the TurboRVB Quantum Monte Carlo simulation program, which can be used to study complex phenomena in materials such as superconductivity. The modified code, based on an implementation of the OmpSs offload task model, has been successfully tested on the MareNostrum supercomputer at the Barcelona Supercomputing Center.	booster (electric power);cuda;code;computational engineering;computational science;grand challenges;image scaling;monte carlo method;openmp;prototype;quantum monte carlo;scope (computer science);simulation;software engineering;supercomputer;xeon phi	Andrew Emerson;Fabio Affinito	2015	2015 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2015.7237075	computer architecture;parallel computing;computer science;operating system	HPC	-7.806802841650919	38.88979922648788	10434
a8e75bb4f80249c06c413fca407b50335ec81427	common lisp: a gentle introduction to symbolic computation	symbolic computation	Touretzky explains how to symbolic computing, as java big data. But having read the language, interface editing debugging garbage collection handling built in exploring. The basics but that's clear reader friendly manner. Introducing themes like writing long way giving no expense shop. His fancy trace and diagrams the lisp out php. Out and more we had a, key tool for artificial intelligence programming treatment incorporates several innovative. The version available online use google, reader friendly.	artificial intelligence;big data;common lisp;debugging;diagram;garbage collection (computer science);google reader;java;php;symbolic computation	David S. Touretzky	1989			exception handling;read–eval–print loop;knowledge engineering environment;scheme;interpreter;computer science;fexpr;theoretical computer science;lisp;*lisp;programming language;preprocessor;s-expression;algorithm;rational data type	AI	-29.858326200117904	21.388172578645406	10450
936bb35514d284a497bde68588c6180fa237a63d	steps towards scenario-based programming with a natural language interface		Programming, i.e., the act of creating a runnable artifact applicable to multiple inputs/tasks, is an art that requires substantial knowledge of programming languages and development techniques. As the use of software is becoming far more prevalent in all aspects of life, programming has changed and the need to program has become relevant to a much broader community. In the interest of broadening the pool of potential programmers, we believe that a natural language interface to an intuitive programming language may have a major role to play. In this paper, we discuss recent work on carrying out scenario-based programming directly in a controlled natural language, and sketch possible	controlled natural language;natural language user interface;programmer;programming language	Michal Gordon;David Harel	2014		10.1007/978-3-642-54848-2_9	natural language processing;first-generation programming language;programming domain;reactive programming;programming language implementation;extensible programming;programming paradigm;low-level programming language;fifth-generation programming language;programming language;high-level programming language	PL	-28.56309363211703	23.174258905687545	10458
08f9bb80745680c98b7103212bf10f8585bef424	towards a unified approach to the testability of co-designed systems	formal specification;system testing circuit testing software testing hardware controllability observability software measurement data analysis fault detection embedded system;information transfer;program testing;data flow computing;data flow;software specification;software implementation;real time systems data flow computing formal specification program testing;real time systems;testability estimates unified approach co designed systems testability analysis dataflow co designed systems data flow specification hardware software implementation choice hardware testability model data flow software specification information transfer graph	This paper deals with the testability analysis of dataflow co-designed systems. As a data-flow specification is independent from the hardware/software implementation choice, a uniform approach m y be used to evaluate the specifcation with respect to testability. The difficulty of generating test sets, and of detecting and diagnosing faults is discussed and estimated. We chose to use an existing hardware testability model which is suitcble for data-flow software specifcation; this model, based on information transfers, is called the Information Transfer Graph. A real case study supplied by Ae‘rospatiale illustrates the proposed testabilily estimates.		Yves Le Traon;Chantal Robach	1995		10.1109/ISSRE.1995.497668	reliability engineering;data flow diagram;software requirements specification;computer architecture;real-time computing;information transfer;formal methods;computer science;software engineering;formal specification;software testing	SE	-47.85321145254286	33.42542459048201	10459
24953fbe5c7830f441cdad317b865e06964cb9d1	accessing rdf(s) data resources in service-based grid infrastructures	rdf schema;rdf schema ontology handling;accessing rdf data;deleting rdf data;editorial process;future work;standardization process;database access;integration services;john wiley;accessing rdf;data resource;service-based grid infrastructure	We describe the results of the RDF(S) activity within the Open Grid Forum (http://www.ogf.org) (OGF) Database Access and Integration Services (DAIS) Working Group (http://forge.gridforum.org/projects/ dais-wg) whose objective is to develop standard service-based grid access mechanisms for data expressed in RDF and RDF Schema. We produce two specifications, focused on the provision of SPARQL querying capabilities for accessing RDF data and a set of RDF Schema ontology handling primitives for creating, retrieving, updating, and deleting RDF data. In this paper we present a set of use cases that justify this work and an overview of these specifications, which will enter in editorial process at OGF25. We conclude by outlining the future work that will be made in the context of this standardization process. Copyright © 2009 John Wiley & Sons, Ltd.	digimon;grid computing;john d. wiley;oracle soa suite;rdf schema;resource description framework;sparql;scalability;steven anson coons	Miguel Esteban Gutiérrez;Isao Kojima;Said Mirza Pahlevi;Óscar Corcho;Asunción Gómez-Pérez	2009	Concurrency and Computation: Practice and Experience	10.1002/cpe.1409	use case;rdf/xml;cwm;working group;bibliographic ontology;computer science;sparql;simple knowledge organization system;semantic web;rdf;linked data;data mining;database;rdf query language;integrated services;world wide web;standardization;rdf schema	DB	-37.831116936956214	37.11944360106099	10462
223c8613449bf2cc9e4d85a570d04fae0fd392f0	the guha-dbs data base system		"""Suitable means must be made available to the user and p rog rammer for effective use of existing G U H A p r o c e d u r e s as well as for p rogramming additional ones. The measure of efficiency with which the G U H A p r o c e d u r e s can be utilized is, in principle, de termined by the quality of the user language adopted for specifying the G U H A p r o c e d u r e parameters , by the requirements posed on the interpretation as well as by the requirements ensuing f rom users necessary manipulat ions with observat ion data. The format and the content of the computer outputs themselves is also looked upon as a constituent of this language. The G U H A User Language (GUL) will not be described here and reference is made to H~ijek, Havr~nek & Chytil (1982) where its first version is described. It is important for the purpose of this paper that the programming of a G U L interpreter requires """" re t r ieve"""" , """" insert"""" , """"modi fy"""" or """"de le te"""" type operat ions to be executed on information describing the observation model, on information defining the individual model quantities, on procedure paramete r specifications, simple sentences or, possibly, on some additional data structures. The programming of the G U H A procedures themselves (included in the G U L interpreter) requires, apart f rom those already mentioned, additional operat ions with cards of quantities (see Rauch, 1981). All the operat ions ment ioned which the p rog rammer needs for his work on a G U L interpreter correspond, in fact, to the statements of a certain special programming language whose constituents also include declarative s tatements for the necessary data structures (e.g. model-describing information or quantity card). This language will be referred to as the G U H A Data Manipulation Language (GDML). Another important requirement becomes obvious apart f rom that of the existence of a user language (GUL) and a p rog rammer language (GDML), namely, the independence of these languages and of the software as a whole on the physical data representat ion and on the data access paths. Such requirements are also generally posed on data base systems and, consequently, it is most oppor tune to satisfy these requirements by adopting the existing sophisticated data base system techniques."""	algorithmic efficiency;data access;data manipulation language;data structure;database;direct-broadcast satellite;programming language;requirement;rog-o-matic	Jaroslav Pokorný	1981	International Journal of Man-Machine Studies	10.1016/S0020-7373(81)80012-0	embedded system;human–computer interaction;data mining	DB	-28.14967786682435	14.149051739184623	10469
731471a2a8b613caccb0bb878695e8fd6210a389	differential files: their application to the maintenance of large databases	data sharing;data compression;database maaintenance;data bases;data storage systems;reference point;computer files;data reduction;backup and recovery;differential files;large data	The representation of a collection of data in terms of its differences from some preestablished point of reference is a basic storage compaction technique which finds wide applicability. This paper describes a differential database representation which is shown to be an efficient method for storing large and volatile databases. The technique confines database modifications to a relatively small area of physical storage and as a result offers two significant operational advantages. First, because the “reference point” for the database is inherently static, it can be simply and efficiently stored. Second, since all modifications to the database are physically localized, the process of backup and the process of recovery are relatively fast and inexpensive.	backup;data compaction;database	Dennis G. Severance;Guy M. Lohman	1976	ACM Trans. Database Syst.	10.1145/320473.320484	data compression;data reduction;computer file;computer science;data administration;database model;data mining;database;view;physical data model;information retrieval;database testing;database design	DB	-28.855780970254738	4.610292637191886	10486
6ab53a22d5fb9c0d90e8fb5c0e0c72ede6bfab75	metaprogramming in the large	reconfiguration;developpement logiciel;reconfiguracion;software systems;program transformation;transformation programme;transformacion programa;metamodel;metamodele;object oriented;desarrollo logicial;software development;oriente objet;orientado objeto	Software evolution demands continuous adaptation of software systems to continuously changing requirements. Our goal is to cope with software evolution by automating program transformation and system recon guration. We show that this can be achieved with a static metaprogramming facility and a library of suitable metaprograms. We show that former approaches of program transformations are not suÆcient for large object oriented systems and outline two base transformations that ll the gap.	metaprogramming;program transformation;requirement;software evolution;software system	Andreas Ludwig;Dirk Heuzeroth	2000		10.1007/3-540-44815-2_13	metamodeling;real-time computing;computer science;control reconfiguration;software development;software engineering;programming language;object-oriented programming;software system	SE	-52.72893912135656	29.425527513880862	10488
197720c414f8fa0fcad1637a3b3abda294b399f2	quantifying risks to data assets using formal metrics in embedded system design	data assets;confidentiality loss;model based;security risks;integrity loss;embedded systems;attack modelling;smart meter;stochastic modelling	This paper addresses quantifying security risks associated with data assets within design models of embedded systems. Attack and system behaviours are modelled as time-dependent stochastic processes. The presence of the time dimension allows accounting for dynamic aspects of potential attacks and a considered system: the probability of a successful attack may change as time progresses; and a system may possess different data assets as its execution unfolds. For system modelling, we employ semi-Markov chains that are a powerful tool to capture system dynamics. For attack modelling, we adapt existing formalisms of attack trees and attack graphs. These models are used to analyse and quantify two important attributes of security: confidentiality and integrity. In particular, likelihood/consequence-based measures of confidentiality and integrity losses are proposed to characterise security risks to data assets. Identifying these risks in embedded systems is especially relevant in order to be able to trade them off against other constraints, e.g. limited resources. In our method, we consider attack and system behaviours as two separate models that are later elegantly combined for security analysis. This promotes knowledge reuse and avoids adding extra complexity in the system design process. We demonstrate the effectiveness of the proposed method and metrics on real smart metering devices.	attack tree;confidentiality;embedded system;markov chain;security awareness;semiconductor industry;smart meter;stochastic process;system dynamics;systems design	Maria Vasilevskaya;Simin Nadjm-Tehrani	2015		10.1007/978-3-319-24255-2_25	reliability engineering;asset;computer science;engineering;stochastic modelling;data mining;computer security	Security	-52.918662763396135	49.00707753087432	10497
347cfb42cac74525c2ff88702a5964a63fa8f4f7	a high-level distributed execution framework for scientific workflows	computers;scientific workflows distributed execution;distributed execution;grid workflows;scientific workflow;distributed computing;grid workflows high level distributed execution framework kepler community scientific workflow management systems;kepler community;scientific workflows;computer architecture;computational modeling;monitoring;registers;high level distributed execution framework;distributed databases;workflow management software;workflow management software grid computing natural sciences computing;scientific workflow management systems;natural sciences computing;grid computing;peer to peer computing distributed computing supercomputers network synthesis biological system modeling user interfaces collaborative work computer architecture workflow management software remote monitoring	Domain scientists synthesize different data and computing resources to solve their scientific problems. Making use of distributed execution within scientific workflows is a growing and promising way to achieve better execution performance and efficiency. This paper presents a high-level distributed execution framework, which is designed based on the distributed execution requirements identified within the Kepler community. It also discusses mechanisms to make the presented distributed execution framework easy-to-use, comprehensive, adaptable, extensible and efficient.	high- and low-level;kepler;requirement	Jianwu Wang;Ilkay Altintas;Chad Berkley;Lucas Gilbert;Matthew B. Jones	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.166	computer science;theoretical computer science;database;distributed computing	HPC	-30.932616614037464	50.100134115174924	10509
41355790c9b204adc8adc50227b57c3d642851ac	aggio: a coupon safe for privacy-preserving smart retail environments		Researchers and industry experts are looking at how to improve a shopperu0027s experience and a storeu0027s revenue by leveraging and integrating technologies at the edges of the network, such as Internet-of-Things (IoT) devices, cloud-based systems, and mobile applications. The integration of IoT technology can now be used to improve purchasing incentives through the use of electronic coupons. Research has shown that targeted electronic coupons are the most effective and coupons presented to the shopper when they are near the products capture the most shoppersu0027 dollars. Although it is easy to imagine coupons being broadcast to a shopperu0027s mobile device over a low-power wireless channel, such a solution must be able to advertise many products, target many individual shoppers, and at the same time, provide shoppers with their desired level of privacy. To support this type of IoT-enabled shopping experience, we have designed Aggio, an electronic coupon distribution system that enables the distribution of localized, targeted coupons while supporting user privacy and security. Aggio uses cryptographic mechanisms to not only provide security but also to manage shopper groups e.g., bronze, silver, and gold reward programs) and minimize resource usage, including bandwidth and energy. The novel use of cryptographic management of coupons and groups allows Aggio to reduce bandwidth use, as well as reduce the computing and energy resources needed to process incoming coupons. Through the use of local coupon storage on the shopperu0027s mobile device, the shopper does not need to query the cloud and so does not need to expose all of the details of their shopping decisions. Finally, the use of privacy preserving communication between the shopperu0027s mobile device and the CouponHubs that are distributed throughout the retail environment allows the shopper to expose their location to the store without divulging their location to all other shoppers present in the store.		Albert F. Harris;Robin Snader;Robin Kravets	2018	2018 IEEE/ACM Symposium on Edge Computing (SEC)	10.1109/SEC.2018.00020	computer science;computer security;internet privacy;cryptography;cloud computing;wireless;coupon;mobile device;edge computing;communication channel;purchasing	Mobile	-41.180922844502376	58.965070857689675	10511
270ba1b7b929a19a5253e8946f0e45df186244a3	a case for wafer-scale interconnected memory arrays	vlsi;parallel processing;fabrication;low latency;indexation;computer aided software engineering;production	Due to the immense working-set characteristics of large-scale computational problems, data caching is less useful in bridging the speed gap between processors and memories for supercomputer applications. In addition, the cost of main memory subsystems usually accounts for a substantial portion of a supercomputer system. As a result main memory subsystems have become an important part of supercomputer design. This paper describes a new memory architecture called Wa~er-scale Interconnected Memory Array (WIMA) that is intended to replace ultra-density monolithic DRAM ICS. This architecture employs the high-performance highdensity interconnects provided by the multichip module technology, cache-embedding, and prime-degree interleaving to expose the internal parallelism not exploited by monolithic DRAMs. Using WIMA modules as the basic building blocks, a high-bandwidth, low latency, and low cost main memory system is proposed that could support the parallelism among multiple vector access streams. A novel indexing mechanism for prime-degree interleaving is developed in this work, which delivers fast and predictable memory access latency with modest hardware requirements.	bridging (networking);central processing unit;computation;computational problem;computer data storage;dynamic random-access memory;electrical connection;flash memory;forward error correction;multi-chip module;parallel computing;requirement;supercomputer;wafer (electronics);working set	Tzi-cker Chiueh	1992			uniform memory access;distributed shared memory;shared memory;parallel processing;interleaved memory;computer architecture;semiconductor memory;parallel computing;real-time computing;memory refresh;computer science;operating system;computer memory;overlay;redundant array of independent memory;very-large-scale integration;extended memory;flat memory model;fabrication;registered memory;computer-aided software engineering;cache-only memory architecture;memory map;memory management;low latency	HPC	-11.785884428137487	50.30679965491614	10520
6661c02d4335d88f0f33ca9b5a0a5957fea48859	pacaca: mining object correlations and parallelism for enhancing user experience with cloud storage		Object-based cloud storage presents an unconventional storage model. Exploiting its unique characteristics, such as the strong semantic correlations among objects and the high I/O parallelism potential, can greatly enhance user experience. Unfortunately, current storage optimization techniques, such as the caching and prefetching schemes, are designed for conventional storage and thus are sub-optimal for cloud storage services. In this paper, we propose a client-side cache management framework, called Pacaca, which integrates object clustering, parallelized prefetching, and cost-aware caching to exploit I/O parallelism and object correlations on cloud storage. We first develop an efficient mining scheme, called Frequent Cluster Mining (FCM), to discover object correlations from the access sequence, and then build a prefetching scheme to fetch the correlated objects in parallel. These two schemes are closely coordinated for achieving high prefetching accuracy, proper control on parallelism degree, and effective mis-prefetching detection and handling. After studying the impact of parallelized prefetching on cache management, we further present a cost-aware caching scheme to differentiate low-cost and high-cost objects for efficient caching by leveraging the awareness of parallelism and object correlations. Our experimental results show that our optimization schemes can effectively reduce the access latency, outperforming traditional schemes by up to 58%.	cpu cache;cache (computing);client-side;cloud storage;cluster analysis;fuzzy cognitive map;input/output;mathematical optimization;object-based language;parallel computing;storage model;user experience	Binbing Hou;Feng Chen	2018	2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)	10.1109/MASCOTS.2018.00036	cloud storage;cluster analysis;latency (engineering);cache;computer science;user experience design;storage model;distributed computing;exploit;fetch	Arch	-15.001214793136715	54.99222192582692	10524
cf734d3dd8bb94eb001165314cf1e32218aebc5a	performance analysis of the fft algorithm on a shared-memory parallel architecture	parallel calculus;computers;digital computers;data transmission;integral transformations;general and miscellaneous mathematics computing and information science;shared memory;communications;fourier transform;multiprocessor;efficiency;performance;simulation;mathematical logic;transformacion fourier rapida;interconnection network;computer architecture;integral transforms;calculo paralelo;architecture ordinateur;community computing;fourier transformation;array processors;analyse performance;computerized simulation;performance analysis;parallel computer;transformations 990210 supercomputers 1987 1989;communication delay;algorithms;performance prediction;arquitectura ordenador;parallel architecture;logic programs;multiprocesador;transformation fourier rapide;calcul parallele;programming;memory devices;supercomputers;parallel processing;fast fourier transformation;analisis eficacia;multiprocesseur	This paper presents a model for the performance prediction of FFT algorithms executed on a shared-memory parallel computer consisting of N processors and the same number of memory modules. The model applies a deterministic analysis to estimate the communication delay through the interconnection network by assuming that all requests arrive at the network in bursts. The results indicate that the communication delay is significantly affected by the method applied to allocate data to memory modules. For the case in which all data items referenced by a processor during an iteration are allocated to a single memory module. The authors present the best and worst case.	algorithm;fast fourier transform;profiling (computer programming);shared memory	Zarka Cvetanovic	1987	IBM Journal of Research and Development	10.1147/rd.314.0435	fourier transform;parallel processing;parallel computing;telecommunications;computer science;electrical engineering;theoretical computer science;operating system;programming language;algorithm	HPC	-14.511533868583273	43.83813914094195	10541
22b4c8aa734f63f3da6ce45e2aa1d9b02c15060f	ccdm: ladder-logic programming for wireless sensors and actuators with central controller-based device management	data processing	This paper proposes ladder-logic programming model for sensor actuator networks. We also demonstrate optimized operations of them with central controller-based device management (CCDM) architecture. A wireless sensor actuator network consists of distributed wireless nodes, and implementing data streams and data processors onto these wireless nodes has been challenging. System programmers have to describe their instructions by a programming language, and data processors must be placed so that it optimizes, for example, total network traffic. The ladder-logic model enables the programming of them, and CCDM makes various types of optimizations feasible, including the optimization of network traffic, delivery latency, load-balancing and fault-tolerance even though these algorithms are not lightweight. In this paper, we focus on traffic reduction case, and propose two moderately complex algorithms. The experiment has shown that CCDM achieves optimizations even with such moderately complex algorithms.	logic programming	Hideya Ochiai;Hiroshi Esaki	2011	IEICE Transactions		embedded system;real-time computing;data processing;computer science;operating system;key distribution in wireless sensor networks;wi-fi array;computer network	Mobile	-37.09077111347325	45.770940303248516	10554
408145c93105f6ed0eafd0565add636679523ec7	subsystem decomposition in simulation of a pcb assembly line	subsystem decomposition;pcb assembly line;complex system;printed circuits;printed circuit board assembly;simulation model;software systems	Simulation modeling of large complex systems is time consuming. In this paper we examine a modeling decomposition approach that would expedite the modeling process. We discuss some of the theoretical and practical problems of decomposition and provide an example found in printed circuit board assembly.	assembly language;printed circuit board;simulation	M. Eric Johnson;Joakim Kalvenes	1993		10.1109/WSC.1993.718321	embedded system;complex systems;computer science;engineering;circuit design;software construction;printed circuit board;world wide web;manufacturing engineering;software system;computer engineering	EDA	-57.131972962036116	9.053853116405918	10575
4cd9ca0628e6b8d048a0cfb18c64198a4659463e	blade: an attack-agnostic approach for preventing drive-by malware infections	malware protection;unconsented content execution prevention;drive by download	Web-based surreptitious malware infections (i.e., drive-by downloads) have become the primary method used to deliver malicious software onto computers across the Internet. To address this threat, we present a browser independent operating system kernel extension designed to eliminate driveby malware installations. The BLADE (Block All Drive-by download Exploits) system asserts that all executable files delivered through browser downloads must result from explicit user consent and transparently redirects every unconsented browser download into a nonexecutable secure zone of disk. BLADE thwarts the ability of browser-based exploits to surreptitiously download and execute malicious content by remapping to the file system only those browser downloads to which a programmatically inferred user-consent is correlated, BLADE provides its protection without explicit knowledge of any exploits and is thus resilient against code obfuscation and zero-day threats that directly contribute to the pervasiveness of today's drive-by malware. We present the design of our BLADE prototype implementation for the Microsoft Windows platform, and report results from as extensive empirical evaluation of its effectiveness on popular browsers. Our evaluation includes multiple versions of IE and Firefox, against 1,934 active malicious URLs, representing a broad spectrum of web-based exploits not plaguing the Internet. BLADE successfully blocked all drive-by malware install attempts with zero false positives and a 3% worst-case performance cost.	blade;best, worst and average case;computer;drive-by download;exploit (computer security);firefox;internet;kernel (operating system);loadable kernel module;malware;microsoft windows;obfuscation (software);operating system;prototype;web application	Long Lu;Vinod Yegneswaran;Phillip A. Porras;Wenke Lee	2010		10.1145/1866307.1866356	computer science;cryptovirology;internet privacy;world wide web;computer security	Security	-55.305756079185095	59.29444075436692	10576
be61357a7df9515f686aab72233b3216f498ec58	a new speculation technique to optimize floating-point performance while preserving bit-by-bit reproducibility	instruction level parallel;striding;processor architecture;reassociation;prefetching;bit by bit reproducibility;floating point speculation;accuracy;loop unrolling;just in time compiler;numerical computation;privatization;floating point;ieee 754;software pipelining;instruction level parallelism;fused multiply add;java;ia 64	The bit-by-bit reproducibility of floating-point results, which is defined by the IEEE 754 standard, prohibits optimizations such as reassociation and the use of native operations such as fused multiply-add (FMA), and thus it significantly impairs floating-point performance. Recent network-oriented languages such as Java strictly conform to the standard, and thus their numerical computing performance becomes inherently lower than conventional languages.In this paper, we propose a new software technique, called floating-point (FP) speculation, to optimize floating-point performance while preserving the bit-by-bit reproducibility of the results. We execute the fast unsafe code and the slow verification code in parallel. The unsafe code does not wait for the verification code, and is immediately followed by the subsequent code that uses the probable result from the unsafe code assuming the speculation will succeed. The improvement from FP speculation results from this earlier start of the subsequent code.Unlike other speculation techniques, FP speculation does not require any special instructions or hardware support. Rather, it exploits unused floating-point registers and execution units. Therefore it is generally applicable for processor architectures that have sufficient floating-point resources.	card security code;execution unit;fma instruction set;floating-point unit;java;list of cpu architectures;multiply–accumulate operation;numerical analysis;processor register	Mikio Takeuchi;Hideaki Komatsu;Toshio Nakatani	2003		10.1145/782814.782857	software pipelining;computer architecture;parallel computing;real-time computing;microarchitecture;computer science;floating point;operating system;just-in-time compilation;loop unrolling;accuracy and precision;multiply–accumulate operation;programming language;java;instruction-level parallelism;ieee floating point	Arch	-6.848424673837974	50.378379657794966	10582
ebec0cdbbfdf1b5b429da7a8630be9a250ebb089	automatic heap layout manipulation for exploitation		Heap layout manipulation is integral to exploiting heap-based memory corruption vulnerabilities. In this paper we present the first automatic approach to the problem, based on pseudo-random black-box search. Our approach searches for the inputs required to place the source of a heap-based buffer overflow or underflow next to heap-allocated objects that an exploit developer, or automatic exploit generation system, wishes to read or corrupt. We present a framework for benchmarking heap layout manipulation algorithms, and use it to evaluate our approach on several real-world allocators, showing that pseudo-random black-box search can be highly effective. We then present SHRIKE, a novel system that can perform automatic heap layout manipulation on the PHP interpreter and can be used in the construction of control-flow hijacking exploits. Starting from PHPu0027s regression tests, SHRIKE discovers fragments of PHP code that interact with the interpreteru0027s heap in useful ways, such as making allocations and deallocations of particular sizes, or allocating objects containing sensitive data, such as pointers. SHRIKE then uses our search algorithm to piece together these fragments into programs, searching for one that achieves a desired heap layout. SHRIKE allows an exploit developer to focus on the higher level concepts in an exploit, and to defer the resolution of heap layout constraints to SHRIKE. We demonstrate this by using SHRIKE in the construction of a control-flow hijacking exploit for the PHP interpreter.		Sean Heelan;Thomas F. Melham;Daniel Kroening	2018			computer science;pointer (computer programming);theoretical computer science;allocator;memory corruption;exploit;search algorithm;interpreter;buffer overflow;heap (data structure)	Security	-57.58703918513628	55.789018253824224	10601
72abb8e7990fed14b4963d2baf311a6d4bdbf075	aviation safety: modeling and analyzing complex interactions between humans and automated systems	verification;air traffic control;aviation safety;pilot support systems;research outputs;uberligen;program verification computers;uberlingen collision;simulation;research publications;muti agent system;aircraft approach spacing;aircraft safety;warning systems;collision avoidance;flight safety;authority and autonomy;software reliability;models;brahms;human computer interface;human factors engineering	The on-going transformation from the current US Air Traffic System (ATS) to the Next Generation Air Traffic System (NextGen) will force the introduction of new automated systems and most likely will cause automation to migrate from ground to air. This will yield new function allocations between humans and automation and therefore change the roles and responsibilities in the ATS. Yet, safety in NextGen is required to be at least as good as in the current system. We therefore need techniques to evaluate the safety of the interactions between humans and automation. We think that current human factor studies and simulation-based techniques will fall short in front of the ATS complexity, and that we need to add more automated techniques to simulations, such as model checking, which offers exhaustive coverage of the non-deterministic behaviors in nominal and off-nominal scenarios. In this work, we present a verification approach based both on simulations and on model checking for evaluating the roles and responsibilities of humans and automation. Models are created using Brahms (a multi-agent framework) and we show that the traditional Brahms simulations can be integrated with automated exploration techniques based on model checking, thus offering a complete exploration of the behavioral space of the scenario. Our formal analysis supports the notion of beliefs and probabilities to reason about human behavior. We demonstrate the technique with the Überlingen accident since it exemplifies authority problems when receiving conflicting advices from human and automated systems.	adobe air;human factors and ergonomics;interaction;model checking;multi-agent system;simulation	Neha Rungta;Guillaume Brat;William J. Clancey;Charlotte Linde;Franco Raimondi;Chin Seah;Michael G. Shafto	2013		10.1145/2494493.2494498	simulation;engineering;operations management;computer security	SE	-40.508201157316705	29.50291508615602	10607
b681c3d5b02bfb487407fde51a225ede929c67f0	automated measurement of uml models: an open toolset approach	measurement tool;object oriented software;unified modeling language	The Unified Modeling Language (UML) is the de facto standard language for modeling object-oriented software systems. As the importance of UML within organizations increases, the need for measuring UML models arises. This paper describes a UML measurement tool that not only fully supports the measurement of models according to the most popular metrics definitions, but also provides an open measurement base supporting user-defined metrics, unforeseen analysis, and process measurement.	database schema;design for testing;documentation;existential quantification;hoc (programming language);interoperability;java;metamodeling;object constraint language;requirement;software system;unified modeling language;xml metadata interchange	Luigi Lavazza;Alberto Agostini	2005	Journal of Object Technology	10.5381/jot.2005.4.4.a2	reliability engineering;unified modeling language;model-driven architecture;uml state machine;communication diagram;systems modeling language;uml tool;computer science;systems engineering;applications of uml;class diagram;database;shlaer–mellor method;node;object constraint language	SE	-53.12148767216309	26.33885561088622	10613
65332aa2fd4c5ff09357182e914ddac3f39b9e52	a solution for data inconsistency in data integration	experimental design;integridad de datos;data integrity;integrite donnee;integration information;decision borrosa;plan experiencia;prise de decision;decision floue;data fusion;information integration;plan experience;object oriented;fusion donnee;data source quality criteria;integracion informacion;data inconsistency;oriente objet;coherence;modele donnee;coherencia;fusion datos;toma decision;orientado objeto;fuzzy decision;data integration;data models	Data integration is a problem of combining data residing at different sources and providing the user with a unified view of these data. An important issue in data integration is the possibility of conflicts among the different data sources. Data sources may conflict with each other at data value level which is defined as data inconsistency. So in this paper, a solution for data inconsistency in data integration is proposed. An approximate object-oriented data model extended with data source quality criteria is defined. On the basis of our data model, we provide a data inconsistency solution strategy. To accomplish our strategy, fuzzy multi-attribute decision making approach based on data source quality criteria is applied to select the “best” data source’s data as the data inconsistency solution. A set of experiments is designed and performed to evaluate the effectiveness of our strategy and algorithm. The experimental results indicate that our solution performs ideally.	approximation algorithm;data model;experiment	Xin Wang;Linpeng Huang;Xiaohui Xu;Yi Zhang;Jun-Qing Chen	2011	J. Inf. Sci. Eng.		data modeling;data quality;coherence;computer science;artificial intelligence;information integration;data integration;data warehouse;data integrity;data mining;database;sensor fusion;data pre-processing;data efficiency;object-oriented programming;design of experiments;logical data model	DB	-34.169550044481554	10.776774706027842	10625
22f943c7b17f6f35b0d04af95e287eafbe39e83d	deadlock-freedom in component systems with architectural constraints	architectural constraints;004 informatik;interaction systems;component systems;deadlock freedom;sufficient condition	We present a compositional analysis of deadlock-freedom in component systems with multiway cooperation. We require the systems to satisfy an architectural constraint which makes sure that the communication structure between the components is given by a tree. Only pairs of components have to be examined for the analysis, therefore the cost is polynomial in the size of the input. We shortly discuss a prototype algorithm which is based on our results and can be used for the investigation of deadlock-freedom of systems satisfying the architectural constraint.	algorithm;deadlock;polynomial;prototype	Moritz Martens;Mila E. Majster-Cederbaum	2012	Formal Methods in System Design	10.1007/s10703-012-0160-6	distributed computing	Logic	-34.995261327301606	32.96102791481374	10626
a96cc2530609d46d9a39f5884d8a9f8192991114	hybrid scheduling in the devide dataflow visualisation environment		Dataflow application builders such as AVS, OpenDX and MeVisLab are popular and effective tools for the rapid prototyping of visualisation algorithms. They enable researchers to build applications by graphically connecting functional modules together to form a network. A usually hidden yet important aspect of these tools is the scheduling of network execution: Most of these environments can be classified as employing event-driven or demand-driven scheduling. The scheduling strategy has important implications for the component developer. In this paper, we present our recently opensourced dataflow application builder, called DeVIDE, for the rapid prototyping of medical visualisation and image processing techniques. Apart from the unique interaction possibilities and ease of integration that it offers, DeVIDE differentiates itself from similar environments by implementing a hybrid scheduling approach that adaptively applies demandand event-driven scheduling to a single network. In this way, ease of component development and execution efficiency can be combined.	algorithm;dataflow;event-driven programming;ibm opendx;image processing;mevislab;rapid prototyping;scheduling (computing)	Charl P. Botha;Frits H. Post	2008			image processing;visualization;dataflow;real-time computing;scheduling (computing);hybrid scheduling;distributed computing;rapid prototyping;computer science	HPC	-31.438979270774222	35.59226343754866	10631
45cec5cdebbf2ac4ffe5e62bb300e05d16b16e7f	deferred repair of inconsistencies resulting from retroactive updates of temporal xml currency data		PurposernrnrnrnrnA temporal XML database could become an inconsistent model of the represented reality after a retroactive update. Such an inconsistency state must be repaired by performing corrective actions (e.g. payment of arrears after a retroactive salary increase) either immediately (i.e. at inconsistency detection time) or in a deferred manner, at one or several chosen repair times according to application requirements. The purpose of this work is to deal with deferred and multi-step repair of detected data inconsistencies.rnrnrnrnrnDesign/methodology/approachrnrnrnrnrnA general approach for deferred and stepwise repair of inconsistencies that result from retroactive updates of currency data (e.g. the salary of an employee) in a valid-time or bitemporal XML database is proposed. The approach separates the inconsistency repairs from the inconsistency detection phase and deals with the execution of corrective actions, which also take into account enterprise’s business rules that define some relationships between data.rnrnrnrnrnFindingsrnrnrnrnrnAlgorithms, methods and support data structures for deferred and multi-step inconsistency repair of currency data are presented. The feasibility of the approach has been shown through the development and testing of a system prototype, named Deferred-Repair Manager.rnrnrnrnrnOriginality/valuernrnrnrnrnThe proposed approach implements a new general and flexible strategy for repairing detected inconsistencies in a deferred manner and possibly in multiple steps, according to varying user’s requirements and to specifications which are customary in the real world.	xml schema	Hind Hamrouni;Fabio Grandi;Zouhaier Brahmia	2017	IJWIS	10.1108/IJWIS-02-2017-0009	xml;arrears;computer science;data mining;database;xml database;originality;salary;payment;data structure;business rule	DB	-26.76139147898685	13.53258507111751	10642
7b782d3a5234c2369025027970fb61529f8ede86	cosmic: a middleware for event-based interaction on can	quality of service middleware field buses factory automation cooperative systems distributed object management;middleware communication system control computer architecture intelligent sensors distributed computing manufacturing automation maintenance application software computer industry workstations;communication model;field buses;can bus cosmic middleware event based interaction cooperating smart devices distributed factory automation systems field bus standard low level communication objects communication quality aspects middleware architecture communication abstraction distributed cooperating objects event based communication model object control autonomy event channels real time requirements reliability requirements middleware layer;cooperative systems;distributed object management;factory automation;middleware;quality of service	Distributed factory automation systems benefit from field-busses which, in general, provide support for reliable and timely communication. These field-busses, however, provide rather low level communication objects and their features regarding quality aspects of communication are difficult to assess and use for applications. The paper presents COSMIC (Cooperating Smart devices), a middleware architecture which allows to use communication abstractions appropriate for high level applications based on distributed cooperating objects. The middleware supports an event-based communication model which enables spontaneous dissemination of events, maintains control autonomy of objects and allows to specify different real-time and reliability requirements on the application level. The basic abstractions presented at the middleware layer are events and event channels. As an example, the paper describes how these abstractions are mapped to the CAN-Bus which constitutes a widely used field-bus standard.	application programming interface;autonomous robot;can bus;cosmic;event (computing);fieldbus;high-level programming language;middleware;publish–subscribe pattern;real-time locating system;real-time transcription;requirement;smart device;spontaneous order	Jörg Kaiser;Carlos Mitidieri;Cristiano Brudna;Carlos Eduardo Pereira	2003		10.1109/ETFA.2003.1248763	embedded system;middleware;real-time computing;models of communication;quality of service;computer science;message oriented middleware;operating system;automation;middleware;distributed computing;computer network	HPC	-36.47514910206038	45.64971162986997	10648
916771661455d754e47d46332d79cc1c87a2a286	improving web service composition with user requirement transformation and capability model		In order to discover and compose relevant Web services, most Web service composition approaches require users to describe composition require- ments and constraints in formal expressions. However, requirements still focus on the technical level as they require domain-specific knowledge on functional and non-functional properties. As a matter of fact, the gap between users' high- level requirements describing business objectives and composition requirements remains a challenge in Web service composition. In this paper, we propose an end-to-end transformation approach to build a set of technical composition re- quirements from user high-level requirements, which are specified by an Eng- lish-structured language to capture business objectives, and to specify actions that Web services can achieve.	web service	Wenbin Li;Youakim Badr;Frédérique Biennier	2013		10.1007/978-3-642-41030-7_21	web service;web modeling;computer science;systems engineering;knowledge management;ws-policy;world wide web	AI	-48.8648118359209	17.307939440830296	10658
772df831058d0927bda3d75294a32220afe203fe	portable desktop applications based on user-level virtualization	computers;software;virtual machine;ubiquitous computing application program interfaces portable computers;portable desktop;performance evaluation;prototypes;ubiquitous computing portable desktop personalized desktop environment user level virtualization technologies portable usb device performance evaluation storage capacity;time factors;graphical user interfaces;internet;portable computers;storage capacity;application program interfaces;computers software internet prototypes time factors file systems graphical user interfaces;ubiquitous computing;personalized desktop environment;virtual environment;user level virtualization technologies;portable usb device;file systems	As computing is becoming increasingly ubiquitous today, it would be very attractive for common computer users to access same personalized desktop environment on any compatible PC anytime and anywhere. This paper presents such a solution for Windows systems based on user-level virtualization technologies. Namely, the userpsilas data, applications and their configurations are stored on a portable USB device. At run-time, the portable desktop-applications on the device will run in a user-mode virtualization environment where some resource (registry, files/directories, environment variables, etc.) accessing APIs are intercepted and redirected to the portable device as necessary. User can access her personalized applications and data conveniently on any compatible computer, although they do not exist on local disk. This paper describes the whole design, technical details and performance evaluation, and presents a demo application. Compared with some existing solutions based on virtual machine technologies, this solution is more efficient in performance and storage capacity.	anytime algorithm;application programming interface;desktop computer;environment variable;hardware virtualization;host (network);internet;microsoft windows;mobile device;peer-to-peer;performance evaluation;personalization;protection ring;prototype;software portability;usb;user (computing);user space;virtual machine	Youhui Zhang;Xiaoling Wang;Liang Hong;Gelin Su;Dongsheng Wang	2008	2008 13th Asia-Pacific Computer Systems Architecture Conference	10.1109/APCSAC.2008.4625473	embedded system;full virtualization;virtualization;human–computer interaction;computer science;virtual machine;operating system;ubiquitous computing	HPC	-36.10636141708172	51.48542634712703	10665
be863a135389411367091b8e615ac6568467641b	discrete dynamic simulation models and technique for complex control systems	hierarchical structure;control scenarios;dynamic model;hierarchical state diagrams;large data sets;hierarchical control;control system;state diagram;dynamic simulation;expert knowledge;model synthesis;information system;discrete simulation;knowledge representation;dynamic models;simulation model;computer simulation;object oriented paradigm	The method for dynamic model synthesis and discrete simulation of complex hierarchical control systems is presented. The method provides integration of large data sets, monitoring data and expert knowledge with the process of simulation and analysis of system state dynamics, thus providing an extensible and evolvable environment and reuse of knowledge and simulation models. The method is based on the hierarchical state diagrams technique and control scenarios methodology. The general structure of corresponding computer simulation system is also proposed. We also outline general principles of computer realization of our simulation approach, and schemes of model-based knowledge representation. The proposed method is based on the object-oriented paradigm and is especially powerful in information-intensive environments.	control system;dynamic simulation	Armen Bagdasaryan	2011	Simulation Modelling Practice and Theory	10.1016/j.simpat.2010.12.010	computer simulation;dynamic simulation;state diagram;simulation;computer science;systems engineering;control system;theoretical computer science;discrete event simulation;simulation modeling;information system;simulation language	Embedded	-36.381516818303304	27.65781332919364	10669
1fbc4f1ee9c08a430a4d3183b2ff1da1a19db975	symbolic semantics and verification of stochastic process algebras	automatic verification;labelled transition system;stochastic petri net;temporal logic;specification language;continuous time markov chain;denotational semantic;semantic model;compact representation;complex system;stochastic process algebra;state space;reliability analysis;state space explosion;data structure	ion can be seen as the combination of all possible restrictions to a set of variables combined by an associative binary operator ⋆. Formally, this can be defined as follows: Abstract(B, (xi, ..., xm), ⋆) := B ∣ ∣ xi,...,xm:=(0,...,0) ⋆ ... ⋆ B ∣	stochastic process	Matthias Kuntz	2006			semantic data model;stochastic petri net;data structure;specification language;temporal logic;state space;continuous-time markov chain	AI	-10.766276938766557	23.129973651751012	10670
80c49924bfd8bb3a2b41ce849025582f1332efd9	how to avoid the generation of logic loops in the construction of fault trees	recursive operability analysis;fault tree;hazard operability analysis;recursion operator;infinite series;failure mode and effect analysis;loop reduction	Generation of an infinite series of identical sub-trees may occur during the construction of a Fault Tree (FT) when one item of equipment in a plant is considered several times in the same sub-tree in the course of the tree extraction from a HazOp (Hazard Operability analysis) analysis. Generation of loops in the construction of an FT can be avoided by means of an ad hoc logical analysis in which certain simple rules of syntax are taken into account. A radical solution, however, can be obtained if identification of unwanted events in a process plant is not undertaken with conventional procedures, such as HazOp (Operability Analysis with guide words, failure mode and effect analysis (FMEA) etc.), but with a more modern and structured version, such as Recursive Operability Analysis (ROA), which is both systematic and complete, and allows direct extraction of logic trees, (FT, event trees, etc.) for subsequent quantification. This feature means that, by contrast with conventional operability analysis, the congruence of the ROA itself can be checked. The ROA method is illustrated in this paper with the aid of some simple examples. q 2003 Elsevier Ltd. All rights reserved.	congruence of squares;failure cause;failure mode and effects analysis;fault tree analysis;hazard analysis;hoc (programming language);operability;rams;recursion (computer science);resource-oriented architecture;software design	Micaela Demichela;Norberto Piccinini;Italo Ciarambino;Sergio Contini	2004	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(03)00141-8	reliability engineering;fault tree analysis;engineering;mathematics;series;failure mode and effects analysis;algorithm	Logic	-49.676025255307096	36.53678233490513	10672
d8cd7531a3379761695d2e75c40b28ebd306d4a3	dynamic slicing research of uml statechart specifications	statechart specification;formal semantics;slicing;specification analysis;state explosion problem;slice criterion;reactive systems;specification verification	This paper extends the well-known technique of dynamic slicing to Statechart specifications of reactive systems. Statechart language extends state machines along hierarchy, concurrency and communication – resulting in a compact visual notation that allows engineers to structure and modularize system descriptions. Dynamic slicing is well known in the domain of sequential transformational programs and has been found to be useful in understanding, analysis and verification. The classical definition of dynamic slicing is unsuitable for Statechart specifications. In this paper, we firstly formally define a formal semantics model -observable semantics, which is very suitable for dynamic slicing, because that it only describes outside observable behavior and conceals unobservable behavior of Statechart specifications, and it fully captures the run-time dependence relation among the state transitions in the Statechart specification. Then we propose a new notion of dynamic slicing that, in our opinion, is more natural for Statechart specifications. We formally define notions of dynamic slicing criterion, dynamic slice and minimal dynamic slice, and we also explain how to produce valid dynamic slicing criterion and propose a simple and practical approximation algorithm for minimal dynamic slice generation using observable semantics as an intermediate representation.	application domain;approximation algorithm;concurrency (computer science);display resolution;intermediate representation;observable;semantics (computer science);state diagram;uml state machine;unified modeling language;whole earth 'lectronic link	ChunYu Miao	2011	JCP	10.4304/jcp.6.4.792-798	real-time computing;reactive system;computer science;formal semantics;programming language	SE	-18.830449472166862	28.425985300513354	10674
0ef3408baef324ac41f1e8c7a00413411f53606e	multiple objective programming support	decision maker;multiple objective programming	This paper gives a brief introduction into multiple objective programming support. We will overview basic concepts, formulations, and principles of solving multiple objective programming problems. To solve those problems requires the intervention of a decisionmaker. That’s why behavioral assumptions play an important role in multiple objective programming. Which assumptions are made affects which kind of support is given to a decision maker. We will demonstrate how a free search type approach can be used to solve multiple objective programming problems.	cochrane library;columbia (supercomputer);completeness (knowledge bases);computation;computer graphics;decision analysis;decision support system;interactive programming;john d. wiley;linear programming;management science;mathematical optimization;multi-objective optimization;multiple inheritance;operations research;springer (tank);vector optimization;zionts–wallenius method	Pekka J. Korhonen	2009		10.1007/978-0-387-74759-0_431	mathematical optimization;constraint programming;reactive programming;goal programming;mathematics;management science;inductive programming;algorithm	AI	-5.606785600243384	5.7664439176743905	10682
1484eb5ced5ebe4eaf6f29a762f74f56d1d39340	analyzing plan diagrams of database query optimizers	query optimization;database query;cost model	A “plan diagram” is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. In this paper, we present and analyze representative plan diagrams on a suite of popular commercial query optimizers for queries based on the TPC-H benchmark. These diagrams, which often appear similar to cubist paintings, provide a variety of interesting insights, including that current optimizers make extremely fine-grained plan choices, which may often be supplanted by less efficient options without substantively affecting the quality; that the plan optimality regions may have highly intricate patterns and irregular boundaries, indicating strongly non-linear cost models; that nonmonotonic cost behavior exists where increasing result cardinalities decrease the estimated cost; and, that the basic assumptions underlying the research literature on parametric query optimization often do not hold in practice.	benchmark (computing);database;diagram;emoticon;ibm tivoli storage productivity center;image;mathematical optimization;nonlinear system;optimizing compiler;query optimization;query plan;scientific literature;selectivity (electronic);speculative execution	Naveen Reddy;Jayant R. Haritsa	2005			sargable;query optimization;boolean conjunctive query;computer science;theoretical computer science;data mining;database;view;query language	DB	-25.372498588004905	5.566409239434963	10688
3a964ff682d1f81bc132e222eafc1656a65fbce0	a scalable tool architecture for diagnosing wait states in massively parallel applications	pattern search;sequential search;event tracing;performance analysis;message passing;scalability;parallel applications	When scaling message-passing applications to thousands of processors, their performance is often affected by wait states that occur when processes fail to reach synchronization points simultaneously. As a first step in reducing the performance impact, we have shown in our earlier work that wait states can be diagnosed by searching event traces for characteristic patterns. However, our initial sequential search method did not scale beyond several hundred processes. Here, we present a scalable approach, based on a parallel replay of the target application’s communication behavior, that can efficiently identify wait states at the previously inaccessible scale of 65,536 processes and that has potential for even larger configurations. We explain how our new approach has been integrated into a comprehensive parallel tool architecture, which we use to demonstrate that wait states may consume a major fraction of the execution time at larger scales.	application programming interface;blue gene;central processing unit;computer data storage;data structure;directory (computing);distributed memory;extrapolation;input/output;job stream;linear search;mathematical optimization;message passing interface;message passing;microsoft outlook for mac;online and offline;openmp;parallel computing;prototype;real-time clock;run time (program lifecycle phase);scalability;sensor;serialization;simulation;tcp global synchronization;tracing (software);wait state	Markus Geimer;Felix Wolf;Brian J. N. Wylie;Bernd Mohr	2009	Parallel Computing	10.1016/j.parco.2009.02.003	linear search;pattern search;parallel computing;message passing;real-time computing;scalability;computer science;operating system;distributed computing;programming language	HPC	-16.99332736498039	48.62592137264995	10691
65deeab0a1f5b1fa36dcfd54a85b01c79d347b33	guiding genetic algorithms using importance measures for reliable design of embedded systems	reliability engineering;space exploration;embedded systems;redundancy;genetic algorithms;optimization	Reliability importance measures (IMs) support analysts in understanding the contributions of components to the reliability of the system under investigation. This understanding can be of use to improve the reliability of a system and at the same time, restrict the cost penalty by upgrading only the highly important components to more reliable ones. This paper studies how IMs can enhance the design of embedded systems, more specifically to guide the optimization process. The observations are later employed to modify a well-known Genetic Algorithm (GA) to create new offsprings using the IMs of the components of their parents. The experimental results prove the efficiency of the proposed algorithm which not only seeks for more reliable designs, but also reckons with other design objectives-in this paper resource cost and power consumption-concurrently to ensure that they are not degraded through the optimization process.	embedded system;genetic algorithm;instant messaging;mathematical optimization;multi-objective optimization;pareto efficiency;software release life cycle;test case	Hananeh Aliee;Stefan Vitzethum;Michael Glaß;Jürgen Teich;Emanuele Borgonovo	2016	2016 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)	10.1109/DFT.2016.7684069	reliability engineering;embedded system;electronic engineering;real-time computing;simulation;genetic algorithm;computer science;engineering;space exploration;operating system;distributed computing;redundancy	Embedded	-34.83178827090129	21.729502197241203	10695
350546cc45236c5e5c61aa8acdbdfcd4db6db2c8	context-aware services for multiple-users		This paper presents a framework for providing contextaware services in public spaces, e.g., museums. The framework is unique among other existing context-aware systems in implementing services as mobile agents and supporting groups of users in addition to single users. It maintains a location model as containment relationships between digital representations, called virtual counterparts, corresponding to people, terminals, or spaces, according to their locations in the real world. When a visitor moves between exhibits in a museum, it dynamically deploys his/her service provider agents at the computers close to the exhibits via virtual counterparts. When two visitors stand in front of an exhibit, service-provider agents are mutually executed or configured according to the member of the visitors. To demonstrate the utility and effectiveness of the system, we constructed location/user-aware visitor-guide services and experimented with them for two weeks in a public museum.	computer;context-aware network;context-aware pervasive systems;mobile agent;multi-user	Ichiro Satoh	2011			knowledge management;data mining;computer science;context-aware services	HCI	-38.20643117574855	48.43366239712467	10704
349841c7fbb56817d472271b005125e249a6df19	a generalized notion of weak interpretability and the corresponding modal logic	modal logic	Dzhaparidze, G., A generalized notion of weak interpretability and the corresponding modal logic, Annals of Pure and Applied Logic 61 (1993) 113-160. A tree Tr(T,, , T,,) of theories T,, . , T, is called tolerant, if there are consistent extensions T:, , Ti of T,, , T,, where each T,? interprets its successors in the tree Tr(T:, , TT). We consider a propositional language with the following modal formation rule: if Tr is a (finite) tree of formulas, then OTr is a formula, and axiomatically define in this language the decidable logics TLR and TLRw. It is proved that TLR (resp. TLRw) yields exactly the schemata of PA-provable (resp. true) sentences, if OTr(A,, , A,) is understood as (a formalization of) “Tr(PA + A,, . , PA + A,) is tolerant”. In fact, TLR axiomatizes a considerable fragment of provability logic with quantifiers over P,-sentences, and many relations that have been studied in the literature can be expressed in terms of tolerance. We introduce and study two more relations between theories: cointerpretability and cotolerance which are, in a sense, dual to interpretability and tolerance. Cointerpretability is a characterization of Z,-conservativity for essentially reflexive theories in terms of translations.	formation rule;modal logic;pa-risc;provability logic;provable security;theory	Giorgie Dzhaparidze	1993	Ann. Pure Appl. Logic	10.1016/0168-0072(93)90201-N	modal logic;mathematical analysis;discrete mathematics;topology;mathematics;algorithm;algebra	Logic	-10.54127753662709	12.837981010539913	10729
f9cb9e1fbb341d453f2e9290ddba5074a5925649	dynamically adapting clients to web services changing	service oriented architecture soa;process algebra pa.;aspect oriented programming aop;bpel4ws;web services ws	Web Service is the fitted technical solution which provides the required loose coupling to achieve Service Oriented Architecture (SOA). However, there is still much to be done in order to increase flexibility and adaptability to SOA-based applications. In previous researches, we proposed approaches based on Aspect Oriented Programming (AOP) and Process Algebra (PA) to address flexibility and client generation issues in the Web Service context. In this paper, we extend our previous formalism defined for abstract BPEL processes, with three AOP constructs. The new formalism allows to specify dynamic change-prone BPEL processes. We also define the extended interaction relation which characterizes the concept of correct interaction between the adaptable BPEL process and its client. Then, we propose an algorithm to generate a client which dynamically adapt itself to the service changing.	algorithm;aspect-oriented programming;automata theory;business process execution language;client (computing);loose coupling;process calculus;semantics (computer science);service-oriented architecture;web service;world wide web	Mehdi Ben Hmida;Céline Boutrous-Saab;Serge Haddad;Valérie Monfort;Ricardo Ferraz	2006			computer science;database;distributed computing;world wide web	Web+IR	-39.222305889981044	39.511621128346356	10738
810f77f7a8af557621f4961a69e4a92f2cc633bd	bisimulation congruence for asymmetric \chi ^ \ne -calculus	process algebra bisimulation equivalence;null;bisimulation congruence;l bisimilarities;asymmetric chi ne calculus;tau laws;bisimulation lattice;tau laws bisimulation congruence asymmetric chi ne calculus l bisimilarities bisimulation lattice;process algebra;calculus computer science lattices equations distributed computing;bisimulation equivalence	In this paper a systematic study of bisimilarities on asymmetric chine-processes is carried out. The notion of L-bisimilarities on asymmetric chine-processes is introduced. Twelve distinct L-bisimilarities are derived from all of L-bisimilarities by constructing a bisimulation lattice. For each of these twelve distinct L-bisimilarities, its open version is defined and showed to coincide with it, and then its congruence is presented. Three update laws are proposed and three tau laws are modified. Finally, sound complete equational systems are established for twelve congruences	agile software development;bisimulation;congruence of squares;inductive reasoning;simulation;turing completeness;zeller's congruence	Farong Zhong;Yuxi Fu;Xiaoju Dong	2006	2006 Fifth International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2006.13	process calculus;computer science;programming language	Arch	-10.610263979142776	19.34424275349108	10744
cb25100243a614c07c010c4415a138365a98e6ca	automatic generation of the behavior definition of distributed design tools from task method diagrams and method flux diagrams by diagram composition	developpement logiciel;metodo diagramatico;multiagent system;generacion automatica;base connaissance;logical programming;automatic generation;resolucion problema;software architecture;methode diagrammatique;generation automatique;diagram method;programmation logique;desarrollo logicial;algorithme reparti;software development;base conocimiento;algoritmo repartido;sistema multiagente;programacion logica;distributed algorithm;simulation tool;knowledge modeling;architecture logiciel;problem solving;resolution probleme;systeme multiagent;distributed design;distributed architecture;knowledge base	Developing a knowledge-based version of a greenhouse design and simulation tools, DAMOCIA-Design and DAMOCIA-Sim, we realized the convenience of using multiple diagrammatic notations, and their advantages. First, we used an extension of the Task-Method Diagrams of the CommonKADS methodology to model the relation between the different tasks to be done and the methods we could use to solve them. Method Flux Diagrams model the relation between the different elements composing the method. In order to implement the design software, we selected a distributed architecture, DACAS, that integrated agents using behavior definitions. These are modeled as execution plan diagrams. In this work, we present how these execution plan diagrams can be generated in a general way from task-method and method-flux diagrams included into the knowledge model of the system.		José Fernando Bienvenido;Isabel M. Flores-Parra	2004		10.1007/978-3-540-25931-2_58	block diagram;software architecture;distributed algorithm;knowledge base;activity diagram;interaction overview diagram;computer science;artificial intelligence;software development;distributed computing;story-driven modeling;algorithm	HCI	-39.085018156628216	24.542216025196375	10749
06e1f1eca74f2d99b6faa18dec8d8a5f7a46b4b0	formal validation of pattern matching code	proof assistant;theorem prover;first order;intermediate language;pattern matching;compilation;term rewriting;multi match;verified code	When addressing the formal validation of generated software, two main alternatives consist either to prove the correctness of compilers or to directly validate the generated code. Here, we focus on directly proving the correctness of compiled code issued from powerful pattern matching constructions typical of ML like languages or rewrite based languages such as ELAN, Maude or Tom. In this context, our first contribution is to define a general framework for anchoring algebraic pattern-matching capabilities in existing languages like C, Java or ML. Then, using a just enough powerful intermediate language, we formalize the behavior of compiled code and define the correctness of compiled code with respect to pattern-matching behavior. This allows us to prove the equivalence of compiled code correctness with a generic first-order proposition whose proof could be achieved via a proof assistant or an automated theorem prover. We then extend these results to the multi-match situation characteristic of the ML like languages. The whole approach has been implemented on top of the Tom compiler and used to validate the syntactic matching code of the Tom compiler itself.	automated theorem proving;compiler;correctness (computer science);elan;first-order predicate;java;linear algebra;maude system;pattern matching;proof assistant;rewrite (programming);tom;turing completeness	Claude Kirchner;Pierre-Etienne Moreau;Antoine Reilles	2005		10.1145/1069774.1069792	computer science;theoretical computer science;pattern matching;dead code elimination;first-order logic;redundant code;compiled language;automated theorem proving;proof assistant;programming language;intermediate language;algorithm;code generation	PL	-21.106446977198658	26.608190844761776	10766
a4ff620fc7f8b6bca434fea5fe67aecaf68250e1	a note on fine covers and iterable factors of vas languages	graphe couvrabilite;facteur iterable;complexite calcul;formal languages;computational complexity;recouvrement langage;finite automata;language cover;automate fini;langage vas;petri nets;petri net;reseau petri;langage formel	Vector addition systems (VA%, for short), or equivalently, Petri nets, represent a formalism useful for modeling concurrent systems. Once modeled by a VAS, the behavior of a system can be characterized by the set of all executable sequences, which in turn can be viewed as a language over an alphabet of symbols corresponding to the addition rules of the underlying VAS. It is known that all VAS languages are context-sensitive (assuming that a transition’s symbol cannot be A), and are incomparable with regular and context-free languages. A useful tool for analyzing VAS problems is based on the Karp-Miller coverability graph analysis [ 11. A coverability graph is a generalized reachability graph in which each potentially unbounded position of the VAS is represented by a special symbol “w”. It has been shown in [ 1 ] that for every VAS, its coverability graph is finite. As a result, a VAS is unbounded iff an w occurs in its coverability graph. Aside from their direct application to the analysis of VASs, Karp and Miller’s coverability graphs are also of interest to the language aspect of VASs. The finiteness of coverability graphs suggests a way to approximate VAS	approximation algorithm;concurrency (computer science);context-free language;context-sensitive grammar;executable;formal system;petri net;reachability;vector addition system	Hsu-Chun Yen	1995	Inf. Process. Lett.	10.1016/0020-0190(95)00168-2	combinatorics;computer science;mathematics;finite-state machine;petri net;algorithm	Logic	-5.110415333365004	21.64817947012261	10767
a8759791f051eec59f3605564fa89ec27701ce7e	developing android applications using agent-oriented programming		Designing applications for current generation context aware mobile devices is a complex task. Programmers are required to have a deep knowledge of the underlying systems and APIs in order to complete an application. This paper presents a system that takes advantage of the high level of abstraction common to agent-oriented programming languages to reduce the difficulty in creating context-sensitive applications for Android devices.	abstraction layer;agent-oriented programming;agentspeak;android;application programming interface;context-sensitive grammar;high-level programming language;jade;level of detail;middleware;mobile device;operating system;programmer;smartphone;software agent	Sean Edward Russell;Owen Doyle;Rem W. Collier	2017	2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)	10.1109/ISKE.2017.8258771	humanoid robot;agent-oriented programming;android (operating system);abstraction;distributed computing;mobile device;java;computer science	Robotics	-40.68842708802219	42.00661003662986	10801
d1624e5bd595b4805ddde288d21ff7d56332f831	bidirectional programming and software adaptation: towards a happy marriage		Bidirectional transformations and bidirectional programming have been attracting a lot of attention lately, both in the programming languages community, and in the software engineering community. As bidirectional programming languages are growing more mature, they are getting easier to use for software engineers, more efficient, and more reliable. The strongest argument in favor of bidirectional programming is its ability to provide a synchronization mechanisms between a source and a view, that is guaranteed to be correct by construction. On the other hand, software adaptation is an ability to adapt at run-time to changing user needs, system intrusions or faults, and changing operational environment. In this talk, we shall explain the essence of bidirectional transformation, introduce a powerful language for bidirectional programming, and show how bidirectional programming can provide a powerful mechanism to modularize adaptive software. This mechanism would be very useful not only for reusing a adaptive software for different target systems, but also for maintaining separation of concerns when developing complex adaptive software.		Zhenjiang Hu	2017		10.1109/KSE.2017.8119422	separation of concerns;machine learning;computer science;synchronization;software;reuse;artificial intelligence;distributed computing	SE	-52.718894181044405	29.698499394519107	10825
1126bb8f538b135cccac20f1fe0b934e2ac4c251	hierarchical multiset theories of cardinality-based feature diagrams	trees mathematics reverse engineering set theory software product lines;cardinality based feature diagrams;feature model reverse engineering hierarchical multiset theories cardinality based feature diagrams feature modeling software product lines feature tree feature diagram languages diagram retrieval feature model management;hierarchical multiset theory;flat multiset theory;hierarchical multiset theory cardinality based feature diagrams flat multiset theory;computational fluid dynamics gears semantics manuals vehicles axles brakes	Feature modeling is the most common approach to specify software product lines. The main part of a feature model is a special tree of features called a feature diagram. Cardinality-based feature diagrams provide the most expressive tool among the current feature diagram languages. The most common characterization of the semantics of a cardinality-based diagram is the set of flat multisets over features satisfying the constraints. However, this semantics provides a poor abstract view of the diagram. We address this problem by proposing another multiset theory for the diagram, called the hierarchical theory. We show that the theory captures all information of the diagram so that one can retrieve the diagram from its theory. This provides us with a theoretical framework for addressing some challenging issues in feature modeling, e.g., feature model management and reverse engineering of feature models.	diagram;feature model;reverse engineering;software product line	Aliakbar Safilian;T. S. E. Maibaum	2016	2016 10th International Symposium on Theoretical Aspects of Software Engineering (TASE)	10.1109/TASE.2016.14	system context diagram;elementary diagram;algorithm;feature model	SE	-29.522497398962432	15.565077975839118	10830
8d530c1cc31b5096c83869f2bd61b92bada304cd	a process algebraic framework for service coordination	algebraic specification;service coordination modeling;protocols;multiagent system;information systems;probability density function;computational intelligence;web services algebraic specification multi agent systems process algebra protocols;formal process algebraic framework;data mining;software engineering;finite state process algebra;multi agent systems;algebra;generic interaction protocol;business;web services;algebraic specification service coordination modeling multiagent system formal process algebraic framework finite state process algebra web service coordination specification generic interaction protocol;protocols web services algebra proposals computational intelligence informatics information systems software engineering decision making environmental management;agent systems;interaction protocol;informatics;process algebra;finite state processes;environmental management;proposals;context modeling;context;web service coordination specification	We propose a formal framework based on finite state process algebra for modeling service coordination in agent systems. The model is inspired by WS-coordination specification. Several participant agents can coordinate their actions towards reaching a semantically consistent outcome in a distributed activity. The model defines coordinator and participant agents and their generic interaction protocols and allows integration of application specific protocols by means of protocol handlers. The approach is demonstrated by a sample model showing how coordination is achieved between a set of agents involved in contracting activities.	backward chaining;contract net protocol;linear algebra;local tangent space alignment;one-to-one (data model);process calculus;ws-coordination	Amelia Badica;Costin Badica;Elvira Popescu;Mihnea Scafes	2009	2009 5th International Symposium on Applied Computational Intelligence and Informatics	10.1109/SACI.2009.5136303	web service;communications protocol;probability density function;process calculus;computer science;systems engineering;knowledge management;artificial intelligence;theoretical computer science;computational intelligence;data mining;context model;informatics;information system	Embedded	-44.0329326186172	18.575470175019934	10837
6c06797e0e23dbe21fc9983d80662664a9bdaf68	new decidability results for fragments of first-order logic and application to cryptographic protocols	automatic proving;protocole transmission;cryptographic protocol;demostracion automatica;demonstration automatique;protocolo transmision;theorie equationnelle;logique ordre 1;reecriture;number;decidibilidad;rewriting;nombre;decidabilite;equational theory;first order logic;numero;reescritura;decidability;logica orden 1;teoria ecuacional;transmission protocol	We consider a new extension of the Skolem class for first- order logic and prove its decidability by resolution techniques. We then extend this class including the built-in equational theory of exclusive or. Again, we prove the decidability of the class by resolution techniques. Considering such fragments of first-order logic is motivated by the auto- matic verification of cryptographic protocols, for an arbitrary number of sessions; the first-order formalization is an approximation of the set of possible traces, for instance relaxing the nonce freshness assumption. As a consequence, we get some new decidability results for the verification of cryptographic protocols with exclusive or.	first-order logic;first-order predicate	Hubert Comon-Lundh;Véronique Cortier	2003		10.1007/3-540-44881-0_12	decidability;discrete mathematics;numero sign;rewriting;computer science;first-order logic;cryptographic protocol;mathematics;grammatical number;algorithm	Logic	-7.820411766313278	20.71501356245118	10845
15ef3a987f58f3c24127eae0590cb5900a2f56cd	dynamic updates of non-monotonic knowledge bases	dynamic program;program generation;dynamic logic;non monotonic reasoning;logic programs;knowledge representation;knowledge base	In this paper we investigate updates of knowledge bases represented by logic programs. In order to represent negative information, we use generalized logic programs which allow default negation not only in rule bodies but also in their heads. We start by introducing the notion of an update P U of one logic program P by another logic program U. Subsequently, we provide a precise semantic characterization of P U , and study some basic properties of program updates. In particular, we show that our update programs generalize the notion of interpretation update. We then extend this notion to compositional sequences of logic programs updates P1 P2 ; de®ning a dynamic program update, and thereby introducing the paradigm of dynamic logic programming. This paradigm signi®cantly facilitates modularization of logic programming, and thus modularization of non-monotonic reasoning as a whole. Speci®cally, suppose that we are given a set of logic program modules, each describing a dierent state of our knowledge of the world. Dierent states may represent dierent time points or dierent sets of priorities or perhaps even dierent viewpoints. Consequently, program modules may contain mutually contradictory as well as overlapping information. The role of the dynamic program update is to employ the mutual relationships existing between dierent modules to precisely determine, at any given module composition stage, the declarative as well as the procedural semantics of the combined program resulting from the modules. Ó 2000 Elsevier Science Inc. All rights reserved.	abductive reasoning;belief revision;computer science;continuous design;dynamic programming;hypertext transfer protocol;knowledge base;knowledge representation and reasoning;logic programming;morgan;non-monotonic logic;program transformation;programming paradigm;quantum information;software agent;software maintenance;systems modeling;temporal database;top-down and bottom-up design;universal networking language;well-founded semantics;whole earth 'lectronic link	José Júlio Alferes;João Leite;Luís Moniz Pereira;Halina Przymusinska;Teodor C. Przymusinski	2000	J. Log. Program.	10.1016/S0743-1066(99)00065-5	dynamic logic;knowledge base;description logic;computer science;artificial intelligence;theoretical computer science;data mining;programming language;multimodal logic;algorithm;autoepistemic logic	AI	-17.452867313557142	8.766890341788208	10847
23a9a5b58a824c0cb5f3f2762298714ed31d06f6	transparent authentication scheme with adaptive biometrie features for iot networks	internet of things iot;machine learning;support vector machine;transparent authentication;security	With the comprehensive evolution of information communication technologies on mobile sensing objects, versatile ubiquitous networks embedded with specific-purpose sensors and intelligent wearable devices have promptly been developed and deployed, called the Internet of Things (IoT). On account of the popularity of IoT, the security issues have been promptly focused due to potential threats from IoT architectures. In consideration of the heterogeneous network property of IoT, in this paper we propose an authentication system which applies machine learning techniques to extract user bio-features as authentication tokens and transparently performs continual or real-time entity verification in the back-ground without the user's notices.	authentication;british informatics olympiad;embedded system;internet of things;machine learning;real-time transcription;sensor;wearable technology	Kuo-Hui Yeh;Chunhua Su;Chien-Lung Hsu;Wayne Chiu;Yu-Fan Hsueh	2016	2016 IEEE 5th Global Conference on Consumer Electronics	10.1109/GCCE.2016.7800550	computer science;internet privacy;world wide web;computer security	Mobile	-45.6912537201481	58.65235556156155	10848
3e98c8208ad8185440131bea223c3f5f4149950c	characterizing thread placement in the ibm power7 processor	ibm power7;multi threading;performance evaluation;resource allocation;desktop parallel applications ibm power7 processor design thread level parallelism paradigms performance improvement hardware resource sharing running threads thread placement configuration characterization target metric improvement execution time reduction;smt thread placement ibm power7 resource sharing;resource allocation multiprocessing systems multi threading performance evaluation;resource sharing;thread placement;smt;multiprocessing systems;context benchmark testing pipelines instruction sets hardware resource management registers	There is a clear trend in current processor design towards the combination of several thread level parallelism paradigms on the same chip, exemplified by processors such as the IBM POWER7. In those processors, the way threads are assigned to different hardware contexts, denoted thread placement, plays a key role in improving overall performance. In this paper we analyze the thread placement problem in the IBM POWER7 processor. Under each thread placement setup we analyze in detail how hardware resources are shared among running threads. We show to which extent a software designer can characterize an application on the POWER7 and based on that characterization, select the best thread placement configuration to improve a target metric. Our results show that a 54% reduction in execution time can be obtained (11.2% on average) when running pairs of desktop parallel applications under the appropriate thread placement.	central processing unit;desktop computer;parallel computing;processor design;run time (program lifecycle phase);software design;task parallelism;thread (computing)	Stelios Manousopoulos;Miquel Moretó;Roberto Gioiosa;Nectarios Koziris;Francisco J. Cazorla	2012	2012 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2012.6402916	shared resource;computer architecture;parallel computing;real-time computing;win32 thread information block;multithreading;resource allocation;computer science;operating system;fiber;thread safety	Arch	-8.401171610633417	49.495963577493676	10858
306ffd22c5e4e5355bc62fddf96a82559df367ef	grid-enabled workflow management system based on bpel	grid workflow;workflow management;business process execution language;wsrf;grid applications;web service;satisfiability;workflow system;web service resource framework;grid service;workflow;workflow management system;bpel;cluster model;high throughput;grid computing	A grid-enabled workflow management system provides a set of tools to facilitate building high-level grid application services by orchestrating low-level grid services. BPEL (Business Process Execution Language) is the de facto standard in the Web service community. The purpose of this paper is to design and implement a workflow management system based on BPEL in a grid environment. The approach to integrating BPEL with WSRF (Web Services Resource Framework), is to put forward a simple cluster model of BPEL engines and application component oriented workflow defining respectively to satisfy the specific requirements in a grid environment, such as stateful interactions, dynamic service binding, high throughput and scalability, and friendly workflow defining. Finally, a prototype of a workflow system implemented in ChinaGrid Supporting Platform (CGSP) is also given.	business process execution language;high- and low-level;interaction;management system;prototype;requirement;scalability;stateful firewall;throughput;web services resource framework;web service;world wide web	Ruyue Ma;Yongwei Wu;Xiangxu Meng;Shijun Liu;Li Pan	2008	IJHPCA	10.1177/1094342007086224	workflow;xpdl;business process execution language;computer science;workflow management coalition;database;distributed computing;windows workflow foundation;world wide web;workflow management system;workflow engine;workflow technology	HPC	-42.40785624665238	12.461214082114749	10860
0833c77770080dd9f20b710f114d646bbb0514c9	database replication techniques: a three parameter classification	file servers;transaction termination database replication techniques 3 parameter classification workstation clusters performance lazy replication protocols consistency eager replication protocols cost server architecture server interaction;perforation;database replication;cluster of workstations;data replication;workstation clusters;file servers replicated databases workstation clusters;replicated databases;protocols transaction databases costs laboratories distributed databases space exploration information systems workstations software systems availability	Data replication is an increasingly important topic as databases are more and more deployed over clusters of workstations. One of the challenges in database replicatio n is to introduce replication without severely affecting per formance. Because of this difficulty, current database product s use lazy replication, which is very efficient but can compromise consistency. As an alternative, eager replication guarantees consistency but most existing protocols have a prohibitive cost. In order to clarify the current state of th e art and open up new avenues for research, this paper analyses existing eager techniques using three key parameters. In our analysis, we distinguish eight classes of eager repli cation protocols and, for each category, discuss its requir ements, capabilities, and cost. The contribution lies in sho wing when eager replication is feasible and in spelling out the different aspects a database replication protocol must account for.	backup;database;interaction;lazy evaluation;nv network;optimistic replication;overhead (computing);replication (computing);requirement;statistical classification;windows update;workstation	Matthias Wiesmann;André Schiper;Fernando Pedone;Bettina Kemme;Gustavo Alonso	2000		10.1109/RELDI.2000.885408	multi-master replication;computer science;operating system;database;distributed computing;replication	DB	-23.69976313223431	49.19889114419269	10868
f37371e48f18cc6f923a356cb194165909c31d05	normalized curve signatures for shape representation and retrieval		A computer system having a power monitoring device places a write-back cache memory into a write-through mode upon detection of a low-battery condition or a user request. Under write-through mode, the cache memory need not be flushed every time a suspend mode of the computer system is entered. Thus significant power is saved during typical portable computer operations.	electronic signature	Marius Brezovan;Dumitru Dan Burdescu;Eugen Ganea;Liana Stanescu	2009			active shape model;computer hardware;portable computer;cpu cache;shape analysis (digital geometry);normalization (statistics);computer science	Vision	-13.610584089981588	51.96459078421853	10870
68813a71a8e526a887b46c79dd78b4aa2b2704d3	optimizing transformations of stencil operations for parallel cache-based architectures	optimizing transformation;poisson equation;iterative solver;general and miscellaneous mathematics computing and information science;transformations;program transformation;data partitioning;iterative methods;computer architecture;parallel stencil operations;optimization parallel stencil operations;task scheduling;iteration method;99 general and miscellaneous mathematics computing and information science;parallel processing;jacobi method;parallel cache based architectures	This paper describes a new technique for optimizing serial and parallel stencil-and stencil-like operations for cache-based architectures. This technique takes advantage of the semantic knowledge implicitly in stencil-like computations. The technique is implemented as a source-to-source program transformation; because of its speci-city it could not be expected of a conventional compiler. Empirical results demonstrate a uniform factor of two speedup. The experiments clearly show the beneets of this technique to be a consequence, as intended, of the reduction in cache misses. The test codes are based on a 5-point stencil obtained by the discretization of the Poisson equation and applied to a two-dimensional uniform grid using the Jacobi method as an iterative solver. Results are presented for a 1-D and 2-D tiling for a single processor. For the parallel case both blocking and non-blocking communication have been tested. However, the parallel case is not discussed here.	blocking (computing);cpu cache;code;computation;discretization;experiment;iterative method;jacobi method;non-blocking algorithm;optimizing compiler;parallel algorithm;program transformation;solver;speedup;stencil buffer;tiling window manager	Federico Bassetti;Kei Davis	1999			parallel processing;parallel computing;computer science;theoretical computer science;operating system;distributed computing;iterative method;programming language	HPC	-6.4153555270206075	41.38747152559039	10876
e87a71a37e3cb792b6aae34b2adcc8393f516866	discriminability analysis of supervision patterns by net unfoldings		In this paper, we are interested in the discriminability of supervision patterns, in discrete event systems (DES). Discriminability — as opposed to diagnosability — is the possibility to detect the exclusive occurrence of a particular behavior of interest — called the supervision pattern. To this end, we propose to adapt the classical twin-plant approach to Petri nets unfolding. The usage of unfoldings permits us to avoid the combinatorial explosion associated with marking graphs. The method can also be used to solve the classical problem of discrete event systems' diagnosability.		Houssam-Eddine Gougam;Audine Subias;Yannick Pencolé	2014		10.3182/20140514-3-FR-4046.00136	discrete mathematics;artificial intelligence;mathematics;algorithm	Logic	-7.826582359908048	23.756751641265403	10884
22c4e079b1306dd06fd8850baa5714311ab23931	real-time qos monitoring from the end user perspective in large scale social networks	social networking services;real time monitoring;quality of services;sla monitoring;qos monitoring;social networks;continuous monitoring;apis;external sns;end user perspectives	Social networking (SN) activities account for a major fraction of the time that internet users collectively spend on the web and they represent a valuable source of information and services. The SocIoS project defined an API that enables the aggregation of data and functionality of underlying SN services (SNS) APIs and allows their combination, so to build new application workflows and/or to complement existing ones. While this scheme provides SN users with a tool that has a dramatic potential in terms of productivity, it also introduces a dependency on external SNS, over which the SocIoS API end user has limited control. In this context, the availability of a dependable (i.e., unbiased, reliable, and timely) facility for continuous monitoring of the QoS being actually delivered by external SNS is thus of paramount importance. Such a facility is implemented by the QoS-MONaaS component, a portable architecture developed within the context of the SRT-15 FP7 project.	quality of service;real-time transcription;social network	Luigi Coppolino;Salvatore D'Antonio;Luigi Romano;Fotis Aisopos;Konstantinos Tserpes	2017	IJCSE	10.1504/IJCSE.2017.10003825	real-time computing;simulation;application programming interface;computer science;operating system;database;distributed computing;world wide web;computer security;social network	Metrics	-24.823026147970236	59.947801457315556	10886
2d2a9c36c1312260e98ddde14503fa92e64a0559	the future of energy markets and the challenge of decentralized self-management	large scale adaptive system;local goal;adaptive communication structure;power grid;decentralized self-management;p2p research;multi-agent research;local management;communication structure;energy market	Complex, intelligent, distributed systems in dynamic environments, such as the power grid need to be designed to adapt autonomously. Self-management, in particular of large scale adaptive systems such as the power grid, is necessarily distributed. Agent and peer-to-peer based decentralized self-management can change the future of energy markets in which the power grid plays a core role. Assuming that both consumers and providers of energy are autonomous systems, represented by software agents or peers capable of self-management, virtual organizations of systems can emerge and adapt when necessary. Communication structures between systems, e.g., hierarchical or clustered organizations, can emerge, organizations between and within which systems can choose to cooperate and coordinate their actions, or compete. Overlay structures (as defined within p2p research) define such adaptive communication structures, multi-agent research provides interaction patterns. Global goals are achieved by local management on the basis of local goals and knowledge. The appropriate delegation of managerial responsibility determines the control structure. Aggregate information differs depending on the position of a system in an organization, the aggregation mechanisms and policies.	adaptive system;aggregate function;autonomic computing;autonomous system (internet);control flow;distributed computing;multi-agent system;peer-to-peer;self-management (computer science);software agent	Frances M. T. Brazier;Elth Ogston;Martijn Warnier	2009		10.1007/978-3-642-31809-2_9	simulation;knowledge management;operations management;business	AI	-42.59102941988927	18.416163758632074	10889
bc343bd992ed59f552cc8b6518488a8747ab5a9b	a logical characterization of timed regular languages	constraint linear temporal logic;dense time temporal logic;timed automata	CLTLoc (Constraint LTL over clocks) is a quantifier-free extension of LTL allowing variables behaving like clocks over real numbers. CLTLoc is in PSPACE   [1]   and its satisfiability can polynomially be reduced to a Satisfiability Modulo Theories (SMT) problem, allowing a feasible implementation of a decision procedure. We used CLTLoc to capture the semantics of metric temporal logics over continuous time, such as Metric Interval Temporal Logic (MITL), resulting in the first successful implementation of a tool for checking MITL satisfiability  [2]  ;   [3] . In this paper, we assess the expressive power of CLTLoc, by comparing it with various temporal formalisms over dense time. We restrict the analysis to  well initialized  models of formulae where the value of all clocks in the origin is either 0 or equal to a positive real constant. Under this assumption, when interpreted over timed words, the class of timed languages defined by CLTLoc formulae coincides with the class defined by Timed Automata. We also define a timed monadic first order logic of order, extending the one introduced by Kamp, which is expressively equivalent to CLTLoc for the class of timed languages that are defined by well initialized models.	descriptive complexity theory;regular expression;regular language	Marcello M. Bersani;Matteo Rossi;Pierluigi San Pietro	2017	Theor. Comput. Sci.	10.1016/j.tcs.2016.07.020	discrete mathematics;linear temporal logic;theoretical computer science;mathematics;timed automaton;algorithm	Theory	-11.282004349919005	24.419090202407556	10890
18040ba2a9115eff8a42564acd1d41a8c9b61f80	a uml-based quantitative framework for early prediction of resource usage and load in distributed real-time systems	distributed system;resource overuse detection;uml;behavior modeling;load forecasting;distributed real time system;network traffic;control flow;software life cycle;distributed systems;resource usage prediction;sequence diagram;load analysis;real time systems	This paper presents a quantitative framework for early prediction of resource usage and load in distributed real-time systems (DRTS). The prediction is based on an analysis of UML 2.0 sequence diagrams, augmented with timing information, to extract timed-control flow information. It is aimed at improving the early predictability of a DRTS by offering a systematic approach to predict, at the design phase, system behavior in each time instant during its execution. Since behavioral models such as sequence diagrams are available in early design phases of the software life cycle, the framework enables resource analysis at a stage when design decisions are still easy to change. Though we provide a general framework, we use network traffic as an example resource type to illustrate how the approach is applied. We also indicate how usage and load analysis of other types of resources (e.g., CPU and memory) can be performed in a similar fashion. A case study illustrates the feasibility of the approach.	central processing unit;control flow;network traffic control;real-time clock;real-time computing;real-time operating system;real-time transcription;sequence diagram;software release life cycle;unified modeling language	Vahid Garousi;Lionel C. Briand;Yvan Labiche	2008	Software & Systems Modeling	10.1007/s10270-008-0099-7	sequence diagram;behavioral modeling;unified modeling language;real-time computing;simulation;computer science;systems engineering;software engineering;programming language;control flow;software development process	Embedded	-43.34041005285046	34.23822570779703	10895
23bb0e92dc2a166c2d7c5bfff1d6aac3c62c7192	strong, weak and branching bisimulation for transition systems and markov reward chains: a unifying matrix approach	matrix theory	We first study labeled transition systems with explicit successful termination. We establish the notions of strong, weak, and branching bisimulation in terms of boolean matrix theory, introducing thus a novel and powerful algebraic apparatus. Next we consider Markov reward chains which are standardly presented in real matrix theory. By interpreting the obtained matrix conditions for bisimulations in this setting, we automatically obtain the definitions of strong, weak, and branching bisimulation for Markov reward chains. The obtained strong and weak bisimulations are shown to coincide with some existing notions, while the obtained branching bisimulation is new, but its usefulness is questionable.	bisimulation;markov chain	Nikola Trcka	2009			combinatorics;discrete mathematics;mathematics;algorithm;matrix	Logic	-10.075026458756893	20.79550480259255	10898
f25cacfaaf2d63d7ad9b0c4075124ebe9d83c0c0	programming agents with emotions	agent oriented programming	This paper presents the syntax and semantics of a simplified version of a logic-based agent-oriented programming language to implement agents with emotions. Four types of emotions are distinguished: happiness, sadness, anger and fear. These emotions are defined relative to agent’s goals and plans. The emotions result from the agent’s deliberation process and influence the deliberation process. The semantics of each emotion type is incorporated in the transition semantics of the presented agent-oriented programming language.	agent-oriented programming;computational semantics;intelligent agent;interpreter (computing);mental state;programming language;robot;sadness;selection rule;transition system	Mehdi Dastani;Christiaan Floor;John-Jules Ch. Meyer	2006		10.1007/978-3-319-12973-0_4	computer science;artificial intelligence;well-founded semantics	AI	-23.846853676318748	17.605071502827606	10900
87374f4d183394d1884e678e38782bbe62b352f4	security in distributed and client/server systems - a management view	client server	There is a trend at present in both the public and private sectors towards flatter organizational structures, in which groups and individuals are given greater autonomy and delegated financial powers. This makes the business more responsive to customer demands, improves productivity and promotes accountability. In this environment, cooperation between groups may be achieved using information technology it is the use of technology which provides the glue to bind the organization into a fully functional unit.	client (computing);client–server model;computer security;execution unit;ian cullimore;information security;information system;requirement;security controls;server (computing)	Ian M. Symonds	1994	Computers & Security	10.1016/0167-4048(91)90133-X	client;smart client;reverse proxy;log shipping;computer science;internet authentication service;appleshare;client-side;database;server-side;fat client;distributed system security architecture;world wide web;windows server;application server;client–server model;computer network;game client;server farm;remote evaluation	HCI	-47.41674091954016	57.823018017563015	10912
06ab32f1c1a00e0e96318a65e942c07a5d9e81fd	system level design from hw/sw to memory for embedded systems		Given that the population is aging, it is crucial to develop technologies which will not only help the elderly to age in place, but also live in place with independent and healthy lifestyle. Ambient Assisted Living (AAL) environments can help the elderly and people with functional diversity by anticipating their needs in specific situations and acting proactively in order to properly assist them in performing their activities of daily living (ADLs). Since the users needs tend to be very diverse in regard to functioning and disability levels, it is crucial to have personalized services capable of providing tailored assistance to a user based on their unique preferences, requirements, and desires. This paper introduces the ontology named AATUM (Ambient Assistive Technology User Model), to be adopted by systems whose goal is to enhance user quality of life within ALL environments through service personalization. Its main feature is the use of The International Classification of Functioning, Disability and Health (ICF) to model the user’s functioning and disability levels in a consistent and internationally comparable way. The use of the proposed ontology is illustrated through its application in two different case studies.	atm adaptation layer;assistive technology;embedded system;home automation;level design;ontology (information science);personalization;personally identifiable information;population;requirement;service-orientation;service-oriented device architecture;shattered world;user modeling;user profile;windows firewall	Marcelo Götz;Gunar Schirner;Marco A. Wehrmeister;Mohammad Abdullah Al Faruque;Achim Rettberg	2015		10.1007/978-3-319-90023-0		HCI	-41.362728017981844	44.325133375608644	10916
99e009747d88f89e92600bf93a73a47554e64b00	log-compact r-tree: an efficient spatial index for ssd	spatial index;indexation;tree structure	R-Tree structure is widely adopted as a general spatial index with the assumption that the deployed system is equipped with magnetic hard disk. While the application of SSD becomes more and more popular, traditional optimization of R-Tree structure on SSD is much less desirable than that on magnetic hard disk. Existing flash-aware index approaches employ log mechanism to reduce random writes at a cost of large amount of read. A novel index named Log Compact R-Tree (LCRtree) is proposed in this paper. Distinguished from previous attempts, compacted log is introduced to combine newly arrival log with origin ones on the same node, which renders great decrement of random writes with at most one additional read for each node access. Extensive experiments illustrate that the proposed LCR-Tree can achieve up to 3 times benefit against existing approaches.	experiment;hard disk drive;increment and decrement operators;input/output;mathematical optimization;overhead (computing);r-tree;redo log;rendering (computer graphics);solid-state drive;spatial database;synthetic intelligence;tree structure	Yanfei Lv;Jing Li;Bin Cui;Xuexuan Chen	2011		10.1007/978-3-642-20244-5_20	real-time computing;computer science;data mining;database;tree structure;spatial database	DB	-13.23948058392551	54.40311153233531	10920
dac7cbcc39d657a0a8410916a67db44b29198f85	towards a theory of software design: timeless principles of software system design.	design principle;axiomatic design;design practice;software systems;incomplete information;design and implementation;software development;software design	The design and implementation of a software system is the result of many interwoven sequences of decisions. Often, these decisions are made under less than ideal conditions of uncertainty and/or incomplete information. Furthermore, many of these decisions do not have clear “right/wrong” answers — they are value judgments dependent upon the current knowledge of the system and its intended domain. Unfortunately, many of the “design principles” of software development are either specific to a particular aspect of the design or applicable only in a particular implementation paradigm. This paper presents a set of axiomatic design principles, based on established design practices from other disciplines, and illustrates their applicability to software system design. Several wellknown specific software design principles are derived from this fundamental set. Preliminary results of an ongoing study using these principles are discussed to demonstrate their value in guiding judgment decisions made with incomplete knowledge.	axiomatic design;programming paradigm;software design;software development;software system;systems design;theory	David R. Wright	2007			personal software process;verification and validation;axiomatic design;computer science;package development process;software design;social software engineering;component-based software engineering;software development;software design description;object-oriented design;software engineering;software construction;design education;software walkthrough;resource-oriented architecture;software deployment;computer-aided software engineering;complete information;goal-driven software development process;software requirements;software system;software peer review	SE	-60.369291491458604	27.92221732035685	10923
654b63176ac82506ed74ce12b98e0ea6e1b7af39	evaluation of a local adaptive protocol for distributed discrete event simulation	local adaptation;queueing system;network of workstation;discrete event simulation	The performance of a local adaptive protocol for distributed discrete event simulation is evaluated. This protocol allows each process to adapt at runtime to its environment on a per channel basis with the possible range of behavior varying from conservative to optimistic. This evaluation includes a comparison of the performance of the algorithm with that of both a conservative and an optimistic protocol, as well as, an examination of a number of unique characteristics of the algorithm. Results from simulations using a network of workstations on a closed queueing system are presented. The results demonstrate the importance of various protocol features and show that it performs better than either the conservative or optimistic protocols in certain simulations.	algorithm;computer cluster;optimistic concurrency control;queueing theory;run time (program lifecycle phase);simulation;workstation	Donald O. Hamnes;Anand R. Tripathi	1994		10.1109/ICPP.1994.106	parallel computing;real-time computing;simulation;computer science;discrete event simulation;distributed computing	Embedded	-15.466884591435898	58.52245709413158	10927
ff4fa82c03cae217168d82a750c1093bf3d41358	formal information model for representing production resources		This paper introduces a concept and associated descriptions to formally describe physical production resources for modular and reconfigurable production systems. These descriptions are source of formal information for (automatic) production system design and (re-)configuration. They can be further utilized during the system deployment and execution. The proposed concept and the underlying formal resource description model is composed of three different description levels, namely Abstract Resource Description (ARD), Resource Description (RD) and Resource Instance Description (RID), each having different scope and objectives. This paper discusses in details the content and differences between these description levels.	information model	Niko Siltala;Eeva Järvenpää;Minna Lanz	2016		10.1007/978-3-319-51133-7_7	knowledge management	NLP	-50.02680969608069	16.816951879711944	10942
f7617ffc3937286c2ce5276d19843d56f679691c	an ontology for actioncenter-oriented collaboration platforms	groupware;owl;collaborative work;collaborative application;cascading style sheets;knowledge standardization ontology actioncenter oriented collaboration technology platforms collaboration expertise collaboration tools collaborative work practices facilitator in a box strategy collaboration patterns high value recurring tasks owl ambiguity reduction knowledge sharing promotion knowledge reuse;ontologies artificial intelligence;ontologies collaborative work cascading style sheets owl collaborative software communities;knowledge representation languages;side effect;knowledge sharing;ontologies artificial intelligence groupware knowledge representation languages;ontologies;communities;collaborative software	There is a growing number of collaboration technology platforms that support rapid development and deployment of Action Center applications -- collaborative applications that encapsulate both collaboration expertise and tools for effective collaborative work practices using the facilitator-in-a-box strategy. This strategy enables instantiation and diffusion of state-of-the-art collaboration patterns for high-value recurring tasks. A side effect of the growing number of platforms is the potential for incompatibilities among Action Centers that can reduce interoperability, knowledge sharing and reuse. We present an ontology for Action Center-oriented collaboration platforms that formalizes key concepts of the approach using OWL. The resulting ontology can reduce ambiguities and promote knowledge sharing, reuse and standardization.	interoperability;software deployment;universal instantiation;web ontology language	Azamatbek Mametjanov;Cuong Nguyen;Douglas Kjeldgaard;Robert O. Briggs	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.108	computer science;knowledge management;artificial intelligence;data mining;database;management;world wide web;collaborative software	HCI	-53.666645816615166	13.9273973997191	10946
3b5a6d6ee75df5c200cdade043ef1a3ff98faa41	ragobot: a new platform for wireless mobile sensor networks	mobile sensor network	We present Ragobot, a fully-featured validated platform for use in mobile sensor networks. Ragobot is a robot of small dimensions with features that surpass those provided by many other robots in this category. Ragobot hardware and software are implemented with modularity as one of the main considerations; therefore, these are easy to upgrade and customize according to the needs of each specific application. Moreover, we present Ragoworld, a controlled physical space for the development and evaluation of mobile sensor network algorithms.		Jonathan Friedman;David Lee;Ilias Tsigkogiannis;Sophia Wong;Dennis Chao;David Levin;William J. Kaiser;Mani B. Srivastava	2005		10.1007/11502593_43	embedded system;real-time computing;mobile search;simulation;computer science;key distribution in wireless sensor networks;mobile station;mobile wireless sensor network;mobile computing;visual sensor network	Mobile	-38.73019817928049	46.76193627838506	10950
6ce962aed1350777ee8a08824cf738cb6537f24b	a framework and generator for large parameterized feature models		The customer oriented individualization of products is getting more and more important for the production industry. Especially in car manufacturing industry we can observe a dramatically increasing number of product variants not only related to different car concepts but also concerning different functionality as for example in car entertainment. In order to cope with this increasing complexity in terms of product features and their interrelationships, manufacturers more and more build on a formal approach called feature modeling that allows for a formal analysis of the specified variability artifacts with the help of specialized proving engines as for example SAT-solvers. The development of such proving engines and their test is quite complicated also due to the fact that manufacturers do not disclose the real development data for understandable reasons. Thus, a framework is needed that enables the proving engine developers to test their engines on nearly real data and to show the potential and possibilities of their engines without having the real development data at hand. This paper presents a framework for generating especially large parameterized feature models used for load testing and benchmarking feature model analysis tools, as well as two usage scenarios: the first one runs a typical benchmark with large feature models on two versions of the theorem prover SPASS, the second shows the integration of the generator in a client-server environment where its functionality is hosted on a website, i.e. using the browser as a frontend working on tablets and modern smartphones.		Robert Rößger;Georg Rock	2013		10.3233/978-1-61499-302-5-333	simulation;feature model;data mining;in-car entertainment;parameterized complexity;benchmarking;automated theorem proving;project commissioning;manufacturing;computer science;load testing	Vision	-51.53690570294071	42.77200396997693	10979
a3881d2d310ef1910b93b7e232ac938330884102	translating z to alloy	z specification;brief overview;alloy notation;error-prone activity;visualising alloy model;discusses alternative translation approach;complex construct;difficult task;interactive tool;alloy analyzer	Few tools are available to help with the difficult task of validating that a Z specification captures its intended meaning. One tool that has been proven to be useful for validating specifications is the Alloy Analyzer, an interactive tool for checking and visualising Alloy models. However, Z specifications need to be translated to Alloy notation to make use of the Alloy Analyzer. These translations have been performed manually so far, which is a cumbersome and error-prone activity. The aim of this paper is to explore to what extent this process can be automated. The paper identifies a subset of Z that can be straightforwardly translated to Alloy, and the translation for this subset is formalised. More complex constructs, like schemas, are harder to translate. The paper gives a brief overview of the problems, and discusses alternative translation approaches.	alloy (specification language);alloy analyzer;cognitive dimensions of notations	Petra Malik;Lindsay Groves;Clare Lenihan	2010		10.1007/978-3-642-11811-1_28	alloy analyzer;computer science;data mining;engineering drawing	SE	-46.79467852973006	26.655014454096516	10980
819eb7c9c3c08a9f674fda0b8c365a2bfbb7bb78	a systematic map on verifying and validating software process simulation models		Verification and Validation (Vu0026V) is a critical step in software process modelling to secure the modelu0027s quality and credibility. Software Process Simulation Models (SPSMs) that are based on descriptive process models offer the executability that is able to demonstrate the dynamic changes of software process over time. The Vu0026V of process simulation models go beyond static process models and turn to be more complex and challenging to software modelers. This study aims to identify what aspects of process simulation models are verified and validated by using which Vu0026V methods in what conditions in software engineering research. We conducted a systematic literature review (mapping study) on the studies of software process simulation that report of their Vu0026V activities. We identified 72 relevant studies from a pool of 331 papers on SPSM until 2015. These studies can be mapped to ten Vu0026V methods applied for five aspects of process models to be verified and validated, i.e., syntactic quality, semantic quality, pragmatic quality, performance, and value. A systematic map is presented to illustrate the relationships between the identified Vu0026V methods and their supporting aspects of process models. This mapping will provide the community reference value when developing, verifying, and validating software process (simulation) models.	abstraction layer;exponent bias;map;process modeling;simulation;software development process;software engineering;systematic review;vagueness;verification and validation	Haojie Gong;He Zhang;Dexian Yu;Bohan Liu	2017		10.1145/3084100.3084106	software development process;data mining;process simulation;verification and validation;software;software verification and validation;verification and validation of computer simulation models;systems engineering;software process simulation;computer science;process modeling	SE	-59.70248507051968	28.417542651367533	10984
be67dc5ab6513c3da6a42fb43ea301ea349d5a82	improving performance of optimistic simulation for distributed simulation system using speculative computation	synchronisation discrete event simulation;weighted moving time window optimistic simulation distributed simulation system time management parallel discrete event simulation synchronization management global virtual time gvt speculative computation mechanism breathing time bucket algorithm;event horizon breathing time bucket speculative computation synchronization mechanism time management;synchronization computational modeling prediction algorithms discrete event simulation computers parallel processing analytical models	Synchronization and Time management are the important mechanisms for the parallel discrete event simulation. Time management ensures events are executed in the correct order without any repeated execution. Synchronization management is important in ensuring faster execution of synchronization procedure and while reducing the wait time for synchronization. In this paper, we studied about predicting the synchronization points, Global Virtual Time (GVT) using a speculative computation mechanism. For our study, we considered Breathing Time Bucket algorithm and applied the prediction mechanism controlled over weighted moving time window. Finally, we analytically compute and analyze the performance of the algorithm.	algorithm;computation;distributed computing;iteration;simulation;speculative execution;synchronization (computer science);x86 virtualization	Murugadoss Venu;Inwhee Joe	2014	2014 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2014.6983173	real-time computing;computer science;theoretical computer science;distributed computing	HPC	-15.258711297196877	59.52319872575228	10990
3ba716570b28ca2c7dfdc7142f5fffa0abc6191f	workflow operational assurance platform for security-by-design certified service-based coalitions	highly dynamic coalitions;workflow operational assurance platform coalition operation assert4soa project service security certificates highly dynamic coalition concept hdc concept security properties coalition workflow design certification authority composite service sme small and medium size enterprises service based business integration security by design certified service based coalitions;certification;small to medium enterprises;security by design certified coalition;computational modeling;workflow security assurance;variable speed drives;abstracts;business;web services;authorization;web services certification security of data small to medium enterprises;security of data;service security certificate;abstracts variable speed drives certification business computational modeling authorization	The concept of Dynamic Coalitions (DCs) provides a scalable approach for service-based business integration suitable to Small and Medium-size Enterprises (SMEs). An outcome of a DC model is a composite service offered to a market place. The notion of security-by-design certified coalition enables coalition designers/owners to request a certification authority to certify whether the coalition workflow design supports certain security properties of interest by stipulating the security properties individual services have to conform to. This paper presents an approach based on a novel Highly Dynamic Coalition (HDC) concept able to provide workflow operation assurance for security-by-design certified service-based coalitions. Certified HDC models can become an enabler for SMEs to participate in coalition formations guaranteeing a certified level of security. Users of HDC-based services will have assurance for the properties preserved during coalition operation, while service providers will have assurance in providing services during HDC formations and partners' selection phases. We will show how workflow operation assurance is realized by means of service security certificates developed in ASSERT4SOA project.	certificate authority;enterprise application integration;graphics device interface;scalability;secure by design;service-oriented modeling	Javier Espinar;Antonio Maña;Hristo Koshutanski	2013	2013 IEEE Ninth World Congress on Services	10.1109/SERVICES.2013.14	public relations;knowledge management;business;computer security	DB	-47.60552546629374	54.60208224169812	10993
23d8c4fd13537dd5efbdbc9fd1256d76e8210fea	une approche pour un chargement contextuel de services dans les environnements pervasifs	sensibilidad contexto;coloracion grafo;context aware;components;informatique mobile;logicial personalizado;orientado servicio;graphs;intergiciel;coloration graphe;middleware;services;oriente service;sensibilite contexte;mobile computing;constraint devices;graph colouring;service oriented	While installing applications on mobile devices, issues like the lack of memory are encountered. In this paper, we introduce AxSeL (A conteXtual Service Loader) a contextual service loading architecture. Considered applications are serviceoriented, and services are distributed in remote repositories. In AxSeL, services and their dependencies are represented as a bidimensional graph which is coloured considering devices and services constraints. Colouring goal is to choose to load (or not) a service according to the execution context (device constraints, available services, etc.) . MOTS-CLÉS : Intergiciel, dispositifs contraints, services, composants, graphes, coloration de graphes	mobile device	Amira Ben Hamida;Frédéric Le Mouël;Stéphane Frénot;Mohamed Ben Ahmed	2008	Ingénierie des Systèmes d'Information	10.3166/isi.13.3.59-82	service;computer science;artificial intelligence;middleware;graph;mobile computing;algorithm	Web+IR	-29.761327901903623	43.00029964605656	11006
54ddca8255f75f45a8f6b974b47e6b7725617775	contributions to middleware architectures to prototype distribution infrastructures	distributed application;formal specification;software prototyping;message oriented middleware middleware architecture prototype distribution infrastructure distribution model run time constraint polyorb schizophrenic middleware configurable middleware corba soap ada 95 distributed system annex;prototypes message oriented middleware application software software prototyping runtime simple object access protocol hardware guidelines message passing computer architecture;message oriented middleware;formal specification middleware distributed object management software prototyping;distributed object management;distributed models;middleware;distributed systems annex;requirement specification;time constraint	Distributed applications require specific middleware support for semantics and run-time constraints for a wide range of hardware or software configurations. However, their full specifications and existing implementations show they share functional notions and run-time mechanisms. Thus, distribution infrastructures could be prototyped from a given set of middleware components. Generic middleware proposes patterns to describe distribution models; configurable middleware constructs to abide to run-time constraints. We have introduced the schizophrenic middleware concept as a comprehensive solution to rapidly implement different distribution models. PolyORB, our implementation of a schizophrenic middleware, supports CORBA, SOAP, the Ada 95 Distributed System Annex and Message Oriented Middleware distribution models. In this paper, we describe existing generic and configurable middleware; we introduce PolyORB’s key concepts and design; then we compare our platform design to existing generic and configurable middleware, and discuss their respective use to prototype specific distribution models.	ada;common object request broker architecture;distributed computing;middleware;polyorb;prototype;run time (program lifecycle phase);soap	Jérôme Hugues;Laurent Pautet;Fabrice Kordon	2003		10.1109/IWRSP.2003.1207039	embedded system;middleware;real-time computing;computer science;message oriented middleware;operating system;middleware;formal specification;distributed computing;message broker	SE	-38.07218080208571	39.57765074400176	11013
274c0260d234f9535bb31ddfb0854162b1793076	model based architecting and construction of embedded systems	architectural design;real time;software engineering;embedded system;design and implementation;domain specificity	Architecture selection and design optimization are critical stages of the Electronics/Controls/ Software (ECS) -based vehicle design flow. In automotive systems design, complex functions are deployed onto the physical HW and implemented in a SW architecture consisting of a set of tasks and messages. The talk will present work performed in cooperation with GM R&D and UC Berkeley, in which we optimized several aspects of the software architecture design, including the definition of the task periods, the task placement and the signal-to-message mapping and we automated the assignment of priorities to tasks and messages in order to meet end-toend deadlines and minimize latencies. Architecture selection can be accomplished by leveraging worst case response time analysis within an optimization framework and we provide hints on how to use stochastic or statistical analysis to further improve the approach. However, current work has only scantly addressed the issues of preserving the semantics of functional models during implementation. Semantics preservation issues impose additional constraints on the optimization problem, but also reveal very interesting tradeoffs between memory and time/performance. In addition, the need to deal with heterogeneous models and standards (like AUTOSAR in the automotive business) further complicates the scenario. MoDELS'09 ACES-MB Workshop Proceedings Denver, CO, USA, October 6, 2009 9 SOPHIA: a Modeling Language for Model-Based Safety Engineering Daniela Cancila 1 , Francois Terrier 1 , Fabien Belmonte 2 , Hubert Dubois 1 , Huascar Espinoza 1 , Sébastien Gérard 1 , and Arnaud Cuccuru 1	autosar;best, worst and average case;embedded system;mathematical optimization;modeling language;optimization problem;response time (technology);safety engineering;software architecture;systems design;uc browser	Iulian Ober;Stefan Van Baelen;Susanne Graf;Mamoun Filali;Thomas Weigert;Sébastien Gérard	2008		10.1007/978-3-642-01648-6_1	software design pattern;computer science;operating system;software engineering;programming language	Embedded	-39.52539233209674	35.0679658809942	11016
dbbbf328e4618ccfd73a7b375e28801c54e91206	a proposal for inductive learning agent using first-order logic.	inductive logic programming;adaptive behavior;first order;inductive learning;agent architecture;first order logic			Tohgoroh Matsui;Nobuhiro Inuzuka;Hirohisa Seki	2000			logic optimization;inductive bias;statistical relational learning;computer science;artificial intelligence;machine learning;first-order logic;inductive programming;multimodal logic;algorithm	ML	-19.088691251122423	11.173812861829012	11024
8b6c84f6cb42724194712ca11a73fdb8263dd97a	the fika parser generator	grammar;developer productivity;computer languages;generators;rivers;context free grammars;parser development;context free;fika parser generator;grammar generators productivity rivers computer languages marine vehicles;grammar reuse;marine vehicles;inheritance like module reuse fika parser generator context free executable parser developer productivity parser development grammar development grammar reuse;context free grammar;program compilers context free grammars inheritance;executable parser;grammar development;productivity;program compilers;inheritance;inheritance like module reuse	Parser generators automate conversion of a context-free grammar into an executable parser and therefore increase developers’ productivity. However, as soon as the entire process of parser development is concerned, their overall usefulness slightly diminishes because the development of the grammar itself is not addressed at all: every grammar is implicitly expected to be written from scratch and the only available mechanism of grammar reuse is copy and paste. In this paper, we briefly describe the Fika parser generator, our tool that facilitates grammar reuse by modularizing context-free grammar descriptions and providing an inheritance-like module reuse primitive.	compiler-compiler;context-free grammar;context-free language;cut, copy, and paste;executable;parser	Michael Píse	2010	2010 10th IEEE Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2010.27	natural language processing;parser combinator;compiler-compiler;lalr parser;ll grammar;parsing expression grammar;operator-precedence grammar;computer science;affix grammar;parsing;glr parser;context-free grammar;programming language;attribute grammar;recursive descent parser;ll parser;top-down parsing	NLP	-25.363888599157377	24.876317520393805	11030
9dfafb44adb768c8ededa97c7a607c2c49fe9914	vague normalization in a relational database model		Vague relational database models generalize the classical relational database model by allowing uncertain and imprecise information by means of vague set theory. In this paper, we have introduced vague extensions of the normal forms for a similarity based vague relational database model. Firstly, we have designed an algorithm to find the vague closure of an attribute set which can be utilized to find vague key. Next, with the concepts of α-vfd and partial α-vfd as introduced in [1], we have defined different vague normal forms in a vague relational database. We have also presented examples based on real life application to demonstrate how normalization based on α-vfds of vague relation may be done.	database model;relational model;vagueness	Jaydev Mishra;Sharmistha Ghosh	2012		10.1007/978-3-642-35603-2_49	relational model;entity–relationship model;relational database;data mining;database;functional dependency;database design	DB	-27.385354439381256	8.840983949838463	11034
c3d1761cbbc182f7d6368add6ded271fa0224c00	a cloud-based dynamic waste management system for smart cities	ant colony optimization;smart city;waste management	A smart city is a vision to adopt multiple information and communication technology (ICT) solutions in the management of public affairs. Waste management problem is acute in the cities and urban areas now a days. Number of trucks roaming around, collecting waste at any time, excessive manpower requirement and inefficient monitoring are some of the difficulties we face with the conventional waste collection approach. The purpose of our work is to introduce a smart and intelligent waste management system that is able to handle the process dynamically and cost effectively. In our approach weight and volume of waste thrown in the waste bins are collected by economical sensors and then sent to cloud server using a micro-controller and GPRS. This data is used to find the waste collection schedule to maximize the collection. Location of vehicles and waste bins are used to find the shortest possible collection route for each truck which is implemented by Ant Colony Optimization(ACO) technique. The system is adaptable to dynamic changes i.e. routes blocked during waste collection process. The whole process can be monitored centrally and thus provide a high quality service to the citizens of a smart city.	ant colony;apache ant (another neat tool);display resolution;management system;microcontroller;quality of service;requirement;sensor;server (computing);smart city;virtual private server;waste	Sadia Sharmin;Sikder Tahsin Al-Amin	2016		10.1145/3001913.3006629	engineering;operations management;transport engineering;waste management	HCI	-30.03721779774031	18.51323898221311	11037
81e3a7267061b95db57c28cb72f82faeed5faf39	demo: uiwear: easily adapting user interfaces for wearable devices		Wearable devices such as smartwatches offer exciting new opportunities for users to interact with their applications. However, the current wearable programming model requires the developer to write a custom companion app for each wearable form factor; the companion app extends the smartphone display onto the wearable, relays user interactions from the wearable to the phone, and updates the wearable display as needed. The development effort required to write a companion app is significant and will not scale to an increasing diversity of form factors. This paper argues for a different programming model for wearable devices. The developer writes an application for the smartphone, but only specifies a UI design for the wearable. Our UIWear system abstracts a logical model of the smartphone GUI, re-tailors the GUI for the wearable device based on the specified UI design, and compiles it into a companion app that we call the UICompanion app. We implemented UIWear on Android smartphones, AndroidWear smartwatches, and Sony SmartEyeGlasses. We evaluate 20 developer-written companion apps from the AndroidWear category on Google Play against the UIWear-created UICompanion apps. The lines-of-code required for the developer to specify the UI design in UIWear is an order-of-magnitude smaller compared to the companion app lines-of-code. Further, in most cases, the UICompanion app performed comparably or better than the corresponding companion app both in terms of qualitative metrics, including latency and energy, and quantitative metrics, including look-and-feel.	computer form factor;encode;graphical user interface;metaprogramming;mobile app;programming model;prototype;scalability;second screen;smartphone;smartwatch;virtualize;wearable computer;wearable technology	Jian Xu;Qingqing Cao;Aruna Balasubramanian;Donald E. Porter	2017		10.1145/3117811.3124769	human–computer interaction;android wear;world wide web;computer science;wearable technology;computer network;programming paradigm;logical data model;android (operating system);smartwatch;wearable computer;user interface	Mobile	-35.23360969136432	52.532394379293855	11045
8d539f60917febc52326e3cb6ae88171497e22ca	querying database knowledge	query language;database system	The role of database knowledge is usually limited to the evaluation of data queries. In this paper we argue that when this knowledge is of substantial volume and complexity, there is genuine need to query this repository of information. Moreover, since users of the database may not be able to distinguish between information that is data and information that is knowledge, access to knowledge and data should be provided with a single, coherent instrument. We provide an informal review of various kinds of knowledge queries, with possible syntax and semantics. We then formalize a framework of knowledge-rich databases, and a simple query language consisting of a pair of retrieve and describe statements. The retrieve statement is for querying the data (it corresponds to the basic retrieval statement of various knowledge-rich database systems). The describe statement is for querying the knowledge. Essentially, it inquires about the meaning of a concept under specified circumstances. We provide algorithms for evaluating sound and finite knowledge answers to describe queries, and we demonstrate them with examples.	algorithm;coherence (physics);database;query language	Amihai Motro;Qiuhui Yuan	1990		10.1145/93597.98727	computer science;data mining;database;knowledge extraction;view;information retrieval;query language	DB	-26.144181839442016	8.797255200845338	11047
ae2c484fcbd66fdf47b6d83172de88488bbb576a	sla-based management of software licenses as web service resources in distributed computing infrastructures	distributed computing infrastructures;service level agreements;security;software licensing	Until recently the use of applications requiring a software license for execution was quite limited in distributed environments. Due to the mandatory centralised control of license usage at application runtime, e.g. heartbeat control by the license server running at the home site of a user, traditional software licensing practices are not suitable especially when the distributed computing infrastructure stretches across administrative domains. In this paper we present a novel approach for managing software licenses as web service resources in distributed service oriented environments. Licenses become mobile objects, which may travel to the environment where required to authorise the execution of a license protected application. A first implementation has been realised for dynamic Grid environments in the European SmartLM project co-funded by the European Commission. The SmartLM solution decouples authorisation for license usage from authorisation for application execution. All authorisations are expressed as and guaranteed by Service Level Agreements. We will present the core technology, discuss various security aspects and how they are addressed in the SmartLM prototype, and present the evaluation of the prototype through a number of usage scenarios. Finally, we will give an outlook on specific issues and current work extending the solution to Clouds and service based systems in general.	distributed computing;service-level agreement;software license;web service	Claudio Cacciari;Daniel Mallmann;Csilla Zsigri;Francesco D'Andria;Björn Hagemeier;Angela Rumpl;Wolfgang Ziegler;Josep Martrat	2012	Future Generation Comp. Syst.	10.1016/j.future.2011.11.005	computer science;information security;operating system;database;distributed computing;world wide web;computer security	HPC	-31.67339410951564	54.62861015548987	11057
4e22cd4f2d2b742601434fdb0939fc707457944c	the complexity of quantitative information flow in recursive programs	004;quantitative information flow recursive programs program analysis verification computational complexity	Information-theoretic measures based upon mutual information can be employed to quantify the information that an execution of a program reveals about its secret inputs. The information leakage bounding problem asks whether the information leaked by a program does not exceed a given threshold. We consider this problem for two scenarios: a) the outputs of the program are revealed, and b) the timing (measured in the number of execution steps) of the program is revealed. For both scenarios, we establish complexity results in the context of deterministic boolean programs, both for programs with and without recursion. In particular, we prove that for recursive programs the information leakage bounding problem is no harder than checking reachability. 1998 ACM Subject Classification D.2.4 Software/Program Verification, D.4.6 Security and Protection, F.3.1 Specifying and Verifying and Reasoning about Programs	emoticon;formal specification;formal verification;information flow;information leakage;mutual information;reachability;recursion (computer science);spectral leakage;theory;timing closure	Rohit Chadha;Michael Ummels	2012		10.4230/LIPIcs.FSTTCS.2012.534	computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-54.996818685868035	52.54214627570016	11063
1242ca2abb2255fe596b46f7e120a6dd8d0d36e0	hybrid file system using nand-flash ssd	performance evaluation resource management file systems partitioning algorithms bandwidth algorithm design and analysis benchmark testing;disc drives;performance evaluation;performance evaluation hybrid file system nand flash ssd hdd n hybrid flexible internal structure sequential performance legacy file system;performance evaluation disc drives file organisation hard discs nand circuits;ssd;nand circuits;data section ssd hybrid file system;data section;hard discs;hybrid file system;file organisation	In this paper, we present a hybrid file system, called N-hybrid (New-form of hybrid) file system, whose main objective is to integrate the strengths of HDD and SSD, to provide a large-scale file system space in a cost-effective way. N-hybrid was designed to take advantage of SSD's high I/O performance while providing a flexible internal structure to integrate excellent sequential performance of a legacy file system, such as ext2 or xfs. The performance evaluation demonstrates that achieving high I/O performance is possible by combining vast, low-cost HDD storage space with a small portion of SSD. Several experiments were conducted to verify the performance of N-hybrid.	bonnie++;cpu cache;experiment;flash memory;hard disk drive;iozone;input/output;metafile;overhead (computing);performance evaluation;semiconductor;solid-state drive;speedup	Jaechun No	2011	2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery	10.1109/CyberC.2011.67	flash file system;self-certifying file system;parallel computing;device file;computer hardware;computer science;operating system;file system fragmentation	HPC	-13.856691561108178	52.9918269940884	11076
74edbf0db2654ab12b360e03a51c92e9dabe04ba	runtime dvfs control with instrumented code in power-scalable cluster system	interrupt based runtime dvfs control;instruments;frequency control;runtime energy consumption gears instruments power demand frequency control time frequency analysis;runtime;workstation clusters interrupts power aware computing;power aware computing;gears;energy consumption;cluster system;pc cluster;pc clusters;interrupts;dynamic voltage and frequency scaling power scalable cluster system pc clusters interrupt based runtime dvfs control code instrumented runtime dvfs control;workstation clusters;power consumption;power scalable cluster system;power demand;time frequency analysis;control method;code instrumented runtime dvfs control;dynamic voltage and frequency scaling	Recently, several energy reduction techniques using DVFS have been presented for PC clusters. This work proposes a Code-instrumented Runtime DVFS control, in which the combination of frequency and voltage (called a gear) is managed at the instrumented code at runtime. The codes are inserted by defining the program regions that have the same characteristics. The Code-instrumented Runtime DVFS control method is better than the Interrupt-based Runtime DVFS control method, in which the gear is managed by periodic interrupt, because it can reflect the program information to control DVFS. Though Static DVFS control, which makes use of the power profile before execution, gives better energy reduction, the proposed Code-instrumented Runtime DVFS control is easier to use, because it requires no information such as profile. The proposed DVFS control method was designed and implemented. The beta-adaptation was used as the runtime algorithm to choose the appropriate gear. The results show that the proposed method can improve the performance and energy consumption compared with Interrupt-based Runtime DVFS control. Although our Code-instrumented Runtime DVFS control can select lower voltages and frequencies than the present Runtime DVFS control given a certain deadline, unfortunately, it was also found to increase power consumption of the PC cluster due to an increase in the execution time.	algorithm;code;computer cluster;dynamic voltage scaling;interrupt;run time (program lifecycle phase);runtime system;scalability	Hideaki Kimura;Mitsuhisa Sato;Takayuki Imada;Yoshihiko Hotta	2008	2008 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2008.4663795	embedded system;parallel computing;real-time computing;time–frequency analysis;gear;computer science;operating system;automatic frequency control;interrupt	EDA	-4.59505563614042	57.0098515952418	11088
064e3b2c1942c9f65d783a35f2cb7545c8b05a78	the omni openmp compiler on the distributed shared memory of cenju-4	parallelisme;distributed system;systeme reparti;compilateur;shared memory;memoria compartida;distributed memory machine;compiler;parallelism;sistema repartido;paralelismo;parallel computer;openmp;distributed shared memory;compilador;memoire partagee	This paper describes an implementation and a preliminary evaluation of the Omni OpenMP compiler on a parallel computer Cenju- 4. The Cenju-4 is a parallel computer which support hardware distributed shared memory (DSM) system. The shared address space is explicitly allocated at the initialization phase of the program. The Omni converts all global variable declarations into indirect references through the pointers, and generates code to allocate those variables in the shared address space at runtime. The OpenMP programs can execute on a distributed memory machine with hardware DSM by using the Omni. The preliminary results using benchmark programs show that the performance of OpenMP programs didn't scales. While its performance of OpenMP benchmark programs scales poorly, it enables users to execute the same program on both a shared memory machine and a distribute memory machine.	compiler;distributed shared memory;openmp	Kazuhiro Kusano;Mitsuhisa Sato;Takeo Hosomi;Yoshiki Seo	2001		10.1007/3-540-44587-0_3	distributed shared memory;shared memory;computer architecture;compiler;parallel computing;distributed memory;computer science;operating system;distributed computing;programming language;data diffusion machine	HPC	-16.496423265591886	42.480811990427554	11092
982679bb2d81ddc9a319f93c9aa55cc7fe8bf33b	language support and compiler optimizations for stm and transactional boosting	optimizing compiler;ease of use;programming model;software transactional memory;system design;compiler optimization;language extension	"""We describe compiler optimizations for object-based Software Transactional Memory (STM) systems designed to make STM more accessible and efficient than is possible in the context of a purely library-based approach. Current object-based STM libraries are faced with a difficult tradeoff: offer an efficient but complex programming model or, as some recent libraries have done, sacrifice efficiency for ease-of-use. In this paper, we show that this dichotomy is not necessary. Our research suggests that an STM-enabled optimizing compiler can achieve performance comparable to that of the most efficient STM algorithms coded by a programmer, while still supporting a nearly transparent programming model. We also propose novel language extensions to support transactional boosting, a powerful new technique for transforming existing linearizable objects into transactional objects, thus permitting highly concurrent objects such as those found in the java.util.concurrent package to participate in STM transactions. When applied in conjunction with compiler support, we show that transactional boosting is both a flexible and natural way to escape the standard transactional model, and thus offers a promising alternative to existing """"expert"""" approaches, such as open nesting and early release. Based on our results, we conclude that appropriate language support and high quality compiler optimizations are necessary parts of any STM system."""	compiler;software transactional memory	Guy Eddon;Maurice Herlihy	2007		10.1007/978-3-540-77115-9_22	compiler;transactional memory;parallel computing;real-time computing;compiler correctness;computer science;operating system;compiler construction;software transactional memory;optimizing compiler;database;distributed computing;programming language;functional compiler	PL	-17.916925673908967	33.496159533245795	11117
ee6dd025eb5b21c3201047d2fcda7234db4e7414	associated paper: a coherent notation for object-oriented software engineering	object oriented software engineering		coherent;software engineering	Julian M. Edwards;Brian Henderson-Sellers	1991			component-based software engineering;resource-oriented architecture;feature-oriented domain analysis;model-driven architecture;software development process;theoretical computer science;software development;software construction;computer engineering;social software engineering;computer science	SE	-51.278311475921925	27.255996875639177	11125
0364bc6a1a6f1167661eae763f39e8e150f254b2	team programming with gorite	agent based systems;team programming;team oriented organisational models;belief desire intention;mas;agent oriented software engineering;multi agent systems;intelligent agents;complex systems development;goal oriented process models;aose;java framework;goal oriented teams;gorite;bdi	The intelligent agent paradigm has shown considerable promise as a candidate for general complex systems development. However, it is our belief that this promise will remain unfulfilled, with application of the paradigm constrained to situations where there is an overriding requirement to model human behaviour in a cognitively realistic manner. In our experience, such applications are uncommon and we have found that a team programming paradigm, as realised in frameworks such as GORITE, is generally more appropriate for complex systems development, particularly for engineered systems. In this paper, we introduce both the team programming paradigm and its realisation using GORITE.	application programming interface;complex systems;gorite;intelligent agent;java;mit engineering systems division;modeling perspective;process modeling;programmer;programming paradigm;seamless3d;software development process;team programming	Dennis Jarvis;Jacqueline Jarvis;Ralph Rönnquist	2008	IJIDSS	10.1504/IJIDSS.2008.020274	simulation;computer science;systems engineering;engineering;knowledge management;artificial intelligence;multi-agent system;java collections framework;intelligent agent	HCI	-43.06507649008655	21.375556228942674	11128
0606c74d4d986db2a269952a47b6a9c371f06c60	meet cyrus - the query by voice mobile assistant for the tutoring and formative assessment of sql learners		Being declarative, SQL stands a beer chance at being the programming language for conceptual computing next to natural language programming. We examine the possibility of using SQL as a backend for natural language database programming. Distinctly from keyword based SQL querying, keyword dependence and SQL’s table structure constraints are signicantly less pronounced in our approach. We present a mobile device voice query interface, called Cyrus, to arbitrary relational databases. Cyrus supports a large type of query classes, sucient for an entry level database class. Cyrus is also application independent, allows test database adaptation, and not limited to specic sets of keywords or natural language sentence structures. It’s cooperative error reporting is more intuitive, and iOS based mobile platform is also more accessible compared to most contemporary mobile and voice enabled systems.	aggregate data;algorithm;exception handling;intellect;mobile device;natural language processing;natural language programming;programming language;relational database;sql;ios	Josue Espinosa Godinez;Hasan M. Jamil	2018	CoRR			DB	-33.47187866289824	4.675135088437848	11138
b7347c7d59780f256a049a86a8678b8c76b5f2bc	the challenge of qualitative spatial reasoning	representacion conocimientos;qualitative spatial reasoning;base connaissance;intelligence artificielle;raisonnement qualitatif;artificial intelligence;base conocimiento;razonamiento calitativo;inteligencia artificial;qualitative reasoning;knowledge representation;representation connaissances;knowledge base	The principal goal of qualitative reasoning (QR,) is to represent not only our everyda<y commonsense knowledge about the physical world, but also the underlying abstractions used by engineers and scientists when they create quantitative models. Endowed with such knowledge and appropriate reasoning methods, a computer could make predictions and diagnoses and explain the behavior of physical systems in a qualitative manner, even when a precise quantitative description is unavailable or computationally intractable. The key to a qualitative representation is not simply that it is symbolic and utilizes discrete quantity spaces, lbut that the distinctions made in these discretizations are relevant to the behavior being modeled. QR has now become a mature subfield of AI, as evidenced by its tenth annual international workshop, several books (e.g., Weld and DeKleer [1990]) and a wealth of conference and journal publications. Although the field has broadened to become more than just qualitative physics (as it was first known), the bulk of the work has dealt with reasoning about scalar quantities, whether they denote the level of a liquid in a tank, the operatirlg region of a transistor, or the amount of unemployment in a model of an economy. Space, which is multidimensional and not adequately represented by single scalar quantities, has only recently become a significant research area with the field of QR and, more generally, in the knowledge representation community. In part this may be due to the Poverty Conjecture promulgated by Forbus et al. [Weld and DeKleer, 1990]: “there is no purely qualitative, generalpurpose kinematics.” Of course, qualitative spatial reasoning (QSR) is more than just kinematics, but it is instructive to recall their third (and strongest) argwment for the conjecture—”No total order: Quantity spaces don’t work in more than one dimension, leaving little hope for concluding much about combining weak information about spatial properties.” They correctly identify transitivity of values as a key feature of a qualitative quantity space but doubt that this can be exploited much in higher dimensions and conclude: “We suspect the space of representations in higher dimensions is sparse; that for spatial reasoning almost nothing weaker than numbers will do.” Happily, over the last few years an increasing amount of research has tended to refute or at least weaken this conjecture. There is a surprisingly rich diversity of qualitative spatial representations, and these can exploit transitivity, as demonstrated by the relatively sparse transitivity tables (cf. the well known table for Allen’s interval temporal logic [Weld and DelKleer 1990]) which have been built for these representations (actually “composition tables” is a better name for these structures). Below, I briefly survey the current state of the art in QSR (see Herrkindez [1994] for a partial survey and bibliography) and attempt to indicate some directions for future research in this area. —	automated reasoning;book;commonsense knowledge (artificial intelligence);computational complexity theory;hash table;interval temporal logic;inverse kinematics;knowledge representation and reasoning;qr code;sparse matrix;spatial–temporal reasoning;transistor;vertex-transitive graph;whole earth 'lectronic link	Anthony G. Cohn	1995	ACM Comput. Surv.	10.1145/212094.212112	knowledge base;qualitative reasoning;computer science;artificial intelligence;machine learning;algorithm	AI	-10.0524832167968	4.71238204238112	11153
6e764e31c8d08b5811a8e96a8f31048106e463d5	designing and implementing a web-based network controlling system (ncs) for automated real time routing service over the web, based on open source technologies: a case study for tehran			network computing system;open-source software;routing;web application	Javad Sadidi	2013				OS	-58.483553952201554	5.3826005820368525	11154
9cd15975cfbcb7e6c56f8147dd0efaed8f6e821c	naming and identity in epistemic logic part ii: a first-order logic for naming	distributed system;quantization;belief;cuantificacion;semantics;logique propositionnelle;quantification;raisonnement;semantica;semantique;artificial intelligent;modal logic;first order;croyance;propositional logic;logique ordre 1;logique modale;indexation;razonamiento;logica modal;epistemic logic;logica proposicional;reasoning;creencia;possible worlds;first order logic;logica orden 1	Abstract   Modal epistemic logics for many agents sometimes ignore or simplify the distinction between the agents themselves, and the  names  these agents use when reasoning about each other. We consider problems motivated by practical computer science applications, and show that the simplest theories of naming are often inadequate. The issues we raise are related to some well-known philosophical concerns, such as indexical descriptions,  de re  knowledge, and the problem of referring to nonexistent objects. However, our emphasis is on epistemic logic as a descriptive tool for distributed systems and artificial intelligence applications, which leads to some nonstandard solutions.  The main technical result of this paper is a first-order modal logic, specified both axiomatically and semantically (by a variant of possible-worlds semantics), that is expressive enough to cope with all the difficulties we discuss.	epistemic modal logic;first-order logic;first-order predicate	Adam J. Grove	1995	Artif. Intell.	10.1016/0004-3702(95)98593-D	epistemic modal logic;computer science;artificial intelligence;first-order logic;mathematics;semantics;epistemic possibility;multimodal logic;algorithm	AI	-15.637788183200593	6.870252631250221	11156
11ecf2096c665ea238f50278f8616996e66a83ad	towards querying of traceability information in the context of software evolution	software evolution;software systems	Traceability of various artifacts created during the development of software systems plays an important role in software evolution. Subsequent changes to artifacts must be traced to other artifacts potentially affected by the change, thus ensuring the system’s consistency or enabling to estimate the impact of changes. Using a querying approach, this paper shows how to extract traceability information on the basis of an integrated metamodel. The metamodel allows for the representation of artifacts such as requirements, design models, or code. It may also be customized in order to accommodate for specific needs.	metamodeling;requirement;software evolution;software system;traceability	Hannes Schwarz;Jürgen Ebert;Volker Riediger;Andreas Winter	2008			software peer review;software design description;resource-oriented architecture;software verification and validation;data mining;package development process;software development;social software engineering;systems engineering;computer science;software sizing	SE	-55.00147310812425	27.52798370139632	11158
d59363a4793c2fa67c4003c93dfc2111e558b7b8	high performance computation based on semantic p2p network	computers;time complexity;semantics;p2p;distribted computation;semantic p2p network;virtual groups;knapsack problem;patents;peer to peer computing;conferences	High performance computations are widely used in scientific research and industries. As supercomputers are expensive, how to use cheap computers such as clusters and personal computers is important issue. In this paper we describe a novel distributed computation based on semantic P2P network, in which the peers can be grouped virtually into hierarchical classified domains and the problems are partitioned into subproblems and scheduled to those peers. This strategy is scalable to millions of computers effectively in theory. We have implemented α version distributed knapsack problem solution in our semantic P2P network platform.	computation;computational problem;computer cluster;distributed computing;knapsack problem;peer-to-peer;personal computer;scalability;solver;supercomputer	Lican Huang	2015	2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)	10.1109/SmartCity.2015.228	semantic computing;computer science;theoretical computer science;distributed computing	HPC	-29.000299020456392	50.21290972477594	11163
0cea39c7053b9b1d9efe6c9ec867f0656ab19fb6	quasi-dynamic two-phase locking	phase locking;resource contention;adaptability;on line transaction processing;two phase locking;data contention;concurrency control;simulation study;pre declaration	Among the plethora of concurrency control algorithms that have been proposed and analyzed, two-phase locking (2PL) has been adapted as the industry de facto standard concurrency control. In accord, current research in concurrency control is focusing on enhancing the scalability of 2PL performance in highly concurrent and contentious environments. This is especially needed in future on-line transaction processing systems, where thousand Transaction Per Second performance will be required. Static locking (SL) and dynamic locking (DL) are two famous adaptations of 2PL that are used under different degrees of data contention. In this paper, we offer our observation that 2PL is indeed a family of methods, of which SL and DL are extreme case members. Further, we argue for and verify the existence of other 2PL member methods that, under variable conditions, outperform SL and DL. We propose two novel schemes which we categorize as quasi-dynamic two-phase locking on account of their behavior in comparison with dynamic/static two-phase locking. We present a simulation study of the performance of the proposed schemes and their comparison to dynamic and static locking methods.	algorithm;categorization;concurrency (computer science);concurrency control;lock (computer science);online and offline;online transaction processing;sl (complexity);scalability;simulation;transaction processing system;two-phase locking	Abdelsalam Helal;Tung-Hui Ku;Judson Fortner	1994		10.1145/191246.191281	double-checked locking;parallel computing;real-time computing;adaptability;isolation;computer science;two-phase locking;concurrency control;database;distributed computing	DB	-21.504202183759592	48.60711228238227	11167
d004e68750ebf0f63c317d465db671d5229a8bb3	a distributed framework for collaborative supply network integration	collaborative supply network integration	This paper presents the latest developments of the NetMan generic framework for the design and operation of distributed manufacturing and supply networks. This framework enables the implementation of specific collaborative strategies, as well as the integration of distributed business processes into a seamless value creation process. This approach, described as a collaborative integration of the business-to-business (B-to-B) interactions, allows the distributed units of a network to exchange information in a coordinated manner and to collaboratively plan, control and manage day-to-day operations and contingencies in a dynamic environment.		Jean-Marc Frayret;Louis Cloutier;Benoît Montreuil;Sophie D'Amours	2000			systems engineering;distributed manufacturing;business process;supply chain;supply network;computer science	Vision	-57.65349685703749	12.741116345743388	11173
c36266cfe34b4b634e2886f9dc7fadcbcf6f102e	towards extended safety in connected vehicles	vehicular ad hoc networks intelligent transportation systems telecommunication security;electronic control units;threats;agora framework;motor vehicles vehicle safety accidental failure electronic control units ecu secure in vehicle network vehicle to mobile v2m vehicle to vehicle v2v vehicle to infrastructure v2i intelligent transportation systems agora framework boiler plate code;intelligent transportation systems;safety and security;vehicle to infrastructure communications;vehicular ad hoc networks;telecommunication security;vehicles security computer architecture hazards standards software;vehicle to vehicle communications	Current standards for vehicle safety consider only accidental failures; they do not consider failures caused by malicious attackers. The standards implicitly assume that the sensors and Electronic Control Units (ECUs) of each vehicle compose a secure in-vehicle network because no external entity communicates with the nodes of the network. These standards assume that safety and security aspects are independent. Connecting vehicles to external entities, e.g., through Vehicle to Mobile (V2M), Vehicle to Vehicle (V2V), and Vehicle to Infrastructure (V2I), proved to be useful: it enables using Intelligent Transportation Systems (ITS) applications that improve our safety, efficiency, and comfort; but vulnerable to security threats. This paper provides an overview of AGORA framework: a framework generating secure and tested boiler-plate code needed for ITS applications, demonstrates that safety and security aspects in motor vehicles are not independent, and proposes extending safety assurance by considering security aspects. It also discusses a set of research challenges related to extended safety assurance in connected vehicles.	agora;data flow diagram;entity;error-tolerant design;interaction;malware;sensor;vehicle-to-vehicle	Lotfi Ben Othmane;Ala I. Al-Fuqaha;Elyes Ben Hamida;Mark van den Brand	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728305	vehicle tracking system;engineering;automotive engineering;transport engineering;computer security;ivms	Robotics	-54.41402856903968	48.89939502332497	11174
871f42c4f330291cb92a04651acf1742f3747497	design, specification, and implementation of a distributed virtual community system	document handling;formal specification;information retrieval;virtual community;open archive initiative;object oriented programming;knowledge worker;information retrieval software libraries collaborative work feedback computer science mathematics physics protocols software engineering distributed databases;xml;formal specification open distributed virtual community system information retrieval online literature document sharing open standard open archive initiative standard xml rpc standard distributed component heterogeneous component software design;software standards;object oriented programming information retrieval remote procedure calls xml open systems document handling software standards formal specification;open systems;remote procedure calls;open standard	We introduce an open distributed virtual community system that supports knowledge workers with functionality for searching and browsing in preprint (e-print) archives as well as for sharing and exchanging their findings in virtual communities. Its functionality combines the strengths of existing systems for information retrieval of online literature with those for sharing and exchanging documents in groups or communities. Being based on open standards such as the open archive initiatives (OAi) standard and XML-RPC, it allows the flexible integration of distributed and heterogeneous components.	archive;functional design;heterogeneous computing;information retrieval;unified modeling language;unix-like;virtual community;xml-rpc	Tom Gross	2004	12th Euromicro Conference on Parallel, Distributed and Network-Based Processing, 2004. Proceedings.	10.1109/EMPDP.2004.1271449	xml;open standard;computer science;operating system;data mining;formal specification;database;distributed computing;open system;programming language;object-oriented programming;remote procedure call;world wide web	HPC	-50.16387346428456	18.846834010815254	11176
e84ec3db2ef29062546b999f39bb9bfb185916bc	prime subprogram parsing of a program	linear time;large classes;flow analysis	A parsing method based on the triconnected decomposition of a biconnected graph is presented. The parsing algorithm runs in linear time and handles a large class of flow graphs. The applications of this algorithm to flow analysis and to the automatic structuring of programs are discussed.	algorithm;biconnected graph;data-flow analysis;parsing;subroutine;time complexity	Robert E. Tarjan	1980		10.1145/567446.567456	time complexity;parser combinator;computer science;bottom-up parsing;theoretical computer science;data-flow analysis;programming language;top-down parsing;algorithm	PL	-14.137616005736238	24.202661942083857	11178
2fb507038fe9fbb195e9c82f795c56034cd40707	a framework and language support for dynamic security policy in service-oriented architecture	soa;web services;workflow;bpel;security policy	In today’s global network-based environment, where mission-critical applications typically run on highly distributed systems, customers expect reliable, available, and secure services. Supporting security becomes an important issue in service-oriented architecture (SOA). This paper describes how to simultaneously support both dynamic security policies and separation of concerns when developing an SOA application. We propose the DPSL (dynamic policy specification language) for managing and controlling the security according to the dynamic behavior of the workflow in SOA. The operation model is compatible with existing SOA standards, such as the WSDL, WS-Policy, WS-SecurityPolicy, WS-ReliableMessaging, and the BPEL. As a result, existing standard Web-services engines and BPEL engines can be employed directly to support dynamic policies in SOA. The implementation and experimental results demonstrate the feasibility of the proposed architecture.	access control;business process execution language;distributed computing;exception handling;fault tolerance;global network;mission critical;requirement;soa security;separation of concerns;service-oriented architecture;service-oriented device architecture;specification language;ws-policy;ws-reliablemessaging;ws-securitypolicy;web services description language	Chi Wu-Lee;Gwan-Hwan Hwang	2014	J. Inf. Sci. Eng.		web service;workflow;business process execution language;computer science;security policy;service-oriented architecture;database;distributed computing;world wide web;computer security;oasis soa reference model	SE	-43.34440252856366	41.595877426870686	11180
7e61bb6d14f98c911c0c71c1286ce7de666ddbd6	partitioned optimization of complex queries	query language;decision support;computacion informatica;query processing;efficient algorithm;complexity analysis;olap;relational database;satisfiability;query languages;very large database;ciencias basicas y experimentales;difference set;grupo a;decision support queries	Performing complex analysis on top of massive data stores is essential to most modern enterprises and organizations and requires significant aggregation over different attribute sets (dimensions) of the participating relations. Such queries may take hours or days, a time period unacceptable in most cases. As a result, it is important to study these queries and identify special frequent cases that can be evaluated with specialized algorithms. Understanding complex aggregate queries leads to better execution plans and, consequently, performance. The idea of partitioning is fundamental and central in aggregate queries. This concept can be used to define a class of queries called group queries. The main characteristic of a group query is that it can be evaluated in a partitioned (or groupwise) fashion, i.e. the underlying relation(s) can be partitioned (based on a set of attributes) into disjoint groups and each group can be processed separately, possibly in parallel. For example, a query that performs a complex operation (e.g. joins and/or selections and/or aggregations) within each group is a group query. To express it in SQL, one has to join/ correlate several views and/or subqueries on the grouping attributes. A naive plan (where the joins are executed) may be very expensive, even for relatively small base relations. On the other hand, a groupwise evaluation can lead to huge performance gains. We present a syntactic criterion to identify group queries in SQL and show that every group query can be expressed in a way that satisfies this criterion. This work is based on Chatziantoniou and Ross [Querying Multiple Features of Groups in Relational Databases. in: 22nd International Conference on Very Large Databases, VLDB, 1996, pp. 295–306]. The concept of group queries is useful not only in terms of evaluation, but also in terms of analyzing a complex decision support query that aggregates over different sets of attributes. In such a case the query may be decomposable to one or more query components, where each component is a group query. This observation allows parallel execution, multi-query processing and identification of special cases. We present in this paper two algorithms to decompose a complex aggregate query to its group query components. The value of groupwise processing has been recently recognized by the research community and implemented in at least a major commercial system. To be of use however in a relational system, partitioned evaluation has to be modeled as a relational operator. We review three different approaches for such an operator and propose a generalized groupwise operator. We also perform some experiments to show that naive optimization with the new operator incorporated without taking into consideration decompositions to group query components does not always lead to the most efficient plans. An extended syntax is another way to identify special frequent cases and apply efficient algorithms. Having specific operators for common operations contributes to the succinctness and optimizability of certain queries (e.g. datacubes). An e front matter r 2005 Elsevier B.V. All rights reserved. 2005.09.003 ing author. Tel.: +30 210 613 7689; fax: +30 210 613 7889. esses: damianos@aueb.gr (D. Chatziantoniou), kar@cs.columbia.edu (K.A. Ross). ARTICLE IN PRESS D. Chatziantoniou, K.A. Ross / Information Systems 32 (2007) 248–282 249 extended syntax is presented with emphasis for multi-feature queries, a frequent and practical subclass of group queries that is amenable to specialized evaluation, involving (potentially repeated) selection, grouping and aggregation over the same groups. r 2005 Elsevier B.V. All rights reserved.	aggregate data;aggregate function;algorithm;data store;database;decision support system;experiment;fax;focus group;information system;mathematical optimization;relational operator;sql;syntactic predicate;vldb	Damianos Chatziantoniou;Kenneth A. Ross	2007	Inf. Syst.	10.1016/j.is.2005.09.003	online aggregation;range query;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;decision support system;computer science;query by example;theoretical computer science;machine learning;data mining;database;conjunctive query;programming language;range query;query language;spatial query	DB	-28.191120954006156	6.216866223019976	11193
85f79232c64cf7b3fa18b233ab598de24bb36c1f	motorola weavr: aspect and model-driven engineering	model driven engineering		model-driven engineering;motorola 68000	Thomas Cottenier;Aswin van den Berg;Tzilla Elrad	2007	Journal of Object Technology	10.5381/jot.2007.6.7.a3	model-driven architecture;computer science;systems engineering	SE	-51.72527722883422	26.655931658574023	11213
dfd4df92362bdc9ecd8681119bc351263197185a	semantics, decision procedures, and abstraction refinement for symbolic trajectory evaluation	information technology;computer and information science;semantics;abstraction;symbolic trajectory evaluation;satisfiability solver;natural sciences;formal verification;model checking;informationsteknik	The rapid growth in hardware complexity has led to a need for formal verification of hardware designs to prevent bugs from entering the final silicon. Model-checking is a verification method in which  a model of a system is checked against a property, describing the desired behaviour of the system over time. Today, all major hardware companies use model-checkers in order to reduce the number of bugs in their designs.#R##N##R##N#Symbolic Trajectory Evaluation (STE) is a model-checking technique for hardware. STE uses abstraction, meaning that details of the circuit behaviour are removed from the circuit model. This improves the capacity limits of the method, but has as down-side that certain properties cannot be proved if the wrong abstraction is chosen. STE is limited to properties ranging over a finite number#R##N#of time-steps. Generalised Symbolic Trajectory Evaluation (GSTE) is an extension of STE that can deal with properties ranging over unbounded time.#R##N##R##N#This thesis describes several important contributions to research on STE and GSTE.#R##N##R##N#First of all, the thesis describes a SAT-based method for abstraction refinement in STE.  A main drawback of STE is that the user needs to spend time on finding the right abstraction.  Often, a great deal of time is spent on such manual abstraction refinement.#R##N##R##N#To address this problem, we have invented a method  for assisting STE users with manual abstraction refinement.  As a case study, we have demonstrated the usefulness of the algorithm by showing how to refine and verify an STE specification of a Content-Addressable Memory (CAM).#R##N##R##N#Furthermore, the thesis describes faithful semantics for STE and GSTE.#R##N#The reason for developing these semantics is that we have discovered that#R##N#the existing semantics for STE and GSTE do not correspond to the proving power of the corresponding model-checking algorithms.#R##N##R##N#We believe that the semantics are an important contribution for at least two reasons. First of all, a faithful semantics makes STE and GSTE more accessible to novice users:  a faithful semantics enables users to understand the abstraction used in STE and GSTE, without having to understand the details of the model-checking algorithm.#R##N##R##N#Secondly, a faithful semantics can be used as basis for research on new model-checking algorithms and other tools for STE and GSTE.#R##N##R##N#To illustrate this, building upon our faithful semantics for STE, we have developed the third contribution of this thesis: a new SAT-based model-checking algorithm for STE.  In the thesis, we  demonstrate on a series of benchmarks that our new algorithm outperforms other SAT-based model-checking algorithms for STE.	symbolic trajectory evaluation	Jan-Willem Roorda	2006			computer science;theoretical computer science;symbolic trajectory evaluation;algorithm	EDA	-14.460463088580225	27.01371972369691	11218
c53e838a77809c75f40315a367776daf39323b87	reference algorithm for 3d mesh model in service rendering middleware	content management;service rendering middleware;service level;middle layer model reference algorithm 3d mesh model service rendering middleware adaptive service 3d graphic contents restrictions ubiquitous environment;application software;computer graphics;middleware computer graphics rendering computer graphics application software engines games animation computer science content management environmental management;reference algorithm;service model;engines;3d graphic contents restrictions;adaptive service;ubiquitous computing middleware rendering computer graphics;middle layer model;games;3d mesh model;animation;ubiquitous computing;middleware;computer science;rendering computer graphics;environmental management;user satisfaction;3d graphics;ubiquitous environment	As needs arise for an adaptive service with a new concept according to 3D graphic contents restrictions in the user environments, this study proposes an adaptive service model for 3D graphic contents in ubiquitous environment. In advance, we proposed Service Rendering Middleware to reconstruct the elements of existing contents according to the changing user environments as a middle layer model. This study provides a reference algorithm that selects an appropriate mesh model in Service Rendering Middleware. The algorithm provides a service level adequate for the current environment, allowing accurate creation and management of new 3D graphic contents according to device capabilities. An increasing amount of 3D graphic contents is being serviced to a wide range of terminal devices, and the approaches suggested in this paper are expected to enable fast adaptive services that enhance user satisfaction.	3d computer graphics;algorithm;middleware;real-time clock;requirement;user requirements document	Hakran Kim;Yongik Yoon;Hwajin Park	2008	2008 International Conference on Multimedia and Ubiquitous Engineering (mue 2008)	10.1109/MUE.2008.41	anime;games;application software;simulation;service level;content management;computer science;operating system;service-oriented modeling;middleware;multimedia;computer graphics;ubiquitous computing;computer graphics (images)	HPC	-35.49210100251429	48.921131289607935	11231
9ddeddfcfc14af0aadbb509b23932ee42a0d0af4	exploring discrete structures using a complete variant of prolog			prolog	Norman Neff	1994			theoretical computer science;discrete mathematics;algorithm;prolog;computer science	Logic	-20.430565626964064	19.21434304898563	11250
c1b87002d92a550875ddf3806dff8be69f2ab3b1	views: customizable abstractions for contextaware applications in manets	verification;time triggered;context information;distributed protocol;automotive;integration;theorem proving;dynamic environment;model based software engineering;mobile ad hoc network;middleware	Programming applications for highly dynamic environments such as mobile ad hoc networks (MANETs) is complex, since the working context of applications changes continuously. This paper presents “views” as abstractions for representing and maintaining context information, tailored to applications in MANETs. An application agent can define a view by declaratively describing the context information it is interested in. A supporting middleware platform, called ObjectPlaces, ensures that the information represented by a view continuously reflects the agent’s context information, despite the dynamic situation in a MANET. We elaborate on the distributed protocol that ObjectPlaces uses to maintain the information of views, and give a thorough evaluation.	best-effort delivery;declarative programming;experience;hoc (programming language);middleware	Kurt Schelfthout;Tom Holvoet;Yolande Berbers	2005	ACM SIGSOFT Software Engineering Notes	10.1145/1082983.1082973	real-time computing;verification;mobile ad hoc network;computer science;middleware;database;distributed computing;automated theorem proving	PL	-38.05044305198505	35.90007563716211	11252
ae191f0453ceb5b01f088e650c0d9a5b6aa4c03d	symbolic state space exploration of rt systems in the cloud	symbolic computation;settore inf 01 informatica;real time systems cloud computing discrete event systems java petri nets;state space explosion problem;distributed computing;computational modeling abstracts peer to peer computing explosions petri nets concrete real time systems;discrete event systems;map reduce;command line java tool symbolic state space exploration rt systems cloud computing framework distributed computing framework discrete event systems exponential complexity distributed memory real time systems petri nets sequential algorithm;map reduce petri nets real time systems symbolic computation state space explosion problem distributed computing cloud computing;petri nets;cloud computing;java;real time systems	The growing availability of distributed and cloud computing frameworks makes it possible to face complex computational problems in a more effective and convenient way. A notable example is state-space exploration of discrete-event systems specified in a formal way. The exponential complexity of this task is a major limitation to the usage of consolidated analysis techniques and tools. Several techniques for addressing the state space explosion problem within this context have been studied in the literature. One of these is to use distributed memory and computation to deal with the state space explosion problem. In this paper we study and compare two different approaches, relying on distributed and cloud frameworks, respectively. These approaches were designed and implemented following the same computational schema, a sort of map & fold. They are applied on symbolic state-space exploration of real-time systems specified by (a timed extension of) Petri Nets, by re-adapting a sequential algorithm implemented as a command-line Java tool. The outcome of several tests performed on a benchmarking specification are presented, thus showing the convenience of distributed approaches.	binary file;cloud computing;command-line interface;computation;computational problem;distributed computing;distributed memory;fold (higher-order function);item unique identification;java;mapreduce;performance;petri net;reachability;real-time clock;real-time computing;sequential algorithm;state space;system analysis;terabyte;time complexity;windows rt	Carlo Bellettini;Matteo Camilli;Lorenzo Capra;Mattia Monga	2012	2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2012.18	symbolic computation;real-time computing;cloud computing;computer science;theoretical computer science;operating system;machine learning;database;distributed computing;process architecture;programming language;java;petri net;algorithm;algebra	SE	-10.944392723679568	31.655992996110427	11260
26bf3bb1eedf942cb1adc773b2ad0f11a7b100b6	checking formal verification models for human-automation interaction	batteries surveillance automation predictive models atmospheric modeling;interfaces;human computer interaction formal verification;uav ground control simulation formal verification models human automation interaction human machine system paparazzi uav ground control station unmanned aerial vehicles;formal verification;human factors;formal verification ergonomics human factors interfaces automation;ergonomics;automation	In complex human-machine systems, unforeseen failures in the interaction between human agents, automation and the environment provide an important contribution to incidents and accidents. The complexity of many systems precludes a designer from foreseeing all possible states of interaction. Formal verification methods are explored as a means of making human-machine systems more robust against failures arising from these unforeseen interactions. For these methods, a analytic model of the operator task is combined with a formal model of the system (automation), and, using model checking tools, a formal verification of the interaction is performed. Validity of the results, however, does require a sufficient correspondence between the model and the actual system. To validate this correspondence, this study explores an approach where the predictions from a formal model are compared to behavior of the human-machine system. The Paparazzi UAV ground control station is used as a test case, and a framework was created to automatically play back results from the formal verification tool to the UAV ground control simulation. The results show a good correspondence between the actual system and the model results, even if the model is by necessity a simplified description of actual system behavior. A remaining problem is creating enough variation in verification tool traces to properly test the correspondence between the formal model and the system.	automation;formal verification;glossary of computer graphics;human–machine system;interaction;mathematical model;model checking;simulation;test case;tracing (software);unmanned aerial vehicle	René van Paassen;Matthew L. Bolton;Noelia Jimenez	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974507	verification and validation of computer simulation models;verification;simulation;formal methods;formal verification;computer science;human factors and ergonomics;automation;interface;formal specification;runtime verification;intelligent verification;functional verification	EDA	-40.75890348916139	29.783176180060746	11264
738469e568e7c9471fb46b72210a32b57adf8b80	the deep project - pursuing cluster-computing in the many-core era	deep project homogeneous cluster architectures deep programming environment task based ompss programming paradigm abstraction layer message passing interface mpi standard offloading functionality programming environment cluster booster architecture application codes booster cluster many core processors standard hpc cluster accelerator elements dynamical exascale entry platform cluster computing many core era high performance computing;juser;parallel programming;computer architecture program processors scalability hardware fabrics parallel processing programming;exascale hpc computer architecture;websearch;parallel programming application program interfaces message passing multiprocessing systems;computer architecture;exascale;hpc;application program interfaces;message passing;fabrics;scalability;multiprocessing systems;publications database;programming;program processors;parallel processing;hardware	Homogeneous cluster architectures dominating high-performance computing (HPC) today are challenged, in particular when thinking about reaching Exascale by the end of the decade, by heterogeneous approaches utilizing accelerator elements. The DEEP (Dynamical Exascale Entry Platform) project aims for implementing a novel architecture for high-performance computing consisting of two components - a standard HPC Cluster and a cluster of many-core processors called Booster. In order to make the adaptation of application codes to this Cluster-Booster architecture as seamless as possible, DEEP provides a complete programming environment. It integrates the offloading functionality given by the MPI standard with an abstraction layer based on the task-based OmpSs programming paradigm. This paper presents the DEEP project with an emphasis on the DEEP programming environment.	abstraction layer;backplane;bayesian information criterion;booster (electric power);central processing unit;code;computer cluster;high- and low-level;integrated development environment;manycore processor;message passing interface;microsoft outlook for mac;programming paradigm;scalability;seamless3d;supercomputer;xeon phi	Norbert Eicker;Thomas Lippert;Thomas Moschny;Estela Suarez	2013	2013 42nd International Conference on Parallel Processing	10.1109/ICPP.2013.105	parallel processing;programming;computer architecture;parallel computing;message passing;real-time computing;scalability;computer science;operating system	HPC	-8.208481200078106	44.481517869280175	11265
1e71c07ecea1c6cc2fe4828222a1801c0a4c2e88	bringing order to chaos: barrier-enabled i/o stack for flash storage		This work is dedicated to eliminating the overhead required for guaranteeing the storage order in the modern IO stack. The existing block device adopts a prohibitively expensive approach in ensuring the storage order among write requests: interleaving the write requests with Transfer-and-Flush. For exploiting the cache barrier command for flash storage, we overhaul the IO scheduler, the dispatch module, and the filesystem so that these layers are orchestrated to preserve the ordering condition imposed by the application with which the associated data blocks are made durable. The key ingredients of Barrier-Enabled IO stack are Epoch-based IO scheduling, Order-Preserving Dispatch, and Dual-Mode Journaling. Barrier-enabled IO stack can control the storage order without Transfer-and-Flush overhead. We implement the barrier-enabled IO stack in server as well as in mobile platforms. SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In a server storage, BarrierFS brings as much as by 43 × and by 73× performance gain in MySQL and SQLite, respectively, against EXT4 via relaxing the durability of a transaction.	aclarubicin;allocate-on-flush;cpu cache;cache (computing);dimethoxyamphetamine;direct memory access;dispense;dual;durability (database systems);dynamic dispatch;flash memory;flushing;forward error correction;hard disk drive;mobile device;mysql;overhead (computing);sqlite;scheduling (computing);scheduling - hl7 publishing domain;server (computer);server (computing);smartphone;source code;anatomical layer;contents - htmllinktype	Youjip Won;Joontaek Oh;Jaemin Jung;Gyeongyeol Choi;Seongbae Son;Joo Young Hwang;Sangyeun Cho	2018	TOS	10.1145/3242091	real-time computing;ext4;computer science;journaling file system;parallel computing;device file;scheduling (computing);interleaving;cache;input/output;durability	OS	-13.431171792610792	53.54365182011225	11272
925c8f1fae5a7723c6bacad512c7b781888f24f9	an approach for automatic query expansion based on nlp and semantics		Nowadays, there is a huge amount of digital data stored in repositories that are queried by search systems that rely on keyword-based interfaces. Therefore, the retrieval of information from repositories has become an important issue. Organizations usually implement architectures based on relational databases that do not consider the syntax and semantics of the data. To solve this problem, they perform complex Extract, Transform and Load (ETL) processes from relational repositories to triple stores. However, most organizations do not carry out this migration due to lack of time, money and knowledge.	natural language processing;query expansion	Maria G. Buey;Angel L. Garrido;Sergio Ilarri	2014		10.1007/978-3-319-10085-2_32	query optimization;query expansion;web search query	NLP	-34.58953238968312	4.379131287382402	11276
d3f491d6c4733f40b3734fcffb19698c52b62efd	a framework to provide customized reuse of open corpus content for adaptive systems	conference paper;content semantic slicing;semantic web;open corpus content	One of the main services that Adaptive Systems offer to their users is the provision of content that is tailored to individual user's needs. Some Adaptive Systems use a closed corpus content that has been prepared for them a priori, hence, they accept only a narrow field of content. Furthermore, the content is tightly coupled with other parts of the system, which also hinders its re-usability. To address these limitations, recent systems started to make use of open Web content to provide a wider variety of content. Previous approaches have attempted to harness the information available on the web by providing adaptive systems with customizable information objects. Since adaptive systems are evolving towards the Semantic Web and the use of ontologies, existing systems are limited by their ability to service these documents solely through keyword-based queries. In this research we propose a novel framework that extends existing content provision system, Slicepedia. Our framework uses the conceptual representation of content to segment it in a semantic manner. The framework removes unnecessary content from web pages, such as navigation bars, and then semantically reveals the structural representation of text to build a tree-like hierarchy. This tree can be traversed to obtain different levels of content granularity that facilitate content discoverability and adaptivity.	adaptive system;discoverability;navigation bar;ontology (information science);open web;semantic web;usability;web content;web page	Mostafa Bayomi	2015		10.1145/2700171.2804450	content management;computer science;semantic web;semantic web stack;database;multimedia;world wide web	Web+IR	-40.94268872040094	8.78592429061864	11280
130accbaaa8dd7c3268de9cdd70672ae1427bb2a	contention elimination by replication of sequential sections in distributed shared memory programs	distributed system;programa paralelo;sequential access;estacion trabajo;systeme reparti;shared memory;eliminacion;analisis datos;proceso ligero;memoria compartida;station travail;multidestinatario;analyse temporelle;analisis temporal;distributed supercomputing;time analysis;software distributed shared memory;data analysis;codificacion;workstation;acceso secuencial;sistema repartido;processus leger;clustered wide area systems;network of workstation;coding;analyse donnee;procesador;elimination;thread;processeur;distributed shared memory;acces sequentiel;parallel program;multidestinataire;processor;multicast;memoire partagee;codage;java;programme parallele	In shared memory programs contention often occurs at the transition between a sequential and a parallel section of the code. As all threads start executing the parallel section, they often access data just modified by the thread that executed the sequential section, causing a flurry of data requests to converge on that processor. We address this problem in a software distributed shared memory system by replicating the execution of the sequential sections on all processors. Communication during this replicated sequential execution is reduced by using multicast. We have implemented replicated sequential execution with multicast support in OpenMP/NOW, a version of of OpenMP that runs on networks of workstations. We do not rely on compile-time data analysis, and therefore we can handle irregular and pointer-based applications. We show significant improvement for two pointer-based applications that suffer from severe contention without replicated sequential execution.	central processing unit;compile time;compiler;converge;distributed shared memory;multicast;openmp;pointer (computer programming);workstation	Honghui Lu;Alan L. Cox;Willy Zwaenepoel	2001		10.1145/379539.379568	distributed shared memory;shared memory;thread;parallel computing;real-time computing;multicast;workstation;computer science;operating system;distributed computing;coding;data analysis;programming language;java;sequential access;elimination	Arch	-16.764859091094287	42.69747697711924	11287
e811a1b01318b13141246edccbcb6c148f9ada3d	specification translation of state machines from equational theories into rewrite theories	algebraic specification;rewrite theory;maude;state machine;theorem proving;model checking;cafeobj;rewriting logic;equational theory;equational logic;automatic translation	Specifications of state machines in CafeOBJ are called equational theory specifications (EQT Specs) which are based on equational logic, and in Maude are called rewrite theory specifications (RWT Specs) which are based on rewriting logic. The translation from EQT Specs to RWT Specs achieves the collaboration between CafeOBJ’s theorem proving facilities and Maude’s model checking facilities. However, translated specifications by existing strategies are of inefficiency and rarely used for model checking in practice. This paper defines a specific class of EQT Specs called EADS Specs, and proposes a strategy for the translation from EADS Specs to RWT Specs. It is proved that translated specifications by the strategy are more efficient than those by existing strategies.	automated theorem proving;correctness (computer science);init;mathematical optimization;maude system;model checking;prototype;rewrite (programming);rewriting;spec#;state transition table	Min Zhang;Kazuhiro Ogata;Masaki Nakamura	2010		10.1007/978-3-642-16901-4_44	model checking;equational logic;rewriting;computer science;automated theorem proving;finite-state machine;programming language;algorithm	SE	-20.432805058640263	25.951254049568067	11288
670be0bac39bdbedb6855a870988c366d6b72712	practical shape analysis	shape analysis;berkeley eric brewer mccloskey;william terrence;program analysis;computer science;abstract interpretation;computer science practical shape analysis university of california	Practical Shape Analysis by William Terrence McCloskey Doctor of Philosophy in Computer Science University of California, Berkeley Professor Eric Brewer, Chair Shape analysis is a program analysis technique used to prove that imperative programs using manual memory management will not crash. In the past, shape analysis has been applied to data structures like linked lists and binary trees. It has also been used on simplified versions of Windows device drivers. We describe techniques that allow us to apply shape analysis to data structures that occur commonly in systems code. These data structures often use arrays, hash tables, C strings, and buffers of a known size. Sometimes, memory in these data structures is managed by manual reference counting. Analyzing such code is difficult or impossible with existing shape analyses. Most difficult of all, many data structures use several of these patterns at the same time, such as a hash table pointing to reference counted objects through which a doubly linked list threads. We describe an analysis capable of handling these data structures easily and efficiently. Our technique uses abstract interpretation over the combination of two abstract domains. One, based on three-valued logic, is used for analyzing the heap. The other domain reasons about integers and set cardinality. The key feature of the combined domain is that quantified facts can be shared between the integer and heap domains. The precision we achieve is significantly greater than if either domain were used independently. Besides improvements in precision, we also describe changes that make both domains more scalable and efficient. We present the results of experiments analyzing the cache data structure of the thttpd web server, which uses a hash table, linked lists, and reference counting in a single data structure. We successfully prove the absence of memory errors in about two minutes.	abstract interpretation;array data structure;binary tree;computer science;device driver;doubly linked list;experiment;hash table;imperative programming;manual memory management;microsoft windows;null-terminated string;program analysis;reference counting;scalability;server (computing);shape analysis (digital geometry);three-valued logic;web server;eric	William Terrence McCloskey	2010			computer science;theoretical computer science;database;shape analysis;algorithm	PL	-19.046106139384264	31.9889000773258	11289
932f1c2adde0de206c1d3ca782200070f10631d5	investigating the partial relationships between testability and the dynamic range-to-domain ratio	empirical study;statistical test;metric;testability;it value;dynamic range;design for test;dynamic range to domain ratio	The word ‘testability’ has been used variously in the software community to represent a number of different concepts such as how easy it is to test a program or how easy it is to achieve execution coverage of certain program components. Voas and colleagues have used the word to capture a slightly different notion, namely the ease with which faults, if present in a program, can be revealed by the testing process. The significance of this concept is twofold. First, if it is possible to measure or estimate testability, it can guide the tester in deciding where to focus the testing effort. Secondly, knowledge about what makes some programs more testable than others can guide the developer so that design-fortest features are built in to the software. The propagation, infection and execution (PIE) analysis technique has been proposed as a way of estimating the Voas notion of testability. Unfortunately, estimating testability via the PIE technique is a difficult and costly process. However, Voas has suggested a link with the metric, domain-to-range ratio (DRR). This paper reviews the various testability concepts and summarises the PIE technique. A prototype tool developed by the authors to automate part of the PIE analysis is described and a method is proposed for dynamically determining the inverse of the domain-to-range ratio. This inverse ratio can be considered more natural in some sense and the idea of calculating its value from program execution leads to the possibility of automating its determination. Some experiments have been performed to investigate empirically whether there is a partial link between testability and this dynamic range-to-domain ratio (DRDR). Statistical tests have shown that for some programs and computational functions there is a strong relationship, but for others the relationship is weak.	computation;deficit round robin;dynamic range;experiment;mod (video gaming);prototype;quadratic equation;resultant;software propagation;software testability;while	Zuhoor A. Al-Khanjari;Martin R. Woodward	2003	Australasian J. of Inf. Systems	10.3127/ajis.v11i1.139	testability;reliability engineering;statistical hypothesis testing;dynamic range;metric;computer science;engineering;artificial intelligence;design for testing;database;empirical research;engineering drawing;algorithm	SE	-61.724511108951305	34.12310074075337	11290
c75f1baf23aeea80fa876c5f045ea97e5814434b	multi-stage parallel machines and lot-streaming scheduling problems - a case study for solar cell industry			scheduling (computing);solar cell	Hi-Shih Wang;Li-Chih Wang;Tzu-Li Chen;Yin-Yann Chen;Chen-Yang Cheng	2013		10.1007/978-3-642-41266-0_19	parallel computing;real-time computing;computer science;operations management	HPC	-5.285872782718728	32.830526922655	11309
5536d6d1460f2a948bb5512847ba4e9b8d49d36c	lightweight parametric polymorphism for oberon	parametric polymorphism;polymorphism	Strongly typed polymorphism is necessary for expressing safe reusable code. Two orthogonal forms of polymorphism exist: inclusion and parametric, the Oberon language only supports the former. We describe a simple extension to Oberon to support parametric polymor-phism. The extension is in keeping with the Oberon language: it is simple and has an explicit cost. In the paper we motivate the need for paramet-ric polymorphism and describe an implementation in terms of translating extended Oberon to standard Oberon.	compiler;dynamic-link library;library (computing);oberon;object code;parametric polymorphism;programmer;type safety;type system;universal instantiation	Paul Roe;Clemens A. Szyperski	1997		10.1007/3-540-62599-2_36	programming language;simple extension;oberon;parametric polymorphism;parametric statistics;object code;computer science	PL	-24.220024080465706	27.165887058014018	11314
a06c44b07210cfddac1cb2b7d94b6bcb01399538	valid-2: a practical modeling, simulation and verification software for distributed systems	distributed system;system reliability;formal specification;distributed processing;formal specifications;distributed computing;software systems;automatic logic units;distributed systems verification;software engineering;formal method;theorem proving;verification software;uml notation;simulation software;computational modeling;formal verification;model checking;rewriting systems;specification languages;concurrent system;rewriting logic;unified modeling language;software systems computational modeling unified modeling language formal specifications distributed computing software engineering automatic logic units java computer simulation mathematical model;mathematical model;theorem proving valid 2 modeling software simulation software verification software distributed systems verification software engineering formal specification formal method mathematical description model system reliability concurrent system uml notation rewriting logic model checking;valid 2;computer simulation;mathematical description model;digital simulation;digital simulation distributed processing formal specification formal verification specification languages rewriting systems theorem proving;modeling software;model simulation;java	Summary form only given. Distributed systems verification is one of the main issues in software engineering. It is considered as the major field of the formal specification techniques. However, many difficulties remain. In fact, the principal problem is in producing a coherent specification and providing a fully integrated semantics. Since formal methods are mathematical description models that try to give a response concerning the reliability of a system. It remains a hard way for the designers. Thus, we present, here, an open environment for the integration of formal methods in the description and verification of distributed and concurrent systems. The system currently uses UML notation and provides rewriting logic, model checking, theorem proving, and simulation techniques.	automated theorem proving;coherence (physics);concurrency (computer science);denotational semantics;description logic;distributed computing;formal methods;formal specification;model checking;rewriting;simulation;software engineering;unified modeling language	Mohamed Larbi Rebaiaia;Jihad Mohamad Jaam	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303298	computer simulation;computer architecture;verification;formal methods;formal verification;software verification;computer science;theoretical computer science;formal specification;runtime verification;programming language	Logic	-43.454837921471885	30.45821302585557	11316
9f37afaffae4a41a38d67a051b100830064b28af	reconstructing development artifacts for change impact analysis		Software architectural models are widely used to represent the structure of software systems. Software systems need to evolve continuously during their life time, for instance, to adapt to new requirements. During the evolution various change requests have to be implemented. However, analysing the architecture of a system alone does not provide sufficient information for an adequate estimation of the impact resulting by such change requests. In addition, many other development artifacts, such as test cases, have to be considered. Creating models of these artifacts by hand is time-consuming and error-prone. In this paper, we present an approach that automatically extracts development artifacts and annotates them to a software architectural model.	change request;cognitive dimensions of notations;requirement;software architectural model;software system;test case	Kiana Rostami;Michael Langhammer;Axel Busch;Joshua Gleitze;Robert Heinrich;Ralf H. Reussner	2017	Softwaretechnik-Trends		systems engineering;computer science;change impact analysis	SE	-55.1210506679247	27.83532661647331	11325
754c7944a06af3241040dddf9601f4994b376242	integration of fms performance evaluation models using patterns for an information system design	tecnologia industrial tecnologia mecanica;object oriented model;bottom up;computacion informatica;performance evaluation;numerical method;production system;flexible manufacturing system;ciencias basicas y experimentales;patterns;information system;tecnologias;point of view;information system design;grupo a;object oriented modeling;integration of viewpoints;meta model	The objective of our work is to create an information system able to integrate different view points concerning the design and the control of a Flexible Manufacturing System. Numerous methods based on generic reference frameworks have been proposed for the modeling of all the aspects of production systems. These models have to be instantiated in order to be applied to specific systems. The difficulties are then to integrate pre-existing models of the studied systems with those obtained by the instantiation of the generic reference framework. The proposed approach tackles the problem from the information system point of view. A meta-modeling bottom-up approach is presented based on the notion of patterns in order to facilitate the integration step. An example concerned with the performance evaluation and scheduling of a production system is presented to show how the product viewpoint can be built in order to be integrated later with other viewpoints. q 2004 Elsevier Ltd. All rights reserved.	bottom-up parsing;information system;mathematical optimization;metamodeling;performance evaluation;petri net;production system (computer science);relational database management system;scheduling (computing);systems design;top-down and bottom-up design;unified modeling language;universal instantiation	Michel Bigand;Ouajdi Korbaa;Jean Pierre Bourey	2004	Computers & Industrial Engineering	10.1016/j.cie.2004.05.014	metamodeling;simulation;numerical analysis;computer science;systems engineering;engineering;artificial intelligence;operations management;top-down and bottom-up design;mathematics;production system;pattern;information system	AI	-58.460546795395736	12.534204060990845	11331
fc64f6a4ef88ce5c8deaae506b0c0d31bd4448b8	a static and dynamic visual debugger for malware analysis	reverse engineering computer viruses data visualisation internet program debugging;visualization malware debugging monitoring software reverse engineering;visualization debugger;data visualisation;computer viruses;internet;visualization debugger static analysis dynamic analysis;program debugging;static analysis;dynamic analysis;reverse engineering;control flow graph information static visual debugger dynamic visual debugger malware analysis malicious software internet reverse engineering binary executable software viruses trojans general security flow machine instructions graph visualization system execution flow graph	The number of viruses and malware has grown dramatically over the last few years, and this number is expected to grow in all likelihood. Due to the increasing amount of malicious software circulated over the Internet, it is almost impossible to reverse engineering all binary executable software line by line as it is very challenging and time consuming. In order to provide immediate security solutions and reduce the amount of time on understanding malicious portion consisted in viruses, Trojans and other general security flow, a comprehensive design of visual debugger is introduced in this paper. The research involves with the reverse engineering of binary executable by transforming a stream of bytes that constitutes the program into a corresponding sequence of machine instructions. Both static and dynamic debugger will be developed and interacted with a graph visualization system to visualize the parse instructions of a targeted executable file in execution flow graph. With the intention of improving the effectiveness, graph visualization is developed to accelerate the analysis progress. We reconstruct the targeted program's control flow and broke it into smaller regions. Fragment of malicious instructions can be easily determined via the control flow graph information.	byte;control flow graph;debugger;debugging;executable;graph drawing;interaction;internet;malware analysis;parsing;reverse engineering;trojan horse (computing)	Chan Lee Yee;Lee Ling Chuan;Mahamod Ismail;Nasharuddin Zainal	2012	2012 18th Asia-Pacific Conference on Communications (APCC)	10.1109/APCC.2012.6388211	real-time computing;computer science;operating system;programming language	Security	-58.558037842550064	57.78580306949362	11334
8b70ed5b9b0f935e8cda4a00ce0f27388e458623	stealth malware analysis from kernel space with kolumbo	virus informatique;debugging;puesta a punto programa;virtual memory;gestion memoire;punto ruptura;virtualisacion proceso;storage management;securite informatique;point cassure;program verification;sistema de deteccion de intrusiones;debogage;breakpoint;computer security;gestion memoria;verificacion programa;virus informatico;process virtualization;seguridad informatica;computer virus;memoire virtuelle;intrusion detection systems;virtualisation processus;verification programme;systeme detection intrusion;memoria virtual	Most of today’s malware are able to detect traditional debuggers and change their behavior whenever somebody tries to analyze them. The analysis of such malware becomes then a much more complex task. In this paper, we present the functionalities provided by the Kolumbo kernel module that can help simplify the analysis of malware. Four functionalities are provided for the analyst: system calls monitoring, virtual memory contents dumping, pseudo-breakpoints insertion and eluding anti-debugging protections based on ptrace. The module as been designed to minimize its impact on the system and to be as undetectable as possible. However, it has not been conceived to analyze programs with kernel access.	32-bit;64-bit computing;binary file;breakpoint;debugging;gnu debugger;intel matrix raid;kernel (operating system);linux;loadable kernel module;malware analysis;multi-core processor;ptrace;sourceforge;stealth;subversion;system call;user space;strace	Julien Desfossez;Justine Dieppedale;Gabriel Girard	2009	Journal in Computer Virology	10.1007/s11416-009-0139-z	intrusion detection system;embedded system;computer science;virtual memory;operating system;cryptovirology;breakpoint;debugging;computer security;computer virus	Security	-56.57829792835435	54.61625416977408	11344
724775262513635cb678b94df825d46209f96ebc	mimd computers for scientific applications	scientific application	The purpose of this talk is twofold: to give an overview of the major parallel processor projects at IBM T.J. Watson research and then to discuss in some detail the VICTOR [1] message-passing parallel machine. Finally, concepts of a possible massively-parallel machine for very high performance are briefly discussed. In the first part of the talk the RP3 [2], the ACE [3], the GFII [4], the M/370, the Hybrid Dataflow [5], and the VLIW [6] projects will be discussed at a level sufficient to explain the key features of each of these projects, but without going into technical details. In summary, the RP3 is a 64-node shared memory machine, the ACE a 8-node shared bus workstation, the GF11 a switch-based SIMD machine with up to 576 nodes, the M/370 a project to study commercial applications of message-passing machines. The Hybrid Data Flow and the VLIW projects intend to explore very fine grained parallelism. The VICTOR project is a family of transputer based machines which were designed and build at T.J. Watson. It includes V32 and V256 with 32 and 256 nodes, respectively, and a set of 16-node workstations. All of these are presently operational. All machines except V32 are T800 based, with 4 MB of memory/node. V256 incorporates a distributed 16-node fileserver with 10 GB capacity. Special hardware supports multiple-users via spatial partitioning and non-intrusive processor monitoring. The operating environment is based on a mixture of vendor compiler and loader technology and a runtime system developed in the VICTOR group. In particular, this includes a set of routing routines and support for the filesystem. Languages used are OCCAM, C, Pascal and Fortran 77. Presently, the Trollius [7] operating system which has been developed at Cornell University is being ported onto VICTOR. Applications programs which have been written for VICTOR include presently fractals, ray-tracing, a nuclear physics Monte Carlo code, a computer pipeline model and a neural network code. Under development presently are two codes for VLSI design, dealing with parallel fault simulation and with circuit simulation. The latter one uses a parallel version of waveform relaxation techniques and is expected to be able to handle circuits with up to 106 transistors. Other applications being developed are graphical process monitors, database research, 3D-seismic code and multi-robot simulation. Work in the Mathematical Sciences department is focussed on developing a parallel language with emphasis on dynamic process scheduling. In the last part of the talk issues of scaling message-passing architectures into the Teraflop performance range are discussed.	mimd	Winfried W. Wilcke	1993	International Journal of High Speed Computing	10.1142/S0129053393000177	parallel computing;real-time computing;computer science;theoretical computer science;operating system;algorithm	HPC	-8.777777801127598	39.60389216719214	11346
a6accd217f6212486e09c3eaa6aa9c7504f1de3a	adoption of hierarchical structure for web document analysis in knowledge management system	hierarchical structure;cognition erbium fault trees text analysis search engines;web documents;search engine;document handling;knowledge management case based reasoning document handling internet;knowledge management;tourism management web document analysis knowledge management system evidential reasoning logical hierarchy structure link analysis watada;evidential reasoning;link analysis;internet;web document;link analysis evidential reasoning web document application fault tree analysis;knowledge management system;case based reasoning;application;fault tree analysis	The objective of this paper is to analyze a web structure by means of using evidential reasoning to logical hierarchy structure. During the searching on the web, the search engine will return a set of web documents. But some web documents do not fit what we are looking for. The targeted documents are called relevant document, and the rests are irrelevant documents. Our focus is placed on the web document structure and link analysis. The web documents are grouped in an appropriate label and organized in logical hierarchy structure. The theorems proposed by Watada will employed to analyze the value of concepts or events in logical hierarchy structure according to belief and plausibility functions. From these values “influence events” can be determining when an irrelevant document is included in the web document about Tourism Management.	document;knowledge management;link analysis;plausibility structure;relevance;web page;web search engine;world wide web	Rozlini Mohamed;Junzo Watada	2011	2011 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2011.6117999	case-based reasoning;web modeling;the internet;data web;fault tree analysis;link analysis;web standards;computer science;knowledge management;social semantic web;data mining;database;evidential reasoning approach;search engine	Web+IR	-42.39505873531328	6.67806557759066	11351
3c19c3c5087e3b8391eeb356eafe44d9b25a29af	composing services into structured processes	service composition;structured processes;collaborative cross organizational process management	Service composition languages like BPEL and many enactment tools only support structured process models, but most service composition approaches only consider unstructured process models. This paper defines an efficient algorithm that composes a set of cooperative services with their dependencies into a structured process. The algorithm takes as input a dependency graph and returns a structured process model that orchestrates the services while respecting their dependencies. The algorithm is embedded in a lightweight, semi-automated service composition approach, in which first dependencies between services are derived in a semi-automated way and next the algorithm is used to construct a structured composition. The approach has been implemented in a prototype that supports the dynamic formation and collaboration of dynamic virtual enterprises using cross-organizational service-oriented technology.	algorithm;business process execution language;control flow;embedded system;postcondition;process modeling;prototype;quality of service;semiconductor industry;service composability principle;service-oriented software engineering;sven jaschan;time complexity;workflow pattern	Rik Eshuis;Paul W. P. J. Grefen	2009	Int. J. Cooperative Inf. Syst.	10.1142/S0218843009002026	computer science;knowledge management;database;distributed computing	SE	-52.44665536080537	17.030971318134213	11372
97ae7420346e6ab74acab7c08d95ffef4626777e	thermal-aware on-line task allocation for 3d multi-core processor throughput optimization	temperature uniformity;three dimensional integrated circuits microprocessor chips multiprocessing systems thermal management packaging;thermal management packaging;multi core processor;lingo;resource management three dimensional displays throughput heat sinks mathematical model computational modeling runtime;optimization modeling software;thermal awareness;computer model;temperature uniformity multi core processor task allocation thermal awareness three dimensional integration throughput optimization;resource manager;resource management;runtime;3d multi core processor throughput optimization;three dimensional;emerging technology;computational modeling;heat sink;three dimensional displays;heat removal;time 0 932 ms;task to core allocation;mathematical model;3d ic;multiprocessing systems;throughput optimization;time 0 932 ms thermal aware on line task allocation 3d multicore processor throughput optimization three dimensional integrated circuit 3d ic heterogeneous integration heat removal optimization modeling software lingo task to core allocation;heterogeneous integration;thermal aware on line task allocation;heat sinks;3d multicore processor throughput optimization;optimization model;three dimensional integrated circuit;three dimensional integrated circuits;microprocessor chips;task allocation;throughput;three dimensional integration	Three-dimensional integrated circuit (3D IC) has become an emerging technology in view of its advantages in packing density and flexibility in heterogeneous integration. The multi-core processor (MCP), which is able to deliver equivalent performance with less power consumption, is a candidate for 3D implementation. However, when maximizing the throughput of 3D MCP, due to the inherent heat removal limitation, thermal issues must be taken into consideration. Furthermore, since the temperature of a core strongly depends on its location in the 3D MCP, a proper task allocation helps to alleviate any potential thermal problem and improve the throughput. In this paper, we present a thermal-aware on-line task allocation algorithm for 3D MCPs. The results of our experiments show that our proposed method achieves 16.32X runtime speedup, and 23.18% throughput improvement. These are comparable to the exhaustive solutions obtained from optimization modeling software LINGO. On average, our throughput is only 0.85% worse than that of the exhaustive method. In 128 task-to-core allocations, our method takes only 0.932 ms, which is 57.74 times faster than the previous work.	3d computer graphics;algorithm;experiment;lingo (programming language);mathematical optimization;multi-core processor;online and offline;set packing;speedup;three-dimensional integrated circuit;throughput	Chiao-Ling Lung;Yi-Lun Ho;Ding-Ming Kwai;Shih-Chieh Chang	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763008	three-dimensional integrated circuit;embedded system;parallel computing;real-time computing;computer science;resource management;operating system;heat sink	EDA	-4.96837253696702	57.610511933725206	11377
21eb86c431a4c98dd02d3a6558e6021bc2bba740	on testing uml statecharts	labeled transition system;formal semantics;test case generation;conformance testing;concurrent systems;model based testing;process algebra	We present a formal framework for notions related to testing and model based test generation for a behavioural subset of UML Statecharts (UMLSCs). This framework builds, on one hand, upon formal testing and conformance theory that has originally been developed in the context of process algebras and Labeled Transition Systems (LTSs), and, on the other hand, upon our previous work on formal semantics for UMLSCs. The paper covers the development of proper extensional testing preorders and equivalence for UMLSCs. We present an algorithm for testing equivalence verification which is based on an intensional characterization of the testing relations. Testing equivalence verification is reduced to bisimulation equivalence verification. We also address the issue of conformance testing and present a formal conformance relation together with a test case generation algorithm which is proved sound and exhaustive w.r.t. the conformance relation. We show results on the formal relationship of the testing relations with the conformance one. The comprehensive and uniform approach presented in this paper sets the theoretical basis for UMLSCs testing frameworks and makes them available for practitioners in industry where the UML has become a de facto standard, in particular there where it is used for the development of complex concurrent systems. © 2006 Elsevier Inc. All rights reserved.	algorithm;bisimulation;concurrency (computer science);conformance testing;formal verification;intensional logic;process calculus;recurrence relation;semantics (computer science);test case;turing completeness;uml state machine;unified modeling language;verification and validation	Mieke Massink;Diego Latella;Stefania Gnesi	2006	J. Log. Algebr. Program.	10.1016/j.jlap.2006.03.001	process calculus;discrete mathematics;model-based testing;computer science;conformance testing;formal semantics;mathematics;programming language;algorithm	SE	-43.86112251683525	28.393121892560572	11388
4949e6dbe9967617534a3e9e5d66e1694a541d74	an experimental applicative programming language for linguistics and string processing	string processing;experimental applicative programming language;programming language	"""A PATTERN determines the structure of the string to which it is matched. The pattern contains a sequence of concatenated elements, which are themselves PATTERNS, PRIMITIVE PATTERNS (utilizing most of the SNOBOL4 primitives) or STRINGS. The value returned by the pattern is either FALSE (if it fails to match) or a """"parse tree"""" designating the structure of the string that corresponds to portions of the pattern. As an example, suppose that a pattern is P:=p1^P2^...^pn"""" It may be matched to a string S=SoSl...s n by the use of the operator '~in"""", and if each of the Pi match a successive letter s j, one can conceptualize the """"tree"""" returned as"""	applicative programming language;comparison of programming languages (string functions);concatenation;parse tree;parsing;snobol;string (computer science)	P. A. C. Bailes;L. H. Reeker	1980			natural language processing;fourth-generation programming language;first-generation programming language;natural language programming;very high-level programming language;language primitive;object language;programming domain;computer science;programming language implementation;extensible programming;functional logic programming;linguistics;programming paradigm;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language theory;programming language;language technology;programming language specification;high-level programming language	Theory	-25.970223070859472	19.35093997692803	11393
804ce378ddbb3c62e1781494491e5165572f13b4	kb/rms: an intelligent assistant for requirement definition	context modeling natural language processing programming application software databases space technology software systems software engineering natural languages knowledge based systems;knowledge based system;system modeling;inference mechanisms;natural languages;indexing terms;software engineering;conceptual framework;design representation;formal method;context model;semantic model;knowledge acquisition;user interfaces inference mechanisms knowledge based systems natural languages software engineering;user interfaces;natural language processing;knowledge based systems;validation and verification;verification problem spaces intelligent assistant requirement definition requirements context model formal methods logical schema kb rms database knowledge based system natural language processing semantic model solution spaces inference driven augmentation validation;knowledge base	A conceptual framework and a system model for an intelligent assistant for requirement defition, KB/RMS, is presented. The requirement definition process is characterkd by the Requirements Context Model. Informal and formal methods for requirement defiition are considered in light of this model, which serves as the logical schema for the KB/RMS database. Conventional and knowledge-based system support for requirement defiition is summarized. The use of natural language processing, a semantic model of the problem and solution spaces, domain and technology models, and inference driven augmentation, validation, and verification of the semantic model is discussed. Production of design representations from the augmented semantic model is covered.	formal methods;kilobyte;knowledge-based systems;logical data model;natural language processing;requirement	Robert V. Binder;Jeffrey J. P. Tsai	1990		10.1109/TAI.1990.130407	semantic data model;natural language processing;verification and validation;systems modeling;index term;computer science;artificial intelligence;knowledge-based systems;conceptual framework;database;context model;natural language;programming language;user interface	AI	-51.63767325571896	21.25489850396881	11424
5bd9374195809c73157ba876f463ea7c4ec9abb5	mocgraph: scalable distributed graph processing using message online computing	会议论文	Existing distributed graph processing frameworks, e.g., Pregel, Giraph, GPS and GraphLab, mainly exploit main memory to support flexible graph operations for efficiency. Due to the complexity of graph analytics, huge memory space is required especially for those graph analytics that spawn large intermediate results. Existing frameworks may terminate abnormally or degrade performance seriously when the memory is exhausted or the external storage has to be used. In this paper, we propose MOCgraph, a scalable distributed graph processing framework to reduce the memory footprint and improve the scalability, based on message online computing. MOCgraph consumes incoming messages in a streaming manner, so as to handle larger graphs or more complex analytics with the same memory capacity. MOCgraph also exploits message online computing with external storage to provide an efficient out-of-core support. We implement MOCgraph on top of Apache Giraph, and test it against several representative graph algorithms on large graph datasets. Experiments illustrate that MOCgraph is efficient and memory-saving, especially for graph analytics with large intermediate results.	apache giraph;computer data storage;dspace;dataflow;external storage;graph (abstract data type);graph operations;graph theory;line graph;memory footprint;out-of-core algorithm;scalability;spawn (computing);speedup;terminate (software)	Chang Zhou;Jun Gao;Binbin Sun;Jeffrey Xu Yu	2014	PVLDB	10.14778/2735496.2735501	wait-for graph;computer science;theoretical computer science;data mining;database;distributed computing;graph database	DB	-16.980059361023127	54.24770568255568	11440
d9821288923e72c09f91dcf77feada2239526277	design and implementation of qbism, a 3d medical image database system	medical image database system	We describe the design and implementation of QBISM (Query By Interactive, Spatial Multimedia), a prototype for querying and visualizing 3D spatial data. Our driving application is in an area in medical research, in particular, Functional Brain Mapping. The system is built on top of the Starburst DBMS, extended to handle spatial data types, and, specifically, scalar fields and arbitrary regions of space within such fields. In this paper we list the requirements of the application, discuss the logical and physical database design issues, and present timing results from our prototype. We observed that the DBMS’ early spatial filtering results in significant performance savings because the system response time is dominated by the amount of data retrieved, transmitted, and rendered.		Manish Arya;William F. Cody;Christos Faloutsos;Joel E. Richardson;Arthur Toya	1996			scalar (physics);logical conjunction;database;spatial analysis;geographic information system;brain mapping;response time;spatial query;database design;computer science	Vision	-29.715423816427286	7.691177135046761	11444
d08b1a647ab700349595dd934df24452c94be47c	on the applicability of evolutionary computation for software defect prediction	software metrics evolutionary computation program testing software fault tolerance;friedman ranking software defect prediction defect removal long term error free operation software testing process high quality maintainable software products software practitioners defect prediction model software metrics software engineering problems hybridized evolutionary computation techniques apache software foundation defect collection and reporting system;software metrics defect prediction search based software engineering evolutionary computation;measurement evolutionary computation predictive models accuracy software systems computational modeling	Removal of defects is the key in ensuring long-term error free operation of a software system. Although improvements in the software testing process has resulted in better coverage, it is evident that some parts of a software system tend to be more defect prone than the other parts and identification of these parts can greatly benefit the software practitioners in order to deliver high quality maintainable software products. A defect prediction model is built by training a learner using the software metrics. These models can later be used to predict defective classes in a software system. Many studies have been conducted in the past for predicting defective classes in the early phases of the software development. However, the evolutionary computation techniques have not yet been explored for predicting defective classes. The nature of evolutionary computation techniques makes them better suited to the software engineering problems. In this study we explore the predictive ability of the evolutionary computation and hybridized evolutionary computation techniques for defect prediction. This work contributes to the literature by examining the effectiveness of the 15 evolutionary computation and hybridized evolutionary computation techniques to 5 datasets obtained from the Apache Software Foundation using the Defect Collection and Reporting System. The results are evaluated in terms of the values of accuracy. We further compare the evolutionary computation techniques using the Friedman ranking. The results suggest that the defect prediction models built using the evolutionary computation techniques perform well over all the datasets in terms of prediction accuracy.	algorithm;altered level of consciousness;artificial neural network;display resolution;evolutionary computation;machine learning;nat friedman;open-source software;phase-shift oscillator;population;software bug;software development;software development process;software engineering;software metric;software system;software testing	Ruchika Malhotra;Nakul Pritam;Yogesh Singh	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968592	reliability engineering;verification and validation;software sizing;software verification;search-based software engineering;computer science;systems engineering;software reliability testing;theoretical computer science;software development;software construction;software testing;software quality;software metric;software system	SE	-62.67995238455315	34.5546597037364	11454
6f12e8c47445796272bba56072133d2c0c34d0da	fostering a symbiotic handheld environment	handheld systems symbiotic handheld environment;symbiosis handheld computers personal digital assistants organisms portable computers wearable computers biology computing computer displays application software cities and towns;notebook computers	A short walk through the Akihabara Electrical Town in Tokyo (www.akiba.or.jp) reveals the latest trends in handheld devices, including their tremendous variety. Such devices range from high-end PDAs to miniature cell phones, MP3 players, digital cameras, GPS navigators, and devices that integrate all these capabilities into a single portable unit. Handheld devices, especially when endowed with a programmable computing engine and connectivity, let people work or entertain themselves while on the move. Their key advantages thus include portability, sole ownership, and privacy. However, handhelds face important limitations. They must be small and light enough for users to carry them. Size restrictions place a limit on their display and number of buttons or dials. Voice-based interfaces cannot always solve these problems because handheld use often occurs in public places. Further, from now through the near future, using powerful handhelds will be restricted by their relatively short battery life. Given these limitations, we can reasonably ask whether handhelds will become more common in the midterm future. For example, the widespread deployment of WiFi networks leads people to favor always-connected laptops over intermittently connected handhelds. It seems that connectivity, better I/O capabilities, and CPU power offset the larger size of these laptops. At the spectrum’s other end, wearable computers or even body-implanted computers could displace handhelds if these alternatives become easily available and safe. To predict the future of handhelds, we borrow some ideas and models from biology and look at our current computational environment as a jungle where multiple device types compete for market survival. As in nature, the fittest devices rapidly increase their presence and succeed. In this jungle, success is measured by how useful humans perceive a device to be. In nature, organisms pursue many different survival strategies, but we will explore only one survival mechanism that biological organisms use: symbiosis. Initially defined by Anton de Bary in 1879, symbiosis describes a mutually beneficial relationship between dissimilar organisms. Normally, we can distinguish three types of symbiotic relationships:	anton (computer);central processing unit;digital camera;gps navigation device;global positioning system;handheld game console;input/output;laptop;mp3;mobile device;mobile phone;personal digital assistant;software deployment;software portability;wearable computer	Mandayam T. Raghunath;Chandrasekhar Narayanaswami;Claudio S. Pinhanez	2003	IEEE Computer	10.1109/MC.2003.1231195	simulation;human–computer interaction;multimedia	HCI	-38.147173457449206	54.37450724175621	11462
57b2fa4d2f2bca840415b34fa17ebf2253b21d9f	information systems interoperability: what lies beneath?	semantic conflict resolution;query processing;heterogeneous databases;semantic interoperability;proof of concept;semantic heterogeneity;heterogeneous information;information integration;schema mapping;information system;mediators;conflict resolution;ontology	Interoperability is the most critical issue facing businesses that need to access information from multiple information systems. Our objective in this research is to develop a comprehensive framework and methodology to facilitate semantic interoperability among distributed and heterogeneous information systems. A comprehensive framework for managing various semantic conflicts is proposed. Our proposed framework provides a unified view of the underlying representational and reasoning formalism for the semantic mediation process. This framework is then used as a basis for automating the detection and resolution of semantic conflicts among heterogeneous information sources. We define several types of semantic mediators to achieve semantic interoperability. A domain-independent ontology is used to capture various semantic conflicts. A mediation-based query processing technique is developed to provide uniform and integrated access to the multiple heterogeneous databases. A usable prototype is implemented as a proof-of-concept for this work. Finally, the usefulness of our approach is evaluated using three cases in different application domains. Various heterogeneous datasets are used during the evaluation phase. The results of the evaluation suggest that correct identification and construction of both schema and ontology-schema mapping knowledge play very important roles in achieving interoperability at both the data and schema levels.	application domain;heterogeneous database system;information system;prototype;semantic interoperability;semantics (computer science);utility	Jinsoo Park;Sudha Ram	2004	ACM Trans. Inf. Syst.	10.1145/1028099.1028103	upper ontology;semantic interoperability;semantic computing;semantic integration;semantic grid;computer science;knowledge management;information integration;conflict resolution;ontology;data mining;semantic web stack;database;semantic technology;proof of concept;information retrieval;information system	DB	-38.30467005198381	6.048059100126358	11467
aaece2e6bef0c6f94c689cd43bdc82158d7d0d78	towards shrink-wrapped security: a taxonomy of security-relevant context	object recognition;context aware;availability;mobile computer;taxonomy information security mobile computing pervasive computing application software educational institutions portable computers communication system security availability appropriate technology;security of data mobile computing;guidelines;context aware security systems;shrink wrapped security;taxonomy;secure system;security relevant context taxonomy;context aware systems;mobile computing;ubiquitous access;security;corporate resources;context aware security systems shrink wrapped security security relevant context taxonomy mobile computing corporate resources ubiquitous access;tight coupling;security of data;context	The emerging mobile computing paradigm makes it feasible for workers to access corporate resources anytime and anywhere. While ubiquitous access has its benefits, it creates unique challenges for maintaining the security of these resources - challenges that traditional, context-insensitive approaches to security are not appropriate to handle. We propose the notion of shrink-wrapped security, in which a tight coupling is provided between a user's situation and security. In order to support shrink-wrapped security, a more comprehensive notion of context than what is currently used by context-aware security systems is necessary. This paper presents a first step towards achieving shrink-wrapped security by addressing a key challenge that developers of context-aware systems face…identifying relevant context.	anytime algorithm;context-aware pervasive systems;mobile computing;programming paradigm;shrink wrap contract;taxonomy (general)	Gleneesha M. Johnson	2009	2009 IEEE International Conference on Pervasive Computing and Communications	10.1109/PERCOM.2009.4912819	software security assurance;computer security model;cloud computing security;availability;security through obscurity;security information and event management;security engineering;security convergence;coupling;asset;computer science;security policy;operating system;cognitive neuroscience of visual object recognition;human-computer interaction in information security;security service;internet privacy;security testing;mobile computing;network security policy;world wide web;computer security	DB	-45.447545698051904	58.206405771131244	11470
5f938c62e251c665b8835f22ad15bb331d56eb22	improving inter-node communications in multi-core clusters using a contention-free process mapping algorithm	process mapping algorithm;high performance clusters;network interface;inter node communications	High performance clusters, which are established by connecting many computing nodes together, are known as one of main architectures to obtain extremely high performance. Currently, these systems are moving from multi-core architectures to many-core architectures to enhance their computational capabilities. This trend would eventually cause network interfaces to be a performance bottleneck because these interfaces are few in number and cannot handle multiple network requests at a time. The consequence of such issue would be higher waiting time at the network interface queue and lower performance. In this paper, we tackle this problem by introducing a process mapping algorithm, which attempts to improve inter-node communications in multi-core clusters. Our mapping strategy reduces accesses to the network interface by distributing communication-intensive processes among computing nodes, which leads to lower waiting time at the network interface queue. Performance results for synthetic and real workloads reveal that the proposed strategy improves the performance from 8 % up to 90 % in tested cases compared to other methods.	algorithm;computer cluster;dhrystone;manycore processor;multi-core processor;network interface controller;supercomputer;synthetic intelligence	Mohsen Soryani;Morteza Analoui;Ghobad Zarrinchian	2013	The Journal of Supercomputing	10.1007/s11227-013-0918-7	parallel computing;real-time computing;computer science;network interface;theoretical computer science;operating system;distributed computing	HPC	-10.183909053975762	46.35141911201571	11471
0065a1ee156120136c9fde8451e51cef2399495e	a semantic framework for identifying events in a service oriented architecture	fabrication;web services ontologies artificial intelligence semantic web;nonfunctional objectives;service request;service requirement descriptions;extended constrained rho;subsequent adaptation scheme semantic framework service oriented architecture adaptive middleware service requestor nonfunctional objectives service request service requirement descriptions semantic templates multiple ontologies event identification;performance evaluation;semantic association semantic web services soa sawsdl extended constrained rho middleware event driven programming;formal model;adaptive middleware;bepress selected works;service requestor;semantic templates;service oriented architecture ontologies web services semantic web computer science middleware feedback performance evaluation fires fabrication;soa;event driven programming;ontologies artificial intelligence;sawsdl;semantic web services;feedback;event identification;web services;semantic web;middleware;ontologies;subsequent adaptation scheme;computer science;multiple ontologies;service oriented architecture;fires;semantic framework;semantic association	We propose a semantic framework for automatically identifying events as a step towards developing an adaptive middleware for Service Oriented Architecture (SOA). Current related research focuses on adapting to events that violate certain non-functional objectives of the service requestor. Given the large of number of events that can happen during the execution of a service, identifying events that can impact the non-functional objectives of a service request is a key challenge. To address this problem we propose an approach that allows service requestors to create semantically rich service requirement descriptions, called semantic templates. We propose a formal model for expressing semantic templates and for measuring the relevance of an event to both the action being performed and the nonfunctional objectives. This model is extended to adjust the relevance of the events based on feedback from the underlying adaptation framework. We present an algorithm that utilizes multiple ontologies for identifying relevant events and present our evaluations that measure the efficiency of both the event identification and the subsequent adaptation scheme.	algorithm;association rule learning;feedback;mathematical model;middleware;ontology (information science);relevance;service-oriented architecture;ws-security	Karthik Gomadam;Ajith Ranabahu;Lakshmish Ramaswamy;Amit P. Sheth;Kunal Verma	2007	IEEE International Conference on Web Services (ICWS 2007)	10.1109/ICWS.2007.17	computer science;service-oriented architecture;data mining;database;law;world wide web	SE	-44.10720749604299	14.502652642171432	11475
b31d17258e3ec7b7fd3e00582c1148f95677caab	clone detection in source code by frequent itemset techniques	c++ language;java;prolog;xml;program debugging;source coding;c++;java;prolog;xml;clone detection;data mining;frequent itemset techniques;multiple programming languages;source code	In this paper we describe a new approach for the detection of clones in source code, which is inspired by the concept of frequent itemsets from data mining. The source code is represented as an abstract syntax tree in XML. Currently, such XML representations exist for instance for Java, C++, or PROLOG. Our approach is very flexible; it can be configured easily to work with multiple programming languages	abstract syntax tree;algorithm;association rule learning;c++;code refactoring;data mining;duplicate code;interactive computing;java;level of detail;nsa product types;preprocessor;programming language;prolog;xml;xslt;srcml	Vera Wahler;Dietmar Seipel;Jürgen Wolff von Gudenberg;Gregor Fischer	2004	Source Code Analysis and Manipulation, Fourth IEEE International Workshop on	10.1109/SCAM.2004.6	computer science;theoretical computer science;database;programming language;code generation;source code	SE	-53.8648830771142	34.88936708951058	11492
945a4775e7622ca0673abc829c903e9edac622e4	multithreading for degree controllable parallel string pattern matching with k-mismatches	pattern matching		multithreading (computer architecture);pattern matching;thread (computing)	Jin Hwan Park	2004			computer science;parallel computing;multithreading;pattern matching;distributed computing	ML	-10.47902359364616	42.2457081824891	11494
bd148ef3a62274cfdd58654eef205f003cae2a72	체언표현 개념분류체계와 owl 온톨로지의 상관관계 연구	owl web ontology language;rdf resource description framework;분류체계 taxonomy;시맨틱 웹 semantic web;온톨로지 ontology	컴퓨터에 의한 지능형 의미기반 지식/정보의 자동처리를 위해서는 사람이 보유하고 활용하는 상식을 포함한 지식을 정형화하고 체계적으로 표상하여 컴퓨터에게 이해시키고 활용할 수 있도록 하여야 한다. 이의 필요성은 각 분야에서 널리 공감되고 있고 온톨로지라는 지식/정보 표현 포맷으로 그 표상 형식이 수렴되고 있다. 그러나 사람이 가지고 있는 지식과 정보는 매우 비정형적이고 때로는 모호한 개념에 기반하고 있어, 이를 정형화하기가 어렵다. 본고에서는 비질료적인 개념에서 직접 온톨로지를 구축하지 않고 개념을 그대로 사상한다고 여겨지는 언어기호 간의 관계로부터 온톨로지를 구축하는 방법론을 논의한다. 기존의 개념분류체계에서 고찰된 개념 간의 관계와 언어학적으로 규명된 어휘 간의 관계가 밀접히 일치함을 보이고 바로 활용할 수 있는 자료가 풍부한 어휘 간의 관계로부터 온톨로지를 구축하는 구체적인 알고리듬을 제시한다. 여기에서 온톨로지 표현 포맷은 월드와이드웹 컨소시엄(W3C)의 OWL을 채택했다.		송도규	2006		10.1007/978-3-319-17885-1_100924	database;world wide web;information retrieval	NLP	-39.36761026579926	6.789054324145969	11499
bae84b0efc077880182d42f309404b314116b135	domain-specific languages for better forensic software			domain-specific language;list of digital forensics tools	Jeroen van den Bos;Tijs van der Storm	2012	ERCIM News		data mining;forensic science;computer science;software;domain-specific language	PL	-48.53117863709405	22.446485700847365	11514
2fbf63d6e22097bcf1acafb70dccef340e160dfc	applying the model-view-controller paradigm to adaptive test	analytical models;mvc;analog and rf test adaptive test yield learning mvc;adaptive testing;analog and rf test;model view controller;yield learning;model adaptation;testing;data model;software architecture;software architecture electronic engineering computing integrated circuit testing;adaptive systems;semiconductor device modeling;integrated circuit testing;engineering productivity model view controller paradigm adaptive testing ic testing integrated circuit testing mvc architecture;adaptation models adaptive systems semiconductor device modeling data models testing analytical models;electronic engineering computing;adaptation models;analytical model;data models;adaptive test	The paper states that adaptive testing has been a focus area for IC testing in the last few years. The “Model-View-Controller”(MVC) architecture has the potential to improve engineering productivity for analysis and application of Adaptive Testing.	model–view–controller;programming paradigm;semiconductor device fabrication	Nathan Kupp;Yiorgos Makris	2012	IEEE Design & Test of Computers	10.1109/MDT.2011.2179370	real-time computing;simulation;computer science;adaptive system;software engineering;computerized adaptive testing;model–view–controller;computer engineering	SE	-59.71859845100189	30.519111209796066	11515
370baef5b5f9e2933a195bc025c93feb02baf494	delay-hiding energy management mechanisms for dram	cache storage;data intensive application;delay energy management random access memory memory management power system management energy consumption hardware operating systems degradation energy efficiency;high memory energy savings delay hiding energy management mechanisms dram data intensive applications physical memory buffer cache power management energy consumption i o handling routines os kernel memory state transition;low power;energy consumption;power management;operating system kernels cache storage dram chips;operating system kernels;dram chips;state transition;energy saving;energy management	Current trends in data-intensive applications increase the demand for larger physical memory, resulting in the memory subsystem consuming a significant portion of system's energy. Furthermore, data-intensive applications heavily rely on a large buffer cache that occupies a majority of physical memory. Subsequently, we are focusing on the power management for physical memory dedicated to the buffer cache. Several techniques have been proposed to reduce energy consumption by transitioning DRAM into low-power states. However, transitions between different power states incur delays and may affect whole system performance. We take advantage of the I/O handling routines in the OS kernel to hide the delay incurred by the memory state transition so that performance degradation is minimized while maintaining high memory energy savings. Our evaluation shows that the best of the proposed mechanisms hides almost all transition latencies while only consuming 3% more energy as compared to the existing on-demand mechanism, which can expose significant delays.	computer data storage;data-intensive computing;dynamic random-access memory;elegant degradation;high memory;input/output;kernel (operating system);low-power broadcasting;mathematical optimization;operating system;page cache;power management;state transition table	Mingsong Bi;Ran Duan;Chris Gniady	2010	HPCA - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture	10.1109/HPCA.2010.5416646	uniform memory access;interleaved memory;semiconductor memory;parallel computing;real-time computing;cache coloring;static random-access memory;cpu cache;computer hardware;computer science;operating system;universal memory;registered memory;cache pollution;non-uniform memory access;memory management;energy management	Arch	-9.076926706306049	54.08451017079755	11516
200ec167d013b1392846ad9265dddb54bf56fdc1	framekit and the prototyping of case environments	distributed system;programming environments;software platform;software prototyping;user interface;reference model;development kit prototyping case environments framekit software platform rapid prototyping generic user interface;code generation;prototyping;computer aided software engineering;rapid prototyping;generic user interface;framekit;case environments;graphical representation;software component;software tools;software tools software prototyping computer aided software engineering programming environments;prototypes computer aided software engineering software prototyping computer architecture design methodology application software software engineering software tools operating systems throughput;development kit;model simulation;design methodology	Software engineering methodologies rely on various and complex graphical representations as SA-RT [ 2], OMT [16] Class-Relation [ 6] etc. They are more useful when associated to CASE (Computer Aided Software Engineering) tools designed to take care of constraints that have to be respected. Such tools help engineers and facilitate the promotion of such methods. However, implementation of such CASE tools is a complex task since they require various functions such as a graphical user interface, database facilities and, of course, the operations related to the methodology they implement (compilation, animation/simulation of specifications, code generation from specification, etc.). CASE tools may share a common architecture and rely on a software platform [ 10] enabling the sharing of software components (i.e. a model compiler may produce infor mation for both simulation and code generation) and easy tool evolution by addition of new functions. The impact of the ECMA-NIST reference model is important in the Software engineering industry. CASE tools have now given way to CASE environments which may be adapted to a specific adaptation of a design methodology. Thus, tools architecture have changed dramatically to support the new possibilities derived from these techniques. The ECMA-NIST reference model has been derived to other application domains : for example, CORBA [ 5] is dedicated to the integration of various application at a the source level. New Operating Systems, as Chorus, MACH, MacOS or Windows-95/NT may be considered as open shells, increasingly adapted to software plug-in. The evolution of Operating Systems goes with sophisticated implementation environments (and tools) composed of API’s (Application Programming Interfaces). Some development environments now enable cross development over a set of target platforms. As an example, Metrowerks CodeWarrior on macintosh [ 15] allows cross development over a set of arget architectures (hardware + OS) : Mac/MacOS, Mac/ OpenStep and PC/Win32/x86, Java, Be/BeOS, embedded processor PPC821/860 and MagicCap. In this paper, we present FrameKit, a software platform dedicated to the prototyping and quick implementation of CASE environments. FrameKit is valuable to quickly implement a CASE environment, either for evaluation purposes or as a final product, especially if graphical representations are involved. Our aim is to provide a generic CASE environment that can be filled with specific information enabling to customize it in discrete ways. FrameKit emphazizes the following aspects : • A generic graphical user interface is able to be quickly adapted to a new graphical representation, • A system platform manages users and proposes services offered by tools to end-users; • The dynamic integration of software components is achieved without recompilation of the environment by means of configuration files; • Enhanced API’s for discrete languages is provided for Ada, C and Unix shell. Section 2 shows how we parameterized a CASE environment and section 3 presents our interpretation of the five integration axis introduced in [ 19]. Then, section 4 presents main characteristics of FrameKit and section 5 focuses on the construction procedure of a dedicated CASE environment. We present an example and discuss advantages of this prototyping approach.	ada;apache axis;application programming interface;beos;care-of address;chorusos;code generation (compiler);codewarrior;common object request broker architecture;compiler;component-based software engineering;computer-aided software engineering;ecmascript;embedded system;graphical user interface;java;openstep;operating system;plug-in (computing);reference model;simulation;unix shell	Fabrice Kordon;Jean-Luc Mounier	1997		10.1109/IWRSP.1997.618846	reference model;design methods;computer science;systems engineering;component-based software engineering;software engineering;prototype;programming language;user interface;computer-aided software engineering;code generation;computer engineering	SE	-40.08170504844863	35.197933734585405	11525
6a8f65381a627a2db6c756a7185d9106f0acefec	control-flow integrity	binary rewriting;control flow graph;safety properties;inlined reference monitors;vulnerabilities;control flow integrity;access control;security policy;software implementation	Current software attacks often build on exploits that subvert machine-code execution. The enforcement of a basic safety property, Control-Flow Integrity (CFI), can prevent such attacks from arbitrarily controlling program behavior. CFI enforcement is simple, and its guarantees can be established formally even with respect to powerful adversaries. Moreover, CFI enforcement is practical: it is compatible with existing software and can be done efficiently using software rewriting in commodity systems. Finally, CFI provides a useful foundation for enforcing further security policies, as we demonstrate with efficient software implementations of a protected shadow call stack and of access control for memory regions.	access control;call stack;control-flow integrity;exploit (computer security);rewriting	Martín Abadi;Mihai Budiu;Úlfar Erlingsson;Jay Ligatti	2005		10.1145/1102120.1102165	vulnerability;computer science;security policy;access control;database;distributed computing;computer security;control flow graph	Security	-54.8556873215322	54.39564799556014	11536
1fd46ec816f2a5ad9d48a05745058d2a6f4af5f9	melange: a meta-language for modular and reusable development of dsls	language reuse;melange;language composition;lan guage composition;model typing;domain specific languages	Domain-Specific Languages (DSLs) are now developed for a wide variety of domains to address specific concerns in the development of complex systems. When engineering new DSLs, it is likely that previous efforts spent on the development of other languages could be leveraged, especially when their domains overlap. However, legacy DSLs may not fit exactly the end user requirements and thus require further extension, restriction, or specialization. While current language workbenches provide import mechanisms, they usually lack an explicit support for such customizations of imported artifacts. In this paper, we propose an approach for building DSLs by safely assembling and customizing legacy DSLs artifacts. This approach is based on typing relations that provide a reasoning layer for manipulating DSLs while ensuring type safety. On top of this reasoning layer, we provide an algebra of operators for extending, restricting, and assembling separate DSL artifacts. We implemented the typing relations and algebra into the Melange meta-language. We illustrate Melange through the modular definition of an executable modeling language for the Internet Of Things domain. We show how it eases the definition of new DSLs by maximizing the reuse of legacy artifacts without introducing issues in terms of performance, technical ecosystem compatibility, or generated code volume.	artifact (software development);complex systems;digital subscriber line;domain-specific language;ecosystem;executable;internet of things;language workbench;modeling language;partial template specialization;requirement;type safety;user requirements document	Thomas Degueule;Benoît Combemale;Arnaud Blouin;Olivier Barais;Jean-Marc Jézéquel	2015		10.1145/2814251.2814252	real-time computing;computer science;domain-specific language;engineering;mélange;software engineering;programming language;engineering drawing;algorithm	SE	-48.58011761950935	25.375237582057576	11538
28efb87939eb4eb9fc795d85bf908abde4e147e5	noninterference for a practical difc-based operating system	dynamic programming;control systems;kernel;pediatrics;information security;information flow control;operating systems information security data security kernel control systems linux dynamic programming communication system security privacy artificial intelligence;communicating sequential process;resource management;flume system;data mining;web applications;covert channels;covert channel;communicating sequential processes;internet;operating system;practical difc based operating system;communicating sequential processes information flow control covert channels noninterference;security of data internet linux operating systems computers;artificial intelligence;linux;decentralized information flow control;noninterference proof practical difc based operating system flume system decentralized information flow control linux operating system web applications asbestos communicating sequential processes;linux operating system;security;noninterference proof;security of data;operating systems computers;privacy;noninterference;data models;operating systems;communication system security;data security;asbestos	The Flume system is an implementation of decentralized information flow control (DIFC) at the operating system level. Prior work has shown Flume can be implemented as a practical extension tothe Linux operating system, allowing real Web applications to achieve useful security guarantees. However, the question remains if the Flume system is actually secure. This paper compares Flume with other recent DIFC systems like Asbestos, arguing that the latter is inherently susceptible to certain wide-bandwidth covert channels, and proving their absence in Flume by means of a noninterference proof in the Communicating Sequential Processes formalism.	communicating sequential processes;covert channel;linux;non-interference (security);operating system;semantics (computer science)	Maxwell N. Krohn;Eran Tromer	2009	2009 30th IEEE Symposium on Security and Privacy	10.1109/SP.2009.23	covert channel;computer science;information security;theoretical computer science;operating system;distributed computing;programming language;computer security	Security	-53.04408908702414	52.68456604543574	11540
bae138f9e76637a917ca2a8faf1f5ce9edcfcacd	experience report: how do structural dependencies influence change propagation? an empirical study	structural dependencies;software analysis;dependency analysis;mining software repositories;software maintenance;software maintenance data mining java object oriented programming public domain software;java context couplings software systems terminology yttrium;software maintenance change propagation structural dependencies dependency analysis mining software repositories software analysis;change propagation;software repository mining structural dependencies change propagation real world object oriented systems maintenance cost code snapshots open source java projects low level entities dependency analysis	Real world object-oriented systems are composed of hundreds or even thousands of classes that are structurally interconnected in many different ways. In this highly complex scenario, it is unclear how changes propagate. Given the high maintenance cost brought by change propagation, these questions become particularly relevant in practice. In this paper, we set out to investigate the influence of structural dependencies on change propagation. We historically analyzed thousands of code snapshots coming from 4 open-source Java projects of different sizes and domains. Our results indicated that, in general, it is more likely that two artifacts will not co-change just because one depends on the other. However, the rate with which an artifact co-changes with another is higher when the former structurally depends on the latter. This rate becomes higher if we track down dependencies to the low-level entities that are changed in commits. This implies, for instance, that developers should be aware of dependencies on methods that are added or changed, as these dependencies tend to propagate changes more often. Finally, we also found several cases where software changes could not be justified using structural dependencies, meaning that co-changes might be induced by other subtler kinds of relationships.	commitment scheme;entity;high- and low-level;java;loose coupling;open-source software;sociotechnical system;software crisis;software development;software engineering;software propagation;software system;version control	Gustavo Ansaldi Oliva;Marco Aurélio Gerosa	2015	2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2015.7381818	dependency hell;software sizing;dependency theory;computer science;engineering;software development;software analysis pattern;software engineering;data mining;database;software maintenance;world wide web;dependence analysis	SE	-57.240505748756064	34.072720086938844	11542
43d31626601a4d4e63597e66b2a8fe8969e6dceb	a multiply hierarchical automaton semantics for the iwim coordination model	fibration;automata;iwim;coordination model;coordination	The drawbacks of programming coordination activities directly within the applications software that needs them are briefly reviewed. Coordination programming helps to separate concerns, making complex coordination protocols into standalone entities; permitting separate development, verification, maintenance, and reuse. The IWIM coordination model is described, and a formal automata theoretic version of the model is developed, capturing the essentials of the framework in a fibration based approach. Specifically, families of worker automata have their communication governed by a state of a manager automaton, whose transitions correspond to reconfigurations. To capture the generality of processes in IWIM systems, the construction is generalised so that process automata can display both manager and worker traits. The relationship with other formalisations of the IWIM conception of the coordination principle is explored.	asynchrony (computer programming);automaton;categorical logic;emulator;entity;formal language;indivisible;linear algebra;model of computation;pipeline (unix);process calculus;smoothing	Richard Banach;Farhad Arbab;George Angelos Papadopoulos;John R. W. Glauert	2003	J. UCS	10.3217/jucs-009-01-0002	simulation;computer science;artificial intelligence;automaton;fibration;algorithm	SE	-35.182401234007756	28.926989760277465	11570
3296853739e844628cafe01e677fa0482e21be16	constructing deliberative agents with case-based reasoning technology	multiagent system;raisonnement base sur cas;razonamiento fundado sobre caso;case base reasoning;agent based;business strategy;intelligence artificielle;computer network;autonomous agent;artificial intelligence;inteligencia artificial;case based reasoning;sistema multiagente;systeme multiagent	To appear in the INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS Abstract This paper shows how autonomous agents may by constructed with the help of casebased reasoning systems. The advantages and disadvantages of deliberative agents are discussed, and it is shown how to solve some of their inconvenient, especially those related to their implementation and adaptation. Internet is one of the most popular vehicle for disseminating and sharing information through computer networks and it is influencing in the business world. An agent-based solution is presented to illustrate how the proposed technology may facilitate and improve an e-business strategy.	agent-based model;autonomous robot;case-based reasoning;database;electronic business;enterprise life cycle;internet;reasoning system;strategic management	Juan Manuel Corchado;Rosalía Laza	2003	Int. J. Intell. Syst.	10.1002/int.10138	case-based reasoning;simulation;computer science;artificial intelligence;autonomous agent;operations research;strategic management	AI	-39.07526505136347	16.131583326296667	11577
42c9893bea77d880ccf4b01d3526e19c62eed585	a hybrid photonic burst-switched interconnection network for large-scale manycore system	3d stacking;flattened butterfly;manycore system;network on chip;photonic burst switching		interconnection;manycore processor;multi-core processor	Quanyou Feng;Huanzhong Li;Wenhua Dou	2012	IEICE Transactions		embedded system;parallel computing;real-time computing;computer science;network on a chip;computer network	HPC	-9.873514287091824	43.52994800579469	11582
91dc174a88dc27eb152bf48fbe5812c4a6c84af8	fuzzy markup language: a new solution for transparent intelligent agents	fuzzy control theory;programming language;specific purpose computer language fuzzy markup language transparent intelligent agent fuzzy control theory system controller legacy programming language heterogeneous virtual organization;programming languages control engineering computing fuzzy control multi agent systems;fuzzy control;specific purpose computer language;development process;multi agent systems;system controller;complex system;fuzzy markup language;virtual organization;intelligent agent;legacy programming language;control engineering computing;heterogeneous virtual organization;point of view;development time;institutional repository research archive oaister;markup language;programming languages;transparent intelligent agent	From an industrial and technological point of view, fuzzy control theory deals with the development of a particular system controller on a specific hardware by means of an open or legacy programming language that is useful to address, in a high-level fashion, the hardware constraints. Independently from the complexity of the addressed application problem, the development time may be very expensive if the designers want to exploit the possibility to collect different controllers in terms of an active, cooperative and heterogeneous virtual organization. In order to bridge this gap, we introduce the Fuzzy Markup Language (FML), a novel specific-purpose computer language that defines a detailed structure of fuzzy control independent from its legacy representation. This choice allows systems' designers to to express their ideas in fast and simple way and, as a consequence, speeds up the whole development process of a given complex system.	ambient intelligence;complex system;computational intelligence;computer language;control theory;distributed computing;face modeling language;fuzzy control system;fuzzy markup language;high- and low-level;intelligent agent;legacy code;programming language;smart environment;virtual organization (grid computing)	Giovanni Acampora;Vincenzo Loia	2011	2011 IEEE Symposium on Intelligent Agent (IA)	10.1109/IA.2011.5953621	computer science;artificial intelligence;theoretical computer science;operating system;multi-agent system;database;markup language;programming language;world wide web;intelligent agent;software development process;fuzzy control language	Robotics	-44.11491054940906	22.825451243312283	11586
07ed9394930f578ddc9a601f289daeb82b3601a6	ontology based and context-aware hospital nurse call optimization	owl;context aware;heart;context information;medical administrative data processing;information extraction;ontologies algorithm design and analysis knowledge based systems data mining context modeling heart medical services;hospitals;software systems;web service;data mining;ontologies artificial intelligence;web service interface;context aware hospital nurse call optimization;osgi framework;information extraction ontology context aware hospital nurse call optimization context information reasoning algorithms person oriented nurse call system casp context framework osgi framework owl web service interface knowledge base;medical services;technology and engineering;web services health care knowledge based systems medical administrative data processing ontologies artificial intelligence;casp context aware ontology nurse call osgi;casp context framework;web services;reasoning algorithms;mobile handsets;competitive intelligence;ontologies;nurse call;osgi;person oriented nurse call system;context modeling;use case;ontology;algorithm design and analysis;knowledge based systems;knowledge base;context aware services;health care;casp	In this paper, the focus is on how context information can be efficiently modeled with an ontology. This ontology can than be used by reasoning algorithms which are based on this context information. This is illustrated with a use case which studies the evolution from a place oriented to a person oriented nurse call system. An ontology was designed which holds the necessary context information. A nurse call algorithm that uses this information was constructed. The CASP Context framework was extended to implement the use case. This framework is bases on an OSGi framework. Rules are formulated to implement the algorithm. OWL was applied to integrate the ontology into the framework. A Web service interface was designed which allows to insert new information into the knowledge base or extract information from it. At last a simulation was set up to show the advantages of the person oriented approach. The results of a performance study are shown as well.	algorithm;casp;knowledge base;osgi;simulation;web service	Femke Ongenae;Matthias Strobbe;Jan Hollez;Gregory De Jans;Filip De Turck;Tom Dhaene;Piet Demeester;Piet Verhoeve	2008	2008 International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2008.80	upper ontology;computer science;knowledge management;ontology;data mining;ontology-based data integration;world wide web;process ontology;suggested upper merged ontology	Robotics	-41.67771480446328	14.347647611017555	11591
2ea4e10e320bd7749784706fe6bab78ab5327d21	ccmc: a conditional csl model checker for continuous-time markov chains		We present CCMC (Conditional CSL Model Checker), a model checker for continuous-time Markov chains (CTMCs) with respect to properties specified in continuous-time stochastic logic (CSL). Existing CTMC model checkers such as PRISM or MRMC handle only binary CSL until path formulas. CCMC is the first tool that supports algorithms for analyzing multiple until path formulas. Moreover, CCMC supports a recent extension of CSL – conditional CSL – which makes it possible to verify a larger class of properties on CTMC models. Our tool is based on our recent algorithmic advances for CSL, that construct a stratified CTMC before performing transient probability analyses. The stratified CTMC is a product obtained from the original CTMC and an automaton extracted from a given formula, aiming to filter out the irrelevant paths and make the computation more efficient.	algorithm;automaton;computation;markov reward model checker;markov chain;model checking;prism (surveillance program);relevance	Yang Gao;Ernst Moritz Hahn;Naijun Zhan;Lijun Zhang	2013		10.1007/978-3-319-02444-8_36	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-11.14760647253727	27.29732178881933	11601
e547fe7a8a1802a243a031fa4b810b9c7f900947	supporting natural language updates in database systems.	database system;natural language		natural language;relational database management system	David Maier;Sharon C. Salveter	1982			natural language processing;data definition language;database theory;universal networking language;question answering;database server;intelligent database;data manipulation language;natural language user interface;data control language;computer science;database;natural language;programming language;temporal annotation;database schema;database testing;database design	DB	-31.559454350158244	8.95984756662189	11602
696529a35e7c5d3ae8ed88c8856f2d244664524d	multi-context systems with activation rules	information retrieval;004 informatik;natural extension;context dependent;ddc 004	Multi-Context Systems provide a formal basis for the integration of knowledge from different knowledge sources. Yet, it is easy to conceive of applications where not all knowledge sources may be used together all the time. We present a natural extension of Multi-Context Systems by adding the notion of activation rules that allows modeling the applicability or relevance of contexts depending on beliefs in the various contexts and their mutual dependencies. We give a short account on possible consequence relations for Multi-Context System with Activation Rules and discuss a potential application in information retrieval.	closed-world assumption;information retrieval;knowledge representation and reasoning;open research;open world;open-world assumption;relevance	Stefan Mandl;Bernd Ludwig	2010		10.1007/978-3-642-16111-7_15	computer science;artificial intelligence;data mining;communication	AI	-21.20244744995621	7.826392902847316	11605
0039a00642d1da703d297b18f1ceee63b5f04548	implementing a distributed lecture-on-demand multimedia presentation system	multimedia systems streaming media computer aided instruction job shop scheduling computer science application software tv time factors speech control systems;control systems;extended timed petri net model;distance learning environment;web based presentation system;distance learning environment distributed lecture on demand multimedia presentation system web based presentation system extended timed petri net model extended media streaming technologies browser windows media services live video synchronized images presentation slides;application software;job shop scheduling;computer aided instruction;distance learning;distributed multimedia;speech;synchronized images;video on demand multimedia computing multimedia communication internet distance learning educational technology petri nets;distributed lecture on demand multimedia presentation system;multimedia systems;interactive tv;multimedia computing;presentation slides;time petri net;time factors;internet;streaming media;live video;video on demand;multimedia communication;extended media streaming technologies;media streaming;computer science;tv;petri nets;windows media services;educational technology;multimedia presentation;browser;communication service;time constraint	Lecture-on-demand (LOD) multimedia presentation technologies in networks are most often used in communication services. Examples of those applications include video-on demand, interactive TV and communication tools in a distance learning system, etc. In this paper, we describe how to present different multimedia objects on a Web-based presentation system. The distributed approach is based on an extended timed Petri net model. Using characterization of extended media streaming technologies, we developed a Web-based multimedia presentation system. For a real-world example, suppose a well-known teacher is giving a lecture/presentation to his students. Because of time constraints and other commitments, many students cannot attend the presentation. The main goal of our system is to provide a feasible method to record and represent a lecture/presentation. Using a browser with windows media services allows those students to view live video of the teacher giving his speech, along with synchronized images of his presentation slides and all the annotations/comments. In our experience, this approach is sufficient for distance learning environments.		Lawrence Y. Deng;Timothy K. Shih;Sheng-Hua Shiau;Wen-Chih Chang;Yi-Jen Liu	2002		10.1109/ICDCSW.2002.1030756	distance education;job shop scheduling;educational technology;presentation logic;application software;the internet;simulation;computer science;speech;operating system;database;distributed computing;multimedia;law;world wide web;petri net	HPC	-33.06431710776371	35.46514326461296	11606
40462a37501769818f068ac3649269e8c8378989	semi-automated adaptation of service interactions	service adaptation;web service;service protocol adaptation;service interface matching;web services	In today's Web, many functionality-wise similar Web services are offered through heterogeneous interfaces (operation definitions) and business protocols (ordering constraints defined on legal operation invocation sequences). The typical approach to enable interoperation in such a heterogeneous setting is through developing adapters. There have been approaches for classifying possible mismatches between service interfaces and business protocols to facilitate adapter development. However, the hard job is that of identifying, given two service specifications, the actual mismatches between their interfaces and business protocols.  In this paper we present novel techniques and a tool that provides semi-automated support for identifying and resolution of mismatches between service interfaces and protocols, and for generating adapter specification. We make the following main contributions: (i) we identify mismatches between service interfaces, which leads to finding mismatches of type of signature, merge/split, and extra/missing messages; (ii) we identify all ordering mismatches between service protocols and generate a tree, called mismatch tree, for mismatches that require developers' input for their resolution. In addition, we provide semi-automated support in analyzing the mismatch tree to help in resolving such mismatches. We have implemented the approach in a tool inside IBM WID (WebSphere Integration Developer). Our experiments with some real-world case studies show the viability of the proposed approach. The methods and tool are significant in that they considerably simplify the problem of adapting services so that interoperation is possible.	experiment;interaction;interoperation;semiconductor industry;web service;websphere integration developer	Hamid R. Motahari Nezhad;Boualem Benatallah;Axel Martens;Francisco Curbera;Fabio Casati	2007		10.1145/1242572.1242706	web service;computer science;database;distributed computing;law;world wide web	Web+IR	-41.16894763348174	11.714764186376543	11608
6ec2bbaf0c46d9710dfbbe702241a562c960c9e7	a review of computer applications in raw sugar manufacture.	computer application			C. R. Murry	1974	Australian Computer Journal		computer science;industrial engineering;process engineering	Theory	-58.445080098638314	6.571574154543876	11609
4dead18ff9d9fded9ca2712f145080815ab6e325	managing stack frames in smalltalk	programming language;garbage collection;reference counting	The Smalltalk programming language allows contexts (stack frames) to be accessed and manipulated in very general ways. This sometimes requires that contexts be retained even after they have terminated executing, and that they be reclaimed other than by LIFO stack discipline. The authoritative definition of Smalltalk [Goldberg and Robson 83] uses reference counting garbage collection to manage contexts, an approach found to be inadequate in practice [Krasner, et al. 83]. Deutsch and Schiffman have described a technique that uses an actual stack as much as possible [Deutsch and Schiffman 84]. Here we offer a less complex technique that we expect will have lower total overhead and reclaim many frames sooner and more easily. We are implementing our technique as part of a state of the art Smalltalk interpreter. The approach may apply to other languages that allow indefinite lifetimes for execution contexts, be they interpreted or compiled.	compiler;drew mcdermott;expect;garbage collection (computer science);intel turbo memory;interpreter (computing);overhead (computing);programming language;reference counting;smalltalk	J. Eliot B. Moss	1987		10.1145/29650.29675	reference counting;computer science;database;garbage collection;programming language;algorithm	PL	-19.52662079894123	34.12000028599857	11618
4da69dd8aebf0fbd60745fe80132b4efce7c34b7	a scalable snoopy coherence scheme on distributed shared-memory multiprocessors	broadcasting;bandwidth;cache memory;computer science;shared memory;distributed shared memory;topology;coherence	Maintaining data coherence among multiple caches is one of the most important problems in building shared-memory multiprocessors. This problem has been efficiently solved on small-scale systems by using snoopy schemes based on the single-bus-connected architecture. Because of the development of highperformance processors, the bottleneck caused by the shared bus will significantly limit the allowable number of processors. We present a scalable snoopy scheme based on a single-hop-connected multiple-bus topology. To reduce the hardware complexity, each processor will snoop on a dynamically changing subset of the busses. The physicallydistributed-logically-shared memory model is chosen to let processors take advantage of local memory accesses for private data.	bus network;cache coherence;central processing unit;distributed shared memory;information privacy;scalability;scheme;snoop	Tyan-Shu Jou;Richard J. Enbody	1992			distributed shared memory;shared memory;snoopy cache;parallel computing;real-time computing;coherence;cpu cache;computer science;operating system;distributed computing;broadcasting;bandwidth	HPC	-11.862333645515202	49.375190008790725	11624
db5077224f66aa349a67bcb78004695b1f667d2f	geophysical parameters retrieval from sentinel-1 sar data: a case study for high performance computing at eodc		In this paper, we show first experimental results of big data processing for geophysical parameters retrieval from Synthetic Aperture Radar (SAR) data, carried out at the Earth Observation Data Center (EODC) for Water Resources Monitoring. The EODC is utilizing the high-performance computing platform provided by the third generation of the Vienna Scientific Cluster (VSC-3). Different Level-1 SAR datasets including data acquisitions from Sentinel-1 in four case studies were processed at the VSC-3 using the new SAR Geophysical Retrieval Toolbox (SGRT) developed at the Vienna University of Technology. SGRT produces a series of different geophysical parameters through time series analysis of satellite SAR data. This study presents the recent improvements and new features of the SGRT together with the findings obtained from the case studies performed at the EODC high-performance data processing environment.	supercomputer	Vahid Naeimi;Stefano Elefante;Senmao Cao;Wolfgang Wagner;Alena Dostálová;Bernhard Bauer-Marschallinger	2016			supercomputer;computer science;specific absorption rate;data science;data mining	HPC	-26.273261079519255	15.44664676202865	11645
882cee09812c9ab77986a4cef53459ebd955b1a5	letter from the tc chair		This paper reviews key query optimization techniques required by industrial-strength commercial query optimizers, using the DB2 family of relational database products as examples. The currently available members of this family are DB2/2, DB2/6000, and DB2 for MVS1.	mathematical optimization;query optimization;relational database	Rakesh Agrawal	1993	IEEE Data Eng. Bull.		data mining;computer science	DB	-31.099729458570263	6.500655068083129	11648
5aa354e91ece0bdbae25e4c808757a3e33862048	learning c# - introducing the language, .net programming and object-oriented software development	object oriented software development		software development	Jesse Liberty	2002			natural language processing;computer science;software design;software framework;component-based software engineering;software development;software engineering;software construction;programming paradigm;programming language;resource-oriented architecture;system programming;software development process	PL	-50.9403387033409	28.799567667879202	11660
32c63a56f886726c796c10a4ff46db5fb6ed6ef0	balancing memory and performance through selective flushing of software code caches	embedded system;software dynamic translation;code cache;execution environment;virtual execution environments;dynamic binary translation;eviction;multicore processors;flushing	Dynamic binary translators (DBTs) are becoming increasingly important because of their power and flexibility. However, the high memory demands of DBTs present an obstacle for all platforms, and especially embedded systems. The memory demand is typically controlled by placing a limit on cached translations and forcing the DBT to flush all translations upon reaching the limit. This solution manifests as a performance inefficiency because many flushed translations require retranslation. Ideally, translations should be selectively flushed to minimize retranslations for a given memory limit. However, three obstacles exist:(1) it is difficult to predict which selections will minimize retranslation,(2) selective flushing results in greater book-keeping overheads than full flushing, and(3) the emergence of multicore processors and multi-threaded programming complicates most flushing algorithms. These issues have led to the widespread adoption of full flushing as a standard protocol. In this paper, we present a partial flushing approach aimed at reducing retranslation overhead and improving overall performance, given a fixed memory budget. Our technique applies uniformly to single-threaded and multi-threaded guest applications	algorithm;allocate-on-flush;central processing unit;embedded system;emergence;high memory;multi-core processor;overhead (computing);thread (computing)	Apala Guha;Kim M. Hazelwood;Mary Lou Soffa	2010		10.1145/1878921.1878923	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	Arch	-5.294869559353895	53.307976052116146	11664
1135fb105b1cf56bf68c3603d62a5a5fab891c5e	cache-based model checking of networked applications: from linear to branching time	yarn;communication scheduling;redundant communication operations;caching;software model checking;networked applications;software verification;radiation detectors;space exploration;program verification;input output;cache based model checking;real life programs;servers;yarn processor scheduling application software electronic mail state space methods space exploration communication system control software testing system testing software engineering;state space exploration scheme;model checking;state space;linear time;networking;backtracking;schedules;branching time cache;caching software model checking software verification networking input output;linear time cache;peer to peer computing;branching time cache cache based model checking networked applications state space exploration scheme backtracking redundant communication operations real life programs linear time cache;data models	Many applications are concurrent and communicate over a network. The non-determinism in the thread and communication schedules makes it desirable to model check such systems. However, a simple state space exploration scheme is not applicable, as backtracking results in repeated communication operations. A cache-based approach solves this problem by hiding redundant communication operations from the environment. In this work, we propose a change from a linear-time to a branching-time cache, allowing us to relax restrictions in previous work regarding communication traces that differ between schedules. We successfully applied the new algorithm to real-life programs where a previous solution is not applicable.	backtracking;cpu cache;concurrent computing;model checking;nondeterministic algorithm;relax ng;real life;state space;time complexity;tracing (software)	Cyrille Artho;Watcharin Leungwattanakit;Masami Hagiya;Yoshinori Tanabe;Mitsuharu Yamamoto	2009	2009 IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2009.43	time complexity;model checking;input/output;data modeling;real-time computing;software verification;schedule;computer science;state space;theoretical computer science;space exploration;distributed computing;programming language;particle detector;server;backtracking	SE	-16.729666471489953	29.864248459871007	11665
73967230b1f2eed172cc67eb72a67f273911cc18	composing lifetime enhancing techniques for non-volatile main memories		Emerging byte-addressable non-volatile memory (NVM) technologies, such as PCM and ReRAM, offer significant gains in terms of density and power consumption over their volatile counterparts. Their write endurance is, however, orders of magnitude lower than DRAM, potentially causing devices to fail in seconds. Therefore, to use NVM as DRAM replacement, writes must be managed carefully.  In this paper, we study the endurance problem for NVM main memories with realistic server workloads. We explore three existing techniques to extend NVM lifetime: last-level cache replacement policies, compression, and NVM wear-leveling. The first two approaches increase lifetime by reducing the write traffic from the cache to the main memory. Wear-leveling spreads writes and reduces hotspots responsible for fast failures.  Even though custom replacement policies and compression are common in DRAM caches inside NAND flash devices, we find that they provide insufficient lifetime gains for NVM main memories with realistic server workloads. Caching writes is effective, but adapting the replacement policy only provides modest write reductions by 10%, while compression schemes must quadruple the cache capacity to achieve reductions of 20%. In both cases, the lifetime increases by an order of magnitude, which, for example, translates in an improvement from 12 days to 6 months for PCM. In contrast, wear-leveling algorithms can increase overall lifetime by at least two orders of magnitude, for instance, from 12 days to 15 years for PCM. These results indicate that wear-leveling techniques are more promising to ensure that NVM technologies are feasible to use as DRAM replacement.	algorithm;best, worst and average case;byte addressing;cpu cache;computer data storage;dspace;data compression;dynamic problem (algorithms);dynamic random-access memory;experiment;flash memory;memcached;memory footprint;non-volatile memory;pagerank;quadruple-precision floating-point format;resistive random-access memory;server (computing);volatile memory;wear leveling	Andrés Amaya García;René de Jong;William Wang;Stephan Diestelhorst	2017		10.1145/3132402.3132411	order of magnitude;parallel computing;wear leveling;cache;nand gate;dram;resistive random-access memory;orders of magnitude (numbers);engineering	Arch	-10.341489294213822	54.35371948704751	11668
822ea2f796212712569e870946a8cd8ea495de36	efficient deduction in many-valued logics	many valued logics deduction;many valued logics;many valued logics theorem proving;multivalued logic cost accounting eyes;theorem proving;cost accounting;eyes;multivalued logic;automated theorem proving;automated theorem proving many valued logics deduction;many valued logic	This paper tries to identify the basic problems e.countered in automated theorem proving in many-valued logics and demonstrates to which extent they call be currently solved. To this end a .umber of recently developed techniques are reviewed. We list. tile avenues of research in manyvalued theorem proving that are in our eyes the most promising.	automated theorem proving;first-order predicate;logic programming;long division;method of analytic tableaux;microsoft outlook for mac;natural deduction;non-monotonic logic;rice's theorem;sicstus prolog	Reiner Hähnle	1994		10.1109/ISMVL.1994.302195	monoidal t-norm logic;t-norm fuzzy logics;discrete mathematics;computer science;łukasiewicz logic;mathematics;automated theorem proving;programming language;algorithm	Logic	-13.19684615981469	12.597335379276432	11674
e8503d70ddc9d5a10cbfbdadf3bab6953a743e64	automated debugging with tractable probabilistic programming	statistical relational ai;automated debugging;probabilistic programming	Probabilistic programming languages allow domain experts to specify generative models in a high-level language, and reason about those models using domainindependent algorithms. Given an input, a probabilistic program generates a distribution over outputs. In this work, we instead use probabilistic programming to explicitly reason about the distribution over programs, rather than outputs. We propose Tractable Probabilistic Programs (TPP), a language to represent rich probabilistic dependencies between different parts of a program; we make use of the recent work on sum-product networks to ensure that inference remains tractable. We explain how TPP can be applied to the problem of automated program debugging; given a corpus of buggy programs, a TPP model can be learned to capture a probability distribution over the location of the bug. The model can also incorporate additional sources of information, such as coverage statistics on test suites. We also briefly outline how TPP can be used to solve the more ambitious problem of fault correction, i.e. predicting the most probable true program conditioned on a buggy one. The ability to learn common patterns of bugs and incorporate multiple sources of information potentially makes TPP useful as a unifying framework for automated program debugging.	cobham's thesis;debugging;high- and low-level;high-level programming language;probabilistic turing machine;randomized algorithm;software bug;tpp;test suite	Aniruddh Nath;Pedro M. Domingos	2014			computer science;artificial intelligence;theoretical computer science;machine learning;algorithmic program debugging;algorithm;statistics	ML	-25.680134604126795	16.555141931644624	11689
05dd1c61a8c47baff40ffc62d00300eeb15c6bd6	an architectural approach to composing reputation-based trustworthy services	trust;auction mechanisms;software testing;protocols;atmospheric measurements;performance evaluation;software testing automatic testing software algorithms application software performance evaluation particle swarm optimization automation genetic algorithms simulated annealing software engineering;application software;auction based trust negotiation protocol reputation based trustworthy services service oriented architecture user recommendations auction mechanisms;particle measurements;automatic testing;trust management;reputation based trustworthy services;swinburne;simulated annealing;software engineering;force;empirical evidence;computer architecture;strategic behavior;auction mechanism;software architecture;engines;data privacy;user recommendations;particle swarm optimization;service oriented computing;software algorithms;genetic algorithms;quality of service;service oriented architecture;auction based trust negotiation protocol;user interfaces data privacy software architecture;user interfaces;software architecture service oriented computing trust auction mechanism;trust negotiation;automation	In SOA, Reputation-Based Trust (RBT) mechanism is applied to achieve trust management. RBT enables services to assess the trust level of other services based on the reputation accumulated from user recommendations. A key challenge to apply RBT is to prevent the strategic behavior of users when they provide recommendations -- they might give unfair ratings to benefit themselves. In this paper, we propose a novel architectural approach to integrating auction mechanisms into the trust framework to prevent benefits from untruthful incentives. In this architecture we define an auction-based trust negotiation protocol and realize it in the trust framework. The contribution of our architecture is that it scales and produces accurate results to achieve protection against untruthful incentives, especially when a majority of ratings are unfair, without the potential increase in a computation overhead. An example on a travel agent scenario is devised to collect empirical evidence.	centralized computing;computation;correctness (computer science);overhead (computing);service-oriented architecture;trust management (information system);trustworthy computing	Suronapee Phoomvuthisarn;Yan Liu;Jun Han	2010	2010 21st Australian Software Engineering Conference	10.1109/ASWEC.2010.15	information privacy;computer science;knowledge management;software engineering;service-oriented architecture;world wide web;computer security	SE	-45.61069637684626	56.2308288933216	11699
962d3d65091556226585d7f1a5217ce2f83566c9	an internet-based data mining engine in a steel factory installation	steel industry data mining distributed databases internet;information technology;application server;internet data mining search engines steel production facilities data analysis algorithm design and analysis databases information technology ip networks;data mining;scattered data;internet;steel industry;distributed databases;scattered data data mining engine steel factory installation industrial environment local databases information technologies internet network application servers	Data mining studies need a set of algorithms to analyze the data and a way to administer the data under study. In the industrial environment this data (specially low level data) is usually stored at several local databases spread over the factory installations. This paper describes a novel attempt to set up a data mining engine in a steel factory installation based on new information technologies (Internet network, application servers, etc.) to facilitate data mining studies involving such scattered data. Its main benefits and drawbacks are addressed.	data mining;internet	Ana López;J. Sirgo;R. Blanco;R. Janez;N. Abajo;R. Perez;M. Tarro	2003		10.1109/ETFA.2003.1247720	the internet;computer science;engineering;data mining;database;data stream mining;information technology;world wide web;application server	ML	-36.324570975850406	14.334121209337052	11704
377c08f5213f5bd6b07beca508041d96b7a1f88a	the system kato: detecting cases of plagiarism for answer-set programs	answer set programming;source code;program analysis;logic programs;plagiarism detection	Plagiarism detection is a growing need among educational institutions and solutions for different purposes exist. An important field in this direction is detecting cases of source-code plagiarism. In this paper, we present the tool Kato for supporting the detection of this kind of plagiarism in the area of answer-set programming (ASP). Currently, the tool is implemented for DLV programs but it is designed to handle other logic-programming dialects as well. We review the basic features of Kato, introduce its theoretical underpinnings, and discuss an application of Kato for plagiarism detection in the context of courses on logic programming at the Vienna University of Technology.	dlv;logic programming;sensor;syntactic monoid	Johannes Oetsch;Jörg Pührer;Martin Schwengerer;Hans Tompits	2010	TPLP	10.1017/S1471068410000402	program analysis;computer science;artificial intelligence;answer set programming;programming language;algorithm;source code	AI	-24.293517289927994	21.349127466019375	11708
4cfbb2ed3e4ac23df8e5a8a9d3afc7d6f1902fd1	mobile robots and autonomic ambient assisted living		The use of Smart Environments in the delivery of pervasive care is a research topic that has witnessed increasing interest in recent years. These environments aim to deliver pervasive care through ubiquitous sensing by monitoring the occupants Activities of Daily Living. In order for these environments to succeed in achieving their goal, it is crucial that sensors deployed in the environment perform faultlessly. In this research we investigate addressing anomalous sensor behavior through the utilization of a mobile robot. The robot’s role is twofold; it must provide substitution in the presence of suspected sensor faults and act as an observer of anomalous sensor behavior in order to understand the changes that occur in the behavior of sensors deployed within the environment over time. The aim of this work is to explore a paradigm shift to the use of Autonomic Ambient Assisted Living.We have discovered that the use of a mobile robot is a viable means of introducing this paradigm to a Smart Environment.	autonomic computing	Guanitta Brady;Roy Sterritt;F. George Wilkie	2015	Paladyn		embedded system;simulation;computer security	Robotics	-44.754613879972204	49.81222653959859	11711
d3afe9aa8490aadc120e32083b07f86767cfcef6	clap: clustered look-ahead prefetching for energy-efficient dram system	memory management;prefetching;memory intensive programs clap clustered look ahead prefetching scheme energy efficient dram system computer systems battery operated embedded mobile systems dram energy aware prefetching scheme row buffer probability energy conservation cluster potential future clustered dram accesses first ready first come first serve memory request scheduling row activation idle energy consumption prefetching based memory traffic clustering scheme;memory traffic clustering data prefetching dram energy saving low power;probability dram chips energy conservation energy consumption;energy consumption;prefetching energy consumption power demand memory management dram chips;power demand;dram chips	DRAM is one of the main sources of energy consumption in computer systems. Thus, reducing the energy consumption of DRAM can prolong the lifetime of battery-operated embedded/mobile systems. To this end, we propose a DRAM energy-aware prefetching scheme to increase row buffer hits and idle periods of DRAM by clustering its accesses. Although prefetching schemes have traditionally been used to improve the system performance, utilizing them for the energy conservation of DRAM has yet to be investigated. For such energy conservation, our scheme accurately predicts and clusters potential future DRAM accesses. Clustered DRAM accesses exploit a popular first-ready first-come first-serve memory request scheduling and a power-down mode of DRAM more effectively; the probability of row buffer hits and idle periods is significantly increased by our clustering scheme. As a result, large amounts of row activation and idle energy consumption, which are major energy consumption factors in modern DRAM, can be saved. Our prefetching-based memory traffic-clustering scheme was shown to reduce the power and energy consumption of DRAM and improve its performance by an average of 0.2%, 28.9%, and 15.7%, respectively, for memory-intensive programs.	cpu cache;cluster analysis;dynamic random-access memory;embedded system;experiment;parallel computing;scheduling (computing);task parallelism	Yebin Lee;Soontae Kim	2016	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2015.2488282	embedded system;parallel computing;real-time computing;cas latency;computer science;operating system;universal memory;memory management	Arch	-8.850013557794806	54.382207365655894	11714
e437692ba18a56edb444e02c8b3b5fac11771181	synergy-based workload management		Workload management aims at the efficient execution of queries on a database. In this context, scheduling plays a crucial role. A vast number of scheduling approaches have been developed, most of them belonging to one of two categories: analysis and monitoring. However, they mainly either focus only on one possible kind of impact of queries on each other’s execution time, or require an offline phase for information gathering. In contrast, the approach we pursue does not require any offline phase and flexibly adapts to any database system or hardware configuration. It bases on the fact that the multiple requests that are executed concurrently in a database for performance purposes may have a positive impact on the execution of each other, e.g. due to caching or complementary resource consumption, or impede each other’s execution, e.g. in the case of resource contention. Both kinds of impacts are reflected by the execution time of the workload. We apply a monitoring approach to derive those impacts – called synergies – between request types fully automated at runtime from measured execution times. Thereby, our approach works completely independent from changing synergies or configurations and easily handles new query types.	database;heterogeneous database system;online and offline;resource contention;run time (program lifecycle phase);scheduling (computing);synergy	Martina-Cezara Albutiu;Alfons Kemper	2009			real-time computing;computer science;foreground-background;database;distributed computing	DB	-21.18721173808677	57.430001474051785	11737
878e8ec0a751865e83de6f77a267e409e0117ba2	reasoning under incomplete information in artificial intelligence: a comparison of formalisms using a single example	artificial intelligence;incomplete information;single example;artificial intelligent	Why New Logics Are Needed. Default Logic. Nonmonotonic Modal Logics. Closed World Assumption and Circumscription. A Nonmonotonic Logic Based on Suppositions. Conditional Logics and u0027u0027Roughu0027u0027 Implications. Logics of Uncertainty. Fuzzy Logic. Numerical Quantifiers and Conditional Probabilities. Causal Models. Reasoning by Analogy. Truth-Maintenance Systems. Theories of Action and the Frame Problem. Revision Theory. A Tentative Conclusion. References. Index.	artificial intelligence	Léa Sombé	1990	Int. J. Intell. Syst.	10.1002/int.4550050404	t-norm fuzzy logics;artificial intelligence;non-monotonic logic;machine learning;mathematics;probabilistic logic;deductive reasoning;algorithm	AI	-16.93359337372959	9.68579344449415	11738
72f6a456170ea15cc5325173110c43bd7372187b	supporting ad hoc analyses on enterprise models	enterprise modeling	Enterprises are socio technical systems whose management involves multiple stakeholders each demanding for a distinct perspective on the enterprise. A large number of modeling languages for describing different viewpoints on an enterprise have been developed in practice and academia. These viewpoints typically reflect the areas of interest that certain stakeholders pertain in respect to the enterprise, e.g. process modeling, IT resource modeling. While these viewpoints support analysis confined to their area of interest, crosscutting analysis, e.g. of the IT support for processes, demand for ad hoc viewpoints spanning different areas of interest. In this paper, we present a building-block based approach to enable stakeholders to create ad hoc viewpoints on enterprise models, which are complementingly operationalized via a model-transformation based method for generating visualizations.	file spanning;hoc (programming language);model transformation;modeling language;process modeling	Sabine Buckl;Jens Gulden;Christian M. Schweda	2010			enterprise systems engineering;enterprise software;enterprise modelling;computer science;systems engineering;engineering;knowledge management;integrated enterprise modeling;data mining;database;enterprise information system	SE	-61.176242963444615	16.653483423727945	11750
41c1c56306ff7337d382a792caef3282c20bd530	a type safe design to allow the separation of different responsibilities into parallel hierarchies	refactoring;generics;software design;parametric polymorphism;design patterns	The Tease Apart Inheritance refactoring is used to avoid tangled inheritance hierarchies that lead to code duplication. This big refactoring creates two parallel hierarchies and uses delegation to invoke one from the other. One of the drawbacks of this approach is that the root class of the new refactored hierarchy should be general enough to provide all its services. This weakness commonly leads to meaningless interfaces that violate the Liskov substitution principle. This paper describes a behavioral design pattern that allows modularization of different responsibilities in separate hierarchies that collaborate to achieve a common goal. It allows using the specific interface of each class in the parallel hierarchy, without imposing a meaningless interface to its root class. The proposed design is type safe, meaning that the compile-time type checking ensures that no type error will be produced at runtime, avoiding the use of dynamic type checking and reflection.	behavioral pattern;c++;code refactoring;compile time;compiler;delegation (computer security);duplicate code;dynamic language runtime;high- and low-level;high-level programming language;interface (java);java;parametric polymorphism;reflection (computer programming);run time (program lifecycle phase);software design pattern;type safety;type system	Francisco Ortin;Miguel García	2011			mathematical optimization;simulation;mathematics;algorithm	PL	-25.858460726406605	29.70802440577354	11753
2456e22e885094c3b206cf7a9cadba5ae369264f	workflow management systems: ensuring organizational flexibility by possibilities of adaptation and negotiation	task performance;design principle;workflow management system;organizational structure	Characteristics of workflow management systems must be adaptable by the users themselves according to the dynamism of organizational structures and of the conditions of cooperative task performance. In the case of those systems, adaptations which are initiated by individuals also affect other users in most cases. Therefore, a process of negotiation is required. A design principle called negotiability should be introduced. It helps users affected by adaptations to comment on them or to reject, accept or modify them. Supposing that the configuration of workflow management systems leads to a fixing of organizational structures by technical means, adaptation based on negotiability can be used to make those structures more flexible.	comment (computer programming);computer multitasking	Thomas Herrmann	1995		10.1145/224019.224028	systems engineering;engineering;knowledge management;management science;workflow technology	HCI	-52.160203818944694	14.225657549539985	11763
5e5f795f74da8bb95c1cfdb4e1017421b76b35cd	design and development of algorithms for identifying termination of triggers in active databases	triggering graph;triggering graphs;active rules;tg;triggers;termination;active databases	An active database system is a conventional database system extended with a facility for managing triggers (or active rules). Active database systems can react to the occurrence of some predefined events automatically. In many applications, active rules may interact in complex and sometimes unpredictable ways, thus possibly yielding infinite rule executions by triggering each other indefinitely causing non-termination. Almost all of the work to date on termination analysis for active databases relies on checking that a directed graph called the ‘triggering graph’ for a set of rules is acyclic. This paper presents new algorithms for detecting termination/non-termination of rule execution using triggering graph. These algorithms detect termination/ non-termination among the rules in a triggering graph without any limitation on the number of rules.	active database;algorithm;database trigger;directed acyclic graph;directed graph;divergence (computer science);event condition action;sensor;termination analysis	R. Manicka Chezian;T. Devi	2011	IJISCM	10.1504/IJISCM.2011.044527	termination factor;real-time computing;computer science;data mining;distributed computing	DB	-18.226890361586367	23.31516624055096	11774
26e72340c47b7348e1b1de285f89dd96cc925b27	reducing memory interference in multicore systems via application-aware memory channel partitioning	system configuration;interference;memory controllers;memory scheduling;system performance;multicore;data allocation;memory systems;main memory	Main memory is a major shared resource among cores in a multicore system. If the interference between different applications' memory requests is not controlled effectively, system performance can degrade significantly. Previous work aimed to mitigate the problem of interference between applications by changing the scheduling policy in the memory controller, i.e., by prioritizing memory requests from applications in a way that benefits system performance.  In this paper, we first present an alternative approach to reducing inter-application interference in the memory system: application-aware memory channel partitioning (MCP). The idea is to map the data of applications that are likely to severely interfere with each other to different memory channels. The key principles are to partition onto separate channels 1) the data of light (memory non-intensive) and heavy (memory-intensive) applications, 2) the data of applications with low and high row-buffer locality.  Second, we observe that interference can be further reduced with a combination of memory channel partitioning and scheduling, which we call integrated memory partitioning and scheduling (IMPS). The key idea is to 1) always prioritize very light applications in the memory scheduler since such applications cause negligible interference to others, 2) use MCP to reduce interference among the remaining applications.  We evaluate MCP and IMPS on a variety of multi-programmed workloads and system configurations and compare them to four previously proposed state-of-the-art memory scheduling policies. Averaged over 240 workloads on a 24-core system with 4 memory channels, MCP improves system throughput by 7.1% over an application-unaware memory scheduler and 1% over the previous best scheduler, while avoiding modifications to existing memory schedulers. IMPS improves system throughput by 11.1% over an application-unaware scheduler and 5% over the previous best scheduler, while incurring much lower hardware complexity than the latter.	computer data storage;image scaling;interference (communication);locality of reference;memory controller;multi-core processor;scheduling (computing);social security;speedup;symmetric multiprocessing;synergy;throughput	Sai Prashanth Muralidhara;Lavanya Subramanian;Onur Mutlu;Mahmut T. Kandemir;Thomas Moscibroda	2011	2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1145/2155620.2155664	multi-core processor;uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;computer architecture;parallel computing;real-time computing;distributed memory;computer science;operating system;static memory allocation;interference;computer performance;extended memory;flat memory model;registered memory;computing with memory;cache-only memory architecture;memory management	Arch	-9.540703358755104	52.321834964205294	11775
a5dd7acf1aefcc9a4f7da1130c113fdf7ec692a8	access control in iot: from requirements to a candidate vision	access control ac;internet of things iot;architecture;smart homes	This paper investigates the main requirements for achieving Access Control (AC) in IoT as induced from literature. A novel AC architecture is then proposed as a candidate approach for AC in IoT taking into account the addressed requirements. That is, AC is proposed to be administered at the level of IoT communities sharing common attributes, i.e. mission, location, resource capability, or device owner, etc. Thus, AC is enforced via resource-capable devices, referred to as gatekeepers, on behalf of other resource-limited nodes in a given IoT community.	access control;requirement	Dina Hussein;Emmanuel Bertin;Vincent Frey	2017	2017 20th Conference on Innovations in Clouds, Internet and Networks (ICIN)	10.1109/ICIN.2017.7899435	engineering;internet privacy;world wide web;computer security	Robotics	-45.231258867746476	58.400376126484815	11777
fae7cb1ead18b7865ba21a9ecdfec748b00ff91d	architectural approaches to a science network software-defined exchange				Joaquin Chung Miranda	2018			orchestration (computing);systems engineering;software-defined networking;software;computer science	Logic	-47.08502593473747	46.59458202353026	11780
4e82e8648652e07c67612addff84d6f959739e1e	new curry-howard terms for full linear logic	first order;natural deduction;strong normalization;linear logic	In this paper we (1) provide a natural deduction system for full first-order linear logic, (2) introduce Curry-Howard-style terms for this version of linear logic, (3) prove strong normalization for the system, and (4). prove that given a proof of V~‘x3y a(.~, y) and any individual term t we can compute a term u such that a(t,u) is provable.	curry;curry–howard correspondence;first-order predicate;formal system;linear logic;natural deduction;normalization property (abstract rewriting);provable security	David W. Albrecht;John Newsome Crossley;John S. Jeavons	1997	Theor. Comput. Sci.	10.1016/S0304-3975(97)00044-3	linear logic;discrete mathematics;computer science;first-order logic;mathematics;programming language;natural deduction;algorithm	Logic	-12.189241458983904	15.852998557119237	11787
2826b5586b195527caa760824a3c06aab6956f17	energy efficient register file with reduced window partition	power saving;art;energy efficiency registers radiofrequency interference decoding energy consumption microelectronics power dissipation process design energy measurement power measurement;reduced window partition;energy efficient;sparc;resource management;energy efficient register file;energy consumption energy efficient register file reduced window partition power dissipation sparc partitioned register file inspector;processor power register file window;program processors power consumption;argon;partitioned register file inspector;radio frequency;registers;energy consumption;power dissipation;optimization;register file;power consumption;program processors;power;benchmark testing;processor;window	As power dissipation of the register file in modern processor designs tends to dominate, measures must be taken to keep it under control. This paper introduces an approach for reducing the SPARC windowed register file power based on the operation of the Partitioned Register File Inspector added in the decode stage of the pipeline. The power savings show that, when the size of the Register File Inspector is properly fixed, the average saving on the energy consumption of the windowed register file could be up to 77% compared with the traditional register file control scheme.	register file;sparc;window function	Ming Yang;Lixin Yu;Heping Peng	2008	2008 14th IEEE International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2008.22	benchmark;parallel computing;real-time computing;computer hardware;control register;computer science;memory buffer register;stub file;dissipation;resource management;operating system;journaling file system;stack register;power;efficient energy use;open;processor register;file system fragmentation;argon;radio frequency;register file;status register;memory data register;file control block	Arch	-5.751616674978585	54.47506827371066	11795
a3a3c6a5e8e2aa9f44a6a2a5dd91b5bd9d4be4c9	applying recommendation systems for composing dynamic services for mobile devices	runtime composition recommendation system dynamic services mobile device resource microphones cameras collaborative composite services video streaming intelligent information system user preferences under constrained problems knowledge based recommendation technique social device platform proximity based service composition knowledge based recommender system;mobile handsets;recommender systems mobile handsets;knowledge based systems mobile handsets recommender systems streaming media collaboration runtime context;recommender systems	In the context of mobile devices, services from several devices can be composed. Such compassable services may include data and device resources, such as microphones or cameras. The services can be composed to create collaborative composite services, such as video streaming. The mobility of the devices makes the service compositions inherently dynamic. Further, the set of possible service compositions may be large, and thus finding a suitable service composition at runtime becomes a challenge. Recommender systems are intelligent information systems that propose items to a user based on the user preferences, they are particularly useful in under-constrained problems, where the most suitable item is recommended from a large set of possible items that satisfy the user preferences. The main contribution of this paper is the application of knowledge-based recommendation techniques for composing dynamic services for mobile devices. The contribution is exemplified and validated with the Social Device Platform, which provides interactive, proximity-based service compositions for mobile devices. Knowledge-based recommender systems were found to be applicable technologically and feasible in terms of performance for composing dynamic services in environments where runtime composition is necessary.	inference engine;information system;knowledge-based recommender system;microphone;mobile device;run time (program lifecycle phase);service composability principle;streaming media;user (computing)	Jari Pääkkö;Mikko Raatikainen;Varvana Myllärniemi;Tomi Männistö	2012	2012 19th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2012.145	computer science;multimedia;internet privacy;world wide web;recommender system	HCI	-46.49998296651295	14.271849165804792	11799
11252aa483a19220a87c6c38578cf49a07126b41	automated modular specification and verification of real-time reactive systems	verification;finite state real time safety critical systems;programming environments;real time temporal logic;formal specification;concurrent computing;top down;resource allocation;temporal logic;prototypes;real time;program derivation;data variables;resource management;model checking method;automated modular specification;loosely coupled;real time reactive systems;real time systems prototypes power system modeling explosions logic testing software safety concurrent computing computer science resource management software tools;formal verification;model checking;specification and verification;real time resource allocation problem;software safety;statetime toolset automated modular specification verification real time reactive systems model checking finite state real time safety critical systems combinatorial explosion system complexity compositional reasoning real time temporal logic data variables top down hierarchical program derivation reachability graphs loosely coupled model checking method real time resource allocation problem;safety critical software;logic testing;combinatorial explosion;safety critical system;transition systems;reactive system;explosions;software tools;top down hierarchical program derivation;statetime toolset;software tools formal specification formal verification real time systems safety critical software temporal logic reachability analysis resource allocation programming environments;computer science;power system modeling;reachability graphs;reachability analysis;system complexity;compositional reasoning;real time systems	Model-checking is a powerful automated technique for verifying finite state real-time safety critical systems, but suffers from a combinatorial explosion of states as system complexity increases. In this paper, we introduce a method for compositional reasoning in real-time temporal logic that is suitable for model-checking finite state real-time reactive modules with data variables. This allows for the formal development of systems by top-down hierarchical program derivation. A system can be decomposed into modules, and the modules checked separately instead of checking the complete system all at once. This procedure often results in a significant decrease in the size of the reachability graphs that must be checked, particularly if the modules are loosely coupled. The modular model-checking method developed in this paper is illustrated using a real time resource allocation problem and the StateTime toolset. StateTime is a prototype toolset that uses visual specifications and temporal logic for the design and verification of real-time systems. The StateTime toolset has been used on small but non-trivial industrial examples. The incorporation of the modular methods discussed in this paper will allow StateTime to evolve from a prototype into an industrial strength tool.	formal methods;loose coupling;model checking;program derivation;prototype;reachability;real-time clock;real-time computing;real-time locating system;real-time transcription;temporal logic;top-down and bottom-up design;verification and validation	Jonathan S. Ostroff	1995		10.1109/WIFT.1995.515483	real-time computing;computer science;theoretical computer science;programming language	Embedded	-42.7692742635091	31.475775938186725	11801
a431ccf1e948c950e7c55820aac0705b631f6335	robolift: simple gui-based unit testing of student-written android applications (abstract only)	incremental development;test first coding;test driven development;unit testing;android;computer science education;graphical user interfaces;operating system;gui testing;introductory computer science;apps;mobile development;gui;graphic user interface;smartphone;junit;tablet;tdd	Many computer science educators have adopted test-driven development practices in their introductory computer science courses, as a way of encouraging incremental development and decreasing defects in student code. This practice is straightforward for basic data-driven objects, but making unit testing of GUI applications approachable for students poses a larger challenge. We have previously addressed this problem for Swing applications by developing LIFT, a library that allows students to easily write JUnit tests for Swing interfaces. Since then, we have transitioned away from Swing to Android as the development platform in CS2 to better motivate and excite our students about their assignments. To fully support this change, we had to ensure that our students could fully test the GUI portions of their solutions on that platform as well. The Android operating system has significant built-in support for GUI testing, but the standard API is too complex for students to use. In order to address this, we developed RoboLIFT, a framework that eases the task of writing concise and complete unit tests for Android applications. Furthermore, RoboLIFT also has support for automated grading on the Web-CAT automated assessment system, so even if instructors do not require their students to follow test-driven development practices, they can still enjoy the benefits of automated grading by writing correctness tests that use RoboLIFT to exercise the students' graphical user interfaces.	android;application programming interface;computer science;correctness (computer science);excite;graphical user interface testing;iterative and incremental development;junit;operating system;swing (java);test-driven development;unit testing	Anthony Allevato;Stephen H. Edwards	2012		10.1145/2157136.2157389	simulation;human–computer interaction;computer science;operating system;software engineering;graphical user interface;programming language	SE	-53.99740434784736	37.439632966560744	11819
7157aeb7feed11756eeda9fd524abc78f4a75c0e	weighted grid authorization graph (wgag)		Grid is a collection of distributed computing resources. These resources are available over a local or wide area network that appears to an end user or application as one large virtual computing system. The vision is to create virtual dynamic organizations through secure and coordinated resource-sharing among individuals and institutions. Access control to these resources is a problem difficult to manage, how to store and manage the security policies of such a system plays a key role. If these policies are not properly stored, the response time to access control requests will be dramatically increased. Grid Authorization Graph (GAG) was proposed to improve the authorization efficiency by eliminating the redundancy in checking security rules. This article proposes the Weighted Grid Authorization Graph (WGAG) as an enhancement to GAG which further improves the authorization efficiency by avoiding a lots of security rule checking. Finally, as proof of concept, we implement the WGAG simulator where simulations were done. The obtained results show that the proposed model can effectively reduce the complexity of security rule checking and gave better results than GAG.	access control;authorization;distributed computing;grid computing;response time (technology);simulation	Sara Namane;Mustafa Kaiiali;Nacira Ghoualmi;Rajeev Wankar;C. R. Rao;Sakir Sezer	2017	2017 Sixth International Conference on Communications and Networking (ComNet)	10.1109/COMNET.2017.8285589	data mining;database	HPC	-47.50829447808599	53.194901104109555	11822
96833a2677da8e6e8db9065264f8930c9537218b	towards role based trust management without distributed searching of credentials	trust management;complexity analysis;distributed search	Trust management systems enable decentralized authorization by searching distributed credentials from network. We argue that such distributed searching processes may encounter many technical or non-technical problems, and can be avoided by storing delegation credentials redundantly with acceptable costs. We propose a scoped-role based trust management system ScoRT, using a novel credential affiliation model to compute the credentials necessary for role membership decisions, which can be used to guide the storage, retrieval and revocation of credentials. The algorithm for distributed credential storage and retrieval is designed based on the model and its sound and complete properties are formally analyzed with respect to ScoRT semantics. Complexity analysis and estimation show that, by redundantly storing acceptable amount of delegation credentials, ScoRT enables more practical and automatic authorization without searching credentials from remote entities, and thus helps to overcome the deficiencies of existing approaches.	credential;trust management (managerial science)	Gang Yin;Huaimin Wang;Jian-quan Ouyang;Ning Zhou;Dian-xi Shi	2008		10.1007/978-3-540-88625-9_15	computer science;internet privacy;world wide web;computer security;computer network	Security	-47.67917011640176	53.690520396394625	11828
1e4338fa539c571b411e8c6e5d76f022cc9faf68	representing the zoo world and the traffic world in the language of the causal calculator	action language;commonsense reasoning;action languages;reasoning about action;reasoning about actions;knowledge representation	The work described in this report is motivated by the desire to test the expressive possibilities of action language C+. The Causal Calculator (CCALC) is a system that answers queries about action domains described in a fragment of that language. The Zoo World and the Traffic World have been proposed by Erik Sandewall in his Logic Modelling Workshop—an environment for communicating axiomatizations of action domains of nontrivial size. The Zoo World consists of several cages and the exterior, gates between them, and animals of several species, including humans. Actions in this domain include moving within and between cages, opening and closing gates, and mounting and riding animals. The Traffic World includes vehicles moving continuously between road crossings subject to a number of restrictions, such as speed limits and keeping a fixed safety distance away from other vehicles on the road. We show how to represent the two domains in the input language of CCALC, and how to use CCALC to test these representations.  2003 Elsevier B.V. All rights reserved.	action algebra;action language;action potential;actor-based concurrent language;approximation;causal filter;causality;closing (morphology);defeasible reasoning;fluent (artificial intelligence);formal language;hudson;ibm notes;interaction;logic programming;michael gelfond;nondeterministic algorithm;vladimir lifschitz	Varol Akman;Selim T. Erdogan;Joohyung Lee;Vladimir Lifschitz;Hudson Turner	2004	Artif. Intell.	10.1016/j.artint.2003.08.002	knowledge representation and reasoning;commonsense reasoning;action language;computer science;artificial intelligence;mathematics;algorithm	AI	-14.401008235309108	11.29493269769978	11832
551e4d16d71b28e343687afe28ffd25e241507f9	crowdits: crowdsourcing in intelligent transportation systems	information sources;intelligent transport system;data collection;automated highways;global position system;vehicles servers mobile handsets google sensors real time systems global positioning system;mobile phone;mobile radio automated highways;mobile radio;communication protocol;communication technology;iphone crowdits crowdsourcing intelligent transportation system data collection data dispersion smart mobile phone driver geo location android;real time systems	Intelligent Transportation Systems (ITS) and their applications are attracting significant attention in research and industry. ITS makes use of various sensing and communication technologies to assist transportation authorities and vehicles drivers in making informative decisions and provide leisure and safe driving experience. Data collection and dispersion are of utmost importance for the proper operation of ITS applications. Numerous standards, architectures and communication protocols have been anticipated for ITS applications. However, existing schemes are based on assumption that vehicles and roadside devices are equipped with sensing and communication capabilities. One of the major gaps of these approaches is their inability to capture events that can easily be logged by drivers using their mobile phones. In this paper, we propose to fill the gap by the use of Crowdsourcing in ITS namely, CrowdITS. In CrowdITS human inputs, along with available sensory data, are collected and communicated to a processing server using mobile phones. The basic idea is to use the Crowd with smart mobile phones to enable certain ITS applications without the need of any special sensors or communication devices, both in-vehicle and on-road. Alternatively, we integrate and aggregate human inputs with multiple information sources, and then selectively disseminate the aggregated information based on the driver's geo-location. Conceptually, the major change is to integrate human inputs, with multiple information sources, aggregate and finally it is localized according to the driver's geo-location. We describe the design of CrowdITS, report on the development of key ITS applications using Android and iPhone mobile phones, and outline the future work in the development of crowdsourced-based applications for intelligent transportation systems.	aggregate data;algorithm;android;crowdsourcing;data aggregation;device driver;geolocation;information;mobile phone;network congestion;real-time web;routing;sensor;server (computing);software deployment	Kashif Ali;Dina Al-Yaseen;Ali Ejaz;Tayyab Javed;Hossam S. Hassanein	2012	2012 IEEE Wireless Communications and Networking Conference (WCNC)	10.1109/WCNC.2012.6214379	embedded system;communications protocol;information and communications technology;mobile search;simulation;telecommunications;computer science;operating system;mobile station;mobile computing;computer security;statistics;computer network;data collection	Mobile	-41.864585225528415	51.24332424306103	11843
9f778f09c6ee8875ed1ab3c9fd2e5ee08c67f415	combinators for bidirectional tree transformations: a linguistic approach to the view-update problem	traitement liste;lenguaje programacion;linguistique;mise a jour;navegacion informacion;computacion informatica;bookmark;programming language;view update problem;navigation information;xml language;metodo arborescente;signet;information browsing;semantics;tratamiento lista;semantica;semantique;functional programming;ease of use;langage dedie;actualizacion;linguistica;marcador;bidirectional programming;ciencias basicas y experimentales;robustesse;tree structure;estructura datos;lenses;xml;domain specific language;langage programmation;harmony;tree structured method;robustness;structure donnee;programmation fonctionnelle;methode arborescente;grupo a;programacion funcional;data structure;languages;langage html;langage xml;lenguaje xml;domain specificity;updating;html language;lenguaje html;list processing;lenguaje dedicado;robustez;linguistics	We propose a novel approach to the  view-update problem  for tree-structured data: a domain-specific programming language in which all expressions denote bidirectional transformations on trees. In one direction, these transformations---dubbed  lenses ---map a concrete tree into a simplified abstract view; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong well-behavedness and totality properties for well-typed lenses.   We begin by identifying a natural space of well-behaved bidirectional transformations over arbitrary structures, studying definedness and continuity in this setting. We then instantiate this semantic framework in the form of a collection of  lens combinators  that can be assembled to describe bidirectional transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bidirectional list-processing transformations as derived forms. An extended example shows how our combinators can be used to define a lens that translates between a native HTML representation of browser bookmarks and a generic abstract bookmark format.	combinatory logic	Nate Foster;Michael B. Greenwald;Jonathan T. Moore;Benjamin C. Pierce;Alan Schmitt	2007	ACM Trans. Program. Lang. Syst.	10.1145/1232420.1232424	xml;data structure;computer science;theoretical computer science;database;semantics;programming language;functional programming;algorithm	PL	-27.011197681450156	16.728908969755505	11845
802c7af010f62cdd7039f5a4aef06d7571db9c9c	efficient unificatioin with infinite terms in logic programming			logic programming	Alberto Martelli;Gianfranco Rossi	1984			bunched logic;dynamic logic (modal logic);computational logic;discrete mathematics;horn clause;functional logic programming;distributed computing;logic programming;predicate functor logic;computer science;algorithm;prolog	Logic	-14.710924791534724	13.80855484577426	11856
1e220714cf56842d22790d5a10f78a9f4af9194a	an objective comparison of the cost effectiveness of three testing methods	computacion informatica;collateral coverage;grupo de excelencia;structural testing;jj path testing;ciencias basicas y experimentales;test methods;jj pair testing;cost effectiveness;white box testing;branch testing	Branch testing is a well established method for exercising software. JJ-path testing, whilst employed by some practitioners, is less popular, and the testing of JJ-pairs finds few adherents. In this paper an objective, practical study of the cost-effectiveness of these three testing methods is reported. The effectiveness of each method is assessed, in the presence of infeasible paths, not only on its ability to cover the specific structural element of code that it targets, but also on its ability to cover the structural elements targeted by the other two methods - the collateral coverage it achieves. The assessment is based on the results derived from experiments in which each of the three methods is applied to 35 units of program code.		Derek F. Yates;Nicos Malevris	2007	Information & Software Technology	10.1016/j.infsof.2006.10.009	basis path testing;simulation;orthogonal array testing;white-box testing;computer science;engineering;software engineering;engineering drawing	SE	-60.44775713916105	34.451573034538185	11864
c1bd1ab9ff9b9675b89c7169a39ea54bacbb9777	a workflow system for supporting group activities of an enterprise	computers;external groupware tools integration;groupware;design automation;collaborative work;prototypes;visual description;cooperative workflow;integrated software workflow management software groupware;collaborative tools;workflow system;responsibility flow;synchronous execution;enterprise group activities support;support group;constraint based method workflow system enterprise group activities support responsibility flow business processes synchronous execution external groupware tools integration human human synchronous interactions cflow cooperative workflow implementation techniques process model visual description;integrated software;collaborative work collaborative software software tools product design design automation laboratories computers prototypes workflow management software collaborative tools;workflow management software;software tools;process model;product design;cflow;human human synchronous interactions;constraint based method;business process;collaborative software;business processes;implementation techniques	Workjlow technology provides a good way to mediate the responsibility flow in a business process from person to person and from task to task. However, most current workjlow systems are not able to do this very well. The main reasons are: most activities of a workjlow are executed asynchronously and respectively according to their workjlow model, and cannot be executed synchronously; Extemal Group Ware tools cannot be integrated very well, which makes the system not be best of supporting human-human synchronous interactions. CFlow (Cooperative workFlow) has been developed by our group recently to overcome the above shortcomings. In this paper, the main implementation techniques of CFlow are described; a process model visually describing method based on constraints is introduced; and an approach to integrate Group Ware tools is presented.	business process;collaborative software;gnu cflow;interaction;process modeling	Feng Li;Zongkai Lin;Yuchai Guo;Jintao Li	2001		10.1109/CSCWD.2001.942300	workflow;computer science;knowledge management;software engineering;business process;product design;management;workflow management system;collaborative software;workflow engine;workflow technology	SE	-52.11506352368151	15.468252090258778	11869
940fbfb44470756d44f34b5ef7095d1aee4b74f6	procov: probabilistic output coverage model	probability;procov tool probabilistic output coverage model reliability complex digital systems functional verification process random constrained functional verification sound coverage models random testing item coverage quantitative metrics system parameters verification engineers expertise output coverage modeling actual output value distribution coverage model profile testbench execution time overhead probabilistic output coverage tool;mathematical model educational institutions finite impulse response filters equations;formal verification;random processes;software reliability;software reliability formal verification probability random processes	In order to guarantee high level of reliability of current complex digital systems, a robust functional verification process is mandatory. Random constrained functional verification has been a common technique used in the industry, but sound coverage models are needed in order to monitor and limit the amount of random testing. Item coverage refers to quantitative metrics based on occurrences of system parameters or variables, in general, specified under verification engineers expertise, particularly the output coverage modeling. In most cases, the actual output value distribution does not conform the established coverage model profile, leading to testbench execution time overhead. This work presents a methodology for a fast computation of profile similar to the real output value distribution, to assist the engineer in the selection of the proper check points or output ranges of interest. At the core of this methodology is the Probabilistic Output Coverage (PrOCov) tool, which was developed with the above goals.	computation;coverage data;digital electronics;high-level programming language;overhead (computing);parameter (computer programming);random testing;run time (program lifecycle phase);test bench	Joel Ivan Munoz Quispe;Marius Strum;Wang Jiang Chau	2013	2013 14th Latin American Test Workshop - LATW	10.1109/LATW.2013.6562664	modified condition/decision coverage;reliability engineering;real-time computing;simulation;computer science;functional verification	EDA	-50.953930435999254	38.49592510138918	11882
129b2b55c6a32c5bee85ef32f472a87562813429	strong scaling for numerical weather prediction at petascale with the atmospheric model numa	article	Numerical weather prediction (NWP) has proven to be computationally challenging due to its inherent multiscale nature. Currently, the highest resolution NWP models use a horizontal resolution of about 10 km. At this resolution many important processes in the atmosphere are not resolved. Needless to say this introduces errors. In order to increase the resolution of NWP models highly scalable atmospheric models are needed. The Non-hydrostatic Unified Model of the Atmosphere (NUMA), developed by the authors at the Naval Postgraduate School, was designed to achieve this purpose. NUMA is used by the Naval Research Laboratory, Monterey as the engine inside its next generation weather prediction system NEPTUNE. NUMA solves the fully compressible Navier-Stokes equations by means of high-order Galerkin methods (both spectral element as well as discontinuous Galerkin methods can be used). Mesh generation is done using the p4est library. NUMA is capable of running middle and upper atmosphere simulations since it does not make use of the shallowatmosphere approximation. This paper presents the performance analysis and optimization of the spectral element version of NUMA. The performance at different optimization stages is analyzed using a theoretical performance model as well as measurements via hardware counters. Machine independent optimization is compared to machine specific optimization using BG/Q vector intrinsics. By using vector intrinsics the main computations reach 1.2 PFlops on the entire machine Mira (12% of the theoretical peak performance). The paper also presents scalability studies for two idealized test cases that are relevant for NWP applications. The atmospheric model NUMA delivers an excellent strong scaling efficiency of 99% on the entire supercomputer Mira using a mesh with 1.8 billion grid points. This allows to run a global forecast of a baroclinic wave test case at 3 km uniform horizontal resolution and double precision within the time frame required for operational weather prediction.	adaptive multi-rate audio codec;adaptive mesh refinement;algorithm;approximation;archive;atmospheric model;aurora;automated mathematician;blue gene;bode plot;cpu cache;category utility;computation;computational mathematics;computational science;declaration (computer programming);double-precision floating-point format;expect;flops;galerkin method;heterojunction;ibm websphere extreme scale;image resolution;image scaling;intrinsic function;job control (unix);lc circuit;list of numerical analysis software;mathematical optimization;memory bandwidth;mesh generation;navier–stokes equations;neptune;next-generation network;numerical methods for ordinary differential equations;numerical weather prediction;octree;parallel computing;petascale computing;refinement (computing);requirement;scalability;simulation;supercomputer;test case;unified model	Andreas Müller;Michal A. Kopera;Simone Marras;Lucas C. Wilcox;Tobin Isaac;Francis X. Giraldo	2015	CoRR		parallel computing;simulation;computer science	HPC	-6.628318874364352	38.558746825785	11883
60f250e7997866e01cbe35e75e0b114ff2ecd254	interpretability degrees of finitely axiomatized sequential theories	logic;degrees;interpretability;03f30;03f25;sequential theories;wijsbegeerte;philosophy	In this paper we show that the degrees of interpretability of finitely axiomatized extensions-in-the-same-language of a finitely axiomatized sequential theory— like Elementary Arithmetic EA, I 1, or the Gödel–Bernays theory of sets and classes GB—have suprema. This partially answers a question posed by Švejdar in his paper (Commentationes Mathematicae Universitatis Carolinae 19:789–813, 1978). The partial solution of Švejdar’s problem follows from a stronger fact: the convexity of the degree structure of finitely axiomatized extensions-in-the-same-language of a finitely axiomatized sequential theory in the degree structure of the degrees of all finitely axiomatized sequential theories. In the paper we also study a related question: the comparison of structures for interpretability and derivability. In how far can derivability mimic interpretability? We provide two positive results and one negative result.	gödel;set theory	Albert Visser	2014	Arch. Math. Log.	10.1007/s00153-013-0353-8	mathematical analysis;discrete mathematics;mathematics;logic;algorithm;algebra	AI	-8.152722396400316	11.495240285640788	11891
3e3fa559be250deb8a6675f6939273e6ab0de6b9	optimizing communication scheduling using dataflow semantics	application development;spmd algorithm;communication scheduling;coarse grain dataflow;scheduling data flow analysis message passing;distributed computing message passing processor scheduling concurrent computing aggregates parallel processing scheduling algorithm application software space exploration computer architecture;coordination language;data mining;data distribution;single program multiple data algorithm;npb parallel programming models coarse grain dataflow message passing pgas;computational modeling;parallel programming models;slabs;coodination language;scheduling;replicated data;message passing;coarse grain dataflow semantics;data flow analysis;pgas;optimization;design space exploration;parallel programming model;npb;coarse grained;programming;parallel processing;coodination language communication scheduling coarse grain dataflow semantics spmd algorithm single program multiple data algorithm message passing	We show how coarse grain dataflow semantics (CGD) applied to SPMD algorithms makes application development and design space exploration simpler compared to message passing, at the same time providing on par performance. CGD applications are specified as dependencies between computation modules and data distributions. Communication and synchronization are added automatically and optimized for specific architectures, relieving programmers of this task. Many high level algorithm changes are easy to implement in CGD by redefining data distributions. These include exposing communication overlap by decreasing task grain, and aggregating communication by replicating data and computation. We briefly present a coordination language with dataflow semantics that implements the CGD model. Our implementation currently supports MPI, SHMEM, and pthreads. Results on Altix 4700 show our optimized CGD FT is 27% faster than original NPB 2.3 MPI implementation, and optimized CGD stencil has a 41% advantage over handwritten MPI.	algorithm;central processing unit;computation;dataflow;definition;design space exploration;fortran;high-level programming language;message passing interface;optimizing compiler;posix threads;programmer;programming model;shmem;spmd	Adrian Soviani;Jaswinder Pal Singh	2009	2009 International Conference on Parallel Processing	10.1109/ICPP.2009.66	parallel processing;programming;parallel computing;message passing;computer science;partitioned global address space;operating system;data-flow analysis;distributed computing;programming language;rapid application development;computational model;scheduling;parallel programming model	HPC	-12.234711494132927	39.778923468415655	11892
05e9d4e160caf484f314ce7e07723665e5a54d3e	generic and composable latecomer accomodation service for centralized shared systems	user interface;collaborative system;levels of abstraction;multi user interfaces	It is important that a shared application allow a latecomer to join other users who are already working together with the application. We have developed a latecomer accommodation service framework for centralized shared systems (applications and infrastructures). It employs an independent latecomer accommodation server that is dynamically composable with its clients. The server, also called the logger, logs a shared application’s user interface (UI) changes in response to calls made by the client, also called the loggable. Later, when the time comes to accommodate a latecomer, the logger replays the logged changes to the loggable, which, in turn, creates the latecomer’s user interface. To deal with UI protocols at different levels of abstraction, we have defined the API in terms of a generic UI model. This reduces the burden on a loggable from a complete service implementation to a translation between its specific UI protocol and our generic UI model. To reduce the space and time overhead, the logger performs complex log compression. The extent of compression depends on the amount of semantic knowledge that the loggable provides to the logger. In this paper, we motivate, describe and illustrate the approach, and outline how it is implemented.	application programming interface;centralized computing;data logger;overhead (computing);principle of abstraction;server (computing);user interface	Goopeel Chung;Prasun Dewan;Sadagopan Rajaram	1998		10.1007/978-0-387-35349-4_8	real-time computing;computer science;operating system;world wide web	Mobile	-33.202505779387906	41.17618522232609	11896
594333ad3303b8a55e4e9b5b262edf4a27478cd6	an autonomic approach to manage elasticity of business processes in the cloud	elasticity;service business process;qos;occi;autonomic computing;cloud computing	Cloud Computing is gaining more and more importance in the Information Technologies (IT) scope. One of the major assets of this paradigm is its economic model based on pay-as-you-go model. Cloud Computing gets more attention from IT users when it fits their required QoS and reduces their expenses. This task cannot be done without increasing the autonomy of the provisioned Cloud resources. In this paper, we propose a holistic approach that allows to dynamically adding autonomic management facilities to Cloud resources even if they were designed without these facilities. Based on the Open Cloud Computing Interface (OCCI) standard, we propose a generic model that allows describing the needed resources to render autonomic a given Cloud resource independently of the service level (Infrastructure, Platform or Software). Herein, we define new OCCI Resources, Links and Mixins that allow provisioning autonomic Cloud Resources. In order to illustrate our approach, we propose a use case that specializes our autonomic infrastructure to ensure the elasticity of Service-based Business Processes (SBPs). The elasticity approach that we are using is based on a formal model that features duplication/consolidation mechanisms and a generic Controller that defines and evaluates elasticity strategies. To validate our proposal, we present an end to end scenario of provisioning an elastic SBP on a public PaaS. Evaluation of our approach on a realistic situation shows its efficiency. We propose an autonomic management model for Cloud resources.We extend Open Cloud Computing Interface to describe our autonomic model.We illustrate our autonomic model with an approach for Business Processes elasticity.We propose a formal approach for Business Processes elasticity.We show the efficiency of our approach on a realistic Cloud environment.	autonomic computing;business process;cloud computing;elasticity (cloud computing)	Mohamed Mohamed;Mourad Amziani;Djamel Belaïd;Samir Tata;Tarek Melliti	2015	Future Generation Comp. Syst.	10.1016/j.future.2014.10.017	real-time computing;simulation;quality of service;cloud computing;computer science;operating system;database;elasticity;world wide web;autonomic computing	Arch	-58.41003845031478	18.730124319227595	11903
7b452a465e67ec1805221069dd5a05a482199667	a flexible querying framework (fqf): some implementation issues	fuzzy relational database;base relacional dato;reponse temporelle;marco;query language;database system;interfase usuario;base donnee;standards;fuzzy data;user interface;implementation;logique floue;interrogation base donnee;database;standard;interrogacion base datos;base dato;logica difusa;langage java;relational database;lenguaje interrogacion;fuzzy logic;societe information;systeme incertain;technology and engineering;extremite;numerical model;end;time response;relational model;norma;extremidad;base donnee orientee objet;base donnee relationnelle;sociedad informacion;information society;lenguaje java;interface utilisateur;langage interrogation;object oriented databases;object oriented database;implementacion;sistema incierto;respuesta temporal;etalon;fuzzy database;uncertain system;norme;database query;java language	Fuzzy data are a common concept in today's information society. Some data can be unknown, other data may be inaccurate or uncertain. Still, this fuzzy data must be accounted for in modern businesses and therefore must be stored. Fuzzy relational databases have been studied extensively over time, which resulted in numerous models and representation techniques, some of which have been implemented as software layers on top of database systems. Different query languages and end-user interfaces have been extended to perform flexible queries on both regular and fuzzy databases. In this paper, a framework is presented that not only enables flexible querying on the relational model, but on other database models as well, of which the most important are object-oriented database models. This framework, called FQF or Flexible Querying Framework, is built on the recently developed Java Data Objects (JDO) standard.		Bert Callens;Guy De Tré;Jörg Verstraete;Axel Hallez	2003		10.1007/978-3-540-39737-3_33	fuzzy logic;fabry–pérot interferometer;end;relational model;relational database;computer science;artificial intelligence;operating system;data mining;database;programming language;user interface;implementation;world wide web;algorithm;query language	DB	-33.4856551077582	10.156699244639638	11920
28ed7bfd8a3f2a1934d51ee0e461cef1decfef0b	memory organizations for 3d-drams and pcms in processor memory hierarchy	set associate addressing;energy modeling;memory latency modeling;memory hierarchy;3d drams;pcm	In this paper, we describe and evaluate three possible architectures for using 3D-DRAMs and PCMs in the processor memory hierarchy. We explore: (i) using 3D-DRAM as main memory with PCM as backing store; (ii) using 3D-DRAM as the Last Level Cache and PCM as the main memory; and (iii) using both 3D-DRAM and PCM as main memory. In each of these configurations, since the proposed memories are significantly faster than today's off-chip 2D DRAMs for main memories and magnetic hard drives for secondary storage, we introduce hardware assistance to speedup virtual to physical address translation.We use Simics, a full system simulator, and benchmarks from both SPEC and OLTP suites to evaluate our designs. We use CACTI for obtaining energy and latency values for our configurations. We measure energy consumed and execution performance for the selected benchmarks.Our studies lead to the following conclusions. The best performance is obtained when 3D-DRAMs are used as last level caches (LLC) and PCM as the main memory. However, this organization performs poorly in terms of energy consumed. Our 3D-DRAM together with PCM as main memory is the best choice in terms of energy consumed. In terms of write-backs, 3D-DRAM as LLC causes fewer writes to PCM than the other organization.These experiments can be extended to explore specific memory organizations, capacities of 3D-DRAM needed as LLC or main memory and how the hybrid PCM/DRAM memory should be used for specific application contexts.	memory hierarchy	Krishna M. Kavi;Stefano Pianelli;Giandomenico Pisano;Giuseppe Regina;Mike Ignatowski	2015	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2015.07.009	pulse-code modulation;uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;memory refresh;computer hardware;telecommunications;computer science;physical address;operating system;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;computing with memory;cache-only memory architecture;memory map;memory management	EDA	-9.1574005307144	53.265290335405794	11926
0cd1dfa8261d5c26be6c0f1b90261a1474e4f52b	basic protocols, message sequence charts, and the verification of requirements specifications	langage oriente objet;multiagent system;interaction;specification;metodo formal;methode formelle;formal methods;systeme deterministe;methode algebrique;formal method;agent;formal verification;basic protocols;sistema determinista;especificacion;telecommunication system;environment;algebraic method;verification formelle;systeme telecommunication;sistema telecomunicacion;metodo algebraico;process;analisis semantico;analyse semantique;sistema multiagente;requirement capturing;object oriented languages;insertion function;semantic analysis;systeme multiagent;deterministic system	Message sequence charts are a widely used notation to express requirements specifications of multi-agent systems. The semantics of message sequence charts can be defined algebraically in the theory of agents and insertion functions. Using this algebra, one can split message sequence chart scenarios into sets of Hoare triples consisting of precondition, the specification of a finite process, and a postcondition. We refer to such triples as “basic protocols.” In this paper, we discuss tools to prove properties of systems described as basic protocols, such as the completeness (at each of its stages the system behavior has a possible continuation) and consistency (at each stage the system behavior is deterministic) of the specification, or the correspondence of the specified behavior to given scenarios. Together, these tools constitute a powerful environment for the formal verification of requirements specifications expressed through message sequence charts.	continuation;formal verification;hoare logic;message sequence chart;multi-agent system;postcondition;precondition;requirement	Alexander A. Letichevsky;Julia V. Kapitonova;A. A. Letichevsky;Vladislav A. Volkov;Sergey Baranov;Thomas Weigert	2005	Computer Networks	10.1016/j.comnet.2005.05.005	interaction;formal methods;formal verification;computer science;theoretical computer science;deterministic system;natural environment;object-oriented programming;specification;algorithm;process	SE	-39.70660826074247	25.89293911167603	11938
1f861c8d4f4a23d2c3c85319750a43263091be13	a mathematical analysis of theories of parthood	part whole relation;set theory;mathematical analysis;boolean algebra;duality theory;mereology;ontologies;reasoning	This paper presents a mathematical analysis of different formal ontological theories of parthood (mereologies). We summarize variants of the theory of General Extensional Mereology (GEM) and compare them with their abstract mathematical counterpart, set theory. In particular, we prove by set theoretical means that there exists a model of GEM where arbitrary summation of entities is not possible. Further, we use Stone's duality theory for Boolean algebras to classify models of the different mereologies.	mereology	Carsten Pontow;Rainer Schubert	2006	Data Knowl. Eng.	10.1016/j.datak.2005.07.010	boolean algebra;duality;ontology;mereology;reason;set theory	ML	-9.72537506026842	10.03053022412585	11939
9acc5a7821484dd2677e74ce78195169909750c7	conflicting imperatives and dyadic deontic logic	logica deontica;conflict;axiomatic;semantics;complete axiomatization;logique deontique;semantica;semantique;calculo diadico;deontic logic;axiomatico;calcul dyadique;informatique theorique;conflicto;dyadic calculus;conflit;axiomatique;computer theory;informatica teorica	Often a set of imperatives or norms seems satisfiable from the outset, but conflicts arise when ways to fulfill all are ruled out by unfortunate circumstances. Semantic methods to handle normative con- flicts were devised by B. van Fraassen and J. F. Horty, but these are not sensitive to circumstances. The present paper extends these resolution mechanisms to circumstantial inputs, defines according dyadic deontic operators, and provides a sound and (weakly) complete axiomatic sys- tem for such a deontic semantics.	deontic logic;dyadic transformation	Jörg Hansen	2004		10.1007/978-3-540-25927-5_10	philosophy;artificial intelligence;deontic logic;mathematics;semantics;linguistics;axiom;algorithm	AI	-13.63252275831325	9.735758499289675	11941
3a99bf0c433962512644ffa18ca6c9247a156dce	towards a service-oriented approach for managing context in mobile environment	context aware application;context aware;context information;service orientation;mobile environment;quality requirement;context management;quality of context;context aware systems	The current development of context-awareness has introduced various emerging research areas to reduce complexity in developing context aware applications by applying service-oriented approach in managing context and establish context service. The establishment of context service will enable context aware systems to access and utilize context from context providers without paying necessary attention on how context information are composed and managed. Frequent changes of available context providers with different context quality are common phenomena in mobile environment. Hence, dealing with quality of context is a very important issue to provide reliable services for context management in this environment. We have identified some key requirements to establish context service and propose a service-oriented framework to facilitate context management in mobile environment. Furthermore we show our approach to deal with problem in providing appropriate context based on its quality requirements and the preferences of the corresponding context request.	context awareness;context-aware pervasive systems;requirement;service-oriented architecture;service-oriented device architecture;trust (emotion)	Waskitho Wibisono;Arkady B. Zaslavsky;Sea Ling	2008		10.1007/978-3-540-89652-4_18	knowledge management;database;world wide web	HCI	-42.690837467864995	43.652584294583896	11944
c255526935964c0ab6466a2672deac5553b675d1	socialgq: towards semantically approximated and user-aware querying of social-graph data			approximation algorithm;social graph	Riccardo Martoglia	2018		10.18293/SEKE2018-052	data mining;information retrieval;computer science;software;social graph	DB	-36.04365037156467	4.697025097239242	11966
077f1b10186a2fe00367a99a5e5708e36b50f03a	bayesian graphical models for software testing	belief networks;software testing;decision support;bayesian graphical models;statistical methods;belief networks software reliability program testing;program testing;bayesian methods graphical models software testing;graphical model;knowledge capture;software reliability bayesian graphical models software testing formal mechanisms logical structuring test design;expert judgment;software reliability;test design	This paper describes a new approach to the problem of software testing. The approach is based on Bayesian graphical models and presents formal mechanisms for the logical structuring of the software testing problem, the probabilistic and statistical treatment of the uncertainties to be addressed, the test design and analysis process, and the incorporation and implication of test results. Once constructed, the models produced are dynamic representations of the software testing problem. They may be used to drive test design, answer what-if questions, and provide decision support to managers and testers. The models capture the knowledge of the software tester for further use. Experiences of the approach in case studies are briefly discussed.	graphical model;software testing	David A. Wooff;Michael Goldstein;Frank P. A. Cohen	2002	IEEE Trans. Software Eng.	10.1109/TSE.2002.1000453	non-regression testing;test strategy;reliability engineering;verification and validation;regression testing;model-based testing;decision support system;software performance testing;white-box testing;manual testing;system integration testing;computer science;systems engineering;acceptance testing;software reliability testing;software development;software engineering;software construction;data mining;risk-based testing;software testing;graphical model;test design;graphical user interface testing;test management approach;software quality	SE	-61.08332384230865	31.237248200934193	11974
fbea45878b6871b318db6bfbc398bc3e07da49b0	inference-security analysis using resolution theorem-proving	security analysis;military security true statement probabilities resolution theorem proving inference rules compromiser indirect logical inferences information systems;database management systems;rule based;inference mechanisms;statistical method;theorem proving;conference paper;inference rule;marine vehicles information analysis data security information security expert systems military computing navigation humans computer science computer security;information flow;theorem proving database management systems database theory inference mechanisms security of data;information system;database theory;security of data;expert system	Indirect logical inferences can provide a significant security threat to information processing systems, but they have not been much studied. Classification of data can reduce the threat, but classification decisions are typically left to the intuitive judgment of experts. Progress has been made on analyzing indirect statistical inferences that may compromise security of a database system ([3], chapter 6). We describe and implement a nonnumeric analog of these methods for proving security. Our approach involves analyzing facts and inference rules assumed to be known to a compromiser, deriving all their possible consequences using resolution theorem-proving, a technique which we argue is far more appropriate to this problem than rulebased expert systems or information flow analysis. An important contribution of our work is augmentation of resolution to handle associated time intervals and probabilities of statements being true. Our augmentation is simple to use by domain experts untrained in computers, and we believe it will provide the first truly practical tool for analysis of indirect logical inferences in information systems. We demonstrate capabilities with an example from military security. This paper appeared in the Fifth International Conference on Data Engineering, Los Angeles, CA, February 1989, 410-416.	computer;data-flow analysis;database;expert system;information flow (information theory);information processing;information system;resolution (logic);threat (computer)	Neil C. Rowe	1989		10.1109/ICDE.1989.47242	database theory;information flow;computer science;theoretical computer science;data mining;database;automated theorem proving;security analysis;expert system;information system;statistics;rule of inference	DB	-26.83933570975157	15.359345155936037	11983
1bef5b4472c355ba766e85a47fa72f8a326a38b5	aspect refinement ð unifying aop and stepwise refinement.		Stepwise refinement (SWR) is fundamental to software engineering. As aspectoriented programming (AOP) is gaining momentum in software development, aspects should be considered in the light of SWR. In this paper, we elaborate the notion of aspect refinement that unifies AOP and SWR at the architectural level. To reflect this unification to the programming language level, we present an implementation technique for refining aspects based on mixin composition along with a set of language mechanisms for refining all kinds of structural elements of aspects in a uniform way (methods, pointcuts, advice). To underpin our proposal, we contribute a fully functional compiler on top of AspectJ, present a non-trivial, medium-sized case study, and derive a set of programming guidelines.	apel (emacs);ar (unix);aspect-oriented software development;aspectj;computer science;coupling (computer programming);don woods (programmer);embedded system;functional compiler;günter böckle;heterogeneous database system;informatics;information systems;information system;interaction;long line (telecommunications);magdeburg;mixin;pointcut;program synthesis;programming language;programming paradigm;refinement (computing);software construction;software development;software engineering;software product line;software project management;stepwise regression;sven jaschan;the journal of object technology;top-down and bottom-up design;unification (computer science)	Sven Apel;Christian Kästner;Thomas Leich;Gunter Saake	2007	Journal of Object Technology	10.5381/jot.2007.6.9.a1	systems engineering;computational science;top-down and bottom-up design;computer science	PL	-33.11977947415053	28.68071125839611	11992
f52125d81652f77a5d8c2d0f7eada2abbd67edfa	on the uniqueness of fixed points of endofunctors in a category of complete metric spaces	lenguaje programacion;complete metric space;programming language;language theory;004 informatik;concurrent program;teoria lenguaje;fixed point;synchronisation;synchronization;informatique theorique;programa competidor;langage programmation;sincronizacion;theorie langage;programme concurrent;computer theory;informatica teorica	In 1982, De Bakker and Zucker proposed to use complete metric spaces for the semantic definition of programming languages that allow for concurrency and synchronisation. The use of the tools of metric topology has been advocated by Nivat and his colleagues already in the seventies and metric topology was successfully applied to various problems (Nivat, 1979/1980). Recently, the question under which circumstances fixed point equations involving complete metric spaces can be (uniquely) solved has attracted attention (see, e.g., America and Rutten. 1988. and the present author, 1987). America and Rutten (1988) provide a criterion for the existence of a solution, namely the contractiveness of the respective functor. Contractiveness together with an additional criterion, the horn-contractiveness was shown by America and Rutten (1988) to guarantee uniqueness. The problem of uniqueness is the topic of our contribution.	concurrency (computer science);fixed point (mathematics);programming language;spaces	Mila E. Majster-Cederbaum	1988	Inf. Process. Lett.	10.1016/0020-0190(88)90224-4	convex metric space;synchronization;combinatorics;discrete mathematics;product metric;computer science;pure mathematics;mathematics;algorithm	Theory	-12.640814702859736	17.95492394733967	12002
37cf80be2fec347a459fcf43f468e30ba784a6ce	aligning textual and model-based process descriptions		Process model descriptions are an ubiquitous source of information that exists in any organization. To reach different types of stakeholders, distinct descriptions are often kept, so that process understandability is boosted with respect to individual capabilities. While the use of distinct representations allows more stakeholders to interpret process information, it also poses a considerable challenge: to keep different process descriptions aligned. In this paper, a novel technique to align process models and textual descriptions is proposed. The technique is grounded on projecting knowledge extracted from these two representations into a uniform representation that is amenable for comparison. It applies a tailored linguistic analysis of each description, so that the important information is considered when aligning description’ elements. Compared to existing approaches that address this use case, our technique provides more comprehensive alignments, which encompass process model activities, events and gateways. Furthermore, the technique, which has been implemented into the platform nlp4bpm.cs.upc.edu, shows promising results based on experiments with real-world data.		Josep Sànchez-Ferreres;Han van der Aa;Josep Carmona;Lluís Padró	2018	Data Knowl. Eng.	10.1016/j.datak.2018.09.001	information retrieval;process modeling;computer science	DB	-52.71045794787402	18.484837094818293	12016
bb10b26cfafea295253eda21ea71701c061a5194	performance modelling of magnetohydrodynamics codes	qa76 computer software	Performance modelling is an important tool utilised by the High Performance Computing industry to accurately predict the runtime of science applications on a variety of different architectures. Performance models aid in procurement decisions and help to highlight areas for possible code optimisations. This paper presents a performance model for a magnetohydrodynamics physics application, Lare. We demonstrate that this model is capable of accurately predicting the run-time of Lare across multiple platforms with an accuracy of 90% (for both strong and weak scaled problems). We then utilise this model to evaluate the performance of future optimisations. The model is generated using SST/macro, the machine level component of the Structural Simulation Toolkit (SST) from Sandia National Laboratories, and is validated on both a commodity cluster located at the University of Warwick and a large scale capability resource located at Lawrence Livermore National Laboratory.	beowulf cluster;code;expectation propagation;image scaling;procurement;scalability;simulation	Robert F. Bird;Steven A. Wright;D. A. Beckingsale;Stephen A. Jarvis	2012		10.1007/978-3-642-36781-6_14	simulation;engineering;operations research	HPC	-7.342869916225195	39.59789955724181	12022
507d41b56abec8da865918817ad58f17511dc14f	dynamic module replacement in distributed protocols	protocols;banking;mutual exclusion protocols;distributed protocol;information science;pattern based design strategy;resource allocation;resource management;protocols runtime programming java middleware testing resource management production facilities information science banking;language neutral technique;testing;object oriented programming;runtime;software engineering;mutual exclusion;distributed resource allocation;distributed protocols;distributed resource allocation dynamic module replacement distributed protocols middleware research languages software development language neutral technique service facilities pattern based design strategy serf strategy mutual exclusion protocols;software engineering protocols object oriented programming middleware resource allocation;software development;production facilities;middleware;serf strategy;research languages;programming;dynamic module replacement;service facilities;java	Dynamic module replacement — the ability to hot swap a component’s implementation at runtime — is fundamental to supporting evolutionary change in long-lived and highlyavailable systems. Most existing solutions require specialpurpose middleware or depend on research languages with limited support for mainstream software development. We present a language-neutral technique for dynamic module replacement using Service Facilities (Serfs) — a patternbased design strategy for decoupling runtime dependencies. We demonstrate the sufficiency of Serfs with respect to a litmus test of criteria for module replacement. Next, we extend the traditional scope of module replacement to encompass the domain of modules for distributed protocols. We conclude by applying the Serf strategy to illustrate dynamic replacement of mutual exclusion protocols in modules for distributed resource allocation.	coupling (computer programming);hot swapping;litmus;middleware;mutual exclusion;run time (program lifecycle phase);software development	Nigamanth Sridhar;Scott M. Pike;Bruce W. Weide	2003		10.1109/ICDCS.2003.1203513	communications protocol;programming;real-time computing;mutual exclusion;information science;resource allocation;computer science;resource management;software development;operating system;middleware;database;distributed computing;software testing;programming language;object-oriented programming;java	PL	-38.736212616872464	39.92456796125714	12023
73b149327fcef4df5e906233851143acf7a14cc6	observation-driven geo-ontology engineering		Big Data, Linked Data, Smart Dust, Digital Earth, and e-Science are just some of the names for research trends that surfaced over the last years. While all of them address different visions and needs, they share a common theme: How do we manage massive amounts of heterogeneous data, derive knowledge out of them instead of drowning in information, and how do we make our findings reproducible and reusable by others? In a network of knowledge, topics span across scientific disciplines and the idea of domain ontologies as common agreements seems like an illusion. In this work, we argue that these trends require a radical paradigm shift in ontology engineering away from a small number of authoritative, global ontologies developed top-down, to a high number of local ontologies that are driven by application needs and developed bottom-up out of observation data. Similarly as the early Web was replaced by a social Web in which volunteers produce data instead of purely consuming it, the next generation of knowledge infrastructures has to enable users to become knowledge engineers themselves. Surprisingly, existing ontology engineering frameworks are not well suited for this new perspective. Hence, we propose an observation-driven ontology engineering framework, show how its layers can be realized using specific methodologies, and relate the framework to existing work on geo-ontologies.	antivirus software;baseline (configuration management);big data;bottom-up proteomics;data mining;data-intensive computing;design pattern;digital signature;e-science;electronic signature;formal specification;geographic information science;geographic information system;interoperability;knowledge engineer;knowledge management;library (computing);linked data;local-density approximation;machine learning;microsoft outlook for mac;ontology (information science);ontology alignment;ontology engineering;point of interest;programming paradigm;sampling (signal processing);sensor;software engineering;software framework;theory;top-down and bottom-up design;type signature;type system;vagueness	Krzysztof Janowicz	2012	Trans. GIS	10.1111/j.1467-9671.2012.01342.x	geography;computer science;knowledge management;artificial intelligence;data science;data mining;database;cartography	Web+IR	-39.61506841275534	5.074977838984188	12027
78f53165e028f9e3956d13adc132f7e530366ba8	applications and challenges in satisfiability modulo theories		The area of software analysis, testing and verification is no w undergoing a revolution thanks to the use of automated and scalable support for logical meth ods. A well-recognized premise is that at the core of software analysis engines is invariably a component using logical formulas for describing states and transformations between system stat es. One can thus say that symbolic logic is the calculus of computation. The process of using this inf ormation for discovering and checking program properties (including such important properties a s afety and security) amounts to automatic theorem proving. In particular, theorem provers that direc tly support common software constructs offer a compelling basis. Such provers are commonly called s atisfiability modulo theories (SMT) solvers. Z3 is the leading SMT solver. It is developed by the a uthors at Microsoft Research. It can be used to check the satisfiability of logical formulas over o ne r more theories such as arithmetic, bit-vectors, lists, records and arrays. This paper examines three applications of Z3 in the context o f invariant generation. The first lets Z3 infer invariants as a constraint satisfaction problem, t he second application illustrates the use of Z3 for bit-precise analysis and our third application exemp lifies using Z3 for calculations.	array data structure;automated theorem proving;boolean satisfiability problem;computation;constraint satisfaction problem;microsoft research;modulo operation;ne (complexity);satisfiability modulo theories;scalability;solver;theory;z3 (computer)	Leonardo Mendonça de Moura;Nikolaj Bjørner	2010			discrete mathematics;satisfiability modulo theories;mathematics	SE	-18.90357845018404	18.89927535952089	12032
edab2f95fe512898b465b32f376bbec2e05961c0	qualitative probabilities for default reasoning, belief revision, and causal modeling		This paper presents a formalism that combines useful properties of both logic and probabilities. Like logic, the formalism admits qualitative sentences and provides symbolic machinery for deriving deductively closed beliefs and, like probability, it permits us to express if-then rules with different levels of firmness and to retract beliefs in response to changing observations. Rules are interpreted as order-of-magnitude approximations of conditional probabilities which impose constraints over the rankings of worlds. Inferences are supported by a unique priority ordering on rules which is syntactically derived from the knowledge base. This ordering accounts for rule interactions, respects specificity considerations and facilitates the construction of coherent states of beliefs. Practical algorithms are developed and analyzed for testing consistency, computing rule ordering, and answering queries. Imprecise observations are incorporated using qualitative versions of Jeffrey's rule and Bayesian updating, with the result that coherent belief revision is embodied naturally and tractably. Finally, causal rules are interpreted as imposing Markovian conditions that further constrain world rankings to reflect the modularity of causal organizations. These constraints are shown to facilitate reasoning about causal projections, explanations, actions and change. 1. Rankings as an order-of-magnitude abstraction of probabilities The uncertainty encountered in common sense reasoning fluctuates over an extremely wide range. For example, the probability that the new book on my desk is about astrology may be less than one in a million. However, if I open the wrappings and	algorithm;approximation;belief revision;causal filter;coherence (physics);coherent states;commonsense reasoning;default logic;interaction;knowledge base;semantics (computer science);sensitivity and specificity	Judea Pearl	1996	Artif. Intell.	10.1016/0004-3702(95)00090-9	artificial intelligence;data mining;mathematics;algorithm	AI	-16.3664706718672	6.978191344899956	12038
6dace424fb4e676aa1d3dc6bc095d8f284f0a4e8	building tools for emergent design with coppeer	buildings peer to peer computing collaborative work military computing content addressable storage proposals guidelines multiagent systems adaptive systems hierarchical systems;groupware;collaborative work;multi agent system;emergent design tools;requirement specifications;hierarchical systems;tools for cscw in design emergent design multi agent systems;tools for cscw in design;requirements elicitation;intelligent design assistants;multi agent systems;coppeer framework;peer to peer computing groupware intelligent design assistants multi agent systems;collaborative peer to peer applications;adaptive systems;emergent design;guidelines;multiagent systems emergent design tools coppeer framework requirement specifications collaborative peer to peer applications;peer to peer computing;content addressable storage;peer to peer;proposals;requirement specification;buildings;military computing;multiagent systems	In this work, we first present a model of emergent design which allows a community of designers to collectively translate a set of requirement specifications into a completed design. Then, we describe the COPPEER framework, an environment for developing and running collaborative peer-to-peer applications which directly supports the main requirements elicited in our proposal	emergent design;peer-to-peer;requirement	Mutaleci Miranda;Geraldo Xexéo;Jano Moreira de Souza	2006	2006 10th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2006.253217	computer science;knowledge management;artificial intelligence;requirements elicitation;multi-agent system;database;distributed computing;emergent design	EDA	-40.93472636607358	18.744329781163596	12055
1933e3c4fab14a63f2892c5faa6eaf00a930b353	independent model-driven software performance assessments of uml designs	real time;software performance evaluation;satisfiability;software performance;software development;unified modeling language;performance analysis;software lifecycle model driven software performance uml;software performance unified modeling language performance analysis object oriented modeling hardware application software real time systems software quality programming signal design;high performance;unified modeling language software performance evaluation	In many software development projects, performance requirements are not addressed until after the application is developed or deployed, resulting in costly changes to the software or the acquisition of expensive high-performance hardware. To remedy this, researchers have developed model-driven performance analysis techniques for assessing how well performance requirements are being satisfied early in the software lifecycle. In some cases, companies may not have the expertise to perform such analysis on their software; therefore they have an independent assessor perform the analysis. This paper describes an approach for conducting independent model-driven software performance assessments of UML 2.0 designs and illustrates this approach using a real-time signal generator as a case study	model-driven architecture;model-driven engineering;model-driven integration;performance engineering;real-time clock;requirement;software development process;software performance testing;unified modeling language;unix signal	Julie A. Street;IV G. Pettit RobertG.Pettit;Hassan Gomaa	2007	10th IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing (ISORC'07)	10.1109/ISORC.2007.39	unified modeling language;personal software process;verification and validation;real-time computing;software sizing;software performance testing;computer science;package development process;software design;social software engineering;software reliability testing;software framework;component-based software engineering;software development;software design description;software engineering;software construction;software walkthrough;programming language;software measurement;software deployment;goal-driven software development process;software requirements;software metric;software system;satisfiability;software peer review	SE	-60.917886147771625	30.00855244665345	12064
1684489c3e7c35ba96e7a636a27ae3c49830155b	visualized semantic model based on p/t nets	p/t nets;semantic model;visualized specification			Guofu Zhou;Yanxiang He;Zhuomin Du	2005			semantic data model;natural language processing;artificial intelligence;computer science	EDA	-29.920230602841727	17.57655622499919	12096
cc0a112f26c5b0017e1800f2b017848479ca6a9b	relcat: a relation registry for isocat data categories		The ISOcat Data Category Registry contains basically a flat and easily extensible list of data category specifications. To foster reuse and standardization only very shallow relationships among data categories are stored in the registry. However, to assist crosswalks, possibly based on personal views, between various (application) domains and to overcome possible proliferation of data categories more types of ontological relationships need to be specified. RELcat is a first prototype of a Relation Registry, which allows storing arbitrary relationships. These relationships can reflect the personal view of one linguist or a larger community. The basis of the registry is a relation type taxonomy that can easily be extended. This allows on one hand to load existing sets of relations specified in, for example, an OWL (2) ontology or SKOS taxonomy. And on the other hand allows algorithms that query the registry to traverse the stored semantic network to remain ignorant of the original source vocabulary. This paper describes first experiences with RELcat and explains some initial design decisions.	algorithm;iso 12620;prototype;semantic network;simple knowledge organization system;traverse;vocabulary;web ontology language	Menzo Windhouwer	2012			data mining;natural language processing;simple knowledge organization system;traverse;artificial intelligence;reuse;ontology;standardization;semantic network;extensibility;computer science;vocabulary	DB	-37.05339881371668	6.95959724024246	12122
1da69e13a414c5274badcafd7128f39a042a7d15	decision station: a notion for a situated dss	decision support;intelligent design;e business;sensors;agent technologies;software agents;decision support system;research and development;software agents decision support systems;intelligent agents decision support problem domain intelligence generic architecture decision station;decision support systems;intelligent agent;decision support systems spread spectrum communication intelligent sensors intelligent agent prototypes problem solving internet research and development kernel explosions;effectors	Despite the growing need for decision support in the digital age, there has not been an adequate increase of interest in research and development in Decision Support Systems (DSSs). In our view, the vision for a new type of DSS should provision a tighter integration with the problem domain, and include implementation phase in addition to the traditional intelligence, design, and choice phases. We argue that an adequate DSS in our dynamic electronic era should be situated in the problem environment. We propose a generic architecture for such a DSS incorporating sensors, effectors, and enhanced interfaces in addition to the traditional DSS kernel. We suggest the term “Decision Station” to refer to such situated DSS. We further elaborate on the possibilities to implement situated DSS in different segments of ebusiness. We argue in favor of using intelligent agents as the basis of new type of DSS. We further propose an architecture and describe a prototype for such DSS.	decision support system;electronic business;intelligent agent;problem domain;prototype;sensor;situated	Rustam M. Vahidov	2002		10.1109/HICSS.2002.993999	effector;r-cast;decision support system;intelligent decision support system;computer science;knowledge management;sensor;artificial intelligence;software agent;data mining;intelligent design;business decision mapping	AI	-51.8964733172255	8.863180474321986	12132
c1af3be67f34610dc2894897bae648b3fb3516dc	building high-assurance systems using cots components: whether, why, when and how?	cots product;cots components;building high-assurance systems;cots software;adaptive fault tolerance4;cots hardware;critical application;high-assurance system;high-assurance property;cots product selection;cots component;design fault	"""The implementation of COTS-based high assurance is becoming a major challenge today when cost concern has led to increased use of COTS products for critical applications. On the other hand, vendors remain reluctant to incorporate fault tolerance features into COTS products because doing so is likely to increase development and production costs and thus weaken the market competitiveness of their products. Therefore, it is crucial for us to cope with the current state of COTS.This panel brings together the researchers and practitioners with expertise, experiences and insights on using COTS components to build high-assurance systems. The purpose of the panel is to foster debating, exchanging and integrating opinions, ideas and solutions from various perspective (e.g., COTS software versus COTS hardware, COTS for long-life deep-space systems versus COTS for highly-available communication applications). We specially solicitate different opinions on the following issues: Whether can we build high-assurance systems using COTS components? Why is it inappropriate or impossible to build high-assurance systems using COTS components? (If the answer to the first question is """"No."""") Why is it possible to use COTS components that are not designed for critical applications to build high-assurance systems? (If the answer to the first question is """"Yes."""") When (that is, under which circumstances and conditions) is it appropriate to use COTS components for high-assurance systems? How do we derive solutions to mitigate the problems and inadequacies of COTS products?Among the particular questions we intend to discuss are: 1. What are the evaluation criteria and tradeoff strategies for COTS product selection for high-assurance systems?2. Is it viable to influence the vendors to provide or enhance high-assurance properties for the future versions of their COTS products? What are the strategies?3. Which will be the most practical and effective basis for us to develop methodologies that mitigate the effects of design faults and/or inadequacies of COTS software: fault predication, fault containment, or adaptive fault tolerance4. Is it possible and practical to integrate the methods for mitigating the effects of the design faults/inadequacies of COTS software and hardware in a high-assurance system? And how, if the answer is positive?"""		Raymond A. Paul;Ann T. Tai	1999		10.1109/HASE.1999.809482	reliability engineering;systems engineering;engineering;operating system;computer security	Robotics	-61.09169207995377	27.834799237884695	12135
2efc5d6d946123e80eef776b32465c262a671608	integrating constraints and concurrent objects in musical applications: a calculus and its visual language	concepcion asistida;programmation logique avec contrainte;computer aided design;amplitude shift keying;concurrent programming;calcul formel;musica;programacion logica con restriccion;concurrent constraint;concurrent program;transmission message;modulation deplacement amplitude;computer aided music composition;pico;info eu repo semantics article;message transmission;calculo formal;musique;visual languages;tyco;langage visuel;envoi message;concurrent constraint objects;programa competidor;integrity constraints;constraint programming;visual language;message passing;conception assistee;concurrent programs;constraint system;constraint logic programming;formal calculi;modulacion desplazamiento amplitud;synchronous communication;computer algebra;mobile processes;concurrent process;music;composition sur ordinateur;computer controlled typesetting;programme concurrent;object model;transmision mensaje	We propose PiCO, a calculus integrating concurrent objects and constraints, as a base for music composition tools. In contrast with calculi such as NiehrenMueller:Free, milner.parrow.ea:calculus-mobile or TyCO vasconcelos:typed-concurrent, both constraints and objects are primitive notions in PiCO. In PiCO a base object model is extended with constraints by orthogonally adding the notion of constraint system found in the ρ-calculus OzCalculus. Concurrent processes make use of a constraint store to synchronize communications either via the ask and tell operations of the constraint model or the standard message-passing mechanism of the object model. A message delegation mechanism built into the calculus allows encoding of general forms of inheritance. This paper includes encodings in PiCO of the concepts of class and sub-class. These allow us to represent complex partially defined objects such as musical structures in a compact way. We illustrate the transparent interaction of constraints and objects by a musical example involving harmonic and temporal relations. The relationship between Cordial, a visual language for music composition applications, and its underlying model PiCO is described.	abstract machine;abstract rewriting system;comparison of computer-aided design editors;compiler;constraint logic programming;high-level programming language;immutable object;instance (computer science);java;message passing;ontology components;operational semantics;pico;process calculus;programming tool;turing completeness;visual editor;visual language	Camilo Rueda;Gloria Alvarez;Luis Quesada;Gabriel Tamura;Frank D. Valencia;Juan Francisco Díaz;Gérard Assayag	2001	Constraints	10.1023/A:1009849309451	constraint logic programming;constraint programming;message passing;real-time computing;object model;concurrent computing;computer science;artificial intelligence;computer aided design;music;mathematics;programming language;algorithm	PL	-24.936689299007988	30.679748228421346	12143
d57af9915ab7e6fffeea656172a70af9f0048f70	a modality-independent mmi system architecture	system architecture	This paper discusses the design of a modality-independent MMI system architecture. In the architecture, the MMI system is divided into three modules: the document server module which holds dialog scenarios and contents, the dialog manager which controls dialog flow, and the front-end module which manages the user’s inputs and the system’s outputs. This division enables us to reuse the document server module and the dialog manager when introducing new terminals with different types of modalities because they are independent of modalities. Moreover, we propose an MMI description language XISL. Since it has the flexibility to describe the user’s inputs and the system’s outputs, it can be used for describing interactions on various terminals without introducing a new description language and its processor. We show a prototype system of an online shopping application implemented on our architecture, and compare the difference between XISL and other languages.	dialog manager;interaction;modality (human–computer interaction);multi media interface;online shopping;prototype;server (computing);systems architecture	Kouichi Katsurada;Yoshihiko Ootani;Yusaku Nakamura;Satoshi Kobayashi;Hirobumi Yamada;Tsuneo Nitta	2002			speech recognition;computer science;systems architecture	Web+IR	-49.049901792403496	13.090187437161173	12147
023b1869bdd8fb657eb6c2ef79db2fcf2b75d4bb	4-way parallel processor partition of an atmospheric primitive-equation prediction model	numerical technique;prediction model	"""A principal mission of the Fleet Numerical Weather Central is to provide, on an operational basis, numerical meteorological and oceanographic products peculiar to the needs of the Navy. Toward this end the FNWC is also charged with the development and test of numerical techniques applicable to Navy environmental forecasting problems. A recent achievement of this development program has been the design, development, and beginning in September 1970, operational use of the FNWC five-layer, baroclinic, atmospheric prediction model, based on the so-called """"primitive-equations,"""" and herein defined as the Primitive Equation Model (PEM)."""	numerical analysis;parallel computing;primitive equations;programmer	Edward Morenoff;W. Beckett;P. G. Kesel;F. J. Winninghoff;P. M. Wolff	1971		10.1145/1478786.1478793	meteorology;navy operational global atmospheric prediction system;simulation;engineering;operations research	HPC	-7.048135668928667	34.94451100729312	12165
aeb60924e7d7d66f1115e4bff81a3c1682c0053e	deployment of service-based processes in the cloud using petri net decomposition		Cloud Computing is a new distributed computing paradigm that con- sist in provisioning of infrastructure, software and platform resources as services. Platform services are limited to proprietary or specific programming frameworks and APIs. This issue is not adequate for the deployment of service-based pro- cesses which are likely to be composed of a diverse and heterogeneous set of ser- vices. In this paper, we propose a new approach to provision appropriate platform resources in order to deploy service-based processes in existing Cloud platforms. Our approach consists in slicing a given process to deploy into a set of elementary services through a Petri net decomposition approach. Source codes of obtained services are generated. After that, the services are packaged in our already devel- oped service micro-containers and deployed in any target PaaS. For the slicing, we defined algorithms to slice their correspondent Petri net into a set of depen- dent WF-nets and to determine the orchestration to follow for their execution. We also provided the proof of preservation of initial business process semantics when executing the WF-nets. To illustrate and show the feasibility of our proposition, we provide a realistic use case scenario, i.e. Shop process deployment in Cloud Foundry PaaS.		Sami Yangui;Kais Klai;Samir Tata	2014		10.1007/978-3-662-45563-0_4	real-time computing;computer science;database;distributed computing;services computing	Robotics	-45.59517989238366	41.944213409469036	12177
7dbc7f7e18cc2137172c05ed06803e4acf6c6fd6	possible forms of evaluation or reduction in martin-löf type theory	logique combinatoire;type theory;combinatorial logic	In his type theory, Martin-Lff considers certain evaluation procedures for his expressions. These evaluation procedures or reductions can be interpreted in various ways; this paper examines the properties that such reductions must have to satisfy Martin-Lrf's rules.	type theory	Martin W. Bunder	1985	Theor. Comput. Sci.	10.1016/0304-3975(85)90065-9	discrete mathematics;computer science;mathematics;combinational logic;programming language;type theory;algorithm	PL	-10.287318805959904	15.842526704523289	12187
d2a19544c3fe73f27bd1cf1f7738179673b9bf75	a method for ontology conflict resolution and integration on relation level	conflict resolution	Conflict of ontologies can appear on one of three levels: instance level, concept level and relation level. Conflict on instance level appears when the same instance occurs in different ontologies with different descriptions. Two ontologies are in conflict on concept level if the same concept has different structures in these ontologies. Conflict on relation level refers to different relations between the same two concepts in different ontologies. In this article a method for ontology conflict resolution and integration on relation level is presented. Within this method we present a model for representing ontology conflict and consensus-based algorithms for conflict resolution.		Ngoc Thanh Nguyen	2007	Cybernetics and Systems	10.1080/01969720701601098	computer science;knowledge management;artificial intelligence;data mining	NLP	-20.515862319786294	4.344967790172121	12195
c82f3010184804e74a1fb1868707f80ed3e035a9	xigt: extensible interlinear glossed text for natural language processing	annotation;storage format;interlinear glossed text igt	This paper presents Xigt, an extensible storage format for interlinear glossed text (IGT). We review design desiderata for such a format based on our own use cases as well as general best practices, and then explore existing representations of IGT through the lens of those desiderata. We give an overview of the data model and XML serialization of Xigt, and then describe its application to the use case of representing a large, noisy, heterogeneous set of IGT.	best practice;data model;database schema;documentation;gloss (annotation);information governance;natural language processing;open-source software;serialization;xml	Michael Wayne Goodman;Joshua Crowgey;Fei Xia;Emily M. Bender	2015	Language Resources and Evaluation	10.1007/s10579-014-9276-1	computer science;data mining;database;information retrieval;java annotation	NLP	-36.26544272667446	7.625883239483873	12201
7fe520fb7c9d19d6594e12b2a15f9a8a99647ca6	a multiagent system for hierarchical control and monitoring	multiagent system;hierarchical control;control system;process control;discrete event;process control system	This paper presents the architecture of a multiagent system based on new OPC Unified Architecture (UA) technology for hierarchical control and monitoring of a complex process control system. This architecture is proposed with utilization of the OPC technology, which contains both a continuous-event component and a discreteevent component by incorporating XML for the negotiation and cooperation with the multiagent system’s environments. The practical applications of the proposed architecture are provided and the discussion demonstrates that the proposed architecture is both reliable and effective for applying to multiagent-based complex control system	agent-based model;algorithm;automation;distributed control system;hierarchical control system;multi-agent system;opc unified architecture;open platform communications;real-time clock;real-time computing;requirement;systems architecture;thomas j. watson research center;user agent;xml	Vu Van Tan;Dae-Seung Yoo;Jun-Chol Shin;Myeong-Jae Yi	2009	J. UCS	10.3217/jucs-015-13-2485	real-time computing;real-time control system;process control;distributed computing;hierarchical control system	Robotics	-38.118200199357396	22.197332418429145	12203
059e3830b1f6a29079e79cfcfdfb10a8fc2e6d7d	an exercise in resource allocation	resource allocation;performance;multiprogramming;monitoring;scheduling;analysis;cdc 6000;resource	Abstract#R##N##R##N#In a large operating system, the probability that bottlenecks exist is high. The outcome of modifications to the system attempting to overcome these bottlenecks are often not easy to predict. It is frequently difficult to discover: #R##N##R##N#(1)#R##N#Whether an improvement has actually been made to the system.#R##N##R##N#(2)#R##N#Where exactly the improvement in system performance, if any, is occurring.#R##N##R##N#(3)#R##N#How to adjust parameters of the system to achieve an improved performance.#R##N##R##N##R##N##R##N##R##N##R##N#Performance tools are described in this paper which were used to help resolve these points in the implementation of a Peripheral Processor and Channel Scheduling mechanism in the operating system used at CERN on a CDC 6000 system. The paper shows how analysis of the performance data provided a clearer appreciation of the performance of the scheduling mechanism.		Hassan Gomaa	1974	Softw., Pract. Exper.	10.1002/spe.4380040303	real-time computing;simulation;computer multitasking;performance;resource allocation;computer science;operating system;analysis;scheduling;resource	ECom	-15.850266074109719	60.44601572136746	12207
61f737795d9173718c3b5992f91e0730a32030cd	generalizing agm to a multi-agent setting	belief revision;epistemic logic	We generalize AGM belief revision theory to the multi-agent case. To do so, we first generalize the semantics of the single-agent case, based on the notion of interpretation, to the multi-agent case. Then we show that, thanks to the shape of our new semantics, all the results of the AGM framework transfer. Afterwards we investigate some postulates that are specific to our multi-agent setting. Finally, we give an example of revision operator that fulfills one of these new postulates and give an example of revision on a concrete example.	belief revision;multi-agent system	Guillaume Aucher	2010	Logic Journal of the IGPL	10.1093/jigpal/jzp037	epistemology;mathematics;algorithm	AI	-17.006558864960745	5.3282139225502005	12222
099f34c0563dcdd41ee21a6ebcc985412418b427	lemmas on demand for the extensional theory of arrays	hardware verification;satisfiability;decision problem;sat;arrays;decision procedure;smt	Deciding satisfiability in the theory of arrays, particularly in combination with bit-vectors, is essential for software and hardware verification. We precisely describe how the lemmas on demand approach can be applied to this decision problem. In particular, we show how our new propagation based algorithm can be generalized to the extensional theory of arrays. Our implementation achieves competitive performance.	algorithm;boolean satisfiability problem;decision problem;software propagation	Robert Brummayer;Armin Biere	2008	JSAT	10.1145/1512464.1512467	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-14.231947017282007	23.996052187678835	12241
9c0827a436435e2415c29b2f16ab9f5278e0a3d9	best practices for the security evaluation of biometric systems	biometrics access control security testing iso standards iec standards guidelines;operational environment security evaluation methodology biometric systems nondeterministic technology user behavior;security of data biometrics access control human factors;attack potential biometric systems security testing best practices	This paper describes best practices for the security evaluation of biometric systems. This type of evaluation has been addressed in several documents. However, not all of these documents describe the complete evaluation methodology, or are focused on biometrics or do propose clear testing procedures. Therefore, this work defines the most proper way to carry out this evaluation methodology considering biometrics and the special characteristics of these kind of systems (i.e. biometrics is non-deterministic technology highly influenced by user behavior and the operational environment). In particular, this paper specifies how to select which attacks shall be studied, how to conduct such attacks and which is the most suitable method to report the evaluation results.	best practice;biometrics;requirement;system under test	Belen Fernandez-Saavedra;Raul Sánchez-Reillo;Judith Liu-Jimenez;Jorge Gutierrez Ruiz	2014	2014 International Carnahan Conference on Security Technology (ICCST)	10.1109/CCST.2014.6987034	software security assurance;standard of good practice;cloud computing security;reliability engineering;itil security management;certified information systems security professional;information security management system;security information and event management;information security;logical security;security service;business;internet privacy;security analysis;security testing;computer security	SE	-56.4531778148731	48.33584452030573	12258
7bb88ad8b0a8715313a23b1ef6dced946d1a41e3	understanding and improving uml package merge	formal model;tool support;first order;modelling language;meta model	Package merge allows the content of one package to be combined with that of another package. Package merge is used extensively in the UML 2 specification to modularize the definition of the UML 2 meta model and to define the four compliance levels of UML 2. Package merge is a novel construct in UML and currently not well understood. This paper summarizes our work to understand and improve package merge. First, we identify ambiguous and missing rules in the package merge definition and suggest corrections. Then, we formalize package merge and analyze it with respect to some desirable properties. Our analyses employs Alloy, a first-order modelling language with tool support, and concepts from mathematical logic which allow us to develop a general taxonomy of package extension mechanisms. The analyses reveal the unexpected failure of important properties.	alloy (specification language);automated theorem proving;backward compatibility;eclipse;first-order predicate;ibm centers for advanced studies;idempotence;interaction;linear algebra;meta-object facility;metamodeling;model checking;requirement;taxonomy (general);theory;unified modeling language	Jürgen Dingel;Zinovy Diskin;Alanna Zito	2007	Software & Systems Modeling	10.1007/s10270-007-0073-9	metamodeling;package diagram;computer science;software engineering;applications of uml;first-order logic;data mining;database;programming language;package	SE	-45.96366882521075	27.09504192771433	12264
3d9be4ede9f2b19d78a92ff0cbe92e1b9d12ef39	design and implementation of cohesion	release consistency;sequential consistency;personal computer;design and implementation;network traffic;matrix multiplication;parallel programs;critical section;distributed shared memory;memory consistency model	This paper describes a prototype DSM system called Cohesion which supports two memory consistency models, namely, Sequential consistency and Release consistency, within a single program to improve performance and support a wide variety of parallel programs for the system. Memory that is sequentially consistent is further divided into object-based and conventional (page-based) memory, where they are constructed at the user-level and kernel-level, respectively. In object-based memory, the shared data are kept consistent in terms of the granularity of an object; this is provided to improve the performance of fine-grained parallel applications that may incur a significant overhead in conventional or release memory as well as to eliminate unnecessary movement of pages which are protected in a critical section. On the other hand, the Release consistency model is supported in Cohesion to attack the problem of excessive network traffic and false sharing. Cohesion programs are written in C++, and the annotation of shared objects for release and object-based memory is accomplished by inheriting a system-provided base class. Cohesion is built up on a network of Intel 486-33 personal computers which are connected by a 10Mbps Ethernet. Three application programs, including Matrix Multiplication, SOR, and N-body, have been employed to evaluate the efficiency of Cohesion. In addition, a Producer-Consumer program has been tested to show that the object-based memory will benefit us in a critical section.	atm turbo;c++;central processing unit;cohesion (computer science);compiler;consistency model;critical section;false sharing;library (computing);matrix multiplication;microprocessor;n-body problem;network traffic control;object-based language;overhead (computing);personal computer;preprocessor;programmer;prototype;release consistency;sequential consistency;shared memory;speedup;thrashing (computer science);type inference;user space	Ce-Kuen Shieh;Jyh-Chang Ueng;An-Chow Lai	1998	J. Inf. Sci. Eng.		uniform memory access;distributed shared memory;shared memory;memory model;cache coherence;interleaved memory;parallel computing;real-time computing;distributed memory;matrix multiplication;computer science;consistency model;operating system;release consistency;database;distributed computing;overlay;critical section;extended memory;flat memory model;programming language;computer security;sequential consistency;algorithm;memory map;memory management	Arch	-14.148171165755924	47.338424523990255	12266
725903cbef3080bfddb2466f8592fc61b764f521	enabling enriched tv shopping experience via computational and temporal aware view-centric multimedia abstraction	detectors;video signal processing multimedia computing object detection purchasing retail data processing user interfaces;roi tv shopping experience computational aware view centric multimedia abstraction temporal aware view centric multimedia abstraction smart tvs internet pc technologies product purchasing multimedia content content understanding user interface computational aware multimedia abstraction framework temporal aware multimedia abstraction framework object detection tasks computational constraints temporal rate constraints optimal video frames optimal abstraction frame selection process regions of interest;visualization;streaming media;multimedia communication;video motion dynamic programming multimedia content abstraction region of interest roi smart tv tv shopping;tv object detection computational efficiency multimedia communication visualization detectors streaming media;tv;computational efficiency;object detection	Smart TVs have realized the convergence of TV, Internet , and PC technologies, but still do not provide a seamless content interaction for TV-enabled shopping. To purchase interesting items displayed in a TV show, consumers must resort to a store or the Web, which is an inconvenient way of purchasing products. The fundamental challenge in realizing such a use case consists of understanding the multimedia content being streamed. Such a challenge can be realized by utilizing object detection to facilitate content understanding though it has to be executed as a computationally bound process so that consumers are provided with a responsive and exciting user interface. To this end, we propose a computational- and temporal-aware multimedia abstraction framework that facilitates the efficient execution of object detection tasks. Given computational and temporal rate constraints, the proposed framework selects the optimal video frames that best represent the video content and allows the execution of the object detection task as a computationally bound process. In this sense, the framework is computationally scalable as it can adapt to the given constraints and generate optimal abstraction results accordingly. Additionally, the framework utilizes “object views” as the basis for the frame selection process, which depict salient information and are represented as regions of interest (ROI). In general, an ROI can be a whole frame or a region that discards background information. Experimental results demonstrate the computational scalability of the proposed framework and the benefits of using the regions of interest as the basis of the abstraction process.	computation;digital video;object detection;purchasing;region of interest;scalability;seamless3d;smart tv;streaming media;user interface;world wide web	Fausto Fleites;Haohong Wang;Shu-Ching Chen	2015	IEEE Transactions on Multimedia	10.1109/TMM.2015.2433213	detector;real-time computing;simulation;visualization;computer science;operating system;multimedia;world wide web	Vision	-47.09704281948397	12.320738682555529	12268
58c6981ac78a971b9d3ef2642b99ebf9256b18bc	lossless conditional schema evolution	entity relationship model;database system;base donnee;software maintenance;integration information;interrogation base donnee;conceptual analysis;database;interrogacion base datos;base dato;semantics;modelo entidad relacion;modele entite relation;semantica;semantique;analisis conceptual;maintenance logiciel;information integration;integracion informacion;systeme gestion base donnee;information system;analyse conceptuelle;sistema gestion base datos;database management system;schema evolution;database query;systeme information;sistema informacion	Conditional schema changes change the schema of the tuples that satisfy the change condition. When the schema of a relation changes some tuples may no longer fit the current schema. Handling the mismatch between the intended schema of tuples and the recorded schema of tuples is at the core of a DBMS that supports schema evolution. We propose to keep track of schema mismatches at the level of individual tuples, and prove that evolving schemas with conditional schema changes, in contrast to database systems relying on data migration, are lossless when the schema evolves. The lossless property is a precondition for a flexible semantics that allows to correctly answer general queries over evolving schemas. The key challenge is to handle attribute mismatches between the intended and recorded schema in a consistent way. We provide a parametric approach to resolve mismatches according to the needs of the application. We introduce the mismatch extended completed schema (MECS) which records attributes along with their mismatches, and we prove that relations with MECS are	algebraic equation;database;lossless compression;mathematical optimization;parametric polymorphism;precondition;query optimization;schema evolution	Ole Guttorm Jensen;Michael H. Böhlen	2004		10.1007/978-3-540-30464-7_46	schema;semi-structured model;entity–relationship model;computer science;conceptual schema;information integration;document structure description;star schema;data mining;xml schema;database;semantics;software maintenance;superkey;database schema;information system;algorithm	DB	-31.697127742673707	11.36879303095392	12273
dbd0cdbd3cb0b280de7f1b8116e9f910c93c6e42	an abstract database machine for cost driven design of object-oriented database schemas	machine abstraite;maquina abstracta;abstract machine;object oriented;oriente objet;object oriented database;systeme gestion base donnee;cost estimation;sistema gestion base datos;orientado objeto;database management system	The process of designing an object-oriented database schema consists of several phases. During the phase of abstract logical formalisation one of many possible abstract object-oriented database schemas must be chosen. This choice can be driven by the costs of the ultimately implemented schema: How much space is needed? How long does it take to compute queries and updates including enforcement of semantic constraints? Because abstract logical formalisation is done independently of an actual database management system, we need an abstract database machine. Queries and updates are formulated as programs for this database machine. Such programs are composed of steps which are connected by channels for typed streams of value lists. In each step, a basic or compound operation is executed, accepting input streams and further parameters, delivering output streams for subsequent steps, and accessing the persistent database state. The abstract database machine is designed to meet two goals: to be expressive enough to implement queries and updates, as considered for schema design, and to be simple enough to allow cost estimations.	database machine	Joachim Biskup;Ralf Menzel	2001		10.1007/3-540-44803-9_28	database theory;intelligent database;semi-structured model;database tuning;computer science;artificial intelligence;theoretical computer science;operating system;database model;data mining;database;distributed computing;abstract machine;programming language;object-oriented programming;view;database schema;database testing;algorithm;database design;cost estimate	DB	-29.197968177670774	11.22033114526182	12278
3269b88b17cffbee95eab16044bf9687aa4fce04	modeling wildcard-free mpi programs for verification	formal methods;finite state verification;parallel computation;spin;formal method;model checking;concurrent systems;message passing interface;state space;parallel computer;deadlock;mpi;analysis;parallel programs	We give several theorems that can be used to substantially reduce the state space that must be considered in applying finite-state verification techniques, such as model checking, to parallel programs written using a subset of MPI. We illustrate the utility of these theorems by applying them to a small but realistic example.	finite-state machine;model checking;state space	Stephen F. Siegel;George S. Avrunin	2005		10.1145/1065944.1065957	parallel computing;formal methods;computer science;message passing interface;theoretical computer science;distributed computing;programming language	Logic	-12.182831811698753	26.017625317941533	12279
a3bf00f07b4d93368137a58fde52eb9efb71ab8e	a tool for an analysis of the dynamic behavior of logistic systems with the instruments of complex networks		It is known that the whole is more than the sum of its parts. In production for each machine a lot of information is available due to today’s integration of automatic data recording. In this context, one way of representing the whole is the modeling as a complex network. Yet, present complex network analysis tools can either not manage the amount of data of such systems or neglect their dynamic behavior. Therefore, we present a tool, which meets these requirements of the logistic field, and demonstrate its abilities for a real-world example.		T. Funke;Till Becker	2018		10.1007/978-3-319-74225-0_57	neglect;complex network;machine learning;computer science;artificial intelligence	HPC	-55.83633829020278	12.374582033019674	12284
9e8bca9e4b6badfb7660b7aeacd2dc34382533b2	distributed memory system architecture based on the analyses of human brain memory			distributed memory	Muneo Kitajima;Makoto Toyota	2013			data diffusion machine;psychology;memory map;cognitive psychology;cache-only memory architecture;systems architecture;distributed shared memory;distributed memory;human brain;distributed computing	EDA	-10.398270736161294	43.157001272009126	12287
81ef853135d39c8a5dd10b131758130772f8f21e	xml-enabled relational database for xml document update	xml relational databases sql;xml schema;investments;research needs;digital repository;sql;iso standards;xml enabled database;structure query language;data engineering;relational database;structure query language xml data storage xml enabled database xendb relational database semantic constraint sql;data storage;semantic constraint;database systems;xml;xendb;native xml database;xml document;relational databases;object oriented databases;la trobe university research online;xml data storage;database management system;relational databases xml object oriented databases data models australia database systems iso standards data engineering memory investments;memory;australia;data models	With increasing demands for a proper and efficient XML data storage, XML-enabled database (XEnDB) has emerged as one of the popular answers. It claims to combine the strengths and limit the shortcomings of the traditional database management systems and native XML database. The implication is more research need to be done for this database family. This paper focuses on the XML update management in XEnDB. Our aim is to preserve the conceptual semantic constraints in XML data during update operations. The constraints are classified and represented in SQL/XML schema. Then, we propose the update methodology that utilizes the proposed schema and implement the method in one of the current XEnDB products	computer data storage;relational database;sql;sql/xml;xml database;xml schema	Eric Pardede;J. Wenny Rahayu;David Taniar	2006	20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)	10.1109/AINA.2006.354	xml catalog;xml validation;xml encryption;xml base;simple api for xml;xml;semi-structured model;relax ng;information engineering;xml schema;streaming xml;relational database;computer science;document structure description;database model;xml framework;data mining;xml database;xml schema;database;xml signature;database schema;xml schema editor;information retrieval;efficient xml interchange	DB	-31.514789485505318	6.891846738760071	12299
42f26cd7904dda8843e357c283f323e9101f37c3	optimization of mpi collective communication on bluegene/l systems	shared memory;collective communication;point to point;performance;programming model;low power;bluegene;mpi;optimization;high speed	BlueGene/L is currently the world's fastest supercomputer. It consists of a large number of low power dual-processor compute nodes interconnected by high speed torus and collective networks, Because compute nodes do not have shared memory, MPI is the the natural programming model for this machine. The BlueGene/L MPI library is a port of MPICH2.In this paper we discuss the implementation of MPI collectives on BlueGene/L. The MPICH2 implementation of MPI collectives is based on point-to-point communication primitives. This turns out to be suboptimal for a number of reasons. Machine-optimized MPI collectives are necessary to harness the performance of BlueGene/L. We discuss these optimized MPI collectives, describing the algorithms and presenting performance results measured with targeted micro-benchmarks on real BlueGene/L hardware with up to 4096 compute nodes.	algorithm;blue gene;fastest;l-system;mpich;message passing interface;point-to-point protocol;point-to-point (telecommunications);programming model;shared memory;supercomputer;top500	George Almási;Philip Heidelberger;Charles Archer;Xavier Martorell;C. Christopher Erway;José E. Moreira;Burkhard D. Steinmacher-Burow;Yili Zheng	2005		10.1145/1088149.1088183	shared memory;parallel computing;real-time computing;performance;point-to-point;computer science;message passing interface;operating system;distributed computing;programming paradigm	HPC	-10.177750135777456	44.98229671113559	12319
1c4725b6dafce3a22de3dfa9a30eebaa34cdac3d	the super warp architecture with random address shift	storage management;rsdmm super warp architecture random address shift theoretical parallel computing model streaming multiprocessor cuda enabled gpu memory banks shared memory memory access congestion random super discrete memory machine;shared memory systems;parallel architectures;graphics processing units;instruction sets graphics processing units;randomized technique gpu cuda memory bank conflicts memory access congestion;storage management concurrency theory graphics processing units parallel algorithms parallel architectures shared memory systems;concurrency theory;parallel algorithms	The Discrete Memory Machine (DMM) is a theoretical parallel computing model that captures the essence of memory access by a streaming multiprocessor on CUDA-enabled GPUs. The DMM has w memory banks that constitute a shared memory, and each warp of w threads access the shared memory at the same time. However, memory access requests destined for the same memory bank are processed sequentially. Hence, it is very important for developing efficient algorithms to reduce the memory access congestion, the maximum number of memory access requests destined for the same bank. However, it is not easy to minimize the memory access congestion for some problems. The main contribution of this paper is to present novel and practical parallel computing models in which the congestion is small for any memory access requests. We first present the Super Discrete Memory Machine (SDMM), an extended version of the DMM, which supports a super warp with multiple warps. Memory access requests by multiple warps in a super warp are packed through pipeline registers to reduce the memory access congestion. We then go on to apply the random address shift technique to the SDMM. The resulting machine, the Random Super Discrete Memory Machine (RSDMM) can equalize memory access requests by a super warp. Quite surprisingly, for any memory access requests by a super warp on the RSDMM, the overhead of the memory access congestion is within a constant factor of perfectly scheduled memory access. Thus, unlike the DMM, developers of parallel algorithms do not have to consider the memory access congestion on the RSDMM. The congestion on the RSDMM is evaluated by theoretical analysis as well as by experiments.	cuda;digital molecular matter (dmm);experiment;graphics processing unit;memory bank;multiprocessing;network congestion;overhead (computing);parallel algorithm;parallel computing;pipeline (computing);shared memory	Koji Nakano;Susumu Matsumae	2013	20th Annual International Conference on High Performance Computing	10.1109/HiPC.2013.6799118	cuda pinned memory;uniform memory access;distributed shared memory;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;computer hardware;computer science;physical address;operating system;distributed computing;computer memory;parallel algorithm;overlay;conventional memory;extended memory;flat memory model;registered memory;sequential access memory;data diffusion machine;computing with memory;memory map;non-uniform memory access;memory management	HPC	-10.839286110779767	50.727353197515086	12327
cce3b07ac87d8c7ff10cb3c703de9af1c2bc1680	a version management method for distributed information	production facilities;version management	Information networks in which workstations access central data collections primarily to extract information are a growing application segment. When a workstation is likely to use an item more than once it may be economical to retain a copy locally. We describe how to propagate updates for an arbitrary relationship between the source database and cached items when the network connection is intermittent, unreliable, and/or slow.	cache (computing);version control;workstation	Henry M. Gladney;Douglas J. Lorch;Richard L. Mattson	1987	1987 IEEE Third International Conference on Data Engineering	10.1109/ICDE.1987.7272425	computer science;data mining;database;distributed computing	DB	-22.45344365192336	50.181190687881745	12337
3a76add36aee4b1e84aa3695f8cb9ddff5366f63	consistency checking of functional requirements		Requirements are informal and semi-formal descriptions of the expected behavior of a system. They are usually expressed in the form of natural language sentences and checked for errors manually, e.g., by peer reviews. Manual checks are error-prone, time-consuming and not scalable. With the increasing complexity of cyber-physical systems and the need of operating in safetyand security-critical environments, it became essential to automatize the consistency check of requirements and build artifacts to help system engineers in the design process.	cognitive dimensions of notations;cyber-physical system;natural language;requirement;scalability;semiconductor industry;software peer review	Simone Vuotto	2018	CoRR		algorithm;mathematics;functional requirement;peer review;natural language;scalability;engineering design process;theoretical computer science	SE	-55.12171022266621	25.21654043685406	12338
2d94e85455e05c35643b3521d2aec5bd9c5d526b	risk-based interoperability testing using reinforcement learning		Risk-based test strategies enable the tester to harmonize the number of specified test cases with imposed time and cost constraints. However, the risk assessment itself often requires a considerable effort of cost and time, since it is rarely automated. Especially for complex tasks such as testing the interoperability of different components it is expensive to manually assess the criticality of possible faults. We present a method that operationalizes the risk assessment for interoperability testing. This method uses behavior models of the system under test and reinforcement learning techniques to break down the criticality of given failure situations to the relevance of single system actions for being tested. Based on this risk assessment, a desired number of test cases is generated which covers as much relevance as possible. Risk models and test cases have been generated for a mobile payment system within an industrial case study.	reinforcement learning	André Reichstaller;Benedikt Eberhardinger;Alexander Knapp;Wolfgang Reif;Marcel Gehlen	2016		10.1007/978-3-319-47443-4_4	knowledge management;machine learning;world wide web	ML	-48.5487009449297	38.132163652250654	12341
4ef7e8e9ce216122d6768b9499db212261922c37	improving the scalability of parallel jobs by adding parallel awareness to the operating system	system kernel modification;empirical result;operating system;collective operation;traditional operating system scheduling;run-time system;spmd bulk-synchronous programming style;processes increase;parallel jobs;parallel awareness;fine-grain collective activity;synchronizing collective;parallel application benefit;kernel;scalability;mpi;operating systems;switches;application software;linux	A parallel application benefits from scheduling policies that include a global perspective of the application's process working set. As the interactions among cooperating processes increase, mechanisms to ameliorate waiting within one or more of the processes become more important. In particular, collective operations such as barriers and reductions are extremely sensitive to even usually harmless events such as context switches among members of the process working set. For the last 18 months, we have been researching the impact of random short-lived interruptions such as timer-decrement processing and periodic daemon activity, and developing strategies to minimize their impact on large processor-count SPMD bulk-synchronous programming styles. We present a novel co-scheduling scheme for improving performance of fine-grain collective activities such as barriers and reductions, describe an implementation consisting of operating system kernel modifications and run-time system, and present a set of empirical results comparing the technique with traditional operating system scheduling. Our results indicate a speedup of over 300% on synchronizing collectives.	operating system;scalability	Terry Jones;Shawn Dawson;Rob Neely;William G. Tuel;Larry Brenner;Jeffrey Fier;Robert Blackmore;Patrick Caffrey;Brian Maskell;Paul Tomlinson;Mark Roberts	2003		10.1109/SC.2003.10024	application software;parallel computing;kernel;real-time computing;scalability;network switch;computer science;message passing interface;operating system;distributed computing;programming language;linux kernel	HPC	-14.648418411000238	48.48277979966379	12344
47b0cca4d8da7675eca7f72fd3a70a8d6c7dcb9a	space-efficient multi-versioning for input-adaptive feedback-driven program optimizations	feedback driven program optimization;function versioning;program inputs	Function versioning is an approach to addressing input-sensitivity of program optimizations. A major side effect of it is notable code size increase, which has been hindering its broad applications to large code bases and space-stringent environments. In this paper, we initiate a systematic exploration into the problem, providing answers to some fundamental questions: Given a space constraint, to which function we should apply versioning? How many versions of a function should we include in the final executable? Is the optimal selection feasible to do in polynomial time? This study proves selecting the best set of versions under a space constraint is NP-complete and proposes a heuristic algorithm named CHoGS which yields near optimal results in quadratic time. We implement the algorithm and conduct experiments through the IBM XL compilers. We observe significant performance enhancement with only slight code size increase; the results from CHoGS show factors of higher space efficiency than those from traditional hotness-based methods.	algorithm;compiler;executable;experiment;heuristic (computer science);np-completeness;program optimization;time complexity;version control	Mingzhou Zhou;Xipeng Shen;Yaoqing Gao;Graham Yiu	2014		10.1145/2660193.2660229	real-time computing;computer science;theoretical computer science;programming language;algorithm	PL	-15.92887102910904	25.49727990978816	12347
2b909cfaef66b006357a0e252a3a647205d9d0c5	`contextual objects' or goal orientation for business process modeling	goal orientation;business process model;object oriented;business process	Traditional object orientation doesn’t meet business process modeling requirements since objects do not incorporate the goals for which they collaborate. Indeed, business processes like Order Management, Sales, etc. and their steps (activities) must be driven by a goal in order to take dynamic decisions at any step of the process and to evaluate the progression of activities. Thus a business process may be assumed as a goal-oriented graph with each step representing a goal-oriented node. Listing attributes and operations within classes does not help to model business process steps since the goal and the emerging context of objects are not expressed in today’s object-oriented diagrams (even in the UML’s activity, sequence, statetransition or collaboration diagrams). ‘Contextual Objects’ represent Goals and Contexts by object’s contextual behaviors that express implicitly attributes, relationships and methods that depend on a goal and on a context. Modeling goals requires that each activity inside a process step is conducted by a driver object [1]. Contextual objects collaborate to realize the goal whenever a driver object enters into one of its lifecycle stages. For example, an ‘order’ incites other objects, such as product, customer, delivery, invoice, etc., to react, depending on the context. UML 1.1. has a work unit [2] whose state is reflected by the state of its dominant entity (driver object ) [3]. But nothing is said about the internal dynamics of a work unit whose detailed description becomes necessary to define the responsibilities of its participating objects. In such a way, contextual objects should contribute to modelling activities inside business processes (see Figure 1). Secondly, we propose the Behavioral State Transition Diagram to model the goal and responsibilities of objects inside each step of a business process. This diagram (Figure 2) represents an activity's internal behavior as a reusable component. As a bridge toward use cases, a Work Unit Class should be considered as a Use Case Class and Nested Work Units represent target (destination) use cases classes or Packages within ‘uses / includes’ relationships. That way, we can obtain business process driven use cases and their relationships. In summary, ‘contextual objects’ provide the following : • a robust implementation of executable specifications	business process;color gradient;contextual inquiry;executable;goal modeling;orientation (graph theory);process modeling;requirement;state diagram;unified modeling language	Birol Berkem	1998		10.1007/3-540-49255-0_52	business domain;computer science;knowledge management;artifact-centric business process model;goal orientation;process modeling;management science;business process model and notation;business process;object-oriented programming;business process discovery;business rule;business process modeling	DB	-53.321351974208056	20.317754401873195	12349
5b518cf1e3e53e8dd6c90bd47954c7fd4ef4df20	sequent calculi for process verification: hennessy-milner logic for an arbitrary gsos	operational semantics;satisfiability;sequent calculus;process calculus;formal verification;hennessy milner logic;structured operational semantics;process algebra	We argue that, by supporting a mixture of “compositional” and “structural” styles of proof, sequent-based proof systems provide a useful framework for the formal verification of processes. As a worked example, we present a sequent calculus for establishing that processes from a process algebra satisfy assertions in HennessyMilner logic. The main novelty lies in the use of the operational semantics to derive introduction rules, on the left and right of sequents, for the operators of the process calculus. This gives a generic proof system applicable to any process algebra with an operational semantics specified in the GSOS format. Using a general algebraic notion of GSOS model, we prove a completeness theorem for the cut-free fragment of the proof system, thereby establishing the admissibility of the cut rule. Under mild (and necessary) conditions on the process algebra, an ω-completeness result, relative to the “intended” model of closed process terms, follows.	cut rule;formal verification;gs/os;geosynchronous orbit;hennessy–milner logic;interpretation (logic);linear algebra;operational semantics;process calculus;proof calculus;sequent calculus	Alex K. Simpson	2004	J. Log. Algebr. Program.	10.1016/j.jlap.2004.03.004	process calculus;discrete mathematics;cut-elimination theorem;geometry of interaction;computer science;mathematics;proof calculus;noncommutative logic;sequent;programming language;natural deduction;structural proof theory;algorithm	Logic	-13.424017126190106	19.26900192435106	12355
80af10a2c6fde1f06eb56b2b6f3e092ecb8461ec	abstractions for nonblocking supervisory control of extended finite automata	manufacturing systems;supervisory control;abstraction;observers;automata;automata computational modeling observers supervisory control computational complexity doped fiber amplifiers;engineering and technology;computational modeling;teknik och teknologier;computational complexity;industrial robots;industrial control;finite automata;manufacturing systems finite automata industrial control industrial robots;maximally permissive control extended finite automata nonblocking supervisory control abstraction method efa transition projection manufacturing system internal interacting dependencies subsystem synthesis subsystem verification sufficient conditions;doped fiber amplifiers	An abstraction method for Extended Finite Automata (EFAs), i.e., finite automata extended with variables, using transition projection is presented in this work. A manufacturing system modeled by EFAs is abstracted into subsystems that embody internal interacting dependencies. Synthesis and verification of subsystems are achieved through their model abstractions rather than their global model. Sufficient conditions are presented to guarantee that supervisors result in maximally permissive and nonblocking control. An examples demonstrate the computational effectiveness and practical usage of the approach.	automata theory;automaton;computation;computational complexity theory;exploratory factor analysis;finite-state machine;high- and low-level;interaction;semantics (computer science)	Mohammad Reza Shoaei;Lei Feng;Bengt Lennartson	2012	2012 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2012.6386446	control engineering;real-time computing;computer science;distributed computing	Robotics	-35.15633669670666	30.573979092123572	12364
51bf8904682cbee3b59618bf0a18bb960dc3777f	mobile agent based market basket analysis on cloud	databases;location based services;json;mobile cloud computing;association rules;association rule mining;gcm mobile agent based market basket analysis cloud computing location based mobile shopping application bakery product shops three tier architecture front end level android mobile devices middleware level web service java script object notation output json output relational database back end level apache tomcat web server my sql database google cloud messaging;servers;mobile communication;mobile handsets;mobile communication servers association rules databases cloud computing mobile handsets;web services bakeries cloud computing file servers java middleware mobile agents mobile computing relational databases retail data processing;cloud computing;json mobile cloud computing association rule mining location based services	This paper describes the design and development of a location-based mobile shopping application for bakery product shops. Whole application is deployed on cloud. The three-tier architecture consists of, front-end, middle-ware and back-end. The front-end level is a location-based mobile shopping application for android mobile devices, for purchasing bakery products of nearby places. Front-end level also displays association among the purchased products. The middle-ware level provides a web service to generate JSON (Java Script Object Notation) output from the relational database. It exchanges information and data between mobile application and servers in cloud. The back-end level provides the Apache Tomcat Web server and My SQL database. The application also uses the Google Cloud Messaging (GCM) for generating and sending notification of orders to shopkeeper.	affinity analysis;android;apache tomcat;front and back ends;global positioning system;google cloud messaging;json;java;location-based service;mobile agent;mobile app;mobile cloud computing;mobile device;mobile payment;multitier architecture;purchasing;relational database;routing;server (computing);warez;web server;web service	Vijayata Waghmare;Debajyoti Mukhopadhyay	2014	2014 International Conference on Information Technology	10.1109/ICIT.2014.21	mobile search;mobile web;association rule learning;mobile database;computer science;operating system;mobile technology;database;internet privacy;mobile computing;world wide web;computer security;mobile payment	SE	-36.1751209747132	50.06810601801498	12376
1840cac6ca542c9d55a864d322991685dcd66371	integrating software product lines: a study of reuse versus stability	software;object oriented programming;reuse;stability;software reusability;product line integration;software reusability computer games object oriented programming product development;spl design software product line integration large scale reuse time to market independently developed spl programming technique feature code reuse stepwise integration board game domain programming mechanisms aspect oriented programming feature oriented programming product line stability;conferences software;computer games;stability software product lines product line integration reuse;software product lines;conferences;product development	To achieve large-scale reuse and accelerate time-to-market, integration of multiple software product lines (SPLs) is becoming a trend. The integration of independently-developed SPLs enables the derivation of new products on demand in a particular domain. The basic goal is to foster the reuse of previously-implemented features across a family of independently-developed SPLs. The programming technique employed in this context should promote the reuse of feature code across the SPLs with minimum change effort. Otherwise, the stability of the target SPLs would be compromised. This paper presents an exploratory study on the stepwise integration of three product lines from the board game domain. We investigate how the programming mechanisms supported by aspect-oriented and feature-oriented programming impacted the reuse and stability of those product lines. In particular, we also analyse and compare how the use of these mechanisms made possible to reach a better tradeoff of reuse and stability of the SPL designs.	aspect-oriented programming;aspect-oriented software development;feature-oriented programming;software product line;stepwise regression	Alessandro Cavalcante Gurgel;Francisco Dantas;Alessandro F. Garcia;Cláudio Sant'Anna	2012	2012 IEEE 36th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2012.18	domain analysis;real-time computing;stability;computer science;systems engineering;component-based software engineering;software development;feature-oriented domain analysis;software engineering;domain engineering;software construction;reuse;programming language;object-oriented programming;new product development;statistics	SE	-58.38849152839084	27.719594712008007	12379
f9df66f48001a3138da522cbd7d1d63088acde75	evaluating the integrability of the quake-catcher network (qcn)		This paper reviews the Quake-Catcher Network (QCN), a distributed computing seismic network that uses lowcost USB accelerometers to record earthquakes, and discusses the potential to incorporate QCN stations with traditional seismic networks. These very dense urban networks could then be used to create a working earthquake early warning system, as has been shown by our preliminary tests of the QCN in Christchurch, New Zealand. Although we have not yet attempted to add traditional seismometers to the QCN or supplement existing seismic networks with QCN sensors, we suggest that to do so would not be difficult, due to the simple nature of our network.	distributed computing;quake engine;quake-catcher network;sensor;usb	Angela I. Chung;Jesse F. Lawrence;Carl Christensen	2013			remote sensing;seismometer;usb;earthquake warning system;accelerometer;quake (series);computer science	HPC	-30.842597263642357	19.260151743929114	12402
e303039795b20d9173b6fe358de306f0dae0aca8	timed automata semantics for visual e-contracts	formal semantics;software engineering;visual representation;logic in computer science;timed automata;time constraint	C-O Diagrams have been introduced as a means to have a more vis ual representation of electronic contracts, where it is possible to represent the obligation s, permissions and prohibitions of the different signatories, as well as what are the penalties in case of not fulfillment of their obligations and prohibitions. In such diagrams we are also able to represent absolute and relative timing constraints. In this paper we present a formal semantics for C-O Diagrams b ased on timed automata extended with an ordering of states and edges in order to represent dif ferent deontic modalities.	automata theory;deontic logic;diagram;real-time web;semantics (computer science);signature;timed automaton;uppaal	Enrique Martínez;María-Emilia Cambronero;Gregorio Díaz;Gerardo Schneider	2011		10.4204/EPTCS.68.3	discrete mathematics;computer science;theoretical computer science;formal semantics;algorithm	Logic	-34.68800184979369	32.53792992862014	12406
2100d14e855e4c66e845fb4dbddf00849b1be758	a dpll(t) theory solver for a theory of strings and regular expressions		An increasing number of applications in verification and security rely on or could benefit from automatic solvers that can check the satisfiability of constraints over a rich set of data types that includes character strings. Unfortunately, most string solvers today are standalone tools that can reason only about (some fragment) of the theory of strings and regular expressions, sometimes with strong restrictions on the expressiveness of their input language. These solvers are based on reductions to satisfiability problems over other data types, such as bit vectors, or to automata decision problems. We present a set of algebraic techniques for solving constraints over the theory of unbounded strings natively, without reduction to other problems. These techniques can be used to integrate string reasoning into general, multi-theory SMT solvers based on the DPLL(T ) architecture. We have implemented them in our SMT solver CVC4 to expand its already large set of built-in theories to a theory of strings with concatenation, length, and membership in regular languages. Our initial experimental results show that, in addition, over pure string problems, CVC4 is highly competitive with specialized string solvers with a comparable input language.	automata theory;bit array;concatenation;decision problem;linear algebra;newman's lemma;quantifier (logic);rl (complexity);regular expression;regular language;satisfiability modulo theories;simultaneous multithreading;solver	Tianyi Liang;Andrew Reynolds;Cesare Tinelli;Clark W. Barrett;Morgan Deters	2014		10.1007/978-3-319-08867-9_43	theoretical computer science;programming language;algorithm	Logic	-14.140585025045874	24.337325648998146	12429
51483a6e98c45aecb701646d9feb233e26120026	the quick migration of file servers		Upgrading file servers is indispensable for improving the performance, reducing the possibility of failures, and reducing the power consumption. To upgrade file servers, files must be migrated from the old to new servers, which poses three challenges: reducing the downtime during migration, reducing the migration overhead, and supporting the migration between heterogeneous servers. Existing technologies are difficult to achieve all of the three challenges. We propose a quick file migration scheme for heterogeneous servers. To reduce the downtime, we exploit the post-copy approach and introduce on-demand migration that allows file access before completing the migration. To reduce the overhead, we introduce background migration that migrates files as soon as possible without affecting the performance and incurs no overhead after the migration. To support heterogeneity, we introduce stub-based file management that requires no internal states of the old server. We implemented our scheme for Linux and supported the NFS and SMB protocols. The experimental results depict that the downtime was a maximum of 23 s in a 4-level 1000-file directory and the migration time was 70 min in NFS and 204 min in SMB with 242 GiB of data.	downtime;file server;gibibyte;linux;multistage interconnection networks;overhead (computing);server (computing);server message block	Keiichi Matsuzawa;Mitsuo Hayasaka;Takahiro Shinagawa	2018		10.1145/3211890.3211894	file server;computer network;upgrade;downtime;exploit;directory;computer science;server	OS	-16.926291303969364	52.07894119853897	12433
5986dfa558fdefb5d31347e5f8eb189e53b0fe79	risc microprocessors and scientific computing	scientific computing;risc microprocessors;microcomputers;reduced instruction set computing	This paper discusses design features in currently available RISC microprocessors that result in less-than-optimal sustained performance on large-scale scientific calculations. Recommendations for future designs are suggested. The author is with the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames R_earch Center, Moffett Field, CA 94035.	computational science;microprocessor;recommender system;simulation	David H. Bailey	1993		10.1109/SUPERC.1993.1263519	reduced instruction set computing;computer architecture;parallel computing;computer science;operating system;microcomputer	HPC	-7.955307411811479	39.21726602594426	12438
16e42a1027ccd17321fe1a5e5fb4a2655ed4af06	real-time digital signal processing of component-oriented phased array radars	resource scheduling;digital signal processing;processor scheduling;resource allocation;real time;phased array;array signal processing;commercial off the shelf;phased array radar;real time scheduling;resource allocation phased array radar subroutines real time systems radar signal processing array signal processing digital signal processing chips processor scheduling;digital signal processing chips;processing unit number bound real time digital signal processing component oriented phased array radars commercial off the shelf components real time resource scheduling component oriented signal processor task allocation policy real time scheduling algorithm design objectives;phased arrays digital signal processing radar signal processing signal processing real time systems processor scheduling hardware radar tracking spaceborne radar computer science;radar signal processing;subroutines;task allocation;real time systems	With the advance of hardware and software technology, modern phased array radars are now built with commercial-of-the-shelf (COTS) components, and it opens up a new era in real-time resource scheduling of digital signal processing. This paper targets the essential issues in building component-oriented Signal Processor (SP), which is one of the two major modules in modern phased array radars. We propose a simple but effective task allocation policy and a real-time scheduling algorithm to address the design objectives of SP’s. We are able to bound the number of processing units needed for a component-oriented SP in the design time, while everything was done empirically in the past. A series of experiments were done to demonstrate the strength of our methodology.	algorithm;digital signal processing;experiment;phased array;radar;real-time clock;real-time operating system;real-time transcription;scheduling (computing)	Chin-Fu Kuo;Tei-Wei Kuo;Cheng Chang	2000		10.1109/REAL.2000.896000	embedded system;phased array;real-time computing;computer science	Embedded	-8.063661384759525	59.184345393728776	12439
8e7f9896ef6a469699cf10db85c4ae830f02e6c8	initial results for glacial variable analysis	data flow analysis;code generation	Run-time code generation that uses speciic values to generate specialized code is called value-speciic optimization. Variables which provide values for value-speciic optimization are called candidate variables. They are modiied much less frequently than they are referenced. In current systems that use run-time code generation, candidate variables are identiied by programmer directives. We describe a novel technique, staging analysis, for automatically identifying candidate variables. We refer to such variables as glacial variables. Glacial variables are excellent candidate variables. Glacial Variable Analysis is an interprocedural analysis. We perform several experiments with glacial variable analysis to characterize the programs in the PERFECT benchmark suite. We explain the imprecision of our results due to procedure boundaries. We examine the structure of the programs to determine how often value-speciic optimization might be applied. We will explain how staging analysis relates to run-time code generation ; brieey describe Glacial Variable Analysis; and, present some initial results.	benchmark (computing);code generation (compiler);disk staging;experiment;interprocedural optimization;mathematical optimization;programmer;self-modifying code	Tito Autrey;Michael Wolfe	1996		10.1007/BFb0017249	data flow diagram;parallel computing;computer science;variable;artificial intelligence;data-flow analysis;optimizing compiler;programming language;algorithm;code generation	PL	-16.419667998260053	36.27946458965179	12444
11dc6e028e023aeda3cff6ec6a887dcbc5fb78c6	stalnaker conditionals and quantum logic	quantum logic	Since Birkhoff and von Neumann [l] first proposed that the logic appropriate to the elementary propositions of quantum mechanics (QM) is nonclassical, a considerable amount of research has been pursued on the quantum logical foundations of QM and on the mathematical structures associated with quantum logic (QL). Birkhoff and von Neumann singled out the distributive law of classical logic as suspect, replacing it by the weaker modular law. More recently, even the modular law has been abandoned in favor of the still weaker law of orthomodularity (weak modularity or quasimodularity). This move has resulted from the fact that the lattice of subspaces of an infinite dimensional Hilbert space, which provides the concrete model for QL, is orthomodular but not modular. In many respects the algebraic study of QL has been subsumed under the general study of orthomodular lattices and, in some cases, orthomodular partially ordered sets (posets). It is commonplace for mathematicians to appropriate terms from ordinary discourse and confer upon them esoteric technical meanings which have little in common with the ordinary meanings. In most cases, it is useless to question the mathematicians’ usage; for example, it seems quite inappropriate to ask whether the mathematicians’ ‘field’ is ‘really’ a field. Along the same lines, as a simple matter of technical definition, one need not dispute employing the term ‘logic’ to refer to certain lattices and posets. On the other hand, claims concerning these abstract ‘quantum logic? are often intended to have a peculiar relevance to logic, as ordinarily construed, in a way quite unlike the intended relevance of the mathematical theory of fields to cornfields, wheatfields, or electromagnetic fields. Accordingly, it does seem appropriate to ask whether ‘quantum logic’ is really a logic. More specifically, the question is whether it is proper to regard the lattice of QM, or more generally the class of orthomodular lattices (posets), as a logic as opposed to merely a class of algebraic structures only formally analogous to logic properly so called. Doubts along these lines have in particular been	birkhoff interpolation;hilbert space;lattice graph;linear algebra;mathematical structure;quantum logic;quantum mechanics;relevance;residuated lattice	Gary M. Hardegree	1975	J. Philosophical Logic	10.1007/BF00558757	predicate logic;zeroth-order logic;quantum logic;higher-order logic;paraconsistent logic;philosophy;many-valued logic;intermediate logic;predicate functor logic;mathematics;autoepistemic logic	Theory	-12.454126938484377	4.9274353789300545	12453
730acfbdcf846dda7c891ec44be9ae8a8fdaab7b	fault-tolerant real-time analytics with distributed oracle database in-memory	high availability;oltap;distributed in memory fault tolerant analytics real time analytics oltap oracle database in memory distributed architecture high availability;distributed in memory fault tolerant analytics;computer architecture servers distributed databases context real time systems fault tolerance fault tolerant systems;real time analytics;distributed architecture;oracle database in memory	Modern data management systems are required to address new breeds of OLTAP applications. These applications demand real time analytical insights over massive data volumes not only on dedicated data warehouses but also on live mainstream production environments where data gets continuously ingested and modified. Oracle introduced the Database In-memory Option (DBIM) in 2014 as a unique dual row and column format architecture aimed to address the emerging space of mixed OLTAP applications along with traditional OLAP workloads. The architecture allows both the row format and the column format to be maintained simultaneously with strict transactional consistency. While the row format is persisted in underlying storage, the column format is maintained purely in-memory without incurring additional logging overheads in OLTP. Maintenance of columnar data purely in memory creates the need for distributed data management architectures. Performance of analytics incurs severe regressions in single server architectures during server failures as it takes non-trivial time to recover and rebuild terabytes of in-memory columnar format. A distributed and distribution aware architecture therefore becomes necessary to provide real time high availability of the columnar format for glitch-free in-memory analytic query execution across server failures and additions, besides providing scale out of capacity and compute to address real time throughput requirements over large volumes of in-memory data. In this paper, we will present the high availability aspects of the distributed architecture of Oracle DBIM that includes extremely scaled out application transparent column format duplication mechanism, distributed query execution on duplicated in-memory columnar format, and several scenarios of fault tolerant analytic query execution across the in-memory column format at various stages of redistribution of columnar data during cluster topology changes.	column-oriented dbms;distributed computing;fault tolerance;glitch;high availability;in-memory database;java persistence api;online analytical processing;online transaction processing;oracle database;real-time clock;requirement;scalability;server (computing);terabyte;throughput	Niloy Mukherjee;Shasank Chavan;Maria Colgan;Mike Gleeson;Xiaoming He;Allison Holloway;Jesse Kamp;Kartik Kulkarni;Tirthankar Lahiri;Juan Loaiza;Neil MacNaughton;Atrayee Mullick;Sujatha Muthulingam;Vivekanandhan Raja;Raunak Rungta	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498333	real-time computing;computer science;data mining;database;distributed computing;high availability;programming language	DB	-18.447436575091057	54.31668882629094	12458
52f1575b451ac17440f3937a5beb020ab828edfc	extending the scope of syntactic abstraction	types;continuations;module system;separate compilation;bytecode;polymorphic recursion;subroutines;java	The benefits of module systems and lexically scoped syntactic abstraction (macro) facilities are well-established in the literature. This paper presents a system that seamlessly integrates modules and lexically scoped macros. The system is fully static, permits mutually recursive modules, and supports separate compilation. We show that more dynamic module facilities are easily implemented at the source level in the extended language supported by the system.	compiler;metalinguistic abstraction;mutual recursion;scope (computer science)	Oscar Waddell;R. Kent Dybvig	1999		10.1145/292540.292559	real-time computing;computer science;subroutine;continuation;programming language;java;algorithm	PL	-24.753240648085917	27.427925634190895	12460
bee580b22da65d48f7db3985a4d2421c453c049d	brabo: a program for large-scale ab-initio calculations using the pvm-system			parallel virtual machine	C. Van Alsenoy	1995			computational chemistry;theoretical computer science;computer science;ab initio quantum chemistry methods	HCI	-5.225401747384849	37.551594251455406	12464
bb5d7e130b984e809e2188d1f8fda24017bd4192	selecting a security architecture for a new model of distributed tutorial support	groupware;virtual organisations;atomic measurements;information security;authorisation;theoretical modelling;distributed tutorial support;distributed collaborative computing;distributed processing;collaboration;distributed computing;role based access control;packaging;software architecture;feedback;it security;security architecture selection;access control models;virtual organisation;security architecture;authorization;access control;distributed processing authorisation groupware courseware software architecture;security architecture selection distributed tutorial support role based access control theoretical modelling virtual organisations remote collaboration distributed collaborative tutoring mantchi project;mantchi project;courseware;distributed collaboration;tutorial access control collaboration information security distributed computing atomic measurements military computing feedback packaging authorization;remote collaboration;military computing;distributed collaborative tutoring;tutorial	Traditional access control models cannot effectively manage authorization for independent and geographically dispersed information. This drives the research interest in more flexible and efficient access control approaches, in particular role-based access control. This report covers both RBAC subfields - theoretical modelling and practical deployment, provides an introduction and overview of role-based access control appropriate for the emerging world of virtual organisations and remote collaboration. We take a specific application area, distributed collaborative tutoring as pioneered in the MANTCHI project, and show the merits of the RBAC architecture for meeting its security requirements.		Xiaofeng Gong;Julian Newman	2002		10.1109/ENABL.2002.1029994	computer science;knowledge management;information security;operating system;software engineering;role-based access control;database;distributed computing;distributed system security architecture;authorization;world wide web;computer security	Arch	-45.67233665396644	54.322221256790144	12471
1d25445aa5f4213eef95f69392fa42d6a6929d32	a method for semi-automatic creation of ontologies based on texts	knowledge management;semantic web;ontology construction	The recent developments related to knowledge management, the semantic web and the exchange of electronic information through the use of agents have increased the need for ontologies to describe in a formal way shared understanding of a given domain. For computers and people to work in cooperation it is necessary that information have well defined and shared definitions. Ontologies are enablers of that cooperation. However, ontology construction remains a very complex and costly process, which has hindered its use in a wider scale. This article presents a method for the semi-automatic construction of ontologies using texts of any domain for the extraction of concepts and relations. By comparing the relative frequency of terms in the text with their typical, expected use, the method identifies concepts and relations and specifies the corresponding ontology using OWL for use by other applications.	ontology (information science);semiconductor industry	Luiz C. C. Carvalheira;Edson Satoshi Gomi	2007		10.1007/978-3-540-76292-8_18	upper ontology;idef5;ontology components;computer science;knowledge management;ontology;semantic web;data mining;database;ontology-based data integration;web ontology language;owl-s;process ontology	AI	-43.03821632342402	7.234482731889036	12485
b651879272aa54be332136618bb3c06c00ba2805	the traceability information management platform of duck product industry chain		In response to various problems existing in the current duck product industry chain. For example, decentralized of the industry chain, lack of data exchange between each link, lack of unified information collection equipment, and so on. An intelligent information management platform for duck product chain was developed, based on the specific needs of each link of the duck and poultry industrial chain. The platform is mainly composed of the information collection equipment and the intelligent management systems. The information acquisition equipment comprehensively uses the current well-perceived and reliable transmission technology of the Internet of Things to achieve the seamless collection of information in each link of the duck product industry chain and the seamless convergence of information in each link. The intelligent management system utilizes big data analysis technology to realize internal automation and digital management. The long-term test of the system shows that the data in each link of the system seamlessly connects. The data collected by the system is accurate and reliable. Its operation is simple and convenient. The system is highly scalable and suitable for use in production.	information management;traceability	Lining Liu;Pingzeng Liu;Wanming Ren;Yong Zheng;Chao Zhang;Junmei Wang	2018		10.1007/978-3-030-00021-9_14	computer science;computer engineering;automation;traceability;information management;scalability;big data;management system;data exchange;convergence (routing);distributed computing	DB	-61.58057247353382	9.837403474301768	12497
2e5f320ee470306ba900b87e2da5e8a75a1cdcab	business process management enabled compliance-aware medical record sharing	cross organisation data sharing;business process execution;ehrs;electronic health records;regulatory compliance	Data sharing about electronic health records (EHRs) across healthcare organisations is still a challenging task due to compliance requirements with regulatory policies that can vary across states and countries, and organisations’ internal business requirements. Even when adopting the same regulatory policies, each organisation can interpret and implement these policies and requirements differently in its internal IT environments. This paper proposes a compliance-aware data management solution for EHR systems. It allows healthcare organisations to define their own security and regulatory compliance requirements for accessing and sharing healthcare data, and enables policy enforcement while sharing data with other organisations. The policy requirements are expressed in the form of business processes that govern the access and sharing of data between people and systems. The business process operations are mapped into low-level operations on internal and remote record stores and policy enforcement points. We ...	business process	Jovan Stevovic;Jun Li;Hamid R. Motahari Nezhad;Fabio Casati;Giampaolo Armellin	2013	IJBPIM	10.1504/IJBPIM.2013.056961	public relations;business requirements;knowledge management;data mining;database	DB	-50.228398393617454	51.59843565614138	12512
957f34723f068db75460b2611acdef05e3b76d79	structured planning and debugging	human problem;artificial intelligence;structured information processing model;program planning;spade theory;debugging process;logo project;complementary taxonomy;basic planning concept;program design;structured planning	The SPADE theory uses linguistic formalisms to model the program planning and debugging processes. The theory begins with a taxonomy of basic planning concepts covering strategies for identification, decomposition and reformulation. A handle is provided for recognizing interactions between goals and deriving a lincnr solution. A complementary taxonomy of rational bugs and associated repair techniques is also provided. SPA OK. introduces a new data st ructure to facil i tate debugging -the derivation tree of the program. SPADE generalizes recent work in Artificial Intelligence by Suasman and Sacerdoti on automatic programming, and extends The theory of program design developed by the Structured Programming movement. It provides a more structured information processing model of human problem solving than the production systems of Newell and Simon, and articulates the type of problem solving curriculum advocated by Papert's Logo Project. 1. A Multi-Faceted Approach The SPADE theory is being developed in three contexts: 1. Education: an editor called SPADEE-0 has bern implemented that encourages students to define and debug programs in terms of explicit SPADE design choices, thereby providing a highly structured programming environment. 2. A I : an automatic programmer called PATN has been designed using an augmented transition network embodiment of the SPADE theory. This results in a framework which unifies recent work on planning and debugging by Sacerdoti [75] and Sussrnan [75]. 3. Psychology: a parser called PAZATN has been designed that applies the SPADE theory to the analysis of programming protocols. PAZATN produces a parse of the protocol that delineates the planning and debugging strategics employed by the problem solver. PAZATN extends the series of automatic protocol analysers developed at Carnegie-Mellon University [Waterman & Newell 72, 73; Bhaskar & Simon 76]. Hand-simulat ions of PATN and PAZATN on elementary programming problems and informal experiments with the SPADEE-0 editor attest to the theory's cogency in accounting for a wide range of planning and debugging techniques [Goldstein & Mil ler 76a,b; Miller & Goldstein 76b,c,d]. 2. A Linguistic Analogy In developing a representation for problem solving techniques, we have been guided by an analogy to computational linguistics, for three reasons. 1. Thc concepts and algorithms of computational linguistics, though originally intended to explain the nature of language per .se, supply perspicuous yet powerful descriptions of complex compulations in general. 2. Computational linguistics decomposes computations into syntactic, semantic, and pragmatic components. This decomposition clarif ies the explanation of complex processes when viewed in the following manner: syntax formalizes the range of possible decisions; semantics the problem description, and pragmatics the. procedural relationship between the two. 3. Computational linguistics has undergone an evolution of procedural formalisms, beginning with finite state automata, later employing recursive transition networks (context free grammars), next moving on to augmented transition networks, and culminating in the current set of theories involving frames [Minsky 75, Winograd 75, Schank 75]. Each phase captured some properties of language, but was incomplete and required generalisation to more powerful and elaborate formal ism*. Following this evolut ionary sequence illuminates the complexity of language theory. We have pursued a similar evolutionary approach to clarify the complexity of prohlrm solving processes. • To date, our theory of program design has evolved as follows: we first explored context free grammars for planning and debugging, and subsequently their generalisation to ATN's; we then examined the metaphor of protocol analysis as parsing, init ial ly using the planning and debugging grammars to reveal the constituent structure of protocols and later using the derivations produced by the ATN formal ism; and, most recently, we have studied the use of a chart-based parser to discover these analyses. introspection, by examining problem solving protocols [Mi l ler & Goldstein 76b], by studying the l i terature on problem solving [Pglya 57, 65, 68; Newell & Simon 72; Sussman 75; Saecrdoli 75], and by enumerating techniques for finding procedural solutions to problems expressed at predicate calculus formulae [Emden & Kowalski 76] This last criterion demonstrates that the taxonomy is cu r ren t l y incomplete -for example, techniques for handl ing disjunctions have not yet been analysed thoroughly enough to warrant inclusion. However, the taxonomy is adequate for a wide range of elementary programming problems. There are three major classes of plans in the taxonomy: identification, decomposition, and reformulation. Identification means recognizing a problem at previously solved. Decomposition refers to strategics for d iv id ing a problem into simpler sub-problems. Reformulat ion plans alter the problem descript ion, seeking a representat ion which is more amenable to iden t i f i ca t ion or decomposition. The figure indicates how these classes of plans are further subdivided in the SPADE theory. Planning, according to the theory, is a process in which the problem solver sclccis the appropriate plan type, and then carries nut the subgoals defined by that plan applied to the current problem. From this viewpoint, the planning taxonomy represents a decision tree of alternative plans. The decision process can be modeled by the context free grammar given below. The grammar explicitly stairs which planning rules involve recursive application of solution techniques to subgoals: setup, interface, mainstep, cleanup, and Spec ia l i zed Systems-1: 773 M i l l e r	artificial intelligence;augmented transition network;automata theory;automatic programming;computation;computational linguistics;context-free grammar;coppersmith–winograd algorithm;debugging;decision tree;decomposition (computer science);elementary;experiment;faceted classification;finite-state machine;first-order logic;information processing;init;integrated development environment;interaction;introspection;iterative and incremental development;logo;parse tree;parsing;problem solving;programmer;recursion;smith–waterman algorithm;software bug;solver;spec#;structured programming;taxonomy (general)	Mark L. Miller;Ira P. Goldstein	1977			computer science;artificial intelligence;machine learning;mathematics;programming language;algorithm	AI	-21.993097208443398	14.804823522213956	12516
1856facbd10646198db9b5740c872342b7ccd8d1	using intelligent proxies to develop self-adaptive service-based systems	reliability;standards;software maintenance;telemedicine;scattering;web services bioinformatics health care software maintenance software reliability telemedicine;scattering web services standards reliability probabilistic logic unified modeling language java;unified modeling language;web services;probabilistic logic;software reliability;bioinformatics intelligent proxies self adaptive service based systems sbs telehealth case study compliance maintenance reliability requirements web service proxy internet e commerce;java;bioinformatics;health care	We present the theory underpinning the operation of a new tool-supported approach to engineering self-adaptive service-based systems (SBSs), and preliminary results from its evaluation in a telehealth case study. SBSs developed using our approach select their services dynamically, in order to maintain compliance with reliability requirements in the presence of changes in service behaviour. This adaptation is enabled by a new type of web service proxy called an intelligent proxy.	proxy server;requirement;requirements analysis;web service	Radu Calinescu;Yasmin Rafiq	2013	2013 International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2013.41	web service;unified modeling language;computer science;software engineering;reliability;database;probabilistic logic;scattering;programming language;software maintenance;java;world wide web;software quality;health care	SE	-47.49797618056541	40.395179929186	12542
287287d29f6d8ed8477e230b754a8d76cd042bfe	svt: schema validation tool for microsoft sql-server	schema validation tool;commercial relational;database schema;microsoft sql-server;desirable property;sql server;satisfiability	We present SVT, a tool for validating database schemas in SQL Server. This is done by means of testing desirable properties that a database schema should satisfy. To our knowledge, no commercial relational DBMS provides yet a tool able to perform such kind of validation.	database schema;microsoft sql server;relational database management system	Ernest Teniente;Carles Farré;Toni Urpí;Carlos Beltrán;David Gañán	2004			schema migration;computer science;data mining;database;programming language;database schema;satisfiability	DB	-33.3861158401119	9.891765671585095	12564
7825f89e7e843fe8c67e92a424cac4e84099aad1	dynamic analysis of multi-threaded embedded software to expose atomicity violations		Concurrency bugs are one of the most notorious software bugs and may not be observed easily. Significant work has been done on detection of atomicity violations bugs for high performance systems but there is not much work related to detect these bugs for embedded systems. Although criteria to claim existence of bugs remains same, approach changes a bit for embedded systems. The main focus of this research is to develop a systemic methodology to address the issue from embedded systems perspective. A framework is developed which predicts the access interleaving patterns that may violate atomicity using memory references of shared variables and provides support to force and analyze these patterns for any output change, system fault, or change in execution path.	atomicity (database systems);embedded software;embedded system;forward error correction;pointer (computer programming);program counter;race condition;sensor;shared variables;software bug	Jay Patel;Yann-Hang Lee	2016	2016 13th International Conference on Embedded Software and Systems (ICESS)	10.1109/ICESS.2016.30	real-time computing;software bug;atomicity;multithreading;interleaving;concurrency;computer science;embedded software	Embedded	-21.98667735529014	38.883051364251	12567
1055b84da5e88bbc8a426da9a5795f29b8b48cda	bogor: a flexible framework for creating software model checkers	object oriented design modeling support;quality assurance;software testing;object oriented modeling language bogor software model checking framework software verification software debugging multicore architecture distributed computing software quality assurance technique software testing software inspection software system requirements software model checking tool domain specific model checking engine creation object oriented design modeling support;domain specific modeling;software quality assurance;object oriented design;software verification;domain specific model checking engine creation;bogor software model checking framework;distributed computing;software systems;object oriented programming;program verification;modeling language;software system requirements;software model checking tool;model checking;program testing;software development;software quality object oriented programming program debugging program testing program verification quality assurance;software debugging;program debugging;multicore architecture;object oriented modeling context modeling engines hardware software debugging computer architecture distributed computing software quality software testing inspection;software quality assurance technique;object oriented modeling language;software inspection;software quality	Model checking has proven to be an effective technology for verification and debugging in hardware and more recently in software domains. With the proliferation of multi-core architectures and a greater emphasis on distributed computing, model checking is an increasingly important software quality assurance technique that can complement existing testing and inspection methods. We believe that recent trends in both the requirements for software systems and the processes by which systems are developed suggests that domain-specific model checking engines may be more effective than general purpose model checking tools. To overcome limitations of existing tools which tend to be monolithic and non-extensible, we have developed an extensible and customizable model checking framework called Bogor. In this article, we summarize how Bogor provides direct support for modeling object-oriented designs and implementations, how its modeling language and algorithms can be extended and customized to create domain-specific model checking engines, and how Bogor can be deployed in broader software development contexts in conjunction with complementary quality assurance techniques	algorithm;debugging;distributed computing;domain-specific language;model checking;modeling language;multi-core processor;requirement;software development;software quality assurance;software system	Robby;Matthew B. Dwyer;John Hatcliff	2006	Testing: Academic & Industrial Conference - Practice And Research Techniques (TAIC PART'06)	10.1109/TAIC-PART.2006.5	computer architecture;verification and validation;computer science;programming language;computer engineering	SE	-52.598624494953455	31.148723414563303	12579
dadc090afce364452641c228ae95991ec654b327	enhancing workflow-nets with data for trace completion		The growing adoption of IT-systems for modeling and executing (business) processes or services has thrust the scientific investigation towards techniques and tools which support more complex forms of process analysis. Many of them, such as conformance checking, process alignment, mining and enhancement, rely on complete observation of past (tracked and logged) executions. In many real cases, however, the lack of human or IT-support on all the steps of process execution, as well as information hiding and abstraction of model and data, result in incomplete log information of both data and activities. This paper tackles the issue of automatically repairing traces with missing information by notably considering not only activities but also data manipulated by them. Our technique recasts such a problem in a reachability problem and provides an encoding in an action language which allows to virtually use any state-of-the-art planning to return solutions.	action language;conformance testing;data mining;reachability problem;thrust;tracing (software)	Riccardo De Masellis;Chiara Di Francescomarino;Chiara Ghidini;Sergio Tessaris	2017		10.1007/978-3-319-74030-0_6	information hiding;conformance checking;machine learning;software engineering;artificial intelligence;encoding (memory);computer science;abstraction;workflow;reachability problem;action language	SE	-60.01690972278375	45.63408009200752	12583
352524c353c3c5af5364a113be08a741d14de342	on the need for data flow graph visualization of forensic lucid programs and encoded evidence, and their evaluation by gipsy	formal specification;forensic lucid;forensic computing forensic lucid dfg gipsy;computer forensics;forensics visualization three dimensional displays semantics unified modeling language computational modeling;formal backing data flow graph visualization forensic lucid programs encoded evidence gipsy data flow programs lucid dialect reason about cyberforensic cases evidence encoding crime scene modeling event reconstruction digital evidence lucid dfg programming forensic lucid case modeling visual programming dfg modeling;data flow graphs;computer model;semantics;forensic computing;three dimensional;visual programming;data flow graph;data visualisation;visualization;computational modeling;reasoning about programs;forensic science;design and implementation;dfg;specification languages;three dimensional displays;unified modeling language;digital evidence;gipsy;data flow;forensics;visual programming computer forensics data flow graphs data visualisation forensic science formal specification reasoning about programs specification languages	Lucid programs are data-flow programs and can be visually represented as data flow graphs (DFGs) and composed visually. Forensic Lucid, a Lucid dialect, is a language to specify and reason about cyberforensic cases. It includes the encoding of the evidence (representing the context of evaluation) and the crime scene modeling in order to validate claims against the model and perform event reconstruction, potentially within large swaths of digital evidence. To aid investigators to model the scene and evaluate it, instead of typing a Forensic Lucid program, we propose to expand the design and implementation of the Lucid DFG programming onto Forensic Lucid case modeling and specification to enhance the usability of the language and the system. We briefly discuss the related work on visual programming and DFG modeling in an attempt to define and select one approach or a composition of approaches for Forensic Lucid based on various criteria such as previous implementation, wide use, formal backing in terms of semantics and translation. In the end, we solicit the readers' constructive, opinions, feedback, comments, and recommendations within the context of this short discussion.	comment (computer programming);dataflow;feedback;graph drawing;lucid;usability;visual programming language	Serguei A. Mokhov;Joey Paquet;Mourad Debbabi	2011	2011 Ninth Annual International Conference on Privacy, Security and Trust	10.1109/PST.2011.5971973	unified modeling language;three-dimensional space;data flow diagram;visualization;computer science;theoretical computer science;data-flow analysis;formal specification;database;visual programming language;programming language;forensic science;computational model	SE	-47.15593108868798	31.243786292254047	12584
e4c7ac66233560c0a0f46cf8e32746b2347cfd57	discussion on development of chinese automobile information technology based on intelligent materials	automobile;development and discussion;information technology	Current, with the development and progress of automobile-technology, Automobile informationised-technology which is made up of NET and E-BUSIESS, is making great progress; It brings about challenges and chances to automobile manufacturers. Hence, it is necessary to solve the problem that automobile manufacturers how to face up to automobile informationised-time. The article discusses the problem and brings up some measures. © 2011 Springer-Verlag Berlin Heidelberg.		Yali Chi;Yong Wu	2011		10.1007/978-3-642-23345-6_80	control engineering;electronic engineering;engineering drawing	Robotics	-61.40087170797311	5.949452185026707	12590
437a0d43b344e90c6f12b666af90be14a2d34ad6	the robotics api: an object-oriented framework for modeling industrial robotics applications	application development;domain model;welding service robots object oriented modeling real time systems robot control programming;object oriented design;robotics api;welding;service robots;object oriented framework;application program interface;object oriented programming;software engineering;software engineering application program interfaces industrial robots object oriented programming robot programming;robot control;imperative robot programming languages;object oriented;industrial robots;application program interfaces;software development;industrial robotics;object oriented analysis and design;application programming interface robotics api object oriented framework industrial robotics software development object oriented analysis object oriented design software models imperative robot programming languages;object oriented analysis;programming;object oriented modeling;robot programming;application programming interface;software models;real time systems	During the last two decades, software development has evolved continuously into an engineering discipline with systematic use of methods and tools to model and implement software. For example, object-oriented analysis and design is structuring software models according to real-life objects of the problem domain and their relations. However, the industrial robotics domain is still dominated by old-style, imperative robot programming languages, making software development difficult and expensive. For this reason, we introduce the object-oriented Robotics Application Programming Interface (Robotics API) for developing software for industrial robotic applications. The Robotics API offers an abstract, extensible domain model and provides common functionality, which can be easily used by application developers. The advantages of the Robotics API are illustrated with an application example.	application programming interface;domain model;imperative programming;industrial robot;problem domain;programming language;real life;robotics;software development	Andreas Angerer;Alwin Hoffmann;Andreas Schierl;Michael Vistein;Wolfgang Reif	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5649098	domain analysis;object-oriented analysis and design;simulation;application programming interface;computer science;software development;software engineering;robotic paradigms;adaptable robotics;geography of robotics;programming language;object-oriented programming;future of robotics	Robotics	-50.70239981853008	30.064532482794473	12591
67230f777bb600f92761d30c9c1f9688b8055439	self-supervising bpel processes	web services business process re engineering program verification service oriented architecture;software engineering tools;inf;software tool;design tool;business process execution language;assertion checkers;application software;logic;performance;stakeholder;business logic;self supervising bpel process;program verification;runtime;software engineering;software performance;distributed internet based software engineering tools and techniques;business process execution language self supervising bpel process quality assessment stakeholder business logic supervision aware runtime framework industrial partner;assertion languages;quality assessment;monitoring;software program verification;web services;runtime monitoring;industrial partner;robustness;distributed internet based software engineering tools and techniques software engineering software program verification assertion checkers assertion languages performance design tools and techniques;software tools;quality of service;business process re engineering;runtime monitoring robustness software engineering application software quality assessment quality of service logic software performance software tools;service oriented architecture;design tools and techniques;software quality;supervision aware runtime framework	Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.	business process execution language;business domain;business logic;coherence (physics);cross-cutting concern;data recovery;dependability;experiment;feedback;prototype;quality of service;robustness (computer science);run time (program lifecycle phase);runtime verification	Luciano Baresi;Sam Guinea	2011	IEEE Transactions on Software Engineering	10.1109/TSE.2010.37	reliability engineering;application software;business logic;business process execution language;stakeholder;quality of service;performance;computer science;systems engineering;operating system;software engineering;programming language;logic;software quality;robustness	SE	-55.647407671523105	26.792784118426898	12592
31cbb88817be4ce0c17ad19bbd9690c11eea0522	a complete logical system for the equality of recursive terms for sets	recursive term;recursion inference rule;complete logical system;main axiom;finite transitive closure;extensionality axiom;logical system;additional axiom;finite graph;sequent-style deduction system;proof system	This paper presents a sound and complete logical system whose atomic sentences are the equalities of recursive terms involving sets. There are two interpretations of this language: one makes use of non-wellfounded sets with finite transitive closure, and the other uses pointed finite graphs modulo bisimulation. Our logical system is a sequent-style deduction system. The main axioms and inference rules come from the       $\mbox{{\it FLR}$_0$}$    -proof system from [6], including the Recursion Inference Rule (but an additional axiom is needed), and also axioms corresponding to the extensionality axiom of set theory.	formal system;recursion	Lawrence S. Moss;Erik Wennstrom;Glen T. Whitney	2012		10.1007/978-3-642-29485-3_12	axiom of extensionality;discrete mathematics;mathematics;algorithm	Logic	-12.171996505571087	15.15177768218545	12606
3dd71ee5155093ca1dcc7f1af73aa31f90d52756	distributed algorithms for load balancing in very large homogeneous systems	load balancing;large homogeneous system;fixed-threshold level;correct operation;central controller;graph theory;large system;proposed algorithm;instantaneous change;fair service;effective pairing;global balance	In order to improve the performance of a system it is necessary to balance the loads among the processors in the system. We propose distributed load balancing algorithms for very large systems with possibly thousands of processors. The proposed algorithms are truly distributed, i.e., there does not exist any central controller to provide control, coordination, or mediation among the processors. Processors, with the use of local knowledge and interaction with their neighbors, maintain global balance in the system. The way processors are paired to exchange their loads will have an impact on the performance of the system. We have shown that an effective pairing of processors is possible with the use of the concepts of edge-coloring and node-coloring from graph theory. The proposed algorithms are truly dynamic and adaptable to instantaneous changes in the system. Their correct operation does not depend on any fixed-threshold level. They provide a fair service to every job regardless of its source of delivery to the system.		Seyed Hossein Hosseini;Bruce E. Litow;M. I. Malkawi;K. Vairavan	1987			parallel computing;real-time computing;computer science;distributed computing	Theory	-18.509934855019146	59.35456335096553	12611
6bedd5bc986e7a12d9b805e6af93be613d74be7c	high-level abstractions for efficient concurrent systems	difference operator;programming language;building block;concurrent systems;control structure;operating system;load balance;system architecture;high performance	ion that permit significant programmer-level control over mapping, scheduling and load-balancing of lightweight threads. Section 7 describes abstract physical machines, and Sting’s exception handling mechanism. Section 8 provides benchmark results. Conclusions and comparison to related work is given in Section 9.	abstract machine;address space;admissible numbering;benchmark (computing);central processing unit;concurrency (computer science);continuation;data structure;exception handling;first-class function;high- and low-level;high-level programming language;interrupt;light-weight process;load balancing (computing);memory management;message passing;operating system;programmer;sting;scheduling (computing);scheme;thread (computing);user interface;user space;virtual machine	Suresh Jagannathan;James Philbin	1994		10.1007/3-540-57840-4_31	real-time computing;computer science;theoretical computer science;distributed computing	OS	-25.873735160618516	36.57409297426164	12616
5dae3d108cf1fe6c27f6a7f228d7e37491e432ec	formal concepts for an integrated internal model of the uml	software systems;formal semantics;modelling language;abstract syntax;natural language;unified modeling language;internal model	Abstract   In the Unified Modeling Language (UML) different views of software systems are specified by different models. The abstract syntax of the modelling languages is defined precisely in the UML standard, but the (dynamic) semantics up to now are only sketched in natural language descriptions. Moreover, the correspondences between the different models are not described precisely. In this paper I present an abstract semantic domain that has been defined independently of the UML and can be used to provide formal semantics for the different modelling languages. Since one common domain is employed also the integration of the different viewpoint models is supported by this approach.	unified modeling language	Martin Große-Rhode	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80945-9	natural language processing;unified modeling language;abstract syntax;internal model;formal semantics;object language;systems modeling language;uml tool;computer science;applications of uml;class diagram;formal semantics;natural language;programming language;node;object constraint language;software system	Theory	-45.196417484150686	26.70545233710908	12617
6446f4d1ac04256d5bcf182501bc29286195f907	a distributed persistent object store for scalable service	estensibilidad;distributed system;eficacia sistema;interfase usuario;tsinghua object data store;tratamiento transaccion;architecture systeme;systeme reparti;fichier;availability;user interface;disponibilidad;data management;performance systeme;simultaneidad informatica;stockage donnee;fichero;system performance;data storage;concurrency;distributed objects;sistema repartido;file;object oriented;file system;digital signature;persistent object store;almacenamiento datos;oriente objet;arquitectura sistema;interface utilisateur;extensibilite;scalability;mobile agent;transaction processing;high throughput;system architecture;threshold signature;security;simultaneite informatique;orientado objeto;disponibilite;traitement transaction	This paper presents a distributed persistent object store designed to simplify scalable service in cluster environment. This distributed object store, called TODS (Tsinghua Object Data Store), presents a single-imaged, transparent persistent and object-oriented view of the storage devices of the whole cluster. TODS is designed to be incremental scalable and efficient, and also has the properties of the high concurrency, high throughput and availability which are necessary for scalable service. TODS supports distributed ACID transactions within the cluster, which qualifies its use in the building of complex transactional services. And the user interface of TODS is fitter for building service than that of file system, and significantly easier to use than that of RDBMS. TODS is a reusable platform for scalable service in cluster by forming many general data management functions into one independent layer. This paper gives the motivation, principle and architecture of TODS. Some technique details are also discussed. In our performance experiments, the system scales smoothly to a 36-node server cluster and achieves 11,160 In-memory reads/sec and 396 transactions/sec.	acid;acm transactions on database systems;archive;autostereogram;computer cluster;concurrency (computer science);data store;distributed object;experiment;garbage collection (computer science);input/output;modem;object storage;peer-to-peer;persistence (computer science);persistent object store;portable document format;relational database management system;requirement;scalability;schema evolution;server (computing);smoothing;throughput;user interface	Chao Jin;Weimin Zheng;Feng Zhou;Yinghui Wu	2002	Operating Systems Review	10.1145/583800.583805	high-throughput screening;availability;digital signature;real-time computing;scalability;concurrency;computer file;transaction processing;data management;computer science;operating system;computer data storage;mobile agent;database;computer performance;object-oriented programming;user interface;computer security;systems architecture	OS	-29.027032528905266	43.48856101640514	12627
78fe625143918cbed3ac6c7dbf40f4a16a4b87f4	a hybrid aggregation and compression technique for road network databases	evaluation performance;informatique mobile;distributed database;systeme information geographique;performance evaluation;personal digital assistant;geographic information system;client server architecture;data compression;query processing;road network;architecture client serveur;evaluacion prestacion;interrogation base donnee;base repartida dato;interrogacion base datos;multi resolution compression vector data;mobile computer;analyse multiresolution;system performance;aggregation;assistant numerique personnel;spatial database;road networks;red carretera;gis;base de donnees repartie;multi resolution compression;relacion compresion;spatial databases;traitement de la requete;reseau routier;arquitectura cliente servidor;compression ratio;base dato especial;vector data;tratamiento pregunta;compresion dato;taux compression;base de donnees spatiale;aggregation road networks;multi resolution;mobile computing;auxiliar personal digital;multiresolution analysis;database query;sistema informacion geografica;analisis multiresolucion;compression donnee	Vector data and in particular road networks are being queried, hosted and processed in many application domains such as in mobile computing. Many client systems such as PDAs would prefer to receive the query results in unrasterized format without introducing an overhead on overall system performance and result size. While several general vector data compression schemes have been studied by different communities, we propose a novel approach in vector data compression which is easily integrated within a geospatial query processing system. It uses line aggregation to reduce the number of relevant tuples and Huffman compression to achieve a multi-resolution compressed representation of a road network database. Our experiments performed on an end-to-end prototype verify that our approach exhibits fast query processing on both client and server sides as well as high compression ratio.	algorithm;application domain;client-side;cultural objects name authority;data compression;database;end-to-end principle;experiment;huffman coding;mobile computing;network model;overhead (computing);personal digital assistant;polygon (computer graphics);principle of abstraction;prototype;raster graphics;requirement;response time (technology);server (computing);server-side;spatial query;symbolic computation;throughput;time server	Ali Khoshgozaran;Ali Khodaei;Mehdi Sharifzadeh;Cyrus Shahabi	2008	Knowledge and Information Systems	10.1007/s10115-008-0132-8	data compression;multiresolution analysis;simulation;telecommunications;computer science;compression ratio;database;lossless compression;mobile computing;spatial database;client–server model	DB	-29.55154570173426	16.729880287356657	12629
4a00d8bc4a989834aa314674e02e0c87d071fee3	generalized answer set planning with incomplete information		Answer Set Planning was one of the first challenging applications of Answer Set Programming (ASP). However, when putting plans into practice, their execution and monitoring lead to an increased complexity. Foremost, this is due to the inherent incompleteness of information faced in real-world scenarios. This fundamental problem has already been addressed in various ways, as in conformant, assertional, and assumption-based planning, and often combined with additional sensing actions. Our objective is to conceive a framework for planning and execution under incomplete information by combining the key concepts of the aforementioned approaches in a uniform ASP-based framework. This paper reports on our first step in this direction. More precisely, we define a series of increasingly complex ASP problems reflecting the successive integration of the key concepts. Our prime concern lies in obtaining a uniform characterization that matches the expressive realm of ASP. This is reflected by developing appropriate approximations that are implemented by means of metaencoding techniques.	answer set programming;approximation;automated planning and scheduling;complexity;foremost;stable model semantics	Javier Romero;Torsten Schaub;Tran Cao Son	2017			complete information;data mining;computer science	AI	-20.275930924223125	7.860129039878855	12639
d8bce0ac8dd8970abed54d446f0c37e4de49b6a8	logical spaces in multi-agent only knowing systems	multiagent system;multi agent system;intelligence artificielle;logical programming;programmation logique;artificial intelligence;inteligencia artificial;sistema multiagente;programacion logica;systeme multiagent;no reference	We present a weak multi-agent system of Only knowing and an analysis of the logical spaces that can be defined in it. The logic complements the approach to generalizing Levesque‘s All I Know system made by Halpern and Lakemeyer. A novel feature of our approach is that the logic is defined entirely at the object level with no reference to meta-concepts in the definition of the axiom system. We show that the logic of Halpern and Lakemeyer can be encoded in our system in the form of a particular logical space.	analysis of algorithms;autoepistemic logic;axiomatic system;cobham's thesis;complexity;computation;default logic;hector levesque;international conference on logic programming;international joint conference on artificial intelligence;lecture notes in computer science;modal logic;multi-agent system;multimodal interaction;non-monotonic logic;rationality;spaces;springer (tank);tree-meta;theory	Bjørnar Solhaug;Arild Waaler	2005		10.1007/11750734_5	non-classical logic;artificial intelligence;mathematics;algorithm	AI	-17.05137469197535	10.63537951483122	12643
9b71152bf665aed372eccf884c116d8a9ab1eb9b	opportunities and challenges for mobile crowdsourcing - conceptualisation of a platform architecture	mobile crowdsourcing platform;crowdsourcing platforms;context aware mobile crowdsourcing;0805 distributed computing;college of business;platform architecture;mobile communication;hetnets;heterogeneous networks	The emergence of a broad variety of effective approaches under the umbrella term 'crowdsourcing', which involves the engagement of potentially large and open crowds of participants for the undertaking of a task, has become a phenomenon that businesses have increasingly taken interest in lately. Given the significant increase in mobile phone penetration and the expansion of existing mobile network capacity, e.g., based on heterogeneous networks (HetNets), new forms of crowdsourcing can be conceived that leverage the mobile and ubiquitous user base. This paper aims to explore some of the opportunities and challenges of new approaches to mobile and ubiquitous crowdsourcing solutions. In addition, we present the conceptual architecture of a context-aware mobile crowdsourcing platform that brings together 'crowdsourcers' as service requesters and 'mobile workers' as service providers.	crowdsourcing	Axel Korthaus;Wei Dai	2015	IJHPCN	10.1504/IJHPCN.2015.066537	mobile search;simulation;mobile web;heterogeneous network;mobile telephony;crowdsourcing software development;computer science;operating system;internet privacy;mobile computing;world wide web	Mobile	-40.17302374748469	52.70284985873127	12646
4d019695304cf2e25e4d94d316fcbc20befd6af8	σ11-completeness of a fragment of the theory of trees with subtree relation	labeled tree;first order;binary relation	We consider the structure ITS of all labeled trees, called also infinite terms, in the first order language L with function symbols in a recursive signature S of cardinality at least two and at least a symbol of arity two, with equality and a binary relation symbol which is interpreted to be the subtree relation. The existential theory over L of this structure is decidable (see Tulipani [9]), but more complex fragments of the theory are undecidable. We prove that the ∃ theory of the structure is in 1, where ∃ formulas are those in prenex form consisting of a string of unbounded existential quantifiers followed by a string of arbitrary quantifiers all bounded with respect to . Since the fragment of the theory was already known to be 1-hard (see Marongiu and Tulipani [5]), it is now established to be 1-complete. 1 Preliminaries and Introduction A signature S is a set of operation symbols on which is defined a function ar : S → IN into the set of natural numbers, called arity. Symbols of arity zero are called constant symbols. Throughout this paper we assume at least that S is a nonempty recursive set. For every nonempty set A let A∗ denote the free monoid of finite sequences of elements of A, including the empty sequence . Let · be the operation of concatenation on A∗. A set D ⊆ A∗ is called prefix-closed if p · q ∈ D implies p ∈ D. A set D is called a domain-tree if: (1) D ⊆ IN∗+ and ∈ D, where IN+ is the set of positive integers; (2) D is prefix closed. When D is a domain-tree, the elements of D are called positions. Moreover, a mapping t : D → S is called a tree (or infinite term) over the signature S if (3) ∀p ∈ D, if t(p) = g and ar(g) = k then ∀ j ∈ IN+, p · j ∈ D ←→ 1 ≤ j ≤ k. Received February 1, 1994; revised November 16, 1994	active directory;concatenation;conditional (computer programming);first-order logic;free monoid;prenex normal form;recursion;recursive set;string operations;symbol (formal);tree (data structure);undecidable problem	Patrizio Cintioli;Sauro Tulipani	1994	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1040511348	random binary tree;combinatorics;discrete mathematics;topology;first-order logic;binary relation;mathematics;algorithm	Theory	-6.346714790704384	15.316184598783984	12651
38eeb6d4edbd0cc0fb88ebdb507dcbac36f00289	side-effects in automatic file updating	programa;systeme unix;mise a jour;program;maintenance;automatic system;on line;unix system;en linea;off line;gestion fichier;file management;algorithme;algorithm;algorritmo;sistema automatico;side effect;manejo archivos;systeme automatique;programme;mantenimiento;en ligne;puesta al dia;updating;hors ligne			Webb Miller;Eugene W. Myers	1986	Softw., Pract. Exper.		embedded system;computer file;computer science;operating system;journaling file system;programming language;design rule for camera file system;side effect	SE	-20.058243818191595	42.579066120809074	12652
658c9381d81566c8d0a763d57a43e4814ef9760e	semantic iot middleware-enabled mobile complex event processing for integrated pest management		Agricultural domain presents challenges typical of the Cyber-Physical Systems field and the hard-core of information technology industry of the new generation, such as Cloud computing and Internet of Things. In fact, modern agricultural management strongly relies on many different sensing methodologies to provide accurate information about crop, climate, and environmental conditions. In this paper we propose an approach to model a mobile-driven and distributed Complex Event Processing solution which is enabled by an IoT middleware. The proposed framework is robust with reference to contextual and environmental changes also thanks to the exploitation of an ontological model.	angular defect;cloud computing;complex event processing;cyber-physical system;ecosystem;event-driven programming;experiment;internet of things;middleware;mobile app;mobile device;ontology (information science);ontology engineering;screenshot;server (computing);server-side;universal instantiation;web ontology language	Francesco Nocera;Tommaso Di Noia;Marina Mongiello;Eugenio Di Sciascio	2017			real-time computing;data mining;computer security	DB	-42.591036481809255	47.13920115050108	12656
4f8ee6d4f96b14687cba3ff488493c48759c9adb	write-aware replacement policies for pcm-based systems	programacion de ordenadores	The gap between processor and memory speeds is one of the greatest challenges that current designers face in order to develop more powerful computer systems. In addition, the scalability of the Dynamic Random Access Memory (DRAM) technology is very limited nowadays, leading to consider new memory technologies as candidates for the replacement of conventional DRAM. Phase-Change Memory (PCM) is currently postulated as the prime contender due to its higher scalability and lower leakage. However, compared to DRAM, PCM also exhibits some drawbacks, like lower endurance or higher dynamic energy consumption and write latency, that need to be mitigated before it can be used as the main memory technology for the next computers generation. This work addresses the PCM endurance constraint. For this purpose, we present an analysis of conventional cache replacement policies in terms of the amount of writebacks to main memory they imply and we also propose some new replacement algorithms for the last level cache (LLC) with the goal of cutting the write traffic to memory and consequently to increase PCM lifetime without degrading system performance. In this paper we target general purpose processors provided with this kind of non-volatile main memory and we exhaustively evaluate our proposed policies in both single and multi-core environments. Experimental results show that on average, compared to a conventional Least Recently Used (LRU) algorithm, some of our proposals manage to reduce the amount of writes to main memory up to 20-30% depending on the scenario evaluated, which leads to memory endurance extensions up to 20-45%, reducing also the energy consumption in the memory hierarchy up to 9% and hardly degrading performance.	algorithm;cpu cache;cache (computing);central processing unit;computer data storage;dynamic random-access memory;memory hierarchy;multi-core processor;non-volatile memory;phase-change memory;random access;scalability;spectral leakage	Roberto Rodríguez-Rodríguez;Fernando Castro;Daniel Chaver;Rekai González-Alberquilla;Luis Piñuel;Francisco Tirado	2015	Comput. J.	10.1093/comjnl/bxu104	uniform memory access;interleaved memory;parallel computing;real-time computing;distributed memory;computer hardware;computer science;operating system;database;flat memory model;cache pollution;algorithm;memory management	Arch	-9.685053464629659	54.30053999003996	12659
3b0f6cc6d4d6965848642eda03e1a447c8e039e4	cloud logic programming for integrating language technology resources		The main goal of the CLT Cloud project is to equip lexica, morphological processors, parsers and other software components developed within CLT (Centre of Language Technology) with so called web API:s, thus making them available on the Internet in the form of web services. We present a proof-of-concept implementation of the CLT Cloud server where we use the logic programming language Prolog for composing and aggregating existing web services into new web services in a way that encourages creative exploration and rapid prototyping of LT applications.	application programming interface;central processing unit;component-based software engineering;language technology;lexicon;logic programming;parsing;programming language;prolog;rapid prototyping;server (computing);virtual private server;web api;web service	Markus Forsberg;Torbjörn Lager	2012			first-generation programming language;declarative programming;reactive programming;computer science;programming language implementation;theoretical computer science;functional logic programming;database;programming paradigm;symbolic programming;inductive programming;datalog;fifth-generation programming language;programming language;high-level programming language	PL	-31.384997701535543	26.944492102041778	12669
fe845bf7be4c79ffc9350a735c7f9f745f8aba23	the methodology for finding suitable ontology matching approaches	matcher characteristic;moma framework;metadata based ontology matching framework;ontology matching	Interoperability has gained in importance and become an essential issue within the Semantic Web community. The more standardized and widespread the data manipulation tools are, the easier and more attractive using the Semantic Web approach has become. Though Semantic Web technologies can support the unambiguous identification of concepts and formally describe relationships between concepts, thereby allowing the representation of data in a more meaningful and more machine-understandable way, Web developers are still faced with the problem of semantic interoperability, which stands in the way of achieving the Web’s full potential. To attain semantic interoperability, systems must be capable of exchanging data in such a way that the precise meaning of the data is readily accessible, and the data itself can be translated by any system into a form that it understands. Hence, a central problem of interoperability and data integration issues in Semantic Web vision is schema or ontology matching and mapping. Considering this situation in Semantic Web research, we wish to contribute to the enhancement of (semantic) interoperability by contributing to the ontology matching solution. The number of use cases for ontology matching justifies the great importance of this topic in the Semantic Web. Furthermore, the development and existence of tried and tested ontology matching algorithms and support tools will be one of the crucial issues that may have a significant impact on future development. Therefore, we have developed a Metadata-based Ontology Matching (MOMA) Framework that addresses data integration and the interoperability issue by creating and maintaining awareness of the link between matching algorithms and various ontologies. Our approach allows for a more flexible manual and (semi-)automatic deployment of matching algorithms, depending on the specific requirements of the application (e.g. suitability to certain types of input) to which the matchers are to be utilized. Since it is difficult to theoretically compare the existing approaches due	algorithm;database schema;ontology (information science);ontology alignment;requirement;semantic web;semantic interoperability;software deployment;web developer;world wide web	Malgorzata Mochól	2009			upper ontology;ontology alignment;ontology inference layer;ontology;data mining;database;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-44.19050322033419	7.195105382070347	12670
7a89ada444907e529b37e3feeddc48e28202d435	from describing to prescribing parallelism: translating the spec accel openacc suite to openmp target directives		Current and next generation HPC systems will exploit accelerators and self-hosting devices within their compute nodes to accelerate applications. This comes at a time when programmer productivity and the ability to produce portable code has been recognized as a major concern. One of the goals of OpenMP and OpenACC is to allow the user to specify parallelism via directives so that compilers can generate device specific code and optimizations. However, the challenge of porting codes becomes more complex because of the different types of parallelism and memory hierarchies available on different architectures. In this paper we discuss our experience with porting the SPEC ACCEL benchmarks from OpenACC to OpenMP 4.5 using a performance portable style that lets the compiler make platform-specific optimizations to achieve good performance on a variety of systems. The ported SPEC ACCEL OpenMP benchmarks were validated on different platforms including Xeon Phi, GPUs and CPUs. We believe that this experience can help the community and compiler vendors understand how users plan to write OpenMP 4.5 applications in a performance portable style.	openacc;openmp	Guido Juckeland;Oscar R. Hernandez;Arpith C. Jacob;Daniel Neilson;Verónica G. Vergara Larrea;Sandra Wienke;Alexander Bobyr;William C. Brantley;Sunita Chandrasekaran;Mathew E. Colgrove;Alexander Grund;Robert Henschel;Wayne Joubert;Matthias S. Müller;Dave Raddatz;Pavel Shelepugin;Brian Whitney	2016		10.1007/978-3-319-46079-6_33	parallel computing;operating system;programming language	HPC	-5.932267487152081	44.37206086134082	12671
fcee7948552a8f0375986e108623f9d95ea464b8	autonomic web services enhanced by asynchronous checkpointing		The evolution of business software technologies is constant and is becoming increasingly complex which leads to a great probability of software/hardware failures. Business processes are built based on web services as they allow the creation of complex business functionalities. To attack the problem of failures presented by the use of web services, organizations are extrapolating the autonomic computing paradigm to their business processes as it enables them to detect, diagnose, and repair problems improving dependability. Sophisticated solutions that increase system dependability exist, however, those approaches have drawbacks; for example, they affect system performance, have high implementation costs, and or they may jeopardize the scalability of the system. To facilitate evolution to self-management, systems must implement the monitoring, analyzing, planning, and execution (MAPE) control loop. An open challenge for MAPE loop is to carry out in an efficient manner the diagnosis and decision-making processes, recollecting data from which the system can detect, diagnose, and repair potential problems. Also, dealt by systems dependability, specifically as fault tolerant mechanisms. One useful tool for this purpose is the communication induced checkpointing (CiC). We use CiC in attacking the dependability problem of using web services in a distributed and efficient manner. First, we present an approach for web services compositions that supports fault tolerance based on the CiC mechanism. Second, we present an algorithm aimed at web services compositions based on an autonomic computing and checkpointing mechanism. Experimental results support the feasibility of this concept proposal.	algorithm;application checkpointing;automated planning and scheduling;autonomic computing;business process;business software;control system;dependability;extrapolation;fault tolerance;programming paradigm;scalability;self-management (computer science);web service	Mariano Vargas-Santiago;Luis Morales-Rosales;Sa&#x00FA;l Pomares-Hern&#x00E1;ndez;Khalil Drira	2018	IEEE Access	10.1109/ACCESS.2017.2756867	business process;business software;autonomic computing;computer science;fault tolerance;web service;scalability;software;distributed computing;dependability	DB	-43.57368771569145	41.00635992639124	12703
6b1604886c073c5d780fb90bf52c6eb6e4bdc176	formal verification of business processes with temporal and resource constraints	analytical models;quality assurance;resource constraint;bpmn;quality assurance automata theory business data processing formal verification;quality assurance formal verification business processes temporal constraint resource constraint it system development model checking technique business process models verification business process modeling notation bpmn timed automata uppaal model checker;temporal constraint;business process modeling;formal verification;model checking;logic gates;business data processing;business;resource constraint business process modeling bpmn model checking temporal constraint;automata theory;real time systems;analytical models real time systems logic gates business	The correctness of business process models is critical for IT system development. The properties of business processes need to be analyzed when they are designed. In particular, business processes usually have various constraints on time and resources, which may cause serious problems like bottlenecks and deadlocks. In this paper, we propose an approach based on the model checking technique for verifying business process models with temporal and resource constraints. First, we extend Business Process Modeling Notation (BPMN) to handle these constraints. Then, we provide a mapping of the business process models described with this extended BPMN onto timed automata that can be verified by the UPPAAL model checker. This approach helps to eliminate various problems with time and resources in the early phase of development, and enables the quality assurance of business process models.	automata theory;bottleneck (software);business process model and notation;correctness (computer science);deadlock;formal verification;model checking;process modeling;timed automaton;uppaal;verification and validation	Kenji Watahiki;Fuyuki Ishikawa;Kunihiko Hiraishi	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6083857	model checking;quality assurance;business domain;logic gate;formal verification;computer science;artifact-centric business process model;business process management;process modeling;automata theory;database;business process model and notation;process mining;business process discovery;business rule;business process modeling	SE	-45.369809500460946	32.12415777421381	12710
49c322cff7e8df18b4bdd3eb9ffce6dfef544e61	loop-detection in hyper-tableaux by powerful model generation.	power modeling	Automated reasoning systems often su er from redundancy: similar parts of derivations are repeated again and again. This leads us to the problem of loopdetection, which clearly is undecidable in general. Nevertheless, we tackle this problem by extending the hyper-tableau calculus as proposed in [Baumgartner, 1998] by generalized terms with exponents, that can be computed by means of computer algebra systems. Although the proposed loop-detection rule is incomplete, the overall calculus remains complete, because loop-detection is only used as an additional, optional mechanism. In summary, this work combines approaches from tableau-based theorem proving, model generation, and integrates computer algebra systems in the theorem proving process.	automated reasoning;automated theorem proving;cobham's thesis;computer algebra system;cycle detection;data structure;debugging;for loop;long division;method of analytic tableaux;reasoning system;runge–kutta methods;sequent calculus;symbolic computation;thomas baumgartner;undecidable problem	Frieder Stolzenburg	1999	J. UCS	10.3217/jucs-005-03-0135	computer science	Logic	-18.833919424370656	19.968859693716862	12723
4e147c36e5f7f2a38a8d62a8c40aef304c743454	system test: applications of control interface testing.				F. Scott Davidson	1983			integration testing;real-time testing;graphical user interface testing	SE	-49.217987056953454	29.711974944695175	12724
30f48d0b837c405ab4e4c1b8adbb557554230a99	estimating costs for agent oriented software	developpement logiciel;multiagent system;software cost estimation;orientado agente;agent based;metric;intelligence artificielle;agent oriented software engineering;oriente agent;estimation cout logiciel;agent logiciel;software engineering;software agents;maturite;maturity;desarrollo logicial;agent intelligent;software development;intelligent agent;genie logiciel;artificial intelligence;metrico;agent oriented;agente inteligente;inteligencia artificial;sistema multiagente;ingenieria informatica;metrique;madurez;systeme multiagent	Despite the progress in agent oriented software engineering, there is still a long way before achieving maturity. Among others, there is a lack of shared experience in evaluating the cost when developing software using the agent paradigm. This paper provides some results on this issue. It collects data from real agent based projects and gives hints for the application of existing software cost estimation models and what would be appropriate metrics for an agent based software development.	agent-based model;automatic programming;cocomo;capability maturity model;code generation (compiler);download;estimation theory;ingenias;library (computing);programming paradigm;rapid application development;software development effort estimation;software engineering;source lines of code	Jorge J. Gómez-Sanz;Juan Pavón;Francisco J. Garijo	2005		10.1007/11752660_17	long-term support;verification and validation;team software process;simulation;software engineering process group;software sizing;systems engineering;engineering;artificial intelligence;package development process;software design;component-based software engineering;software development;software walkthrough;resource-oriented architecture;software deployment;goal-driven software development process;software metric;avionics software	SE	-60.93159685712372	28.923828173981228	12731
5f40f427521debfa9b0d97455aa53cf9ff27017d	message from the organizers of the workshop on dependable and sustainable peer-to-peer systems (das-p2p 2007)	p2p;peer to peer system		peer-to-peer	Yusuke Doi;Youki Kadobayashi;Kenji Saito	2007		10.1109/SAINT-W.2007.66	real-time computing;computer science;peer-to-peer;distributed computing;world wide web	AI	-30.241112167577718	46.96421128861788	12736
7d8dc13ecc9b3c6f250e31b8372a55e6ec0a96f0	incorruptible self-cleansing intrusion tolerance and its application to dns security	high availability;domain name system;indexing terms;system security;computer security;self cleansing intrusion tolerance;critical system;self cleansing system;intrusion tolerance;information system;critical infrastructure	Despite the increased focus on security, critical information systems remain vulnerable to cyber attacks. The trend lends importance to the concept of intrusion tolerance: there is a high probability that systems will be successfully attacked and a critical system must fend off or at least limit the damage caused by unknown and/or undetected attacks. In prior work, we developed a Self-Cleansing Intrusion Tolerance (SCIT) architecture that achieves the above goal by constantly cleansing the servers and rotating the role of individual servers. In this paper, we show that SCIT operations can be incorruptibly enforced with hardware enhancements. We then present an incorruptible SCIT design for use by one of the most critical infrastructures of the Internet, the domain name systems. We will show the advantages of our designs in the following areas: (1) incorruptible intrusion tolerance, (2) high availability, (3) scalability, the support for using high degrees of hardware/server redundancy to improve both system security and service dependability, and (4) in the case of SCIT-based DNSSEC, protection of the DNS master file and cryptographic keys. It is our belief that incorruptible intrusion tolerance as presented here constitutes a new, effective layer of system defense for critical information systems.	apache continuum;baseline (configuration management);computer cluster;computer security;critical system;cryptography;dependability;domain name system security extensions;high availability;hypertext editing system;information system;internet;intrusion tolerance;key (cryptography);scalability;server (computing)	Yih Huang;David Arsenault;Arun K. Sood	2006	JNW	10.4304/jnw.1.5.21-30	intrusion detection system;embedded system;intrusion tolerance;index term;computer science;operating system;critical infrastructure;distributed computing;internet privacy;high availability;world wide web;computer security;intrusion prevention system;information system;domain name system;computer network	Security	-52.39573347132315	58.75505377095167	12744
35d2f6a3ab0766277e0bb2fa17a5f33c2db532cd	towards statistical reasoning in description logics over finite domains (full version)		We present a probabilistic extension of the description logic ALC for reasoning about statistical knowledge. We consider conditional statements over proportions of the domain and are interested in the probabilistic-logical consequences of these proportions. After introducing some general reasoning problems and analyzing their properties, we present first algorithms and complexity results for reasoning in some fragments of StatisticalALC.	algorithm;automated reasoning;boolean satisfiability problem;description logic;integer programming;nexptime;np-hardness;time complexity	Rafael Peñaloza;Nico Potyka	2017	CoRR		artificial intelligence;discrete mathematics;machine learning;probabilistic logic;description logic;mathematics	AI	-16.402574534883264	11.225882638843274	12747
d27d48bb7f932c0185fc8b6b68e47045cc25eb2d	collaborative control and coordination of hazardous chemicals	collaboration;business logic;sensor network;sensor networks;application	General Terms Algorithms, management.	algorithm	Uwe Kubach;Christian Decker;Ken Douglas	2004		10.1145/1031495.1031552	sensor web;embedded system;wireless sensor network;computer science;data mining;computer security	DB	-37.56262137458338	44.49824021530856	12748
be5496bafa939bf67b5d8b63ee57fb8b9494bd72	progressive retrieval and hierarchical visualization of large remote data	hdf5;remote data access;process capability;visual communication;data distribution;grid;large scale;virtual organization;visualization pipeline;large files	The size of data sets produced on remote supercomputer facilities frequently exceeds the processing capabilities of local visualization workstations. This phenomenon increasingly limits scientists when analyzing results of large-scale scientific simulations. That problem gets even more prominent in scientific collaborations, spanning large virtual organizations, working on common shared sets of data distributed in Grid environments. In the visualization community, this problem is addressed by distributing the visualization pipeline. In particular, early stages of the pipeline are executed on resources closer to the initial (remote) locations of the data sets. This paper presents an efficient technique for placing the first two stages of the visualization pipeline (data access and data filter) onto remote resources. This is realized by exploiting the “extended retrieve” feature of GridFTP for flexible, high performance access to very large HDF5 files. We reduce the number of network transactions for filtering operations by utilizing a server side data processing plugin, and hence reduce latency overhead compared to GridFTP partial file access. The paper further describes the application of hierarchical rendering techniques on remote uniform data sets, which make use of the remote data filtering stage.	data access;file spanning;gridftp;hierarchical data format;overhead (computing);scientific visualization;server-side;simulation;supercomputer;workstation	Hans-Christian Hege;Andrei Hutanu;Ralf Kähler;André Merzky;Thomas Radke;Edward Seidel;Brygg Ullmer	2004	Scalable Computing: Practice and Experience		computer science;data mining;database;world wide web	HPC	-20.92849290874327	53.125481005195674	12749
ca6b79ffc0f0f0ce38bfeac4b00e884be64ef4db	towards vagueness-oriented quality assessment of ontologies		Ontology evaluation has been recognized for a long time now as an important part of the ontology development lifecycle, and several methods, processes and metrics have been developed for that purpose. Nevertheless, vagueness is a quality dimension that has been neglected from most current approaches. Vagueness is a common human knowledge and linguistic phenomenon, typically manifested by terms and concepts that lack clear applicability conditions and boundaries such as high, expert, bad, near etc. As such, the existence of vague terminology in an ontology may hamper the latter’s quality, primarily in terms of shareability and meaning explicitness. With that in mind, in this short paper we argue for the need of including vagueness in the ontology evaluation activity and propose a set of metrics to be used towards that goal.	mathematical optimization;ontology (information science);vagueness;web ontology language	Panos Alexopoulos;Phivos Mylonas	2014		10.1007/978-3-319-07064-3_38	natural language processing;data mining;subject-matter expert;semantic data model;computer science;ontology (information science);terminology;ontology;artificial intelligence;vagueness;phenomenon	AI	-45.94173155652226	6.411843467464559	12754
422a0ccfc30b764103c4a83a3868fefb3ce39ec0	salsa: analyzing logs as state machines	state machine;control flow;data flow	"""SALSA examines system logs to derive state-machine views of the sytem’s e x cution, along with control-flow, data-flow models and related statistics. Exploiting SALSA’s derived views and statistics, we can ffectively construct higher-level useful analyses. We demonstrate SALSA’s approach by analyzing system logs generated in Hadoop cluster, and then illustrate SALSA’s value by developing visualization and failure-diagnosis techniques, for three diffe rent Hadoop workloads, based on our derived statemachine views and statistics. Acknowledgements:This work is partially supported by the NSF CAREER Award CCR0238381, NSF Award CCF-0621508, and the Army Research Office grant number DAAD19-02-1-0389 (""""Perpetuall y Available and Secure Information Systems"""") to the Center for C mputer and Communications Security at Carnegie Mellon University."""	apache hadoop;causality;communications security;control flow;dataflow;distributed control system;finite-state machine;high- and low-level;ibm notes;numerical analysis;operating system;salsa;scientific visualization	Jiaqi Tan;Xinghao Pan;Soila Kavulya;Rajeev Gandhi;Priya Narasimhan	2008			data flow diagram;parallel computing;computer science;operating system;database;finite-state machine;control flow	Security	-27.592638864316562	36.67249940723518	12757
d41ce93e37943943ab10dbabc95255a43cb5b1a0	model-based policy derivation for usage control		Usage control is an extension of access control that additionally specifies and enforces what may or must not happen to data once access to it has been granted. In existing enforcements of usage control policies, the focus has been on the implementation of event monitors. Implementation-level policies have been manually specified by technical experts. Manual specification of policies at the implementation level has certain disadvantages. Firstly, users who are technically not proficient, are kept out of usage control specification. Secondly, implementation-level policies tend to be very complex, lengthy and tedious to specify, even for technical experts. Thirdly, because of the complexity of these policies, their specification requires meticulous care. This makes the manual specification of implementation-level policies error-prone. In addition to this, as there is no universal way of giving meanings to high-level actions like copy and delete, enforcements of usage control policies regarding retention, distribution and deletion of data may not adequately reflect user requirements. An ad hoc specification of semantics might result in the inhibition of legitimate events during enforcement while unwanted events might be allowed. This work addresses the aforementioned issues through a policy derivation framework with domain-specific formal semantics of actions. Meanings of actions are defined at the domain level by instantiating a metamodel that allows for a hierarchical refinement of actions. A well-defined methodology to automate the policy derivation is also described. Human intervention is suggested in two roles: a technical expert power user who configures the policy translation and, an end user who specifies policies for data protection. The input of the policy derivation is a domain model, a specification-level usage control policy and some infrastructure configuration details that vary according to the environment. The output is a set of implementation-level policies that configure enforcement mechanisms at different layers of abstraction in a system. The approach is demonstrated and evaluated through case studies.	abstraction layer;access control;cognitive dimensions of notations;domain model;high- and low-level;hoc (programming language);information privacy;metamodeling;refinement (computing);requirement;semantics (computer science);user requirements document	Prachi Kumari	2015			data mining;enforcement;derivation;abstraction;computer science	Security	-51.017806464158284	51.39372991096983	12762
11478a1eb02127a8a4e1ab16ee19858b651cf754	an o(min(m, n)) parallel deadlock detection algorithm	deadlock detection;process management;operating system;technical report;algorithm design;hardware implementation;software implementation	This article presents a novel Parallel Deadlock Detection Algorithm (PDDA) and its hardware implementation, Deadlock Detection Unit (DDU). PDDA uses simple Boolean representations of request, grant, and no activity so that the hardware implementation of PDDA becomes easier and operates faster. We prove the correctness of PDDA and that the DDU has a runtime complexity of O(min(m,n)), where m is the number of resources and n is the number of processes. The DDU reduces deadlock detection time by 99%, (i.e., 100X) or more compared to software implementations of deadlock detection algorithms. An experiment involving a practical situation with an early deadlock condition showed that the time measured from application initialization to deadlock detection was reduced by 46% by employing the DDU as compared to detecting deadlock in software.	algorithm;correctness (computer science);deadlock;sensor	Jaehwan John Lee;Vincent John Mooney	2005	ACM Trans. Design Autom. Electr. Syst.	10.1145/1080334.1080341	algorithm design;real-time computing;computer science;technical report;theoretical computer science;deadlock;operating system;distributed computing;deadlock prevention algorithms;algorithm	Embedded	-12.204009489435334	42.52705253481835	12766
5efcc79980df978f0e8b8cc8ad1496df39bc00d8	using aop to bring a project back in shape: the ourgrid case	tests;aspectj;design and development;turing test;separation of concern;software architecture;aspect oriented programming;lessons learned;middleware;software reengineering;separation of concerns;grid computing	The design and development of distributed software is a complex task. This was not different in OurGrid, a project whose objective was to develop a free-to-join grid. After two years of development, it was necessary to redesign OurGrid in order to cope with the integration problems that emerged. This paper reports our experience in using Aspect-Oriented Programming (AOP) in the process of redesigning the OurGrid middleware. The essential direction of our approach was to get the project (and the software) back in shape. We discuss how the lack of separation of concerns created difficulties in the project design and development and how AOP has been introduced to overcome these problems. In particular, we present the event-based pattern designed to better isolate the middleware concerns and the threads. Besides, we also present the aspects designed for managing the threads and for aiding the testing of multithreaded code. We also highlight the lessons learned in the process of regaining control of the software.	aspect-oriented programming;distributed computing;middleware;ourgrid;separation of concerns	Ayla Dantas;Walfredo Cirne;Katia Barbosa Saikoski	2006	Journal of the Brazilian Computer Society	10.1007/BF03192380	real-time computing;separation of concerns;computer science;operating system;software engineering;programming language	SE	-52.327642043995716	29.846105291765443	12777
83f1e60d928c7c04fd6ca2dd4176ae885a1c9a47	implicational molecules: a method for extracting meaning from input sentences	input sentence;sentence input;implicational molecule;belief system;pragmatic meaning;sentence type;tha psychology;simple sentence;tha process heuristic overtone;tha system	"""A genera l ly overlooked p o s s i b i l i t y f o r e x t r a c t ing pragmatic meaning from sentence inputs is d i s cussed. This p o s s i b i l i t y draws upon tha psychology o f tha sub jec t i ve a t t r i b u t i o n o f causes to st imulus events. App l i ca t ion to the design of conversat ional programs ia feas ib le , although the praaant work concerns app l i ca t i on to the a imulat ion of a b e l i e f system. Simple sentences known by the system ara grouped i n t o ca tegor i ca l types. An """" i m p l i c a t i o n a l molecule"""" ia a meaningful aet of sentence types w i t h l i n ked elements, f o r example: A doea X, X causes Y, A wants Y. One important way in which meaning can ba ascr ibed to an input sentence is to use i t as the basis f o r a molecule which ia then """" f i l l e d """" by other in format ion in tha system. This no t ion ia def ined mathematically , and a programming a lgor i thm o u t l i n e d . The poss i b i l i t y o f recurs ive use o f a m o l e c u l e f i l l i n g r o u t i n e gives tha process h e u r i s t i c overtones."""	genera	Robert P. Abelson;Carol M. Reich	1969			artificial intelligence;machine learning;mathematics;algorithm	AI	-10.607617344079499	6.440082016806663	12790
e1f56f5506ac3adff3ff19655804c3bf85db9c05	towards a dynamic authorisation planning satisfying intra-instance and inter-instance constraints	publish subscribe system;authorisation planning;intra instance constraint;agent technologie;access control;inter instance constraints	Role-Based Access Control (RBAC) model has been developed as an alternative to traditional approaches to handle access control in workflow systems. Accordingly, authorisation constraints must be defined to enforce the legal assignment of access privileges to roles and roles to users. The authorisation planning ensures that there is at least one way to complete the workflow instance without breaching any of the authorisation constraints. Authorisation planning with considering intra-instance constraints has been discussed in the research literature. However, the inter-instance constraints also need to be considered to mitigate the security fraud. In this paper, a novel authorisation system that incorporates intra-instance and inter-instance constraints is proposed. It includes the planning phase, the execution phase, and the adjustment phase. It is in charge of generating user/role assignment plans, verifying them and eventually updating them to take into account the dynamic (intra-instance and inter-instance) constraints. Besides, grounded upon agent technology and publish-subscribe communication model, a mechanism for the consideration of dynamic constraints (intra-instance and inter-intance) to generate valid assignment plans is demonstrated.	authorization;principle of least privilege;publish–subscribe pattern;role-based access control;scientific literature;verification and validation	Meriam Jemel;Nadia Ben Azzouna;Khaled Ghédira	2013		10.1145/2523514.2523582	computer science;access control;data mining;database;computer security	AI	-49.10249407483241	52.32011884577541	12802
61acef78db55e1ac1af0b9d47aa76d836568ab6d	safe overprovisioning: using power limits to increase aggregate throughput	overprovisioning;data center;power management;technical report;power consumption;managing power limits	Management of power in data centers is driven by the need to not exceed circuit capacity. The methods employed in the oversight of these power circuits are typically static and ad-hoc. New power-scalable system components allow for dynamically controlling power consumption with an accompanying effect on performance. Because the incremental performance gain from operating in a higher performance state is less than the increase in power, it is possible to overprovision the hardware infrastructure to increase throughput and yet still remain below an aggregate power limit. In overprovisioning, if each component operates at maximum power the limit would be exceeded with disastrous results. However, safe overprovisioning regulates power consumption locally to meet the global power budget. Host-based and network-centric models are proposed to monitor and coordinate the distribution of power with the fundamental goal of increasing throughput. This research work presents the advantages of overprovisioning and describes a general framework and an initial prototype. Initial results with a synthetic benchmark indicate throughput increases of nearly 6% from a staticly assigned, power managed environment and over 30% from an unmanaged environment.	advanced configuration and power interface;aggregate data;aggregate function;benchmark (computing);data center;dhrystone;hoc (programming language);maximum power transfer theorem;prototype;scalability;throughput	Mark E. Femal;Vincent W. Freeh	2004		10.1007/11574859_11	real-time computing;simulation;engineering;operations management	Arch	-19.513090680954605	56.509148863692666	12822
d6977de0e9f07c99f5c150b0c587ca330a5898c7	advances in intelligent grid and cloud computing	qa75 electronic computers computer science;ta engineering general civil engineering general	Grid and Cloud Computing (GCC) frameworks have been one of the most popular research topics in recent years. These technologies are very important for supporting efficient user collaboration, since various resources for a project may be geographically distributed and heterogeneous (Foster and Kesselman 1999). Users can transparently access resources and services without worrying about how many services the platform has and where the services are located (Jung 2009). Especially, cloud computing can be seen as the next natural step towards the grid-utility model. However, advanced technologies and issues still need to be investigated, such as the appropriate platform, hybrid cloud architecture, service discovery and intelligent management, workflow (Jung 2011), QoS issues, green issues, cost-models, and so on, to provide transparent,	cloud computing	Jason J. Jung;Yue-Shan Chang;Ying Liu;Chao-Chin Wu	2012	Information Systems Frontiers	10.1007/s10796-012-9349-x	computational science;computing;computer science;grid computing;computer engineering	HPC	-31.743809282000004	54.12658373811465	12836
0da1af5741e903122f39e6dca9e0ae30fb36b8a9	a dynamic application-driven data communication strategy	simulation;semi lagrangian;data communication;computational science;performance analysis;communication cost;parallel architecture;communication pattern;data driven dynamic applications	The use of semi-Lagrangian formulations in numerical weather predication models (NWP) allows for an increase in time step size. Use of this method can increase performance of these models. However, on parallel architectures, communication between processors can become a huge bottleneck, limiting speedup. Furthermore, the communication pattern is dependent on the application's execution. We discus a novel strategy, called Halo on Demand, which dynamically drives the communication between the processors by examining the content of the data at runtime in order to reduce communication costs. With an extensive performance analysis of the execution of the model we show that our strategy can decrease communication time and thus decrease total execution time.	central processing unit;numerical analysis;numerical weather prediction;run time (program lifecycle phase);semiconductor industry;speedup	Paul van der Mark;Lex Wolters;Gerard Cats	2004		10.1145/1006209.1006231	parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-4.938669248588931	39.37051790198327	12840
30aba9b216e5110e9b3482b593783593cb6098db	some problems concerning armstrong relations of dual schemes and relation schemes in the relational datamodel	qa75 electronic computers computer science szamitastechnika;szamitogeptudomany		armstrong's axioms	János Demetrovics;Vu Duc Thi	1993	Acta Cybern.		computer science;theoretical computer science;algorithm	Crypto	-11.9014050333651	12.605425320936446	12851
f6441f1a446ab58ea1859b82edcc8416768e9ab6	arbitrary announcements in propositional belief revision		Public announcements cause each agent in a group to modify their beliefs to incorporate some new piece of information, while simultaneously being aware that all other agents are doing the same. Given some fixed goal formula, it is natural to ask if there exists an announcement that will make the formula true in a multi-agent context. This problem is known to be undecidable in a general modal setting, where the presence of nested beliefs can lead to complex dynamics. In this paper, we consider not necessarily truthful public announcements in the setting of propositional belief revision. We are given a goal formula for each agent, and we are interested in finding a single announcement that will make each agent believe the corresponding goal following AGM-style belief revision. If the goals are inconsistent, then this can be seen as a form of ampliative reasoning. We prove that determining if there is an arbitrary public announcement in this setting is not only decidable, but that it is simpler than the corresponding problem in the most simplified modal logics. Moreover, we argue that propositional announcements and beliefs are sufficient for modelling many practical problems, including simple robot controllers.	belief revision;complex dynamics;decision problem;modal logic;multi-agent system;robot;undecidable problem	Aaron Hunter;François Schwarzentruber	2015			artificial intelligence;mathematics;social psychology;algorithm	AI	-17.39611394770692	4.438607389743638	12857
b19e39f2a4f53636669de92a5bc6d72ee9afeb42	development of electronic tandem service (ets) features for the dimension pbx	routing;testing;telephony;personnel;circuits;authorization;telephony programming circuits costs routing authorization software design testing personnel productivity;productivity;software design;programming	This paper describes the software development process used in generating Electronic Tandem Service (ETS) features for the DIMENSION 2000 PBX. ETS provides the corporate customer with a uniform numbering plan for all locations (unique 7 digit number for each station on the network), network cost control features (routing, queuing, and authorization privileges), and methods for admin istering and controlling network performance. The paper is organized into four sections. Section 2 describes the DIMENSION 2000 PBX design structure and software development environment. Section 3 covers the methods used for requirements, design, testing, personnel organization, and tracking. Lastly, Section 4 discusses the results in terms of trouble rates and productivity figures.	enterprise test software	A. M. Gerrish;D. C. Opferman	1979		10.1109/CMPSAC.1979.762471	programming;electronic circuit;routing;productivity;simulation;computer science;engineering;artificial intelligence;software design;operating system;software engineering;database;software testing;authorization;telephony;management;computer security	Theory	-59.403831940465665	4.60904779291455	12858
c901eac89d9b8a66846615cf95ef9a94c66fefc0	service oriented platform for drones competition		Drones are facing a huge success. They are available almost anywhere, either online or at a near shop, not only for enthusiasts but also for professionals of many areas, such as agriculture, construction, multimedia and military missions. In the same way, online platforms are growing fast. Many applications that we were used to be installed and ran locally in our devices are now deployed in the internet, with the benefits like high availability, high scalability and high compatibility with almost any laptop, computer, smartphone or tablet. Thus, the high level of interest and utility of drones and the crescent demand of service-oriented platforms surged up to be a great fusion of concepts. Nowadays, training and competition requires users to have their own aircraft(s) and to move, usually, to an arena where they can play with each other. Having it in consideration, in this work it’s proposed a platform that combines real-time online gaming and drones’ training and competition, in which users from around the world can socialize and play together. This paper runs through an overview of the concept, presenting the specifications and the requirements that the platform must meet to accomplish the desired objective. The proposed platform provides access to anyone, anytime, anywhere and with any device. It opens the possibility for users to communicate with each other and to control aircrafts with real-time video transmission, only achievable due to the distributed architecture designed, that minimizes the existence of single points of failure.		Jo&#x00E3;o Ramos;David Safadinho;Roberto Ribeiro;Ant&#x00F3;nio Pereira	2018	2018 2nd International Conference on Technology and Innovation in Sports, Health and Wellbeing (TISHW)	10.1109/TISHW.2018.8559555	laptop;the internet;computer security;single point of failure;high availability;scalability;computer science	DB	-46.76590103571718	49.124556986819	12888
5edefa1fa7c555c97851e587cb765545acfdc797	distributed file system: efficiency experiments for data access and communication	remote write operation;disk input output operations;computers;kernel;i o tuning;remote read operation;distributed processing;buffer management;buffer storage;network communication;i o tuning distributed file system buffer management disk input output performance;data communication;input output;servers;disk input output operations distributed file system data access data communication remote read operation remote write operation application level buffer network communication;data access;distributed databases;writing;distributed processing buffer storage;distributed file system;titanium;file systems writing servers computers titanium kernel distributed databases;file systems;application level buffer;reading and writing;disk input output performance	In a distributed file system, operations such as read, write, send and receive should be efficient and must increase the overall performance of the system. Especially every remote read and write operation we encounter network and secondary storage latencies. In order to reduce this latency time and improve the performance of distributed file system, we analyze the problem of selecting application-level buffer size for the network communication and disk input/output operations. A number of experiments were carried out to identify the optimal size for our application-level buffer.	auxiliary memory;clustered file system;computer data storage;dce distributed file system;data access;experiment;input/output	Bipin Upadhyaya;Fahriddin Azimov;Doan Thanh Tran;Eunmi Choi;SangBum Kim;Pilsung Kim	2008	2008 Fourth International Conference on Networked Computing and Advanced Information Management	10.1109/NCM.2008.164	self-certifying file system;data access;input/output;titanium;kernel;real-time computing;computer hardware;computer science;stub file;operating system;journaling file system;distributed computing;open;distributed file system;file system fragmentation;writing;distributed database;server	DB	-14.135881530927094	51.605996151109274	12889
983ee8ff34572425a6d81203d8c0d8c39f5d9600	co-processor system design for fine-grain message handling in kump/d	tratamiento paralelo;processing element;processor architecture;evaluation performance;performance evaluation;traitement parallele;evaluacion prestacion;mensajeria;messagerie;remote procedure call;computer architecture;media processor;architecture ordinateur;system design;message handling;arquitectura ordenador;remote memory access;concurrent process;parallel processing	In parallel processing, fine-grain parallel processing is quite effective solution for latency problem caused by remote memory accesses and remote procedure calls. We have proposed a processor architecture, called Datarol-II, that promotes efficient fine-grain multi-thread execution by performing fast context switching among fine-grain concurrent processes. We are now building a prototype multi-media machine KUMP/D (Kyushu University Multi-media Processor on Datarol-II) on the basis of the fine-grain multi-threading architecture. In the design of the KUMP/D, we used the commercial microprocessor for its processing element, and designed a co-processor, called FMP(Fine-grain Message Processor), for fine-grain message handling and communication control. In this paper, we show the KUMP/D processor design and its performance evaluation.		Hiroshi Tomiyasu;Shigeru Kusakabe;Tetsuo Kawano;Makoto Amamiya	1997		10.1007/BFb0002813	embedded system;parallel processing;parallel computing;real-time computing;media processor;microarchitecture;computer science;operating system;database;distributed computing;remote procedure call;systems design	EDA	-12.381811498760168	45.52453984174103	12909
c72d62e50a9feb025176c6d896a009071ac13105	exploiting frequent field values in java objects for reducing heap memory requirements	mobile device;java virtual machine;heap;frequent field value;form factor;garbage collection;experimental evaluation;power consumption;embedded device	The capabilities of applications executing on embedded and mobile devices are strongly influenced by memory size limitations. In fact, memory limitations are one of the main reasons that applications run slowly or even crash in embedded/mobile devices. While improvements in technology enable the integration of more memory into embedded devices, the amount memory that can be included is also limited by cost, power consumption, and form factor considerations. Consequently, addressing memory limitations will continue to be of importance.Focusing on embedded Java environments, this paper shows how object compression can improve memory space utilization. The main idea is to make use of the observation that a small set of values tend to appear in some fields of the heap-allocated objects much more frequently than other values. Our analysis shows the existence of such frequent field values in the SpecJVM98 benchmark suite. We then propose two object compression schemes that eliminate/reduce the space occupied by the frequent field values. Our extensive experimental evaluation using a set of eight Java benchmarks shows that these schemes can reduce the minimum heap size allowing Java applications to execute without out-of-memory exceptions by up to 24% (14% on an average).	benchmark (computing);crash (computing);dspace;embedded java;embedded system;heap (data structure);memory management;mobile device;out of memory;requirement	Guangyu Chen;Mahmut T. Kandemir;Mary Jane Irwin	2005		10.1145/1064979.1064990	memory footprint;garbage;parallel computing;real-time computing;heap;form factor;computer science;operating system;mobile device;distributed computing;overlay;garbage collection;programming language	PL	-20.565586042567034	36.084147095288216	12943
1aebcc5f25bb1f47ef4bd3cf98678266203a2632	cost analysis of redundancy schemes for distributed storage systems	cluster computing;system configuration;distributed storage;distributed storage system;cost analysis;internet architecture;storage capacity;communication cost	Distributed storage infrastructures require the use of dat a redundancy to achieve high data reliability. Unfortunate ly, the use of redundancy introduces storage and communication overhe ads, which can either reduce the overall storage capacity of the system or increase its costs. To mitigate the storage and communication overhead, different redundancy schemes hav e been proposed. However, due to the great variety of underlay ing storage infrastructures and the different application needs, optimizing these redundancy schemes for each storage infra structure is cumbersome. The lack of rules to determine the optimal level of redundancy for each storage configuration l eads developers in industry to often choose simpler redunda ncy schemes, which are usually not the optimal ones. In this pape r we analyze the cost of different redundancy schemes and derive a set of rules to determine which redundancy scheme mi nimizes the storage and the communication costs for a given system configuration. Additionally, we use simulation to sh w that theoretically-optimal schemes may not be viable in a realistic setting where nodes can go off-line and repairs ma y be delayed. In these cases, we identify which are the tradeoffs between the storage and communication overheads of the redu ndancy scheme and its data reliability.	clustered file system;code (cryptography);computer data storage;data redundancy;distributed database;erasure code;experiment;online and offline;optimization problem;overhead (computing);oxford spelling;redundancy (engineering);replication (computing);scalability;simulation;system configuration;wilhelm pape	Lluis Pamies-Juarez;Ernst W. Biersack	2011	CoRR		triple modular redundancy;dual modular redundancy;real-time computing;converged storage;redundancy;distributed data store;computer cluster;computer science;cost–benefit analysis;theoretical computer science;database;distributed computing;redundancy	HPC	-19.180794939955916	53.16218306250139	12946
306d589976b94d385fcaef6924edd6f4c7fd840e	selective cache ways: on-demand cache resource allocation	resource allocation	Increasing levels of microprocessor power dissipation call for new approaches at the architectural level that save energy by better matching of on-chip resources to application requirements. Selective cache ways provides the ability to disable a subset of the ways in a set associative cache during periods of modest cache activity, while the full cache may remain operational for more cache-intensive periods. Because this approach leverages the subarray partitioning that is already present for performance reasons, only minor changes to a conventional cache are required, and therefore, full-speed cache operation can be maintained. Furthermore, the tradeoff between performance and energy is flexible, and can be dynamically tailored to meet changing application and machine environmental conditions. We show that trading off a small performance degradation for energy savings can produce a significant reduction in cache energy dissipation using this approach.	binary space partitioning;elegant degradation;equivalence partitioning;microprocessor;power supply;random-access memory;requirement	David H. Albonesi	1999	J. Instruction-Level Parallelism		cache coloring;resource allocation;cache algorithms;cache pollution	Arch	-6.6605277500200915	55.40106812331961	12953
6a1e25e68080699775799113a36377f9ac60720f	equivalence checking of scheduling with speculative code transformations in high-level synthesis	high-level synthesis;formal method;common subexpression extraction;finite state machine;equivalence checking;existing state-of-the-art equivalence checking;speculative code transformation;finite set;basic block;code transformation;path-based scheduling;merging;automata;electronic system level;algorithm design;algorithm design and analysis;high level synthesis;scheduling;finite state machines;scheduling algorithm	This paper presents a formal method for equivalence checking between the descriptions before and after scheduling in high-level synthesis (HLS). Both descriptions are represented by finite state machine with datapaths (FSMDs) and are then characterized through finite sets of paths. The main target of our proposed method is to verify scheduling employing code transformations -- such as speculation and common subexpression extraction (CSE), across basic block (BB) boundaries - which have not been properly addressed in the past. Nevertheless, our method can verify typical BB-based and path-based scheduling as well. The experimental results demonstrate that the proposed method can indeed outperform an existing state-of-the-art equivalence checking algorithm.	algorithm;basic block;finite-state machine;formal equivalence checking;formal methods;high- and low-level;high-level synthesis;scheduling (computing);speculative execution;turing completeness	Chi-Hui Lee;Che-Hua Shih;Juinn-Dar Huang;Jing-Yang Jou	2011	16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)		algorithm design;parallel computing;real-time computing;computer science;theoretical computer science;operating system;finite-state machine;programming language;scheduling;algorithm	EDA	-15.818045581876305	30.11840948827598	12983
11603b19f76a074e4ef6932c959ae73989eff1ce	set theoretic properties of loeb measure		We ask the question: to what extent do basic set theoretic properties of Loeb measure depend on the nonstandard universe and on properties of the model of set theory in which it lies? We show that, assuming Martinu0027s axiom and κ-saturation, the smallest cover by Loeb measure zero sets must have cardinality less than κ. In contrast to this we show that the additivity of Loeb measure cannot be greater than ω 1 . We answer a question of Paris and Mills concerning cuts in nonstandard models of number theory. We show that it is consistent that there exists a Sierpinski set in the reals but no Loeb-Sierpinski set in any nonstandard universe		Arnold W. Miller	1990	J. Symb. Log.		cardinality;mathematical logic;mathematical analysis;number theory;topology;measure;calculus;continuum hypothesis;mathematics;axiom;saturation;algorithm;set theory	Logic	-6.775345395277187	12.377530463315138	13005
266fb03e193fc156be1b2369da1c5f2499bf8084	fractal prefetching b±trees: optimizing both cache and disk performance	cache optimization;edge caching;cache performance;proxy based caching;dynamic content	"""B+-Trees have been traditionally optimized for I/O performance with disk pages as tree nodes. Recently, researchers have proposed new types of B+-Trees optimized for CPU cache performance in main memory environments, where the tree node sizes are one or a few cache lines. Unfortunately, due primarily to this large discrepancy in optimal node sizes, existing disk-optimized B+-Trees suffer from poor cache performance while cache-optimized B+-Trees exhibit poor disk performance. In this paper, we propose fractal prefetching B+-Trees (fpB+-Trees), which embed """"cache-optimized"""" trees within """"disk-optimized"""" trees, in order to optimize both cache and I/O performance. We design and evaluate two approaches to breaking disk pages into cache-optimized nodes: disk-first and cache-first. These approaches are somewhat biased in favor of maximizing disk and cache performance, respectively, as demonstrated by our results. Both implementations of fpB+-Trees achieve dramatically better cache performance than disk-optimized B+-Trees: a factor of 1.1-1.8 improvement for search, up to a factor of 4.2 improvement for range scans, and up to a 20-fold improvement for updates, all without significant degradation of I/O performance. In addition, fpB+-Trees accelerate I/O performance for range scans by using jump-pointer arrays to prefetch leaf pages, thereby achieving a speed-up of 2.5-5 on IBM's DB2 Universal Database."""	cpu cache;cache (computing);central processing unit;computer data storage;database;discrepancy function;elegant degradation;fractal;input/output;link prefetching;locality of reference;memory hierarchy;pointer (computer programming)	Shimin Chen;Phillip B. Gibbons;Todd C. Mowry;Gary Valentin	2002		10.1145/564691.564710	cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cache;computer science;cache invalidation;dynamic web page;distributed computing;smart cache;cache algorithms	DB	-13.172953885528095	53.520716598772005	13015
f5edb4f6b1f28bf49449f9c388570506c4c45921	a standard for real-time smart transducer interface	time triggered;time triggered systems;real time;file system;fieldbus systems;object management group;communication protocol;real time communication;smart transducers;dynamic configuration	In order to handle the inherent complexity of the multitude of available different transducer components, a generic interface approach is necessary. Such a universal smart transducer interface should provide precisely defined interfaces between smart transducers and their users, which are simple and precisely specified within the value domain and the temporal domain. The Object Management Group adopted a smart transducer interface standard that incorporates (i) realtime characteristics and functionalities for the smart transducer network (ii) online diagnostic service capability (iii) support for start-up and dynamic configuration (iv) a uniform naming and addressing scheme for all relevant data in the smart transducer system (v) a generic interface that enables the smart transducer system to interact with other systems via CORBA. This paper describes the main concepts and implementation experiences of this smart transducer interface. The approach integrates a time-triggered communication protocol with an appropriate access scheme, the so-called interface file system. This interface file system provides a unique naming and addressing scheme enabling access to internal transducer data via a CORBA gateway.	addressing scheme;common object request broker architecture;communications protocol;emoticon;real-time clock;smart transducer	Wilfried Elmenreich;Wolfgang Haidinger;Hermann Kopetz;Thomas Losert;Roman Obermaisser;Michael Paulitsch;Philipp Peti	2006	Computer Standards & Interfaces	10.1016/j.csi.2005.09.001	embedded system;communications protocol;real-time computing;computer hardware;computer science;operating system	SE	-36.91772452374397	42.92647078745981	13021
0e3d01e7782aae1c9c900b327363acbb6c19b633	a unified knowledge representation and context-aware recommender system in internet of things		Within the rapidly developing Internet of Things (IoT), numerous and diverse physical devices, Edge devices, Cloud infrastructure, and their quality of service requirements (QoS), need to be represented within a unified specification in order to enable rapid IoT application development, monitoring, and dynamic reconfiguration. But heterogeneities among different configuration knowledge representation models pose limitations for acquisition, discovery and curation of configuration knowledge for coordinated IoT applications. This paper proposes a unified data model to represent IoT resource configuration knowledge artifacts. It also proposes IoT-CANE (Context-Aware recommendatioN systEm) to facilitate incremental knowledge acquisition and declarative context driven knowledge recommendation.	algorithm;cloud computing;data model;digital curation;internet of things;knowledge acquisition;knowledge representation and reasoning;non-functional requirement;prototype;recommender system;ripple effect;software deployment;software testing controversies;terms of service	Yinhao Li;Awa Alqahtani;Ellis Solaiman;Charith Perera;Prem Prakash Jayaraman;Boualem Benatallah;Rajiv Ranjan	2018	CoRR		knowledge acquisition;recommender system;edge device;knowledge representation and reasoning;control reconfiguration;cloud computing;quality of service;computer science;data model;distributed computing	Robotics	-43.04470774608559	44.11954758569543	13043
ea9b365f92ca6bfd7b80001aceb7d9665570d2e8	proposed security for critical air force missions	information security;computer aided instruction;logic;computer crime;critical air force missions;orange book methods;security of data computer viruses military computing;computer security;orange book methods critical air force missions enemy agent malicious logic attack;protection;malicious logic attack;computer viruses;system evaluation;critical system;cryptography;fault detection;enemy agent;protection logic computer crime cryptography military computing object detection fault detection information security computer aided instruction computer security;security of data;object detection;military computing	Air Force missions could be forced to fail by an enemy agent launching a malicious logic attack. These missions must be protected. Because of the imminent, potential danger, a protection approach has been developed that is easily understood and implemented for a minimum cost-because it uses Orange Book methods and mechanisms. The criteria for protection of critical systems are given as the G3 division/class of the Air Force Trusted Critical Computer System Evaluation Criteria (AFTCCSEC). >		Howard L. Johnson;Chuck Arvin;Earl Jenkinson;Bob Pierce	1991		10.1109/CSAC.1991.213004	simulation;computer science;cryptography;information security;theoretical computer science;computer security;logic	EDA	-61.97278215698119	57.190083701131016	13048
712f028100df8841678661b38dae7ecb149009d8	formal verification meets robustness checking — techniques and challenges	formal verification technique;property checking;process variation;assertion based verification;network synthesis;fault tolerant;formal verification robustness logic testing circuit faults design automation circuit testing automatic testing computer science fault tolerance transistors;design flow;network synthesis fault tolerance;formal method;formal verification;transistors;fault tolerance;cost effectiveness;symbolic simulation;equivalence checking;robustness checking techniques;property checking formal verification technique robustness checking techniques transistors fault tolerance symbolic simulation assertion based verification equivalence checking	Summary form only given. Modern circuits contain up to several hundred million transistors and this number grows exponentially over time. Thus, the state-of-the-art design flows have to be improved continuously. In the meantime ensuring correctness becomes a major bottleneck. Up to 80% of the overall design costs are due to verification and even more problems are foreseeable - shrinking feature sizes lead to large process variations, increased susceptibility to radiation, etc. As a result even a correct design may contain faulty components. Thus, robustness is required, i.e. correct operation in presence of faults. Formal verification techniques have gained large attention, since they allow proving the correctness of a circuit and thereby ensure 100% functional correctness. Moreover, the underlying techniques can also be used to prove robustness of a design. Besides being more reliable, formal verification approaches have also shown to be more cost effective in many cases, since test bench creation - usually a very time consuming and error prone task - becomes superfluous. Therefore these techniques allow unveiling faulty behaviour automatically. Proving robustness in the sense of fault tolerance is important. As in any implementation step, faults may tamper the behaviour. In particular, fault tolerance may hide flaws during the standard verification step. Therefore instead of relying on the design's architectural precautions against faults, the implementation has to be proven to be fault tolerant. In this tutorial, after a brief motivation of the overall topic and definition of the problem domain, the alternative verification approaches are explained. Next, the application of the underlying formal techniques for robustness checking is considered. The standard approaches for verification are simulation, emulation and formal methods. Details are discussed related to formal verification, symbolic simulation and assertion based verification. The verification scenarios for equivalence checking (EC) and property checking (PC) are presented and the underlying proof techniques are explained. Robustness checking then also builds on the underlying formal methods. This allows analyzing the robustness of a design fully automatically. Practical scenarios covered by this model for robustness of a circuit are discussed. The model not only yields a measure for the robustness of a circuit but also shows critical parts of the implementation that should be reengineered. In comparison alternative approaches, e.g. using statistical measures, are outlined. Further references are given for all topics are given (see below). Directions for future work and research challenges are discussed.	assertion (software development);bottleneck (engineering);cognitive dimensions of notations;correctness (computer science);emulator;fault tolerance;formal equivalence checking;formal methods;formal verification;problem domain;robustness (computer science);symbolic simulation;test bench;transistor;turing completeness	Rolf Drechsler;Görschwin Fey	2010	13th IEEE Symposium on Design and Diagnostics of Electronic Circuits and Systems	10.1109/DDECS.2010.5491833	reliability engineering;fault tolerance;formal methods;computer science;theoretical computer science;formal equivalence checking;runtime verification;programming language;algorithm;functional verification	EDA	-14.82724934883506	27.615258588345082	13049
56db23f341c19cd96d506ded02e1f5cb0e1cfe8e	a bayesian reasoning framework for on-line business information systems	concept design;common kads;data mining;bayesian reasoning;bayesian learning;online information systems;information system;business information systems;use case;business rules;bayesian interface	We describe a Bayesian Reasoning Framework (BRF) that supports business rule operations for on-line information systems. BRF comprises a three-layer environment with business information systems at the top, a middle-ware Bayesian reasoning server, and a Bayesian reasoning engine at the bottom. The top and middle-ware layers communicate via SOAP/XML protocol, while the middle-ware and bottom layers communicate via a Tag-value protocol that fetches business rules from a central repository. BRF is built as a Bayesian Reasoning Agent and tested in a helpdesk system for assigning advisors to users for trouble-shooting in the operation of business information systems. BRF is modeled following a use-case methodology as well as an inference modeling that uses an assignation template from CommonKADS. The concept, design and implementation of BRF for real-world, on-line business information systems are the main contribution of this research project.	advanced mezzanine card;advanced telecommunications computing architecture;americas conference on information systems;artificial intelligence;autonomous agent;autonomous robot;business process;data mining;definition;interaction;knowledge acquisition and documentation structuring;management information system;mechatronics;multitier architecture;online and offline;ontology (information science);soap;semantic reasoner;server (computing);warez;xml protocol	Armando Robles-Pompa;Francisco J. Cantú Ortiz;Rubén Morales-Menéndez	2005			variable-order bayesian network;information engineering;computer science;knowledge management;data science;data mining;reasoning system;business rule	AI	-48.49635223245984	7.254298573159847	13051
498c8f4a6a2b37a623ba96111c6bfb98359587d5	the devolution of functional analysis	denotational methods;logic;abstract data types;formal specifications;data bases;grammars;functional analysis;integrity constraints;data models	The opinions expressed in this paper are those of the authors and do not represent the opinions, statements and policies of their respective employers and of the DDSWP.	data dictionary	Ken Meyer;John Doughty	1983	SIGMOD Record	10.1145/984532.984538	functional analysis;computer science;database;programming language;logic;algorithm	PL	-25.98956757977095	17.404648967790653	13062
380d7d4863a57bc6a5109edcc77c352d91a485da	size bounds for factorised representations of query results	conjunctive queries;computacion informatica;data factorisation;size bounds;ciencias basicas y experimentales;query evaluation;hypertree decompositions;grupo a;succinct representation	We study two succinct representation systems for relational data based on relational algebra expressions with unions, Cartesian products, and singleton relations: f-representations, which employ algebraic factorisation using distributivity of product over union, and d-representations, which are f-representations where further succinctness is brought by explicit sharing of repeated subexpressions.  In particular we study such representations for results of conjunctive queries. We derive tight asymptotic bounds for representation sizes and present algorithms to compute representations within these bounds. We compare the succinctness of f-representations and d-representations for results of equi-join queries, and relate them to fractional edge covers and fractional hypertree decompositions of the query hypergraph.  Recent work showed that f-representations can significantly boost the performance of query evaluation in centralised and distributed settings and of machine learning tasks.	algorithm;cartesian closed category;centralisation;conjunctive query;fractional fourier transform;linear algebra;machine learning;monoid factorisation;relational algebra	Dan Olteanu;Jakub Závodný	2015	ACM Trans. Database Syst.	10.1145/2656335	boolean conjunctive query;computer science;theoretical computer science;database;conjunctive query	DB	-25.332263984066216	8.343370225631595	13067
77d1cce207a89c6df8f52e16759bd65e44fb821c	optimizing real-time equational rule-based systems	optimisation;performance evaluation;rule based system;real time;optimal method;rule based;dependence graph;graphs;upper bound;real time systems equations knowledge based systems runtime upper bound expert systems timing optimization methods algorithm design and analysis logic;logic programming;specification languages;real time rule based systems;runtime optimization;rule based programming;optimisation knowledge based systems logic programming real time systems specification languages;eql language;timing analysis;knowledge based systems;equational logic;eql language real time rule based expert system equational logic rule based program runtime optimization predicate based rule dependency graph;real time systems;expert system;time constraint	Analyzing and reducing the execution-time upper bound of real-time rule-based expert systems is a very important task because of the stringent timing constraints imposed on this class of systems. We present a new runtime optimization to reduce the execution-time upper bound of real-time rule-based expert systems. In order to determine rules to be evaluated at runtime, a predicate dependency list, which consists of a predicate, its active rule set and corresponding inactive rule set, is created for each predicate in a real-time rule-based program. Based on the predicate dependency list and the current value of each variable, the new runtime optimization dynamically selects rules to be evaluated at runtime. For the timing analysis of the proposed algorithm, we introduce a predicate-based rule dependency graph, a predicate-based enable-rule graph, and their construction algorithm. We also discuss the bounded time of the equational logic rule-based program using the predicate-based rule dependency graph as well as the predicate-based enable-rule graph. The implementation and performance evaluation of the proposed algorithm using both synthetic and practical real-time rule-base programs are also presented. The performance evaluation shows that the runtime optimizer reduces the number of rule evaluations and predicate evaluations as well as the response time upper bound significantly, and the new algorithm yields better execution-time upper bound compared to other optimization methods.	algorithm;expert system;logic programming;mathematical optimization;optimizing compiler;performance evaluation;real-time clock;real-time locating system;response time (technology);rule 184;rule-based system;run time (program lifecycle phase);static timing analysis;synthetic intelligence	Yun-Hong Lee;Albert Mo Kim Cheng	2004	IEEE Transactions on Software Engineering	10.1109/TSE.2004.1265816	rule-based system;functional predicate;equational logic;computer science;theoretical computer science;hard-core predicate;predicate variable;graph;upper and lower bounds;programming language;logic programming;static timing analysis;algorithm;universal generalization	Embedded	-13.320271868368408	30.88319179906018	13077
8e6ca20363f1705162fb89de4f9ed4cde9f81767	an approach to feature-based software construction for enhancing maintainability	maintainer;object oriented design;maintenance;essentiality;metrics;dependency alignment;feature;unidirectional dependency;article	While the way we build software affects significantly its maintenance in terms of the effort and cost, the experience level of the maintainer in a software acquirers’ organization is also one of concern. In this context, often the maintainer is the user of the system. Unfortunately, it is quite possible to lose the trustworthiness of the software due to the inexperience of the maintainer, especially when the maintainer is without the help of the original developers. One remedy for providing security against the effects of the maintainer’s software modifications is to restrict the access to software parts (modules) relative to the experience level of the maintainers. For such a remedy to be successful, the software should be constructed in such a way that its parts under maintenance affect others as little as possible. We propose an approach to software construction aligning the dependencies among software parts in one direction so that they are allocated to maintainers based on their experience level. Our approach decomposes the software into parts based on functionality and orders the parts by essentiality, which indicates how difficult it is to change each part. Then, we align the dependencies in such a way that the less essential functionality is dependent on the more essential functionality. Consequently, any modification on less essential functionality does not affect the essential functionalities. To demonstrate the feasibility of our proposed approach, we applied it to a military application and found that the constructed software enables us to confine maintainers’ activity within a limited working area, and thus the software is safer against maintainers’ modification. Copyright c © 2006 John Wiley & Sons, Ltd.	align (company);john d. wiley;software construction;software maintainer;trust (emotion)	Jungyoon Kim;Doo-Hwan Bae	2006	Softw., Pract. Exper.	10.1002/spe.738	reliability engineering;long-term support;software sizing;feature;computer science;systems engineering;engineering;package development process;backporting;software framework;software development;software design description;operating system;object-oriented design;software maintenance;engineering drawing;metrics;software metric	SE	-61.21576097992839	27.89276856836909	13080
0e50fb4288a3650cc62aa6c64f7213a0ddcede50	time driven operating systems: a case study on the mars kernel	fault tolerant;termination detection;global states;operating system;token passing;time domain;computer control;runtime system;distributed systems;causality;real time systems	Time driven real-time systems are of increasing importance in the field of critical computer control applications [Sta90]. Because of their predictable behavior they are well suited for systems whose correct operation in the time domain must be guaranteed already in the design phase of an application. Time driven systems allow the proof of the correct timing behavior of an application by construction of a feasible schedule.In the MARS system [Kop89] the time driven approach is realized. The structure of the MARS operating system kernel differs significantly from that of others because of the specific demands which a distributed time driven system imposes on its underlying operating system. Based on the experiences with the first prototype of the MARS operating system [Dam89] (MARS-1), a new operating system kernel, MARS-2, has been developed from scratch. There have been some motivations for the development of MARS-2:• New processor boards ('MARS components') have been developed to fully support the MARS concepts [Ste91]. These boards provide mechanisms to achieve a high self-checking coverage and a highly predictable timing behavior.• The introduction of new concepts and mechanisms into the MARS system (e.g. membership protocol, time redundant process execution, shadow component [Kop90], [Kop91]) requires support by the runtime system.• A predictable timing behavior should be achieved by the new kernel. Although the system overhead caused by the old implementation was boundable in principle, the calculated bounds were too high to guarantee the correct timing behavior of an application already at design time [Vrc91].• The self-checking coverage of the MARS components has to be high because the fault tolerance mechanisms of MARS are based on it. Whereas the old kernel was not specifically designed in order to meet this requirement, MARS-2 uses both hardware and software mechanisms to increase the self-checking coverage to a sufficiently high degree.MARS-2 is based on a microkernel operating system architecture in contrast to the monolithic kernel of MARS-1.	computer control company;fault tolerance;kernel (operating system);microkernel;monolithic kernel;operating system;overhead (computing);prototype;real-time clock;real-time computing;runtime system;systems architecture	Johannes Reisinger	1992		10.1145/506378.506416	embedded system;real-time computing;computer science;distributed computing	Embedded	-25.14524357625015	43.47390206995821	13086
87e8470cd5af0d8eda89d66bccd9bf7ba6b384e7	on the development of an efficient coscheduling system	workload;tratamiento paralelo;time average;evaluation performance;performance evaluation;traitement parallele;gestion labor;gang scheduling;evaluacion prestacion;promedio temporal;gestion tâche;scheduling;charge travail;ordonamiento;conmutador;task scheduling;carga trabajo;080307 operating systems;parallel processing;ordonnancement;commutateur;selector switch;moyenne temporelle	Applying gang scheduling can alleviate the blockade problem caused by exclusively space-sharing scheduling. To simply allow jobs to run simultaneously on the same processors as in the conventional gang scheduling, however, may introduce a large number of time slots in the system. In consequence the cost of context switches will be greatly increased, and each running job can only obtain a small portion of resources including memory space and processor utilisation and so no jobs can finish their computations quickly. In this paper we present some experimental results to show that to properly divide jobs into different classes and to apply different scheduling strategies to jobs of different classes can greatly reduce the average number of time slots in the system and significantly improve the performance in terms of average slowdown.	central processing unit;computation;context switch;coscheduling;dspace;experiment;gang scheduling;job stream;memory management;network switch;requirement;scheduling (computing);time complexity	Bing Bing Zhou;Richard P. Brent	2001		10.1007/3-540-45540-X_7	parallel processing;parallel computing;real-time computing;gang scheduling;computer science;operating system;distributed computing;scheduling;multiprocessor scheduling	HPC	-16.20610812120503	44.7242949196442	13098
6a5bf8d656c5f20c85353c7113720f29c3f4dd32	exploiting the trading-paradigm for locating entry-points into distributed object-oriented databases	distributed objects	In this paper we discuss the shortcomings of the conventional methods to locate entry-points into objectbases. Afterward, we present the way the distributed systems community solved a similar problem. We identify the notion of logical dissemination structures as the cornerstone of this solution. Finally, we treat how this notion can be used for object-oriented databases in integrating a trader subsystem and present our current prototype.	database;distributed object;paradigm	Arnd G. Grosse;Dietmar A. Kottmann;Ludwig Keller	1995			distributed object;database;computer science	DB	-36.779762187532434	15.45841266939902	13119
1224e82ac19f7fa986e2b494546eba3751e6296e	running molecular dynamics simulations in a grid environment	globus toolkit;fault tolerant;potential difference;large hadron collider;resource management;collaboration;web service;molecular dynamic simulation;checkpointing;computational modeling;grid computing computational modeling collaboration large hadron collider web services fault tolerance resource management switches checkpointing computer science;fault tolerance;web services;middleware;computer science;switches;grid computing	Grid computing enables resources in different administrative domains to be shared. Researchers are able to collaborate more easily and can gain access to more computing power, enabling more studies to be run and larger problems to be considered. A common middleware employed by grid projects is the Globus Toolkit. Recently, a new version of the Globus Toolkit (GT4), based on standards that build on Web Services, has been released. However, most grid projects using the Globus Toolkit still employ GT2. This paper presents an approach for running molecular dynamics simulations in a mixed GT2/GT4 grid environment. Simulations are automatically checkpointed and migrated between sites as needed, increasing fault tolerance should a site fail. Users do not need to manually discover what resources are available and do not need to learn the potentially different approaches for submitting jobs at different sites. All jobs are submitted to a metascheduler which handles the site specific details. Simulations have been successfully run in a grid environment comprised of resources from across Canada, including resources from the Grid Research Centre at the University of Calgary, WestGrid and ACEnet.	computer simulation;fault tolerance;grid computing;meta-scheduling;middleware;molecular dynamics;web service;westgrid	Cameron Kiddle;Mark Fox;Rob Simmonds	2006	20th International Symposium on High-Performance Computing in an Advanced Collaborative Environment (HPCS'06)	10.1109/HPCS.2006.37	web service;fault tolerance;parallel computing;semantic grid;computer science;resource management;operating system;data grid;database;distributed computing;programming language;world wide web;grid computing	HPC	-29.90222659103406	51.82664749179403	13135
9d30ce04fdff627090b1cc9ccec294d6837a0d0a	use cases and object modelling using argouml	uml;use cases;software engineering;open source software	The key factor in the process of good quality software development is using proper techniques for requirements elicitation, representation and modelling providing foundations to build a logical model of the projected system. One of the most popular functional requirements description methods is use cases and scenarios approach introduced by Jacobson in 1993. Use case diagrams and analysis of functionality necessary for future users of the system allows precise definition of object model of target software system. However there are few fully functional free CASE tools supporting system designer at this software development stage.	argouml;computer-aided software engineering;diagram;functional requirement;requirements elicitation;software development;software release life cycle;software system;systems design	Wojciech Complak;Adam Wojciechowski;Alok Mishra;Deepti Mishra	2011		10.1007/978-3-642-25126-9_35	requirements analysis;software requirements specification;verification and validation;uml tool;computer science;systems engineering;package development process;software design;social software engineering;component-based software engineering;software development;software design description;software engineering;applications of uml;software construction;database;software walkthrough;software deployment;goal-driven software development process;software requirements;use case points;software system	SE	-52.93947022811465	26.268221266044836	13148
64c2acfdef1973fd9c90026c5021b91fadf24749	how effective is mobile browser cache?	cache;smartphone;browser	We report a study on the effectiveness of the mobile browser's cache. The study is based on the browsing history from 24 smartphone users over one year. We make two interesting findings. Firstly, increasing the cache size of the browser on smartphones will not improve the effectiveness of the browser cache very much. Secondly, revalidations greatly reduce the effectiveness of the browser cache. The findings reveal the limitations of the cache design for mobile browsers and motivate a new level of cache design and speculative revalidation and loading.	cpu cache;cache (computing);smartphone;speculative execution;web cache	Zhen Wang;Felix Xiaozhu Lin;Lin Zhong;Mansoor Chishtie	2011		10.1145/2030686.2030693	cache coloring;page cache;cache;computer science;cache invalidation;operating system;smart cache;internet privacy;cache algorithms;cache pollution;world wide web	Web+IR	-36.67773943142582	55.87739115933002	13161
d7bb4ec68b7974564e6e4bbbef0b293bcee6e288	user services in a network environment	software tool;general and miscellaneous mathematics computing and information science;communications;computer networks;long distance;programming 990200 mathematics computers;network management;management;documentation	User services in a network environment are quite different from those in environments with users in close proximity. Communication with remote users with advanced degrees requires different attitudes, knowledge, and techniques. The problems and solutions of education, documentation, software tools, and helping users debug programs over long distances will be presented.	documentation	Jay O'Dell;Arthur Scott	1980		10.1145/800086.802749	human–computer interaction;computer science;theoretical computer science;user analysis;world wide web	HPC	-27.0724759698206	40.456657768449425	13175
8c0f07a66a7595aafd275eb93b4b233d9cb68581	automated support to capture and validate security requirements for mobile apps		Mobile application usage has become widespread and significant as it allows interactions between people and services anywhere and anytime. However, issues related to security have become a major concern among mobile users as insecure applications may lead to security vulnerabilities that make them easily compromised by hackers. Thus, it is important for mobile application developers to validate security requirements of mobile apps at the earliest stage to prevent potential security problems. In this paper, we describe our automated approach and tool, called MobiMEReq that helps to capture and validate the security attributes requirements of mobile apps. We employed the concept of Test Driven Development (TDD) with a model-based testing strategy using Essential Use Cases (EUCs) and Essential User Interface (EUI) models. We also conducted an evaluation to compare the performance and correctness of our tool in various application domains. The results of the study showed that our tool is able to help requirements engineers to easily capture and validate security-related requirements of mobile applications.		Noorrezam Yusop;Massila Kamalrudin;Safiah Sidek;John C. Grundy	2016		10.1007/978-981-10-3256-1_7	internet privacy;world wide web;computer security	SE	-51.20489737188685	47.99995493570977	13182
03e07e00eb05b0f58a4d0ce822c77e2e76643cf8	fault patterns in matlab	programming language;software engineering;end user software engineering;program analysis;static analysis;program development;software quality	Fault patterns are code idioms that may constitute faults. Software engineers have various program analysis techniques and tools to assist them in the detection of such patterns, resulting in increased software quality. End user programmers, however, often lack such support. In this paper we take a first step to address this limitation in the context of Matlab. First, we adapt fault patterns commonly used in other programming languages to Matlab. Second, we present a tool to detect such patterns in fifteen popular Matlab programs. Our results reveal that these simple and quickly identifiable patterns are commonly found in Matlab programs developed by end users and shared across the large Matlab community of end user programmers.		Fidel Nkwocha;Sebastian G. Elbaum	2005	ACM SIGSOFT Software Engineering Notes	10.1145/1082983.1083235	program analysis;software design pattern;computer science;software development;software engineering;programming language;static analysis;software quality;software fault tolerance;computer engineering	SE	-56.44261533896865	36.51487723698048	13204
431c2bbfb35f29eec1eff79dda1d67049605cddb	secure web browsing with the op web browser	design principle;user interfaces formal verification internet operating system kernels security of data;tracking system;web pages;operating system design principle;browser level information flow tracking system;user interface;computer hacking testing communication system security operating systems design methodology kernel user interfaces web pages information analysis delay;browser level information flow tracking system op web browser security operating system design principle formal method innovative network architecture user interface;web security;formal method;innovative network architecture;information flow;formal verification;internet;operating system;browser web security security;web browsing;op web browser security;causal relation;operating system kernels;security;browser;security policy;user interfaces;security of data;innovation network	Current Web browsers are plagued with vulnerabilities, providing hackers with easy access to computer systems via browser-based attacks. Browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed. To enable more secure web browsing, we design and implement a new browser, called the OP Web browser, that attempts to improve the state-of-the-art in browser security. Our overall design approach is to combine operating system design principles with formal methods to design a more secure Web browser by drawing on the expertise of both communities. Our overall design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit. At the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features. To show the utility of our browser architecture, we design and implement three novel security features. First, we develop novel and flexible security policies that allows us to include plugins within our security framework. Our policy removes the burden of security from plugin writers, and gives plugins the flexibility to use innovative network architectures to deliver content while still maintaining the confidentiality and integrity of our browser, even if attackers compromise the plugin. Second, we use formal methods to prove that the address bar displayed within our browser user interface always shows the correct address for the current Web page. Third, we design and implement a browser-level information-flow tracking system to enable post-mortem analysis of browser-based attacks. If an attacker is able to compromise our browser, we highlight the subset of total activity that is causally related to the attack, thus allowing users and system administrators to determine easily which Web site lead to the compromise and to assess the damage of a successful attack. To evaluate our design, we implemented OP and tested both performance and filesystem impact. To test performance, we measure latency to verify OP's performance penalty from security features are be minimal from a users perspective. Our experiments show that on average the speed of the OP browser is comparable to Firefox and the audit log occupies around 80 KB per page on average.	accessibility;browser security;browser user interface;confidentiality;experiment;firefox;formal methods;list of code lyoko episodes;operating system;plug-in (computing);system administrator;systems design;tracking system;vulnerability (computing);web application;web page	Chris Grier;Shuo Tang;Samuel T. King	2008	2008 IEEE Symposium on Security and Privacy (sp 2008)	10.1109/SP.2008.19	browser security;address bar;formal methods;framing;web-based simulation;comet;computer science;information security;operating system;web navigation;web page;internet security;database;internet privacy;client-side scripting;user interface;world wide web;wyciwyg;computer security	Security	-54.95716151383624	59.24857734745676	13211
ce5d2cea60624733dec82243d81d3480686d9f92	on secure profiling	smart card;authorisation;storage management;authentication;security management;smart cards;ubiquitous environments;smart cards secure profiling ubiquitous environments authorization authentication structured data storage secure management;secure management;ubiquitous computing space technology data security information security computer science authorization authentication environmental management secure storage performance analysis;authorization;secure profiling;structured data storage;mobile computing;storage management authorisation mobile computing smart cards;structured data	Ubiquitous environments have several drawbacks to be solved. Most of them are focused on security, and relevant ones are authorization and authentication. Amongst the essential elements to adequately provide solutions, we can find profiles. A profile can be defined as a repository to store structured data from users, networks, devices, applications, etc. As profiles are needed in ubiquitous environments, and these need of secure management as well, in this paper, we provide some initial guidance on the security storage of profiles and on security levels needed for each type of profile. Additionally, we review different alternatives to bear profiles, concluding that smart cards are the most suitable devices	authentication;authorization;smart card	Antonio Muñoz;Jose Antonio Onieva;Javier López	2005	16th International Workshop on Database and Expert Systems Applications (DEXA'05)	10.1109/DEXA.2005.146	smart card;computer science;authorization;internet privacy;mobile computing;world wide web;computer security	DB	-42.50918849987045	59.55451856840506	13227
aaa6006de959b64c1d7d74840db569a560ba5ef7	from algebra transformation to labelled transition systems	labelled transition system;formal specification;computer model	The formal speciication of multiple viewpoints of a system requires multiple speciication formalisms, suitable for the speciic concerns of the viewpoints. For rather diierent viewpoints, such as for instance the information and the computational model of a component of a system, even the underlying paradigms of the speciication formalisms may be diierent. In this paper a general semantical framework for the formal speciication of dynamically evolving systems is presented. Its models, algebra transformation systems, have states whose internal data structures are given by partial algebras, which are manipulated by the application of replacement rules. Its paradigm is the descriptive one of general model theory respectively institutions. Partial observations of the internal state structures yield a translation from algebra transformation systems to labelled transition systems, the granularity of which is determined by the speciication of the admissible observations. Since labelled transition systems can be considered as the general (operational) models for process calculi, this translation allows comparisons between the descriptive paradigm of the model theoretic approach and the operational one of process calculi. Thus consistency checks of multiple viewpoint speciications are supported.	computation;computational model;data structure;emergence;eventual consistency;process calculus;programming paradigm;theory	Martin Große-Rhode	1997		10.1007/3-540-64299-4_39	computer simulation;computer science;formal specification;programming language	Logic	-11.877565637048907	21.897150482107968	13232
9961fb9987b336d35b4881a2a8e3e6835e176e27	automated support for database design (abstract of tutorial)	database design		database design	David S. Reiner	1987			computer science;data mining;database;information retrieval;database design;component-oriented database	DB	-31.66396160764747	9.65292353230457	13239
08832863bc3f041222f381c8ae143f8a66449059	rethinking the library os from the top down	top down method;methode descendante;sistema operativo;virtual machine;drawbridge;movilidad;libos;personality;top down;mobility;reutilizacion;performance;securite informatique;personnalite;abstraction;program library;mobilite;machine virtuelle;abstraccion;reuse;system security;computer security;noyau systeme exploitation;construction system;internet;operating system;system evolution;metodo descendente;system integration;seguridad informatica;personalidad;nucleo de sistema operativo;bibliotheque programme;systeme exploitation;sistema construccion;library os;operating system kernels;experimentation;maquina virtual;biblioteca programa;systeme construction;reutilisation	This paper revisits an old approach to operating system construc-tion, the library OS, in a new context. The idea of the library OS is that the personality of the OS on which an application depends runs in the address space of the application. A small, fixed set of abstractions connects the library OS to the host OS kernel, offering the promise of better system security and more rapid independent evolution of OS components.  We describe a working prototype of a Windows 7 library OS that runs the latest releases of major applications such as Microsoft Excel, PowerPoint, and Internet Explorer. We demonstrate that desktop sharing across independent, securely isolated, library OS instances can be achieved through the pragmatic reuse of net-working protocols. Each instance has significantly lower overhead than a full VM bundled with an application: a typical application adds just 16MB of working set and 64MB of disk footprint. We contribute a new ABI below the library OS that enables application mobility. We also show that our library OS can address many of the current uses of hardware virtual machines at a fraction of the overheads. This paper describes the first working prototype of a full commercial OS redesigned as a library OS capable of running significant applications. Our experience shows that the long-promised benefits of the library OS approach better protection of system integrity and rapid system evolution are readily obtainable.	address space;application binary interface;computer security;desktop sharing;internet explorer;linux;microsoft windows;operating system;overhead (computing);prototype;system integrity;top-down and bottom-up design;virtual machine;working set	Donald E. Porter;Silas Boyd-Wickizer;Jon Howell;Reuben Olinsky;Galen C. Hunt	2011		10.1145/1950365.1950399	embedded system;the internet;simulation;performance;computer science;virtual machine;basic sequential access method;operating system;process management;top-down and bottom-up design;reuse;abstraction;basic direct access method;personality;mobile computing;system integration	Arch	-27.212730540849456	42.362151153442575	13248
ea95abcf65d1ad7cd0bf3e0789d821d1711ae1dd	xperm: fast index canonicalization for tensor computer algebra	graph theory;mathematica;symbolic computation;key word index;programming language;distributed programs;02 70 wz;computer algebra system;large scale;operating system;04 20 cv;indexation;index canonicalization;computer algebra;02 10 0x	We present a very fast implementation of the Butler-Portugal algorithm for index canonicalization with respect to permutation symmetries. It is called xPerm, and has been written as a combination of a Mathematica package and a C subroutine. The latter performs the most demanding parts of the computations and can be linked from any other program or computer algebra system. We demonstrate with tests and timings the effectively polynomial performance of the Butler-Portugal algorithm with respect to the number of indices, though we also show a case in which it is exponential. Our implementation handles generic tensorial expressions with several dozen indices in hundredths of a second, or one hundred indices in a few seconds, clearly outperforming all other current canonicalizers. The code has been already under intensive testing for several years and has been essential in recent investigations in large-scale tensor computer algebra.	algorithm;computer algebra system;polynomial;s-expression;subroutine;symbolic computation;time complexity;wolfram mathematica	José M. Martín-García	2008	Computer Physics Communications	10.1016/j.cpc.2008.05.009	symbolic computation;computer science;theoretical computer science;mathematics;algorithm	Networks	-11.912173152547648	34.07458852166345	13251
9ef88191eb0f2701b63847c919f23504f1c9ca32	my compiler does not understand me	programming language	Until our programming languages catch up, code will be full of horrors.	compiler;programming language	Poul-Henning Kamp	2012	Commun. ACM	10.1145/2209249.2209265	computer architecture;parallel computing;computer science;programming language	PL	-22.376732380532886	33.710504087032994	13302
fe7aa435d3cd3a667b8cf31f5e327a2c4b479286	ha-vmsi: a lightweight virtual machine isolation approach with commodity hardware for arm	virtualization;vm security;arm trustzone;multi tenant cloud	Once compromising the hypervisor, remote or local adversaries can easily access other customers' sensitive data in the memory and context of guest virtual machines (VMs). VM isolation is an efficient mechanism for protecting the memory of guest VMs from unauthorized access. However, previous VM isolation systems either modify hardware architecture or introduce a software module without being protected, and most of them focus on the x86 architecture.  This paper proposes HA-VMSI, a lightweight hardware-assisted VM isolation approach for ARM, to provide runtime protection of guest VMs, even with a compromised hypervisor. In the ARM TrustZone secure world, a thin security monitor is introduced as HA-VMSI's entire TCB. Hence, the security monitor is much less vulnerable and safe from attacks that can compromise the hypervisor. The key of HA-VMSI is decoupling the functions of memory isolation among VMs from the hypervisor into the security monitor. As a result, the hypervisor can only update the Stage-2 page tables of VMs via the security monitor, which inspects and approves each new mapping. It is worth noting that HA-VMSI is more secure and effective than current software approaches, and more flexible and compatible than hardware approaches. We have implemented a prototype for KVM hypervisor with multiple Linux as guest OSes on Juno board. The security assessment and performance evaluation show that HA-VMSI is effective, efficient and practical.	arm architecture;address space;authorization;commodity computing;coupling (computer programming);direct memory access;event-driven programming;hypervisor;juno (company);linux;linux;memory protection;microprocessor development board;page (computer memory);page table;performance evaluation;prototype;trusted computing base;virtual machine;x86	Min Zhu;Bibo Tu;Wei Wei;Dan Meng	2017		10.1145/3050748.3050767	embedded system;real-time computing;virtualization;storage hypervisor;computer science;operating system	Security	-54.169152626132025	56.3488990630962	13309
6ac7dd8c6a0f99ce760793dd5079786c8683ca14	symbolic execution with mixed concrete-symbolic solving	random testing;linear constraint;satisfiability;conference paper;symbolic execution;test case generation;decision procedure;constraint solving;proceedings international;static program analysis;dart	Symbolic execution is a powerful static program analysis technique that has been used for the automated generation of test inputs. Directed Automated Random Testing (DART) is a dynamic variant of symbolic execution that initially uses random values to execute a program and collects symbolic path conditions during the execution. These conditions are then used to produce new inputs to execute the program along different paths. It has been argued that DART can handle situations where classical static symbolic execution fails due to incompleteness in decision procedures and its inability to handle external library calls.  We propose here a technique that mitigates these previous limitations of classical symbolic execution. The proposed technique splits the generated path conditions into (a) constraints that can be solved by a decision procedure and (b) complex non-linear constraints with uninterpreted functions to represent external library calls. The solutions generated from the decision procedure are used to simplify the complex constraints and the resulting path conditions are checked again for satisfiability. We also present heuristics that can further improve our technique. We show how our technique can enable classical symbolic execution to cover paths that other dynamic symbolic execution approaches cannot cover. Our method has been implemented within the Symbolic PathFinder tool and has been applied to several examples, including two from the NASA domain.	boolean satisfiability problem;decision problem;emoticon;heuristic (computer science);nonlinear system;random testing;static program analysis;symbolic execution;uninterpreted function	Corina S. Pasareanu;Neha Rungta;Willem Visser	2011		10.1145/2001420.2001425	random testing;computer science;theoretical computer science;programming language;concolic testing;symbolic trajectory evaluation;algorithm;static program analysis;satisfiability	SE	-16.845365015345177	25.514314022397414	13313
d86c821e65bbdd0a597a2ba4fe09383c185d7e7d	a’um —a stream-based concurrent object-oriented language—	object oriented language;programming language;large scale;object oriented;concurrent programs	This paper presents a computation model and its programming language,A’UM,* as a result of our pursuit of high parallelism and high expressivity for the development of a large scale software. By basing it on streams and integrating it with objects and relations,A’UM realizes an elegant model, natural representation and efficient execution, all at once, that have never been done by any other approaches.	model of computation;parallel computing;programming language	Kaoru Yoshida;Takashi Chikayama	1988	New Generation Computing	10.1007/BF03037203	natural language processing;first-generation programming language;natural language programming;very high-level programming language;language primitive;object language;computer science;programming language implementation;database;low-level programming language;programming language;object-oriented programming;high-level programming language	HPC	-24.808460500492977	22.65700803297944	13318
7e895b11b562a62e5393b976ec3bb444d42a2aea	a conceptional framework for modeling complex adaptation of collaborative networks		The paper elaborates methodological basis of collaborative networks (CN) complex adaptation. We consider challenges and underlying principles of the CN complex adaptation. Subsequently, the DIMA-methodology of the integrated CN modeling is considered. The paper ends with the presentation of the fivelevel CN complex adaptation concept and summarizing DIMA-methodology application in the CN complex adaptation settings.	collaborative network;dima;resource description framework	Dmitry A. Ivanov;Joachim Käschel;Boris V. Sokolov;Alexander V. Arkhipov	2006		10.1007/978-0-387-38269-2_2	systems engineering;collaborative network;computer science	ML	-60.936175896485516	16.151123033451345	13329
76467d177cfe9d9f2881250e8907257eae52643d	relational data base model simplified and generalized as algebraic lattice with attribution	relational data		relational database management system	Jørgen Fischer Nilsson	1994			relational model;data mining;conjunctive query;codd's theorem;lattice (order);relational database;statistical relational learning;relational theory;computer science;relational calculus	DB	-29.99073804006442	9.120301880626984	13332
3613b76d178d0172fbc067845310a1f506948b8f	component-based design for adaptive large-scale infectious disease simulation	incremental development;multi agent system;component based software engineering;heterogeneous computing;large scale;infectious disease;component based design;performance tuning	Component-based design improves productivity by concentrating development efforts on one component at a time without having to worry about a change having an application-wide effect. In this paper, we demonstrate the usefulness of componentbased approach in the development of an infectious disease simulator. Specifically, we have explored the possibility of self performance tuning at runtime through the use of hot-swappable components by incrementally develop optimised component variants easily. The application has achieved 4 times speedup using dynamic kernel adaptation and a further 5.3 times speedup through parallelisation on a multicore and GPU server.	component-based software engineering;graphics processing unit;hot swapping;image scaling;kernel (operating system);mathematical optimization;multi-core processor;parallel computing;performance tuning;random number generation;run time (program lifecycle phase);runtime system;server (computing);simulation;speedup;systems design;thread (computing)	Thorsten Matthias Riechers;Shyh-Hao Kuo;Rick Siow Mong Goh;Terence Hung	2010		10.1016/j.procs.2010.04.057	real-time computing;simulation;infectious disease;computer science;component-based software engineering;operating system;distributed computing;programming language	Mobile	-10.935413513496648	39.044699796502925	13334
34613e6d5ebc447a29db30b82c98ed085959c0e6	quality of experience: user's perception about web services	service composition;service selection;quality of service quality of experience user perception web service composition high quality services web service selection web service discovery qoe qos;quality of service web services data mining synchronization html equations security;data mining;quality of experience;html;quality of experience service composition service selection quality of service;synchronization;web services;quality of service;security;web services quality of experience quality of service	Web service composition enables seamless and dynamic integration of Web services. The behavior of participant Web services determines the overall performance of a composition. Therefore, it is important to choose high quality services for service composition. Existing Web service selection and discovery approaches rely on non-functional aspects (also known as quality of service or QoS), e.g., response time and availability. Though these parameters are crucial for selecting Web services, they may not reflect the user's perspective of quality. In this paper, we explore the feasibility of incorporating perceived quality from user's perspective for service selection and composition. We name such quality attributes as quality of experience (QoE). First, we propose a solution that automatically mines and identifies QoE attributes from the Web. Second, we study the application of such dynamically extracted QoE attributes for service selection. For the evaluation purpose, we collected more than 34,000 reviews from 58 different services in six domains. Our findings show that it is possible to automatically identify QoE attributes with an average precision and recall of 92 and 80 percent respectively. Our study shows that there is a strong positive correlation between QoS and QoE. Hence QoE can be used during service selection especially when QoS data are not available. Furthermore, we found 70 percent of service discovery queries indeed contain QoE attributes showing the importance of QoE attributes during the service discovery phase.	aggregate data;bootstrapping (compilers);display resolution;information retrieval;list of system quality attributes;precision and recall;quality of service;real life;response time (technology);seamless3d;service composability principle;service discovery;usability testing;web search query;web service;world wide web	Bipin Upadhyaya;Ying Zou;Iman Keivanloo;Joanna W. Ng	2015	IEEE Transactions on Services Computing	10.1109/TSC.2014.2387751	service provider;web service;service level requirement;service level objective;synchronization;mobile qos;quality of service;html;differentiated service;computer science;information security;service delivery framework;ws-policy;service design;database;service discovery;internet privacy;law;world wide web;computer security;service system	Web+IR	-48.88378994390992	43.341432719269	13344
63a2950c8cf50637a9414ede0a9fbf9c6fadb9f2	searching for the optimal proof schedule in a proof-carrying survivability paradigm - a dynamic, interactive approach	proof generation;user verification;reliability;theorem proving failure analysis formal verification reliability security of data;schedules security fault tolerance fault tolerant systems dynamic scheduling educational institutions systematics;malicious attack;prototyping system;systematics;dynamic proof evidence collection process;optimal proof schedule;prototyping system optimal proof schedule proof carrying survivability malicious attack system failure user verification threshold based survivability requirement model user requirement threshold operator proof generation interactive proof evidence collection process dynamic proof evidence collection process;theorem proving;failure analysis;threshold operator;formal verification;fault tolerant systems;proof carrying survivability;user requirement;fault tolerance;schedules;model;model information system survivability proof;system failure;threshold based survivability requirement model;information system;survivability;interactive proof evidence collection process;security;proof;security of data;dynamic scheduling	Survivability represents a system's capability to withstand malicious attacks and system failures and to provide essential services in a challenging environment. In a proof-carrying survivability scenario, a user publishes his/her survivability requirements and a system provider constructs a proof to show that the system satisfies those requirements. Finally, the user verifies that the proof is valid. In this paper, we propose a systematic approach for the system provider to search for an optimal proof schedule to construct a valid proof. Our framework is applied to a threshold-based survivability requirement model, where the user's requirements are represented using pre-defined threshold operators and proof generation relies on the evaluations of system survivability properties by authorized evaluators. We develop algorithms and techniques to explore different proof options and schedules and identify the most efficient (optimal) schedule. We study how the dynamic, interactive proof evidence collection process proceeds between the system provider and the evaluators. A prototyping system has been developed to implement the framework.	algorithm;authorization;interactive proof system;malware;requirement;schedule (computer science)	Yanjun Zuo;John Babin	2012	2012 Ninth International Conference on Information Technology - New Generations	10.1109/ITNG.2012.35	failure analysis;fault tolerance;dynamic priority scheduling;formal verification;schedule;computer science;information security;user requirements document;reliability;proof;distributed computing;systematics;automated theorem proving;programming language;computer security;information system	Embedded	-59.90074457541328	52.2298003215917	13345
07c9a1afc77d0c197f0f85519a323290be05853f	an interprocedural framework for placement of asynchronous i/o operations	circuit retiming;guaranteed heuristic;cyclic scheduling;list scheduling;technical report;software pipelining	Overlapping memory accesses with computations is a standard technique for improving performance on modern architectures, which have deep memory hierarchies. In this paper, we present a compiler technique for overlapping accesses to secondary memory (disks) with computation. We have developed an Interprocedural Balanced Code Placement (IBCP) framework, which performs analysis on arbitrary recursive procedures and arbitrary control ow and replaces synchronous I/O operations with a balanced pair of asynchronous operations. We demonstrate how this analysis is useful for the applications which perform frequent and large accesses to secondary memory, including the applications which snapshot or checkpoint their computations or the out-of-core applications.	asynchronous i/o;auxiliary memory;balanced line;compiler;computation;input/output;memory hierarchy;out-of-core algorithm;recursion;snapshot (computer storage);transaction processing system	Gagan Agrawal;Anurag Acharya;Joel H. Saltz	1996		10.1145/237578.237636	fair-share scheduling;software pipelining;parallel computing;real-time computing;computer science;technical report;distributed computing	PL	-10.634058709333543	48.4545297297297	13346
6ffcd9a1bb0ad5ec1c6925f52c8c1bd9d7ee6216	optimizing disjunctive queries with expensive predicates	object oriented;relational model;process planning;optimal algorithm	In this work, we propose and assess a technique called bypass processing for optimizing the evaluation of disjunctive queries with expensive predicates. The technique is particularly useful for optimizing selection predicates that contain terms whose evaluation costs vary tremendously; e.g., the evaluation of a nested subquery or the invocation of a user-defined function in an object-oriented or extended relational model may be orders of magnitude more expensive than an attribute access (and comparison). The idea of bypass processing consists of avoiding the evaluation of such expensive terms whenever the outcome of the entire selection predicate can already be induced by testing other, less expensive terms. In order to validate the viability of bypass evaluation, we extend a previously developed optimizer architecture and incorporate three alternative optimization algorithms for generating bypass processing plans.	algorithm;disjunctive normal form;mathematical optimization;optimizing compiler;relational model;sql;user-defined function	Alfons Kemper;Guido Moerkotte;Klaus Peithner;Michael Steinbrunn	1994		10.1145/191839.191906	relational model;computer science;data mining;database;programming language;object-oriented programming;algorithm	DB	-26.793096065028536	11.533537401915464	13368
3b18e0e4446a10bc2d4461d929e34c6de571f754	virtual transport enterprise integration	integrable system;reference model;information flow;enterprise integration;reference architecture	A virtual transport enterprise (VTE) is a temporary alliance of independent transport enterprises that come together to share resources, skills, and costs, supported by Information and Communication Technologies, in order to better attend market opportunities. To design an efficient and flexible VTE that presents the semblance of a single enterprise to the customers, is a very complex task. There are an extensive set of approaches to help in the co-ordination of the different objectives during of the design and execution of a single enterprise. However, they do not provide the specialized tools for the integration of VTE’s.	enterprise integration	Ricardo Chalmeta	2000	Transactions of the SDPS		enterprise architecture framework;functional software architecture;reference architecture;enterprise application integration;enterprise systems engineering;enterprise software;systems engineering;engineering;knowledge management;architecture domain;operations architecture;operations management;enterprise architecture management;solution architecture;enterprise architecture;enterprise integration;view model;enterprise information security architecture;enterprise information system;data architecture;business architecture;enterprise life cycle	HPC	-57.66881325429993	13.968469976232944	13376
8e19f01fc6b3e934b7383f319273d2397e8853d7	infrastructure-aware autonomic manager for change management	change management;small to medium enterprises;policy language;it environment;infrastructure aware autonomic manager;medium to large size organizations;small to medium enterprises management of change;management of change;business relevant application infrastructure aware autonomic manager change management it environment medium to large size organizations;business relevant application;it management;engines environmental management automation crisis management computer network management availability computer science network servers application software resource management	Typical IT environments of medium to large size organizations consist of tens of networks that connect hundreds of servers to support the running of a large variety of business-relevant applications; usually from different vendors. Change management is an important management processes that, if automated, can have a direct impact on increasing service availability in IT environments. Although such automation is considered important, the requirements of the appropriate policy engine, and policy language to express both high level and low level policies are far from clear. In this paper, we report our experiences in addressing these problems. In particular, we concentrate on availability policies - policies through which IT managers express the required availability of systems - and the autonomic manager that enforces them.	autonomic computing;autonomic networking;change management (engineering);high- and low-level;high-level programming language;requirement	Hady S. AbdelSalam;Kurt Maly;Ravi Mukkamala;Mohammad Zubair	2007	Eighth IEEE International Workshop on Policies for Distributed Systems and Networks (POLICY'07)	10.1109/POLICY.2007.27	systems management;systems engineering;knowledge management;environmental resource management;change management;business;information management	DB	-31.07839180424962	56.75513627264725	13411
20382716e40872d69cd8284b11dbc33a029c2d6e	horn clause contraction functions: belief set and belief base approaches		Standard approachs to belief change assume that the underlying logic contains classical propositional logic. Recently there has been interest in investigating approaches to belief change, specifically contraction, in which the underlying logic is not as expressive as full propositional logic. In this paper we consider approaches to belief contraction in Horn knowledge bases. We develop two broad approaches for Horn contraction, corresponding to the two major approaches in belief change, based on Horn belief sets and Horn belief bases. We argue that previous approaches, which have taken Horn remainder sets as a starting point, have undesirable properties, and moreover that not all desirable Horn contraction functions are captured by these approaches. This is shown in part by examining model-theoretic considerations involving Horn contraction. For Horn belief set contraction, we develop an account based in terms of weak remainder sets. Maxichoice and partial meet Horn contraction is specified, along with a consideration of package contraction. Following this we consider Horn belief base contraction, in which the underlying knowledge base is not necessarily closed under the Horn consequence relation. Again, approaches to maxichoice and partial meet belief set contraction are developed. In all cases, constructions of the specific operators and sets of postulates are provided, and representation results are obtained. As well, we show that problems arising with earlier work are resolved by these approaches.	belief revision;horn clause;knowledge base;propositional calculus;theory	James P. Delgrande;Renata Wassermann	2010			algorithm;operator (computer programming);horn clause;knowledge base;horn-satisfiability;remainder;propositional calculus;computer science;contraction (grammar)	AI	-15.857851058389137	7.667599441684	13431
8d7a6d201ee49463228a6b424456ee9cb2a4d871	a functional programming approach to hypermedia authoring	unfold;functional programing;functional programming;traversal;co induction;anamorphism;level order;breadth first;fold;program calculation	Hypermedia authoring pie951 faces numerous challenges today. The expansive growth of the WWW and the increasing accesibility to multimedia technologies have made hypermedia preferable to traditional media. As expected, this growth has risen the requirements for tools which ease the hypermedia-generation process. While naive approaches to authoring consist in using WYSIWYG tools directly design and implementation is done in a unique phase -, a more structured approach is needed when authoring in-the-large. Several methodologies were proposed for systematic hypermedia design. Methodologies make a clear separation between the different phases in the hypermedia generation process (conceptual design, navigational design, implementation, etc.) see for example pSB95, SR95]. A common feature of these methodologies is the separation between the design and the implementation of the hypermedia application. In [FNNQB], it is argued that these approaches can leave a wide gap between initial design and iinal production. Consequently, design problems are detected very late and can be very expensive to fix. In [FNNQG, NN95] it is stated that, while a structured approach is necessary, low-cost prototyping is also very important for early evaluation of hypermedias. Functional Programming (FP for short) offers a number of advantages to programmers [BWSS, Hug89]. It features a high level of abstraction, modularity and a con&e declarative style which makes programs easy to read and understand. An attractive feature of FP is that it allows programmers to map their ideas almost directly to functional code, bridging the gap between specification and implementation. This work presents HyCom (Hypermedia Combinators), an hypermedia-authoring framework based in the functional language H&cell [PH+97]. HyCom provides constructions to specify navigational structures (nodes, links) and interface issues in an abstract, platform-independent way, in a style similar to that in [vDMQB]. HyCom uses transformers and combinators a programming technique widely used in FP [HugQS, FJQ6, Hud96] -to express relationships between hypermedia components. HyCom offers several attractive features. On one hand, a functional language is used as a hypermedia-specification language, and thus authors benefit from FP’s well-known expressiveness. On the other hand, using FP provides a high-abstraction level in the design process, and also allows a modular, structured approach to design. Furthermore, hypermedia designs expressed in HyCom can be automatically rendered to WWW pages or other platforms, by using appropiate rendering functions thus prototyping and final implementation involves no additional effort from the author. This is a consequence of the short distance between specification and implementation obtained by using FP. We propose using a FP-based framework for specifying and building hypermedias. We think this is an interesting proposal, since FP expressiveness is well-suited for ab stract specifications, which indeed can be translated almost directly to working implementations a desirable feature for achieving low-cost prototyping. Furthermore, we think this is an original contribution, since the hypermedia field has been traditionally distant from FP.	abstraction layer;bridging (networking);combinatory logic;declarative programming;expressive power (computer science);functional programming;high-level programming language;hypermedia;modularity (networks);programmer;requirement;specification language;transformers;www;wysiwyg	Daniel H. Marcos;Pablo E. Martínez López;Walter A. Risi	1998		10.1145/289423.289476	breadth-first search;computer science;theoretical computer science;fold;programming language;functional programming;tree traversal;algorithm	PL	-26.86913472315755	26.032239201359925	13438
1819ffd22b7b13e3f920ef816d9bd056197c20a4	poster: energy-performance tradeoffs in multilevel checkpoint strategies	libraries;complexity theory;checkpointing power demand libraries power measurement encoding laboratories complexity theory;checkpointing;hpc energy performance tradeoffs multilevel checkpoint library fault tolerance schemes ibm blue gene q fti high performance computing;encoding;power demand;software fault tolerance checkpointing parallel machines;power measurement	Increased complexity of computer architectures, consideration of power constraints, and expected failure rates of hardware components make the design and analysis of energy-efficient fault-tolerance schemes an increasingly challenging and important task. We develop run-time and study FTI, a multilevel checkpoint library, on an IBM Blue Gene/Q. We show that FTI has a low energy footprint and that, consequently optimal checkpoint-interval values with respect to time and energy are similar.	blue gene;computer architecture;fault tolerance;transaction processing system	Leonardo Arturo Bautista-Gomez;Prasanna Balaprakash;Mohamed-Slim Bouguerra;Stefan M. Wild;Franck Cappello;Paul D. Hovland	2014	2014 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2014.6968749	computer architecture;parallel computing;real-time computing;computer science;operating system;encoding	HPC	-4.913691297001185	48.63131281444562	13443
c855398daa57c19ccf44cecc12277f3d1e8f2695	intermediate logics preserving admissible inference rules of heyting calculus	intuitionistic propositional logic;tabular logic;heyting s calculus;inference rule;kripke model;modal logic;admissible inference rule;intermediate logic;finite model property	The aim of this paper is to look from the point of view of admissibility of inference rules a t intermediate logics having the finite model property which extend Heyting’s intuitionistic propositional logic H. A semantic description for logics with the finite model property preserving all admissible inference rules for H is given. I t is shown that there are continuously many logics of this kind. Three special tabular intermediate logics X i , 1 5 i 5 3, are given which describe all tabular logics preserving admissibility: a tabular logic X preserves all admissible rules for H iff X has width not more than 2 and is not included in each X i . MSC: 03B55, 03B20.	finite model property;propositional calculus;table (information)	Vladimir V. Rybakov	1993	Math. Log. Q.	10.1002/malq.19930390144	modal logic;monoidal t-norm logic;t-norm fuzzy logics;structural rule;discrete mathematics;classical logic;principle of bivalence;many-valued logic;intuitionistic logic;intermediate logic;non-monotonic logic;łukasiewicz logic;mathematics;proof calculus;kripke semantics;algorithm;rule of inference	Logic	-11.80160987312976	13.144488571117808	13445
d085b4da3375c086b1df3b9dc066b49151bb36a6	categorizing or generating relation types and organizing ontology design patterns		This article proposes an ontology design pattern leading knowledge providers to represent knowledge in more normalized, precise and inter-related ways, hence in ways that help automatic matching and exploitation of knowledge from different sources. This pattern is also a knowledge sharing best practice that is domain and language independent. It can be used as a criteria for measuring the quality of an ontology. This pattern is: “using binary relation types directly derived from concept types, especially role types or process types”. The article explains this pattern and relates it to other ones, thereby illustrating ways to organize such patterns. It also provides a top-level ontology for generating relation types from concept types, e.g., those from lexical ontologies such as those derived from the WordNet lexical database. This generation and categorization helps normalizing knowledge, reduces having to introduce new relation types and helps keeping all the types organized.	alternating bit protocol;best practice;categorization;concept map;fractal dimension;ibm basic programming support;interactivity;knowledge base;knowledge modeling;lexical database;multi-source;normalization property (abstract rewriting);ontology (information science);organizing (structure);partial template specialization;rm-odp;sparql;server (computing);software design pattern;upper ontology;wordnet	Philippe Martin;Jérémy Bénard	2017	2017 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2017F146	artificial intelligence;data mining;ontology;computer science;categorization;machine learning;software design pattern;lexical database;semantics;knowledge sharing;ontology (information science);wordnet	AI	-41.86659306580276	6.070602877182553	13452
